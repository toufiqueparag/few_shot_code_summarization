## Few-shot training LLMs for project-specific code-summarization

This repository contains the dataset and scripts from our ASE-NIER paper <strong><em>[Few-shot training LLMs for project-specific code-summarization](https://dl.acm.org/doi/abs/10.1145/3551349.3559555)</em></strong>. There are six folders for six programming langages (i.e., Java, Python, Ruby, JS, PHP, Go) and 8 folders for same project experiments.
Each folder contains 4 file.

- final_1.gold: contains the reference/gold summarization.
- test.jsonl: contains the test instances. Each line contains one test sample.
- train_10.txt: contains 10 code samples with summary to be used for few-shot learning.
- result_10.output: contains the output generated by the model. Note that this will be automatically generated after running process.py. 
<br><br>

To execute python process.py, we need two arguments (i.e., OpenAI key and data folder). Please insert your OpenAI key at the right position. Find an example to perform Java code summarization below.
<br><br><em>python process.py --open_key <strong>your OpenAI key</strong> --data_folder Java </em><br><br>

To evaluate the model's performance run the following command. This file takes two arguments: reference_file and model_output. Please find an example below. 
<br><br> <em>python BLEU.py --ref_file Java/final_1.gold --model_output Java/result_10.output </em><br><br>

For baseline models, please follow the repositories given below:<br>

CodeBERT/GraphCodeBERT models: [https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text)
CodeT5 model: [https://github.com/salesforce/CodeT5](https://github.com/salesforce/CodeT5)
