{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/sample.py", "func_name": "sample_chain", "original_string": "def sample_chain(\n    num_results,\n    current_state,\n    previous_kernel_results=None,\n    kernel=None,\n    num_burnin_steps=0,\n    num_steps_between_results=0,\n    trace_fn=lambda current_state, kernel_results: kernel_results,\n    return_final_kernel_results=False,\n    parallel_iterations=10,\n    name=None,\n):\n  \"\"\"Implements Markov chain Monte Carlo via repeated `TransitionKernel` steps.\n\n  This function samples from an Markov chain at `current_state` and whose\n  stationary distribution is governed by the supplied `TransitionKernel`\n  instance (`kernel`).\n\n  This function can sample from multiple chains, in parallel. (Whether or not\n  there are multiple chains is dictated by the `kernel`.)\n\n  The `current_state` can be represented as a single `Tensor` or a `list` of\n  `Tensors` which collectively represent the current state.\n\n  Since MCMC states are correlated, it is sometimes desirable to produce\n  additional intermediate states, and then discard them, ending up with a set of\n  states with decreased autocorrelation.  See [Owen (2017)][1]. Such \"thinning\"\n  is made possible by setting `num_steps_between_results > 0`. The chain then\n  takes `num_steps_between_results` extra steps between the steps that make it\n  into the results. The extra steps are never materialized (in calls to\n  `sess.run`), and thus do not increase memory requirements.\n\n  Warning: when setting a `seed` in the `kernel`, ensure that `sample_chain`'s\n  `parallel_iterations=1`, otherwise results will not be reproducible.\n\n  In addition to returning the chain state, this function supports tracing of\n  auxiliary variables used by the kernel. The traced values are selected by\n  specifying `trace_fn`. By default, all kernel results are traced but in the\n  future the default will be changed to no results being traced, so plan\n  accordingly. See below for some examples of this feature.\n\n  Args:\n    num_results: Integer number of Markov chain draws.\n    current_state: `Tensor` or Python `list` of `Tensor`s representing the\n      current state(s) of the Markov chain(s).\n    previous_kernel_results: A `Tensor` or a nested collection of `Tensor`s\n      representing internal calculations made within the previous call to this\n      function (or as returned by `bootstrap_results`).\n    kernel: An instance of `tfp.mcmc.TransitionKernel` which implements one step\n      of the Markov chain.\n    num_burnin_steps: Integer number of chain steps to take before starting to\n      collect results.\n      Default value: 0 (i.e., no burn-in).\n    num_steps_between_results: Integer number of chain steps between collecting\n      a result. Only one out of every `num_steps_between_samples + 1` steps is\n      included in the returned results.  The number of returned chain states is\n      still equal to `num_results`.  Default value: 0 (i.e., no thinning).\n    trace_fn: A callable that takes in the current chain state and the previous\n      kernel results and return a `Tensor` or a nested collection of `Tensor`s\n      that is then traced along with the chain state.\n    return_final_kernel_results: If `True`, then the final kernel results are\n      returned alongside the chain state and the trace specified by the\n      `trace_fn`.\n    parallel_iterations: The number of iterations allowed to run in parallel. It\n      must be a positive integer. See `tf.while_loop` for more details.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"mcmc_sample_chain\").\n\n  Returns:\n    checkpointable_states_and_trace: if `return_final_kernel_results` is\n      `True`. The return value is an instance of\n      `CheckpointableStatesAndTrace`.\n    all_states: if `return_final_kernel_results` is `False` and `trace_fn` is\n      `None`. The return value is a `Tensor` or Python list of `Tensor`s\n      representing the state(s) of the Markov chain(s) at each result step. Has\n      same shape as input `current_state` but with a prepended\n      `num_results`-size dimension.\n    states_and_trace: if `return_final_kernel_results` is `False` and\n      `trace_fn` is not `None`. The return value is an instance of\n      `StatesAndTrace`.\n\n  #### Examples\n\n  ##### Sample from a diagonal-variance Gaussian.\n\n  I.e.,\n\n  ```none\n  for i=1..n:\n    x[i] ~ MultivariateNormal(loc=0, scale=diag(true_stddev))  # likelihood\n  ```\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  dims = 10\n  true_stddev = np.sqrt(np.linspace(1., 3., dims))\n  likelihood = tfd.MultivariateNormalDiag(loc=0., scale_diag=true_stddev)\n\n  states = tfp.mcmc.sample_chain(\n      num_results=1000,\n      num_burnin_steps=500,\n      current_state=tf.zeros(dims),\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=likelihood.log_prob,\n        step_size=0.5,\n        num_leapfrog_steps=2),\n      trace_fn=None)\n\n  sample_mean = tf.reduce_mean(states, axis=0)\n  # ==> approx all zeros\n\n  sample_stddev = tf.sqrt(tf.reduce_mean(\n      tf.squared_difference(states, sample_mean),\n      axis=0))\n  # ==> approx equal true_stddev\n  ```\n\n  ##### Sampling from factor-analysis posteriors with known factors.\n\n  I.e.,\n\n  ```none\n  # prior\n  w ~ MultivariateNormal(loc=0, scale=eye(d))\n  for i=1..n:\n    # likelihood\n    x[i] ~ Normal(loc=w^T F[i], scale=1)\n  ```\n\n  where `F` denotes factors.\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Specify model.\n  def make_prior(dims):\n    return tfd.MultivariateNormalDiag(\n        loc=tf.zeros(dims))\n\n  def make_likelihood(weights, factors):\n    return tfd.MultivariateNormalDiag(\n        loc=tf.matmul(weights, factors, adjoint_b=True))\n\n  def joint_log_prob(num_weights, factors, x, w):\n    return (make_prior(num_weights).log_prob(w) +\n            make_likelihood(w, factors).log_prob(x))\n\n  def unnormalized_log_posterior(w):\n    # Posterior is proportional to: `p(W, X=x | factors)`.\n    return joint_log_prob(num_weights, factors, x, w)\n\n  # Setup data.\n  num_weights = 10 # == d\n  num_factors = 40 # == n\n  num_chains = 100\n\n  weights = make_prior(num_weights).sample(1)\n  factors = tf.random_normal([num_factors, num_weights])\n  x = make_likelihood(weights, factors).sample()\n\n  # Sample from Hamiltonian Monte Carlo Markov Chain.\n\n  # Get `num_results` samples from `num_chains` independent chains.\n  chains_states, kernels_results = tfp.mcmc.sample_chain(\n      num_results=1000,\n      num_burnin_steps=500,\n      current_state=tf.zeros([num_chains, num_weights], name='init_weights'),\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=unnormalized_log_posterior,\n        step_size=0.1,\n        num_leapfrog_steps=2))\n\n  # Compute sample stats.\n  sample_mean = tf.reduce_mean(chains_states, axis=[0, 1])\n  # ==> approx equal to weights\n\n  sample_var = tf.reduce_mean(\n      tf.squared_difference(chains_states, sample_mean),\n      axis=[0, 1])\n  # ==> less than 1\n  ```\n\n  ##### Custom tracing functions.\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  likelihood = tfd.Normal(loc=0., scale=1.)\n\n  def sample_chain(trace_fn):\n    return tfp.mcmc.sample_chain(\n      num_results=1000,\n      num_burnin_steps=500,\n      current_state=0.,\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=likelihood.log_prob,\n        step_size=0.5,\n        num_leapfrog_steps=2),\n      trace_fn=trace_fn)\n\n  def trace_log_accept_ratio(states, previous_kernel_results):\n    return previous_kernel_results.log_accept_ratio\n\n  def trace_everything(states, previous_kernel_results):\n    return previous_kernel_results\n\n  _, log_accept_ratio = sample_chain(trace_fn=trace_log_accept_ratio)\n  _, kernel_results = sample_chain(trace_fn=trace_everything)\n\n  acceptance_prob = tf.exp(tf.minimum(log_accept_ratio_, 0.))\n  # Equivalent to, but more efficient than:\n  acceptance_prob = tf.exp(tf.minimum(kernel_results.log_accept_ratio_, 0.))\n  ```\n\n  #### References\n\n  [1]: Art B. Owen. Statistically efficient thinning of a Markov chain sampler.\n       _Technical Report_, 2017.\n       http://statweb.stanford.edu/~owen/reports/bestthinning.pdf\n  \"\"\"\n  if not kernel.is_calibrated:\n    warnings.warn(\"supplied `TransitionKernel` is not calibrated. Markov \"\n                  \"chain may not converge to intended target distribution.\")\n  with tf.compat.v1.name_scope(\n      name, \"mcmc_sample_chain\",\n      [num_results, num_burnin_steps, num_steps_between_results]):\n    num_results = tf.convert_to_tensor(\n        value=num_results, dtype=tf.int32, name=\"num_results\")\n    num_burnin_steps = tf.convert_to_tensor(\n        value=num_burnin_steps, dtype=tf.int32, name=\"num_burnin_steps\")\n    num_steps_between_results = tf.convert_to_tensor(\n        value=num_steps_between_results,\n        dtype=tf.int32,\n        name=\"num_steps_between_results\")\n    current_state = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, name=\"current_state\"),\n        current_state)\n    if previous_kernel_results is None:\n      previous_kernel_results = kernel.bootstrap_results(current_state)\n\n    if trace_fn is None:\n      # It simplifies the logic to use a dummy function here.\n      trace_fn = lambda *args: ()\n      no_trace = True\n    else:\n      no_trace = False\n    if trace_fn is sample_chain.__defaults__[4]:\n      warnings.warn(\"Tracing all kernel results by default is deprecated. Set \"\n                    \"the `trace_fn` argument to None (the future default \"\n                    \"value) or an explicit callback that traces the values \"\n                    \"you are interested in.\")\n\n    def _trace_scan_fn(state_and_results, num_steps):\n      next_state, current_kernel_results = mcmc_util.smart_for_loop(\n          loop_num_iter=num_steps,\n          body_fn=kernel.one_step,\n          initial_loop_vars=list(state_and_results),\n          parallel_iterations=parallel_iterations)\n      return next_state, current_kernel_results\n\n    (_, final_kernel_results), (all_states, trace) = mcmc_util.trace_scan(\n        loop_fn=_trace_scan_fn,\n        initial_state=(current_state, previous_kernel_results),\n        elems=tf.one_hot(\n            indices=0,\n            depth=num_results,\n            on_value=1 + num_burnin_steps,\n            off_value=1 + num_steps_between_results,\n            dtype=tf.int32),\n        # pylint: disable=g-long-lambda\n        trace_fn=lambda state_and_results: (state_and_results[0],\n                                            trace_fn(*state_and_results)),\n        # pylint: enable=g-long-lambda\n        parallel_iterations=parallel_iterations)\n\n    if return_final_kernel_results:\n      return CheckpointableStatesAndTrace(\n          all_states=all_states,\n          trace=trace,\n          final_kernel_results=final_kernel_results)\n    else:\n      if no_trace:\n        return all_states\n      else:\n        return StatesAndTrace(all_states=all_states, trace=trace)", "language": "python", "code": "def sample_chain(\n    num_results,\n    current_state,\n    previous_kernel_results=None,\n    kernel=None,\n    num_burnin_steps=0,\n    num_steps_between_results=0,\n    trace_fn=lambda current_state, kernel_results: kernel_results,\n    return_final_kernel_results=False,\n    parallel_iterations=10,\n    name=None,\n):\n  \"\"\"Implements Markov chain Monte Carlo via repeated `TransitionKernel` steps.\n\n  This function samples from an Markov chain at `current_state` and whose\n  stationary distribution is governed by the supplied `TransitionKernel`\n  instance (`kernel`).\n\n  This function can sample from multiple chains, in parallel. (Whether or not\n  there are multiple chains is dictated by the `kernel`.)\n\n  The `current_state` can be represented as a single `Tensor` or a `list` of\n  `Tensors` which collectively represent the current state.\n\n  Since MCMC states are correlated, it is sometimes desirable to produce\n  additional intermediate states, and then discard them, ending up with a set of\n  states with decreased autocorrelation.  See [Owen (2017)][1]. Such \"thinning\"\n  is made possible by setting `num_steps_between_results > 0`. The chain then\n  takes `num_steps_between_results` extra steps between the steps that make it\n  into the results. The extra steps are never materialized (in calls to\n  `sess.run`), and thus do not increase memory requirements.\n\n  Warning: when setting a `seed` in the `kernel`, ensure that `sample_chain`'s\n  `parallel_iterations=1`, otherwise results will not be reproducible.\n\n  In addition to returning the chain state, this function supports tracing of\n  auxiliary variables used by the kernel. The traced values are selected by\n  specifying `trace_fn`. By default, all kernel results are traced but in the\n  future the default will be changed to no results being traced, so plan\n  accordingly. See below for some examples of this feature.\n\n  Args:\n    num_results: Integer number of Markov chain draws.\n    current_state: `Tensor` or Python `list` of `Tensor`s representing the\n      current state(s) of the Markov chain(s).\n    previous_kernel_results: A `Tensor` or a nested collection of `Tensor`s\n      representing internal calculations made within the previous call to this\n      function (or as returned by `bootstrap_results`).\n    kernel: An instance of `tfp.mcmc.TransitionKernel` which implements one step\n      of the Markov chain.\n    num_burnin_steps: Integer number of chain steps to take before starting to\n      collect results.\n      Default value: 0 (i.e., no burn-in).\n    num_steps_between_results: Integer number of chain steps between collecting\n      a result. Only one out of every `num_steps_between_samples + 1` steps is\n      included in the returned results.  The number of returned chain states is\n      still equal to `num_results`.  Default value: 0 (i.e., no thinning).\n    trace_fn: A callable that takes in the current chain state and the previous\n      kernel results and return a `Tensor` or a nested collection of `Tensor`s\n      that is then traced along with the chain state.\n    return_final_kernel_results: If `True`, then the final kernel results are\n      returned alongside the chain state and the trace specified by the\n      `trace_fn`.\n    parallel_iterations: The number of iterations allowed to run in parallel. It\n      must be a positive integer. See `tf.while_loop` for more details.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"mcmc_sample_chain\").\n\n  Returns:\n    checkpointable_states_and_trace: if `return_final_kernel_results` is\n      `True`. The return value is an instance of\n      `CheckpointableStatesAndTrace`.\n    all_states: if `return_final_kernel_results` is `False` and `trace_fn` is\n      `None`. The return value is a `Tensor` or Python list of `Tensor`s\n      representing the state(s) of the Markov chain(s) at each result step. Has\n      same shape as input `current_state` but with a prepended\n      `num_results`-size dimension.\n    states_and_trace: if `return_final_kernel_results` is `False` and\n      `trace_fn` is not `None`. The return value is an instance of\n      `StatesAndTrace`.\n\n  #### Examples\n\n  ##### Sample from a diagonal-variance Gaussian.\n\n  I.e.,\n\n  ```none\n  for i=1..n:\n    x[i] ~ MultivariateNormal(loc=0, scale=diag(true_stddev))  # likelihood\n  ```\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  dims = 10\n  true_stddev = np.sqrt(np.linspace(1., 3., dims))\n  likelihood = tfd.MultivariateNormalDiag(loc=0., scale_diag=true_stddev)\n\n  states = tfp.mcmc.sample_chain(\n      num_results=1000,\n      num_burnin_steps=500,\n      current_state=tf.zeros(dims),\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=likelihood.log_prob,\n        step_size=0.5,\n        num_leapfrog_steps=2),\n      trace_fn=None)\n\n  sample_mean = tf.reduce_mean(states, axis=0)\n  # ==> approx all zeros\n\n  sample_stddev = tf.sqrt(tf.reduce_mean(\n      tf.squared_difference(states, sample_mean),\n      axis=0))\n  # ==> approx equal true_stddev\n  ```\n\n  ##### Sampling from factor-analysis posteriors with known factors.\n\n  I.e.,\n\n  ```none\n  # prior\n  w ~ MultivariateNormal(loc=0, scale=eye(d))\n  for i=1..n:\n    # likelihood\n    x[i] ~ Normal(loc=w^T F[i], scale=1)\n  ```\n\n  where `F` denotes factors.\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Specify model.\n  def make_prior(dims):\n    return tfd.MultivariateNormalDiag(\n        loc=tf.zeros(dims))\n\n  def make_likelihood(weights, factors):\n    return tfd.MultivariateNormalDiag(\n        loc=tf.matmul(weights, factors, adjoint_b=True))\n\n  def joint_log_prob(num_weights, factors, x, w):\n    return (make_prior(num_weights).log_prob(w) +\n            make_likelihood(w, factors).log_prob(x))\n\n  def unnormalized_log_posterior(w):\n    # Posterior is proportional to: `p(W, X=x | factors)`.\n    return joint_log_prob(num_weights, factors, x, w)\n\n  # Setup data.\n  num_weights = 10 # == d\n  num_factors = 40 # == n\n  num_chains = 100\n\n  weights = make_prior(num_weights).sample(1)\n  factors = tf.random_normal([num_factors, num_weights])\n  x = make_likelihood(weights, factors).sample()\n\n  # Sample from Hamiltonian Monte Carlo Markov Chain.\n\n  # Get `num_results` samples from `num_chains` independent chains.\n  chains_states, kernels_results = tfp.mcmc.sample_chain(\n      num_results=1000,\n      num_burnin_steps=500,\n      current_state=tf.zeros([num_chains, num_weights], name='init_weights'),\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=unnormalized_log_posterior,\n        step_size=0.1,\n        num_leapfrog_steps=2))\n\n  # Compute sample stats.\n  sample_mean = tf.reduce_mean(chains_states, axis=[0, 1])\n  # ==> approx equal to weights\n\n  sample_var = tf.reduce_mean(\n      tf.squared_difference(chains_states, sample_mean),\n      axis=[0, 1])\n  # ==> less than 1\n  ```\n\n  ##### Custom tracing functions.\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  likelihood = tfd.Normal(loc=0., scale=1.)\n\n  def sample_chain(trace_fn):\n    return tfp.mcmc.sample_chain(\n      num_results=1000,\n      num_burnin_steps=500,\n      current_state=0.,\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=likelihood.log_prob,\n        step_size=0.5,\n        num_leapfrog_steps=2),\n      trace_fn=trace_fn)\n\n  def trace_log_accept_ratio(states, previous_kernel_results):\n    return previous_kernel_results.log_accept_ratio\n\n  def trace_everything(states, previous_kernel_results):\n    return previous_kernel_results\n\n  _, log_accept_ratio = sample_chain(trace_fn=trace_log_accept_ratio)\n  _, kernel_results = sample_chain(trace_fn=trace_everything)\n\n  acceptance_prob = tf.exp(tf.minimum(log_accept_ratio_, 0.))\n  # Equivalent to, but more efficient than:\n  acceptance_prob = tf.exp(tf.minimum(kernel_results.log_accept_ratio_, 0.))\n  ```\n\n  #### References\n\n  [1]: Art B. Owen. Statistically efficient thinning of a Markov chain sampler.\n       _Technical Report_, 2017.\n       http://statweb.stanford.edu/~owen/reports/bestthinning.pdf\n  \"\"\"\n  if not kernel.is_calibrated:\n    warnings.warn(\"supplied `TransitionKernel` is not calibrated. Markov \"\n                  \"chain may not converge to intended target distribution.\")\n  with tf.compat.v1.name_scope(\n      name, \"mcmc_sample_chain\",\n      [num_results, num_burnin_steps, num_steps_between_results]):\n    num_results = tf.convert_to_tensor(\n        value=num_results, dtype=tf.int32, name=\"num_results\")\n    num_burnin_steps = tf.convert_to_tensor(\n        value=num_burnin_steps, dtype=tf.int32, name=\"num_burnin_steps\")\n    num_steps_between_results = tf.convert_to_tensor(\n        value=num_steps_between_results,\n        dtype=tf.int32,\n        name=\"num_steps_between_results\")\n    current_state = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, name=\"current_state\"),\n        current_state)\n    if previous_kernel_results is None:\n      previous_kernel_results = kernel.bootstrap_results(current_state)\n\n    if trace_fn is None:\n      # It simplifies the logic to use a dummy function here.\n      trace_fn = lambda *args: ()\n      no_trace = True\n    else:\n      no_trace = False\n    if trace_fn is sample_chain.__defaults__[4]:\n      warnings.warn(\"Tracing all kernel results by default is deprecated. Set \"\n                    \"the `trace_fn` argument to None (the future default \"\n                    \"value) or an explicit callback that traces the values \"\n                    \"you are interested in.\")\n\n    def _trace_scan_fn(state_and_results, num_steps):\n      next_state, current_kernel_results = mcmc_util.smart_for_loop(\n          loop_num_iter=num_steps,\n          body_fn=kernel.one_step,\n          initial_loop_vars=list(state_and_results),\n          parallel_iterations=parallel_iterations)\n      return next_state, current_kernel_results\n\n    (_, final_kernel_results), (all_states, trace) = mcmc_util.trace_scan(\n        loop_fn=_trace_scan_fn,\n        initial_state=(current_state, previous_kernel_results),\n        elems=tf.one_hot(\n            indices=0,\n            depth=num_results,\n            on_value=1 + num_burnin_steps,\n            off_value=1 + num_steps_between_results,\n            dtype=tf.int32),\n        # pylint: disable=g-long-lambda\n        trace_fn=lambda state_and_results: (state_and_results[0],\n                                            trace_fn(*state_and_results)),\n        # pylint: enable=g-long-lambda\n        parallel_iterations=parallel_iterations)\n\n    if return_final_kernel_results:\n      return CheckpointableStatesAndTrace(\n          all_states=all_states,\n          trace=trace,\n          final_kernel_results=final_kernel_results)\n    else:\n      if no_trace:\n        return all_states\n      else:\n        return StatesAndTrace(all_states=all_states, trace=trace)", "code_tokens": ["def", "sample_chain", "(", "num_results", ",", "current_state", ",", "previous_kernel_results", "=", "None", ",", "kernel", "=", "None", ",", "num_burnin_steps", "=", "0", ",", "num_steps_between_results", "=", "0", ",", "trace_fn", "=", "lambda", "current_state", ",", "kernel_results", ":", "kernel_results", ",", "return_final_kernel_results", "=", "False", ",", "parallel_iterations", "=", "10", ",", "name", "=", "None", ",", ")", ":", "if", "not", "kernel", ".", "is_calibrated", ":", "warnings", ".", "warn", "(", "\"supplied `TransitionKernel` is not calibrated. Markov \"", "\"chain may not converge to intended target distribution.\"", ")", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "\"mcmc_sample_chain\"", ",", "[", "num_results", ",", "num_burnin_steps", ",", "num_steps_between_results", "]", ")", ":", "num_results", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "num_results", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"num_results\"", ")", "num_burnin_steps", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "num_burnin_steps", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"num_burnin_steps\"", ")", "num_steps_between_results", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "num_steps_between_results", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"num_steps_between_results\"", ")", "current_state", "=", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "x", ":", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "\"current_state\"", ")", ",", "current_state", ")", "if", "previous_kernel_results", "is", "None", ":", "previous_kernel_results", "=", "kernel", ".", "bootstrap_results", "(", "current_state", ")", "if", "trace_fn", "is", "None", ":", "# It simplifies the logic to use a dummy function here.", "trace_fn", "=", "lambda", "*", "args", ":", "(", ")", "no_trace", "=", "True", "else", ":", "no_trace", "=", "False", "if", "trace_fn", "is", "sample_chain", ".", "__defaults__", "[", "4", "]", ":", "warnings", ".", "warn", "(", "\"Tracing all kernel results by default is deprecated. Set \"", "\"the `trace_fn` argument to None (the future default \"", "\"value) or an explicit callback that traces the values \"", "\"you are interested in.\"", ")", "def", "_trace_scan_fn", "(", "state_and_results", ",", "num_steps", ")", ":", "next_state", ",", "current_kernel_results", "=", "mcmc_util", ".", "smart_for_loop", "(", "loop_num_iter", "=", "num_steps", ",", "body_fn", "=", "kernel", ".", "one_step", ",", "initial_loop_vars", "=", "list", "(", "state_and_results", ")", ",", "parallel_iterations", "=", "parallel_iterations", ")", "return", "next_state", ",", "current_kernel_results", "(", "_", ",", "final_kernel_results", ")", ",", "(", "all_states", ",", "trace", ")", "=", "mcmc_util", ".", "trace_scan", "(", "loop_fn", "=", "_trace_scan_fn", ",", "initial_state", "=", "(", "current_state", ",", "previous_kernel_results", ")", ",", "elems", "=", "tf", ".", "one_hot", "(", "indices", "=", "0", ",", "depth", "=", "num_results", ",", "on_value", "=", "1", "+", "num_burnin_steps", ",", "off_value", "=", "1", "+", "num_steps_between_results", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "# pylint: disable=g-long-lambda", "trace_fn", "=", "lambda", "state_and_results", ":", "(", "state_and_results", "[", "0", "]", ",", "trace_fn", "(", "*", "state_and_results", ")", ")", ",", "# pylint: enable=g-long-lambda", "parallel_iterations", "=", "parallel_iterations", ")", "if", "return_final_kernel_results", ":", "return", "CheckpointableStatesAndTrace", "(", "all_states", "=", "all_states", ",", "trace", "=", "trace", ",", "final_kernel_results", "=", "final_kernel_results", ")", "else", ":", "if", "no_trace", ":", "return", "all_states", "else", ":", "return", "StatesAndTrace", "(", "all_states", "=", "all_states", ",", "trace", "=", "trace", ")"], "docstring": "Implements Markov chain Monte Carlo via repeated `TransitionKernel` steps.\n\n  This function samples from an Markov chain at `current_state` and whose\n  stationary distribution is governed by the supplied `TransitionKernel`\n  instance (`kernel`).\n\n  This function can sample from multiple chains, in parallel. (Whether or not\n  there are multiple chains is dictated by the `kernel`.)\n\n  The `current_state` can be represented as a single `Tensor` or a `list` of\n  `Tensors` which collectively represent the current state.\n\n  Since MCMC states are correlated, it is sometimes desirable to produce\n  additional intermediate states, and then discard them, ending up with a set of\n  states with decreased autocorrelation.  See [Owen (2017)][1]. Such \"thinning\"\n  is made possible by setting `num_steps_between_results > 0`. The chain then\n  takes `num_steps_between_results` extra steps between the steps that make it\n  into the results. The extra steps are never materialized (in calls to\n  `sess.run`), and thus do not increase memory requirements.\n\n  Warning: when setting a `seed` in the `kernel`, ensure that `sample_chain`'s\n  `parallel_iterations=1`, otherwise results will not be reproducible.\n\n  In addition to returning the chain state, this function supports tracing of\n  auxiliary variables used by the kernel. The traced values are selected by\n  specifying `trace_fn`. By default, all kernel results are traced but in the\n  future the default will be changed to no results being traced, so plan\n  accordingly. See below for some examples of this feature.\n\n  Args:\n    num_results: Integer number of Markov chain draws.\n    current_state: `Tensor` or Python `list` of `Tensor`s representing the\n      current state(s) of the Markov chain(s).\n    previous_kernel_results: A `Tensor` or a nested collection of `Tensor`s\n      representing internal calculations made within the previous call to this\n      function (or as returned by `bootstrap_results`).\n    kernel: An instance of `tfp.mcmc.TransitionKernel` which implements one step\n      of the Markov chain.\n    num_burnin_steps: Integer number of chain steps to take before starting to\n      collect results.\n      Default value: 0 (i.e., no burn-in).\n    num_steps_between_results: Integer number of chain steps between collecting\n      a result. Only one out of every `num_steps_between_samples + 1` steps is\n      included in the returned results.  The number of returned chain states is\n      still equal to `num_results`.  Default value: 0 (i.e., no thinning).\n    trace_fn: A callable that takes in the current chain state and the previous\n      kernel results and return a `Tensor` or a nested collection of `Tensor`s\n      that is then traced along with the chain state.\n    return_final_kernel_results: If `True`, then the final kernel results are\n      returned alongside the chain state and the trace specified by the\n      `trace_fn`.\n    parallel_iterations: The number of iterations allowed to run in parallel. It\n      must be a positive integer. See `tf.while_loop` for more details.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"mcmc_sample_chain\").\n\n  Returns:\n    checkpointable_states_and_trace: if `return_final_kernel_results` is\n      `True`. The return value is an instance of\n      `CheckpointableStatesAndTrace`.\n    all_states: if `return_final_kernel_results` is `False` and `trace_fn` is\n      `None`. The return value is a `Tensor` or Python list of `Tensor`s\n      representing the state(s) of the Markov chain(s) at each result step. Has\n      same shape as input `current_state` but with a prepended\n      `num_results`-size dimension.\n    states_and_trace: if `return_final_kernel_results` is `False` and\n      `trace_fn` is not `None`. The return value is an instance of\n      `StatesAndTrace`.\n\n  #### Examples\n\n  ##### Sample from a diagonal-variance Gaussian.\n\n  I.e.,\n\n  ```none\n  for i=1..n:\n    x[i] ~ MultivariateNormal(loc=0, scale=diag(true_stddev))  # likelihood\n  ```\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  dims = 10\n  true_stddev = np.sqrt(np.linspace(1., 3., dims))\n  likelihood = tfd.MultivariateNormalDiag(loc=0., scale_diag=true_stddev)\n\n  states = tfp.mcmc.sample_chain(\n      num_results=1000,\n      num_burnin_steps=500,\n      current_state=tf.zeros(dims),\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=likelihood.log_prob,\n        step_size=0.5,\n        num_leapfrog_steps=2),\n      trace_fn=None)\n\n  sample_mean = tf.reduce_mean(states, axis=0)\n  # ==> approx all zeros\n\n  sample_stddev = tf.sqrt(tf.reduce_mean(\n      tf.squared_difference(states, sample_mean),\n      axis=0))\n  # ==> approx equal true_stddev\n  ```\n\n  ##### Sampling from factor-analysis posteriors with known factors.\n\n  I.e.,\n\n  ```none\n  # prior\n  w ~ MultivariateNormal(loc=0, scale=eye(d))\n  for i=1..n:\n    # likelihood\n    x[i] ~ Normal(loc=w^T F[i], scale=1)\n  ```\n\n  where `F` denotes factors.\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Specify model.\n  def make_prior(dims):\n    return tfd.MultivariateNormalDiag(\n        loc=tf.zeros(dims))\n\n  def make_likelihood(weights, factors):\n    return tfd.MultivariateNormalDiag(\n        loc=tf.matmul(weights, factors, adjoint_b=True))\n\n  def joint_log_prob(num_weights, factors, x, w):\n    return (make_prior(num_weights).log_prob(w) +\n            make_likelihood(w, factors).log_prob(x))\n\n  def unnormalized_log_posterior(w):\n    # Posterior is proportional to: `p(W, X=x | factors)`.\n    return joint_log_prob(num_weights, factors, x, w)\n\n  # Setup data.\n  num_weights = 10 # == d\n  num_factors = 40 # == n\n  num_chains = 100\n\n  weights = make_prior(num_weights).sample(1)\n  factors = tf.random_normal([num_factors, num_weights])\n  x = make_likelihood(weights, factors).sample()\n\n  # Sample from Hamiltonian Monte Carlo Markov Chain.\n\n  # Get `num_results` samples from `num_chains` independent chains.\n  chains_states, kernels_results = tfp.mcmc.sample_chain(\n      num_results=1000,\n      num_burnin_steps=500,\n      current_state=tf.zeros([num_chains, num_weights], name='init_weights'),\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=unnormalized_log_posterior,\n        step_size=0.1,\n        num_leapfrog_steps=2))\n\n  # Compute sample stats.\n  sample_mean = tf.reduce_mean(chains_states, axis=[0, 1])\n  # ==> approx equal to weights\n\n  sample_var = tf.reduce_mean(\n      tf.squared_difference(chains_states, sample_mean),\n      axis=[0, 1])\n  # ==> less than 1\n  ```\n\n  ##### Custom tracing functions.\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  likelihood = tfd.Normal(loc=0., scale=1.)\n\n  def sample_chain(trace_fn):\n    return tfp.mcmc.sample_chain(\n      num_results=1000,\n      num_burnin_steps=500,\n      current_state=0.,\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=likelihood.log_prob,\n        step_size=0.5,\n        num_leapfrog_steps=2),\n      trace_fn=trace_fn)\n\n  def trace_log_accept_ratio(states, previous_kernel_results):\n    return previous_kernel_results.log_accept_ratio\n\n  def trace_everything(states, previous_kernel_results):\n    return previous_kernel_results\n\n  _, log_accept_ratio = sample_chain(trace_fn=trace_log_accept_ratio)\n  _, kernel_results = sample_chain(trace_fn=trace_everything)\n\n  acceptance_prob = tf.exp(tf.minimum(log_accept_ratio_, 0.))\n  # Equivalent to, but more efficient than:\n  acceptance_prob = tf.exp(tf.minimum(kernel_results.log_accept_ratio_, 0.))\n  ```\n\n  #### References\n\n  [1]: Art B. Owen. Statistically efficient thinning of a Markov chain sampler.\n       _Technical Report_, 2017.\n       http://statweb.stanford.edu/~owen/reports/bestthinning.pdf", "docstring_tokens": ["Implements", "Markov", "chain", "Monte", "Carlo", "via", "repeated", "TransitionKernel", "steps", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/sample.py#L81-L372", "partition": "test", "index": 1015, "time": "2018-03-06 09:14:41"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/hmc.py", "func_name": "_leapfrog_integrator_one_step", "original_string": "def _leapfrog_integrator_one_step(\n    target_log_prob_fn,\n    independent_chain_ndims,\n    step_sizes,\n    current_momentum_parts,\n    current_state_parts,\n    current_target_log_prob,\n    current_target_log_prob_grad_parts,\n    state_gradients_are_stopped=False,\n    name=None):\n  \"\"\"Applies `num_leapfrog_steps` of the leapfrog integrator.\n\n  Assumes a simple quadratic kinetic energy function: `0.5 ||momentum||**2`.\n\n  #### Examples:\n\n  ##### Simple quadratic potential.\n\n  ```python\n  import matplotlib.pyplot as plt\n  %matplotlib inline\n  import numpy as np\n  import tensorflow as tf\n  from tensorflow_probability.python.mcmc.hmc import _leapfrog_integrator_one_step  # pylint: disable=line-too-long\n  tfd = tfp.distributions\n\n  dims = 10\n  num_iter = int(1e3)\n  dtype = np.float32\n\n  position = tf.placeholder(np.float32)\n  momentum = tf.placeholder(np.float32)\n\n  target_log_prob_fn = tfd.MultivariateNormalDiag(\n      loc=tf.zeros(dims, dtype)).log_prob\n\n  def _leapfrog_one_step(*args):\n    # Closure representing computation done during each leapfrog step.\n    return _leapfrog_integrator_one_step(\n        target_log_prob_fn=target_log_prob_fn,\n        independent_chain_ndims=0,\n        step_sizes=[0.1],\n        current_momentum_parts=args[0],\n        current_state_parts=args[1],\n        current_target_log_prob=args[2],\n        current_target_log_prob_grad_parts=args[3])\n\n  # Do leapfrog integration.\n  [\n      [next_momentum],\n      [next_position],\n      next_target_log_prob,\n      next_target_log_prob_grad_parts,\n  ] = tf.while_loop(\n      cond=lambda *args: True,\n      body=_leapfrog_one_step,\n      loop_vars=[\n        [momentum],\n        [position],\n        target_log_prob_fn(position),\n        tf.gradients(target_log_prob_fn(position), position),\n      ],\n      maximum_iterations=3)\n\n  momentum_ = np.random.randn(dims).astype(dtype)\n  position_ = np.random.randn(dims).astype(dtype)\n  positions = np.zeros([num_iter, dims], dtype)\n\n  with tf.Session() as sess:\n    for i in xrange(num_iter):\n      position_, momentum_ = sess.run(\n          [next_momentum, next_position],\n          feed_dict={position: position_, momentum: momentum_})\n      positions[i] = position_\n\n  plt.plot(positions[:, 0]);  # Sinusoidal.\n  ```\n\n  Args:\n    target_log_prob_fn: Python callable which takes an argument like\n      `*current_state_parts` and returns its (possibly unnormalized) log-density\n      under the target distribution.\n    independent_chain_ndims: Scalar `int` `Tensor` representing the number of\n      leftmost `Tensor` dimensions which index independent chains.\n    step_sizes: Python `list` of `Tensor`s representing the step size for the\n      leapfrog integrator. Must broadcast with the shape of\n      `current_state_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    current_momentum_parts: Tensor containing the value(s) of the momentum\n      variable(s) to update.\n    current_state_parts: Python `list` of `Tensor`s representing the current\n      state(s) of the Markov chain(s). The first `independent_chain_ndims` of\n      the `Tensor`(s) index different chains.\n    current_target_log_prob: `Tensor` representing the value of\n      `target_log_prob_fn(*current_state_parts)`. The only reason to specify\n      this argument is to reduce TF graph size.\n    current_target_log_prob_grad_parts: Python list of `Tensor`s representing\n      gradient of `target_log_prob_fn(*current_state_parts`) wrt\n      `current_state_parts`. Must have same shape as `current_state_parts`. The\n      only reason to specify this argument is to reduce TF graph size.\n    state_gradients_are_stopped: Python `bool` indicating that the proposed new\n      state be run through `tf.stop_gradient`. This is particularly useful when\n      combining optimization over samples from the HMC chain.\n      Default value: `False` (i.e., do not apply `stop_gradient`).\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'hmc_leapfrog_integrator').\n\n  Returns:\n    proposed_momentum_parts: Updated value of the momentum.\n    proposed_state_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n    proposed_target_log_prob: `Tensor` representing the value of\n      `target_log_prob_fn` at `next_state`.\n    proposed_target_log_prob_grad_parts: Gradient of `proposed_target_log_prob`\n      wrt `next_state`.\n\n  Raises:\n    ValueError: if `len(momentum_parts) != len(state_parts)`.\n    ValueError: if `len(state_parts) != len(step_sizes)`.\n    ValueError: if `len(state_parts) != len(grads_target_log_prob)`.\n    TypeError: if `not target_log_prob.dtype.is_floating`.\n  \"\"\"\n  # Note on per-variable step sizes:\n  #\n  # Using per-variable step sizes is equivalent to using the same step\n  # size for all variables and adding a diagonal mass matrix in the\n  # kinetic energy term of the Hamiltonian being integrated. This is\n  # hinted at by Neal (2011) but not derived in detail there.\n  #\n  # Let x and v be position and momentum variables respectively.\n  # Let g(x) be the gradient of `target_log_prob_fn(x)`.\n  # Let S be a diagonal matrix of per-variable step sizes.\n  # Let the Hamiltonian H(x, v) = -target_log_prob_fn(x) + 0.5 * ||v||**2.\n  #\n  # Using per-variable step sizes gives the updates\n  # v'  = v  + 0.5 * matmul(S, g(x))\n  # x'' = x  + matmul(S, v')\n  # v'' = v' + 0.5 * matmul(S, g(x''))\n  #\n  # Let u = matmul(inv(S), v).\n  # Multiplying v by inv(S) in the updates above gives the transformed dynamics\n  # u'  = matmul(inv(S), v')  = matmul(inv(S), v) + 0.5 * g(x)\n  #                           = u + 0.5 * g(x)\n  # x'' = x + matmul(S, v') = x + matmul(S**2, u')\n  # u'' = matmul(inv(S), v'') = matmul(inv(S), v') + 0.5 * g(x'')\n  #                           = u' + 0.5 * g(x'')\n  #\n  # These are exactly the leapfrog updates for the Hamiltonian\n  # H'(x, u) = -target_log_prob_fn(x) + 0.5 * u^T S**2 u\n  #          = -target_log_prob_fn(x) + 0.5 * ||v||**2 = H(x, v).\n  #\n  # To summarize:\n  #\n  # * Using per-variable step sizes implicitly simulates the dynamics\n  #   of the Hamiltonian H' (which are energy-conserving in H'). We\n  #   keep track of v instead of u, but the underlying dynamics are\n  #   the same if we transform back.\n  # * The value of the Hamiltonian H'(x, u) is the same as the value\n  #   of the original Hamiltonian H(x, v) after we transform back from\n  #   u to v.\n  # * Sampling v ~ N(0, I) is equivalent to sampling u ~ N(0, S**-2).\n  #\n  # So using per-variable step sizes in HMC will give results that are\n  # exactly identical to explicitly using a diagonal mass matrix.\n\n  with tf.compat.v1.name_scope(name, 'hmc_leapfrog_integrator_one_step', [\n      independent_chain_ndims, step_sizes, current_momentum_parts,\n      current_state_parts, current_target_log_prob,\n      current_target_log_prob_grad_parts\n  ]):\n\n    # Step 1: Update momentum.\n    proposed_momentum_parts = [\n        v + 0.5 * tf.cast(eps, v.dtype) * g\n        for v, eps, g\n        in zip(current_momentum_parts,\n               step_sizes,\n               current_target_log_prob_grad_parts)]\n\n    # Step 2: Update state.\n    proposed_state_parts = [\n        x + tf.cast(eps, v.dtype) * v\n        for x, eps, v\n        in zip(current_state_parts,\n               step_sizes,\n               proposed_momentum_parts)]\n\n    if state_gradients_are_stopped:\n      proposed_state_parts = [tf.stop_gradient(x) for x in proposed_state_parts]\n\n    # Step 3a: Re-evaluate target-log-prob (and grad) at proposed state.\n    [\n        proposed_target_log_prob,\n        proposed_target_log_prob_grad_parts,\n    ] = mcmc_util.maybe_call_fn_and_grads(\n        target_log_prob_fn,\n        proposed_state_parts)\n\n    if not proposed_target_log_prob.dtype.is_floating:\n      raise TypeError('`target_log_prob_fn` must produce a `Tensor` '\n                      'with `float` `dtype`.')\n\n    if any(g is None for g in proposed_target_log_prob_grad_parts):\n      raise ValueError(\n          'Encountered `None` gradient. Does your target `target_log_prob_fn` '\n          'access all `tf.Variable`s via `tf.get_variable`?\\n'\n          '  current_state_parts: {}\\n'\n          '  proposed_state_parts: {}\\n'\n          '  proposed_target_log_prob_grad_parts: {}'.format(\n              current_state_parts,\n              proposed_state_parts,\n              proposed_target_log_prob_grad_parts))\n\n    # Step 3b: Update momentum (again).\n    proposed_momentum_parts = [\n        v + 0.5 * tf.cast(eps, v.dtype) * g\n        for v, eps, g\n        in zip(proposed_momentum_parts,\n               step_sizes,\n               proposed_target_log_prob_grad_parts)]\n\n    return [\n        proposed_momentum_parts,\n        proposed_state_parts,\n        proposed_target_log_prob,\n        proposed_target_log_prob_grad_parts,\n    ]", "language": "python", "code": "def _leapfrog_integrator_one_step(\n    target_log_prob_fn,\n    independent_chain_ndims,\n    step_sizes,\n    current_momentum_parts,\n    current_state_parts,\n    current_target_log_prob,\n    current_target_log_prob_grad_parts,\n    state_gradients_are_stopped=False,\n    name=None):\n  \"\"\"Applies `num_leapfrog_steps` of the leapfrog integrator.\n\n  Assumes a simple quadratic kinetic energy function: `0.5 ||momentum||**2`.\n\n  #### Examples:\n\n  ##### Simple quadratic potential.\n\n  ```python\n  import matplotlib.pyplot as plt\n  %matplotlib inline\n  import numpy as np\n  import tensorflow as tf\n  from tensorflow_probability.python.mcmc.hmc import _leapfrog_integrator_one_step  # pylint: disable=line-too-long\n  tfd = tfp.distributions\n\n  dims = 10\n  num_iter = int(1e3)\n  dtype = np.float32\n\n  position = tf.placeholder(np.float32)\n  momentum = tf.placeholder(np.float32)\n\n  target_log_prob_fn = tfd.MultivariateNormalDiag(\n      loc=tf.zeros(dims, dtype)).log_prob\n\n  def _leapfrog_one_step(*args):\n    # Closure representing computation done during each leapfrog step.\n    return _leapfrog_integrator_one_step(\n        target_log_prob_fn=target_log_prob_fn,\n        independent_chain_ndims=0,\n        step_sizes=[0.1],\n        current_momentum_parts=args[0],\n        current_state_parts=args[1],\n        current_target_log_prob=args[2],\n        current_target_log_prob_grad_parts=args[3])\n\n  # Do leapfrog integration.\n  [\n      [next_momentum],\n      [next_position],\n      next_target_log_prob,\n      next_target_log_prob_grad_parts,\n  ] = tf.while_loop(\n      cond=lambda *args: True,\n      body=_leapfrog_one_step,\n      loop_vars=[\n        [momentum],\n        [position],\n        target_log_prob_fn(position),\n        tf.gradients(target_log_prob_fn(position), position),\n      ],\n      maximum_iterations=3)\n\n  momentum_ = np.random.randn(dims).astype(dtype)\n  position_ = np.random.randn(dims).astype(dtype)\n  positions = np.zeros([num_iter, dims], dtype)\n\n  with tf.Session() as sess:\n    for i in xrange(num_iter):\n      position_, momentum_ = sess.run(\n          [next_momentum, next_position],\n          feed_dict={position: position_, momentum: momentum_})\n      positions[i] = position_\n\n  plt.plot(positions[:, 0]);  # Sinusoidal.\n  ```\n\n  Args:\n    target_log_prob_fn: Python callable which takes an argument like\n      `*current_state_parts` and returns its (possibly unnormalized) log-density\n      under the target distribution.\n    independent_chain_ndims: Scalar `int` `Tensor` representing the number of\n      leftmost `Tensor` dimensions which index independent chains.\n    step_sizes: Python `list` of `Tensor`s representing the step size for the\n      leapfrog integrator. Must broadcast with the shape of\n      `current_state_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    current_momentum_parts: Tensor containing the value(s) of the momentum\n      variable(s) to update.\n    current_state_parts: Python `list` of `Tensor`s representing the current\n      state(s) of the Markov chain(s). The first `independent_chain_ndims` of\n      the `Tensor`(s) index different chains.\n    current_target_log_prob: `Tensor` representing the value of\n      `target_log_prob_fn(*current_state_parts)`. The only reason to specify\n      this argument is to reduce TF graph size.\n    current_target_log_prob_grad_parts: Python list of `Tensor`s representing\n      gradient of `target_log_prob_fn(*current_state_parts`) wrt\n      `current_state_parts`. Must have same shape as `current_state_parts`. The\n      only reason to specify this argument is to reduce TF graph size.\n    state_gradients_are_stopped: Python `bool` indicating that the proposed new\n      state be run through `tf.stop_gradient`. This is particularly useful when\n      combining optimization over samples from the HMC chain.\n      Default value: `False` (i.e., do not apply `stop_gradient`).\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'hmc_leapfrog_integrator').\n\n  Returns:\n    proposed_momentum_parts: Updated value of the momentum.\n    proposed_state_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n    proposed_target_log_prob: `Tensor` representing the value of\n      `target_log_prob_fn` at `next_state`.\n    proposed_target_log_prob_grad_parts: Gradient of `proposed_target_log_prob`\n      wrt `next_state`.\n\n  Raises:\n    ValueError: if `len(momentum_parts) != len(state_parts)`.\n    ValueError: if `len(state_parts) != len(step_sizes)`.\n    ValueError: if `len(state_parts) != len(grads_target_log_prob)`.\n    TypeError: if `not target_log_prob.dtype.is_floating`.\n  \"\"\"\n  # Note on per-variable step sizes:\n  #\n  # Using per-variable step sizes is equivalent to using the same step\n  # size for all variables and adding a diagonal mass matrix in the\n  # kinetic energy term of the Hamiltonian being integrated. This is\n  # hinted at by Neal (2011) but not derived in detail there.\n  #\n  # Let x and v be position and momentum variables respectively.\n  # Let g(x) be the gradient of `target_log_prob_fn(x)`.\n  # Let S be a diagonal matrix of per-variable step sizes.\n  # Let the Hamiltonian H(x, v) = -target_log_prob_fn(x) + 0.5 * ||v||**2.\n  #\n  # Using per-variable step sizes gives the updates\n  # v'  = v  + 0.5 * matmul(S, g(x))\n  # x'' = x  + matmul(S, v')\n  # v'' = v' + 0.5 * matmul(S, g(x''))\n  #\n  # Let u = matmul(inv(S), v).\n  # Multiplying v by inv(S) in the updates above gives the transformed dynamics\n  # u'  = matmul(inv(S), v')  = matmul(inv(S), v) + 0.5 * g(x)\n  #                           = u + 0.5 * g(x)\n  # x'' = x + matmul(S, v') = x + matmul(S**2, u')\n  # u'' = matmul(inv(S), v'') = matmul(inv(S), v') + 0.5 * g(x'')\n  #                           = u' + 0.5 * g(x'')\n  #\n  # These are exactly the leapfrog updates for the Hamiltonian\n  # H'(x, u) = -target_log_prob_fn(x) + 0.5 * u^T S**2 u\n  #          = -target_log_prob_fn(x) + 0.5 * ||v||**2 = H(x, v).\n  #\n  # To summarize:\n  #\n  # * Using per-variable step sizes implicitly simulates the dynamics\n  #   of the Hamiltonian H' (which are energy-conserving in H'). We\n  #   keep track of v instead of u, but the underlying dynamics are\n  #   the same if we transform back.\n  # * The value of the Hamiltonian H'(x, u) is the same as the value\n  #   of the original Hamiltonian H(x, v) after we transform back from\n  #   u to v.\n  # * Sampling v ~ N(0, I) is equivalent to sampling u ~ N(0, S**-2).\n  #\n  # So using per-variable step sizes in HMC will give results that are\n  # exactly identical to explicitly using a diagonal mass matrix.\n\n  with tf.compat.v1.name_scope(name, 'hmc_leapfrog_integrator_one_step', [\n      independent_chain_ndims, step_sizes, current_momentum_parts,\n      current_state_parts, current_target_log_prob,\n      current_target_log_prob_grad_parts\n  ]):\n\n    # Step 1: Update momentum.\n    proposed_momentum_parts = [\n        v + 0.5 * tf.cast(eps, v.dtype) * g\n        for v, eps, g\n        in zip(current_momentum_parts,\n               step_sizes,\n               current_target_log_prob_grad_parts)]\n\n    # Step 2: Update state.\n    proposed_state_parts = [\n        x + tf.cast(eps, v.dtype) * v\n        for x, eps, v\n        in zip(current_state_parts,\n               step_sizes,\n               proposed_momentum_parts)]\n\n    if state_gradients_are_stopped:\n      proposed_state_parts = [tf.stop_gradient(x) for x in proposed_state_parts]\n\n    # Step 3a: Re-evaluate target-log-prob (and grad) at proposed state.\n    [\n        proposed_target_log_prob,\n        proposed_target_log_prob_grad_parts,\n    ] = mcmc_util.maybe_call_fn_and_grads(\n        target_log_prob_fn,\n        proposed_state_parts)\n\n    if not proposed_target_log_prob.dtype.is_floating:\n      raise TypeError('`target_log_prob_fn` must produce a `Tensor` '\n                      'with `float` `dtype`.')\n\n    if any(g is None for g in proposed_target_log_prob_grad_parts):\n      raise ValueError(\n          'Encountered `None` gradient. Does your target `target_log_prob_fn` '\n          'access all `tf.Variable`s via `tf.get_variable`?\\n'\n          '  current_state_parts: {}\\n'\n          '  proposed_state_parts: {}\\n'\n          '  proposed_target_log_prob_grad_parts: {}'.format(\n              current_state_parts,\n              proposed_state_parts,\n              proposed_target_log_prob_grad_parts))\n\n    # Step 3b: Update momentum (again).\n    proposed_momentum_parts = [\n        v + 0.5 * tf.cast(eps, v.dtype) * g\n        for v, eps, g\n        in zip(proposed_momentum_parts,\n               step_sizes,\n               proposed_target_log_prob_grad_parts)]\n\n    return [\n        proposed_momentum_parts,\n        proposed_state_parts,\n        proposed_target_log_prob,\n        proposed_target_log_prob_grad_parts,\n    ]", "code_tokens": ["def", "_leapfrog_integrator_one_step", "(", "target_log_prob_fn", ",", "independent_chain_ndims", ",", "step_sizes", ",", "current_momentum_parts", ",", "current_state_parts", ",", "current_target_log_prob", ",", "current_target_log_prob_grad_parts", ",", "state_gradients_are_stopped", "=", "False", ",", "name", "=", "None", ")", ":", "# Note on per-variable step sizes:", "#", "# Using per-variable step sizes is equivalent to using the same step", "# size for all variables and adding a diagonal mass matrix in the", "# kinetic energy term of the Hamiltonian being integrated. This is", "# hinted at by Neal (2011) but not derived in detail there.", "#", "# Let x and v be position and momentum variables respectively.", "# Let g(x) be the gradient of `target_log_prob_fn(x)`.", "# Let S be a diagonal matrix of per-variable step sizes.", "# Let the Hamiltonian H(x, v) = -target_log_prob_fn(x) + 0.5 * ||v||**2.", "#", "# Using per-variable step sizes gives the updates", "# v'  = v  + 0.5 * matmul(S, g(x))", "# x'' = x  + matmul(S, v')", "# v'' = v' + 0.5 * matmul(S, g(x''))", "#", "# Let u = matmul(inv(S), v).", "# Multiplying v by inv(S) in the updates above gives the transformed dynamics", "# u'  = matmul(inv(S), v')  = matmul(inv(S), v) + 0.5 * g(x)", "#                           = u + 0.5 * g(x)", "# x'' = x + matmul(S, v') = x + matmul(S**2, u')", "# u'' = matmul(inv(S), v'') = matmul(inv(S), v') + 0.5 * g(x'')", "#                           = u' + 0.5 * g(x'')", "#", "# These are exactly the leapfrog updates for the Hamiltonian", "# H'(x, u) = -target_log_prob_fn(x) + 0.5 * u^T S**2 u", "#          = -target_log_prob_fn(x) + 0.5 * ||v||**2 = H(x, v).", "#", "# To summarize:", "#", "# * Using per-variable step sizes implicitly simulates the dynamics", "#   of the Hamiltonian H' (which are energy-conserving in H'). We", "#   keep track of v instead of u, but the underlying dynamics are", "#   the same if we transform back.", "# * The value of the Hamiltonian H'(x, u) is the same as the value", "#   of the original Hamiltonian H(x, v) after we transform back from", "#   u to v.", "# * Sampling v ~ N(0, I) is equivalent to sampling u ~ N(0, S**-2).", "#", "# So using per-variable step sizes in HMC will give results that are", "# exactly identical to explicitly using a diagonal mass matrix.", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'hmc_leapfrog_integrator_one_step'", ",", "[", "independent_chain_ndims", ",", "step_sizes", ",", "current_momentum_parts", ",", "current_state_parts", ",", "current_target_log_prob", ",", "current_target_log_prob_grad_parts", "]", ")", ":", "# Step 1: Update momentum.", "proposed_momentum_parts", "=", "[", "v", "+", "0.5", "*", "tf", ".", "cast", "(", "eps", ",", "v", ".", "dtype", ")", "*", "g", "for", "v", ",", "eps", ",", "g", "in", "zip", "(", "current_momentum_parts", ",", "step_sizes", ",", "current_target_log_prob_grad_parts", ")", "]", "# Step 2: Update state.", "proposed_state_parts", "=", "[", "x", "+", "tf", ".", "cast", "(", "eps", ",", "v", ".", "dtype", ")", "*", "v", "for", "x", ",", "eps", ",", "v", "in", "zip", "(", "current_state_parts", ",", "step_sizes", ",", "proposed_momentum_parts", ")", "]", "if", "state_gradients_are_stopped", ":", "proposed_state_parts", "=", "[", "tf", ".", "stop_gradient", "(", "x", ")", "for", "x", "in", "proposed_state_parts", "]", "# Step 3a: Re-evaluate target-log-prob (and grad) at proposed state.", "[", "proposed_target_log_prob", ",", "proposed_target_log_prob_grad_parts", ",", "]", "=", "mcmc_util", ".", "maybe_call_fn_and_grads", "(", "target_log_prob_fn", ",", "proposed_state_parts", ")", "if", "not", "proposed_target_log_prob", ".", "dtype", ".", "is_floating", ":", "raise", "TypeError", "(", "'`target_log_prob_fn` must produce a `Tensor` '", "'with `float` `dtype`.'", ")", "if", "any", "(", "g", "is", "None", "for", "g", "in", "proposed_target_log_prob_grad_parts", ")", ":", "raise", "ValueError", "(", "'Encountered `None` gradient. Does your target `target_log_prob_fn` '", "'access all `tf.Variable`s via `tf.get_variable`?\\n'", "'  current_state_parts: {}\\n'", "'  proposed_state_parts: {}\\n'", "'  proposed_target_log_prob_grad_parts: {}'", ".", "format", "(", "current_state_parts", ",", "proposed_state_parts", ",", "proposed_target_log_prob_grad_parts", ")", ")", "# Step 3b: Update momentum (again).", "proposed_momentum_parts", "=", "[", "v", "+", "0.5", "*", "tf", ".", "cast", "(", "eps", ",", "v", ".", "dtype", ")", "*", "g", "for", "v", ",", "eps", ",", "g", "in", "zip", "(", "proposed_momentum_parts", ",", "step_sizes", ",", "proposed_target_log_prob_grad_parts", ")", "]", "return", "[", "proposed_momentum_parts", ",", "proposed_state_parts", ",", "proposed_target_log_prob", ",", "proposed_target_log_prob_grad_parts", ",", "]"], "docstring": "Applies `num_leapfrog_steps` of the leapfrog integrator.\n\n  Assumes a simple quadratic kinetic energy function: `0.5 ||momentum||**2`.\n\n  #### Examples:\n\n  ##### Simple quadratic potential.\n\n  ```python\n  import matplotlib.pyplot as plt\n  %matplotlib inline\n  import numpy as np\n  import tensorflow as tf\n  from tensorflow_probability.python.mcmc.hmc import _leapfrog_integrator_one_step  # pylint: disable=line-too-long\n  tfd = tfp.distributions\n\n  dims = 10\n  num_iter = int(1e3)\n  dtype = np.float32\n\n  position = tf.placeholder(np.float32)\n  momentum = tf.placeholder(np.float32)\n\n  target_log_prob_fn = tfd.MultivariateNormalDiag(\n      loc=tf.zeros(dims, dtype)).log_prob\n\n  def _leapfrog_one_step(*args):\n    # Closure representing computation done during each leapfrog step.\n    return _leapfrog_integrator_one_step(\n        target_log_prob_fn=target_log_prob_fn,\n        independent_chain_ndims=0,\n        step_sizes=[0.1],\n        current_momentum_parts=args[0],\n        current_state_parts=args[1],\n        current_target_log_prob=args[2],\n        current_target_log_prob_grad_parts=args[3])\n\n  # Do leapfrog integration.\n  [\n      [next_momentum],\n      [next_position],\n      next_target_log_prob,\n      next_target_log_prob_grad_parts,\n  ] = tf.while_loop(\n      cond=lambda *args: True,\n      body=_leapfrog_one_step,\n      loop_vars=[\n        [momentum],\n        [position],\n        target_log_prob_fn(position),\n        tf.gradients(target_log_prob_fn(position), position),\n      ],\n      maximum_iterations=3)\n\n  momentum_ = np.random.randn(dims).astype(dtype)\n  position_ = np.random.randn(dims).astype(dtype)\n  positions = np.zeros([num_iter, dims], dtype)\n\n  with tf.Session() as sess:\n    for i in xrange(num_iter):\n      position_, momentum_ = sess.run(\n          [next_momentum, next_position],\n          feed_dict={position: position_, momentum: momentum_})\n      positions[i] = position_\n\n  plt.plot(positions[:, 0]);  # Sinusoidal.\n  ```\n\n  Args:\n    target_log_prob_fn: Python callable which takes an argument like\n      `*current_state_parts` and returns its (possibly unnormalized) log-density\n      under the target distribution.\n    independent_chain_ndims: Scalar `int` `Tensor` representing the number of\n      leftmost `Tensor` dimensions which index independent chains.\n    step_sizes: Python `list` of `Tensor`s representing the step size for the\n      leapfrog integrator. Must broadcast with the shape of\n      `current_state_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    current_momentum_parts: Tensor containing the value(s) of the momentum\n      variable(s) to update.\n    current_state_parts: Python `list` of `Tensor`s representing the current\n      state(s) of the Markov chain(s). The first `independent_chain_ndims` of\n      the `Tensor`(s) index different chains.\n    current_target_log_prob: `Tensor` representing the value of\n      `target_log_prob_fn(*current_state_parts)`. The only reason to specify\n      this argument is to reduce TF graph size.\n    current_target_log_prob_grad_parts: Python list of `Tensor`s representing\n      gradient of `target_log_prob_fn(*current_state_parts`) wrt\n      `current_state_parts`. Must have same shape as `current_state_parts`. The\n      only reason to specify this argument is to reduce TF graph size.\n    state_gradients_are_stopped: Python `bool` indicating that the proposed new\n      state be run through `tf.stop_gradient`. This is particularly useful when\n      combining optimization over samples from the HMC chain.\n      Default value: `False` (i.e., do not apply `stop_gradient`).\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'hmc_leapfrog_integrator').\n\n  Returns:\n    proposed_momentum_parts: Updated value of the momentum.\n    proposed_state_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n    proposed_target_log_prob: `Tensor` representing the value of\n      `target_log_prob_fn` at `next_state`.\n    proposed_target_log_prob_grad_parts: Gradient of `proposed_target_log_prob`\n      wrt `next_state`.\n\n  Raises:\n    ValueError: if `len(momentum_parts) != len(state_parts)`.\n    ValueError: if `len(state_parts) != len(step_sizes)`.\n    ValueError: if `len(state_parts) != len(grads_target_log_prob)`.\n    TypeError: if `not target_log_prob.dtype.is_floating`.", "docstring_tokens": ["Applies", "num_leapfrog_steps", "of", "the", "leapfrog", "integrator", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/hmc.py#L820-L1049", "partition": "test", "index": 1006, "time": "2018-03-06 09:14:41"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/util.py", "func_name": "safe_sum", "original_string": "def safe_sum(x, alt_value=-np.inf, name=None):\n  \"\"\"Elementwise adds list members, replacing non-finite results with alt_value.\n\n  Typically the `alt_value` is chosen so the `MetropolisHastings`\n  `TransitionKernel` always rejects the proposal.\n\n  Args:\n    x: Python `list` of `Tensors` to elementwise add.\n    alt_value: Python scalar used to replace any elementwise sums which would\n      otherwise be non-finite.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"safe_sum\").\n\n  Returns:\n    safe_sum: `Tensor` representing the elementwise sum of list of `Tensor`s\n      `x` or `alt_value` where sums are non-finite.\n\n  Raises:\n    TypeError: if `x` is not list-like.\n    ValueError: if `x` is empty.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'safe_sum', [x, alt_value]):\n    if not is_list_like(x):\n      raise TypeError('Expected list input.')\n    if not x:\n      raise ValueError('Input should not be empty.')\n    in_shape = x[0].shape\n    x = tf.stack(x, axis=-1)\n    x = tf.reduce_sum(input_tensor=x, axis=-1)\n    alt_value = np.array(alt_value, x.dtype.as_numpy_dtype)\n    alt_fill = tf.fill(tf.shape(input=x), value=alt_value)\n    x = tf.where(tf.math.is_finite(x), x, alt_fill)\n    x.set_shape(x.shape.merge_with(in_shape))\n    return x", "language": "python", "code": "def safe_sum(x, alt_value=-np.inf, name=None):\n  \"\"\"Elementwise adds list members, replacing non-finite results with alt_value.\n\n  Typically the `alt_value` is chosen so the `MetropolisHastings`\n  `TransitionKernel` always rejects the proposal.\n\n  Args:\n    x: Python `list` of `Tensors` to elementwise add.\n    alt_value: Python scalar used to replace any elementwise sums which would\n      otherwise be non-finite.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"safe_sum\").\n\n  Returns:\n    safe_sum: `Tensor` representing the elementwise sum of list of `Tensor`s\n      `x` or `alt_value` where sums are non-finite.\n\n  Raises:\n    TypeError: if `x` is not list-like.\n    ValueError: if `x` is empty.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'safe_sum', [x, alt_value]):\n    if not is_list_like(x):\n      raise TypeError('Expected list input.')\n    if not x:\n      raise ValueError('Input should not be empty.')\n    in_shape = x[0].shape\n    x = tf.stack(x, axis=-1)\n    x = tf.reduce_sum(input_tensor=x, axis=-1)\n    alt_value = np.array(alt_value, x.dtype.as_numpy_dtype)\n    alt_fill = tf.fill(tf.shape(input=x), value=alt_value)\n    x = tf.where(tf.math.is_finite(x), x, alt_fill)\n    x.set_shape(x.shape.merge_with(in_shape))\n    return x", "code_tokens": ["def", "safe_sum", "(", "x", ",", "alt_value", "=", "-", "np", ".", "inf", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'safe_sum'", ",", "[", "x", ",", "alt_value", "]", ")", ":", "if", "not", "is_list_like", "(", "x", ")", ":", "raise", "TypeError", "(", "'Expected list input.'", ")", "if", "not", "x", ":", "raise", "ValueError", "(", "'Input should not be empty.'", ")", "in_shape", "=", "x", "[", "0", "]", ".", "shape", "x", "=", "tf", ".", "stack", "(", "x", ",", "axis", "=", "-", "1", ")", "x", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "x", ",", "axis", "=", "-", "1", ")", "alt_value", "=", "np", ".", "array", "(", "alt_value", ",", "x", ".", "dtype", ".", "as_numpy_dtype", ")", "alt_fill", "=", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "value", "=", "alt_value", ")", "x", "=", "tf", ".", "where", "(", "tf", ".", "math", ".", "is_finite", "(", "x", ")", ",", "x", ",", "alt_fill", ")", "x", ".", "set_shape", "(", "x", ".", "shape", ".", "merge_with", "(", "in_shape", ")", ")", "return", "x"], "docstring": "Elementwise adds list members, replacing non-finite results with alt_value.\n\n  Typically the `alt_value` is chosen so the `MetropolisHastings`\n  `TransitionKernel` always rejects the proposal.\n\n  Args:\n    x: Python `list` of `Tensors` to elementwise add.\n    alt_value: Python scalar used to replace any elementwise sums which would\n      otherwise be non-finite.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"safe_sum\").\n\n  Returns:\n    safe_sum: `Tensor` representing the elementwise sum of list of `Tensor`s\n      `x` or `alt_value` where sums are non-finite.\n\n  Raises:\n    TypeError: if `x` is not list-like.\n    ValueError: if `x` is empty.", "docstring_tokens": ["Elementwise", "adds", "list", "members", "replacing", "non", "-", "finite", "results", "with", "alt_value", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/util.py#L132-L165", "partition": "test", "index": 967, "time": "2018-03-06 09:14:41"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/hmc.py", "func_name": "HamiltonianMonteCarlo.one_step", "original_string": "def one_step(self, current_state, previous_kernel_results):\n    \"\"\"Runs one iteration of Hamiltonian Monte Carlo.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s representing the\n        current state(s) of the Markov chain(s). The first `r` dimensions index\n        independent chains, `r = tf.rank(target_log_prob_fn(*current_state))`.\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.\n\n    Raises:\n      ValueError: if there isn't one `step_size` or a list with same length as\n        `current_state`.\n    \"\"\"\n    previous_step_size_assign = (\n        [] if self.step_size_update_fn is None\n        else (previous_kernel_results.extra.step_size_assign\n              if mcmc_util.is_list_like(\n                  previous_kernel_results.extra.step_size_assign)\n              else [previous_kernel_results.extra.step_size_assign]))\n\n    with tf.control_dependencies(previous_step_size_assign):\n      next_state, kernel_results = self._impl.one_step(\n          current_state, previous_kernel_results)\n      if self.step_size_update_fn is not None:\n        step_size_assign = self.step_size_update_fn(  # pylint: disable=not-callable\n            self.step_size, kernel_results)\n        kernel_results = kernel_results._replace(\n            extra=HamiltonianMonteCarloExtraKernelResults(\n                step_size_assign=step_size_assign))\n      return next_state, kernel_results", "language": "python", "code": "def one_step(self, current_state, previous_kernel_results):\n    \"\"\"Runs one iteration of Hamiltonian Monte Carlo.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s representing the\n        current state(s) of the Markov chain(s). The first `r` dimensions index\n        independent chains, `r = tf.rank(target_log_prob_fn(*current_state))`.\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.\n\n    Raises:\n      ValueError: if there isn't one `step_size` or a list with same length as\n        `current_state`.\n    \"\"\"\n    previous_step_size_assign = (\n        [] if self.step_size_update_fn is None\n        else (previous_kernel_results.extra.step_size_assign\n              if mcmc_util.is_list_like(\n                  previous_kernel_results.extra.step_size_assign)\n              else [previous_kernel_results.extra.step_size_assign]))\n\n    with tf.control_dependencies(previous_step_size_assign):\n      next_state, kernel_results = self._impl.one_step(\n          current_state, previous_kernel_results)\n      if self.step_size_update_fn is not None:\n        step_size_assign = self.step_size_update_fn(  # pylint: disable=not-callable\n            self.step_size, kernel_results)\n        kernel_results = kernel_results._replace(\n            extra=HamiltonianMonteCarloExtraKernelResults(\n                step_size_assign=step_size_assign))\n      return next_state, kernel_results", "code_tokens": ["def", "one_step", "(", "self", ",", "current_state", ",", "previous_kernel_results", ")", ":", "previous_step_size_assign", "=", "(", "[", "]", "if", "self", ".", "step_size_update_fn", "is", "None", "else", "(", "previous_kernel_results", ".", "extra", ".", "step_size_assign", "if", "mcmc_util", ".", "is_list_like", "(", "previous_kernel_results", ".", "extra", ".", "step_size_assign", ")", "else", "[", "previous_kernel_results", ".", "extra", ".", "step_size_assign", "]", ")", ")", "with", "tf", ".", "control_dependencies", "(", "previous_step_size_assign", ")", ":", "next_state", ",", "kernel_results", "=", "self", ".", "_impl", ".", "one_step", "(", "current_state", ",", "previous_kernel_results", ")", "if", "self", ".", "step_size_update_fn", "is", "not", "None", ":", "step_size_assign", "=", "self", ".", "step_size_update_fn", "(", "# pylint: disable=not-callable", "self", ".", "step_size", ",", "kernel_results", ")", "kernel_results", "=", "kernel_results", ".", "_replace", "(", "extra", "=", "HamiltonianMonteCarloExtraKernelResults", "(", "step_size_assign", "=", "step_size_assign", ")", ")", "return", "next_state", ",", "kernel_results"], "docstring": "Runs one iteration of Hamiltonian Monte Carlo.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s representing the\n        current state(s) of the Markov chain(s). The first `r` dimensions index\n        independent chains, `r = tf.rank(target_log_prob_fn(*current_state))`.\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.\n\n    Raises:\n      ValueError: if there isn't one `step_size` or a list with same length as\n        `current_state`.", "docstring_tokens": ["Runs", "one", "iteration", "of", "Hamiltonian", "Monte", "Carlo", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/hmc.py#L516-L554", "partition": "test", "index": 1008, "time": "2018-03-06 09:14:41"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/hmc.py", "func_name": "_compute_log_acceptance_correction", "original_string": "def _compute_log_acceptance_correction(current_momentums,\n                                       proposed_momentums,\n                                       independent_chain_ndims,\n                                       name=None):\n  \"\"\"Helper to `kernel` which computes the log acceptance-correction.\n\n  A sufficient but not necessary condition for the existence of a stationary\n  distribution, `p(x)`, is \"detailed balance\", i.e.:\n\n  ```none\n  p(x'|x) p(x) = p(x|x') p(x')\n  ```\n\n  In the Metropolis-Hastings algorithm, a state is proposed according to\n  `g(x'|x)` and accepted according to `a(x'|x)`, hence\n  `p(x'|x) = g(x'|x) a(x'|x)`.\n\n  Inserting this into the detailed balance equation implies:\n\n  ```none\n      g(x'|x) a(x'|x) p(x) = g(x|x') a(x|x') p(x')\n  ==> a(x'|x) / a(x|x') = p(x') / p(x) [g(x|x') / g(x'|x)]    (*)\n  ```\n\n  One definition of `a(x'|x)` which satisfies (*) is:\n\n  ```none\n  a(x'|x) = min(1, p(x') / p(x) [g(x|x') / g(x'|x)])\n  ```\n\n  (To see that this satisfies (*), notice that under this definition only at\n  most one `a(x'|x)` and `a(x|x') can be other than one.)\n\n  We call the bracketed term the \"acceptance correction\".\n\n  In the case of UncalibratedHMC, the log acceptance-correction is not the log\n  proposal-ratio. UncalibratedHMC augments the state-space with momentum, z.\n  Assuming a standard Gaussian distribution for momentums, the chain eventually\n  converges to:\n\n  ```none\n  p([x, z]) propto= target_prob(x) exp(-0.5 z**2)\n  ```\n\n  Relating this back to Metropolis-Hastings parlance, for HMC we have:\n\n  ```none\n  p([x, z]) propto= target_prob(x) exp(-0.5 z**2)\n  g([x, z] | [x', z']) = g([x', z'] | [x, z])\n  ```\n\n  In other words, the MH bracketed term is `1`. However, because we desire to\n  use a general MH framework, we can place the momentum probability ratio inside\n  the metropolis-correction factor thus getting an acceptance probability:\n\n  ```none\n                       target_prob(x')\n  accept_prob(x'|x) = -----------------  [exp(-0.5 z**2) / exp(-0.5 z'**2)]\n                       target_prob(x)\n  ```\n\n  (Note: we actually need to handle the kinetic energy change at each leapfrog\n  step, but this is the idea.)\n\n  Args:\n    current_momentums: `Tensor` representing the value(s) of the current\n      momentum(s) of the state (parts).\n    proposed_momentums: `Tensor` representing the value(s) of the proposed\n      momentum(s) of the state (parts).\n    independent_chain_ndims: Scalar `int` `Tensor` representing the number of\n      leftmost `Tensor` dimensions which index independent chains.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'compute_log_acceptance_correction').\n\n  Returns:\n    log_acceptance_correction: `Tensor` representing the `log`\n      acceptance-correction.  (See docstring for mathematical definition.)\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'compute_log_acceptance_correction',\n      [independent_chain_ndims, current_momentums, proposed_momentums]):\n    log_current_kinetic, log_proposed_kinetic = [], []\n    for current_momentum, proposed_momentum in zip(\n        current_momentums, proposed_momentums):\n      axis = tf.range(independent_chain_ndims, tf.rank(current_momentum))\n      log_current_kinetic.append(_log_sum_sq(current_momentum, axis))\n      log_proposed_kinetic.append(_log_sum_sq(proposed_momentum, axis))\n    current_kinetic = 0.5 * tf.exp(\n        tf.reduce_logsumexp(\n            input_tensor=tf.stack(log_current_kinetic, axis=-1), axis=-1))\n    proposed_kinetic = 0.5 * tf.exp(\n        tf.reduce_logsumexp(\n            input_tensor=tf.stack(log_proposed_kinetic, axis=-1), axis=-1))\n    return mcmc_util.safe_sum([current_kinetic, -proposed_kinetic])", "language": "python", "code": "def _compute_log_acceptance_correction(current_momentums,\n                                       proposed_momentums,\n                                       independent_chain_ndims,\n                                       name=None):\n  \"\"\"Helper to `kernel` which computes the log acceptance-correction.\n\n  A sufficient but not necessary condition for the existence of a stationary\n  distribution, `p(x)`, is \"detailed balance\", i.e.:\n\n  ```none\n  p(x'|x) p(x) = p(x|x') p(x')\n  ```\n\n  In the Metropolis-Hastings algorithm, a state is proposed according to\n  `g(x'|x)` and accepted according to `a(x'|x)`, hence\n  `p(x'|x) = g(x'|x) a(x'|x)`.\n\n  Inserting this into the detailed balance equation implies:\n\n  ```none\n      g(x'|x) a(x'|x) p(x) = g(x|x') a(x|x') p(x')\n  ==> a(x'|x) / a(x|x') = p(x') / p(x) [g(x|x') / g(x'|x)]    (*)\n  ```\n\n  One definition of `a(x'|x)` which satisfies (*) is:\n\n  ```none\n  a(x'|x) = min(1, p(x') / p(x) [g(x|x') / g(x'|x)])\n  ```\n\n  (To see that this satisfies (*), notice that under this definition only at\n  most one `a(x'|x)` and `a(x|x') can be other than one.)\n\n  We call the bracketed term the \"acceptance correction\".\n\n  In the case of UncalibratedHMC, the log acceptance-correction is not the log\n  proposal-ratio. UncalibratedHMC augments the state-space with momentum, z.\n  Assuming a standard Gaussian distribution for momentums, the chain eventually\n  converges to:\n\n  ```none\n  p([x, z]) propto= target_prob(x) exp(-0.5 z**2)\n  ```\n\n  Relating this back to Metropolis-Hastings parlance, for HMC we have:\n\n  ```none\n  p([x, z]) propto= target_prob(x) exp(-0.5 z**2)\n  g([x, z] | [x', z']) = g([x', z'] | [x, z])\n  ```\n\n  In other words, the MH bracketed term is `1`. However, because we desire to\n  use a general MH framework, we can place the momentum probability ratio inside\n  the metropolis-correction factor thus getting an acceptance probability:\n\n  ```none\n                       target_prob(x')\n  accept_prob(x'|x) = -----------------  [exp(-0.5 z**2) / exp(-0.5 z'**2)]\n                       target_prob(x)\n  ```\n\n  (Note: we actually need to handle the kinetic energy change at each leapfrog\n  step, but this is the idea.)\n\n  Args:\n    current_momentums: `Tensor` representing the value(s) of the current\n      momentum(s) of the state (parts).\n    proposed_momentums: `Tensor` representing the value(s) of the proposed\n      momentum(s) of the state (parts).\n    independent_chain_ndims: Scalar `int` `Tensor` representing the number of\n      leftmost `Tensor` dimensions which index independent chains.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'compute_log_acceptance_correction').\n\n  Returns:\n    log_acceptance_correction: `Tensor` representing the `log`\n      acceptance-correction.  (See docstring for mathematical definition.)\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'compute_log_acceptance_correction',\n      [independent_chain_ndims, current_momentums, proposed_momentums]):\n    log_current_kinetic, log_proposed_kinetic = [], []\n    for current_momentum, proposed_momentum in zip(\n        current_momentums, proposed_momentums):\n      axis = tf.range(independent_chain_ndims, tf.rank(current_momentum))\n      log_current_kinetic.append(_log_sum_sq(current_momentum, axis))\n      log_proposed_kinetic.append(_log_sum_sq(proposed_momentum, axis))\n    current_kinetic = 0.5 * tf.exp(\n        tf.reduce_logsumexp(\n            input_tensor=tf.stack(log_current_kinetic, axis=-1), axis=-1))\n    proposed_kinetic = 0.5 * tf.exp(\n        tf.reduce_logsumexp(\n            input_tensor=tf.stack(log_proposed_kinetic, axis=-1), axis=-1))\n    return mcmc_util.safe_sum([current_kinetic, -proposed_kinetic])", "code_tokens": ["def", "_compute_log_acceptance_correction", "(", "current_momentums", ",", "proposed_momentums", ",", "independent_chain_ndims", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'compute_log_acceptance_correction'", ",", "[", "independent_chain_ndims", ",", "current_momentums", ",", "proposed_momentums", "]", ")", ":", "log_current_kinetic", ",", "log_proposed_kinetic", "=", "[", "]", ",", "[", "]", "for", "current_momentum", ",", "proposed_momentum", "in", "zip", "(", "current_momentums", ",", "proposed_momentums", ")", ":", "axis", "=", "tf", ".", "range", "(", "independent_chain_ndims", ",", "tf", ".", "rank", "(", "current_momentum", ")", ")", "log_current_kinetic", ".", "append", "(", "_log_sum_sq", "(", "current_momentum", ",", "axis", ")", ")", "log_proposed_kinetic", ".", "append", "(", "_log_sum_sq", "(", "proposed_momentum", ",", "axis", ")", ")", "current_kinetic", "=", "0.5", "*", "tf", ".", "exp", "(", "tf", ".", "reduce_logsumexp", "(", "input_tensor", "=", "tf", ".", "stack", "(", "log_current_kinetic", ",", "axis", "=", "-", "1", ")", ",", "axis", "=", "-", "1", ")", ")", "proposed_kinetic", "=", "0.5", "*", "tf", ".", "exp", "(", "tf", ".", "reduce_logsumexp", "(", "input_tensor", "=", "tf", ".", "stack", "(", "log_proposed_kinetic", ",", "axis", "=", "-", "1", ")", ",", "axis", "=", "-", "1", ")", ")", "return", "mcmc_util", ".", "safe_sum", "(", "[", "current_kinetic", ",", "-", "proposed_kinetic", "]", ")"], "docstring": "Helper to `kernel` which computes the log acceptance-correction.\n\n  A sufficient but not necessary condition for the existence of a stationary\n  distribution, `p(x)`, is \"detailed balance\", i.e.:\n\n  ```none\n  p(x'|x) p(x) = p(x|x') p(x')\n  ```\n\n  In the Metropolis-Hastings algorithm, a state is proposed according to\n  `g(x'|x)` and accepted according to `a(x'|x)`, hence\n  `p(x'|x) = g(x'|x) a(x'|x)`.\n\n  Inserting this into the detailed balance equation implies:\n\n  ```none\n      g(x'|x) a(x'|x) p(x) = g(x|x') a(x|x') p(x')\n  ==> a(x'|x) / a(x|x') = p(x') / p(x) [g(x|x') / g(x'|x)]    (*)\n  ```\n\n  One definition of `a(x'|x)` which satisfies (*) is:\n\n  ```none\n  a(x'|x) = min(1, p(x') / p(x) [g(x|x') / g(x'|x)])\n  ```\n\n  (To see that this satisfies (*), notice that under this definition only at\n  most one `a(x'|x)` and `a(x|x') can be other than one.)\n\n  We call the bracketed term the \"acceptance correction\".\n\n  In the case of UncalibratedHMC, the log acceptance-correction is not the log\n  proposal-ratio. UncalibratedHMC augments the state-space with momentum, z.\n  Assuming a standard Gaussian distribution for momentums, the chain eventually\n  converges to:\n\n  ```none\n  p([x, z]) propto= target_prob(x) exp(-0.5 z**2)\n  ```\n\n  Relating this back to Metropolis-Hastings parlance, for HMC we have:\n\n  ```none\n  p([x, z]) propto= target_prob(x) exp(-0.5 z**2)\n  g([x, z] | [x', z']) = g([x', z'] | [x, z])\n  ```\n\n  In other words, the MH bracketed term is `1`. However, because we desire to\n  use a general MH framework, we can place the momentum probability ratio inside\n  the metropolis-correction factor thus getting an acceptance probability:\n\n  ```none\n                       target_prob(x')\n  accept_prob(x'|x) = -----------------  [exp(-0.5 z**2) / exp(-0.5 z'**2)]\n                       target_prob(x)\n  ```\n\n  (Note: we actually need to handle the kinetic energy change at each leapfrog\n  step, but this is the idea.)\n\n  Args:\n    current_momentums: `Tensor` representing the value(s) of the current\n      momentum(s) of the state (parts).\n    proposed_momentums: `Tensor` representing the value(s) of the proposed\n      momentum(s) of the state (parts).\n    independent_chain_ndims: Scalar `int` `Tensor` representing the number of\n      leftmost `Tensor` dimensions which index independent chains.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'compute_log_acceptance_correction').\n\n  Returns:\n    log_acceptance_correction: `Tensor` representing the `log`\n      acceptance-correction.  (See docstring for mathematical definition.)", "docstring_tokens": ["Helper", "to", "kernel", "which", "computes", "the", "log", "acceptance", "-", "correction", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/hmc.py#L1052-L1145", "partition": "test", "index": 1007, "time": "2018-03-06 09:14:41"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/diagnostic.py", "func_name": "_broadcast_maybelist_arg", "original_string": "def _broadcast_maybelist_arg(states, secondary_arg, name):\n  \"\"\"Broadcast a listable secondary_arg to that of states.\"\"\"\n  if _is_list_like(secondary_arg):\n    if len(secondary_arg) != len(states):\n      raise ValueError('Argument `%s` was a list of different length ({}) than '\n                       '`states` ({})'.format(name, len(states)))\n  else:\n    secondary_arg = [secondary_arg] * len(states)\n\n  return secondary_arg", "language": "python", "code": "def _broadcast_maybelist_arg(states, secondary_arg, name):\n  \"\"\"Broadcast a listable secondary_arg to that of states.\"\"\"\n  if _is_list_like(secondary_arg):\n    if len(secondary_arg) != len(states):\n      raise ValueError('Argument `%s` was a list of different length ({}) than '\n                       '`states` ({})'.format(name, len(states)))\n  else:\n    secondary_arg = [secondary_arg] * len(states)\n\n  return secondary_arg", "code_tokens": ["def", "_broadcast_maybelist_arg", "(", "states", ",", "secondary_arg", ",", "name", ")", ":", "if", "_is_list_like", "(", "secondary_arg", ")", ":", "if", "len", "(", "secondary_arg", ")", "!=", "len", "(", "states", ")", ":", "raise", "ValueError", "(", "'Argument `%s` was a list of different length ({}) than '", "'`states` ({})'", ".", "format", "(", "name", ",", "len", "(", "states", ")", ")", ")", "else", ":", "secondary_arg", "=", "[", "secondary_arg", "]", "*", "len", "(", "states", ")", "return", "secondary_arg"], "docstring": "Broadcast a listable secondary_arg to that of states.", "docstring_tokens": ["Broadcast", "a", "listable", "secondary_arg", "to", "that", "of", "states", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/diagnostic.py#L401-L410", "partition": "test", "index": 785, "time": "2018-03-06 11:55:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/diagnostic.py", "func_name": "_potential_scale_reduction_single_state", "original_string": "def _potential_scale_reduction_single_state(state, independent_chain_ndims):\n  \"\"\"potential_scale_reduction for one single state `Tensor`.\"\"\"\n  with tf.compat.v1.name_scope(\n      'potential_scale_reduction_single_state',\n      values=[state, independent_chain_ndims]):\n    # We assume exactly one leading dimension indexes e.g. correlated samples\n    # from each Markov chain.\n    state = tf.convert_to_tensor(value=state, name='state')\n    sample_ndims = 1\n\n    sample_axis = tf.range(0, sample_ndims)\n    chain_axis = tf.range(sample_ndims,\n                          sample_ndims + independent_chain_ndims)\n    sample_and_chain_axis = tf.range(\n        0, sample_ndims + independent_chain_ndims)\n\n    n = _axis_size(state, sample_axis)\n    m = _axis_size(state, chain_axis)\n\n    # In the language of Brooks and Gelman (1998),\n    # B / n is the between chain variance, the variance of the chain means.\n    # W is the within sequence variance, the mean of the chain variances.\n    b_div_n = _reduce_variance(\n        tf.reduce_mean(input_tensor=state, axis=sample_axis, keepdims=True),\n        sample_and_chain_axis,\n        biased=False)\n    w = tf.reduce_mean(\n        input_tensor=_reduce_variance(\n            state, sample_axis, keepdims=True, biased=True),\n        axis=sample_and_chain_axis)\n\n    # sigma^2_+ is an estimate of the true variance, which would be unbiased if\n    # each chain was drawn from the target.  c.f. \"law of total variance.\"\n    sigma_2_plus = w + b_div_n\n\n    return ((m + 1.) / m) * sigma_2_plus / w - (n - 1.) / (m * n)", "language": "python", "code": "def _potential_scale_reduction_single_state(state, independent_chain_ndims):\n  \"\"\"potential_scale_reduction for one single state `Tensor`.\"\"\"\n  with tf.compat.v1.name_scope(\n      'potential_scale_reduction_single_state',\n      values=[state, independent_chain_ndims]):\n    # We assume exactly one leading dimension indexes e.g. correlated samples\n    # from each Markov chain.\n    state = tf.convert_to_tensor(value=state, name='state')\n    sample_ndims = 1\n\n    sample_axis = tf.range(0, sample_ndims)\n    chain_axis = tf.range(sample_ndims,\n                          sample_ndims + independent_chain_ndims)\n    sample_and_chain_axis = tf.range(\n        0, sample_ndims + independent_chain_ndims)\n\n    n = _axis_size(state, sample_axis)\n    m = _axis_size(state, chain_axis)\n\n    # In the language of Brooks and Gelman (1998),\n    # B / n is the between chain variance, the variance of the chain means.\n    # W is the within sequence variance, the mean of the chain variances.\n    b_div_n = _reduce_variance(\n        tf.reduce_mean(input_tensor=state, axis=sample_axis, keepdims=True),\n        sample_and_chain_axis,\n        biased=False)\n    w = tf.reduce_mean(\n        input_tensor=_reduce_variance(\n            state, sample_axis, keepdims=True, biased=True),\n        axis=sample_and_chain_axis)\n\n    # sigma^2_+ is an estimate of the true variance, which would be unbiased if\n    # each chain was drawn from the target.  c.f. \"law of total variance.\"\n    sigma_2_plus = w + b_div_n\n\n    return ((m + 1.) / m) * sigma_2_plus / w - (n - 1.) / (m * n)", "code_tokens": ["def", "_potential_scale_reduction_single_state", "(", "state", ",", "independent_chain_ndims", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'potential_scale_reduction_single_state'", ",", "values", "=", "[", "state", ",", "independent_chain_ndims", "]", ")", ":", "# We assume exactly one leading dimension indexes e.g. correlated samples", "# from each Markov chain.", "state", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "state", ",", "name", "=", "'state'", ")", "sample_ndims", "=", "1", "sample_axis", "=", "tf", ".", "range", "(", "0", ",", "sample_ndims", ")", "chain_axis", "=", "tf", ".", "range", "(", "sample_ndims", ",", "sample_ndims", "+", "independent_chain_ndims", ")", "sample_and_chain_axis", "=", "tf", ".", "range", "(", "0", ",", "sample_ndims", "+", "independent_chain_ndims", ")", "n", "=", "_axis_size", "(", "state", ",", "sample_axis", ")", "m", "=", "_axis_size", "(", "state", ",", "chain_axis", ")", "# In the language of Brooks and Gelman (1998),", "# B / n is the between chain variance, the variance of the chain means.", "# W is the within sequence variance, the mean of the chain variances.", "b_div_n", "=", "_reduce_variance", "(", "tf", ".", "reduce_mean", "(", "input_tensor", "=", "state", ",", "axis", "=", "sample_axis", ",", "keepdims", "=", "True", ")", ",", "sample_and_chain_axis", ",", "biased", "=", "False", ")", "w", "=", "tf", ".", "reduce_mean", "(", "input_tensor", "=", "_reduce_variance", "(", "state", ",", "sample_axis", ",", "keepdims", "=", "True", ",", "biased", "=", "True", ")", ",", "axis", "=", "sample_and_chain_axis", ")", "# sigma^2_+ is an estimate of the true variance, which would be unbiased if", "# each chain was drawn from the target.  c.f. \"law of total variance.\"", "sigma_2_plus", "=", "w", "+", "b_div_n", "return", "(", "(", "m", "+", "1.", ")", "/", "m", ")", "*", "sigma_2_plus", "/", "w", "-", "(", "n", "-", "1.", ")", "/", "(", "m", "*", "n", ")"], "docstring": "potential_scale_reduction for one single state `Tensor`.", "docstring_tokens": ["potential_scale_reduction", "for", "one", "single", "state", "Tensor", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/diagnostic.py#L335-L370", "partition": "test", "index": 783, "time": "2018-03-06 11:55:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/diagnostic.py", "func_name": "_effective_sample_size_single_state", "original_string": "def _effective_sample_size_single_state(states, filter_beyond_lag,\n                                        filter_threshold):\n  \"\"\"ESS computation for one single Tensor argument.\"\"\"\n\n  with tf.compat.v1.name_scope(\n      'effective_sample_size_single_state',\n      values=[states, filter_beyond_lag, filter_threshold]):\n\n    states = tf.convert_to_tensor(value=states, name='states')\n    dt = states.dtype\n\n    # filter_beyond_lag == None ==> auto_corr is the full sequence.\n    auto_corr = stats.auto_correlation(\n        states, axis=0, max_lags=filter_beyond_lag)\n    if filter_threshold is not None:\n      filter_threshold = tf.convert_to_tensor(\n          value=filter_threshold, dtype=dt, name='filter_threshold')\n      # Get a binary mask to zero out values of auto_corr below the threshold.\n      #   mask[i, ...] = 1 if auto_corr[j, ...] > threshold for all j <= i,\n      #   mask[i, ...] = 0, otherwise.\n      # So, along dimension zero, the mask will look like [1, 1, ..., 0, 0,...]\n      # Building step by step,\n      #   Assume auto_corr = [1, 0.5, 0.0, 0.3], and filter_threshold = 0.2.\n      # Step 1:  mask = [False, False, True, False]\n      mask = auto_corr < filter_threshold\n      # Step 2:  mask = [0, 0, 1, 1]\n      mask = tf.cast(mask, dtype=dt)\n      # Step 3:  mask = [0, 0, 1, 2]\n      mask = tf.cumsum(mask, axis=0)\n      # Step 4:  mask = [1, 1, 0, 0]\n      mask = tf.maximum(1. - mask, 0.)\n      auto_corr *= mask\n\n    # With R[k] := auto_corr[k, ...],\n    # ESS = N / {1 + 2 * Sum_{k=1}^N (N - k) / N * R[k]}\n    #     = N / {-1 + 2 * Sum_{k=0}^N (N - k) / N * R[k]} (since R[0] = 1)\n    #     approx N / {-1 + 2 * Sum_{k=0}^M (N - k) / N * R[k]}\n    # where M is the filter_beyond_lag truncation point chosen above.\n\n    # Get the factor (N - k) / N, and give it shape [M, 1,...,1], having total\n    # ndims the same as auto_corr\n    n = _axis_size(states, axis=0)\n    k = tf.range(0., _axis_size(auto_corr, axis=0))\n    nk_factor = (n - k) / n\n    if auto_corr.shape.ndims is not None:\n      new_shape = [-1] + [1] * (auto_corr.shape.ndims - 1)\n    else:\n      new_shape = tf.concat(\n          ([-1],\n           tf.ones([tf.rank(auto_corr) - 1], dtype=tf.int32)),\n          axis=0)\n    nk_factor = tf.reshape(nk_factor, new_shape)\n\n    return n / (-1 +\n                2 * tf.reduce_sum(input_tensor=nk_factor * auto_corr, axis=0))", "language": "python", "code": "def _effective_sample_size_single_state(states, filter_beyond_lag,\n                                        filter_threshold):\n  \"\"\"ESS computation for one single Tensor argument.\"\"\"\n\n  with tf.compat.v1.name_scope(\n      'effective_sample_size_single_state',\n      values=[states, filter_beyond_lag, filter_threshold]):\n\n    states = tf.convert_to_tensor(value=states, name='states')\n    dt = states.dtype\n\n    # filter_beyond_lag == None ==> auto_corr is the full sequence.\n    auto_corr = stats.auto_correlation(\n        states, axis=0, max_lags=filter_beyond_lag)\n    if filter_threshold is not None:\n      filter_threshold = tf.convert_to_tensor(\n          value=filter_threshold, dtype=dt, name='filter_threshold')\n      # Get a binary mask to zero out values of auto_corr below the threshold.\n      #   mask[i, ...] = 1 if auto_corr[j, ...] > threshold for all j <= i,\n      #   mask[i, ...] = 0, otherwise.\n      # So, along dimension zero, the mask will look like [1, 1, ..., 0, 0,...]\n      # Building step by step,\n      #   Assume auto_corr = [1, 0.5, 0.0, 0.3], and filter_threshold = 0.2.\n      # Step 1:  mask = [False, False, True, False]\n      mask = auto_corr < filter_threshold\n      # Step 2:  mask = [0, 0, 1, 1]\n      mask = tf.cast(mask, dtype=dt)\n      # Step 3:  mask = [0, 0, 1, 2]\n      mask = tf.cumsum(mask, axis=0)\n      # Step 4:  mask = [1, 1, 0, 0]\n      mask = tf.maximum(1. - mask, 0.)\n      auto_corr *= mask\n\n    # With R[k] := auto_corr[k, ...],\n    # ESS = N / {1 + 2 * Sum_{k=1}^N (N - k) / N * R[k]}\n    #     = N / {-1 + 2 * Sum_{k=0}^N (N - k) / N * R[k]} (since R[0] = 1)\n    #     approx N / {-1 + 2 * Sum_{k=0}^M (N - k) / N * R[k]}\n    # where M is the filter_beyond_lag truncation point chosen above.\n\n    # Get the factor (N - k) / N, and give it shape [M, 1,...,1], having total\n    # ndims the same as auto_corr\n    n = _axis_size(states, axis=0)\n    k = tf.range(0., _axis_size(auto_corr, axis=0))\n    nk_factor = (n - k) / n\n    if auto_corr.shape.ndims is not None:\n      new_shape = [-1] + [1] * (auto_corr.shape.ndims - 1)\n    else:\n      new_shape = tf.concat(\n          ([-1],\n           tf.ones([tf.rank(auto_corr) - 1], dtype=tf.int32)),\n          axis=0)\n    nk_factor = tf.reshape(nk_factor, new_shape)\n\n    return n / (-1 +\n                2 * tf.reduce_sum(input_tensor=nk_factor * auto_corr, axis=0))", "code_tokens": ["def", "_effective_sample_size_single_state", "(", "states", ",", "filter_beyond_lag", ",", "filter_threshold", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'effective_sample_size_single_state'", ",", "values", "=", "[", "states", ",", "filter_beyond_lag", ",", "filter_threshold", "]", ")", ":", "states", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "states", ",", "name", "=", "'states'", ")", "dt", "=", "states", ".", "dtype", "# filter_beyond_lag == None ==> auto_corr is the full sequence.", "auto_corr", "=", "stats", ".", "auto_correlation", "(", "states", ",", "axis", "=", "0", ",", "max_lags", "=", "filter_beyond_lag", ")", "if", "filter_threshold", "is", "not", "None", ":", "filter_threshold", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "filter_threshold", ",", "dtype", "=", "dt", ",", "name", "=", "'filter_threshold'", ")", "# Get a binary mask to zero out values of auto_corr below the threshold.", "#   mask[i, ...] = 1 if auto_corr[j, ...] > threshold for all j <= i,", "#   mask[i, ...] = 0, otherwise.", "# So, along dimension zero, the mask will look like [1, 1, ..., 0, 0,...]", "# Building step by step,", "#   Assume auto_corr = [1, 0.5, 0.0, 0.3], and filter_threshold = 0.2.", "# Step 1:  mask = [False, False, True, False]", "mask", "=", "auto_corr", "<", "filter_threshold", "# Step 2:  mask = [0, 0, 1, 1]", "mask", "=", "tf", ".", "cast", "(", "mask", ",", "dtype", "=", "dt", ")", "# Step 3:  mask = [0, 0, 1, 2]", "mask", "=", "tf", ".", "cumsum", "(", "mask", ",", "axis", "=", "0", ")", "# Step 4:  mask = [1, 1, 0, 0]", "mask", "=", "tf", ".", "maximum", "(", "1.", "-", "mask", ",", "0.", ")", "auto_corr", "*=", "mask", "# With R[k] := auto_corr[k, ...],", "# ESS = N / {1 + 2 * Sum_{k=1}^N (N - k) / N * R[k]}", "#     = N / {-1 + 2 * Sum_{k=0}^N (N - k) / N * R[k]} (since R[0] = 1)", "#     approx N / {-1 + 2 * Sum_{k=0}^M (N - k) / N * R[k]}", "# where M is the filter_beyond_lag truncation point chosen above.", "# Get the factor (N - k) / N, and give it shape [M, 1,...,1], having total", "# ndims the same as auto_corr", "n", "=", "_axis_size", "(", "states", ",", "axis", "=", "0", ")", "k", "=", "tf", ".", "range", "(", "0.", ",", "_axis_size", "(", "auto_corr", ",", "axis", "=", "0", ")", ")", "nk_factor", "=", "(", "n", "-", "k", ")", "/", "n", "if", "auto_corr", ".", "shape", ".", "ndims", "is", "not", "None", ":", "new_shape", "=", "[", "-", "1", "]", "+", "[", "1", "]", "*", "(", "auto_corr", ".", "shape", ".", "ndims", "-", "1", ")", "else", ":", "new_shape", "=", "tf", ".", "concat", "(", "(", "[", "-", "1", "]", ",", "tf", ".", "ones", "(", "[", "tf", ".", "rank", "(", "auto_corr", ")", "-", "1", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ")", ",", "axis", "=", "0", ")", "nk_factor", "=", "tf", ".", "reshape", "(", "nk_factor", ",", "new_shape", ")", "return", "n", "/", "(", "-", "1", "+", "2", "*", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "nk_factor", "*", "auto_corr", ",", "axis", "=", "0", ")", ")"], "docstring": "ESS computation for one single Tensor argument.", "docstring_tokens": ["ESS", "computation", "for", "one", "single", "Tensor", "argument", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/diagnostic.py#L146-L200", "partition": "test", "index": 782, "time": "2018-03-06 11:55:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/diagnostic.py", "func_name": "effective_sample_size", "original_string": "def effective_sample_size(states,\n                          filter_threshold=0.,\n                          filter_beyond_lag=None,\n                          name=None):\n  \"\"\"Estimate a lower bound on effective sample size for each independent chain.\n\n  Roughly speaking, \"effective sample size\" (ESS) is the size of an iid sample\n  with the same variance as `state`.\n\n  More precisely, given a stationary sequence of possibly correlated random\n  variables `X_1, X_2,...,X_N`, each identically distributed ESS is the number\n  such that\n\n  ```Variance{ N**-1 * Sum{X_i} } = ESS**-1 * Variance{ X_1 }.```\n\n  If the sequence is uncorrelated, `ESS = N`.  In general, one should expect\n  `ESS <= N`, with more highly correlated sequences having smaller `ESS`.\n\n  Args:\n    states:  `Tensor` or list of `Tensor` objects.  Dimension zero should index\n      identically distributed states.\n    filter_threshold:  `Tensor` or list of `Tensor` objects.\n      Must broadcast with `state`.  The auto-correlation sequence is truncated\n      after the first appearance of a term less than `filter_threshold`.\n      Setting to `None` means we use no threshold filter.  Since `|R_k| <= 1`,\n      setting to any number less than `-1` has the same effect.\n    filter_beyond_lag:  `Tensor` or list of `Tensor` objects.  Must be\n      `int`-like and scalar valued.  The auto-correlation sequence is truncated\n      to this length.  Setting to `None` means we do not filter based on number\n      of lags.\n    name:  `String` name to prepend to created ops.\n\n  Returns:\n    ess:  `Tensor` or list of `Tensor` objects.  The effective sample size of\n      each component of `states`.  Shape will be `states.shape[1:]`.\n\n  Raises:\n    ValueError:  If `states` and `filter_threshold` or `states` and\n      `filter_beyond_lag` are both lists with different lengths.\n\n  #### Examples\n\n  We use ESS to estimate standard error.\n\n  ```\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  target = tfd.MultivariateNormalDiag(scale_diag=[1., 2.])\n\n  # Get 1000 states from one chain.\n  states = tfp.mcmc.sample_chain(\n      num_burnin_steps=200,\n      num_results=1000,\n      current_state=tf.constant([0., 0.]),\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=target.log_prob,\n        step_size=0.05,\n        num_leapfrog_steps=20))\n  states.shape\n  ==> (1000, 2)\n\n  ess = effective_sample_size(states)\n  ==> Shape (2,) Tensor\n\n  mean, variance = tf.nn.moments(states, axis=0)\n  standard_error = tf.sqrt(variance / ess)\n  ```\n\n  Some math shows that, with `R_k` the auto-correlation sequence,\n  `R_k := Covariance{X_1, X_{1+k}} / Variance{X_1}`, we have\n\n  ```ESS(N) =  N / [ 1 + 2 * ( (N - 1) / N * R_1 + ... + 1 / N * R_{N-1}  ) ]```\n\n  This function estimates the above by first estimating the auto-correlation.\n  Since `R_k` must be estimated using only `N - k` samples, it becomes\n  progressively noisier for larger `k`.  For this reason, the summation over\n  `R_k` should be truncated at some number `filter_beyond_lag < N`.  Since many\n  MCMC methods generate chains where `R_k > 0`, a reasonable criteria is to\n  truncate at the first index where the estimated auto-correlation becomes\n  negative.\n\n  The arguments `filter_beyond_lag`, `filter_threshold` are filters intended to\n  remove noisy tail terms from `R_k`.  They combine in an \"OR\" manner meaning\n  terms are removed if they were to be filtered under the `filter_beyond_lag` OR\n  `filter_threshold` criteria.\n  \"\"\"\n  states_was_list = _is_list_like(states)\n\n  # Convert all args to lists.\n  if not states_was_list:\n    states = [states]\n\n  filter_beyond_lag = _broadcast_maybelist_arg(states, filter_beyond_lag,\n                                               'filter_beyond_lag')\n  filter_threshold = _broadcast_maybelist_arg(states, filter_threshold,\n                                              'filter_threshold')\n\n  # Process items, one at a time.\n  with tf.compat.v1.name_scope(name, 'effective_sample_size'):\n    ess_list = [\n        _effective_sample_size_single_state(s, ml, mlt)\n        for (s, ml, mlt) in zip(states, filter_beyond_lag, filter_threshold)\n    ]\n\n  if states_was_list:\n    return ess_list\n  return ess_list[0]", "language": "python", "code": "def effective_sample_size(states,\n                          filter_threshold=0.,\n                          filter_beyond_lag=None,\n                          name=None):\n  \"\"\"Estimate a lower bound on effective sample size for each independent chain.\n\n  Roughly speaking, \"effective sample size\" (ESS) is the size of an iid sample\n  with the same variance as `state`.\n\n  More precisely, given a stationary sequence of possibly correlated random\n  variables `X_1, X_2,...,X_N`, each identically distributed ESS is the number\n  such that\n\n  ```Variance{ N**-1 * Sum{X_i} } = ESS**-1 * Variance{ X_1 }.```\n\n  If the sequence is uncorrelated, `ESS = N`.  In general, one should expect\n  `ESS <= N`, with more highly correlated sequences having smaller `ESS`.\n\n  Args:\n    states:  `Tensor` or list of `Tensor` objects.  Dimension zero should index\n      identically distributed states.\n    filter_threshold:  `Tensor` or list of `Tensor` objects.\n      Must broadcast with `state`.  The auto-correlation sequence is truncated\n      after the first appearance of a term less than `filter_threshold`.\n      Setting to `None` means we use no threshold filter.  Since `|R_k| <= 1`,\n      setting to any number less than `-1` has the same effect.\n    filter_beyond_lag:  `Tensor` or list of `Tensor` objects.  Must be\n      `int`-like and scalar valued.  The auto-correlation sequence is truncated\n      to this length.  Setting to `None` means we do not filter based on number\n      of lags.\n    name:  `String` name to prepend to created ops.\n\n  Returns:\n    ess:  `Tensor` or list of `Tensor` objects.  The effective sample size of\n      each component of `states`.  Shape will be `states.shape[1:]`.\n\n  Raises:\n    ValueError:  If `states` and `filter_threshold` or `states` and\n      `filter_beyond_lag` are both lists with different lengths.\n\n  #### Examples\n\n  We use ESS to estimate standard error.\n\n  ```\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  target = tfd.MultivariateNormalDiag(scale_diag=[1., 2.])\n\n  # Get 1000 states from one chain.\n  states = tfp.mcmc.sample_chain(\n      num_burnin_steps=200,\n      num_results=1000,\n      current_state=tf.constant([0., 0.]),\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=target.log_prob,\n        step_size=0.05,\n        num_leapfrog_steps=20))\n  states.shape\n  ==> (1000, 2)\n\n  ess = effective_sample_size(states)\n  ==> Shape (2,) Tensor\n\n  mean, variance = tf.nn.moments(states, axis=0)\n  standard_error = tf.sqrt(variance / ess)\n  ```\n\n  Some math shows that, with `R_k` the auto-correlation sequence,\n  `R_k := Covariance{X_1, X_{1+k}} / Variance{X_1}`, we have\n\n  ```ESS(N) =  N / [ 1 + 2 * ( (N - 1) / N * R_1 + ... + 1 / N * R_{N-1}  ) ]```\n\n  This function estimates the above by first estimating the auto-correlation.\n  Since `R_k` must be estimated using only `N - k` samples, it becomes\n  progressively noisier for larger `k`.  For this reason, the summation over\n  `R_k` should be truncated at some number `filter_beyond_lag < N`.  Since many\n  MCMC methods generate chains where `R_k > 0`, a reasonable criteria is to\n  truncate at the first index where the estimated auto-correlation becomes\n  negative.\n\n  The arguments `filter_beyond_lag`, `filter_threshold` are filters intended to\n  remove noisy tail terms from `R_k`.  They combine in an \"OR\" manner meaning\n  terms are removed if they were to be filtered under the `filter_beyond_lag` OR\n  `filter_threshold` criteria.\n  \"\"\"\n  states_was_list = _is_list_like(states)\n\n  # Convert all args to lists.\n  if not states_was_list:\n    states = [states]\n\n  filter_beyond_lag = _broadcast_maybelist_arg(states, filter_beyond_lag,\n                                               'filter_beyond_lag')\n  filter_threshold = _broadcast_maybelist_arg(states, filter_threshold,\n                                              'filter_threshold')\n\n  # Process items, one at a time.\n  with tf.compat.v1.name_scope(name, 'effective_sample_size'):\n    ess_list = [\n        _effective_sample_size_single_state(s, ml, mlt)\n        for (s, ml, mlt) in zip(states, filter_beyond_lag, filter_threshold)\n    ]\n\n  if states_was_list:\n    return ess_list\n  return ess_list[0]", "code_tokens": ["def", "effective_sample_size", "(", "states", ",", "filter_threshold", "=", "0.", ",", "filter_beyond_lag", "=", "None", ",", "name", "=", "None", ")", ":", "states_was_list", "=", "_is_list_like", "(", "states", ")", "# Convert all args to lists.", "if", "not", "states_was_list", ":", "states", "=", "[", "states", "]", "filter_beyond_lag", "=", "_broadcast_maybelist_arg", "(", "states", ",", "filter_beyond_lag", ",", "'filter_beyond_lag'", ")", "filter_threshold", "=", "_broadcast_maybelist_arg", "(", "states", ",", "filter_threshold", ",", "'filter_threshold'", ")", "# Process items, one at a time.", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'effective_sample_size'", ")", ":", "ess_list", "=", "[", "_effective_sample_size_single_state", "(", "s", ",", "ml", ",", "mlt", ")", "for", "(", "s", ",", "ml", ",", "mlt", ")", "in", "zip", "(", "states", ",", "filter_beyond_lag", ",", "filter_threshold", ")", "]", "if", "states_was_list", ":", "return", "ess_list", "return", "ess_list", "[", "0", "]"], "docstring": "Estimate a lower bound on effective sample size for each independent chain.\n\n  Roughly speaking, \"effective sample size\" (ESS) is the size of an iid sample\n  with the same variance as `state`.\n\n  More precisely, given a stationary sequence of possibly correlated random\n  variables `X_1, X_2,...,X_N`, each identically distributed ESS is the number\n  such that\n\n  ```Variance{ N**-1 * Sum{X_i} } = ESS**-1 * Variance{ X_1 }.```\n\n  If the sequence is uncorrelated, `ESS = N`.  In general, one should expect\n  `ESS <= N`, with more highly correlated sequences having smaller `ESS`.\n\n  Args:\n    states:  `Tensor` or list of `Tensor` objects.  Dimension zero should index\n      identically distributed states.\n    filter_threshold:  `Tensor` or list of `Tensor` objects.\n      Must broadcast with `state`.  The auto-correlation sequence is truncated\n      after the first appearance of a term less than `filter_threshold`.\n      Setting to `None` means we use no threshold filter.  Since `|R_k| <= 1`,\n      setting to any number less than `-1` has the same effect.\n    filter_beyond_lag:  `Tensor` or list of `Tensor` objects.  Must be\n      `int`-like and scalar valued.  The auto-correlation sequence is truncated\n      to this length.  Setting to `None` means we do not filter based on number\n      of lags.\n    name:  `String` name to prepend to created ops.\n\n  Returns:\n    ess:  `Tensor` or list of `Tensor` objects.  The effective sample size of\n      each component of `states`.  Shape will be `states.shape[1:]`.\n\n  Raises:\n    ValueError:  If `states` and `filter_threshold` or `states` and\n      `filter_beyond_lag` are both lists with different lengths.\n\n  #### Examples\n\n  We use ESS to estimate standard error.\n\n  ```\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  target = tfd.MultivariateNormalDiag(scale_diag=[1., 2.])\n\n  # Get 1000 states from one chain.\n  states = tfp.mcmc.sample_chain(\n      num_burnin_steps=200,\n      num_results=1000,\n      current_state=tf.constant([0., 0.]),\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=target.log_prob,\n        step_size=0.05,\n        num_leapfrog_steps=20))\n  states.shape\n  ==> (1000, 2)\n\n  ess = effective_sample_size(states)\n  ==> Shape (2,) Tensor\n\n  mean, variance = tf.nn.moments(states, axis=0)\n  standard_error = tf.sqrt(variance / ess)\n  ```\n\n  Some math shows that, with `R_k` the auto-correlation sequence,\n  `R_k := Covariance{X_1, X_{1+k}} / Variance{X_1}`, we have\n\n  ```ESS(N) =  N / [ 1 + 2 * ( (N - 1) / N * R_1 + ... + 1 / N * R_{N-1}  ) ]```\n\n  This function estimates the above by first estimating the auto-correlation.\n  Since `R_k` must be estimated using only `N - k` samples, it becomes\n  progressively noisier for larger `k`.  For this reason, the summation over\n  `R_k` should be truncated at some number `filter_beyond_lag < N`.  Since many\n  MCMC methods generate chains where `R_k > 0`, a reasonable criteria is to\n  truncate at the first index where the estimated auto-correlation becomes\n  negative.\n\n  The arguments `filter_beyond_lag`, `filter_threshold` are filters intended to\n  remove noisy tail terms from `R_k`.  They combine in an \"OR\" manner meaning\n  terms are removed if they were to be filtered under the `filter_beyond_lag` OR\n  `filter_threshold` criteria.", "docstring_tokens": ["Estimate", "a", "lower", "bound", "on", "effective", "sample", "size", "for", "each", "independent", "chain", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/diagnostic.py#L35-L143", "partition": "test", "index": 781, "time": "2018-03-06 11:55:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/diagnostic.py", "func_name": "_axis_size", "original_string": "def _axis_size(x, axis=None):\n  \"\"\"Get number of elements of `x` in `axis`, as type `x.dtype`.\"\"\"\n  if axis is None:\n    return tf.cast(tf.size(input=x), x.dtype)\n  return tf.cast(\n      tf.reduce_prod(input_tensor=tf.gather(tf.shape(input=x), axis)), x.dtype)", "language": "python", "code": "def _axis_size(x, axis=None):\n  \"\"\"Get number of elements of `x` in `axis`, as type `x.dtype`.\"\"\"\n  if axis is None:\n    return tf.cast(tf.size(input=x), x.dtype)\n  return tf.cast(\n      tf.reduce_prod(input_tensor=tf.gather(tf.shape(input=x), axis)), x.dtype)", "code_tokens": ["def", "_axis_size", "(", "x", ",", "axis", "=", "None", ")", ":", "if", "axis", "is", "None", ":", "return", "tf", ".", "cast", "(", "tf", ".", "size", "(", "input", "=", "x", ")", ",", "x", ".", "dtype", ")", "return", "tf", ".", "cast", "(", "tf", ".", "reduce_prod", "(", "input_tensor", "=", "tf", ".", "gather", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "axis", ")", ")", ",", "x", ".", "dtype", ")"], "docstring": "Get number of elements of `x` in `axis`, as type `x.dtype`.", "docstring_tokens": ["Get", "number", "of", "elements", "of", "x", "in", "axis", "as", "type", "x", ".", "dtype", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/diagnostic.py#L388-L393", "partition": "test", "index": 784, "time": "2018-03-06 11:55:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/sample_halton_sequence.py", "func_name": "_primes_less_than", "original_string": "def _primes_less_than(n):\n  # Based on\n  # https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188\n  \"\"\"Returns sorted array of primes such that `2 <= prime < n`.\"\"\"\n  small_primes = np.array((2, 3, 5))\n  if n <= 6:\n    return small_primes[small_primes < n]\n  sieve = np.ones(n // 3 + (n % 6 == 2), dtype=np.bool)\n  sieve[0] = False\n  m = int(n ** 0.5) // 3 + 1\n  for i in range(m):\n    if not sieve[i]:\n      continue\n    k = 3 * i + 1 | 1\n    sieve[k ** 2 // 3::2 * k] = False\n    sieve[(k ** 2 + 4 * k - 2 * k * (i & 1)) // 3::2 * k] = False\n  return np.r_[2, 3, 3 * np.nonzero(sieve)[0] + 1 | 1]", "language": "python", "code": "def _primes_less_than(n):\n  # Based on\n  # https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188\n  \"\"\"Returns sorted array of primes such that `2 <= prime < n`.\"\"\"\n  small_primes = np.array((2, 3, 5))\n  if n <= 6:\n    return small_primes[small_primes < n]\n  sieve = np.ones(n // 3 + (n % 6 == 2), dtype=np.bool)\n  sieve[0] = False\n  m = int(n ** 0.5) // 3 + 1\n  for i in range(m):\n    if not sieve[i]:\n      continue\n    k = 3 * i + 1 | 1\n    sieve[k ** 2 // 3::2 * k] = False\n    sieve[(k ** 2 + 4 * k - 2 * k * (i & 1)) // 3::2 * k] = False\n  return np.r_[2, 3, 3 * np.nonzero(sieve)[0] + 1 | 1]", "code_tokens": ["def", "_primes_less_than", "(", "n", ")", ":", "# Based on", "# https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188", "small_primes", "=", "np", ".", "array", "(", "(", "2", ",", "3", ",", "5", ")", ")", "if", "n", "<=", "6", ":", "return", "small_primes", "[", "small_primes", "<", "n", "]", "sieve", "=", "np", ".", "ones", "(", "n", "//", "3", "+", "(", "n", "%", "6", "==", "2", ")", ",", "dtype", "=", "np", ".", "bool", ")", "sieve", "[", "0", "]", "=", "False", "m", "=", "int", "(", "n", "**", "0.5", ")", "//", "3", "+", "1", "for", "i", "in", "range", "(", "m", ")", ":", "if", "not", "sieve", "[", "i", "]", ":", "continue", "k", "=", "3", "*", "i", "+", "1", "|", "1", "sieve", "[", "k", "**", "2", "//", "3", ":", ":", "2", "*", "k", "]", "=", "False", "sieve", "[", "(", "k", "**", "2", "+", "4", "*", "k", "-", "2", "*", "k", "*", "(", "i", "&", "1", ")", ")", "//", "3", ":", ":", "2", "*", "k", "]", "=", "False", "return", "np", ".", "r_", "[", "2", ",", "3", ",", "3", "*", "np", ".", "nonzero", "(", "sieve", ")", "[", "0", "]", "+", "1", "|", "1", "]"], "docstring": "Returns sorted array of primes such that `2 <= prime < n`.", "docstring_tokens": ["Returns", "sorted", "array", "of", "primes", "such", "that", "2", "<", "=", "prime", "<", "n", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/sample_halton_sequence.py#L368-L384", "partition": "test", "index": 669, "time": "2018-03-07 09:58:22"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/sample_halton_sequence.py", "func_name": "_base_expansion_size", "original_string": "def _base_expansion_size(num, bases):\n  \"\"\"Computes the number of terms in the place value expansion.\n\n  Let num = a0 + a1 b + a2 b^2 + ... ak b^k be the place value expansion of\n  `num` in base b (ak <> 0). This function computes and returns `k+1` for each\n  base `b` specified in `bases`.\n\n  This can be inferred from the base `b` logarithm of `num` as follows:\n    $$k = Floor(log_b (num)) + 1  = Floor( log(num) / log(b)) + 1$$\n\n  Args:\n    num: Scalar `Tensor` of dtype either `float32` or `float64`. The number to\n      compute the base expansion size of.\n    bases: `Tensor` of the same dtype as num. The bases to compute the size\n      against.\n\n  Returns:\n    Tensor of same dtype and shape as `bases` containing the size of num when\n    written in that base.\n  \"\"\"\n  return tf.floor(tf.math.log(num) / tf.math.log(bases)) + 1", "language": "python", "code": "def _base_expansion_size(num, bases):\n  \"\"\"Computes the number of terms in the place value expansion.\n\n  Let num = a0 + a1 b + a2 b^2 + ... ak b^k be the place value expansion of\n  `num` in base b (ak <> 0). This function computes and returns `k+1` for each\n  base `b` specified in `bases`.\n\n  This can be inferred from the base `b` logarithm of `num` as follows:\n    $$k = Floor(log_b (num)) + 1  = Floor( log(num) / log(b)) + 1$$\n\n  Args:\n    num: Scalar `Tensor` of dtype either `float32` or `float64`. The number to\n      compute the base expansion size of.\n    bases: `Tensor` of the same dtype as num. The bases to compute the size\n      against.\n\n  Returns:\n    Tensor of same dtype and shape as `bases` containing the size of num when\n    written in that base.\n  \"\"\"\n  return tf.floor(tf.math.log(num) / tf.math.log(bases)) + 1", "code_tokens": ["def", "_base_expansion_size", "(", "num", ",", "bases", ")", ":", "return", "tf", ".", "floor", "(", "tf", ".", "math", ".", "log", "(", "num", ")", "/", "tf", ".", "math", ".", "log", "(", "bases", ")", ")", "+", "1"], "docstring": "Computes the number of terms in the place value expansion.\n\n  Let num = a0 + a1 b + a2 b^2 + ... ak b^k be the place value expansion of\n  `num` in base b (ak <> 0). This function computes and returns `k+1` for each\n  base `b` specified in `bases`.\n\n  This can be inferred from the base `b` logarithm of `num` as follows:\n    $$k = Floor(log_b (num)) + 1  = Floor( log(num) / log(b)) + 1$$\n\n  Args:\n    num: Scalar `Tensor` of dtype either `float32` or `float64`. The number to\n      compute the base expansion size of.\n    bases: `Tensor` of the same dtype as num. The bases to compute the size\n      against.\n\n  Returns:\n    Tensor of same dtype and shape as `bases` containing the size of num when\n    written in that base.", "docstring_tokens": ["Computes", "the", "number", "of", "terms", "in", "the", "place", "value", "expansion", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/sample_halton_sequence.py#L345-L365", "partition": "test", "index": 668, "time": "2018-03-07 09:58:22"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/sample_halton_sequence.py", "func_name": "_get_indices", "original_string": "def _get_indices(num_results, sequence_indices, dtype, name=None):\n  \"\"\"Generates starting points for the Halton sequence procedure.\n\n  The k'th element of the sequence is generated starting from a positive integer\n  which must be distinct for each `k`. It is conventional to choose the starting\n  point as `k` itself (or `k+1` if k is zero based). This function generates\n  the starting integers for the required elements and reshapes the result for\n  later use.\n\n  Args:\n    num_results: Positive scalar `Tensor` of dtype int32. The number of samples\n      to generate. If this parameter is supplied, then `sequence_indices`\n      should be None.\n    sequence_indices: `Tensor` of dtype int32 and rank 1. The entries\n      index into the Halton sequence starting with 0 and hence, must be whole\n      numbers. For example, sequence_indices=[0, 5, 6] will produce the first,\n      sixth and seventh elements of the sequence. If this parameter is not None\n      then `n` must be None.\n    dtype: The dtype of the sample. One of `float32` or `float64`.\n      Default is `float32`.\n    name: Python `str` name which describes ops created by this function.\n\n  Returns:\n    indices: `Tensor` of dtype `dtype` and shape = `[n, 1, 1]`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, '_get_indices',\n                               [num_results, sequence_indices]):\n    if sequence_indices is None:\n      num_results = tf.cast(num_results, dtype=dtype)\n      sequence_indices = tf.range(num_results, dtype=dtype)\n    else:\n      sequence_indices = tf.cast(sequence_indices, dtype)\n\n    # Shift the indices so they are 1 based.\n    indices = sequence_indices + 1\n\n    # Reshape to make space for the event dimension and the place value\n    # coefficients.\n    return tf.reshape(indices, [-1, 1, 1])", "language": "python", "code": "def _get_indices(num_results, sequence_indices, dtype, name=None):\n  \"\"\"Generates starting points for the Halton sequence procedure.\n\n  The k'th element of the sequence is generated starting from a positive integer\n  which must be distinct for each `k`. It is conventional to choose the starting\n  point as `k` itself (or `k+1` if k is zero based). This function generates\n  the starting integers for the required elements and reshapes the result for\n  later use.\n\n  Args:\n    num_results: Positive scalar `Tensor` of dtype int32. The number of samples\n      to generate. If this parameter is supplied, then `sequence_indices`\n      should be None.\n    sequence_indices: `Tensor` of dtype int32 and rank 1. The entries\n      index into the Halton sequence starting with 0 and hence, must be whole\n      numbers. For example, sequence_indices=[0, 5, 6] will produce the first,\n      sixth and seventh elements of the sequence. If this parameter is not None\n      then `n` must be None.\n    dtype: The dtype of the sample. One of `float32` or `float64`.\n      Default is `float32`.\n    name: Python `str` name which describes ops created by this function.\n\n  Returns:\n    indices: `Tensor` of dtype `dtype` and shape = `[n, 1, 1]`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, '_get_indices',\n                               [num_results, sequence_indices]):\n    if sequence_indices is None:\n      num_results = tf.cast(num_results, dtype=dtype)\n      sequence_indices = tf.range(num_results, dtype=dtype)\n    else:\n      sequence_indices = tf.cast(sequence_indices, dtype)\n\n    # Shift the indices so they are 1 based.\n    indices = sequence_indices + 1\n\n    # Reshape to make space for the event dimension and the place value\n    # coefficients.\n    return tf.reshape(indices, [-1, 1, 1])", "code_tokens": ["def", "_get_indices", "(", "num_results", ",", "sequence_indices", ",", "dtype", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'_get_indices'", ",", "[", "num_results", ",", "sequence_indices", "]", ")", ":", "if", "sequence_indices", "is", "None", ":", "num_results", "=", "tf", ".", "cast", "(", "num_results", ",", "dtype", "=", "dtype", ")", "sequence_indices", "=", "tf", ".", "range", "(", "num_results", ",", "dtype", "=", "dtype", ")", "else", ":", "sequence_indices", "=", "tf", ".", "cast", "(", "sequence_indices", ",", "dtype", ")", "# Shift the indices so they are 1 based.", "indices", "=", "sequence_indices", "+", "1", "# Reshape to make space for the event dimension and the place value", "# coefficients.", "return", "tf", ".", "reshape", "(", "indices", ",", "[", "-", "1", ",", "1", ",", "1", "]", ")"], "docstring": "Generates starting points for the Halton sequence procedure.\n\n  The k'th element of the sequence is generated starting from a positive integer\n  which must be distinct for each `k`. It is conventional to choose the starting\n  point as `k` itself (or `k+1` if k is zero based). This function generates\n  the starting integers for the required elements and reshapes the result for\n  later use.\n\n  Args:\n    num_results: Positive scalar `Tensor` of dtype int32. The number of samples\n      to generate. If this parameter is supplied, then `sequence_indices`\n      should be None.\n    sequence_indices: `Tensor` of dtype int32 and rank 1. The entries\n      index into the Halton sequence starting with 0 and hence, must be whole\n      numbers. For example, sequence_indices=[0, 5, 6] will produce the first,\n      sixth and seventh elements of the sequence. If this parameter is not None\n      then `n` must be None.\n    dtype: The dtype of the sample. One of `float32` or `float64`.\n      Default is `float32`.\n    name: Python `str` name which describes ops created by this function.\n\n  Returns:\n    indices: `Tensor` of dtype `dtype` and shape = `[n, 1, 1]`.", "docstring_tokens": ["Generates", "starting", "points", "for", "the", "Halton", "sequence", "procedure", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/sample_halton_sequence.py#L304-L342", "partition": "test", "index": 667, "time": "2018-03-07 09:58:22"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/sample_halton_sequence.py", "func_name": "_get_permutations", "original_string": "def _get_permutations(num_results, dims, seed=None):\n  \"\"\"Uniform iid sample from the space of permutations.\n\n  Draws a sample of size `num_results` from the group of permutations of degrees\n  specified by the `dims` tensor. These are packed together into one tensor\n  such that each row is one sample from each of the dimensions in `dims`. For\n  example, if dims = [2,3] and num_results = 2, the result is a tensor of shape\n  [2, 2 + 3] and the first row of the result might look like:\n  [1, 0, 2, 0, 1]. The first two elements are a permutation over 2 elements\n  while the next three are a permutation over 3 elements.\n\n  Args:\n    num_results: A positive scalar `Tensor` of integral type. The number of\n      draws from the discrete uniform distribution over the permutation groups.\n    dims: A 1D `Tensor` of the same dtype as `num_results`. The degree of the\n      permutation groups from which to sample.\n    seed: (Optional) Python integer to seed the random number generator.\n\n  Returns:\n    permutations: A `Tensor` of shape `[num_results, sum(dims)]` and the same\n    dtype as `dims`.\n  \"\"\"\n  sample_range = tf.range(num_results)\n  stream = distributions.SeedStream(seed, salt='MCMCSampleHaltonSequence3')\n  def generate_one(d):\n    seed = stream()\n    fn = lambda _: tf.random.shuffle(tf.range(d), seed=seed)\n    return tf.map_fn(\n        fn,\n        sample_range,\n        parallel_iterations=1 if seed is not None else 10)\n  return tf.concat([generate_one(d) for d in tf.unstack(dims)],\n                   axis=-1)", "language": "python", "code": "def _get_permutations(num_results, dims, seed=None):\n  \"\"\"Uniform iid sample from the space of permutations.\n\n  Draws a sample of size `num_results` from the group of permutations of degrees\n  specified by the `dims` tensor. These are packed together into one tensor\n  such that each row is one sample from each of the dimensions in `dims`. For\n  example, if dims = [2,3] and num_results = 2, the result is a tensor of shape\n  [2, 2 + 3] and the first row of the result might look like:\n  [1, 0, 2, 0, 1]. The first two elements are a permutation over 2 elements\n  while the next three are a permutation over 3 elements.\n\n  Args:\n    num_results: A positive scalar `Tensor` of integral type. The number of\n      draws from the discrete uniform distribution over the permutation groups.\n    dims: A 1D `Tensor` of the same dtype as `num_results`. The degree of the\n      permutation groups from which to sample.\n    seed: (Optional) Python integer to seed the random number generator.\n\n  Returns:\n    permutations: A `Tensor` of shape `[num_results, sum(dims)]` and the same\n    dtype as `dims`.\n  \"\"\"\n  sample_range = tf.range(num_results)\n  stream = distributions.SeedStream(seed, salt='MCMCSampleHaltonSequence3')\n  def generate_one(d):\n    seed = stream()\n    fn = lambda _: tf.random.shuffle(tf.range(d), seed=seed)\n    return tf.map_fn(\n        fn,\n        sample_range,\n        parallel_iterations=1 if seed is not None else 10)\n  return tf.concat([generate_one(d) for d in tf.unstack(dims)],\n                   axis=-1)", "code_tokens": ["def", "_get_permutations", "(", "num_results", ",", "dims", ",", "seed", "=", "None", ")", ":", "sample_range", "=", "tf", ".", "range", "(", "num_results", ")", "stream", "=", "distributions", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'MCMCSampleHaltonSequence3'", ")", "def", "generate_one", "(", "d", ")", ":", "seed", "=", "stream", "(", ")", "fn", "=", "lambda", "_", ":", "tf", ".", "random", ".", "shuffle", "(", "tf", ".", "range", "(", "d", ")", ",", "seed", "=", "seed", ")", "return", "tf", ".", "map_fn", "(", "fn", ",", "sample_range", ",", "parallel_iterations", "=", "1", "if", "seed", "is", "not", "None", "else", "10", ")", "return", "tf", ".", "concat", "(", "[", "generate_one", "(", "d", ")", "for", "d", "in", "tf", ".", "unstack", "(", "dims", ")", "]", ",", "axis", "=", "-", "1", ")"], "docstring": "Uniform iid sample from the space of permutations.\n\n  Draws a sample of size `num_results` from the group of permutations of degrees\n  specified by the `dims` tensor. These are packed together into one tensor\n  such that each row is one sample from each of the dimensions in `dims`. For\n  example, if dims = [2,3] and num_results = 2, the result is a tensor of shape\n  [2, 2 + 3] and the first row of the result might look like:\n  [1, 0, 2, 0, 1]. The first two elements are a permutation over 2 elements\n  while the next three are a permutation over 3 elements.\n\n  Args:\n    num_results: A positive scalar `Tensor` of integral type. The number of\n      draws from the discrete uniform distribution over the permutation groups.\n    dims: A 1D `Tensor` of the same dtype as `num_results`. The degree of the\n      permutation groups from which to sample.\n    seed: (Optional) Python integer to seed the random number generator.\n\n  Returns:\n    permutations: A `Tensor` of shape `[num_results, sum(dims)]` and the same\n    dtype as `dims`.", "docstring_tokens": ["Uniform", "iid", "sample", "from", "the", "space", "of", "permutations", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/sample_halton_sequence.py#L269-L301", "partition": "test", "index": 666, "time": "2018-03-07 09:58:22"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/sample_halton_sequence.py", "func_name": "sample_halton_sequence", "original_string": "def sample_halton_sequence(dim,\n                           num_results=None,\n                           sequence_indices=None,\n                           dtype=tf.float32,\n                           randomized=True,\n                           seed=None,\n                           name=None):\n  r\"\"\"Returns a sample from the `dim` dimensional Halton sequence.\n\n  Warning: The sequence elements take values only between 0 and 1. Care must be\n  taken to appropriately transform the domain of a function if it differs from\n  the unit cube before evaluating integrals using Halton samples. It is also\n  important to remember that quasi-random numbers without randomization are not\n  a replacement for pseudo-random numbers in every context. Quasi random numbers\n  are completely deterministic and typically have significant negative\n  autocorrelation unless randomization is used.\n\n  Computes the members of the low discrepancy Halton sequence in dimension\n  `dim`. The `dim`-dimensional sequence takes values in the unit hypercube in\n  `dim` dimensions. Currently, only dimensions up to 1000 are supported. The\n  prime base for the k-th axes is the k-th prime starting from 2. For example,\n  if `dim` = 3, then the bases will be [2, 3, 5] respectively and the first\n  element of the non-randomized sequence will be: [0.5, 0.333, 0.2]. For a more\n  complete description of the Halton sequences see\n  [here](https://en.wikipedia.org/wiki/Halton_sequence). For low discrepancy\n  sequences and their applications see\n  [here](https://en.wikipedia.org/wiki/Low-discrepancy_sequence).\n\n  If `randomized` is true, this function produces a scrambled version of the\n  Halton sequence introduced by [Owen (2017)][1]. For the advantages of\n  randomization of low discrepancy sequences see [here](\n  https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method#Randomization_of_quasi-Monte_Carlo).\n\n  The number of samples produced is controlled by the `num_results` and\n  `sequence_indices` parameters. The user must supply either `num_results` or\n  `sequence_indices` but not both.\n  The former is the number of samples to produce starting from the first\n  element. If `sequence_indices` is given instead, the specified elements of\n  the sequence are generated. For example, sequence_indices=tf.range(10) is\n  equivalent to specifying n=10.\n\n  #### Examples\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  # Produce the first 1000 members of the Halton sequence in 3 dimensions.\n  num_results = 1000\n  dim = 3\n  sample = tfp.mcmc.sample_halton_sequence(\n    dim,\n    num_results=num_results,\n    seed=127)\n\n  # Evaluate the integral of x_1 * x_2^2 * x_3^3  over the three dimensional\n  # hypercube.\n  powers = tf.range(1.0, limit=dim + 1)\n  integral = tf.reduce_mean(tf.reduce_prod(sample ** powers, axis=-1))\n  true_value = 1.0 / tf.reduce_prod(powers + 1.0)\n  with tf.Session() as session:\n    values = session.run((integral, true_value))\n\n  # Produces a relative absolute error of 1.7%.\n  print (\"Estimated: %f, True Value: %f\" % values)\n\n  # Now skip the first 1000 samples and recompute the integral with the next\n  # thousand samples. The sequence_indices argument can be used to do this.\n\n\n  sequence_indices = tf.range(start=1000, limit=1000 + num_results,\n                              dtype=tf.int32)\n  sample_leaped = tfp.mcmc.sample_halton_sequence(\n      dim,\n      sequence_indices=sequence_indices,\n      seed=111217)\n\n  integral_leaped = tf.reduce_mean(tf.reduce_prod(sample_leaped ** powers,\n                                                  axis=-1))\n  with tf.Session() as session:\n    values = session.run((integral_leaped, true_value))\n  # Now produces a relative absolute error of 0.05%.\n  print (\"Leaped Estimated: %f, True Value: %f\" % values)\n  ```\n\n  Args:\n    dim: Positive Python `int` representing each sample's `event_size.` Must\n      not be greater than 1000.\n    num_results: (Optional) Positive scalar `Tensor` of dtype int32. The number\n      of samples to generate. Either this parameter or sequence_indices must\n      be specified but not both. If this parameter is None, then the behaviour\n      is determined by the `sequence_indices`.\n      Default value: `None`.\n    sequence_indices: (Optional) `Tensor` of dtype int32 and rank 1. The\n      elements of the sequence to compute specified by their position in the\n      sequence. The entries index into the Halton sequence starting with 0 and\n      hence, must be whole numbers. For example, sequence_indices=[0, 5, 6] will\n      produce the first, sixth and seventh elements of the sequence. If this\n      parameter is None, then the `num_results` parameter must be specified\n      which gives the number of desired samples starting from the first sample.\n      Default value: `None`.\n    dtype: (Optional) The dtype of the sample. One of: `float16`, `float32` or\n      `float64`.\n      Default value: `tf.float32`.\n    randomized: (Optional) bool indicating whether to produce a randomized\n      Halton sequence. If True, applies the randomization described in\n      [Owen (2017)][1].\n      Default value: `True`.\n    seed: (Optional) Python integer to seed the random number generator. Only\n      used if `randomized` is True. If not supplied and `randomized` is True,\n      no seed is set.\n      Default value: `None`.\n    name:  (Optional) Python `str` describing ops managed by this function. If\n      not supplied the name of this function is used.\n      Default value: \"sample_halton_sequence\".\n\n  Returns:\n    halton_elements: Elements of the Halton sequence. `Tensor` of supplied dtype\n      and `shape` `[num_results, dim]` if `num_results` was specified or shape\n      `[s, dim]` where s is the size of `sequence_indices` if `sequence_indices`\n      were specified.\n\n  Raises:\n    ValueError: if both `sequence_indices` and `num_results` were specified or\n      if dimension `dim` is less than 1 or greater than 1000.\n\n  #### References\n\n  [1]: Art B. Owen. A randomized Halton algorithm in R. _arXiv preprint\n       arXiv:1706.02808_, 2017. https://arxiv.org/abs/1706.02808\n  \"\"\"\n  if dim < 1 or dim > _MAX_DIMENSION:\n    raise ValueError(\n        'Dimension must be between 1 and {}. Supplied {}'.format(_MAX_DIMENSION,\n                                                                 dim))\n  if (num_results is None) == (sequence_indices is None):\n    raise ValueError('Either `num_results` or `sequence_indices` must be'\n                     ' specified but not both.')\n\n  if not dtype.is_floating:\n    raise ValueError('dtype must be of `float`-type')\n\n  with tf.compat.v1.name_scope(\n      name, 'sample', values=[num_results, sequence_indices]):\n    # Here and in the following, the shape layout is as follows:\n    # [sample dimension, event dimension, coefficient dimension].\n    # The coefficient dimension is an intermediate axes which will hold the\n    # weights of the starting integer when expressed in the (prime) base for\n    # an event dimension.\n    if num_results is not None:\n      num_results = tf.convert_to_tensor(value=num_results)\n    if sequence_indices is not None:\n      sequence_indices = tf.convert_to_tensor(value=sequence_indices)\n    indices = _get_indices(num_results, sequence_indices, dtype)\n    radixes = tf.constant(_PRIMES[0:dim], dtype=dtype, shape=[dim, 1])\n\n    max_sizes_by_axes = _base_expansion_size(\n        tf.reduce_max(input_tensor=indices), radixes)\n\n    max_size = tf.reduce_max(input_tensor=max_sizes_by_axes)\n\n    # The powers of the radixes that we will need. Note that there is a bit\n    # of an excess here. Suppose we need the place value coefficients of 7\n    # in base 2 and 3. For 2, we will have 3 digits but we only need 2 digits\n    # for base 3. However, we can only create rectangular tensors so we\n    # store both expansions in a [2, 3] tensor. This leads to the problem that\n    # we might end up attempting to raise large numbers to large powers. For\n    # example, base 2 expansion of 1024 has 10 digits. If we were in 10\n    # dimensions, then the 10th prime (29) we will end up computing 29^10 even\n    # though we don't need it. We avoid this by setting the exponents for each\n    # axes to 0 beyond the maximum value needed for that dimension.\n    exponents_by_axes = tf.tile([tf.range(max_size)], [dim, 1])\n\n    # The mask is true for those coefficients that are irrelevant.\n    weight_mask = exponents_by_axes >= max_sizes_by_axes\n    capped_exponents = tf.where(\n        weight_mask,\n        tf.zeros_like(exponents_by_axes),\n        exponents_by_axes)\n    weights = radixes ** capped_exponents\n    # The following computes the base b expansion of the indices. Suppose,\n    # x = a0 + a1*b + a2*b^2 + ... Then, performing a floor div of x with\n    # the vector (1, b, b^2, b^3, ...) will produce\n    # (a0 + s1 * b, a1 + s2 * b, ...) where s_i are coefficients we don't care\n    # about. Noting that all a_i < b by definition of place value expansion,\n    # we see that taking the elements mod b of the above vector produces the\n    # place value expansion coefficients.\n    coeffs = tf.math.floordiv(indices, weights)\n    coeffs *= 1. - tf.cast(weight_mask, dtype)\n    coeffs %= radixes\n    if not randomized:\n      coeffs /= radixes\n      return tf.reduce_sum(input_tensor=coeffs / weights, axis=-1)\n    stream = distributions.SeedStream(seed, salt='MCMCSampleHaltonSequence')\n    coeffs = _randomize(coeffs, radixes, seed=stream())\n    # Remove the contribution from randomizing the trailing zero for the\n    # axes where max_size_by_axes < max_size. This will be accounted\n    # for separately below (using zero_correction).\n    coeffs *= 1. - tf.cast(weight_mask, dtype)\n    coeffs /= radixes\n    base_values = tf.reduce_sum(input_tensor=coeffs / weights, axis=-1)\n\n    # The randomization used in Owen (2017) does not leave 0 invariant. While\n    # we have accounted for the randomization of the first `max_size_by_axes`\n    # coefficients, we still need to correct for the trailing zeros. Luckily,\n    # this is equivalent to adding a uniform random value scaled so the first\n    # `max_size_by_axes` coefficients are zero. The following statements perform\n    # this correction.\n    zero_correction = tf.random.uniform([dim, 1], seed=stream(), dtype=dtype)\n    zero_correction /= radixes ** max_sizes_by_axes\n    return base_values + tf.reshape(zero_correction, [-1])", "language": "python", "code": "def sample_halton_sequence(dim,\n                           num_results=None,\n                           sequence_indices=None,\n                           dtype=tf.float32,\n                           randomized=True,\n                           seed=None,\n                           name=None):\n  r\"\"\"Returns a sample from the `dim` dimensional Halton sequence.\n\n  Warning: The sequence elements take values only between 0 and 1. Care must be\n  taken to appropriately transform the domain of a function if it differs from\n  the unit cube before evaluating integrals using Halton samples. It is also\n  important to remember that quasi-random numbers without randomization are not\n  a replacement for pseudo-random numbers in every context. Quasi random numbers\n  are completely deterministic and typically have significant negative\n  autocorrelation unless randomization is used.\n\n  Computes the members of the low discrepancy Halton sequence in dimension\n  `dim`. The `dim`-dimensional sequence takes values in the unit hypercube in\n  `dim` dimensions. Currently, only dimensions up to 1000 are supported. The\n  prime base for the k-th axes is the k-th prime starting from 2. For example,\n  if `dim` = 3, then the bases will be [2, 3, 5] respectively and the first\n  element of the non-randomized sequence will be: [0.5, 0.333, 0.2]. For a more\n  complete description of the Halton sequences see\n  [here](https://en.wikipedia.org/wiki/Halton_sequence). For low discrepancy\n  sequences and their applications see\n  [here](https://en.wikipedia.org/wiki/Low-discrepancy_sequence).\n\n  If `randomized` is true, this function produces a scrambled version of the\n  Halton sequence introduced by [Owen (2017)][1]. For the advantages of\n  randomization of low discrepancy sequences see [here](\n  https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method#Randomization_of_quasi-Monte_Carlo).\n\n  The number of samples produced is controlled by the `num_results` and\n  `sequence_indices` parameters. The user must supply either `num_results` or\n  `sequence_indices` but not both.\n  The former is the number of samples to produce starting from the first\n  element. If `sequence_indices` is given instead, the specified elements of\n  the sequence are generated. For example, sequence_indices=tf.range(10) is\n  equivalent to specifying n=10.\n\n  #### Examples\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  # Produce the first 1000 members of the Halton sequence in 3 dimensions.\n  num_results = 1000\n  dim = 3\n  sample = tfp.mcmc.sample_halton_sequence(\n    dim,\n    num_results=num_results,\n    seed=127)\n\n  # Evaluate the integral of x_1 * x_2^2 * x_3^3  over the three dimensional\n  # hypercube.\n  powers = tf.range(1.0, limit=dim + 1)\n  integral = tf.reduce_mean(tf.reduce_prod(sample ** powers, axis=-1))\n  true_value = 1.0 / tf.reduce_prod(powers + 1.0)\n  with tf.Session() as session:\n    values = session.run((integral, true_value))\n\n  # Produces a relative absolute error of 1.7%.\n  print (\"Estimated: %f, True Value: %f\" % values)\n\n  # Now skip the first 1000 samples and recompute the integral with the next\n  # thousand samples. The sequence_indices argument can be used to do this.\n\n\n  sequence_indices = tf.range(start=1000, limit=1000 + num_results,\n                              dtype=tf.int32)\n  sample_leaped = tfp.mcmc.sample_halton_sequence(\n      dim,\n      sequence_indices=sequence_indices,\n      seed=111217)\n\n  integral_leaped = tf.reduce_mean(tf.reduce_prod(sample_leaped ** powers,\n                                                  axis=-1))\n  with tf.Session() as session:\n    values = session.run((integral_leaped, true_value))\n  # Now produces a relative absolute error of 0.05%.\n  print (\"Leaped Estimated: %f, True Value: %f\" % values)\n  ```\n\n  Args:\n    dim: Positive Python `int` representing each sample's `event_size.` Must\n      not be greater than 1000.\n    num_results: (Optional) Positive scalar `Tensor` of dtype int32. The number\n      of samples to generate. Either this parameter or sequence_indices must\n      be specified but not both. If this parameter is None, then the behaviour\n      is determined by the `sequence_indices`.\n      Default value: `None`.\n    sequence_indices: (Optional) `Tensor` of dtype int32 and rank 1. The\n      elements of the sequence to compute specified by their position in the\n      sequence. The entries index into the Halton sequence starting with 0 and\n      hence, must be whole numbers. For example, sequence_indices=[0, 5, 6] will\n      produce the first, sixth and seventh elements of the sequence. If this\n      parameter is None, then the `num_results` parameter must be specified\n      which gives the number of desired samples starting from the first sample.\n      Default value: `None`.\n    dtype: (Optional) The dtype of the sample. One of: `float16`, `float32` or\n      `float64`.\n      Default value: `tf.float32`.\n    randomized: (Optional) bool indicating whether to produce a randomized\n      Halton sequence. If True, applies the randomization described in\n      [Owen (2017)][1].\n      Default value: `True`.\n    seed: (Optional) Python integer to seed the random number generator. Only\n      used if `randomized` is True. If not supplied and `randomized` is True,\n      no seed is set.\n      Default value: `None`.\n    name:  (Optional) Python `str` describing ops managed by this function. If\n      not supplied the name of this function is used.\n      Default value: \"sample_halton_sequence\".\n\n  Returns:\n    halton_elements: Elements of the Halton sequence. `Tensor` of supplied dtype\n      and `shape` `[num_results, dim]` if `num_results` was specified or shape\n      `[s, dim]` where s is the size of `sequence_indices` if `sequence_indices`\n      were specified.\n\n  Raises:\n    ValueError: if both `sequence_indices` and `num_results` were specified or\n      if dimension `dim` is less than 1 or greater than 1000.\n\n  #### References\n\n  [1]: Art B. Owen. A randomized Halton algorithm in R. _arXiv preprint\n       arXiv:1706.02808_, 2017. https://arxiv.org/abs/1706.02808\n  \"\"\"\n  if dim < 1 or dim > _MAX_DIMENSION:\n    raise ValueError(\n        'Dimension must be between 1 and {}. Supplied {}'.format(_MAX_DIMENSION,\n                                                                 dim))\n  if (num_results is None) == (sequence_indices is None):\n    raise ValueError('Either `num_results` or `sequence_indices` must be'\n                     ' specified but not both.')\n\n  if not dtype.is_floating:\n    raise ValueError('dtype must be of `float`-type')\n\n  with tf.compat.v1.name_scope(\n      name, 'sample', values=[num_results, sequence_indices]):\n    # Here and in the following, the shape layout is as follows:\n    # [sample dimension, event dimension, coefficient dimension].\n    # The coefficient dimension is an intermediate axes which will hold the\n    # weights of the starting integer when expressed in the (prime) base for\n    # an event dimension.\n    if num_results is not None:\n      num_results = tf.convert_to_tensor(value=num_results)\n    if sequence_indices is not None:\n      sequence_indices = tf.convert_to_tensor(value=sequence_indices)\n    indices = _get_indices(num_results, sequence_indices, dtype)\n    radixes = tf.constant(_PRIMES[0:dim], dtype=dtype, shape=[dim, 1])\n\n    max_sizes_by_axes = _base_expansion_size(\n        tf.reduce_max(input_tensor=indices), radixes)\n\n    max_size = tf.reduce_max(input_tensor=max_sizes_by_axes)\n\n    # The powers of the radixes that we will need. Note that there is a bit\n    # of an excess here. Suppose we need the place value coefficients of 7\n    # in base 2 and 3. For 2, we will have 3 digits but we only need 2 digits\n    # for base 3. However, we can only create rectangular tensors so we\n    # store both expansions in a [2, 3] tensor. This leads to the problem that\n    # we might end up attempting to raise large numbers to large powers. For\n    # example, base 2 expansion of 1024 has 10 digits. If we were in 10\n    # dimensions, then the 10th prime (29) we will end up computing 29^10 even\n    # though we don't need it. We avoid this by setting the exponents for each\n    # axes to 0 beyond the maximum value needed for that dimension.\n    exponents_by_axes = tf.tile([tf.range(max_size)], [dim, 1])\n\n    # The mask is true for those coefficients that are irrelevant.\n    weight_mask = exponents_by_axes >= max_sizes_by_axes\n    capped_exponents = tf.where(\n        weight_mask,\n        tf.zeros_like(exponents_by_axes),\n        exponents_by_axes)\n    weights = radixes ** capped_exponents\n    # The following computes the base b expansion of the indices. Suppose,\n    # x = a0 + a1*b + a2*b^2 + ... Then, performing a floor div of x with\n    # the vector (1, b, b^2, b^3, ...) will produce\n    # (a0 + s1 * b, a1 + s2 * b, ...) where s_i are coefficients we don't care\n    # about. Noting that all a_i < b by definition of place value expansion,\n    # we see that taking the elements mod b of the above vector produces the\n    # place value expansion coefficients.\n    coeffs = tf.math.floordiv(indices, weights)\n    coeffs *= 1. - tf.cast(weight_mask, dtype)\n    coeffs %= radixes\n    if not randomized:\n      coeffs /= radixes\n      return tf.reduce_sum(input_tensor=coeffs / weights, axis=-1)\n    stream = distributions.SeedStream(seed, salt='MCMCSampleHaltonSequence')\n    coeffs = _randomize(coeffs, radixes, seed=stream())\n    # Remove the contribution from randomizing the trailing zero for the\n    # axes where max_size_by_axes < max_size. This will be accounted\n    # for separately below (using zero_correction).\n    coeffs *= 1. - tf.cast(weight_mask, dtype)\n    coeffs /= radixes\n    base_values = tf.reduce_sum(input_tensor=coeffs / weights, axis=-1)\n\n    # The randomization used in Owen (2017) does not leave 0 invariant. While\n    # we have accounted for the randomization of the first `max_size_by_axes`\n    # coefficients, we still need to correct for the trailing zeros. Luckily,\n    # this is equivalent to adding a uniform random value scaled so the first\n    # `max_size_by_axes` coefficients are zero. The following statements perform\n    # this correction.\n    zero_correction = tf.random.uniform([dim, 1], seed=stream(), dtype=dtype)\n    zero_correction /= radixes ** max_sizes_by_axes\n    return base_values + tf.reshape(zero_correction, [-1])", "code_tokens": ["def", "sample_halton_sequence", "(", "dim", ",", "num_results", "=", "None", ",", "sequence_indices", "=", "None", ",", "dtype", "=", "tf", ".", "float32", ",", "randomized", "=", "True", ",", "seed", "=", "None", ",", "name", "=", "None", ")", ":", "if", "dim", "<", "1", "or", "dim", ">", "_MAX_DIMENSION", ":", "raise", "ValueError", "(", "'Dimension must be between 1 and {}. Supplied {}'", ".", "format", "(", "_MAX_DIMENSION", ",", "dim", ")", ")", "if", "(", "num_results", "is", "None", ")", "==", "(", "sequence_indices", "is", "None", ")", ":", "raise", "ValueError", "(", "'Either `num_results` or `sequence_indices` must be'", "' specified but not both.'", ")", "if", "not", "dtype", ".", "is_floating", ":", "raise", "ValueError", "(", "'dtype must be of `float`-type'", ")", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'sample'", ",", "values", "=", "[", "num_results", ",", "sequence_indices", "]", ")", ":", "# Here and in the following, the shape layout is as follows:", "# [sample dimension, event dimension, coefficient dimension].", "# The coefficient dimension is an intermediate axes which will hold the", "# weights of the starting integer when expressed in the (prime) base for", "# an event dimension.", "if", "num_results", "is", "not", "None", ":", "num_results", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "num_results", ")", "if", "sequence_indices", "is", "not", "None", ":", "sequence_indices", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "sequence_indices", ")", "indices", "=", "_get_indices", "(", "num_results", ",", "sequence_indices", ",", "dtype", ")", "radixes", "=", "tf", ".", "constant", "(", "_PRIMES", "[", "0", ":", "dim", "]", ",", "dtype", "=", "dtype", ",", "shape", "=", "[", "dim", ",", "1", "]", ")", "max_sizes_by_axes", "=", "_base_expansion_size", "(", "tf", ".", "reduce_max", "(", "input_tensor", "=", "indices", ")", ",", "radixes", ")", "max_size", "=", "tf", ".", "reduce_max", "(", "input_tensor", "=", "max_sizes_by_axes", ")", "# The powers of the radixes that we will need. Note that there is a bit", "# of an excess here. Suppose we need the place value coefficients of 7", "# in base 2 and 3. For 2, we will have 3 digits but we only need 2 digits", "# for base 3. However, we can only create rectangular tensors so we", "# store both expansions in a [2, 3] tensor. This leads to the problem that", "# we might end up attempting to raise large numbers to large powers. For", "# example, base 2 expansion of 1024 has 10 digits. If we were in 10", "# dimensions, then the 10th prime (29) we will end up computing 29^10 even", "# though we don't need it. We avoid this by setting the exponents for each", "# axes to 0 beyond the maximum value needed for that dimension.", "exponents_by_axes", "=", "tf", ".", "tile", "(", "[", "tf", ".", "range", "(", "max_size", ")", "]", ",", "[", "dim", ",", "1", "]", ")", "# The mask is true for those coefficients that are irrelevant.", "weight_mask", "=", "exponents_by_axes", ">=", "max_sizes_by_axes", "capped_exponents", "=", "tf", ".", "where", "(", "weight_mask", ",", "tf", ".", "zeros_like", "(", "exponents_by_axes", ")", ",", "exponents_by_axes", ")", "weights", "=", "radixes", "**", "capped_exponents", "# The following computes the base b expansion of the indices. Suppose,", "# x = a0 + a1*b + a2*b^2 + ... Then, performing a floor div of x with", "# the vector (1, b, b^2, b^3, ...) will produce", "# (a0 + s1 * b, a1 + s2 * b, ...) where s_i are coefficients we don't care", "# about. Noting that all a_i < b by definition of place value expansion,", "# we see that taking the elements mod b of the above vector produces the", "# place value expansion coefficients.", "coeffs", "=", "tf", ".", "math", ".", "floordiv", "(", "indices", ",", "weights", ")", "coeffs", "*=", "1.", "-", "tf", ".", "cast", "(", "weight_mask", ",", "dtype", ")", "coeffs", "%=", "radixes", "if", "not", "randomized", ":", "coeffs", "/=", "radixes", "return", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "coeffs", "/", "weights", ",", "axis", "=", "-", "1", ")", "stream", "=", "distributions", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'MCMCSampleHaltonSequence'", ")", "coeffs", "=", "_randomize", "(", "coeffs", ",", "radixes", ",", "seed", "=", "stream", "(", ")", ")", "# Remove the contribution from randomizing the trailing zero for the", "# axes where max_size_by_axes < max_size. This will be accounted", "# for separately below (using zero_correction).", "coeffs", "*=", "1.", "-", "tf", ".", "cast", "(", "weight_mask", ",", "dtype", ")", "coeffs", "/=", "radixes", "base_values", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "coeffs", "/", "weights", ",", "axis", "=", "-", "1", ")", "# The randomization used in Owen (2017) does not leave 0 invariant. While", "# we have accounted for the randomization of the first `max_size_by_axes`", "# coefficients, we still need to correct for the trailing zeros. Luckily,", "# this is equivalent to adding a uniform random value scaled so the first", "# `max_size_by_axes` coefficients are zero. The following statements perform", "# this correction.", "zero_correction", "=", "tf", ".", "random", ".", "uniform", "(", "[", "dim", ",", "1", "]", ",", "seed", "=", "stream", "(", ")", ",", "dtype", "=", "dtype", ")", "zero_correction", "/=", "radixes", "**", "max_sizes_by_axes", "return", "base_values", "+", "tf", ".", "reshape", "(", "zero_correction", ",", "[", "-", "1", "]", ")"], "docstring": "r\"\"\"Returns a sample from the `dim` dimensional Halton sequence.\n\n  Warning: The sequence elements take values only between 0 and 1. Care must be\n  taken to appropriately transform the domain of a function if it differs from\n  the unit cube before evaluating integrals using Halton samples. It is also\n  important to remember that quasi-random numbers without randomization are not\n  a replacement for pseudo-random numbers in every context. Quasi random numbers\n  are completely deterministic and typically have significant negative\n  autocorrelation unless randomization is used.\n\n  Computes the members of the low discrepancy Halton sequence in dimension\n  `dim`. The `dim`-dimensional sequence takes values in the unit hypercube in\n  `dim` dimensions. Currently, only dimensions up to 1000 are supported. The\n  prime base for the k-th axes is the k-th prime starting from 2. For example,\n  if `dim` = 3, then the bases will be [2, 3, 5] respectively and the first\n  element of the non-randomized sequence will be: [0.5, 0.333, 0.2]. For a more\n  complete description of the Halton sequences see\n  [here](https://en.wikipedia.org/wiki/Halton_sequence). For low discrepancy\n  sequences and their applications see\n  [here](https://en.wikipedia.org/wiki/Low-discrepancy_sequence).\n\n  If `randomized` is true, this function produces a scrambled version of the\n  Halton sequence introduced by [Owen (2017)][1]. For the advantages of\n  randomization of low discrepancy sequences see [here](\n  https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method#Randomization_of_quasi-Monte_Carlo).\n\n  The number of samples produced is controlled by the `num_results` and\n  `sequence_indices` parameters. The user must supply either `num_results` or\n  `sequence_indices` but not both.\n  The former is the number of samples to produce starting from the first\n  element. If `sequence_indices` is given instead, the specified elements of\n  the sequence are generated. For example, sequence_indices=tf.range(10) is\n  equivalent to specifying n=10.\n\n  #### Examples\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  # Produce the first 1000 members of the Halton sequence in 3 dimensions.\n  num_results = 1000\n  dim = 3\n  sample = tfp.mcmc.sample_halton_sequence(\n    dim,\n    num_results=num_results,\n    seed=127)\n\n  # Evaluate the integral of x_1 * x_2^2 * x_3^3  over the three dimensional\n  # hypercube.\n  powers = tf.range(1.0, limit=dim + 1)\n  integral = tf.reduce_mean(tf.reduce_prod(sample ** powers, axis=-1))\n  true_value = 1.0 / tf.reduce_prod(powers + 1.0)\n  with tf.Session() as session:\n    values = session.run((integral, true_value))\n\n  # Produces a relative absolute error of 1.7%.\n  print (\"Estimated: %f, True Value: %f\" % values)\n\n  # Now skip the first 1000 samples and recompute the integral with the next\n  # thousand samples. The sequence_indices argument can be used to do this.\n\n\n  sequence_indices = tf.range(start=1000, limit=1000 + num_results,\n                              dtype=tf.int32)\n  sample_leaped = tfp.mcmc.sample_halton_sequence(\n      dim,\n      sequence_indices=sequence_indices,\n      seed=111217)\n\n  integral_leaped = tf.reduce_mean(tf.reduce_prod(sample_leaped ** powers,\n                                                  axis=-1))\n  with tf.Session() as session:\n    values = session.run((integral_leaped, true_value))\n  # Now produces a relative absolute error of 0.05%.\n  print (\"Leaped Estimated: %f, True Value: %f\" % values)\n  ```\n\n  Args:\n    dim: Positive Python `int` representing each sample's `event_size.` Must\n      not be greater than 1000.\n    num_results: (Optional) Positive scalar `Tensor` of dtype int32. The number\n      of samples to generate. Either this parameter or sequence_indices must\n      be specified but not both. If this parameter is None, then the behaviour\n      is determined by the `sequence_indices`.\n      Default value: `None`.\n    sequence_indices: (Optional) `Tensor` of dtype int32 and rank 1. The\n      elements of the sequence to compute specified by their position in the\n      sequence. The entries index into the Halton sequence starting with 0 and\n      hence, must be whole numbers. For example, sequence_indices=[0, 5, 6] will\n      produce the first, sixth and seventh elements of the sequence. If this\n      parameter is None, then the `num_results` parameter must be specified\n      which gives the number of desired samples starting from the first sample.\n      Default value: `None`.\n    dtype: (Optional) The dtype of the sample. One of: `float16`, `float32` or\n      `float64`.\n      Default value: `tf.float32`.\n    randomized: (Optional) bool indicating whether to produce a randomized\n      Halton sequence. If True, applies the randomization described in\n      [Owen (2017)][1].\n      Default value: `True`.\n    seed: (Optional) Python integer to seed the random number generator. Only\n      used if `randomized` is True. If not supplied and `randomized` is True,\n      no seed is set.\n      Default value: `None`.\n    name:  (Optional) Python `str` describing ops managed by this function. If\n      not supplied the name of this function is used.\n      Default value: \"sample_halton_sequence\".\n\n  Returns:\n    halton_elements: Elements of the Halton sequence. `Tensor` of supplied dtype\n      and `shape` `[num_results, dim]` if `num_results` was specified or shape\n      `[s, dim]` where s is the size of `sequence_indices` if `sequence_indices`\n      were specified.\n\n  Raises:\n    ValueError: if both `sequence_indices` and `num_results` were specified or\n      if dimension `dim` is less than 1 or greater than 1000.\n\n  #### References\n\n  [1]: Art B. Owen. A randomized Halton algorithm in R. _arXiv preprint\n       arXiv:1706.02808_, 2017. https://arxiv.org/abs/1706.02808", "docstring_tokens": ["r", "Returns", "a", "sample", "from", "the", "dim", "dimensional", "Halton", "sequence", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/sample_halton_sequence.py#L39-L249", "partition": "test", "index": 665, "time": "2018-03-07 09:58:22"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/util.py", "func_name": "default_loc_scale_fn", "original_string": "def default_loc_scale_fn(\n    is_singular=False,\n    loc_initializer=tf.compat.v1.initializers.random_normal(stddev=0.1),\n    untransformed_scale_initializer=tf.compat.v1.initializers.random_normal(\n        mean=-3., stddev=0.1),\n    loc_regularizer=None,\n    untransformed_scale_regularizer=None,\n    loc_constraint=None,\n    untransformed_scale_constraint=None):\n  \"\"\"Makes closure which creates `loc`, `scale` params from `tf.get_variable`.\n\n  This function produces a closure which produces `loc`, `scale` using\n  `tf.get_variable`. The closure accepts the following arguments:\n\n    dtype: Type of parameter's event.\n    shape: Python `list`-like representing the parameter's event shape.\n    name: Python `str` name prepended to any created (or existing)\n      `tf.Variable`s.\n    trainable: Python `bool` indicating all created `tf.Variable`s should be\n      added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.\n    add_variable_fn: `tf.get_variable`-like `callable` used to create (or\n      access existing) `tf.Variable`s.\n\n  Args:\n    is_singular: Python `bool` indicating if `scale is None`. Default: `False`.\n    loc_initializer: Initializer function for the `loc` parameters.\n      The default is `tf.random_normal_initializer(mean=0., stddev=0.1)`.\n    untransformed_scale_initializer: Initializer function for the `scale`\n      parameters. Default value: `tf.random_normal_initializer(mean=-3.,\n      stddev=0.1)`. This implies the softplus transformed result is initialized\n      near `0`. It allows a `Normal` distribution with `scale` parameter set to\n      this value to approximately act like a point mass.\n    loc_regularizer: Regularizer function for the `loc` parameters.\n      The default (`None`) is to use the `tf.get_variable` default.\n    untransformed_scale_regularizer: Regularizer function for the `scale`\n      parameters. The default (`None`) is to use the `tf.get_variable` default.\n    loc_constraint: An optional projection function to be applied to the\n      loc after being updated by an `Optimizer`. The function must take as input\n      the unprojected variable and must return the projected variable (which\n      must have the same shape). Constraints are not safe to use when doing\n      asynchronous distributed training.\n      The default (`None`) is to use the `tf.get_variable` default.\n    untransformed_scale_constraint: An optional projection function to be\n      applied to the `scale` parameters after being updated by an `Optimizer`\n      (e.g. used to implement norm constraints or value constraints). The\n      function must take as input the unprojected variable and must return the\n      projected variable (which must have the same shape). Constraints are not\n      safe to use when doing asynchronous distributed training. The default\n      (`None`) is to use the `tf.get_variable` default.\n\n  Returns:\n    default_loc_scale_fn: Python `callable` which instantiates `loc`, `scale`\n    parameters from args: `dtype, shape, name, trainable, add_variable_fn`.\n  \"\"\"\n  def _fn(dtype, shape, name, trainable, add_variable_fn):\n    \"\"\"Creates `loc`, `scale` parameters.\"\"\"\n    loc = add_variable_fn(\n        name=name + '_loc',\n        shape=shape,\n        initializer=loc_initializer,\n        regularizer=loc_regularizer,\n        constraint=loc_constraint,\n        dtype=dtype,\n        trainable=trainable)\n    if is_singular:\n      return loc, None\n    untransformed_scale = add_variable_fn(\n        name=name + '_untransformed_scale',\n        shape=shape,\n        initializer=untransformed_scale_initializer,\n        regularizer=untransformed_scale_regularizer,\n        constraint=untransformed_scale_constraint,\n        dtype=dtype,\n        trainable=trainable)\n    scale = (np.finfo(dtype.as_numpy_dtype).eps +\n             tf.nn.softplus(untransformed_scale))\n    return loc, scale\n  return _fn", "language": "python", "code": "def default_loc_scale_fn(\n    is_singular=False,\n    loc_initializer=tf.compat.v1.initializers.random_normal(stddev=0.1),\n    untransformed_scale_initializer=tf.compat.v1.initializers.random_normal(\n        mean=-3., stddev=0.1),\n    loc_regularizer=None,\n    untransformed_scale_regularizer=None,\n    loc_constraint=None,\n    untransformed_scale_constraint=None):\n  \"\"\"Makes closure which creates `loc`, `scale` params from `tf.get_variable`.\n\n  This function produces a closure which produces `loc`, `scale` using\n  `tf.get_variable`. The closure accepts the following arguments:\n\n    dtype: Type of parameter's event.\n    shape: Python `list`-like representing the parameter's event shape.\n    name: Python `str` name prepended to any created (or existing)\n      `tf.Variable`s.\n    trainable: Python `bool` indicating all created `tf.Variable`s should be\n      added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.\n    add_variable_fn: `tf.get_variable`-like `callable` used to create (or\n      access existing) `tf.Variable`s.\n\n  Args:\n    is_singular: Python `bool` indicating if `scale is None`. Default: `False`.\n    loc_initializer: Initializer function for the `loc` parameters.\n      The default is `tf.random_normal_initializer(mean=0., stddev=0.1)`.\n    untransformed_scale_initializer: Initializer function for the `scale`\n      parameters. Default value: `tf.random_normal_initializer(mean=-3.,\n      stddev=0.1)`. This implies the softplus transformed result is initialized\n      near `0`. It allows a `Normal` distribution with `scale` parameter set to\n      this value to approximately act like a point mass.\n    loc_regularizer: Regularizer function for the `loc` parameters.\n      The default (`None`) is to use the `tf.get_variable` default.\n    untransformed_scale_regularizer: Regularizer function for the `scale`\n      parameters. The default (`None`) is to use the `tf.get_variable` default.\n    loc_constraint: An optional projection function to be applied to the\n      loc after being updated by an `Optimizer`. The function must take as input\n      the unprojected variable and must return the projected variable (which\n      must have the same shape). Constraints are not safe to use when doing\n      asynchronous distributed training.\n      The default (`None`) is to use the `tf.get_variable` default.\n    untransformed_scale_constraint: An optional projection function to be\n      applied to the `scale` parameters after being updated by an `Optimizer`\n      (e.g. used to implement norm constraints or value constraints). The\n      function must take as input the unprojected variable and must return the\n      projected variable (which must have the same shape). Constraints are not\n      safe to use when doing asynchronous distributed training. The default\n      (`None`) is to use the `tf.get_variable` default.\n\n  Returns:\n    default_loc_scale_fn: Python `callable` which instantiates `loc`, `scale`\n    parameters from args: `dtype, shape, name, trainable, add_variable_fn`.\n  \"\"\"\n  def _fn(dtype, shape, name, trainable, add_variable_fn):\n    \"\"\"Creates `loc`, `scale` parameters.\"\"\"\n    loc = add_variable_fn(\n        name=name + '_loc',\n        shape=shape,\n        initializer=loc_initializer,\n        regularizer=loc_regularizer,\n        constraint=loc_constraint,\n        dtype=dtype,\n        trainable=trainable)\n    if is_singular:\n      return loc, None\n    untransformed_scale = add_variable_fn(\n        name=name + '_untransformed_scale',\n        shape=shape,\n        initializer=untransformed_scale_initializer,\n        regularizer=untransformed_scale_regularizer,\n        constraint=untransformed_scale_constraint,\n        dtype=dtype,\n        trainable=trainable)\n    scale = (np.finfo(dtype.as_numpy_dtype).eps +\n             tf.nn.softplus(untransformed_scale))\n    return loc, scale\n  return _fn", "code_tokens": ["def", "default_loc_scale_fn", "(", "is_singular", "=", "False", ",", "loc_initializer", "=", "tf", ".", "compat", ".", "v1", ".", "initializers", ".", "random_normal", "(", "stddev", "=", "0.1", ")", ",", "untransformed_scale_initializer", "=", "tf", ".", "compat", ".", "v1", ".", "initializers", ".", "random_normal", "(", "mean", "=", "-", "3.", ",", "stddev", "=", "0.1", ")", ",", "loc_regularizer", "=", "None", ",", "untransformed_scale_regularizer", "=", "None", ",", "loc_constraint", "=", "None", ",", "untransformed_scale_constraint", "=", "None", ")", ":", "def", "_fn", "(", "dtype", ",", "shape", ",", "name", ",", "trainable", ",", "add_variable_fn", ")", ":", "\"\"\"Creates `loc`, `scale` parameters.\"\"\"", "loc", "=", "add_variable_fn", "(", "name", "=", "name", "+", "'_loc'", ",", "shape", "=", "shape", ",", "initializer", "=", "loc_initializer", ",", "regularizer", "=", "loc_regularizer", ",", "constraint", "=", "loc_constraint", ",", "dtype", "=", "dtype", ",", "trainable", "=", "trainable", ")", "if", "is_singular", ":", "return", "loc", ",", "None", "untransformed_scale", "=", "add_variable_fn", "(", "name", "=", "name", "+", "'_untransformed_scale'", ",", "shape", "=", "shape", ",", "initializer", "=", "untransformed_scale_initializer", ",", "regularizer", "=", "untransformed_scale_regularizer", ",", "constraint", "=", "untransformed_scale_constraint", ",", "dtype", "=", "dtype", ",", "trainable", "=", "trainable", ")", "scale", "=", "(", "np", ".", "finfo", "(", "dtype", ".", "as_numpy_dtype", ")", ".", "eps", "+", "tf", ".", "nn", ".", "softplus", "(", "untransformed_scale", ")", ")", "return", "loc", ",", "scale", "return", "_fn"], "docstring": "Makes closure which creates `loc`, `scale` params from `tf.get_variable`.\n\n  This function produces a closure which produces `loc`, `scale` using\n  `tf.get_variable`. The closure accepts the following arguments:\n\n    dtype: Type of parameter's event.\n    shape: Python `list`-like representing the parameter's event shape.\n    name: Python `str` name prepended to any created (or existing)\n      `tf.Variable`s.\n    trainable: Python `bool` indicating all created `tf.Variable`s should be\n      added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.\n    add_variable_fn: `tf.get_variable`-like `callable` used to create (or\n      access existing) `tf.Variable`s.\n\n  Args:\n    is_singular: Python `bool` indicating if `scale is None`. Default: `False`.\n    loc_initializer: Initializer function for the `loc` parameters.\n      The default is `tf.random_normal_initializer(mean=0., stddev=0.1)`.\n    untransformed_scale_initializer: Initializer function for the `scale`\n      parameters. Default value: `tf.random_normal_initializer(mean=-3.,\n      stddev=0.1)`. This implies the softplus transformed result is initialized\n      near `0`. It allows a `Normal` distribution with `scale` parameter set to\n      this value to approximately act like a point mass.\n    loc_regularizer: Regularizer function for the `loc` parameters.\n      The default (`None`) is to use the `tf.get_variable` default.\n    untransformed_scale_regularizer: Regularizer function for the `scale`\n      parameters. The default (`None`) is to use the `tf.get_variable` default.\n    loc_constraint: An optional projection function to be applied to the\n      loc after being updated by an `Optimizer`. The function must take as input\n      the unprojected variable and must return the projected variable (which\n      must have the same shape). Constraints are not safe to use when doing\n      asynchronous distributed training.\n      The default (`None`) is to use the `tf.get_variable` default.\n    untransformed_scale_constraint: An optional projection function to be\n      applied to the `scale` parameters after being updated by an `Optimizer`\n      (e.g. used to implement norm constraints or value constraints). The\n      function must take as input the unprojected variable and must return the\n      projected variable (which must have the same shape). Constraints are not\n      safe to use when doing asynchronous distributed training. The default\n      (`None`) is to use the `tf.get_variable` default.\n\n  Returns:\n    default_loc_scale_fn: Python `callable` which instantiates `loc`, `scale`\n    parameters from args: `dtype, shape, name, trainable, add_variable_fn`.", "docstring_tokens": ["Makes", "closure", "which", "creates", "loc", "scale", "params", "from", "tf", ".", "get_variable", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/util.py#L39-L116", "partition": "test", "index": 807, "time": "2018-03-07 11:12:11"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/util.py", "func_name": "default_mean_field_normal_fn", "original_string": "def default_mean_field_normal_fn(\n    is_singular=False,\n    loc_initializer=tf.compat.v1.initializers.random_normal(stddev=0.1),\n    untransformed_scale_initializer=tf.compat.v1.initializers.random_normal(\n        mean=-3., stddev=0.1),\n    loc_regularizer=None,\n    untransformed_scale_regularizer=None,\n    loc_constraint=None,\n    untransformed_scale_constraint=None):\n  \"\"\"Creates a function to build Normal distributions with trainable params.\n\n  This function produces a closure which produces `tfd.Normal`\n  parameterized by a loc` and `scale` each created using `tf.get_variable`.\n\n  Args:\n    is_singular: Python `bool` if `True`, forces the special case limit of\n      `scale->0`, i.e., a `Deterministic` distribution.\n    loc_initializer: Initializer function for the `loc` parameters.\n      The default is `tf.random_normal_initializer(mean=0., stddev=0.1)`.\n    untransformed_scale_initializer: Initializer function for the `scale`\n      parameters. Default value: `tf.random_normal_initializer(mean=-3.,\n      stddev=0.1)`. This implies the softplus transformed result is initialized\n      near `0`. It allows a `Normal` distribution with `scale` parameter set to\n      this value to approximately act like a point mass.\n    loc_regularizer: Regularizer function for the `loc` parameters.\n    untransformed_scale_regularizer: Regularizer function for the `scale`\n      parameters.\n    loc_constraint: An optional projection function to be applied to the\n      loc after being updated by an `Optimizer`. The function must take as input\n      the unprojected variable and must return the projected variable (which\n      must have the same shape). Constraints are not safe to use when doing\n      asynchronous distributed training.\n    untransformed_scale_constraint: An optional projection function to be\n      applied to the `scale` parameters after being updated by an `Optimizer`\n      (e.g. used to implement norm constraints or value constraints). The\n      function must take as input the unprojected variable and must return the\n      projected variable (which must have the same shape). Constraints are not\n      safe to use when doing asynchronous distributed training.\n\n  Returns:\n    make_normal_fn: Python `callable` which creates a `tfd.Normal`\n      using from args: `dtype, shape, name, trainable, add_variable_fn`.\n  \"\"\"\n  loc_scale_fn = default_loc_scale_fn(\n      is_singular=is_singular,\n      loc_initializer=loc_initializer,\n      untransformed_scale_initializer=untransformed_scale_initializer,\n      loc_regularizer=loc_regularizer,\n      untransformed_scale_regularizer=untransformed_scale_regularizer,\n      loc_constraint=loc_constraint,\n      untransformed_scale_constraint=untransformed_scale_constraint)\n  def _fn(dtype, shape, name, trainable, add_variable_fn):\n    \"\"\"Creates multivariate `Deterministic` or `Normal` distribution.\n\n    Args:\n      dtype: Type of parameter's event.\n      shape: Python `list`-like representing the parameter's event shape.\n      name: Python `str` name prepended to any created (or existing)\n        `tf.Variable`s.\n      trainable: Python `bool` indicating all created `tf.Variable`s should be\n        added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.\n      add_variable_fn: `tf.get_variable`-like `callable` used to create (or\n        access existing) `tf.Variable`s.\n\n    Returns:\n      Multivariate `Deterministic` or `Normal` distribution.\n    \"\"\"\n    loc, scale = loc_scale_fn(dtype, shape, name, trainable, add_variable_fn)\n    if scale is None:\n      dist = tfd.Deterministic(loc=loc)\n    else:\n      dist = tfd.Normal(loc=loc, scale=scale)\n    batch_ndims = tf.size(input=dist.batch_shape_tensor())\n    return tfd.Independent(dist, reinterpreted_batch_ndims=batch_ndims)\n  return _fn", "language": "python", "code": "def default_mean_field_normal_fn(\n    is_singular=False,\n    loc_initializer=tf.compat.v1.initializers.random_normal(stddev=0.1),\n    untransformed_scale_initializer=tf.compat.v1.initializers.random_normal(\n        mean=-3., stddev=0.1),\n    loc_regularizer=None,\n    untransformed_scale_regularizer=None,\n    loc_constraint=None,\n    untransformed_scale_constraint=None):\n  \"\"\"Creates a function to build Normal distributions with trainable params.\n\n  This function produces a closure which produces `tfd.Normal`\n  parameterized by a loc` and `scale` each created using `tf.get_variable`.\n\n  Args:\n    is_singular: Python `bool` if `True`, forces the special case limit of\n      `scale->0`, i.e., a `Deterministic` distribution.\n    loc_initializer: Initializer function for the `loc` parameters.\n      The default is `tf.random_normal_initializer(mean=0., stddev=0.1)`.\n    untransformed_scale_initializer: Initializer function for the `scale`\n      parameters. Default value: `tf.random_normal_initializer(mean=-3.,\n      stddev=0.1)`. This implies the softplus transformed result is initialized\n      near `0`. It allows a `Normal` distribution with `scale` parameter set to\n      this value to approximately act like a point mass.\n    loc_regularizer: Regularizer function for the `loc` parameters.\n    untransformed_scale_regularizer: Regularizer function for the `scale`\n      parameters.\n    loc_constraint: An optional projection function to be applied to the\n      loc after being updated by an `Optimizer`. The function must take as input\n      the unprojected variable and must return the projected variable (which\n      must have the same shape). Constraints are not safe to use when doing\n      asynchronous distributed training.\n    untransformed_scale_constraint: An optional projection function to be\n      applied to the `scale` parameters after being updated by an `Optimizer`\n      (e.g. used to implement norm constraints or value constraints). The\n      function must take as input the unprojected variable and must return the\n      projected variable (which must have the same shape). Constraints are not\n      safe to use when doing asynchronous distributed training.\n\n  Returns:\n    make_normal_fn: Python `callable` which creates a `tfd.Normal`\n      using from args: `dtype, shape, name, trainable, add_variable_fn`.\n  \"\"\"\n  loc_scale_fn = default_loc_scale_fn(\n      is_singular=is_singular,\n      loc_initializer=loc_initializer,\n      untransformed_scale_initializer=untransformed_scale_initializer,\n      loc_regularizer=loc_regularizer,\n      untransformed_scale_regularizer=untransformed_scale_regularizer,\n      loc_constraint=loc_constraint,\n      untransformed_scale_constraint=untransformed_scale_constraint)\n  def _fn(dtype, shape, name, trainable, add_variable_fn):\n    \"\"\"Creates multivariate `Deterministic` or `Normal` distribution.\n\n    Args:\n      dtype: Type of parameter's event.\n      shape: Python `list`-like representing the parameter's event shape.\n      name: Python `str` name prepended to any created (or existing)\n        `tf.Variable`s.\n      trainable: Python `bool` indicating all created `tf.Variable`s should be\n        added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.\n      add_variable_fn: `tf.get_variable`-like `callable` used to create (or\n        access existing) `tf.Variable`s.\n\n    Returns:\n      Multivariate `Deterministic` or `Normal` distribution.\n    \"\"\"\n    loc, scale = loc_scale_fn(dtype, shape, name, trainable, add_variable_fn)\n    if scale is None:\n      dist = tfd.Deterministic(loc=loc)\n    else:\n      dist = tfd.Normal(loc=loc, scale=scale)\n    batch_ndims = tf.size(input=dist.batch_shape_tensor())\n    return tfd.Independent(dist, reinterpreted_batch_ndims=batch_ndims)\n  return _fn", "code_tokens": ["def", "default_mean_field_normal_fn", "(", "is_singular", "=", "False", ",", "loc_initializer", "=", "tf", ".", "compat", ".", "v1", ".", "initializers", ".", "random_normal", "(", "stddev", "=", "0.1", ")", ",", "untransformed_scale_initializer", "=", "tf", ".", "compat", ".", "v1", ".", "initializers", ".", "random_normal", "(", "mean", "=", "-", "3.", ",", "stddev", "=", "0.1", ")", ",", "loc_regularizer", "=", "None", ",", "untransformed_scale_regularizer", "=", "None", ",", "loc_constraint", "=", "None", ",", "untransformed_scale_constraint", "=", "None", ")", ":", "loc_scale_fn", "=", "default_loc_scale_fn", "(", "is_singular", "=", "is_singular", ",", "loc_initializer", "=", "loc_initializer", ",", "untransformed_scale_initializer", "=", "untransformed_scale_initializer", ",", "loc_regularizer", "=", "loc_regularizer", ",", "untransformed_scale_regularizer", "=", "untransformed_scale_regularizer", ",", "loc_constraint", "=", "loc_constraint", ",", "untransformed_scale_constraint", "=", "untransformed_scale_constraint", ")", "def", "_fn", "(", "dtype", ",", "shape", ",", "name", ",", "trainable", ",", "add_variable_fn", ")", ":", "\"\"\"Creates multivariate `Deterministic` or `Normal` distribution.\n\n    Args:\n      dtype: Type of parameter's event.\n      shape: Python `list`-like representing the parameter's event shape.\n      name: Python `str` name prepended to any created (or existing)\n        `tf.Variable`s.\n      trainable: Python `bool` indicating all created `tf.Variable`s should be\n        added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.\n      add_variable_fn: `tf.get_variable`-like `callable` used to create (or\n        access existing) `tf.Variable`s.\n\n    Returns:\n      Multivariate `Deterministic` or `Normal` distribution.\n    \"\"\"", "loc", ",", "scale", "=", "loc_scale_fn", "(", "dtype", ",", "shape", ",", "name", ",", "trainable", ",", "add_variable_fn", ")", "if", "scale", "is", "None", ":", "dist", "=", "tfd", ".", "Deterministic", "(", "loc", "=", "loc", ")", "else", ":", "dist", "=", "tfd", ".", "Normal", "(", "loc", "=", "loc", ",", "scale", "=", "scale", ")", "batch_ndims", "=", "tf", ".", "size", "(", "input", "=", "dist", ".", "batch_shape_tensor", "(", ")", ")", "return", "tfd", ".", "Independent", "(", "dist", ",", "reinterpreted_batch_ndims", "=", "batch_ndims", ")", "return", "_fn"], "docstring": "Creates a function to build Normal distributions with trainable params.\n\n  This function produces a closure which produces `tfd.Normal`\n  parameterized by a loc` and `scale` each created using `tf.get_variable`.\n\n  Args:\n    is_singular: Python `bool` if `True`, forces the special case limit of\n      `scale->0`, i.e., a `Deterministic` distribution.\n    loc_initializer: Initializer function for the `loc` parameters.\n      The default is `tf.random_normal_initializer(mean=0., stddev=0.1)`.\n    untransformed_scale_initializer: Initializer function for the `scale`\n      parameters. Default value: `tf.random_normal_initializer(mean=-3.,\n      stddev=0.1)`. This implies the softplus transformed result is initialized\n      near `0`. It allows a `Normal` distribution with `scale` parameter set to\n      this value to approximately act like a point mass.\n    loc_regularizer: Regularizer function for the `loc` parameters.\n    untransformed_scale_regularizer: Regularizer function for the `scale`\n      parameters.\n    loc_constraint: An optional projection function to be applied to the\n      loc after being updated by an `Optimizer`. The function must take as input\n      the unprojected variable and must return the projected variable (which\n      must have the same shape). Constraints are not safe to use when doing\n      asynchronous distributed training.\n    untransformed_scale_constraint: An optional projection function to be\n      applied to the `scale` parameters after being updated by an `Optimizer`\n      (e.g. used to implement norm constraints or value constraints). The\n      function must take as input the unprojected variable and must return the\n      projected variable (which must have the same shape). Constraints are not\n      safe to use when doing asynchronous distributed training.\n\n  Returns:\n    make_normal_fn: Python `callable` which creates a `tfd.Normal`\n      using from args: `dtype, shape, name, trainable, add_variable_fn`.", "docstring_tokens": ["Creates", "a", "function", "to", "build", "Normal", "distributions", "with", "trainable", "params", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/util.py#L119-L193", "partition": "test", "index": 808, "time": "2018-03-07 11:12:11"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/util/docstring.py", "func_name": "expand_docstring", "original_string": "def expand_docstring(**kwargs):\n  \"\"\"Decorator to programmatically expand the docstring.\n\n  Args:\n    **kwargs: Keyword arguments to set. For each key-value pair `k` and `v`,\n      the key is found as `${k}` in the docstring and replaced with `v`.\n\n  Returns:\n    Decorated function.\n  \"\"\"\n  def _fn_wrapped(fn):\n    \"\"\"Original function with modified `__doc__` attribute.\"\"\"\n    doc = inspect.cleandoc(fn.__doc__)\n    for k, v in six.iteritems(kwargs):\n      # Capture each ${k} reference to replace with v.\n      # We wrap the replacement in a function so no backslash escapes\n      # are processed.\n      pattern = r'\\$\\{' + str(k) + r'\\}'\n      doc = re.sub(pattern, lambda match: v, doc)  # pylint: disable=cell-var-from-loop\n    fn.__doc__ = doc\n    return fn\n  return _fn_wrapped", "language": "python", "code": "def expand_docstring(**kwargs):\n  \"\"\"Decorator to programmatically expand the docstring.\n\n  Args:\n    **kwargs: Keyword arguments to set. For each key-value pair `k` and `v`,\n      the key is found as `${k}` in the docstring and replaced with `v`.\n\n  Returns:\n    Decorated function.\n  \"\"\"\n  def _fn_wrapped(fn):\n    \"\"\"Original function with modified `__doc__` attribute.\"\"\"\n    doc = inspect.cleandoc(fn.__doc__)\n    for k, v in six.iteritems(kwargs):\n      # Capture each ${k} reference to replace with v.\n      # We wrap the replacement in a function so no backslash escapes\n      # are processed.\n      pattern = r'\\$\\{' + str(k) + r'\\}'\n      doc = re.sub(pattern, lambda match: v, doc)  # pylint: disable=cell-var-from-loop\n    fn.__doc__ = doc\n    return fn\n  return _fn_wrapped", "code_tokens": ["def", "expand_docstring", "(", "*", "*", "kwargs", ")", ":", "def", "_fn_wrapped", "(", "fn", ")", ":", "\"\"\"Original function with modified `__doc__` attribute.\"\"\"", "doc", "=", "inspect", ".", "cleandoc", "(", "fn", ".", "__doc__", ")", "for", "k", ",", "v", "in", "six", ".", "iteritems", "(", "kwargs", ")", ":", "# Capture each ${k} reference to replace with v.", "# We wrap the replacement in a function so no backslash escapes", "# are processed.", "pattern", "=", "r'\\$\\{'", "+", "str", "(", "k", ")", "+", "r'\\}'", "doc", "=", "re", ".", "sub", "(", "pattern", ",", "lambda", "match", ":", "v", ",", "doc", ")", "# pylint: disable=cell-var-from-loop", "fn", ".", "__doc__", "=", "doc", "return", "fn", "return", "_fn_wrapped"], "docstring": "Decorator to programmatically expand the docstring.\n\n  Args:\n    **kwargs: Keyword arguments to set. For each key-value pair `k` and `v`,\n      the key is found as `${k}` in the docstring and replaced with `v`.\n\n  Returns:\n    Decorated function.", "docstring_tokens": ["Decorator", "to", "programmatically", "expand", "the", "docstring", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/util/docstring.py#L30-L51", "partition": "test", "index": 614, "time": "2018-03-07 11:12:11"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/hmc.py", "func_name": "HamiltonianMonteCarlo.bootstrap_results", "original_string": "def bootstrap_results(self, init_state):\n    \"\"\"Creates initial `previous_kernel_results` using a supplied `state`.\"\"\"\n    kernel_results = self._impl.bootstrap_results(init_state)\n    if self.step_size_update_fn is not None:\n      step_size_assign = self.step_size_update_fn(self.step_size, None)  # pylint: disable=not-callable\n      kernel_results = kernel_results._replace(\n          extra=HamiltonianMonteCarloExtraKernelResults(\n              step_size_assign=step_size_assign))\n    return kernel_results", "language": "python", "code": "def bootstrap_results(self, init_state):\n    \"\"\"Creates initial `previous_kernel_results` using a supplied `state`.\"\"\"\n    kernel_results = self._impl.bootstrap_results(init_state)\n    if self.step_size_update_fn is not None:\n      step_size_assign = self.step_size_update_fn(self.step_size, None)  # pylint: disable=not-callable\n      kernel_results = kernel_results._replace(\n          extra=HamiltonianMonteCarloExtraKernelResults(\n              step_size_assign=step_size_assign))\n    return kernel_results", "code_tokens": ["def", "bootstrap_results", "(", "self", ",", "init_state", ")", ":", "kernel_results", "=", "self", ".", "_impl", ".", "bootstrap_results", "(", "init_state", ")", "if", "self", ".", "step_size_update_fn", "is", "not", "None", ":", "step_size_assign", "=", "self", ".", "step_size_update_fn", "(", "self", ".", "step_size", ",", "None", ")", "# pylint: disable=not-callable", "kernel_results", "=", "kernel_results", ".", "_replace", "(", "extra", "=", "HamiltonianMonteCarloExtraKernelResults", "(", "step_size_assign", "=", "step_size_assign", ")", ")", "return", "kernel_results"], "docstring": "Creates initial `previous_kernel_results` using a supplied `state`.", "docstring_tokens": ["Creates", "initial", "previous_kernel_results", "using", "a", "supplied", "state", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/hmc.py#L556-L564", "partition": "test", "index": 1009, "time": "2018-03-08 14:14:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/custom_gradient.py", "func_name": "custom_gradient", "original_string": "def custom_gradient(fx, gx, x, fx_gx_manually_stopped=False, name=None):\n  \"\"\"Embeds a custom gradient into a `Tensor`.\n\n  This function works by clever application of `stop_gradient`. I.e., observe\n  that:\n\n  ```none\n  h(x) = stop_gradient(f(x)) + stop_gradient(g(x)) * (x - stop_gradient(x))\n  ```\n\n  is such that `h(x) == stop_gradient(f(x))` and\n  `grad[h(x), x] == stop_gradient(g(x)).`\n\n  In addition to scalar-domain/scalar-range functions, this function also\n  supports tensor-domain/scalar-range functions.\n\n  Partial Custom Gradient:\n\n  Suppose `h(x) = htilde(x, y)`. Note that `dh/dx = stop(g(x))` but `dh/dy =\n  None`. This is because a `Tensor` cannot have only a portion of its gradient\n  stopped. To circumvent this issue, one must manually `stop_gradient` the\n  relevant portions of `f`, `g`. For example see the unit-test,\n  `test_works_correctly_fx_gx_manually_stopped`.\n\n  Args:\n    fx: `Tensor`. Output of function evaluated at `x`.\n    gx: `Tensor` or list of `Tensor`s. Gradient of function at (each) `x`.\n    x: `Tensor` or list of `Tensor`s. Args of evaluation for `f`.\n    fx_gx_manually_stopped: Python `bool` indicating that `fx`, `gx` manually\n      have `stop_gradient` applied.\n    name: Python `str` name prefixed to Ops created by this function.\n\n  Returns:\n    fx: Floating-type `Tensor` equal to `f(x)` but which has gradient\n      `stop_gradient(g(x))`.\n  \"\"\"\n  def maybe_stop(x):\n    if fx_gx_manually_stopped:\n      return x\n    return tf.stop_gradient(x)\n\n  with tf.compat.v1.name_scope(name, 'custom_gradient', [fx, gx, x]):\n    fx = tf.convert_to_tensor(value=fx, name='fx')\n    # We don't want to bother eagerly computing `gx` since we may not even need\n    # it.\n    with tf.control_dependencies([fx]):\n      if is_list_like(x):\n        x = [identity(x_, name='x') for x_ in x]\n      else:\n        x = [identity(x, name='x')]\n\n      if is_list_like(gx):\n        gx = [identity(gx_, dtype=fx.dtype, name='gx')\n              for gx_ in gx]\n      else:\n        gx = [identity(gx, dtype=fx.dtype, name='gx')]\n\n      override_grad = []\n      for x_, gx_ in zip(x, gx):\n        # Observe: tf.gradients(f(x), x)[i].shape == x[i].shape\n        # thus we check that the user is supplying correct shapes.\n        equal_shape = tf.compat.v1.assert_equal(\n            tf.shape(input=x_),\n            tf.shape(input=gx_),\n            message='Each `x` must have the same shape as each `gx`.')\n        with tf.control_dependencies([equal_shape]):\n          # IEEE754 ensures `(x-x)==0.` and that `0.*x==0.` so we make sure to\n          # write the code this way, rather than, e.g.,\n          # `sum_x * stop(gx) + stop(fx - sum_x * gx)`.\n          # For more discussion regarding the relevant portions of the IEEE754\n          # standard, see the StackOverflow question,\n          # \"Is there a floating point value of x, for which x-x == 0 is false?\"\n          # http://stackoverflow.com/q/2686644\n          zeros_like_x_ = x_ - tf.stop_gradient(x_)\n          override_grad.append(\n              tf.reduce_sum(input_tensor=maybe_stop(gx_) * zeros_like_x_))\n      override_grad = sum(override_grad)\n      override_grad /= tf.cast(tf.size(input=fx), dtype=fx.dtype.base_dtype)\n\n      # Proof of correctness:\n      #\n      #  f(x) = x * stop[gx] + stop[fx - x * gx]\n      #       = stop[fx]\n      #\n      #  g(x) = grad[fx]\n      #       = stop[gx] + grad[stop[fx - x * gx]]\n      #       = stop[gx] + 0\n      #\n      # Notice that when x is zero it still works:\n      # grad[x * stop(gx) + stop(fx - x * gx)] = 1 * stop[gx] + 0 = stop[gx]\n      #\n      # The proof is similar for the tensor-domain case, except that we\n      # `reduce_sum` the `stop[gx] * (x - stop[x])` then rescale by\n      # `tf.size(fx)` since this reduced version is broadcast to `fx`.\n      return maybe_stop(fx) + override_grad", "language": "python", "code": "def custom_gradient(fx, gx, x, fx_gx_manually_stopped=False, name=None):\n  \"\"\"Embeds a custom gradient into a `Tensor`.\n\n  This function works by clever application of `stop_gradient`. I.e., observe\n  that:\n\n  ```none\n  h(x) = stop_gradient(f(x)) + stop_gradient(g(x)) * (x - stop_gradient(x))\n  ```\n\n  is such that `h(x) == stop_gradient(f(x))` and\n  `grad[h(x), x] == stop_gradient(g(x)).`\n\n  In addition to scalar-domain/scalar-range functions, this function also\n  supports tensor-domain/scalar-range functions.\n\n  Partial Custom Gradient:\n\n  Suppose `h(x) = htilde(x, y)`. Note that `dh/dx = stop(g(x))` but `dh/dy =\n  None`. This is because a `Tensor` cannot have only a portion of its gradient\n  stopped. To circumvent this issue, one must manually `stop_gradient` the\n  relevant portions of `f`, `g`. For example see the unit-test,\n  `test_works_correctly_fx_gx_manually_stopped`.\n\n  Args:\n    fx: `Tensor`. Output of function evaluated at `x`.\n    gx: `Tensor` or list of `Tensor`s. Gradient of function at (each) `x`.\n    x: `Tensor` or list of `Tensor`s. Args of evaluation for `f`.\n    fx_gx_manually_stopped: Python `bool` indicating that `fx`, `gx` manually\n      have `stop_gradient` applied.\n    name: Python `str` name prefixed to Ops created by this function.\n\n  Returns:\n    fx: Floating-type `Tensor` equal to `f(x)` but which has gradient\n      `stop_gradient(g(x))`.\n  \"\"\"\n  def maybe_stop(x):\n    if fx_gx_manually_stopped:\n      return x\n    return tf.stop_gradient(x)\n\n  with tf.compat.v1.name_scope(name, 'custom_gradient', [fx, gx, x]):\n    fx = tf.convert_to_tensor(value=fx, name='fx')\n    # We don't want to bother eagerly computing `gx` since we may not even need\n    # it.\n    with tf.control_dependencies([fx]):\n      if is_list_like(x):\n        x = [identity(x_, name='x') for x_ in x]\n      else:\n        x = [identity(x, name='x')]\n\n      if is_list_like(gx):\n        gx = [identity(gx_, dtype=fx.dtype, name='gx')\n              for gx_ in gx]\n      else:\n        gx = [identity(gx, dtype=fx.dtype, name='gx')]\n\n      override_grad = []\n      for x_, gx_ in zip(x, gx):\n        # Observe: tf.gradients(f(x), x)[i].shape == x[i].shape\n        # thus we check that the user is supplying correct shapes.\n        equal_shape = tf.compat.v1.assert_equal(\n            tf.shape(input=x_),\n            tf.shape(input=gx_),\n            message='Each `x` must have the same shape as each `gx`.')\n        with tf.control_dependencies([equal_shape]):\n          # IEEE754 ensures `(x-x)==0.` and that `0.*x==0.` so we make sure to\n          # write the code this way, rather than, e.g.,\n          # `sum_x * stop(gx) + stop(fx - sum_x * gx)`.\n          # For more discussion regarding the relevant portions of the IEEE754\n          # standard, see the StackOverflow question,\n          # \"Is there a floating point value of x, for which x-x == 0 is false?\"\n          # http://stackoverflow.com/q/2686644\n          zeros_like_x_ = x_ - tf.stop_gradient(x_)\n          override_grad.append(\n              tf.reduce_sum(input_tensor=maybe_stop(gx_) * zeros_like_x_))\n      override_grad = sum(override_grad)\n      override_grad /= tf.cast(tf.size(input=fx), dtype=fx.dtype.base_dtype)\n\n      # Proof of correctness:\n      #\n      #  f(x) = x * stop[gx] + stop[fx - x * gx]\n      #       = stop[fx]\n      #\n      #  g(x) = grad[fx]\n      #       = stop[gx] + grad[stop[fx - x * gx]]\n      #       = stop[gx] + 0\n      #\n      # Notice that when x is zero it still works:\n      # grad[x * stop(gx) + stop(fx - x * gx)] = 1 * stop[gx] + 0 = stop[gx]\n      #\n      # The proof is similar for the tensor-domain case, except that we\n      # `reduce_sum` the `stop[gx] * (x - stop[x])` then rescale by\n      # `tf.size(fx)` since this reduced version is broadcast to `fx`.\n      return maybe_stop(fx) + override_grad", "code_tokens": ["def", "custom_gradient", "(", "fx", ",", "gx", ",", "x", ",", "fx_gx_manually_stopped", "=", "False", ",", "name", "=", "None", ")", ":", "def", "maybe_stop", "(", "x", ")", ":", "if", "fx_gx_manually_stopped", ":", "return", "x", "return", "tf", ".", "stop_gradient", "(", "x", ")", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'custom_gradient'", ",", "[", "fx", ",", "gx", ",", "x", "]", ")", ":", "fx", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "fx", ",", "name", "=", "'fx'", ")", "# We don't want to bother eagerly computing `gx` since we may not even need", "# it.", "with", "tf", ".", "control_dependencies", "(", "[", "fx", "]", ")", ":", "if", "is_list_like", "(", "x", ")", ":", "x", "=", "[", "identity", "(", "x_", ",", "name", "=", "'x'", ")", "for", "x_", "in", "x", "]", "else", ":", "x", "=", "[", "identity", "(", "x", ",", "name", "=", "'x'", ")", "]", "if", "is_list_like", "(", "gx", ")", ":", "gx", "=", "[", "identity", "(", "gx_", ",", "dtype", "=", "fx", ".", "dtype", ",", "name", "=", "'gx'", ")", "for", "gx_", "in", "gx", "]", "else", ":", "gx", "=", "[", "identity", "(", "gx", ",", "dtype", "=", "fx", ".", "dtype", ",", "name", "=", "'gx'", ")", "]", "override_grad", "=", "[", "]", "for", "x_", ",", "gx_", "in", "zip", "(", "x", ",", "gx", ")", ":", "# Observe: tf.gradients(f(x), x)[i].shape == x[i].shape", "# thus we check that the user is supplying correct shapes.", "equal_shape", "=", "tf", ".", "compat", ".", "v1", ".", "assert_equal", "(", "tf", ".", "shape", "(", "input", "=", "x_", ")", ",", "tf", ".", "shape", "(", "input", "=", "gx_", ")", ",", "message", "=", "'Each `x` must have the same shape as each `gx`.'", ")", "with", "tf", ".", "control_dependencies", "(", "[", "equal_shape", "]", ")", ":", "# IEEE754 ensures `(x-x)==0.` and that `0.*x==0.` so we make sure to", "# write the code this way, rather than, e.g.,", "# `sum_x * stop(gx) + stop(fx - sum_x * gx)`.", "# For more discussion regarding the relevant portions of the IEEE754", "# standard, see the StackOverflow question,", "# \"Is there a floating point value of x, for which x-x == 0 is false?\"", "# http://stackoverflow.com/q/2686644", "zeros_like_x_", "=", "x_", "-", "tf", ".", "stop_gradient", "(", "x_", ")", "override_grad", ".", "append", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "maybe_stop", "(", "gx_", ")", "*", "zeros_like_x_", ")", ")", "override_grad", "=", "sum", "(", "override_grad", ")", "override_grad", "/=", "tf", ".", "cast", "(", "tf", ".", "size", "(", "input", "=", "fx", ")", ",", "dtype", "=", "fx", ".", "dtype", ".", "base_dtype", ")", "# Proof of correctness:", "#", "#  f(x) = x * stop[gx] + stop[fx - x * gx]", "#       = stop[fx]", "#", "#  g(x) = grad[fx]", "#       = stop[gx] + grad[stop[fx - x * gx]]", "#       = stop[gx] + 0", "#", "# Notice that when x is zero it still works:", "# grad[x * stop(gx) + stop(fx - x * gx)] = 1 * stop[gx] + 0 = stop[gx]", "#", "# The proof is similar for the tensor-domain case, except that we", "# `reduce_sum` the `stop[gx] * (x - stop[x])` then rescale by", "# `tf.size(fx)` since this reduced version is broadcast to `fx`.", "return", "maybe_stop", "(", "fx", ")", "+", "override_grad"], "docstring": "Embeds a custom gradient into a `Tensor`.\n\n  This function works by clever application of `stop_gradient`. I.e., observe\n  that:\n\n  ```none\n  h(x) = stop_gradient(f(x)) + stop_gradient(g(x)) * (x - stop_gradient(x))\n  ```\n\n  is such that `h(x) == stop_gradient(f(x))` and\n  `grad[h(x), x] == stop_gradient(g(x)).`\n\n  In addition to scalar-domain/scalar-range functions, this function also\n  supports tensor-domain/scalar-range functions.\n\n  Partial Custom Gradient:\n\n  Suppose `h(x) = htilde(x, y)`. Note that `dh/dx = stop(g(x))` but `dh/dy =\n  None`. This is because a `Tensor` cannot have only a portion of its gradient\n  stopped. To circumvent this issue, one must manually `stop_gradient` the\n  relevant portions of `f`, `g`. For example see the unit-test,\n  `test_works_correctly_fx_gx_manually_stopped`.\n\n  Args:\n    fx: `Tensor`. Output of function evaluated at `x`.\n    gx: `Tensor` or list of `Tensor`s. Gradient of function at (each) `x`.\n    x: `Tensor` or list of `Tensor`s. Args of evaluation for `f`.\n    fx_gx_manually_stopped: Python `bool` indicating that `fx`, `gx` manually\n      have `stop_gradient` applied.\n    name: Python `str` name prefixed to Ops created by this function.\n\n  Returns:\n    fx: Floating-type `Tensor` equal to `f(x)` but which has gradient\n      `stop_gradient(g(x))`.", "docstring_tokens": ["Embeds", "a", "custom", "gradient", "into", "a", "Tensor", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/custom_gradient.py#L39-L133", "partition": "test", "index": 610, "time": "2018-03-12 12:58:49"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/trainable_distributions/trainable_distributions_lib.py", "func_name": "normal", "original_string": "def normal(x,\n           layer_fn=tf.compat.v1.layers.dense,\n           loc_fn=lambda x: x,\n           scale_fn=1.,\n           name=None):\n  \"\"\"Constructs a trainable `tfd.Normal` distribution.\n\n\n  This function creates a Normal distribution parameterized by loc and scale.\n  Using default args, this function is mathematically equivalent to:\n\n  ```none\n  Y = Normal(loc=matmul(W, x) + b, scale=1)\n\n  where,\n    W in R^[d, n]\n    b in R^d\n  ```\n\n  #### Examples\n\n  This function can be used as a [linear regression](\n  https://en.wikipedia.org/wiki/Linear_regression) loss.\n\n  ```python\n  # This example fits a linear regression loss.\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  # Create fictitious training data.\n  dtype = np.float32\n  n = 3000    # number of samples\n  x_size = 4  # size of single x\n  def make_training_data():\n    np.random.seed(142)\n    x = np.random.randn(n, x_size).astype(dtype)\n    w = np.random.randn(x_size).astype(dtype)\n    b = np.random.randn(1).astype(dtype)\n    true_mean = np.tensordot(x, w, axes=[[-1], [-1]]) + b\n    noise = np.random.randn(n).astype(dtype)\n    y = true_mean + noise\n    return y, x\n  y, x = make_training_data()\n\n  # Build TF graph for fitting Normal maximum likelihood estimator.\n  normal = tfp.trainable_distributions.normal(x)\n  loss = -tf.reduce_mean(normal.log_prob(y))\n  train_op = tf.train.AdamOptimizer(learning_rate=2.**-5).minimize(loss)\n  mse = tf.reduce_mean(tf.squared_difference(y, normal.mean()))\n  init_op = tf.global_variables_initializer()\n\n  # Run graph 1000 times.\n  num_steps = 1000\n  loss_ = np.zeros(num_steps)   # Style: `_` to indicate sess.run result.\n  mse_ = np.zeros(num_steps)\n  with tf.Session() as sess:\n    sess.run(init_op)\n    for it in xrange(loss_.size):\n      _, loss_[it], mse_[it] = sess.run([train_op, loss, mse])\n      if it % 200 == 0 or it == loss_.size - 1:\n        print(\"iteration:{}  loss:{}  mse:{}\".format(it, loss_[it], mse_[it]))\n\n  # ==> iteration:0    loss:6.34114170074  mse:10.8444051743\n  #     iteration:200  loss:1.40146839619  mse:0.965059816837\n  #     iteration:400  loss:1.40052902699  mse:0.963181257248\n  #     iteration:600  loss:1.40052902699  mse:0.963181257248\n  #     iteration:800  loss:1.40052902699  mse:0.963181257248\n  #     iteration:999  loss:1.40052902699  mse:0.963181257248\n  ```\n\n  Args:\n    x: `Tensor` with floating type. Must have statically defined rank and\n      statically known right-most dimension.\n    layer_fn: Python `callable` which takes input `x` and `int` scalar `d` and\n      returns a transformation of `x` with shape\n      `tf.concat([tf.shape(x)[:-1], [1]], axis=0)`.\n      Default value: `tf.layers.dense`.\n    loc_fn: Python `callable` which transforms the `loc` parameter. Takes a\n      (batch of) length-`dims` vectors and returns a `Tensor` of same shape and\n      `dtype`.\n      Default value: `lambda x: x`.\n    scale_fn: Python `callable` or `Tensor`. If a `callable` transforms the\n      `scale` parameters; if `Tensor` is the `tfd.Normal` `scale` argument.\n      Takes a (batch of) length-`dims` vectors and returns a `Tensor` of same\n      size. (Taking a `callable` or `Tensor` is how `tf.Variable` intializers\n      behave.)\n      Default value: `1`.\n    name: A `name_scope` name for operations created by this function.\n      Default value: `None` (i.e., \"normal\").\n\n  Returns:\n    normal: An instance of `tfd.Normal`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'normal', [x]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    if callable(scale_fn):\n      y = layer_fn(x, 2)\n      loc = loc_fn(y[..., 0])\n      scale = scale_fn(y[..., 1])\n    else:\n      y = tf.squeeze(layer_fn(x, 1), axis=-1)\n      loc = loc_fn(y)\n      scale = tf.cast(scale_fn, loc.dtype.base_dtype)\n    return tfd.Normal(loc=loc, scale=scale)", "language": "python", "code": "def normal(x,\n           layer_fn=tf.compat.v1.layers.dense,\n           loc_fn=lambda x: x,\n           scale_fn=1.,\n           name=None):\n  \"\"\"Constructs a trainable `tfd.Normal` distribution.\n\n\n  This function creates a Normal distribution parameterized by loc and scale.\n  Using default args, this function is mathematically equivalent to:\n\n  ```none\n  Y = Normal(loc=matmul(W, x) + b, scale=1)\n\n  where,\n    W in R^[d, n]\n    b in R^d\n  ```\n\n  #### Examples\n\n  This function can be used as a [linear regression](\n  https://en.wikipedia.org/wiki/Linear_regression) loss.\n\n  ```python\n  # This example fits a linear regression loss.\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  # Create fictitious training data.\n  dtype = np.float32\n  n = 3000    # number of samples\n  x_size = 4  # size of single x\n  def make_training_data():\n    np.random.seed(142)\n    x = np.random.randn(n, x_size).astype(dtype)\n    w = np.random.randn(x_size).astype(dtype)\n    b = np.random.randn(1).astype(dtype)\n    true_mean = np.tensordot(x, w, axes=[[-1], [-1]]) + b\n    noise = np.random.randn(n).astype(dtype)\n    y = true_mean + noise\n    return y, x\n  y, x = make_training_data()\n\n  # Build TF graph for fitting Normal maximum likelihood estimator.\n  normal = tfp.trainable_distributions.normal(x)\n  loss = -tf.reduce_mean(normal.log_prob(y))\n  train_op = tf.train.AdamOptimizer(learning_rate=2.**-5).minimize(loss)\n  mse = tf.reduce_mean(tf.squared_difference(y, normal.mean()))\n  init_op = tf.global_variables_initializer()\n\n  # Run graph 1000 times.\n  num_steps = 1000\n  loss_ = np.zeros(num_steps)   # Style: `_` to indicate sess.run result.\n  mse_ = np.zeros(num_steps)\n  with tf.Session() as sess:\n    sess.run(init_op)\n    for it in xrange(loss_.size):\n      _, loss_[it], mse_[it] = sess.run([train_op, loss, mse])\n      if it % 200 == 0 or it == loss_.size - 1:\n        print(\"iteration:{}  loss:{}  mse:{}\".format(it, loss_[it], mse_[it]))\n\n  # ==> iteration:0    loss:6.34114170074  mse:10.8444051743\n  #     iteration:200  loss:1.40146839619  mse:0.965059816837\n  #     iteration:400  loss:1.40052902699  mse:0.963181257248\n  #     iteration:600  loss:1.40052902699  mse:0.963181257248\n  #     iteration:800  loss:1.40052902699  mse:0.963181257248\n  #     iteration:999  loss:1.40052902699  mse:0.963181257248\n  ```\n\n  Args:\n    x: `Tensor` with floating type. Must have statically defined rank and\n      statically known right-most dimension.\n    layer_fn: Python `callable` which takes input `x` and `int` scalar `d` and\n      returns a transformation of `x` with shape\n      `tf.concat([tf.shape(x)[:-1], [1]], axis=0)`.\n      Default value: `tf.layers.dense`.\n    loc_fn: Python `callable` which transforms the `loc` parameter. Takes a\n      (batch of) length-`dims` vectors and returns a `Tensor` of same shape and\n      `dtype`.\n      Default value: `lambda x: x`.\n    scale_fn: Python `callable` or `Tensor`. If a `callable` transforms the\n      `scale` parameters; if `Tensor` is the `tfd.Normal` `scale` argument.\n      Takes a (batch of) length-`dims` vectors and returns a `Tensor` of same\n      size. (Taking a `callable` or `Tensor` is how `tf.Variable` intializers\n      behave.)\n      Default value: `1`.\n    name: A `name_scope` name for operations created by this function.\n      Default value: `None` (i.e., \"normal\").\n\n  Returns:\n    normal: An instance of `tfd.Normal`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'normal', [x]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    if callable(scale_fn):\n      y = layer_fn(x, 2)\n      loc = loc_fn(y[..., 0])\n      scale = scale_fn(y[..., 1])\n    else:\n      y = tf.squeeze(layer_fn(x, 1), axis=-1)\n      loc = loc_fn(y)\n      scale = tf.cast(scale_fn, loc.dtype.base_dtype)\n    return tfd.Normal(loc=loc, scale=scale)", "code_tokens": ["def", "normal", "(", "x", ",", "layer_fn", "=", "tf", ".", "compat", ".", "v1", ".", "layers", ".", "dense", ",", "loc_fn", "=", "lambda", "x", ":", "x", ",", "scale_fn", "=", "1.", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'normal'", ",", "[", "x", "]", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ")", "if", "callable", "(", "scale_fn", ")", ":", "y", "=", "layer_fn", "(", "x", ",", "2", ")", "loc", "=", "loc_fn", "(", "y", "[", "...", ",", "0", "]", ")", "scale", "=", "scale_fn", "(", "y", "[", "...", ",", "1", "]", ")", "else", ":", "y", "=", "tf", ".", "squeeze", "(", "layer_fn", "(", "x", ",", "1", ")", ",", "axis", "=", "-", "1", ")", "loc", "=", "loc_fn", "(", "y", ")", "scale", "=", "tf", ".", "cast", "(", "scale_fn", ",", "loc", ".", "dtype", ".", "base_dtype", ")", "return", "tfd", ".", "Normal", "(", "loc", "=", "loc", ",", "scale", "=", "scale", ")"], "docstring": "Constructs a trainable `tfd.Normal` distribution.\n\n\n  This function creates a Normal distribution parameterized by loc and scale.\n  Using default args, this function is mathematically equivalent to:\n\n  ```none\n  Y = Normal(loc=matmul(W, x) + b, scale=1)\n\n  where,\n    W in R^[d, n]\n    b in R^d\n  ```\n\n  #### Examples\n\n  This function can be used as a [linear regression](\n  https://en.wikipedia.org/wiki/Linear_regression) loss.\n\n  ```python\n  # This example fits a linear regression loss.\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  # Create fictitious training data.\n  dtype = np.float32\n  n = 3000    # number of samples\n  x_size = 4  # size of single x\n  def make_training_data():\n    np.random.seed(142)\n    x = np.random.randn(n, x_size).astype(dtype)\n    w = np.random.randn(x_size).astype(dtype)\n    b = np.random.randn(1).astype(dtype)\n    true_mean = np.tensordot(x, w, axes=[[-1], [-1]]) + b\n    noise = np.random.randn(n).astype(dtype)\n    y = true_mean + noise\n    return y, x\n  y, x = make_training_data()\n\n  # Build TF graph for fitting Normal maximum likelihood estimator.\n  normal = tfp.trainable_distributions.normal(x)\n  loss = -tf.reduce_mean(normal.log_prob(y))\n  train_op = tf.train.AdamOptimizer(learning_rate=2.**-5).minimize(loss)\n  mse = tf.reduce_mean(tf.squared_difference(y, normal.mean()))\n  init_op = tf.global_variables_initializer()\n\n  # Run graph 1000 times.\n  num_steps = 1000\n  loss_ = np.zeros(num_steps)   # Style: `_` to indicate sess.run result.\n  mse_ = np.zeros(num_steps)\n  with tf.Session() as sess:\n    sess.run(init_op)\n    for it in xrange(loss_.size):\n      _, loss_[it], mse_[it] = sess.run([train_op, loss, mse])\n      if it % 200 == 0 or it == loss_.size - 1:\n        print(\"iteration:{}  loss:{}  mse:{}\".format(it, loss_[it], mse_[it]))\n\n  # ==> iteration:0    loss:6.34114170074  mse:10.8444051743\n  #     iteration:200  loss:1.40146839619  mse:0.965059816837\n  #     iteration:400  loss:1.40052902699  mse:0.963181257248\n  #     iteration:600  loss:1.40052902699  mse:0.963181257248\n  #     iteration:800  loss:1.40052902699  mse:0.963181257248\n  #     iteration:999  loss:1.40052902699  mse:0.963181257248\n  ```\n\n  Args:\n    x: `Tensor` with floating type. Must have statically defined rank and\n      statically known right-most dimension.\n    layer_fn: Python `callable` which takes input `x` and `int` scalar `d` and\n      returns a transformation of `x` with shape\n      `tf.concat([tf.shape(x)[:-1], [1]], axis=0)`.\n      Default value: `tf.layers.dense`.\n    loc_fn: Python `callable` which transforms the `loc` parameter. Takes a\n      (batch of) length-`dims` vectors and returns a `Tensor` of same shape and\n      `dtype`.\n      Default value: `lambda x: x`.\n    scale_fn: Python `callable` or `Tensor`. If a `callable` transforms the\n      `scale` parameters; if `Tensor` is the `tfd.Normal` `scale` argument.\n      Takes a (batch of) length-`dims` vectors and returns a `Tensor` of same\n      size. (Taking a `callable` or `Tensor` is how `tf.Variable` intializers\n      behave.)\n      Default value: `1`.\n    name: A `name_scope` name for operations created by this function.\n      Default value: `None` (i.e., \"normal\").\n\n  Returns:\n    normal: An instance of `tfd.Normal`.", "docstring_tokens": ["Constructs", "a", "trainable", "tfd", ".", "Normal", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/trainable_distributions/trainable_distributions_lib.py#L284-L387", "partition": "test", "index": 825, "time": "2018-03-14 17:00:41"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/trainable_distributions/trainable_distributions_lib.py", "func_name": "poisson", "original_string": "def poisson(x,\n            layer_fn=tf.compat.v1.layers.dense,\n            log_rate_fn=lambda x: x,\n            name=None):\n  \"\"\"Constructs a trainable `tfd.Poisson` distribution.\n\n  This function creates a Poisson distribution parameterized by log rate.\n  Using default args, this function is mathematically equivalent to:\n\n  ```none\n  Y = Poisson(log_rate=matmul(W, x) + b)\n\n  where,\n    W in R^[d, n]\n    b in R^d\n  ```\n\n  #### Examples\n\n  This can be used as a [Poisson regression](\n  https://en.wikipedia.org/wiki/Poisson_regression) loss.\n\n  ```python\n  # This example fits a poisson regression loss.\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  # Create fictitious training data.\n  dtype = np.float32\n  n = 3000    # number of samples\n  x_size = 4  # size of single x\n  def make_training_data():\n    np.random.seed(142)\n    x = np.random.randn(n, x_size).astype(dtype)\n    w = np.random.randn(x_size).astype(dtype)\n    b = np.random.randn(1).astype(dtype)\n    true_log_rate = np.tensordot(x, w, axes=[[-1], [-1]]) + b\n    y = np.random.poisson(lam=np.exp(true_log_rate)).astype(dtype)\n    return y, x\n  y, x = make_training_data()\n\n  # Build TF graph for fitting Poisson maximum likelihood estimator.\n  poisson = tfp.trainable_distributions.poisson(x)\n  loss = -tf.reduce_mean(poisson.log_prob(y))\n  train_op = tf.train.AdamOptimizer(learning_rate=2.**-5).minimize(loss)\n  mse = tf.reduce_mean(tf.squared_difference(y, poisson.mean()))\n  init_op = tf.global_variables_initializer()\n\n  # Run graph 1000 times.\n  num_steps = 1000\n  loss_ = np.zeros(num_steps)   # Style: `_` to indicate sess.run result.\n  mse_ = np.zeros(num_steps)\n  with tf.Session() as sess:\n    sess.run(init_op)\n    for it in xrange(loss_.size):\n      _, loss_[it], mse_[it] = sess.run([train_op, loss, mse])\n      if it % 200 == 0 or it == loss_.size - 1:\n        print(\"iteration:{}  loss:{}  mse:{}\".format(it, loss_[it], mse_[it]))\n\n  # ==> iteration:0    loss:37.0814208984  mse:6359.41259766\n  #     iteration:200  loss:1.42010736465  mse:40.7654914856\n  #     iteration:400  loss:1.39027583599  mse:8.77660560608\n  #     iteration:600  loss:1.3902695179   mse:8.78443241119\n  #     iteration:800  loss:1.39026939869  mse:8.78443622589\n  #     iteration:999  loss:1.39026939869  mse:8.78444766998\n  ```\n\n  Args:\n    x: `Tensor` with floating type. Must have statically defined rank and\n      statically known right-most dimension.\n    layer_fn: Python `callable` which takes input `x` and `int` scalar `d` and\n      returns a transformation of `x` with shape\n      `tf.concat([tf.shape(x)[:-1], [1]], axis=0)`.\n      Default value: `tf.layers.dense`.\n    log_rate_fn: Python `callable` which transforms the `log_rate` parameter.\n      Takes a (batch of) length-`dims` vectors and returns a `Tensor` of same\n      shape and `dtype`.\n      Default value: `lambda x: x`.\n    name: A `name_scope` name for operations created by this function.\n      Default value: `None` (i.e., \"poisson\").\n\n  Returns:\n    poisson: An instance of `tfd.Poisson`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'poisson', [x]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    log_rate = log_rate_fn(tf.squeeze(layer_fn(x, 1), axis=-1))\n    return tfd.Poisson(log_rate=log_rate)", "language": "python", "code": "def poisson(x,\n            layer_fn=tf.compat.v1.layers.dense,\n            log_rate_fn=lambda x: x,\n            name=None):\n  \"\"\"Constructs a trainable `tfd.Poisson` distribution.\n\n  This function creates a Poisson distribution parameterized by log rate.\n  Using default args, this function is mathematically equivalent to:\n\n  ```none\n  Y = Poisson(log_rate=matmul(W, x) + b)\n\n  where,\n    W in R^[d, n]\n    b in R^d\n  ```\n\n  #### Examples\n\n  This can be used as a [Poisson regression](\n  https://en.wikipedia.org/wiki/Poisson_regression) loss.\n\n  ```python\n  # This example fits a poisson regression loss.\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  # Create fictitious training data.\n  dtype = np.float32\n  n = 3000    # number of samples\n  x_size = 4  # size of single x\n  def make_training_data():\n    np.random.seed(142)\n    x = np.random.randn(n, x_size).astype(dtype)\n    w = np.random.randn(x_size).astype(dtype)\n    b = np.random.randn(1).astype(dtype)\n    true_log_rate = np.tensordot(x, w, axes=[[-1], [-1]]) + b\n    y = np.random.poisson(lam=np.exp(true_log_rate)).astype(dtype)\n    return y, x\n  y, x = make_training_data()\n\n  # Build TF graph for fitting Poisson maximum likelihood estimator.\n  poisson = tfp.trainable_distributions.poisson(x)\n  loss = -tf.reduce_mean(poisson.log_prob(y))\n  train_op = tf.train.AdamOptimizer(learning_rate=2.**-5).minimize(loss)\n  mse = tf.reduce_mean(tf.squared_difference(y, poisson.mean()))\n  init_op = tf.global_variables_initializer()\n\n  # Run graph 1000 times.\n  num_steps = 1000\n  loss_ = np.zeros(num_steps)   # Style: `_` to indicate sess.run result.\n  mse_ = np.zeros(num_steps)\n  with tf.Session() as sess:\n    sess.run(init_op)\n    for it in xrange(loss_.size):\n      _, loss_[it], mse_[it] = sess.run([train_op, loss, mse])\n      if it % 200 == 0 or it == loss_.size - 1:\n        print(\"iteration:{}  loss:{}  mse:{}\".format(it, loss_[it], mse_[it]))\n\n  # ==> iteration:0    loss:37.0814208984  mse:6359.41259766\n  #     iteration:200  loss:1.42010736465  mse:40.7654914856\n  #     iteration:400  loss:1.39027583599  mse:8.77660560608\n  #     iteration:600  loss:1.3902695179   mse:8.78443241119\n  #     iteration:800  loss:1.39026939869  mse:8.78443622589\n  #     iteration:999  loss:1.39026939869  mse:8.78444766998\n  ```\n\n  Args:\n    x: `Tensor` with floating type. Must have statically defined rank and\n      statically known right-most dimension.\n    layer_fn: Python `callable` which takes input `x` and `int` scalar `d` and\n      returns a transformation of `x` with shape\n      `tf.concat([tf.shape(x)[:-1], [1]], axis=0)`.\n      Default value: `tf.layers.dense`.\n    log_rate_fn: Python `callable` which transforms the `log_rate` parameter.\n      Takes a (batch of) length-`dims` vectors and returns a `Tensor` of same\n      shape and `dtype`.\n      Default value: `lambda x: x`.\n    name: A `name_scope` name for operations created by this function.\n      Default value: `None` (i.e., \"poisson\").\n\n  Returns:\n    poisson: An instance of `tfd.Poisson`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'poisson', [x]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    log_rate = log_rate_fn(tf.squeeze(layer_fn(x, 1), axis=-1))\n    return tfd.Poisson(log_rate=log_rate)", "code_tokens": ["def", "poisson", "(", "x", ",", "layer_fn", "=", "tf", ".", "compat", ".", "v1", ".", "layers", ".", "dense", ",", "log_rate_fn", "=", "lambda", "x", ":", "x", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'poisson'", ",", "[", "x", "]", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ")", "log_rate", "=", "log_rate_fn", "(", "tf", ".", "squeeze", "(", "layer_fn", "(", "x", ",", "1", ")", ",", "axis", "=", "-", "1", ")", ")", "return", "tfd", ".", "Poisson", "(", "log_rate", "=", "log_rate", ")"], "docstring": "Constructs a trainable `tfd.Poisson` distribution.\n\n  This function creates a Poisson distribution parameterized by log rate.\n  Using default args, this function is mathematically equivalent to:\n\n  ```none\n  Y = Poisson(log_rate=matmul(W, x) + b)\n\n  where,\n    W in R^[d, n]\n    b in R^d\n  ```\n\n  #### Examples\n\n  This can be used as a [Poisson regression](\n  https://en.wikipedia.org/wiki/Poisson_regression) loss.\n\n  ```python\n  # This example fits a poisson regression loss.\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  # Create fictitious training data.\n  dtype = np.float32\n  n = 3000    # number of samples\n  x_size = 4  # size of single x\n  def make_training_data():\n    np.random.seed(142)\n    x = np.random.randn(n, x_size).astype(dtype)\n    w = np.random.randn(x_size).astype(dtype)\n    b = np.random.randn(1).astype(dtype)\n    true_log_rate = np.tensordot(x, w, axes=[[-1], [-1]]) + b\n    y = np.random.poisson(lam=np.exp(true_log_rate)).astype(dtype)\n    return y, x\n  y, x = make_training_data()\n\n  # Build TF graph for fitting Poisson maximum likelihood estimator.\n  poisson = tfp.trainable_distributions.poisson(x)\n  loss = -tf.reduce_mean(poisson.log_prob(y))\n  train_op = tf.train.AdamOptimizer(learning_rate=2.**-5).minimize(loss)\n  mse = tf.reduce_mean(tf.squared_difference(y, poisson.mean()))\n  init_op = tf.global_variables_initializer()\n\n  # Run graph 1000 times.\n  num_steps = 1000\n  loss_ = np.zeros(num_steps)   # Style: `_` to indicate sess.run result.\n  mse_ = np.zeros(num_steps)\n  with tf.Session() as sess:\n    sess.run(init_op)\n    for it in xrange(loss_.size):\n      _, loss_[it], mse_[it] = sess.run([train_op, loss, mse])\n      if it % 200 == 0 or it == loss_.size - 1:\n        print(\"iteration:{}  loss:{}  mse:{}\".format(it, loss_[it], mse_[it]))\n\n  # ==> iteration:0    loss:37.0814208984  mse:6359.41259766\n  #     iteration:200  loss:1.42010736465  mse:40.7654914856\n  #     iteration:400  loss:1.39027583599  mse:8.77660560608\n  #     iteration:600  loss:1.3902695179   mse:8.78443241119\n  #     iteration:800  loss:1.39026939869  mse:8.78443622589\n  #     iteration:999  loss:1.39026939869  mse:8.78444766998\n  ```\n\n  Args:\n    x: `Tensor` with floating type. Must have statically defined rank and\n      statically known right-most dimension.\n    layer_fn: Python `callable` which takes input `x` and `int` scalar `d` and\n      returns a transformation of `x` with shape\n      `tf.concat([tf.shape(x)[:-1], [1]], axis=0)`.\n      Default value: `tf.layers.dense`.\n    log_rate_fn: Python `callable` which transforms the `log_rate` parameter.\n      Takes a (batch of) length-`dims` vectors and returns a `Tensor` of same\n      shape and `dtype`.\n      Default value: `lambda x: x`.\n    name: A `name_scope` name for operations created by this function.\n      Default value: `None` (i.e., \"poisson\").\n\n  Returns:\n    poisson: An instance of `tfd.Poisson`.", "docstring_tokens": ["Constructs", "a", "trainable", "tfd", ".", "Poisson", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/trainable_distributions/trainable_distributions_lib.py#L390-L478", "partition": "test", "index": 826, "time": "2018-03-14 17:00:41"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/random_variable.py", "func_name": "_numpy_text", "original_string": "def _numpy_text(tensor, is_repr=False):\n  \"\"\"Human-readable representation of a tensor's numpy value.\"\"\"\n  if tensor.dtype.is_numpy_compatible:\n    text = repr(tensor.numpy()) if is_repr else str(tensor.numpy())\n  else:\n    text = \"<unprintable>\"\n  if \"\\n\" in text:\n    text = \"\\n\" + text\n  return text", "language": "python", "code": "def _numpy_text(tensor, is_repr=False):\n  \"\"\"Human-readable representation of a tensor's numpy value.\"\"\"\n  if tensor.dtype.is_numpy_compatible:\n    text = repr(tensor.numpy()) if is_repr else str(tensor.numpy())\n  else:\n    text = \"<unprintable>\"\n  if \"\\n\" in text:\n    text = \"\\n\" + text\n  return text", "code_tokens": ["def", "_numpy_text", "(", "tensor", ",", "is_repr", "=", "False", ")", ":", "if", "tensor", ".", "dtype", ".", "is_numpy_compatible", ":", "text", "=", "repr", "(", "tensor", ".", "numpy", "(", ")", ")", "if", "is_repr", "else", "str", "(", "tensor", ".", "numpy", "(", ")", ")", "else", ":", "text", "=", "\"<unprintable>\"", "if", "\"\\n\"", "in", "text", ":", "text", "=", "\"\\n\"", "+", "text", "return", "text"], "docstring": "Human-readable representation of a tensor's numpy value.", "docstring_tokens": ["Human", "-", "readable", "representation", "of", "a", "tensor", "s", "numpy", "value", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/random_variable.py#L287-L295", "partition": "test", "index": 766, "time": "2018-03-16 11:51:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/random_variable.py", "func_name": "RandomVariable.numpy", "original_string": "def numpy(self):\n    \"\"\"Value as NumPy array, only available for TF Eager.\"\"\"\n    if not isinstance(self.value, ops.EagerTensor):\n      raise NotImplementedError(\"value argument must be a EagerTensor.\")\n\n    return self.value.numpy()", "language": "python", "code": "def numpy(self):\n    \"\"\"Value as NumPy array, only available for TF Eager.\"\"\"\n    if not isinstance(self.value, ops.EagerTensor):\n      raise NotImplementedError(\"value argument must be a EagerTensor.\")\n\n    return self.value.numpy()", "code_tokens": ["def", "numpy", "(", "self", ")", ":", "if", "not", "isinstance", "(", "self", ".", "value", ",", "ops", ".", "EagerTensor", ")", ":", "raise", "NotImplementedError", "(", "\"value argument must be a EagerTensor.\"", ")", "return", "self", ".", "value", ".", "numpy", "(", ")"], "docstring": "Value as NumPy array, only available for TF Eager.", "docstring_tokens": ["Value", "as", "NumPy", "array", "only", "available", "for", "TF", "Eager", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/random_variable.py#L270-L275", "partition": "test", "index": 771, "time": "2018-03-16 11:51:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/random_variable.py", "func_name": "RandomVariable.eval", "original_string": "def eval(self, session=None, feed_dict=None):\n    \"\"\"In a session, computes and returns the value of this random variable.\n\n    This is not a graph construction method, it does not add ops to the graph.\n\n    This convenience method requires a session where the graph\n    containing this variable has been launched. If no session is\n    passed, the default session is used.\n\n    Args:\n      session: tf.BaseSession.\n        The `tf.Session` to use to evaluate this random variable. If\n        none, the default session is used.\n      feed_dict: dict.\n        A dictionary that maps `tf.Tensor` objects to feed values. See\n        `tf.Session.run()` for a description of the valid feed values.\n\n    Returns:\n      Value of the random variable.\n\n    #### Examples\n\n    ```python\n    x = Normal(0.0, 1.0)\n    with tf.Session() as sess:\n      # Usage passing the session explicitly.\n      print(x.eval(sess))\n      # Usage with the default session.  The 'with' block\n      # above makes 'sess' the default session.\n      print(x.eval())\n    ```\n    \"\"\"\n    return self.value.eval(session=session, feed_dict=feed_dict)", "language": "python", "code": "def eval(self, session=None, feed_dict=None):\n    \"\"\"In a session, computes and returns the value of this random variable.\n\n    This is not a graph construction method, it does not add ops to the graph.\n\n    This convenience method requires a session where the graph\n    containing this variable has been launched. If no session is\n    passed, the default session is used.\n\n    Args:\n      session: tf.BaseSession.\n        The `tf.Session` to use to evaluate this random variable. If\n        none, the default session is used.\n      feed_dict: dict.\n        A dictionary that maps `tf.Tensor` objects to feed values. See\n        `tf.Session.run()` for a description of the valid feed values.\n\n    Returns:\n      Value of the random variable.\n\n    #### Examples\n\n    ```python\n    x = Normal(0.0, 1.0)\n    with tf.Session() as sess:\n      # Usage passing the session explicitly.\n      print(x.eval(sess))\n      # Usage with the default session.  The 'with' block\n      # above makes 'sess' the default session.\n      print(x.eval())\n    ```\n    \"\"\"\n    return self.value.eval(session=session, feed_dict=feed_dict)", "code_tokens": ["def", "eval", "(", "self", ",", "session", "=", "None", ",", "feed_dict", "=", "None", ")", ":", "return", "self", ".", "value", ".", "eval", "(", "session", "=", "session", ",", "feed_dict", "=", "feed_dict", ")"], "docstring": "In a session, computes and returns the value of this random variable.\n\n    This is not a graph construction method, it does not add ops to the graph.\n\n    This convenience method requires a session where the graph\n    containing this variable has been launched. If no session is\n    passed, the default session is used.\n\n    Args:\n      session: tf.BaseSession.\n        The `tf.Session` to use to evaluate this random variable. If\n        none, the default session is used.\n      feed_dict: dict.\n        A dictionary that maps `tf.Tensor` objects to feed values. See\n        `tf.Session.run()` for a description of the valid feed values.\n\n    Returns:\n      Value of the random variable.\n\n    #### Examples\n\n    ```python\n    x = Normal(0.0, 1.0)\n    with tf.Session() as sess:\n      # Usage passing the session explicitly.\n      print(x.eval(sess))\n      # Usage with the default session.  The 'with' block\n      # above makes 'sess' the default session.\n      print(x.eval())\n    ```", "docstring_tokens": ["In", "a", "session", "computes", "and", "returns", "the", "value", "of", "this", "random", "variable", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/random_variable.py#L236-L268", "partition": "test", "index": 770, "time": "2018-03-16 11:51:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/random_variable.py", "func_name": "RandomVariable.value", "original_string": "def value(self):\n    \"\"\"Get tensor that the random variable corresponds to.\"\"\"\n    if self._value is None:\n      try:\n        self._value = self.distribution.sample(self.sample_shape_tensor())\n      except NotImplementedError:\n        raise NotImplementedError(\n            \"sample is not implemented for {0}. You must either pass in the \"\n            \"value argument or implement sample for {0}.\"\n            .format(self.distribution.__class__.__name__))\n    return self._value", "language": "python", "code": "def value(self):\n    \"\"\"Get tensor that the random variable corresponds to.\"\"\"\n    if self._value is None:\n      try:\n        self._value = self.distribution.sample(self.sample_shape_tensor())\n      except NotImplementedError:\n        raise NotImplementedError(\n            \"sample is not implemented for {0}. You must either pass in the \"\n            \"value argument or implement sample for {0}.\"\n            .format(self.distribution.__class__.__name__))\n    return self._value", "code_tokens": ["def", "value", "(", "self", ")", ":", "if", "self", ".", "_value", "is", "None", ":", "try", ":", "self", ".", "_value", "=", "self", ".", "distribution", ".", "sample", "(", "self", ".", "sample_shape_tensor", "(", ")", ")", "except", "NotImplementedError", ":", "raise", "NotImplementedError", "(", "\"sample is not implemented for {0}. You must either pass in the \"", "\"value argument or implement sample for {0}.\"", ".", "format", "(", "self", ".", "distribution", ".", "__class__", ".", "__name__", ")", ")", "return", "self", ".", "_value"], "docstring": "Get tensor that the random variable corresponds to.", "docstring_tokens": ["Get", "tensor", "that", "the", "random", "variable", "corresponds", "to", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/random_variable.py#L160-L170", "partition": "test", "index": 769, "time": "2018-03-16 11:51:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/random_variable.py", "func_name": "RandomVariable.sample_shape", "original_string": "def sample_shape(self):\n    \"\"\"Sample shape of random variable as a `TensorShape`.\"\"\"\n    if isinstance(self._sample_shape, tf.Tensor):\n      return tf.TensorShape(tf.get_static_value(self._sample_shape))\n    return tf.TensorShape(self._sample_shape)", "language": "python", "code": "def sample_shape(self):\n    \"\"\"Sample shape of random variable as a `TensorShape`.\"\"\"\n    if isinstance(self._sample_shape, tf.Tensor):\n      return tf.TensorShape(tf.get_static_value(self._sample_shape))\n    return tf.TensorShape(self._sample_shape)", "code_tokens": ["def", "sample_shape", "(", "self", ")", ":", "if", "isinstance", "(", "self", ".", "_sample_shape", ",", "tf", ".", "Tensor", ")", ":", "return", "tf", ".", "TensorShape", "(", "tf", ".", "get_static_value", "(", "self", ".", "_sample_shape", ")", ")", "return", "tf", ".", "TensorShape", "(", "self", ".", "_sample_shape", ")"], "docstring": "Sample shape of random variable as a `TensorShape`.", "docstring_tokens": ["Sample", "shape", "of", "random", "variable", "as", "a", "TensorShape", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/random_variable.py#L133-L137", "partition": "test", "index": 767, "time": "2018-03-16 11:51:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/random_variable.py", "func_name": "_operator", "original_string": "def _operator(attr):\n  \"\"\"Defers an operator overload to `attr`.\n\n  Args:\n    attr: Operator attribute to use.\n\n  Returns:\n    Function calling operator attribute.\n  \"\"\"\n  @functools.wraps(attr)\n  def func(a, *args):\n    return attr(a.value, *args)\n  return func", "language": "python", "code": "def _operator(attr):\n  \"\"\"Defers an operator overload to `attr`.\n\n  Args:\n    attr: Operator attribute to use.\n\n  Returns:\n    Function calling operator attribute.\n  \"\"\"\n  @functools.wraps(attr)\n  def func(a, *args):\n    return attr(a.value, *args)\n  return func", "code_tokens": ["def", "_operator", "(", "attr", ")", ":", "@", "functools", ".", "wraps", "(", "attr", ")", "def", "func", "(", "a", ",", "*", "args", ")", ":", "return", "attr", "(", "a", ".", "value", ",", "*", "args", ")", "return", "func"], "docstring": "Defers an operator overload to `attr`.\n\n  Args:\n    attr: Operator attribute to use.\n\n  Returns:\n    Function calling operator attribute.", "docstring_tokens": ["Defers", "an", "operator", "overload", "to", "attr", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/random_variable.py#L32-L44", "partition": "test", "index": 765, "time": "2018-03-16 11:51:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/generated_random_variables.py", "func_name": "_make_random_variable", "original_string": "def _make_random_variable(distribution_cls):\n  \"\"\"Factory function to make random variable given distribution class.\"\"\"\n\n  @interceptable\n  @functools.wraps(distribution_cls, assigned=('__module__', '__name__'))\n  @docstring_util.expand_docstring(\n      cls=distribution_cls.__name__,\n      doc=inspect.cleandoc(distribution_cls.__init__.__doc__ or ''))\n  def func(*args, **kwargs):\n    # pylint: disable=g-doc-args\n    \"\"\"Create a random variable for ${cls}.\n\n    See ${cls} for more details.\n\n    Returns:\n      RandomVariable.\n\n    #### Original Docstring for Distribution\n\n    ${doc}\n    \"\"\"\n    # pylint: enable=g-doc-args\n    sample_shape = kwargs.pop('sample_shape', ())\n    value = kwargs.pop('value', None)\n    return RandomVariable(distribution=distribution_cls(*args, **kwargs),\n                          sample_shape=sample_shape,\n                          value=value)\n  return func", "language": "python", "code": "def _make_random_variable(distribution_cls):\n  \"\"\"Factory function to make random variable given distribution class.\"\"\"\n\n  @interceptable\n  @functools.wraps(distribution_cls, assigned=('__module__', '__name__'))\n  @docstring_util.expand_docstring(\n      cls=distribution_cls.__name__,\n      doc=inspect.cleandoc(distribution_cls.__init__.__doc__ or ''))\n  def func(*args, **kwargs):\n    # pylint: disable=g-doc-args\n    \"\"\"Create a random variable for ${cls}.\n\n    See ${cls} for more details.\n\n    Returns:\n      RandomVariable.\n\n    #### Original Docstring for Distribution\n\n    ${doc}\n    \"\"\"\n    # pylint: enable=g-doc-args\n    sample_shape = kwargs.pop('sample_shape', ())\n    value = kwargs.pop('value', None)\n    return RandomVariable(distribution=distribution_cls(*args, **kwargs),\n                          sample_shape=sample_shape,\n                          value=value)\n  return func", "code_tokens": ["def", "_make_random_variable", "(", "distribution_cls", ")", ":", "@", "interceptable", "@", "functools", ".", "wraps", "(", "distribution_cls", ",", "assigned", "=", "(", "'__module__'", ",", "'__name__'", ")", ")", "@", "docstring_util", ".", "expand_docstring", "(", "cls", "=", "distribution_cls", ".", "__name__", ",", "doc", "=", "inspect", ".", "cleandoc", "(", "distribution_cls", ".", "__init__", ".", "__doc__", "or", "''", ")", ")", "def", "func", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "# pylint: disable=g-doc-args", "\"\"\"Create a random variable for ${cls}.\n\n    See ${cls} for more details.\n\n    Returns:\n      RandomVariable.\n\n    #### Original Docstring for Distribution\n\n    ${doc}\n    \"\"\"", "# pylint: enable=g-doc-args", "sample_shape", "=", "kwargs", ".", "pop", "(", "'sample_shape'", ",", "(", ")", ")", "value", "=", "kwargs", ".", "pop", "(", "'value'", ",", "None", ")", "return", "RandomVariable", "(", "distribution", "=", "distribution_cls", "(", "*", "args", ",", "*", "*", "kwargs", ")", ",", "sample_shape", "=", "sample_shape", ",", "value", "=", "value", ")", "return", "func"], "docstring": "Factory function to make random variable given distribution class.", "docstring_tokens": ["Factory", "function", "to", "make", "random", "variable", "given", "distribution", "class", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/generated_random_variables.py#L148-L175", "partition": "test", "index": 618, "time": "2018-03-16 11:51:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/interceptor.py", "func_name": "get_next_interceptor", "original_string": "def get_next_interceptor():\n  \"\"\"Yields the top-most interceptor on the thread-local interceptor stack.\n\n  Operations may be intercepted by multiple nested interceptors. Once reached,\n  an operation can be forwarded through nested interceptors until resolved.\n  To allow for nesting, implement interceptors by re-wrapping their first\n  argument (`f`) as an `interceptable`. To avoid nesting, manipulate the\n  computation without using `interceptable`.\n\n  This function allows for nesting by manipulating the thread-local interceptor\n  stack, so that operations are intercepted in the order of interceptor nesting.\n\n  #### Examples\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def model():\n    x = ed.Normal(loc=0., scale=1., name=\"x\")\n    y = ed.Normal(loc=x, scale=1., name=\"y\")\n    return x + y\n\n  def double(f, *args, **kwargs):\n    return 2. * interceptable(f)(*args, **kwargs)\n\n  def set_y(f, *args, **kwargs):\n    if kwargs.get(\"name\") == \"y\":\n      kwargs[\"value\"] = 0.42\n    return interceptable(f)(*args, **kwargs)\n\n  with interception(double):\n    with interception(set_y):\n      z = model()\n  ```\n\n  This will firstly put `double` on the stack, and then `set_y`,\n  resulting in the stack:\n  (TOP) set_y -> double -> apply (BOTTOM)\n\n  The execution of `model` is then (top lines are current stack state):\n  1) (TOP) set_y -> double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"x\")` is intercepted by `set_y`, and as the name is not \"y\"\n  the operation is simply forwarded to the next interceptor on the stack.\n\n  2) (TOP) double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"x\")` is intercepted by `double`, to produce\n  `2*ed.Normal(0., 1., \"x\")`, with the operation being forwarded down the stack.\n\n  3) (TOP) apply (BOTTOM);\n  `ed.Normal(0., 1., \"x\")` is intercepted by `apply`, which simply calls the\n  constructor.\n\n  (At this point, the nested calls to `get_next_interceptor()`, produced by\n  forwarding operations, exit, and the current stack is again:\n  (TOP) set_y -> double -> apply (BOTTOM))\n\n  4) (TOP) set_y -> double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"y\")` is intercepted by `set_y`,\n  the value of `y` is set to 0.42 and the operation is forwarded down the stack.\n\n  5) (TOP) double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"y\")` is intercepted by `double`, to produce\n  `2*ed.Normal(0., 1., \"y\")`, with the operation being forwarded down the stack.\n\n  6) (TOP) apply (BOTTOM);\n  `ed.Normal(0., 1., \"y\")` is intercepted by `apply`, which simply calls the\n  constructor.\n\n  The final values for `x` and `y` inside of `model()` are tensors where `x` is\n  a random draw from Normal(0., 1.) doubled, and `y` is a constant 0.84, thus\n  z = 2 * Normal(0., 1.) + 0.84.\n  \"\"\"\n  try:\n    interceptor = _interceptor_stack.stack.pop()\n    yield interceptor\n  finally:\n    _interceptor_stack.stack.append(interceptor)", "language": "python", "code": "def get_next_interceptor():\n  \"\"\"Yields the top-most interceptor on the thread-local interceptor stack.\n\n  Operations may be intercepted by multiple nested interceptors. Once reached,\n  an operation can be forwarded through nested interceptors until resolved.\n  To allow for nesting, implement interceptors by re-wrapping their first\n  argument (`f`) as an `interceptable`. To avoid nesting, manipulate the\n  computation without using `interceptable`.\n\n  This function allows for nesting by manipulating the thread-local interceptor\n  stack, so that operations are intercepted in the order of interceptor nesting.\n\n  #### Examples\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def model():\n    x = ed.Normal(loc=0., scale=1., name=\"x\")\n    y = ed.Normal(loc=x, scale=1., name=\"y\")\n    return x + y\n\n  def double(f, *args, **kwargs):\n    return 2. * interceptable(f)(*args, **kwargs)\n\n  def set_y(f, *args, **kwargs):\n    if kwargs.get(\"name\") == \"y\":\n      kwargs[\"value\"] = 0.42\n    return interceptable(f)(*args, **kwargs)\n\n  with interception(double):\n    with interception(set_y):\n      z = model()\n  ```\n\n  This will firstly put `double` on the stack, and then `set_y`,\n  resulting in the stack:\n  (TOP) set_y -> double -> apply (BOTTOM)\n\n  The execution of `model` is then (top lines are current stack state):\n  1) (TOP) set_y -> double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"x\")` is intercepted by `set_y`, and as the name is not \"y\"\n  the operation is simply forwarded to the next interceptor on the stack.\n\n  2) (TOP) double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"x\")` is intercepted by `double`, to produce\n  `2*ed.Normal(0., 1., \"x\")`, with the operation being forwarded down the stack.\n\n  3) (TOP) apply (BOTTOM);\n  `ed.Normal(0., 1., \"x\")` is intercepted by `apply`, which simply calls the\n  constructor.\n\n  (At this point, the nested calls to `get_next_interceptor()`, produced by\n  forwarding operations, exit, and the current stack is again:\n  (TOP) set_y -> double -> apply (BOTTOM))\n\n  4) (TOP) set_y -> double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"y\")` is intercepted by `set_y`,\n  the value of `y` is set to 0.42 and the operation is forwarded down the stack.\n\n  5) (TOP) double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"y\")` is intercepted by `double`, to produce\n  `2*ed.Normal(0., 1., \"y\")`, with the operation being forwarded down the stack.\n\n  6) (TOP) apply (BOTTOM);\n  `ed.Normal(0., 1., \"y\")` is intercepted by `apply`, which simply calls the\n  constructor.\n\n  The final values for `x` and `y` inside of `model()` are tensors where `x` is\n  a random draw from Normal(0., 1.) doubled, and `y` is a constant 0.84, thus\n  z = 2 * Normal(0., 1.) + 0.84.\n  \"\"\"\n  try:\n    interceptor = _interceptor_stack.stack.pop()\n    yield interceptor\n  finally:\n    _interceptor_stack.stack.append(interceptor)", "code_tokens": ["def", "get_next_interceptor", "(", ")", ":", "try", ":", "interceptor", "=", "_interceptor_stack", ".", "stack", ".", "pop", "(", ")", "yield", "interceptor", "finally", ":", "_interceptor_stack", ".", "stack", ".", "append", "(", "interceptor", ")"], "docstring": "Yields the top-most interceptor on the thread-local interceptor stack.\n\n  Operations may be intercepted by multiple nested interceptors. Once reached,\n  an operation can be forwarded through nested interceptors until resolved.\n  To allow for nesting, implement interceptors by re-wrapping their first\n  argument (`f`) as an `interceptable`. To avoid nesting, manipulate the\n  computation without using `interceptable`.\n\n  This function allows for nesting by manipulating the thread-local interceptor\n  stack, so that operations are intercepted in the order of interceptor nesting.\n\n  #### Examples\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def model():\n    x = ed.Normal(loc=0., scale=1., name=\"x\")\n    y = ed.Normal(loc=x, scale=1., name=\"y\")\n    return x + y\n\n  def double(f, *args, **kwargs):\n    return 2. * interceptable(f)(*args, **kwargs)\n\n  def set_y(f, *args, **kwargs):\n    if kwargs.get(\"name\") == \"y\":\n      kwargs[\"value\"] = 0.42\n    return interceptable(f)(*args, **kwargs)\n\n  with interception(double):\n    with interception(set_y):\n      z = model()\n  ```\n\n  This will firstly put `double` on the stack, and then `set_y`,\n  resulting in the stack:\n  (TOP) set_y -> double -> apply (BOTTOM)\n\n  The execution of `model` is then (top lines are current stack state):\n  1) (TOP) set_y -> double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"x\")` is intercepted by `set_y`, and as the name is not \"y\"\n  the operation is simply forwarded to the next interceptor on the stack.\n\n  2) (TOP) double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"x\")` is intercepted by `double`, to produce\n  `2*ed.Normal(0., 1., \"x\")`, with the operation being forwarded down the stack.\n\n  3) (TOP) apply (BOTTOM);\n  `ed.Normal(0., 1., \"x\")` is intercepted by `apply`, which simply calls the\n  constructor.\n\n  (At this point, the nested calls to `get_next_interceptor()`, produced by\n  forwarding operations, exit, and the current stack is again:\n  (TOP) set_y -> double -> apply (BOTTOM))\n\n  4) (TOP) set_y -> double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"y\")` is intercepted by `set_y`,\n  the value of `y` is set to 0.42 and the operation is forwarded down the stack.\n\n  5) (TOP) double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"y\")` is intercepted by `double`, to produce\n  `2*ed.Normal(0., 1., \"y\")`, with the operation being forwarded down the stack.\n\n  6) (TOP) apply (BOTTOM);\n  `ed.Normal(0., 1., \"y\")` is intercepted by `apply`, which simply calls the\n  constructor.\n\n  The final values for `x` and `y` inside of `model()` are tensors where `x` is\n  a random draw from Normal(0., 1.) doubled, and `y` is a constant 0.84, thus\n  z = 2 * Normal(0., 1.) + 0.84.", "docstring_tokens": ["Yields", "the", "top", "-", "most", "interceptor", "on", "the", "thread", "-", "local", "interceptor", "stack", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/interceptor.py#L96-L172", "partition": "test", "index": 628, "time": "2018-03-16 11:51:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/interceptor.py", "func_name": "interceptable", "original_string": "def interceptable(func):\n  \"\"\"Decorator that wraps `func` so that its execution is intercepted.\n\n  The wrapper passes `func` to the interceptor for the current thread.\n\n  If there is no next interceptor, we perform an \"immediate\" call to `func`.\n  That is, `func` terminates without forwarding its execution to another\n  interceptor.\n\n  Args:\n    func: Function to wrap.\n\n  Returns:\n    The decorated function.\n  \"\"\"\n  @functools.wraps(func)\n  def func_wrapped(*args, **kwargs):\n    with get_next_interceptor() as interceptor:\n      return interceptor(func, *args, **kwargs)\n\n  return func_wrapped", "language": "python", "code": "def interceptable(func):\n  \"\"\"Decorator that wraps `func` so that its execution is intercepted.\n\n  The wrapper passes `func` to the interceptor for the current thread.\n\n  If there is no next interceptor, we perform an \"immediate\" call to `func`.\n  That is, `func` terminates without forwarding its execution to another\n  interceptor.\n\n  Args:\n    func: Function to wrap.\n\n  Returns:\n    The decorated function.\n  \"\"\"\n  @functools.wraps(func)\n  def func_wrapped(*args, **kwargs):\n    with get_next_interceptor() as interceptor:\n      return interceptor(func, *args, **kwargs)\n\n  return func_wrapped", "code_tokens": ["def", "interceptable", "(", "func", ")", ":", "@", "functools", ".", "wraps", "(", "func", ")", "def", "func_wrapped", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "with", "get_next_interceptor", "(", ")", "as", "interceptor", ":", "return", "interceptor", "(", "func", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "func_wrapped"], "docstring": "Decorator that wraps `func` so that its execution is intercepted.\n\n  The wrapper passes `func` to the interceptor for the current thread.\n\n  If there is no next interceptor, we perform an \"immediate\" call to `func`.\n  That is, `func` terminates without forwarding its execution to another\n  interceptor.\n\n  Args:\n    func: Function to wrap.\n\n  Returns:\n    The decorated function.", "docstring_tokens": ["Decorator", "that", "wraps", "func", "so", "that", "its", "execution", "is", "intercepted", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/interceptor.py#L175-L195", "partition": "test", "index": 629, "time": "2018-03-16 11:51:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/util.py", "func_name": "choose", "original_string": "def choose(is_accepted, accepted, rejected, name=None):\n  \"\"\"Helper which expand_dims `is_accepted` then applies tf.where.\"\"\"\n  if not is_namedtuple_like(accepted):\n    return _choose_base_case(is_accepted, accepted, rejected, name=name)\n  if not isinstance(accepted, type(rejected)):\n    raise TypeError('Type of `accepted` ({}) must be identical to '\n                    'type of `rejected` ({})'.format(\n                        type(accepted).__name__,\n                        type(rejected).__name__))\n  return type(accepted)(**dict(\n      [(fn,\n        choose(is_accepted,\n               getattr(accepted, fn),\n               getattr(rejected, fn),\n               name=name))\n       for fn in accepted._fields]))", "language": "python", "code": "def choose(is_accepted, accepted, rejected, name=None):\n  \"\"\"Helper which expand_dims `is_accepted` then applies tf.where.\"\"\"\n  if not is_namedtuple_like(accepted):\n    return _choose_base_case(is_accepted, accepted, rejected, name=name)\n  if not isinstance(accepted, type(rejected)):\n    raise TypeError('Type of `accepted` ({}) must be identical to '\n                    'type of `rejected` ({})'.format(\n                        type(accepted).__name__,\n                        type(rejected).__name__))\n  return type(accepted)(**dict(\n      [(fn,\n        choose(is_accepted,\n               getattr(accepted, fn),\n               getattr(rejected, fn),\n               name=name))\n       for fn in accepted._fields]))", "code_tokens": ["def", "choose", "(", "is_accepted", ",", "accepted", ",", "rejected", ",", "name", "=", "None", ")", ":", "if", "not", "is_namedtuple_like", "(", "accepted", ")", ":", "return", "_choose_base_case", "(", "is_accepted", ",", "accepted", ",", "rejected", ",", "name", "=", "name", ")", "if", "not", "isinstance", "(", "accepted", ",", "type", "(", "rejected", ")", ")", ":", "raise", "TypeError", "(", "'Type of `accepted` ({}) must be identical to '", "'type of `rejected` ({})'", ".", "format", "(", "type", "(", "accepted", ")", ".", "__name__", ",", "type", "(", "rejected", ")", ".", "__name__", ")", ")", "return", "type", "(", "accepted", ")", "(", "*", "*", "dict", "(", "[", "(", "fn", ",", "choose", "(", "is_accepted", ",", "getattr", "(", "accepted", ",", "fn", ")", ",", "getattr", "(", "rejected", ",", "fn", ")", ",", "name", "=", "name", ")", ")", "for", "fn", "in", "accepted", ".", "_fields", "]", ")", ")"], "docstring": "Helper which expand_dims `is_accepted` then applies tf.where.", "docstring_tokens": ["Helper", "which", "expand_dims", "is_accepted", "then", "applies", "tf", ".", "where", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/util.py#L114-L129", "partition": "test", "index": 966, "time": "2018-03-19 16:31:29"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/util.py", "func_name": "is_namedtuple_like", "original_string": "def is_namedtuple_like(x):\n  \"\"\"Helper which returns `True` if input is `collections.namedtuple`-like.\"\"\"\n  try:\n    for fn in x._fields:\n      _ = getattr(x, fn)\n    return True\n  except AttributeError:\n    return False", "language": "python", "code": "def is_namedtuple_like(x):\n  \"\"\"Helper which returns `True` if input is `collections.namedtuple`-like.\"\"\"\n  try:\n    for fn in x._fields:\n      _ = getattr(x, fn)\n    return True\n  except AttributeError:\n    return False", "code_tokens": ["def", "is_namedtuple_like", "(", "x", ")", ":", "try", ":", "for", "fn", "in", "x", ".", "_fields", ":", "_", "=", "getattr", "(", "x", ",", "fn", ")", "return", "True", "except", "AttributeError", ":", "return", "False"], "docstring": "Helper which returns `True` if input is `collections.namedtuple`-like.", "docstring_tokens": ["Helper", "which", "returns", "True", "if", "input", "is", "collections", ".", "namedtuple", "-", "like", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/util.py#L56-L63", "partition": "test", "index": 964, "time": "2018-03-19 16:31:29"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/program_transformations.py", "func_name": "_get_function_inputs", "original_string": "def _get_function_inputs(f, src_kwargs):\n  \"\"\"Filters inputs to be compatible with function `f`'s signature.\n\n  Args:\n    f: Function according to whose input signature we filter arguments.\n    src_kwargs: Keyword arguments to filter according to `f`.\n\n  Returns:\n    kwargs: Dict of key-value pairs in `src_kwargs` which exist in `f`'s\n      signature.\n  \"\"\"\n  if hasattr(f, \"_func\"):  # functions returned by tf.make_template\n    f = f._func  # pylint: disable=protected-access\n\n  try:  # getargspec was deprecated in Python 3.6\n    argspec = inspect.getfullargspec(f)\n  except AttributeError:\n    argspec = inspect.getargspec(f)\n\n  fkwargs = {k: v for k, v in six.iteritems(src_kwargs) if k in argspec.args}\n  return fkwargs", "language": "python", "code": "def _get_function_inputs(f, src_kwargs):\n  \"\"\"Filters inputs to be compatible with function `f`'s signature.\n\n  Args:\n    f: Function according to whose input signature we filter arguments.\n    src_kwargs: Keyword arguments to filter according to `f`.\n\n  Returns:\n    kwargs: Dict of key-value pairs in `src_kwargs` which exist in `f`'s\n      signature.\n  \"\"\"\n  if hasattr(f, \"_func\"):  # functions returned by tf.make_template\n    f = f._func  # pylint: disable=protected-access\n\n  try:  # getargspec was deprecated in Python 3.6\n    argspec = inspect.getfullargspec(f)\n  except AttributeError:\n    argspec = inspect.getargspec(f)\n\n  fkwargs = {k: v for k, v in six.iteritems(src_kwargs) if k in argspec.args}\n  return fkwargs", "code_tokens": ["def", "_get_function_inputs", "(", "f", ",", "src_kwargs", ")", ":", "if", "hasattr", "(", "f", ",", "\"_func\"", ")", ":", "# functions returned by tf.make_template", "f", "=", "f", ".", "_func", "# pylint: disable=protected-access", "try", ":", "# getargspec was deprecated in Python 3.6", "argspec", "=", "inspect", ".", "getfullargspec", "(", "f", ")", "except", "AttributeError", ":", "argspec", "=", "inspect", ".", "getargspec", "(", "f", ")", "fkwargs", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "six", ".", "iteritems", "(", "src_kwargs", ")", "if", "k", "in", "argspec", ".", "args", "}", "return", "fkwargs"], "docstring": "Filters inputs to be compatible with function `f`'s signature.\n\n  Args:\n    f: Function according to whose input signature we filter arguments.\n    src_kwargs: Keyword arguments to filter according to `f`.\n\n  Returns:\n    kwargs: Dict of key-value pairs in `src_kwargs` which exist in `f`'s\n      signature.", "docstring_tokens": ["Filters", "inputs", "to", "be", "compatible", "with", "function", "f", "s", "signature", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/program_transformations.py#L226-L246", "partition": "test", "index": 799, "time": "2018-03-22 10:28:38"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/program_transformations.py", "func_name": "make_log_joint_fn", "original_string": "def make_log_joint_fn(model):\n  \"\"\"Takes Edward probabilistic program and returns its log joint function.\n\n  Args:\n    model: Python callable which executes the generative process of a\n      computable probability distribution using `ed.RandomVariable`s.\n\n  Returns:\n    A log-joint probability function. Its inputs are `model`'s original inputs\n    and random variables which appear during the program execution. Its output\n    is a scalar tf.Tensor.\n\n  #### Examples\n\n  Below we define Bayesian logistic regression as an Edward program,\n  representing the model's generative process. We apply `make_log_joint_fn` in\n  order to represent the model in terms of its joint probability function.\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def logistic_regression(features):\n    coeffs = ed.Normal(loc=0., scale=1.,\n                       sample_shape=features.shape[1], name=\"coeffs\")\n    outcomes = ed.Bernoulli(logits=tf.tensordot(features, coeffs, [[1], [0]]),\n                            name=\"outcomes\")\n    return outcomes\n\n  log_joint = ed.make_log_joint_fn(logistic_regression)\n\n  features = tf.random_normal([3, 2])\n  coeffs_value = tf.random_normal([2])\n  outcomes_value = tf.round(tf.random_uniform([3]))\n  output = log_joint(features, coeffs=coeffs_value, outcomes=outcomes_value)\n  ```\n\n  \"\"\"\n  def log_joint_fn(*args, **kwargs):\n    \"\"\"Log-probability of inputs according to a joint probability distribution.\n\n    Args:\n      *args: Positional arguments. They are the model's original inputs and can\n        alternatively be specified as part of `kwargs`.\n      **kwargs: Keyword arguments, where for each key-value pair `k` and `v`,\n        `v` is passed as a `value` to the random variable(s) whose keyword\n        argument `name` during construction is equal to `k`.\n\n    Returns:\n      Scalar tf.Tensor, which represents the model's log-probability summed\n      over all Edward random variables and their dimensions.\n\n    Raises:\n      TypeError: If a random variable in the model has no specified value in\n        `**kwargs`.\n    \"\"\"\n    log_probs = []\n\n    def interceptor(rv_constructor, *rv_args, **rv_kwargs):\n      \"\"\"Overrides a random variable's `value` and accumulates its log-prob.\"\"\"\n      # Set value to keyword argument indexed by `name` (an input tensor).\n      rv_name = rv_kwargs.get(\"name\")\n      if rv_name is None:\n        raise KeyError(\"Random variable constructor {} has no name \"\n                       \"in its arguments.\".format(rv_constructor.__name__))\n\n      # If no value is explicitly passed in for an RV, default to the value\n      # from the RV constructor. This may have been set explicitly by the user\n      # or forwarded from a lower-level interceptor.\n      previously_specified_value = rv_kwargs.get(\"value\")\n      value = kwargs.get(rv_name, previously_specified_value)\n      if value is None:\n        raise LookupError(\"Keyword argument specifying value for {} is \"\n                          \"missing.\".format(rv_name))\n      rv_kwargs[\"value\"] = value\n\n      rv = rv_constructor(*rv_args, **rv_kwargs)\n      log_prob = tf.reduce_sum(input_tensor=rv.distribution.log_prob(rv.value))\n      log_probs.append(log_prob)\n      return rv\n\n    model_kwargs = _get_function_inputs(model, kwargs)\n    with interception(interceptor):\n      model(*args, **model_kwargs)\n    log_prob = sum(log_probs)\n    return log_prob\n  return log_joint_fn", "language": "python", "code": "def make_log_joint_fn(model):\n  \"\"\"Takes Edward probabilistic program and returns its log joint function.\n\n  Args:\n    model: Python callable which executes the generative process of a\n      computable probability distribution using `ed.RandomVariable`s.\n\n  Returns:\n    A log-joint probability function. Its inputs are `model`'s original inputs\n    and random variables which appear during the program execution. Its output\n    is a scalar tf.Tensor.\n\n  #### Examples\n\n  Below we define Bayesian logistic regression as an Edward program,\n  representing the model's generative process. We apply `make_log_joint_fn` in\n  order to represent the model in terms of its joint probability function.\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def logistic_regression(features):\n    coeffs = ed.Normal(loc=0., scale=1.,\n                       sample_shape=features.shape[1], name=\"coeffs\")\n    outcomes = ed.Bernoulli(logits=tf.tensordot(features, coeffs, [[1], [0]]),\n                            name=\"outcomes\")\n    return outcomes\n\n  log_joint = ed.make_log_joint_fn(logistic_regression)\n\n  features = tf.random_normal([3, 2])\n  coeffs_value = tf.random_normal([2])\n  outcomes_value = tf.round(tf.random_uniform([3]))\n  output = log_joint(features, coeffs=coeffs_value, outcomes=outcomes_value)\n  ```\n\n  \"\"\"\n  def log_joint_fn(*args, **kwargs):\n    \"\"\"Log-probability of inputs according to a joint probability distribution.\n\n    Args:\n      *args: Positional arguments. They are the model's original inputs and can\n        alternatively be specified as part of `kwargs`.\n      **kwargs: Keyword arguments, where for each key-value pair `k` and `v`,\n        `v` is passed as a `value` to the random variable(s) whose keyword\n        argument `name` during construction is equal to `k`.\n\n    Returns:\n      Scalar tf.Tensor, which represents the model's log-probability summed\n      over all Edward random variables and their dimensions.\n\n    Raises:\n      TypeError: If a random variable in the model has no specified value in\n        `**kwargs`.\n    \"\"\"\n    log_probs = []\n\n    def interceptor(rv_constructor, *rv_args, **rv_kwargs):\n      \"\"\"Overrides a random variable's `value` and accumulates its log-prob.\"\"\"\n      # Set value to keyword argument indexed by `name` (an input tensor).\n      rv_name = rv_kwargs.get(\"name\")\n      if rv_name is None:\n        raise KeyError(\"Random variable constructor {} has no name \"\n                       \"in its arguments.\".format(rv_constructor.__name__))\n\n      # If no value is explicitly passed in for an RV, default to the value\n      # from the RV constructor. This may have been set explicitly by the user\n      # or forwarded from a lower-level interceptor.\n      previously_specified_value = rv_kwargs.get(\"value\")\n      value = kwargs.get(rv_name, previously_specified_value)\n      if value is None:\n        raise LookupError(\"Keyword argument specifying value for {} is \"\n                          \"missing.\".format(rv_name))\n      rv_kwargs[\"value\"] = value\n\n      rv = rv_constructor(*rv_args, **rv_kwargs)\n      log_prob = tf.reduce_sum(input_tensor=rv.distribution.log_prob(rv.value))\n      log_probs.append(log_prob)\n      return rv\n\n    model_kwargs = _get_function_inputs(model, kwargs)\n    with interception(interceptor):\n      model(*args, **model_kwargs)\n    log_prob = sum(log_probs)\n    return log_prob\n  return log_joint_fn", "code_tokens": ["def", "make_log_joint_fn", "(", "model", ")", ":", "def", "log_joint_fn", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "\"\"\"Log-probability of inputs according to a joint probability distribution.\n\n    Args:\n      *args: Positional arguments. They are the model's original inputs and can\n        alternatively be specified as part of `kwargs`.\n      **kwargs: Keyword arguments, where for each key-value pair `k` and `v`,\n        `v` is passed as a `value` to the random variable(s) whose keyword\n        argument `name` during construction is equal to `k`.\n\n    Returns:\n      Scalar tf.Tensor, which represents the model's log-probability summed\n      over all Edward random variables and their dimensions.\n\n    Raises:\n      TypeError: If a random variable in the model has no specified value in\n        `**kwargs`.\n    \"\"\"", "log_probs", "=", "[", "]", "def", "interceptor", "(", "rv_constructor", ",", "*", "rv_args", ",", "*", "*", "rv_kwargs", ")", ":", "\"\"\"Overrides a random variable's `value` and accumulates its log-prob.\"\"\"", "# Set value to keyword argument indexed by `name` (an input tensor).", "rv_name", "=", "rv_kwargs", ".", "get", "(", "\"name\"", ")", "if", "rv_name", "is", "None", ":", "raise", "KeyError", "(", "\"Random variable constructor {} has no name \"", "\"in its arguments.\"", ".", "format", "(", "rv_constructor", ".", "__name__", ")", ")", "# If no value is explicitly passed in for an RV, default to the value", "# from the RV constructor. This may have been set explicitly by the user", "# or forwarded from a lower-level interceptor.", "previously_specified_value", "=", "rv_kwargs", ".", "get", "(", "\"value\"", ")", "value", "=", "kwargs", ".", "get", "(", "rv_name", ",", "previously_specified_value", ")", "if", "value", "is", "None", ":", "raise", "LookupError", "(", "\"Keyword argument specifying value for {} is \"", "\"missing.\"", ".", "format", "(", "rv_name", ")", ")", "rv_kwargs", "[", "\"value\"", "]", "=", "value", "rv", "=", "rv_constructor", "(", "*", "rv_args", ",", "*", "*", "rv_kwargs", ")", "log_prob", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "rv", ".", "distribution", ".", "log_prob", "(", "rv", ".", "value", ")", ")", "log_probs", ".", "append", "(", "log_prob", ")", "return", "rv", "model_kwargs", "=", "_get_function_inputs", "(", "model", ",", "kwargs", ")", "with", "interception", "(", "interceptor", ")", ":", "model", "(", "*", "args", ",", "*", "*", "model_kwargs", ")", "log_prob", "=", "sum", "(", "log_probs", ")", "return", "log_prob", "return", "log_joint_fn"], "docstring": "Takes Edward probabilistic program and returns its log joint function.\n\n  Args:\n    model: Python callable which executes the generative process of a\n      computable probability distribution using `ed.RandomVariable`s.\n\n  Returns:\n    A log-joint probability function. Its inputs are `model`'s original inputs\n    and random variables which appear during the program execution. Its output\n    is a scalar tf.Tensor.\n\n  #### Examples\n\n  Below we define Bayesian logistic regression as an Edward program,\n  representing the model's generative process. We apply `make_log_joint_fn` in\n  order to represent the model in terms of its joint probability function.\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def logistic_regression(features):\n    coeffs = ed.Normal(loc=0., scale=1.,\n                       sample_shape=features.shape[1], name=\"coeffs\")\n    outcomes = ed.Bernoulli(logits=tf.tensordot(features, coeffs, [[1], [0]]),\n                            name=\"outcomes\")\n    return outcomes\n\n  log_joint = ed.make_log_joint_fn(logistic_regression)\n\n  features = tf.random_normal([3, 2])\n  coeffs_value = tf.random_normal([2])\n  outcomes_value = tf.round(tf.random_uniform([3]))\n  output = log_joint(features, coeffs=coeffs_value, outcomes=outcomes_value)\n  ```", "docstring_tokens": ["Takes", "Edward", "probabilistic", "program", "and", "returns", "its", "log", "joint", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/program_transformations.py#L138-L223", "partition": "test", "index": 798, "time": "2018-03-22 10:28:38"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/random_walk_metropolis.py", "func_name": "random_walk_uniform_fn", "original_string": "def random_walk_uniform_fn(scale=1., name=None):\n  \"\"\"Returns a callable that adds a random uniform perturbation to the input.\n\n  For more details on `random_walk_uniform_fn`, see\n  `random_walk_normal_fn`. `scale` might\n  be a `Tensor` or a list of `Tensor`s that should broadcast with state parts\n  of the `current_state`. The generated uniform perturbation is sampled as a\n  uniform point on the rectangle `[-scale, scale]`.\n\n  Args:\n    scale: a `Tensor` or Python `list` of `Tensor`s of any shapes and `dtypes`\n      controlling the upper and lower bound of the uniform proposal\n      distribution.\n    name: Python `str` name prefixed to Ops created by this function.\n        Default value: 'random_walk_uniform_fn'.\n\n  Returns:\n    random_walk_uniform_fn: A callable accepting a Python `list` of `Tensor`s\n      representing the state parts of the `current_state` and an `int`\n      representing the random seed used to generate the proposal. The callable\n      returns the same-type `list` of `Tensor`s as the input and represents the\n      proposal for the RWM algorithm.\n  \"\"\"\n  def _fn(state_parts, seed):\n    \"\"\"Adds a uniform perturbation to the input state.\n\n    Args:\n      state_parts: A list of `Tensor`s of any shape and real dtype representing\n        the state parts of the `current_state` of the Markov chain.\n      seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n        applied.\n        Default value: `None`.\n\n    Returns:\n      perturbed_state_parts: A Python `list` of The `Tensor`s. Has the same\n        shape and type as the `state_parts`.\n\n    Raises:\n      ValueError: if `scale` does not broadcast with `state_parts`.\n    \"\"\"\n    with tf.compat.v1.name_scope(\n        name, 'random_walk_uniform_fn', values=[state_parts, scale, seed]):\n      scales = scale if mcmc_util.is_list_like(scale) else [scale]\n      if len(scales) == 1:\n        scales *= len(state_parts)\n      if len(state_parts) != len(scales):\n        raise ValueError('`scale` must broadcast with `state_parts`.')\n      seed_stream = distributions.SeedStream(seed, salt='RandomWalkUniformFn')\n      next_state_parts = [\n          tf.random.uniform(\n              minval=state_part - scale_part,\n              maxval=state_part + scale_part,\n              shape=tf.shape(input=state_part),\n              dtype=state_part.dtype.base_dtype,\n              seed=seed_stream())\n          for scale_part, state_part in zip(scales, state_parts)\n      ]\n      return next_state_parts\n  return _fn", "language": "python", "code": "def random_walk_uniform_fn(scale=1., name=None):\n  \"\"\"Returns a callable that adds a random uniform perturbation to the input.\n\n  For more details on `random_walk_uniform_fn`, see\n  `random_walk_normal_fn`. `scale` might\n  be a `Tensor` or a list of `Tensor`s that should broadcast with state parts\n  of the `current_state`. The generated uniform perturbation is sampled as a\n  uniform point on the rectangle `[-scale, scale]`.\n\n  Args:\n    scale: a `Tensor` or Python `list` of `Tensor`s of any shapes and `dtypes`\n      controlling the upper and lower bound of the uniform proposal\n      distribution.\n    name: Python `str` name prefixed to Ops created by this function.\n        Default value: 'random_walk_uniform_fn'.\n\n  Returns:\n    random_walk_uniform_fn: A callable accepting a Python `list` of `Tensor`s\n      representing the state parts of the `current_state` and an `int`\n      representing the random seed used to generate the proposal. The callable\n      returns the same-type `list` of `Tensor`s as the input and represents the\n      proposal for the RWM algorithm.\n  \"\"\"\n  def _fn(state_parts, seed):\n    \"\"\"Adds a uniform perturbation to the input state.\n\n    Args:\n      state_parts: A list of `Tensor`s of any shape and real dtype representing\n        the state parts of the `current_state` of the Markov chain.\n      seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n        applied.\n        Default value: `None`.\n\n    Returns:\n      perturbed_state_parts: A Python `list` of The `Tensor`s. Has the same\n        shape and type as the `state_parts`.\n\n    Raises:\n      ValueError: if `scale` does not broadcast with `state_parts`.\n    \"\"\"\n    with tf.compat.v1.name_scope(\n        name, 'random_walk_uniform_fn', values=[state_parts, scale, seed]):\n      scales = scale if mcmc_util.is_list_like(scale) else [scale]\n      if len(scales) == 1:\n        scales *= len(state_parts)\n      if len(state_parts) != len(scales):\n        raise ValueError('`scale` must broadcast with `state_parts`.')\n      seed_stream = distributions.SeedStream(seed, salt='RandomWalkUniformFn')\n      next_state_parts = [\n          tf.random.uniform(\n              minval=state_part - scale_part,\n              maxval=state_part + scale_part,\n              shape=tf.shape(input=state_part),\n              dtype=state_part.dtype.base_dtype,\n              seed=seed_stream())\n          for scale_part, state_part in zip(scales, state_parts)\n      ]\n      return next_state_parts\n  return _fn", "code_tokens": ["def", "random_walk_uniform_fn", "(", "scale", "=", "1.", ",", "name", "=", "None", ")", ":", "def", "_fn", "(", "state_parts", ",", "seed", ")", ":", "\"\"\"Adds a uniform perturbation to the input state.\n\n    Args:\n      state_parts: A list of `Tensor`s of any shape and real dtype representing\n        the state parts of the `current_state` of the Markov chain.\n      seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n        applied.\n        Default value: `None`.\n\n    Returns:\n      perturbed_state_parts: A Python `list` of The `Tensor`s. Has the same\n        shape and type as the `state_parts`.\n\n    Raises:\n      ValueError: if `scale` does not broadcast with `state_parts`.\n    \"\"\"", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'random_walk_uniform_fn'", ",", "values", "=", "[", "state_parts", ",", "scale", ",", "seed", "]", ")", ":", "scales", "=", "scale", "if", "mcmc_util", ".", "is_list_like", "(", "scale", ")", "else", "[", "scale", "]", "if", "len", "(", "scales", ")", "==", "1", ":", "scales", "*=", "len", "(", "state_parts", ")", "if", "len", "(", "state_parts", ")", "!=", "len", "(", "scales", ")", ":", "raise", "ValueError", "(", "'`scale` must broadcast with `state_parts`.'", ")", "seed_stream", "=", "distributions", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'RandomWalkUniformFn'", ")", "next_state_parts", "=", "[", "tf", ".", "random", ".", "uniform", "(", "minval", "=", "state_part", "-", "scale_part", ",", "maxval", "=", "state_part", "+", "scale_part", ",", "shape", "=", "tf", ".", "shape", "(", "input", "=", "state_part", ")", ",", "dtype", "=", "state_part", ".", "dtype", ".", "base_dtype", ",", "seed", "=", "seed_stream", "(", ")", ")", "for", "scale_part", ",", "state_part", "in", "zip", "(", "scales", ",", "state_parts", ")", "]", "return", "next_state_parts", "return", "_fn"], "docstring": "Returns a callable that adds a random uniform perturbation to the input.\n\n  For more details on `random_walk_uniform_fn`, see\n  `random_walk_normal_fn`. `scale` might\n  be a `Tensor` or a list of `Tensor`s that should broadcast with state parts\n  of the `current_state`. The generated uniform perturbation is sampled as a\n  uniform point on the rectangle `[-scale, scale]`.\n\n  Args:\n    scale: a `Tensor` or Python `list` of `Tensor`s of any shapes and `dtypes`\n      controlling the upper and lower bound of the uniform proposal\n      distribution.\n    name: Python `str` name prefixed to Ops created by this function.\n        Default value: 'random_walk_uniform_fn'.\n\n  Returns:\n    random_walk_uniform_fn: A callable accepting a Python `list` of `Tensor`s\n      representing the state parts of the `current_state` and an `int`\n      representing the random seed used to generate the proposal. The callable\n      returns the same-type `list` of `Tensor`s as the input and represents the\n      proposal for the RWM algorithm.", "docstring_tokens": ["Returns", "a", "callable", "that", "adds", "a", "random", "uniform", "perturbation", "to", "the", "input", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/random_walk_metropolis.py#L112-L170", "partition": "test", "index": 648, "time": "2018-03-26 17:17:52"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/random_walk_metropolis.py", "func_name": "random_walk_normal_fn", "original_string": "def random_walk_normal_fn(scale=1., name=None):\n  \"\"\"Returns a callable that adds a random normal perturbation to the input.\n\n  This function returns a callable that accepts a Python `list` of `Tensor`s of\n  any shapes and `dtypes`  representing the state parts of the `current_state`\n  and a random seed. The supplied argument `scale` must be a `Tensor` or Python\n  `list` of `Tensor`s representing the scale of the generated\n  proposal. `scale` must broadcast with the state parts of `current_state`.\n  The callable adds a sample from a zero-mean normal distribution with the\n  supplied scales to each state part and returns a same-type `list` of `Tensor`s\n  as the state parts of `current_state`.\n\n  Args:\n    scale: a `Tensor` or Python `list` of `Tensor`s of any shapes and `dtypes`\n      controlling the scale of the normal proposal distribution.\n    name: Python `str` name prefixed to Ops created by this function.\n        Default value: 'random_walk_normal_fn'.\n\n  Returns:\n    random_walk_normal_fn: A callable accepting a Python `list` of `Tensor`s\n      representing the state parts of the `current_state` and an `int`\n      representing the random seed to be used to generate the proposal. The\n      callable returns the same-type `list` of `Tensor`s as the input and\n      represents the proposal for the RWM algorithm.\n  \"\"\"\n  def _fn(state_parts, seed):\n    \"\"\"Adds a normal perturbation to the input state.\n\n    Args:\n      state_parts: A list of `Tensor`s of any shape and real dtype representing\n        the state parts of the `current_state` of the Markov chain.\n      seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n        applied.\n        Default value: `None`.\n\n    Returns:\n      perturbed_state_parts: A Python `list` of The `Tensor`s. Has the same\n        shape and type as the `state_parts`.\n\n    Raises:\n      ValueError: if `scale` does not broadcast with `state_parts`.\n    \"\"\"\n    with tf.compat.v1.name_scope(\n        name, 'random_walk_normal_fn', values=[state_parts, scale, seed]):\n      scales = scale if mcmc_util.is_list_like(scale) else [scale]\n      if len(scales) == 1:\n        scales *= len(state_parts)\n      if len(state_parts) != len(scales):\n        raise ValueError('`scale` must broadcast with `state_parts`.')\n      seed_stream = distributions.SeedStream(seed, salt='RandomWalkNormalFn')\n      next_state_parts = [\n          tf.random.normal(\n              mean=state_part,\n              stddev=scale_part,\n              shape=tf.shape(input=state_part),\n              dtype=state_part.dtype.base_dtype,\n              seed=seed_stream())\n          for scale_part, state_part in zip(scales, state_parts)\n      ]\n\n      return next_state_parts\n  return _fn", "language": "python", "code": "def random_walk_normal_fn(scale=1., name=None):\n  \"\"\"Returns a callable that adds a random normal perturbation to the input.\n\n  This function returns a callable that accepts a Python `list` of `Tensor`s of\n  any shapes and `dtypes`  representing the state parts of the `current_state`\n  and a random seed. The supplied argument `scale` must be a `Tensor` or Python\n  `list` of `Tensor`s representing the scale of the generated\n  proposal. `scale` must broadcast with the state parts of `current_state`.\n  The callable adds a sample from a zero-mean normal distribution with the\n  supplied scales to each state part and returns a same-type `list` of `Tensor`s\n  as the state parts of `current_state`.\n\n  Args:\n    scale: a `Tensor` or Python `list` of `Tensor`s of any shapes and `dtypes`\n      controlling the scale of the normal proposal distribution.\n    name: Python `str` name prefixed to Ops created by this function.\n        Default value: 'random_walk_normal_fn'.\n\n  Returns:\n    random_walk_normal_fn: A callable accepting a Python `list` of `Tensor`s\n      representing the state parts of the `current_state` and an `int`\n      representing the random seed to be used to generate the proposal. The\n      callable returns the same-type `list` of `Tensor`s as the input and\n      represents the proposal for the RWM algorithm.\n  \"\"\"\n  def _fn(state_parts, seed):\n    \"\"\"Adds a normal perturbation to the input state.\n\n    Args:\n      state_parts: A list of `Tensor`s of any shape and real dtype representing\n        the state parts of the `current_state` of the Markov chain.\n      seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n        applied.\n        Default value: `None`.\n\n    Returns:\n      perturbed_state_parts: A Python `list` of The `Tensor`s. Has the same\n        shape and type as the `state_parts`.\n\n    Raises:\n      ValueError: if `scale` does not broadcast with `state_parts`.\n    \"\"\"\n    with tf.compat.v1.name_scope(\n        name, 'random_walk_normal_fn', values=[state_parts, scale, seed]):\n      scales = scale if mcmc_util.is_list_like(scale) else [scale]\n      if len(scales) == 1:\n        scales *= len(state_parts)\n      if len(state_parts) != len(scales):\n        raise ValueError('`scale` must broadcast with `state_parts`.')\n      seed_stream = distributions.SeedStream(seed, salt='RandomWalkNormalFn')\n      next_state_parts = [\n          tf.random.normal(\n              mean=state_part,\n              stddev=scale_part,\n              shape=tf.shape(input=state_part),\n              dtype=state_part.dtype.base_dtype,\n              seed=seed_stream())\n          for scale_part, state_part in zip(scales, state_parts)\n      ]\n\n      return next_state_parts\n  return _fn", "code_tokens": ["def", "random_walk_normal_fn", "(", "scale", "=", "1.", ",", "name", "=", "None", ")", ":", "def", "_fn", "(", "state_parts", ",", "seed", ")", ":", "\"\"\"Adds a normal perturbation to the input state.\n\n    Args:\n      state_parts: A list of `Tensor`s of any shape and real dtype representing\n        the state parts of the `current_state` of the Markov chain.\n      seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n        applied.\n        Default value: `None`.\n\n    Returns:\n      perturbed_state_parts: A Python `list` of The `Tensor`s. Has the same\n        shape and type as the `state_parts`.\n\n    Raises:\n      ValueError: if `scale` does not broadcast with `state_parts`.\n    \"\"\"", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'random_walk_normal_fn'", ",", "values", "=", "[", "state_parts", ",", "scale", ",", "seed", "]", ")", ":", "scales", "=", "scale", "if", "mcmc_util", ".", "is_list_like", "(", "scale", ")", "else", "[", "scale", "]", "if", "len", "(", "scales", ")", "==", "1", ":", "scales", "*=", "len", "(", "state_parts", ")", "if", "len", "(", "state_parts", ")", "!=", "len", "(", "scales", ")", ":", "raise", "ValueError", "(", "'`scale` must broadcast with `state_parts`.'", ")", "seed_stream", "=", "distributions", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'RandomWalkNormalFn'", ")", "next_state_parts", "=", "[", "tf", ".", "random", ".", "normal", "(", "mean", "=", "state_part", ",", "stddev", "=", "scale_part", ",", "shape", "=", "tf", ".", "shape", "(", "input", "=", "state_part", ")", ",", "dtype", "=", "state_part", ".", "dtype", ".", "base_dtype", ",", "seed", "=", "seed_stream", "(", ")", ")", "for", "scale_part", ",", "state_part", "in", "zip", "(", "scales", ",", "state_parts", ")", "]", "return", "next_state_parts", "return", "_fn"], "docstring": "Returns a callable that adds a random normal perturbation to the input.\n\n  This function returns a callable that accepts a Python `list` of `Tensor`s of\n  any shapes and `dtypes`  representing the state parts of the `current_state`\n  and a random seed. The supplied argument `scale` must be a `Tensor` or Python\n  `list` of `Tensor`s representing the scale of the generated\n  proposal. `scale` must broadcast with the state parts of `current_state`.\n  The callable adds a sample from a zero-mean normal distribution with the\n  supplied scales to each state part and returns a same-type `list` of `Tensor`s\n  as the state parts of `current_state`.\n\n  Args:\n    scale: a `Tensor` or Python `list` of `Tensor`s of any shapes and `dtypes`\n      controlling the scale of the normal proposal distribution.\n    name: Python `str` name prefixed to Ops created by this function.\n        Default value: 'random_walk_normal_fn'.\n\n  Returns:\n    random_walk_normal_fn: A callable accepting a Python `list` of `Tensor`s\n      representing the state parts of the `current_state` and an `int`\n      representing the random seed to be used to generate the proposal. The\n      callable returns the same-type `list` of `Tensor`s as the input and\n      represents the proposal for the RWM algorithm.", "docstring_tokens": ["Returns", "a", "callable", "that", "adds", "a", "random", "normal", "perturbation", "to", "the", "input", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/random_walk_metropolis.py#L48-L109", "partition": "test", "index": 647, "time": "2018-03-26 17:17:52"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/bayesian_neural_network.py", "func_name": "build_fake_data", "original_string": "def build_fake_data(num_examples=10):\n  \"\"\"Build fake MNIST-style data for unit testing.\"\"\"\n\n  class Dummy(object):\n    pass\n\n  num_examples = 10\n  mnist_data = Dummy()\n  mnist_data.train = Dummy()\n  mnist_data.train.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.train.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.train.num_examples = num_examples\n  mnist_data.validation = Dummy()\n  mnist_data.validation.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.validation.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.validation.num_examples = num_examples\n  return mnist_data", "language": "python", "code": "def build_fake_data(num_examples=10):\n  \"\"\"Build fake MNIST-style data for unit testing.\"\"\"\n\n  class Dummy(object):\n    pass\n\n  num_examples = 10\n  mnist_data = Dummy()\n  mnist_data.train = Dummy()\n  mnist_data.train.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.train.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.train.num_examples = num_examples\n  mnist_data.validation = Dummy()\n  mnist_data.validation.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.validation.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.validation.num_examples = num_examples\n  return mnist_data", "code_tokens": ["def", "build_fake_data", "(", "num_examples", "=", "10", ")", ":", "class", "Dummy", "(", "object", ")", ":", "pass", "num_examples", "=", "10", "mnist_data", "=", "Dummy", "(", ")", "mnist_data", ".", "train", "=", "Dummy", "(", ")", "mnist_data", ".", "train", ".", "images", "=", "np", ".", "float32", "(", "np", ".", "random", ".", "randn", "(", "num_examples", ",", "*", "IMAGE_SHAPE", ")", ")", "mnist_data", ".", "train", ".", "labels", "=", "np", ".", "int32", "(", "np", ".", "random", ".", "permutation", "(", "np", ".", "arange", "(", "num_examples", ")", ")", ")", "mnist_data", ".", "train", ".", "num_examples", "=", "num_examples", "mnist_data", ".", "validation", "=", "Dummy", "(", ")", "mnist_data", ".", "validation", ".", "images", "=", "np", ".", "float32", "(", "np", ".", "random", ".", "randn", "(", "num_examples", ",", "*", "IMAGE_SHAPE", ")", ")", "mnist_data", ".", "validation", ".", "labels", "=", "np", ".", "int32", "(", "np", ".", "random", ".", "permutation", "(", "np", ".", "arange", "(", "num_examples", ")", ")", ")", "mnist_data", ".", "validation", ".", "num_examples", "=", "num_examples", "return", "mnist_data"], "docstring": "Build fake MNIST-style data for unit testing.", "docstring_tokens": ["Build", "fake", "MNIST", "-", "style", "data", "for", "unit", "testing", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/bayesian_neural_network.py#L190-L210", "partition": "test", "index": 872, "time": "2018-03-30 17:12:01"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/bayesian_neural_network.py", "func_name": "plot_heldout_prediction", "original_string": "def plot_heldout_prediction(input_vals, probs,\n                            fname, n=10, title=\"\"):\n  \"\"\"Save a PNG plot visualizing posterior uncertainty on heldout data.\n\n  Args:\n    input_vals: A `float`-like Numpy `array` of shape\n      `[num_heldout] + IMAGE_SHAPE`, containing heldout input images.\n    probs: A `float`-like Numpy array of shape `[num_monte_carlo,\n      num_heldout, num_classes]` containing Monte Carlo samples of\n      class probabilities for each heldout sample.\n    fname: Python `str` filename to save the plot to.\n    n: Python `int` number of datapoints to vizualize.\n    title: Python `str` title for the plot.\n  \"\"\"\n  fig = figure.Figure(figsize=(9, 3*n))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n  for i in range(n):\n    ax = fig.add_subplot(n, 3, 3*i + 1)\n    ax.imshow(input_vals[i, :].reshape(IMAGE_SHAPE[:-1]), interpolation=\"None\")\n\n    ax = fig.add_subplot(n, 3, 3*i + 2)\n    for prob_sample in probs:\n      sns.barplot(np.arange(10), prob_sample[i, :], alpha=0.1, ax=ax)\n      ax.set_ylim([0, 1])\n    ax.set_title(\"posterior samples\")\n\n    ax = fig.add_subplot(n, 3, 3*i + 3)\n    sns.barplot(np.arange(10), np.mean(probs[:, i, :], axis=0), ax=ax)\n    ax.set_ylim([0, 1])\n    ax.set_title(\"predictive probs\")\n  fig.suptitle(title)\n  fig.tight_layout()\n\n  canvas.print_figure(fname, format=\"png\")\n  print(\"saved {}\".format(fname))", "language": "python", "code": "def plot_heldout_prediction(input_vals, probs,\n                            fname, n=10, title=\"\"):\n  \"\"\"Save a PNG plot visualizing posterior uncertainty on heldout data.\n\n  Args:\n    input_vals: A `float`-like Numpy `array` of shape\n      `[num_heldout] + IMAGE_SHAPE`, containing heldout input images.\n    probs: A `float`-like Numpy array of shape `[num_monte_carlo,\n      num_heldout, num_classes]` containing Monte Carlo samples of\n      class probabilities for each heldout sample.\n    fname: Python `str` filename to save the plot to.\n    n: Python `int` number of datapoints to vizualize.\n    title: Python `str` title for the plot.\n  \"\"\"\n  fig = figure.Figure(figsize=(9, 3*n))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n  for i in range(n):\n    ax = fig.add_subplot(n, 3, 3*i + 1)\n    ax.imshow(input_vals[i, :].reshape(IMAGE_SHAPE[:-1]), interpolation=\"None\")\n\n    ax = fig.add_subplot(n, 3, 3*i + 2)\n    for prob_sample in probs:\n      sns.barplot(np.arange(10), prob_sample[i, :], alpha=0.1, ax=ax)\n      ax.set_ylim([0, 1])\n    ax.set_title(\"posterior samples\")\n\n    ax = fig.add_subplot(n, 3, 3*i + 3)\n    sns.barplot(np.arange(10), np.mean(probs[:, i, :], axis=0), ax=ax)\n    ax.set_ylim([0, 1])\n    ax.set_title(\"predictive probs\")\n  fig.suptitle(title)\n  fig.tight_layout()\n\n  canvas.print_figure(fname, format=\"png\")\n  print(\"saved {}\".format(fname))", "code_tokens": ["def", "plot_heldout_prediction", "(", "input_vals", ",", "probs", ",", "fname", ",", "n", "=", "10", ",", "title", "=", "\"\"", ")", ":", "fig", "=", "figure", ".", "Figure", "(", "figsize", "=", "(", "9", ",", "3", "*", "n", ")", ")", "canvas", "=", "backend_agg", ".", "FigureCanvasAgg", "(", "fig", ")", "for", "i", "in", "range", "(", "n", ")", ":", "ax", "=", "fig", ".", "add_subplot", "(", "n", ",", "3", ",", "3", "*", "i", "+", "1", ")", "ax", ".", "imshow", "(", "input_vals", "[", "i", ",", ":", "]", ".", "reshape", "(", "IMAGE_SHAPE", "[", ":", "-", "1", "]", ")", ",", "interpolation", "=", "\"None\"", ")", "ax", "=", "fig", ".", "add_subplot", "(", "n", ",", "3", ",", "3", "*", "i", "+", "2", ")", "for", "prob_sample", "in", "probs", ":", "sns", ".", "barplot", "(", "np", ".", "arange", "(", "10", ")", ",", "prob_sample", "[", "i", ",", ":", "]", ",", "alpha", "=", "0.1", ",", "ax", "=", "ax", ")", "ax", ".", "set_ylim", "(", "[", "0", ",", "1", "]", ")", "ax", ".", "set_title", "(", "\"posterior samples\"", ")", "ax", "=", "fig", ".", "add_subplot", "(", "n", ",", "3", ",", "3", "*", "i", "+", "3", ")", "sns", ".", "barplot", "(", "np", ".", "arange", "(", "10", ")", ",", "np", ".", "mean", "(", "probs", "[", ":", ",", "i", ",", ":", "]", ",", "axis", "=", "0", ")", ",", "ax", "=", "ax", ")", "ax", ".", "set_ylim", "(", "[", "0", ",", "1", "]", ")", "ax", ".", "set_title", "(", "\"predictive probs\"", ")", "fig", ".", "suptitle", "(", "title", ")", "fig", ".", "tight_layout", "(", ")", "canvas", ".", "print_figure", "(", "fname", ",", "format", "=", "\"png\"", ")", "print", "(", "\"saved {}\"", ".", "format", "(", "fname", ")", ")"], "docstring": "Save a PNG plot visualizing posterior uncertainty on heldout data.\n\n  Args:\n    input_vals: A `float`-like Numpy `array` of shape\n      `[num_heldout] + IMAGE_SHAPE`, containing heldout input images.\n    probs: A `float`-like Numpy array of shape `[num_monte_carlo,\n      num_heldout, num_classes]` containing Monte Carlo samples of\n      class probabilities for each heldout sample.\n    fname: Python `str` filename to save the plot to.\n    n: Python `int` number of datapoints to vizualize.\n    title: Python `str` title for the plot.", "docstring_tokens": ["Save", "a", "PNG", "plot", "visualizing", "posterior", "uncertainty", "on", "heldout", "data", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/bayesian_neural_network.py#L124-L158", "partition": "test", "index": 871, "time": "2018-03-30 17:12:01"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/logistic_regression.py", "func_name": "toy_logistic_data", "original_string": "def toy_logistic_data(num_examples, input_size=2, weights_prior_stddev=5.0):\n  \"\"\"Generates synthetic data for binary classification.\n\n  Args:\n    num_examples: The number of samples to generate (scalar Python `int`).\n    input_size: The input space dimension (scalar Python `int`).\n    weights_prior_stddev: The prior standard deviation of the weight\n      vector. (scalar Python `float`).\n\n  Returns:\n    random_weights: Sampled weights as a Numpy `array` of shape\n      `[input_size]`.\n    random_bias: Sampled bias as a scalar Python `float`.\n    design_matrix: Points sampled uniformly from the cube `[-1,\n       1]^{input_size}`, as a Numpy `array` of shape `(num_examples,\n       input_size)`.\n    labels: Labels sampled from the logistic model `p(label=1) =\n      logistic(dot(features, random_weights) + random_bias)`, as a Numpy\n      `int32` `array` of shape `(num_examples, 1)`.\n  \"\"\"\n  random_weights = weights_prior_stddev * np.random.randn(input_size)\n  random_bias = np.random.randn()\n  design_matrix = np.random.rand(num_examples, input_size) * 2 - 1\n  logits = np.reshape(\n      np.dot(design_matrix, random_weights) + random_bias,\n      (-1, 1))\n  p_labels = 1. / (1 + np.exp(-logits))\n  labels = np.int32(p_labels > np.random.rand(num_examples, 1))\n  return random_weights, random_bias, np.float32(design_matrix), labels", "language": "python", "code": "def toy_logistic_data(num_examples, input_size=2, weights_prior_stddev=5.0):\n  \"\"\"Generates synthetic data for binary classification.\n\n  Args:\n    num_examples: The number of samples to generate (scalar Python `int`).\n    input_size: The input space dimension (scalar Python `int`).\n    weights_prior_stddev: The prior standard deviation of the weight\n      vector. (scalar Python `float`).\n\n  Returns:\n    random_weights: Sampled weights as a Numpy `array` of shape\n      `[input_size]`.\n    random_bias: Sampled bias as a scalar Python `float`.\n    design_matrix: Points sampled uniformly from the cube `[-1,\n       1]^{input_size}`, as a Numpy `array` of shape `(num_examples,\n       input_size)`.\n    labels: Labels sampled from the logistic model `p(label=1) =\n      logistic(dot(features, random_weights) + random_bias)`, as a Numpy\n      `int32` `array` of shape `(num_examples, 1)`.\n  \"\"\"\n  random_weights = weights_prior_stddev * np.random.randn(input_size)\n  random_bias = np.random.randn()\n  design_matrix = np.random.rand(num_examples, input_size) * 2 - 1\n  logits = np.reshape(\n      np.dot(design_matrix, random_weights) + random_bias,\n      (-1, 1))\n  p_labels = 1. / (1 + np.exp(-logits))\n  labels = np.int32(p_labels > np.random.rand(num_examples, 1))\n  return random_weights, random_bias, np.float32(design_matrix), labels", "code_tokens": ["def", "toy_logistic_data", "(", "num_examples", ",", "input_size", "=", "2", ",", "weights_prior_stddev", "=", "5.0", ")", ":", "random_weights", "=", "weights_prior_stddev", "*", "np", ".", "random", ".", "randn", "(", "input_size", ")", "random_bias", "=", "np", ".", "random", ".", "randn", "(", ")", "design_matrix", "=", "np", ".", "random", ".", "rand", "(", "num_examples", ",", "input_size", ")", "*", "2", "-", "1", "logits", "=", "np", ".", "reshape", "(", "np", ".", "dot", "(", "design_matrix", ",", "random_weights", ")", "+", "random_bias", ",", "(", "-", "1", ",", "1", ")", ")", "p_labels", "=", "1.", "/", "(", "1", "+", "np", ".", "exp", "(", "-", "logits", ")", ")", "labels", "=", "np", ".", "int32", "(", "p_labels", ">", "np", ".", "random", ".", "rand", "(", "num_examples", ",", "1", ")", ")", "return", "random_weights", ",", "random_bias", ",", "np", ".", "float32", "(", "design_matrix", ")", ",", "labels"], "docstring": "Generates synthetic data for binary classification.\n\n  Args:\n    num_examples: The number of samples to generate (scalar Python `int`).\n    input_size: The input space dimension (scalar Python `int`).\n    weights_prior_stddev: The prior standard deviation of the weight\n      vector. (scalar Python `float`).\n\n  Returns:\n    random_weights: Sampled weights as a Numpy `array` of shape\n      `[input_size]`.\n    random_bias: Sampled bias as a scalar Python `float`.\n    design_matrix: Points sampled uniformly from the cube `[-1,\n       1]^{input_size}`, as a Numpy `array` of shape `(num_examples,\n       input_size)`.\n    labels: Labels sampled from the logistic model `p(label=1) =\n      logistic(dot(features, random_weights) + random_bias)`, as a Numpy\n      `int32` `array` of shape `(num_examples, 1)`.", "docstring_tokens": ["Generates", "synthetic", "data", "for", "binary", "classification", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/logistic_regression.py#L58-L86", "partition": "test", "index": 631, "time": "2018-03-30 17:12:01"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/bayesian_neural_network.py", "func_name": "plot_weight_posteriors", "original_string": "def plot_weight_posteriors(names, qm_vals, qs_vals, fname):\n  \"\"\"Save a PNG plot with histograms of weight means and stddevs.\n\n  Args:\n    names: A Python `iterable` of `str` variable names.\n    qm_vals: A Python `iterable`, the same length as `names`,\n      whose elements are Numpy `array`s, of any shape, containing\n      posterior means of weight varibles.\n    qs_vals: A Python `iterable`, the same length as `names`,\n      whose elements are Numpy `array`s, of any shape, containing\n      posterior standard deviations of weight varibles.\n    fname: Python `str` filename to save the plot to.\n  \"\"\"\n  fig = figure.Figure(figsize=(6, 3))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n\n  ax = fig.add_subplot(1, 2, 1)\n  for n, qm in zip(names, qm_vals):\n    sns.distplot(qm.flatten(), ax=ax, label=n)\n  ax.set_title(\"weight means\")\n  ax.set_xlim([-1.5, 1.5])\n  ax.legend()\n\n  ax = fig.add_subplot(1, 2, 2)\n  for n, qs in zip(names, qs_vals):\n    sns.distplot(qs.flatten(), ax=ax)\n  ax.set_title(\"weight stddevs\")\n  ax.set_xlim([0, 1.])\n\n  fig.tight_layout()\n  canvas.print_figure(fname, format=\"png\")\n  print(\"saved {}\".format(fname))", "language": "python", "code": "def plot_weight_posteriors(names, qm_vals, qs_vals, fname):\n  \"\"\"Save a PNG plot with histograms of weight means and stddevs.\n\n  Args:\n    names: A Python `iterable` of `str` variable names.\n    qm_vals: A Python `iterable`, the same length as `names`,\n      whose elements are Numpy `array`s, of any shape, containing\n      posterior means of weight varibles.\n    qs_vals: A Python `iterable`, the same length as `names`,\n      whose elements are Numpy `array`s, of any shape, containing\n      posterior standard deviations of weight varibles.\n    fname: Python `str` filename to save the plot to.\n  \"\"\"\n  fig = figure.Figure(figsize=(6, 3))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n\n  ax = fig.add_subplot(1, 2, 1)\n  for n, qm in zip(names, qm_vals):\n    sns.distplot(qm.flatten(), ax=ax, label=n)\n  ax.set_title(\"weight means\")\n  ax.set_xlim([-1.5, 1.5])\n  ax.legend()\n\n  ax = fig.add_subplot(1, 2, 2)\n  for n, qs in zip(names, qs_vals):\n    sns.distplot(qs.flatten(), ax=ax)\n  ax.set_title(\"weight stddevs\")\n  ax.set_xlim([0, 1.])\n\n  fig.tight_layout()\n  canvas.print_figure(fname, format=\"png\")\n  print(\"saved {}\".format(fname))", "code_tokens": ["def", "plot_weight_posteriors", "(", "names", ",", "qm_vals", ",", "qs_vals", ",", "fname", ")", ":", "fig", "=", "figure", ".", "Figure", "(", "figsize", "=", "(", "6", ",", "3", ")", ")", "canvas", "=", "backend_agg", ".", "FigureCanvasAgg", "(", "fig", ")", "ax", "=", "fig", ".", "add_subplot", "(", "1", ",", "2", ",", "1", ")", "for", "n", ",", "qm", "in", "zip", "(", "names", ",", "qm_vals", ")", ":", "sns", ".", "distplot", "(", "qm", ".", "flatten", "(", ")", ",", "ax", "=", "ax", ",", "label", "=", "n", ")", "ax", ".", "set_title", "(", "\"weight means\"", ")", "ax", ".", "set_xlim", "(", "[", "-", "1.5", ",", "1.5", "]", ")", "ax", ".", "legend", "(", ")", "ax", "=", "fig", ".", "add_subplot", "(", "1", ",", "2", ",", "2", ")", "for", "n", ",", "qs", "in", "zip", "(", "names", ",", "qs_vals", ")", ":", "sns", ".", "distplot", "(", "qs", ".", "flatten", "(", ")", ",", "ax", "=", "ax", ")", "ax", ".", "set_title", "(", "\"weight stddevs\"", ")", "ax", ".", "set_xlim", "(", "[", "0", ",", "1.", "]", ")", "fig", ".", "tight_layout", "(", ")", "canvas", ".", "print_figure", "(", "fname", ",", "format", "=", "\"png\"", ")", "print", "(", "\"saved {}\"", ".", "format", "(", "fname", ")", ")"], "docstring": "Save a PNG plot with histograms of weight means and stddevs.\n\n  Args:\n    names: A Python `iterable` of `str` variable names.\n    qm_vals: A Python `iterable`, the same length as `names`,\n      whose elements are Numpy `array`s, of any shape, containing\n      posterior means of weight varibles.\n    qs_vals: A Python `iterable`, the same length as `names`,\n      whose elements are Numpy `array`s, of any shape, containing\n      posterior standard deviations of weight varibles.\n    fname: Python `str` filename to save the plot to.", "docstring_tokens": ["Save", "a", "PNG", "plot", "with", "histograms", "of", "weight", "means", "and", "stddevs", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/bayesian_neural_network.py#L90-L121", "partition": "test", "index": 870, "time": "2018-03-30 17:12:01"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/logistic_regression.py", "func_name": "visualize_decision", "original_string": "def visualize_decision(features, labels, true_w_b, candidate_w_bs, fname):\n  \"\"\"Utility method to visualize decision boundaries in R^2.\n\n  Args:\n    features: Input points, as a Numpy `array` of shape `[num_examples, 2]`.\n    labels: Numpy `float`-like array of shape `[num_examples, 1]` giving a\n      label for each point.\n    true_w_b: A `tuple` `(w, b)` where `w` is a Numpy array of\n       shape `[2]` and `b` is a scalar `float`, interpreted as a\n       decision rule of the form `dot(features, w) + b > 0`.\n    candidate_w_bs: Python `iterable` containing tuples of the same form as\n       true_w_b.\n    fname: The filename to save the plot as a PNG image (Python `str`).\n  \"\"\"\n  fig = figure.Figure(figsize=(6, 6))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n  ax = fig.add_subplot(1, 1, 1)\n  ax.scatter(features[:, 0], features[:, 1],\n             c=np.float32(labels[:, 0]),\n             cmap=cm.get_cmap(\"binary\"),\n             edgecolors=\"k\")\n\n  def plot_weights(w, b, **kwargs):\n    w1, w2 = w\n    x1s = np.linspace(-1, 1, 100)\n    x2s = -(w1  * x1s + b) / w2\n    ax.plot(x1s, x2s, **kwargs)\n\n  for w, b in candidate_w_bs:\n    plot_weights(w, b,\n                 alpha=1./np.sqrt(len(candidate_w_bs)),\n                 lw=1, color=\"blue\")\n\n  if true_w_b is not None:\n    plot_weights(*true_w_b, lw=4,\n                 color=\"green\", label=\"true separator\")\n\n  ax.set_xlim([-1.5, 1.5])\n  ax.set_ylim([-1.5, 1.5])\n  ax.legend()\n\n  canvas.print_figure(fname, format=\"png\")\n  print(\"saved {}\".format(fname))", "language": "python", "code": "def visualize_decision(features, labels, true_w_b, candidate_w_bs, fname):\n  \"\"\"Utility method to visualize decision boundaries in R^2.\n\n  Args:\n    features: Input points, as a Numpy `array` of shape `[num_examples, 2]`.\n    labels: Numpy `float`-like array of shape `[num_examples, 1]` giving a\n      label for each point.\n    true_w_b: A `tuple` `(w, b)` where `w` is a Numpy array of\n       shape `[2]` and `b` is a scalar `float`, interpreted as a\n       decision rule of the form `dot(features, w) + b > 0`.\n    candidate_w_bs: Python `iterable` containing tuples of the same form as\n       true_w_b.\n    fname: The filename to save the plot as a PNG image (Python `str`).\n  \"\"\"\n  fig = figure.Figure(figsize=(6, 6))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n  ax = fig.add_subplot(1, 1, 1)\n  ax.scatter(features[:, 0], features[:, 1],\n             c=np.float32(labels[:, 0]),\n             cmap=cm.get_cmap(\"binary\"),\n             edgecolors=\"k\")\n\n  def plot_weights(w, b, **kwargs):\n    w1, w2 = w\n    x1s = np.linspace(-1, 1, 100)\n    x2s = -(w1  * x1s + b) / w2\n    ax.plot(x1s, x2s, **kwargs)\n\n  for w, b in candidate_w_bs:\n    plot_weights(w, b,\n                 alpha=1./np.sqrt(len(candidate_w_bs)),\n                 lw=1, color=\"blue\")\n\n  if true_w_b is not None:\n    plot_weights(*true_w_b, lw=4,\n                 color=\"green\", label=\"true separator\")\n\n  ax.set_xlim([-1.5, 1.5])\n  ax.set_ylim([-1.5, 1.5])\n  ax.legend()\n\n  canvas.print_figure(fname, format=\"png\")\n  print(\"saved {}\".format(fname))", "code_tokens": ["def", "visualize_decision", "(", "features", ",", "labels", ",", "true_w_b", ",", "candidate_w_bs", ",", "fname", ")", ":", "fig", "=", "figure", ".", "Figure", "(", "figsize", "=", "(", "6", ",", "6", ")", ")", "canvas", "=", "backend_agg", ".", "FigureCanvasAgg", "(", "fig", ")", "ax", "=", "fig", ".", "add_subplot", "(", "1", ",", "1", ",", "1", ")", "ax", ".", "scatter", "(", "features", "[", ":", ",", "0", "]", ",", "features", "[", ":", ",", "1", "]", ",", "c", "=", "np", ".", "float32", "(", "labels", "[", ":", ",", "0", "]", ")", ",", "cmap", "=", "cm", ".", "get_cmap", "(", "\"binary\"", ")", ",", "edgecolors", "=", "\"k\"", ")", "def", "plot_weights", "(", "w", ",", "b", ",", "*", "*", "kwargs", ")", ":", "w1", ",", "w2", "=", "w", "x1s", "=", "np", ".", "linspace", "(", "-", "1", ",", "1", ",", "100", ")", "x2s", "=", "-", "(", "w1", "*", "x1s", "+", "b", ")", "/", "w2", "ax", ".", "plot", "(", "x1s", ",", "x2s", ",", "*", "*", "kwargs", ")", "for", "w", ",", "b", "in", "candidate_w_bs", ":", "plot_weights", "(", "w", ",", "b", ",", "alpha", "=", "1.", "/", "np", ".", "sqrt", "(", "len", "(", "candidate_w_bs", ")", ")", ",", "lw", "=", "1", ",", "color", "=", "\"blue\"", ")", "if", "true_w_b", "is", "not", "None", ":", "plot_weights", "(", "*", "true_w_b", ",", "lw", "=", "4", ",", "color", "=", "\"green\"", ",", "label", "=", "\"true separator\"", ")", "ax", ".", "set_xlim", "(", "[", "-", "1.5", ",", "1.5", "]", ")", "ax", ".", "set_ylim", "(", "[", "-", "1.5", ",", "1.5", "]", ")", "ax", ".", "legend", "(", ")", "canvas", ".", "print_figure", "(", "fname", ",", "format", "=", "\"png\"", ")", "print", "(", "\"saved {}\"", ".", "format", "(", "fname", ")", ")"], "docstring": "Utility method to visualize decision boundaries in R^2.\n\n  Args:\n    features: Input points, as a Numpy `array` of shape `[num_examples, 2]`.\n    labels: Numpy `float`-like array of shape `[num_examples, 1]` giving a\n      label for each point.\n    true_w_b: A `tuple` `(w, b)` where `w` is a Numpy array of\n       shape `[2]` and `b` is a scalar `float`, interpreted as a\n       decision rule of the form `dot(features, w) + b > 0`.\n    candidate_w_bs: Python `iterable` containing tuples of the same form as\n       true_w_b.\n    fname: The filename to save the plot as a PNG image (Python `str`).", "docstring_tokens": ["Utility", "method", "to", "visualize", "decision", "boundaries", "in", "R^2", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/logistic_regression.py#L89-L131", "partition": "test", "index": 632, "time": "2018-03-30 17:12:01"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/logistic_regression.py", "func_name": "build_input_pipeline", "original_string": "def build_input_pipeline(x, y, batch_size):\n  \"\"\"Build a Dataset iterator for supervised classification.\n\n  Args:\n    x: Numpy `array` of features, indexed by the first dimension.\n    y: Numpy `array` of labels, with the same first dimension as `x`.\n    batch_size: Number of elements in each training batch.\n\n  Returns:\n    batch_features: `Tensor` feed  features, of shape\n      `[batch_size] + x.shape[1:]`.\n    batch_labels: `Tensor` feed of labels, of shape\n      `[batch_size] + y.shape[1:]`.\n  \"\"\"\n  training_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n  training_batches = training_dataset.repeat().batch(batch_size)\n  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n  batch_features, batch_labels = training_iterator.get_next()\n  return batch_features, batch_labels", "language": "python", "code": "def build_input_pipeline(x, y, batch_size):\n  \"\"\"Build a Dataset iterator for supervised classification.\n\n  Args:\n    x: Numpy `array` of features, indexed by the first dimension.\n    y: Numpy `array` of labels, with the same first dimension as `x`.\n    batch_size: Number of elements in each training batch.\n\n  Returns:\n    batch_features: `Tensor` feed  features, of shape\n      `[batch_size] + x.shape[1:]`.\n    batch_labels: `Tensor` feed of labels, of shape\n      `[batch_size] + y.shape[1:]`.\n  \"\"\"\n  training_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n  training_batches = training_dataset.repeat().batch(batch_size)\n  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n  batch_features, batch_labels = training_iterator.get_next()\n  return batch_features, batch_labels", "code_tokens": ["def", "build_input_pipeline", "(", "x", ",", "y", ",", "batch_size", ")", ":", "training_dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "(", "x", ",", "y", ")", ")", "training_batches", "=", "training_dataset", ".", "repeat", "(", ")", ".", "batch", "(", "batch_size", ")", "training_iterator", "=", "tf", ".", "compat", ".", "v1", ".", "data", ".", "make_one_shot_iterator", "(", "training_batches", ")", "batch_features", ",", "batch_labels", "=", "training_iterator", ".", "get_next", "(", ")", "return", "batch_features", ",", "batch_labels"], "docstring": "Build a Dataset iterator for supervised classification.\n\n  Args:\n    x: Numpy `array` of features, indexed by the first dimension.\n    y: Numpy `array` of labels, with the same first dimension as `x`.\n    batch_size: Number of elements in each training batch.\n\n  Returns:\n    batch_features: `Tensor` feed  features, of shape\n      `[batch_size] + x.shape[1:]`.\n    batch_labels: `Tensor` feed of labels, of shape\n      `[batch_size] + y.shape[1:]`.", "docstring_tokens": ["Build", "a", "Dataset", "iterator", "for", "supervised", "classification", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/logistic_regression.py#L134-L152", "partition": "test", "index": 633, "time": "2018-03-30 17:12:01"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/util.py", "func_name": "deserialize_function", "original_string": "def deserialize_function(serial, function_type):\n  \"\"\"Deserializes the Keras-serialized function.\n\n  (De)serializing Python functions from/to bytecode is unsafe. Therefore we\n  also use the function's type as an anonymous function ('lambda') or named\n  function in the Python environment ('function'). In the latter case, this lets\n  us use the Python scope to obtain the function rather than reload it from\n  bytecode. (Note that both cases are brittle!)\n\n  Keras-deserialized functions do not perform lexical scoping. Any modules that\n  the function requires must be imported within the function itself.\n\n  This serialization mimicks the implementation in `tf.keras.layers.Lambda`.\n\n  Args:\n    serial: Serialized Keras object: typically a dict, string, or bytecode.\n    function_type: Python string denoting 'function' or 'lambda'.\n\n  Returns:\n    function: Function the serialized Keras object represents.\n\n  #### Examples\n\n  ```python\n  serial, function_type = serialize_function(lambda x: x)\n  function = deserialize_function(serial, function_type)\n  assert function(2.3) == 2.3  # function is identity\n  ```\n\n  \"\"\"\n  if function_type == 'function':\n    # Simple lookup in custom objects\n    function = tf.keras.utils.deserialize_keras_object(serial)\n  elif function_type == 'lambda':\n    # Unsafe deserialization from bytecode\n    function = generic_utils.func_load(serial)\n  else:\n    raise TypeError('Unknown function type:', function_type)\n  return function", "language": "python", "code": "def deserialize_function(serial, function_type):\n  \"\"\"Deserializes the Keras-serialized function.\n\n  (De)serializing Python functions from/to bytecode is unsafe. Therefore we\n  also use the function's type as an anonymous function ('lambda') or named\n  function in the Python environment ('function'). In the latter case, this lets\n  us use the Python scope to obtain the function rather than reload it from\n  bytecode. (Note that both cases are brittle!)\n\n  Keras-deserialized functions do not perform lexical scoping. Any modules that\n  the function requires must be imported within the function itself.\n\n  This serialization mimicks the implementation in `tf.keras.layers.Lambda`.\n\n  Args:\n    serial: Serialized Keras object: typically a dict, string, or bytecode.\n    function_type: Python string denoting 'function' or 'lambda'.\n\n  Returns:\n    function: Function the serialized Keras object represents.\n\n  #### Examples\n\n  ```python\n  serial, function_type = serialize_function(lambda x: x)\n  function = deserialize_function(serial, function_type)\n  assert function(2.3) == 2.3  # function is identity\n  ```\n\n  \"\"\"\n  if function_type == 'function':\n    # Simple lookup in custom objects\n    function = tf.keras.utils.deserialize_keras_object(serial)\n  elif function_type == 'lambda':\n    # Unsafe deserialization from bytecode\n    function = generic_utils.func_load(serial)\n  else:\n    raise TypeError('Unknown function type:', function_type)\n  return function", "code_tokens": ["def", "deserialize_function", "(", "serial", ",", "function_type", ")", ":", "if", "function_type", "==", "'function'", ":", "# Simple lookup in custom objects", "function", "=", "tf", ".", "keras", ".", "utils", ".", "deserialize_keras_object", "(", "serial", ")", "elif", "function_type", "==", "'lambda'", ":", "# Unsafe deserialization from bytecode", "function", "=", "generic_utils", ".", "func_load", "(", "serial", ")", "else", ":", "raise", "TypeError", "(", "'Unknown function type:'", ",", "function_type", ")", "return", "function"], "docstring": "Deserializes the Keras-serialized function.\n\n  (De)serializing Python functions from/to bytecode is unsafe. Therefore we\n  also use the function's type as an anonymous function ('lambda') or named\n  function in the Python environment ('function'). In the latter case, this lets\n  us use the Python scope to obtain the function rather than reload it from\n  bytecode. (Note that both cases are brittle!)\n\n  Keras-deserialized functions do not perform lexical scoping. Any modules that\n  the function requires must be imported within the function itself.\n\n  This serialization mimicks the implementation in `tf.keras.layers.Lambda`.\n\n  Args:\n    serial: Serialized Keras object: typically a dict, string, or bytecode.\n    function_type: Python string denoting 'function' or 'lambda'.\n\n  Returns:\n    function: Function the serialized Keras object represents.\n\n  #### Examples\n\n  ```python\n  serial, function_type = serialize_function(lambda x: x)\n  function = deserialize_function(serial, function_type)\n  assert function(2.3) == 2.3  # function is identity\n  ```", "docstring_tokens": ["Deserializes", "the", "Keras", "-", "serialized", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/util.py#L219-L257", "partition": "test", "index": 810, "time": "2018-04-10 10:14:57"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/util.py", "func_name": "serialize_function", "original_string": "def serialize_function(func):\n  \"\"\"Serializes function for Keras.\n\n  (De)serializing Python functions from/to bytecode is unsafe. Therefore we\n  return the function's type as an anonymous function ('lambda') or named\n  function in the Python environment ('function'). In the latter case, this lets\n  us use the Python scope to obtain the function rather than reload it from\n  bytecode. (Note that both cases are brittle!)\n\n  This serialization mimicks the implementation in `tf.keras.layers.Lambda`.\n\n  Args:\n    func: Python function to serialize.\n\n  Returns:\n    (serial, function_type): Serialized object, which is a tuple of its\n    bytecode (if function is anonymous) or name (if function is named), and its\n    function type.\n  \"\"\"\n  if isinstance(func, types.LambdaType):\n    return generic_utils.func_dump(func), 'lambda'\n  return func.__name__, 'function'", "language": "python", "code": "def serialize_function(func):\n  \"\"\"Serializes function for Keras.\n\n  (De)serializing Python functions from/to bytecode is unsafe. Therefore we\n  return the function's type as an anonymous function ('lambda') or named\n  function in the Python environment ('function'). In the latter case, this lets\n  us use the Python scope to obtain the function rather than reload it from\n  bytecode. (Note that both cases are brittle!)\n\n  This serialization mimicks the implementation in `tf.keras.layers.Lambda`.\n\n  Args:\n    func: Python function to serialize.\n\n  Returns:\n    (serial, function_type): Serialized object, which is a tuple of its\n    bytecode (if function is anonymous) or name (if function is named), and its\n    function type.\n  \"\"\"\n  if isinstance(func, types.LambdaType):\n    return generic_utils.func_dump(func), 'lambda'\n  return func.__name__, 'function'", "code_tokens": ["def", "serialize_function", "(", "func", ")", ":", "if", "isinstance", "(", "func", ",", "types", ".", "LambdaType", ")", ":", "return", "generic_utils", ".", "func_dump", "(", "func", ")", ",", "'lambda'", "return", "func", ".", "__name__", ",", "'function'"], "docstring": "Serializes function for Keras.\n\n  (De)serializing Python functions from/to bytecode is unsafe. Therefore we\n  return the function's type as an anonymous function ('lambda') or named\n  function in the Python environment ('function'). In the latter case, this lets\n  us use the Python scope to obtain the function rather than reload it from\n  bytecode. (Note that both cases are brittle!)\n\n  This serialization mimicks the implementation in `tf.keras.layers.Lambda`.\n\n  Args:\n    func: Python function to serialize.\n\n  Returns:\n    (serial, function_type): Serialized object, which is a tuple of its\n    bytecode (if function is anonymous) or name (if function is named), and its\n    function type.", "docstring_tokens": ["Serializes", "function", "for", "Keras", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/util.py#L260-L281", "partition": "test", "index": 811, "time": "2018-04-10 10:14:57"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/conv_variational.py", "func_name": "_ConvVariational.from_config", "original_string": "def from_config(cls, config):\n    \"\"\"Creates a layer from its config.\n\n    This method is the reverse of `get_config`, capable of instantiating the\n    same layer from the config dictionary.\n\n    Args:\n      config: A Python dictionary, typically the output of `get_config`.\n\n    Returns:\n      layer: A layer instance.\n    \"\"\"\n    config = config.copy()\n    function_keys = [\n        'kernel_posterior_fn',\n        'kernel_posterior_tensor_fn',\n        'kernel_prior_fn',\n        'kernel_divergence_fn',\n        'bias_posterior_fn',\n        'bias_posterior_tensor_fn',\n        'bias_prior_fn',\n        'bias_divergence_fn',\n    ]\n    for function_key in function_keys:\n      serial = config[function_key]\n      function_type = config.pop(function_key + '_type')\n      if serial is not None:\n        config[function_key] = tfp_layers_util.deserialize_function(\n            serial,\n            function_type=function_type)\n    return cls(**config)", "language": "python", "code": "def from_config(cls, config):\n    \"\"\"Creates a layer from its config.\n\n    This method is the reverse of `get_config`, capable of instantiating the\n    same layer from the config dictionary.\n\n    Args:\n      config: A Python dictionary, typically the output of `get_config`.\n\n    Returns:\n      layer: A layer instance.\n    \"\"\"\n    config = config.copy()\n    function_keys = [\n        'kernel_posterior_fn',\n        'kernel_posterior_tensor_fn',\n        'kernel_prior_fn',\n        'kernel_divergence_fn',\n        'bias_posterior_fn',\n        'bias_posterior_tensor_fn',\n        'bias_prior_fn',\n        'bias_divergence_fn',\n    ]\n    for function_key in function_keys:\n      serial = config[function_key]\n      function_type = config.pop(function_key + '_type')\n      if serial is not None:\n        config[function_key] = tfp_layers_util.deserialize_function(\n            serial,\n            function_type=function_type)\n    return cls(**config)", "code_tokens": ["def", "from_config", "(", "cls", ",", "config", ")", ":", "config", "=", "config", ".", "copy", "(", ")", "function_keys", "=", "[", "'kernel_posterior_fn'", ",", "'kernel_posterior_tensor_fn'", ",", "'kernel_prior_fn'", ",", "'kernel_divergence_fn'", ",", "'bias_posterior_fn'", ",", "'bias_posterior_tensor_fn'", ",", "'bias_prior_fn'", ",", "'bias_divergence_fn'", ",", "]", "for", "function_key", "in", "function_keys", ":", "serial", "=", "config", "[", "function_key", "]", "function_type", "=", "config", ".", "pop", "(", "function_key", "+", "'_type'", ")", "if", "serial", "is", "not", "None", ":", "config", "[", "function_key", "]", "=", "tfp_layers_util", ".", "deserialize_function", "(", "serial", ",", "function_type", "=", "function_type", ")", "return", "cls", "(", "*", "*", "config", ")"], "docstring": "Creates a layer from its config.\n\n    This method is the reverse of `get_config`, capable of instantiating the\n    same layer from the config dictionary.\n\n    Args:\n      config: A Python dictionary, typically the output of `get_config`.\n\n    Returns:\n      layer: A layer instance.", "docstring_tokens": ["Creates", "a", "layer", "from", "its", "config", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/conv_variational.py#L333-L363", "partition": "test", "index": 644, "time": "2018-04-10 10:14:57"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/linalg.py", "func_name": "pinv", "original_string": "def pinv(a, rcond=None, validate_args=False, name=None):\n  \"\"\"Compute the Moore-Penrose pseudo-inverse of a matrix.\n\n  Calculate the [generalized inverse of a matrix](\n  https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) using its\n  singular-value decomposition (SVD) and including all large singular values.\n\n  The pseudo-inverse of a matrix `A`, is defined as: \"the matrix that 'solves'\n  [the least-squares problem] `A @ x = b`,\" i.e., if `x_hat` is a solution, then\n  `A_pinv` is the matrix such that `x_hat = A_pinv @ b`. It can be shown that if\n  `U @ Sigma @ V.T = A` is the singular value decomposition of `A`, then\n  `A_pinv = V @ inv(Sigma) U^T`. [(Strang, 1980)][1]\n\n  This function is analogous to [`numpy.linalg.pinv`](\n  https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html).\n  It differs only in default value of `rcond`. In `numpy.linalg.pinv`, the\n  default `rcond` is `1e-15`. Here the default is\n  `10. * max(num_rows, num_cols) * np.finfo(dtype).eps`.\n\n  Args:\n    a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be\n      pseudo-inverted.\n    rcond: `Tensor` of small singular value cutoffs.  Singular values smaller\n      (in modulus) than `rcond` * largest_singular_value (again, in modulus) are\n      set to zero. Must broadcast against `tf.shape(a)[:-2]`.\n      Default value: `10. * max(num_rows, num_cols) * np.finfo(a.dtype).eps`.\n    validate_args: When `True`, additional assertions might be embedded in the\n      graph.\n      Default value: `False` (i.e., no graph assertions are added).\n    name: Python `str` prefixed to ops created by this function.\n      Default value: \"pinv\".\n\n  Returns:\n    a_pinv: The pseudo-inverse of input `a`. Has same shape as `a` except\n      rightmost two dimensions are transposed.\n\n  Raises:\n    TypeError: if input `a` does not have `float`-like `dtype`.\n    ValueError: if input `a` has fewer than 2 dimensions.\n\n  #### Examples\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  a = tf.constant([[1.,  0.4,  0.5],\n                   [0.4, 0.2,  0.25],\n                   [0.5, 0.25, 0.35]])\n  tf.matmul(tfp.math.pinv(a), a)\n  # ==> array([[1., 0., 0.],\n               [0., 1., 0.],\n               [0., 0., 1.]], dtype=float32)\n\n  a = tf.constant([[1.,  0.4,  0.5,  1.],\n                   [0.4, 0.2,  0.25, 2.],\n                   [0.5, 0.25, 0.35, 3.]])\n  tf.matmul(tfp.math.pinv(a), a)\n  # ==> array([[ 0.76,  0.37,  0.21, -0.02],\n               [ 0.37,  0.43, -0.33,  0.02],\n               [ 0.21, -0.33,  0.81,  0.01],\n               [-0.02,  0.02,  0.01,  1.  ]], dtype=float32)\n  ```\n\n  #### References\n\n  [1]: G. Strang. \"Linear Algebra and Its Applications, 2nd Ed.\" Academic Press,\n       Inc., 1980, pp. 139-142.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'pinv', [a, rcond]):\n    a = tf.convert_to_tensor(value=a, name='a')\n\n    assertions = _maybe_validate_matrix(a, validate_args)\n    if assertions:\n      with tf.control_dependencies(assertions):\n        a = tf.identity(a)\n\n    dtype = a.dtype.as_numpy_dtype\n\n    if rcond is None:\n      def get_dim_size(dim):\n        if tf.compat.dimension_value(a.shape[dim]) is not None:\n          return tf.compat.dimension_value(a.shape[dim])\n        return tf.shape(input=a)[dim]\n\n      num_rows = get_dim_size(-2)\n      num_cols = get_dim_size(-1)\n      if isinstance(num_rows, int) and isinstance(num_cols, int):\n        max_rows_cols = float(max(num_rows, num_cols))\n      else:\n        max_rows_cols = tf.cast(tf.maximum(num_rows, num_cols), dtype)\n      rcond = 10. * max_rows_cols * np.finfo(dtype).eps\n\n    rcond = tf.convert_to_tensor(value=rcond, dtype=dtype, name='rcond')\n\n    # Calculate pseudo inverse via SVD.\n    # Note: if a is symmetric then u == v. (We might observe additional\n    # performance by explicitly setting `v = u` in such cases.)\n    [\n        singular_values,         # Sigma\n        left_singular_vectors,   # U\n        right_singular_vectors,  # V\n    ] = tf.linalg.svd(a, full_matrices=False, compute_uv=True)\n\n    # Saturate small singular values to inf. This has the effect of make\n    # `1. / s = 0.` while not resulting in `NaN` gradients.\n    cutoff = rcond * tf.reduce_max(input_tensor=singular_values, axis=-1)\n    singular_values = tf.where(\n        singular_values > cutoff[..., tf.newaxis], singular_values,\n        tf.fill(tf.shape(input=singular_values), np.array(np.inf, dtype)))\n\n    # Although `a == tf.matmul(u, s * v, transpose_b=True)` we swap\n    # `u` and `v` here so that `tf.matmul(pinv(A), A) = tf.eye()`, i.e.,\n    # a matrix inverse has \"transposed\" semantics.\n    a_pinv = tf.matmul(\n        right_singular_vectors / singular_values[..., tf.newaxis, :],\n        left_singular_vectors,\n        adjoint_b=True)\n\n    if a.shape.ndims is not None:\n      a_pinv.set_shape(a.shape[:-2].concatenate([a.shape[-1], a.shape[-2]]))\n\n    return a_pinv", "language": "python", "code": "def pinv(a, rcond=None, validate_args=False, name=None):\n  \"\"\"Compute the Moore-Penrose pseudo-inverse of a matrix.\n\n  Calculate the [generalized inverse of a matrix](\n  https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) using its\n  singular-value decomposition (SVD) and including all large singular values.\n\n  The pseudo-inverse of a matrix `A`, is defined as: \"the matrix that 'solves'\n  [the least-squares problem] `A @ x = b`,\" i.e., if `x_hat` is a solution, then\n  `A_pinv` is the matrix such that `x_hat = A_pinv @ b`. It can be shown that if\n  `U @ Sigma @ V.T = A` is the singular value decomposition of `A`, then\n  `A_pinv = V @ inv(Sigma) U^T`. [(Strang, 1980)][1]\n\n  This function is analogous to [`numpy.linalg.pinv`](\n  https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html).\n  It differs only in default value of `rcond`. In `numpy.linalg.pinv`, the\n  default `rcond` is `1e-15`. Here the default is\n  `10. * max(num_rows, num_cols) * np.finfo(dtype).eps`.\n\n  Args:\n    a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be\n      pseudo-inverted.\n    rcond: `Tensor` of small singular value cutoffs.  Singular values smaller\n      (in modulus) than `rcond` * largest_singular_value (again, in modulus) are\n      set to zero. Must broadcast against `tf.shape(a)[:-2]`.\n      Default value: `10. * max(num_rows, num_cols) * np.finfo(a.dtype).eps`.\n    validate_args: When `True`, additional assertions might be embedded in the\n      graph.\n      Default value: `False` (i.e., no graph assertions are added).\n    name: Python `str` prefixed to ops created by this function.\n      Default value: \"pinv\".\n\n  Returns:\n    a_pinv: The pseudo-inverse of input `a`. Has same shape as `a` except\n      rightmost two dimensions are transposed.\n\n  Raises:\n    TypeError: if input `a` does not have `float`-like `dtype`.\n    ValueError: if input `a` has fewer than 2 dimensions.\n\n  #### Examples\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  a = tf.constant([[1.,  0.4,  0.5],\n                   [0.4, 0.2,  0.25],\n                   [0.5, 0.25, 0.35]])\n  tf.matmul(tfp.math.pinv(a), a)\n  # ==> array([[1., 0., 0.],\n               [0., 1., 0.],\n               [0., 0., 1.]], dtype=float32)\n\n  a = tf.constant([[1.,  0.4,  0.5,  1.],\n                   [0.4, 0.2,  0.25, 2.],\n                   [0.5, 0.25, 0.35, 3.]])\n  tf.matmul(tfp.math.pinv(a), a)\n  # ==> array([[ 0.76,  0.37,  0.21, -0.02],\n               [ 0.37,  0.43, -0.33,  0.02],\n               [ 0.21, -0.33,  0.81,  0.01],\n               [-0.02,  0.02,  0.01,  1.  ]], dtype=float32)\n  ```\n\n  #### References\n\n  [1]: G. Strang. \"Linear Algebra and Its Applications, 2nd Ed.\" Academic Press,\n       Inc., 1980, pp. 139-142.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'pinv', [a, rcond]):\n    a = tf.convert_to_tensor(value=a, name='a')\n\n    assertions = _maybe_validate_matrix(a, validate_args)\n    if assertions:\n      with tf.control_dependencies(assertions):\n        a = tf.identity(a)\n\n    dtype = a.dtype.as_numpy_dtype\n\n    if rcond is None:\n      def get_dim_size(dim):\n        if tf.compat.dimension_value(a.shape[dim]) is not None:\n          return tf.compat.dimension_value(a.shape[dim])\n        return tf.shape(input=a)[dim]\n\n      num_rows = get_dim_size(-2)\n      num_cols = get_dim_size(-1)\n      if isinstance(num_rows, int) and isinstance(num_cols, int):\n        max_rows_cols = float(max(num_rows, num_cols))\n      else:\n        max_rows_cols = tf.cast(tf.maximum(num_rows, num_cols), dtype)\n      rcond = 10. * max_rows_cols * np.finfo(dtype).eps\n\n    rcond = tf.convert_to_tensor(value=rcond, dtype=dtype, name='rcond')\n\n    # Calculate pseudo inverse via SVD.\n    # Note: if a is symmetric then u == v. (We might observe additional\n    # performance by explicitly setting `v = u` in such cases.)\n    [\n        singular_values,         # Sigma\n        left_singular_vectors,   # U\n        right_singular_vectors,  # V\n    ] = tf.linalg.svd(a, full_matrices=False, compute_uv=True)\n\n    # Saturate small singular values to inf. This has the effect of make\n    # `1. / s = 0.` while not resulting in `NaN` gradients.\n    cutoff = rcond * tf.reduce_max(input_tensor=singular_values, axis=-1)\n    singular_values = tf.where(\n        singular_values > cutoff[..., tf.newaxis], singular_values,\n        tf.fill(tf.shape(input=singular_values), np.array(np.inf, dtype)))\n\n    # Although `a == tf.matmul(u, s * v, transpose_b=True)` we swap\n    # `u` and `v` here so that `tf.matmul(pinv(A), A) = tf.eye()`, i.e.,\n    # a matrix inverse has \"transposed\" semantics.\n    a_pinv = tf.matmul(\n        right_singular_vectors / singular_values[..., tf.newaxis, :],\n        left_singular_vectors,\n        adjoint_b=True)\n\n    if a.shape.ndims is not None:\n      a_pinv.set_shape(a.shape[:-2].concatenate([a.shape[-1], a.shape[-2]]))\n\n    return a_pinv", "code_tokens": ["def", "pinv", "(", "a", ",", "rcond", "=", "None", ",", "validate_args", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'pinv'", ",", "[", "a", ",", "rcond", "]", ")", ":", "a", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "a", ",", "name", "=", "'a'", ")", "assertions", "=", "_maybe_validate_matrix", "(", "a", ",", "validate_args", ")", "if", "assertions", ":", "with", "tf", ".", "control_dependencies", "(", "assertions", ")", ":", "a", "=", "tf", ".", "identity", "(", "a", ")", "dtype", "=", "a", ".", "dtype", ".", "as_numpy_dtype", "if", "rcond", "is", "None", ":", "def", "get_dim_size", "(", "dim", ")", ":", "if", "tf", ".", "compat", ".", "dimension_value", "(", "a", ".", "shape", "[", "dim", "]", ")", "is", "not", "None", ":", "return", "tf", ".", "compat", ".", "dimension_value", "(", "a", ".", "shape", "[", "dim", "]", ")", "return", "tf", ".", "shape", "(", "input", "=", "a", ")", "[", "dim", "]", "num_rows", "=", "get_dim_size", "(", "-", "2", ")", "num_cols", "=", "get_dim_size", "(", "-", "1", ")", "if", "isinstance", "(", "num_rows", ",", "int", ")", "and", "isinstance", "(", "num_cols", ",", "int", ")", ":", "max_rows_cols", "=", "float", "(", "max", "(", "num_rows", ",", "num_cols", ")", ")", "else", ":", "max_rows_cols", "=", "tf", ".", "cast", "(", "tf", ".", "maximum", "(", "num_rows", ",", "num_cols", ")", ",", "dtype", ")", "rcond", "=", "10.", "*", "max_rows_cols", "*", "np", ".", "finfo", "(", "dtype", ")", ".", "eps", "rcond", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "rcond", ",", "dtype", "=", "dtype", ",", "name", "=", "'rcond'", ")", "# Calculate pseudo inverse via SVD.", "# Note: if a is symmetric then u == v. (We might observe additional", "# performance by explicitly setting `v = u` in such cases.)", "[", "singular_values", ",", "# Sigma", "left_singular_vectors", ",", "# U", "right_singular_vectors", ",", "# V", "]", "=", "tf", ".", "linalg", ".", "svd", "(", "a", ",", "full_matrices", "=", "False", ",", "compute_uv", "=", "True", ")", "# Saturate small singular values to inf. This has the effect of make", "# `1. / s = 0.` while not resulting in `NaN` gradients.", "cutoff", "=", "rcond", "*", "tf", ".", "reduce_max", "(", "input_tensor", "=", "singular_values", ",", "axis", "=", "-", "1", ")", "singular_values", "=", "tf", ".", "where", "(", "singular_values", ">", "cutoff", "[", "...", ",", "tf", ".", "newaxis", "]", ",", "singular_values", ",", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "input", "=", "singular_values", ")", ",", "np", ".", "array", "(", "np", ".", "inf", ",", "dtype", ")", ")", ")", "# Although `a == tf.matmul(u, s * v, transpose_b=True)` we swap", "# `u` and `v` here so that `tf.matmul(pinv(A), A) = tf.eye()`, i.e.,", "# a matrix inverse has \"transposed\" semantics.", "a_pinv", "=", "tf", ".", "matmul", "(", "right_singular_vectors", "/", "singular_values", "[", "...", ",", "tf", ".", "newaxis", ",", ":", "]", ",", "left_singular_vectors", ",", "adjoint_b", "=", "True", ")", "if", "a", ".", "shape", ".", "ndims", "is", "not", "None", ":", "a_pinv", ".", "set_shape", "(", "a", ".", "shape", "[", ":", "-", "2", "]", ".", "concatenate", "(", "[", "a", ".", "shape", "[", "-", "1", "]", ",", "a", ".", "shape", "[", "-", "2", "]", "]", ")", ")", "return", "a_pinv"], "docstring": "Compute the Moore-Penrose pseudo-inverse of a matrix.\n\n  Calculate the [generalized inverse of a matrix](\n  https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) using its\n  singular-value decomposition (SVD) and including all large singular values.\n\n  The pseudo-inverse of a matrix `A`, is defined as: \"the matrix that 'solves'\n  [the least-squares problem] `A @ x = b`,\" i.e., if `x_hat` is a solution, then\n  `A_pinv` is the matrix such that `x_hat = A_pinv @ b`. It can be shown that if\n  `U @ Sigma @ V.T = A` is the singular value decomposition of `A`, then\n  `A_pinv = V @ inv(Sigma) U^T`. [(Strang, 1980)][1]\n\n  This function is analogous to [`numpy.linalg.pinv`](\n  https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html).\n  It differs only in default value of `rcond`. In `numpy.linalg.pinv`, the\n  default `rcond` is `1e-15`. Here the default is\n  `10. * max(num_rows, num_cols) * np.finfo(dtype).eps`.\n\n  Args:\n    a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be\n      pseudo-inverted.\n    rcond: `Tensor` of small singular value cutoffs.  Singular values smaller\n      (in modulus) than `rcond` * largest_singular_value (again, in modulus) are\n      set to zero. Must broadcast against `tf.shape(a)[:-2]`.\n      Default value: `10. * max(num_rows, num_cols) * np.finfo(a.dtype).eps`.\n    validate_args: When `True`, additional assertions might be embedded in the\n      graph.\n      Default value: `False` (i.e., no graph assertions are added).\n    name: Python `str` prefixed to ops created by this function.\n      Default value: \"pinv\".\n\n  Returns:\n    a_pinv: The pseudo-inverse of input `a`. Has same shape as `a` except\n      rightmost two dimensions are transposed.\n\n  Raises:\n    TypeError: if input `a` does not have `float`-like `dtype`.\n    ValueError: if input `a` has fewer than 2 dimensions.\n\n  #### Examples\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  a = tf.constant([[1.,  0.4,  0.5],\n                   [0.4, 0.2,  0.25],\n                   [0.5, 0.25, 0.35]])\n  tf.matmul(tfp.math.pinv(a), a)\n  # ==> array([[1., 0., 0.],\n               [0., 1., 0.],\n               [0., 0., 1.]], dtype=float32)\n\n  a = tf.constant([[1.,  0.4,  0.5,  1.],\n                   [0.4, 0.2,  0.25, 2.],\n                   [0.5, 0.25, 0.35, 3.]])\n  tf.matmul(tfp.math.pinv(a), a)\n  # ==> array([[ 0.76,  0.37,  0.21, -0.02],\n               [ 0.37,  0.43, -0.33,  0.02],\n               [ 0.21, -0.33,  0.81,  0.01],\n               [-0.02,  0.02,  0.01,  1.  ]], dtype=float32)\n  ```\n\n  #### References\n\n  [1]: G. Strang. \"Linear Algebra and Its Applications, 2nd Ed.\" Academic Press,\n       Inc., 1980, pp. 139-142.", "docstring_tokens": ["Compute", "the", "Moore", "-", "Penrose", "pseudo", "-", "inverse", "of", "a", "matrix", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/linalg.py#L323-L445", "partition": "test", "index": 1106, "time": "2018-04-16 10:08:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/util.py", "func_name": "default_multivariate_normal_fn", "original_string": "def default_multivariate_normal_fn(dtype, shape, name, trainable,\n                                   add_variable_fn):\n  \"\"\"Creates multivariate standard `Normal` distribution.\n\n  Args:\n    dtype: Type of parameter's event.\n    shape: Python `list`-like representing the parameter's event shape.\n    name: Python `str` name prepended to any created (or existing)\n      `tf.Variable`s.\n    trainable: Python `bool` indicating all created `tf.Variable`s should be\n      added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.\n    add_variable_fn: `tf.get_variable`-like `callable` used to create (or\n      access existing) `tf.Variable`s.\n\n  Returns:\n    Multivariate standard `Normal` distribution.\n  \"\"\"\n  del name, trainable, add_variable_fn   # unused\n  dist = tfd.Normal(loc=tf.zeros(shape, dtype), scale=dtype.as_numpy_dtype(1))\n  batch_ndims = tf.size(input=dist.batch_shape_tensor())\n  return tfd.Independent(dist, reinterpreted_batch_ndims=batch_ndims)", "language": "python", "code": "def default_multivariate_normal_fn(dtype, shape, name, trainable,\n                                   add_variable_fn):\n  \"\"\"Creates multivariate standard `Normal` distribution.\n\n  Args:\n    dtype: Type of parameter's event.\n    shape: Python `list`-like representing the parameter's event shape.\n    name: Python `str` name prepended to any created (or existing)\n      `tf.Variable`s.\n    trainable: Python `bool` indicating all created `tf.Variable`s should be\n      added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.\n    add_variable_fn: `tf.get_variable`-like `callable` used to create (or\n      access existing) `tf.Variable`s.\n\n  Returns:\n    Multivariate standard `Normal` distribution.\n  \"\"\"\n  del name, trainable, add_variable_fn   # unused\n  dist = tfd.Normal(loc=tf.zeros(shape, dtype), scale=dtype.as_numpy_dtype(1))\n  batch_ndims = tf.size(input=dist.batch_shape_tensor())\n  return tfd.Independent(dist, reinterpreted_batch_ndims=batch_ndims)", "code_tokens": ["def", "default_multivariate_normal_fn", "(", "dtype", ",", "shape", ",", "name", ",", "trainable", ",", "add_variable_fn", ")", ":", "del", "name", ",", "trainable", ",", "add_variable_fn", "# unused", "dist", "=", "tfd", ".", "Normal", "(", "loc", "=", "tf", ".", "zeros", "(", "shape", ",", "dtype", ")", ",", "scale", "=", "dtype", ".", "as_numpy_dtype", "(", "1", ")", ")", "batch_ndims", "=", "tf", ".", "size", "(", "input", "=", "dist", ".", "batch_shape_tensor", "(", ")", ")", "return", "tfd", ".", "Independent", "(", "dist", ",", "reinterpreted_batch_ndims", "=", "batch_ndims", ")"], "docstring": "Creates multivariate standard `Normal` distribution.\n\n  Args:\n    dtype: Type of parameter's event.\n    shape: Python `list`-like representing the parameter's event shape.\n    name: Python `str` name prepended to any created (or existing)\n      `tf.Variable`s.\n    trainable: Python `bool` indicating all created `tf.Variable`s should be\n      added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.\n    add_variable_fn: `tf.get_variable`-like `callable` used to create (or\n      access existing) `tf.Variable`s.\n\n  Returns:\n    Multivariate standard `Normal` distribution.", "docstring_tokens": ["Creates", "multivariate", "standard", "Normal", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/util.py#L196-L216", "partition": "test", "index": 809, "time": "2018-04-16 17:12:58"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/vae.py", "func_name": "build_fake_input_fns", "original_string": "def build_fake_input_fns(batch_size):\n  \"\"\"Builds fake MNIST-style data for unit testing.\"\"\"\n  random_sample = np.random.rand(batch_size, *IMAGE_SHAPE).astype(\"float32\")\n\n  def train_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size).repeat()\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  def eval_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return train_input_fn, eval_input_fn", "language": "python", "code": "def build_fake_input_fns(batch_size):\n  \"\"\"Builds fake MNIST-style data for unit testing.\"\"\"\n  random_sample = np.random.rand(batch_size, *IMAGE_SHAPE).astype(\"float32\")\n\n  def train_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size).repeat()\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  def eval_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return train_input_fn, eval_input_fn", "code_tokens": ["def", "build_fake_input_fns", "(", "batch_size", ")", ":", "random_sample", "=", "np", ".", "random", ".", "rand", "(", "batch_size", ",", "*", "IMAGE_SHAPE", ")", ".", "astype", "(", "\"float32\"", ")", "def", "train_input_fn", "(", ")", ":", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "random_sample", ")", ".", "map", "(", "lambda", "row", ":", "(", "row", ",", "0", ")", ")", ".", "batch", "(", "batch_size", ")", ".", "repeat", "(", ")", "return", "tf", ".", "compat", ".", "v1", ".", "data", ".", "make_one_shot_iterator", "(", "dataset", ")", ".", "get_next", "(", ")", "def", "eval_input_fn", "(", ")", ":", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "random_sample", ")", ".", "map", "(", "lambda", "row", ":", "(", "row", ",", "0", ")", ")", ".", "batch", "(", "batch_size", ")", "return", "tf", ".", "compat", ".", "v1", ".", "data", ".", "make_one_shot_iterator", "(", "dataset", ")", ".", "get_next", "(", ")", "return", "train_input_fn", ",", "eval_input_fn"], "docstring": "Builds fake MNIST-style data for unit testing.", "docstring_tokens": ["Builds", "fake", "MNIST", "-", "style", "data", "for", "unit", "testing", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/vae.py#L462-L476", "partition": "test", "index": 820, "time": "2018-04-17 12:22:46"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/vae.py", "func_name": "make_mixture_prior", "original_string": "def make_mixture_prior(latent_size, mixture_components):\n  \"\"\"Creates the mixture of Gaussians prior distribution.\n\n  Args:\n    latent_size: The dimensionality of the latent representation.\n    mixture_components: Number of elements of the mixture.\n\n  Returns:\n    random_prior: A `tfd.Distribution` instance representing the distribution\n      over encodings in the absence of any evidence.\n  \"\"\"\n  if mixture_components == 1:\n    # See the module docstring for why we don't learn the parameters here.\n    return tfd.MultivariateNormalDiag(\n        loc=tf.zeros([latent_size]),\n        scale_identity_multiplier=1.0)\n\n  loc = tf.compat.v1.get_variable(\n      name=\"loc\", shape=[mixture_components, latent_size])\n  raw_scale_diag = tf.compat.v1.get_variable(\n      name=\"raw_scale_diag\", shape=[mixture_components, latent_size])\n  mixture_logits = tf.compat.v1.get_variable(\n      name=\"mixture_logits\", shape=[mixture_components])\n\n  return tfd.MixtureSameFamily(\n      components_distribution=tfd.MultivariateNormalDiag(\n          loc=loc,\n          scale_diag=tf.nn.softplus(raw_scale_diag)),\n      mixture_distribution=tfd.Categorical(logits=mixture_logits),\n      name=\"prior\")", "language": "python", "code": "def make_mixture_prior(latent_size, mixture_components):\n  \"\"\"Creates the mixture of Gaussians prior distribution.\n\n  Args:\n    latent_size: The dimensionality of the latent representation.\n    mixture_components: Number of elements of the mixture.\n\n  Returns:\n    random_prior: A `tfd.Distribution` instance representing the distribution\n      over encodings in the absence of any evidence.\n  \"\"\"\n  if mixture_components == 1:\n    # See the module docstring for why we don't learn the parameters here.\n    return tfd.MultivariateNormalDiag(\n        loc=tf.zeros([latent_size]),\n        scale_identity_multiplier=1.0)\n\n  loc = tf.compat.v1.get_variable(\n      name=\"loc\", shape=[mixture_components, latent_size])\n  raw_scale_diag = tf.compat.v1.get_variable(\n      name=\"raw_scale_diag\", shape=[mixture_components, latent_size])\n  mixture_logits = tf.compat.v1.get_variable(\n      name=\"mixture_logits\", shape=[mixture_components])\n\n  return tfd.MixtureSameFamily(\n      components_distribution=tfd.MultivariateNormalDiag(\n          loc=loc,\n          scale_diag=tf.nn.softplus(raw_scale_diag)),\n      mixture_distribution=tfd.Categorical(logits=mixture_logits),\n      name=\"prior\")", "code_tokens": ["def", "make_mixture_prior", "(", "latent_size", ",", "mixture_components", ")", ":", "if", "mixture_components", "==", "1", ":", "# See the module docstring for why we don't learn the parameters here.", "return", "tfd", ".", "MultivariateNormalDiag", "(", "loc", "=", "tf", ".", "zeros", "(", "[", "latent_size", "]", ")", ",", "scale_identity_multiplier", "=", "1.0", ")", "loc", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "name", "=", "\"loc\"", ",", "shape", "=", "[", "mixture_components", ",", "latent_size", "]", ")", "raw_scale_diag", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "name", "=", "\"raw_scale_diag\"", ",", "shape", "=", "[", "mixture_components", ",", "latent_size", "]", ")", "mixture_logits", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "name", "=", "\"mixture_logits\"", ",", "shape", "=", "[", "mixture_components", "]", ")", "return", "tfd", ".", "MixtureSameFamily", "(", "components_distribution", "=", "tfd", ".", "MultivariateNormalDiag", "(", "loc", "=", "loc", ",", "scale_diag", "=", "tf", ".", "nn", ".", "softplus", "(", "raw_scale_diag", ")", ")", ",", "mixture_distribution", "=", "tfd", ".", "Categorical", "(", "logits", "=", "mixture_logits", ")", ",", "name", "=", "\"prior\"", ")"], "docstring": "Creates the mixture of Gaussians prior distribution.\n\n  Args:\n    latent_size: The dimensionality of the latent representation.\n    mixture_components: Number of elements of the mixture.\n\n  Returns:\n    random_prior: A `tfd.Distribution` instance representing the distribution\n      over encodings in the absence of any evidence.", "docstring_tokens": ["Creates", "the", "mixture", "of", "Gaussians", "prior", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/vae.py#L271-L300", "partition": "test", "index": 817, "time": "2018-04-17 12:22:46"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/replica_exchange_mc.py", "func_name": "default_exchange_proposed_fn", "original_string": "def default_exchange_proposed_fn(prob_exchange):\n  \"\"\"Default exchange proposal function, for replica exchange MC.\n\n  With probability `prob_exchange` propose combinations of replica for exchange.\n  When exchanging, create combinations of adjacent replicas in\n  [Replica Exchange Monte Carlo](\n  https://en.wikipedia.org/wiki/Parallel_tempering)\n\n  ```\n  exchange_fn = default_exchange_proposed_fn(prob_exchange=0.5)\n  exchange_proposed = exchange_fn(num_replica=3)\n\n  exchange_proposed.eval()\n  ==> [[0, 1]]  # 1 exchange, 0 <--> 1\n\n  exchange_proposed.eval()\n  ==> []  # 0 exchanges\n  ```\n\n  Args:\n    prob_exchange: Scalar `Tensor` giving probability that any exchanges will\n      be generated.\n\n  Returns:\n    default_exchange_proposed_fn_: Python callable which take a number of\n      replicas (a Python integer), and return combinations of replicas for\n      exchange as an [n, 2] integer `Tensor`, `0 <= n <= num_replica // 2`,\n      with *unique* values in the set `{0, ..., num_replica}`.\n  \"\"\"\n\n  def default_exchange_proposed_fn_(num_replica, seed=None):\n    \"\"\"Default function for `exchange_proposed_fn` of `kernel`.\"\"\"\n    seed_stream = distributions.SeedStream(seed, 'default_exchange_proposed_fn')\n\n    zero_start = tf.random.uniform([], seed=seed_stream()) > 0.5\n    if num_replica % 2 == 0:\n\n      def _exchange():\n        flat_exchange = tf.range(num_replica)\n        if num_replica > 2:\n          start = tf.cast(~zero_start, dtype=tf.int32)\n          end = num_replica - start\n          flat_exchange = flat_exchange[start:end]\n        return tf.reshape(flat_exchange, [tf.size(input=flat_exchange) // 2, 2])\n    else:\n\n      def _exchange():\n        start = tf.cast(zero_start, dtype=tf.int32)\n        end = num_replica - tf.cast(~zero_start, dtype=tf.int32)\n        flat_exchange = tf.range(num_replica)[start:end]\n        return tf.reshape(flat_exchange, [tf.size(input=flat_exchange) // 2, 2])\n\n    def _null_exchange():\n      return tf.reshape(tf.cast([], dtype=tf.int32), shape=[0, 2])\n\n    return tf.cond(\n        pred=tf.random.uniform([], seed=seed_stream()) < prob_exchange,\n        true_fn=_exchange,\n        false_fn=_null_exchange)\n\n  return default_exchange_proposed_fn_", "language": "python", "code": "def default_exchange_proposed_fn(prob_exchange):\n  \"\"\"Default exchange proposal function, for replica exchange MC.\n\n  With probability `prob_exchange` propose combinations of replica for exchange.\n  When exchanging, create combinations of adjacent replicas in\n  [Replica Exchange Monte Carlo](\n  https://en.wikipedia.org/wiki/Parallel_tempering)\n\n  ```\n  exchange_fn = default_exchange_proposed_fn(prob_exchange=0.5)\n  exchange_proposed = exchange_fn(num_replica=3)\n\n  exchange_proposed.eval()\n  ==> [[0, 1]]  # 1 exchange, 0 <--> 1\n\n  exchange_proposed.eval()\n  ==> []  # 0 exchanges\n  ```\n\n  Args:\n    prob_exchange: Scalar `Tensor` giving probability that any exchanges will\n      be generated.\n\n  Returns:\n    default_exchange_proposed_fn_: Python callable which take a number of\n      replicas (a Python integer), and return combinations of replicas for\n      exchange as an [n, 2] integer `Tensor`, `0 <= n <= num_replica // 2`,\n      with *unique* values in the set `{0, ..., num_replica}`.\n  \"\"\"\n\n  def default_exchange_proposed_fn_(num_replica, seed=None):\n    \"\"\"Default function for `exchange_proposed_fn` of `kernel`.\"\"\"\n    seed_stream = distributions.SeedStream(seed, 'default_exchange_proposed_fn')\n\n    zero_start = tf.random.uniform([], seed=seed_stream()) > 0.5\n    if num_replica % 2 == 0:\n\n      def _exchange():\n        flat_exchange = tf.range(num_replica)\n        if num_replica > 2:\n          start = tf.cast(~zero_start, dtype=tf.int32)\n          end = num_replica - start\n          flat_exchange = flat_exchange[start:end]\n        return tf.reshape(flat_exchange, [tf.size(input=flat_exchange) // 2, 2])\n    else:\n\n      def _exchange():\n        start = tf.cast(zero_start, dtype=tf.int32)\n        end = num_replica - tf.cast(~zero_start, dtype=tf.int32)\n        flat_exchange = tf.range(num_replica)[start:end]\n        return tf.reshape(flat_exchange, [tf.size(input=flat_exchange) // 2, 2])\n\n    def _null_exchange():\n      return tf.reshape(tf.cast([], dtype=tf.int32), shape=[0, 2])\n\n    return tf.cond(\n        pred=tf.random.uniform([], seed=seed_stream()) < prob_exchange,\n        true_fn=_exchange,\n        false_fn=_null_exchange)\n\n  return default_exchange_proposed_fn_", "code_tokens": ["def", "default_exchange_proposed_fn", "(", "prob_exchange", ")", ":", "def", "default_exchange_proposed_fn_", "(", "num_replica", ",", "seed", "=", "None", ")", ":", "\"\"\"Default function for `exchange_proposed_fn` of `kernel`.\"\"\"", "seed_stream", "=", "distributions", ".", "SeedStream", "(", "seed", ",", "'default_exchange_proposed_fn'", ")", "zero_start", "=", "tf", ".", "random", ".", "uniform", "(", "[", "]", ",", "seed", "=", "seed_stream", "(", ")", ")", ">", "0.5", "if", "num_replica", "%", "2", "==", "0", ":", "def", "_exchange", "(", ")", ":", "flat_exchange", "=", "tf", ".", "range", "(", "num_replica", ")", "if", "num_replica", ">", "2", ":", "start", "=", "tf", ".", "cast", "(", "~", "zero_start", ",", "dtype", "=", "tf", ".", "int32", ")", "end", "=", "num_replica", "-", "start", "flat_exchange", "=", "flat_exchange", "[", "start", ":", "end", "]", "return", "tf", ".", "reshape", "(", "flat_exchange", ",", "[", "tf", ".", "size", "(", "input", "=", "flat_exchange", ")", "//", "2", ",", "2", "]", ")", "else", ":", "def", "_exchange", "(", ")", ":", "start", "=", "tf", ".", "cast", "(", "zero_start", ",", "dtype", "=", "tf", ".", "int32", ")", "end", "=", "num_replica", "-", "tf", ".", "cast", "(", "~", "zero_start", ",", "dtype", "=", "tf", ".", "int32", ")", "flat_exchange", "=", "tf", ".", "range", "(", "num_replica", ")", "[", "start", ":", "end", "]", "return", "tf", ".", "reshape", "(", "flat_exchange", ",", "[", "tf", ".", "size", "(", "input", "=", "flat_exchange", ")", "//", "2", ",", "2", "]", ")", "def", "_null_exchange", "(", ")", ":", "return", "tf", ".", "reshape", "(", "tf", ".", "cast", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "shape", "=", "[", "0", ",", "2", "]", ")", "return", "tf", ".", "cond", "(", "pred", "=", "tf", ".", "random", ".", "uniform", "(", "[", "]", ",", "seed", "=", "seed_stream", "(", ")", ")", "<", "prob_exchange", ",", "true_fn", "=", "_exchange", ",", "false_fn", "=", "_null_exchange", ")", "return", "default_exchange_proposed_fn_"], "docstring": "Default exchange proposal function, for replica exchange MC.\n\n  With probability `prob_exchange` propose combinations of replica for exchange.\n  When exchanging, create combinations of adjacent replicas in\n  [Replica Exchange Monte Carlo](\n  https://en.wikipedia.org/wiki/Parallel_tempering)\n\n  ```\n  exchange_fn = default_exchange_proposed_fn(prob_exchange=0.5)\n  exchange_proposed = exchange_fn(num_replica=3)\n\n  exchange_proposed.eval()\n  ==> [[0, 1]]  # 1 exchange, 0 <--> 1\n\n  exchange_proposed.eval()\n  ==> []  # 0 exchanges\n  ```\n\n  Args:\n    prob_exchange: Scalar `Tensor` giving probability that any exchanges will\n      be generated.\n\n  Returns:\n    default_exchange_proposed_fn_: Python callable which take a number of\n      replicas (a Python integer), and return combinations of replicas for\n      exchange as an [n, 2] integer `Tensor`, `0 <= n <= num_replica // 2`,\n      with *unique* values in the set `{0, ..., num_replica}`.", "docstring_tokens": ["Default", "exchange", "proposal", "function", "for", "replica", "exchange", "MC", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/replica_exchange_mc.py#L49-L109", "partition": "test", "index": 986, "time": "2018-04-30 11:04:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/langevin.py", "func_name": "_get_drift", "original_string": "def _get_drift(step_size_parts, volatility_parts, grads_volatility,\n               grads_target_log_prob,\n               name=None):\n  \"\"\"Compute diffusion drift at the current location `current_state`.\n\n  The drift of the diffusion at is computed as\n\n  ```none\n  0.5 * `step_size` * volatility_parts * `target_log_prob_fn(current_state)`\n  + `step_size` * `grads_volatility`\n  ```\n\n  where `volatility_parts` = `volatility_fn(current_state)**2` and\n  `grads_volatility` is a gradient of `volatility_parts` at the `current_state`.\n\n  Args:\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      Euler-Maruyama method. Must broadcast with the shape of\n      `volatility_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    volatility_parts: Python `list` of `Tensor`s representing the value of\n      `volatility_fn(*state_parts)`.\n    grads_volatility: Python list of `Tensor`s representing the value of the\n      gradient of `volatility_parts**2` wrt the state of the chain.\n    grads_target_log_prob: Python list of `Tensor`s representing\n      gradient of `target_log_prob_fn(*state_parts`) wrt `state_parts`. Must\n      have same shape as `volatility_parts`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'mala_get_drift').\n\n  Returns:\n    drift_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, 'mala_get_drift', [\n      step_size_parts, volatility_parts, grads_volatility, grads_target_log_prob\n  ]):\n\n    drift_parts = []\n\n    for step_size, volatility, grad_volatility, grad_target_log_prob in (\n        zip(step_size_parts,\n            volatility_parts,\n            grads_volatility,\n            grads_target_log_prob)):\n      volatility_squared = tf.square(volatility)\n      drift = 0.5 * step_size * (volatility_squared * grad_target_log_prob\n                                 + grad_volatility)\n      drift_parts.append(drift)\n\n    return drift_parts", "language": "python", "code": "def _get_drift(step_size_parts, volatility_parts, grads_volatility,\n               grads_target_log_prob,\n               name=None):\n  \"\"\"Compute diffusion drift at the current location `current_state`.\n\n  The drift of the diffusion at is computed as\n\n  ```none\n  0.5 * `step_size` * volatility_parts * `target_log_prob_fn(current_state)`\n  + `step_size` * `grads_volatility`\n  ```\n\n  where `volatility_parts` = `volatility_fn(current_state)**2` and\n  `grads_volatility` is a gradient of `volatility_parts` at the `current_state`.\n\n  Args:\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      Euler-Maruyama method. Must broadcast with the shape of\n      `volatility_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    volatility_parts: Python `list` of `Tensor`s representing the value of\n      `volatility_fn(*state_parts)`.\n    grads_volatility: Python list of `Tensor`s representing the value of the\n      gradient of `volatility_parts**2` wrt the state of the chain.\n    grads_target_log_prob: Python list of `Tensor`s representing\n      gradient of `target_log_prob_fn(*state_parts`) wrt `state_parts`. Must\n      have same shape as `volatility_parts`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'mala_get_drift').\n\n  Returns:\n    drift_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, 'mala_get_drift', [\n      step_size_parts, volatility_parts, grads_volatility, grads_target_log_prob\n  ]):\n\n    drift_parts = []\n\n    for step_size, volatility, grad_volatility, grad_target_log_prob in (\n        zip(step_size_parts,\n            volatility_parts,\n            grads_volatility,\n            grads_target_log_prob)):\n      volatility_squared = tf.square(volatility)\n      drift = 0.5 * step_size * (volatility_squared * grad_target_log_prob\n                                 + grad_volatility)\n      drift_parts.append(drift)\n\n    return drift_parts", "code_tokens": ["def", "_get_drift", "(", "step_size_parts", ",", "volatility_parts", ",", "grads_volatility", ",", "grads_target_log_prob", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'mala_get_drift'", ",", "[", "step_size_parts", ",", "volatility_parts", ",", "grads_volatility", ",", "grads_target_log_prob", "]", ")", ":", "drift_parts", "=", "[", "]", "for", "step_size", ",", "volatility", ",", "grad_volatility", ",", "grad_target_log_prob", "in", "(", "zip", "(", "step_size_parts", ",", "volatility_parts", ",", "grads_volatility", ",", "grads_target_log_prob", ")", ")", ":", "volatility_squared", "=", "tf", ".", "square", "(", "volatility", ")", "drift", "=", "0.5", "*", "step_size", "*", "(", "volatility_squared", "*", "grad_target_log_prob", "+", "grad_volatility", ")", "drift_parts", ".", "append", "(", "drift", ")", "return", "drift_parts"], "docstring": "Compute diffusion drift at the current location `current_state`.\n\n  The drift of the diffusion at is computed as\n\n  ```none\n  0.5 * `step_size` * volatility_parts * `target_log_prob_fn(current_state)`\n  + `step_size` * `grads_volatility`\n  ```\n\n  where `volatility_parts` = `volatility_fn(current_state)**2` and\n  `grads_volatility` is a gradient of `volatility_parts` at the `current_state`.\n\n  Args:\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      Euler-Maruyama method. Must broadcast with the shape of\n      `volatility_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    volatility_parts: Python `list` of `Tensor`s representing the value of\n      `volatility_fn(*state_parts)`.\n    grads_volatility: Python list of `Tensor`s representing the value of the\n      gradient of `volatility_parts**2` wrt the state of the chain.\n    grads_target_log_prob: Python list of `Tensor`s representing\n      gradient of `target_log_prob_fn(*state_parts`) wrt `state_parts`. Must\n      have same shape as `volatility_parts`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'mala_get_drift').\n\n  Returns:\n    drift_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.", "docstring_tokens": ["Compute", "diffusion", "drift", "at", "the", "current", "location", "current_state", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/langevin.py#L691-L745", "partition": "test", "index": 828, "time": "2018-05-01 08:49:03"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/langevin.py", "func_name": "_compute_log_acceptance_correction", "original_string": "def _compute_log_acceptance_correction(current_state_parts,\n                                       proposed_state_parts,\n                                       current_volatility_parts,\n                                       proposed_volatility_parts,\n                                       current_drift_parts,\n                                       proposed_drift_parts,\n                                       step_size_parts,\n                                       independent_chain_ndims,\n                                       name=None):\n  r\"\"\"Helper to `kernel` which computes the log acceptance-correction.\n\n  Computes `log_acceptance_correction` as described in `MetropolisHastings`\n  class. The proposal density is normal. More specifically,\n\n   ```none\n  q(proposed_state | current_state) \\sim N(current_state + current_drift,\n  step_size * current_volatility**2)\n\n  q(current_state | proposed_state) \\sim N(proposed_state + proposed_drift,\n  step_size * proposed_volatility**2)\n  ```\n\n  The `log_acceptance_correction` is then\n\n  ```none\n  log_acceptance_correctio = q(current_state | proposed_state)\n  - q(proposed_state | current_state)\n  ```\n\n  Args:\n    current_state_parts: Python `list` of `Tensor`s representing the value(s) of\n      the current state of the chain.\n    proposed_state_parts:  Python `list` of `Tensor`s representing the value(s)\n      of the proposed state of the chain. Must broadcast with the shape of\n      `current_state_parts`.\n    current_volatility_parts: Python `list` of `Tensor`s representing the value\n      of `volatility_fn(*current_volatility_parts)`. Must broadcast with the\n      shape of `current_state_parts`.\n    proposed_volatility_parts: Python `list` of `Tensor`s representing the value\n      of `volatility_fn(*proposed_volatility_parts)`. Must broadcast with the\n      shape of `current_state_parts`\n    current_drift_parts: Python `list` of `Tensor`s representing value of the\n      drift `_get_drift(*current_state_parts, ..)`. Must broadcast with the\n      shape of `current_state_parts`.\n    proposed_drift_parts: Python `list` of `Tensor`s representing value of the\n      drift `_get_drift(*proposed_drift_parts, ..)`. Must broadcast with the\n      shape of `current_state_parts`.\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      Euler-Maruyama method. Must broadcast with the shape of\n      `current_state_parts`.\n    independent_chain_ndims: Scalar `int` `Tensor` representing the number of\n      leftmost `Tensor` dimensions which index independent chains.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'compute_log_acceptance_correction').\n\n  Returns:\n    log_acceptance_correction: `Tensor` representing the `log`\n      acceptance-correction.  (See docstring for mathematical definition.)\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, 'compute_log_acceptance_correction', [\n      current_state_parts, proposed_state_parts, current_volatility_parts,\n      proposed_volatility_parts, current_drift_parts, proposed_drift_parts,\n      step_size_parts, independent_chain_ndims\n  ]):\n\n    proposed_log_density_parts = []\n    dual_log_density_parts = []\n\n    for [\n        current_state,\n        proposed_state,\n        current_volatility,\n        proposed_volatility,\n        current_drift,\n        proposed_drift,\n        step_size,\n    ] in zip(\n        current_state_parts,\n        proposed_state_parts,\n        current_volatility_parts,\n        proposed_volatility_parts,\n        current_drift_parts,\n        proposed_drift_parts,\n        step_size_parts,\n    ):\n      axis = tf.range(independent_chain_ndims, tf.rank(current_state))\n\n      state_diff = proposed_state - current_state\n\n      current_volatility *= tf.sqrt(step_size)\n\n      proposed_energy = (state_diff - current_drift) / current_volatility\n\n      proposed_volatility *= tf.sqrt(step_size)\n      # Compute part of `q(proposed_state | current_state)`\n      proposed_energy = (\n          tf.reduce_sum(\n              input_tensor=mcmc_util.safe_sum(\n                  [tf.math.log(current_volatility),\n                   0.5 * (proposed_energy**2)]),\n              axis=axis))\n      proposed_log_density_parts.append(-proposed_energy)\n\n      # Compute part of `q(current_state | proposed_state)`\n      dual_energy = (state_diff + proposed_drift) / proposed_volatility\n      dual_energy = (\n          tf.reduce_sum(\n              input_tensor=mcmc_util.safe_sum(\n                  [tf.math.log(proposed_volatility), 0.5 * (dual_energy**2)]),\n              axis=axis))\n      dual_log_density_parts.append(-dual_energy)\n\n    # Compute `q(proposed_state | current_state)`\n    proposed_log_density_reduce = tf.reduce_sum(\n        input_tensor=tf.stack(proposed_log_density_parts, axis=-1), axis=-1)\n    # Compute `q(current_state | proposed_state)`\n    dual_log_density_reduce = tf.reduce_sum(\n        input_tensor=tf.stack(dual_log_density_parts, axis=-1), axis=-1)\n\n    return mcmc_util.safe_sum([dual_log_density_reduce,\n                               -proposed_log_density_reduce])", "language": "python", "code": "def _compute_log_acceptance_correction(current_state_parts,\n                                       proposed_state_parts,\n                                       current_volatility_parts,\n                                       proposed_volatility_parts,\n                                       current_drift_parts,\n                                       proposed_drift_parts,\n                                       step_size_parts,\n                                       independent_chain_ndims,\n                                       name=None):\n  r\"\"\"Helper to `kernel` which computes the log acceptance-correction.\n\n  Computes `log_acceptance_correction` as described in `MetropolisHastings`\n  class. The proposal density is normal. More specifically,\n\n   ```none\n  q(proposed_state | current_state) \\sim N(current_state + current_drift,\n  step_size * current_volatility**2)\n\n  q(current_state | proposed_state) \\sim N(proposed_state + proposed_drift,\n  step_size * proposed_volatility**2)\n  ```\n\n  The `log_acceptance_correction` is then\n\n  ```none\n  log_acceptance_correctio = q(current_state | proposed_state)\n  - q(proposed_state | current_state)\n  ```\n\n  Args:\n    current_state_parts: Python `list` of `Tensor`s representing the value(s) of\n      the current state of the chain.\n    proposed_state_parts:  Python `list` of `Tensor`s representing the value(s)\n      of the proposed state of the chain. Must broadcast with the shape of\n      `current_state_parts`.\n    current_volatility_parts: Python `list` of `Tensor`s representing the value\n      of `volatility_fn(*current_volatility_parts)`. Must broadcast with the\n      shape of `current_state_parts`.\n    proposed_volatility_parts: Python `list` of `Tensor`s representing the value\n      of `volatility_fn(*proposed_volatility_parts)`. Must broadcast with the\n      shape of `current_state_parts`\n    current_drift_parts: Python `list` of `Tensor`s representing value of the\n      drift `_get_drift(*current_state_parts, ..)`. Must broadcast with the\n      shape of `current_state_parts`.\n    proposed_drift_parts: Python `list` of `Tensor`s representing value of the\n      drift `_get_drift(*proposed_drift_parts, ..)`. Must broadcast with the\n      shape of `current_state_parts`.\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      Euler-Maruyama method. Must broadcast with the shape of\n      `current_state_parts`.\n    independent_chain_ndims: Scalar `int` `Tensor` representing the number of\n      leftmost `Tensor` dimensions which index independent chains.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'compute_log_acceptance_correction').\n\n  Returns:\n    log_acceptance_correction: `Tensor` representing the `log`\n      acceptance-correction.  (See docstring for mathematical definition.)\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, 'compute_log_acceptance_correction', [\n      current_state_parts, proposed_state_parts, current_volatility_parts,\n      proposed_volatility_parts, current_drift_parts, proposed_drift_parts,\n      step_size_parts, independent_chain_ndims\n  ]):\n\n    proposed_log_density_parts = []\n    dual_log_density_parts = []\n\n    for [\n        current_state,\n        proposed_state,\n        current_volatility,\n        proposed_volatility,\n        current_drift,\n        proposed_drift,\n        step_size,\n    ] in zip(\n        current_state_parts,\n        proposed_state_parts,\n        current_volatility_parts,\n        proposed_volatility_parts,\n        current_drift_parts,\n        proposed_drift_parts,\n        step_size_parts,\n    ):\n      axis = tf.range(independent_chain_ndims, tf.rank(current_state))\n\n      state_diff = proposed_state - current_state\n\n      current_volatility *= tf.sqrt(step_size)\n\n      proposed_energy = (state_diff - current_drift) / current_volatility\n\n      proposed_volatility *= tf.sqrt(step_size)\n      # Compute part of `q(proposed_state | current_state)`\n      proposed_energy = (\n          tf.reduce_sum(\n              input_tensor=mcmc_util.safe_sum(\n                  [tf.math.log(current_volatility),\n                   0.5 * (proposed_energy**2)]),\n              axis=axis))\n      proposed_log_density_parts.append(-proposed_energy)\n\n      # Compute part of `q(current_state | proposed_state)`\n      dual_energy = (state_diff + proposed_drift) / proposed_volatility\n      dual_energy = (\n          tf.reduce_sum(\n              input_tensor=mcmc_util.safe_sum(\n                  [tf.math.log(proposed_volatility), 0.5 * (dual_energy**2)]),\n              axis=axis))\n      dual_log_density_parts.append(-dual_energy)\n\n    # Compute `q(proposed_state | current_state)`\n    proposed_log_density_reduce = tf.reduce_sum(\n        input_tensor=tf.stack(proposed_log_density_parts, axis=-1), axis=-1)\n    # Compute `q(current_state | proposed_state)`\n    dual_log_density_reduce = tf.reduce_sum(\n        input_tensor=tf.stack(dual_log_density_parts, axis=-1), axis=-1)\n\n    return mcmc_util.safe_sum([dual_log_density_reduce,\n                               -proposed_log_density_reduce])", "code_tokens": ["def", "_compute_log_acceptance_correction", "(", "current_state_parts", ",", "proposed_state_parts", ",", "current_volatility_parts", ",", "proposed_volatility_parts", ",", "current_drift_parts", ",", "proposed_drift_parts", ",", "step_size_parts", ",", "independent_chain_ndims", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'compute_log_acceptance_correction'", ",", "[", "current_state_parts", ",", "proposed_state_parts", ",", "current_volatility_parts", ",", "proposed_volatility_parts", ",", "current_drift_parts", ",", "proposed_drift_parts", ",", "step_size_parts", ",", "independent_chain_ndims", "]", ")", ":", "proposed_log_density_parts", "=", "[", "]", "dual_log_density_parts", "=", "[", "]", "for", "[", "current_state", ",", "proposed_state", ",", "current_volatility", ",", "proposed_volatility", ",", "current_drift", ",", "proposed_drift", ",", "step_size", ",", "]", "in", "zip", "(", "current_state_parts", ",", "proposed_state_parts", ",", "current_volatility_parts", ",", "proposed_volatility_parts", ",", "current_drift_parts", ",", "proposed_drift_parts", ",", "step_size_parts", ",", ")", ":", "axis", "=", "tf", ".", "range", "(", "independent_chain_ndims", ",", "tf", ".", "rank", "(", "current_state", ")", ")", "state_diff", "=", "proposed_state", "-", "current_state", "current_volatility", "*=", "tf", ".", "sqrt", "(", "step_size", ")", "proposed_energy", "=", "(", "state_diff", "-", "current_drift", ")", "/", "current_volatility", "proposed_volatility", "*=", "tf", ".", "sqrt", "(", "step_size", ")", "# Compute part of `q(proposed_state | current_state)`", "proposed_energy", "=", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "mcmc_util", ".", "safe_sum", "(", "[", "tf", ".", "math", ".", "log", "(", "current_volatility", ")", ",", "0.5", "*", "(", "proposed_energy", "**", "2", ")", "]", ")", ",", "axis", "=", "axis", ")", ")", "proposed_log_density_parts", ".", "append", "(", "-", "proposed_energy", ")", "# Compute part of `q(current_state | proposed_state)`", "dual_energy", "=", "(", "state_diff", "+", "proposed_drift", ")", "/", "proposed_volatility", "dual_energy", "=", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "mcmc_util", ".", "safe_sum", "(", "[", "tf", ".", "math", ".", "log", "(", "proposed_volatility", ")", ",", "0.5", "*", "(", "dual_energy", "**", "2", ")", "]", ")", ",", "axis", "=", "axis", ")", ")", "dual_log_density_parts", ".", "append", "(", "-", "dual_energy", ")", "# Compute `q(proposed_state | current_state)`", "proposed_log_density_reduce", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "stack", "(", "proposed_log_density_parts", ",", "axis", "=", "-", "1", ")", ",", "axis", "=", "-", "1", ")", "# Compute `q(current_state | proposed_state)`", "dual_log_density_reduce", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "stack", "(", "dual_log_density_parts", ",", "axis", "=", "-", "1", ")", ",", "axis", "=", "-", "1", ")", "return", "mcmc_util", ".", "safe_sum", "(", "[", "dual_log_density_reduce", ",", "-", "proposed_log_density_reduce", "]", ")"], "docstring": "r\"\"\"Helper to `kernel` which computes the log acceptance-correction.\n\n  Computes `log_acceptance_correction` as described in `MetropolisHastings`\n  class. The proposal density is normal. More specifically,\n\n   ```none\n  q(proposed_state | current_state) \\sim N(current_state + current_drift,\n  step_size * current_volatility**2)\n\n  q(current_state | proposed_state) \\sim N(proposed_state + proposed_drift,\n  step_size * proposed_volatility**2)\n  ```\n\n  The `log_acceptance_correction` is then\n\n  ```none\n  log_acceptance_correctio = q(current_state | proposed_state)\n  - q(proposed_state | current_state)\n  ```\n\n  Args:\n    current_state_parts: Python `list` of `Tensor`s representing the value(s) of\n      the current state of the chain.\n    proposed_state_parts:  Python `list` of `Tensor`s representing the value(s)\n      of the proposed state of the chain. Must broadcast with the shape of\n      `current_state_parts`.\n    current_volatility_parts: Python `list` of `Tensor`s representing the value\n      of `volatility_fn(*current_volatility_parts)`. Must broadcast with the\n      shape of `current_state_parts`.\n    proposed_volatility_parts: Python `list` of `Tensor`s representing the value\n      of `volatility_fn(*proposed_volatility_parts)`. Must broadcast with the\n      shape of `current_state_parts`\n    current_drift_parts: Python `list` of `Tensor`s representing value of the\n      drift `_get_drift(*current_state_parts, ..)`. Must broadcast with the\n      shape of `current_state_parts`.\n    proposed_drift_parts: Python `list` of `Tensor`s representing value of the\n      drift `_get_drift(*proposed_drift_parts, ..)`. Must broadcast with the\n      shape of `current_state_parts`.\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      Euler-Maruyama method. Must broadcast with the shape of\n      `current_state_parts`.\n    independent_chain_ndims: Scalar `int` `Tensor` representing the number of\n      leftmost `Tensor` dimensions which index independent chains.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'compute_log_acceptance_correction').\n\n  Returns:\n    log_acceptance_correction: `Tensor` representing the `log`\n      acceptance-correction.  (See docstring for mathematical definition.)", "docstring_tokens": ["r", "Helper", "to", "kernel", "which", "computes", "the", "log", "acceptance", "-", "correction", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/langevin.py#L748-L869", "partition": "test", "index": 829, "time": "2018-05-01 08:49:03"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/langevin.py", "func_name": "_maybe_call_volatility_fn_and_grads", "original_string": "def _maybe_call_volatility_fn_and_grads(volatility_fn,\n                                        state,\n                                        volatility_fn_results=None,\n                                        grads_volatility_fn=None,\n                                        sample_shape=None,\n                                        parallel_iterations=10):\n  \"\"\"Helper which computes `volatility_fn` results and grads, if needed.\"\"\"\n  state_parts = list(state) if mcmc_util.is_list_like(state) else [state]\n  needs_volatility_fn_gradients = grads_volatility_fn is None\n\n  # Convert `volatility_fn_results` to a list\n  if volatility_fn_results is None:\n    volatility_fn_results = volatility_fn(*state_parts)\n\n  volatility_fn_results = (list(volatility_fn_results)\n                           if mcmc_util.is_list_like(volatility_fn_results)\n                           else [volatility_fn_results])\n  if len(volatility_fn_results) == 1:\n    volatility_fn_results *= len(state_parts)\n  if len(state_parts) != len(volatility_fn_results):\n    raise ValueError('`volatility_fn` should return a tensor or a list '\n                     'of the same length as `current_state`.')\n\n  # The shape of 'volatility_parts' needs to have the number of chains as a\n  # leading dimension. For determinism we broadcast 'volatility_parts' to the\n  # shape of `state_parts` since each dimension of `state_parts` could have a\n  # different volatility value.\n\n  volatility_fn_results = _maybe_broadcast_volatility(volatility_fn_results,\n                                                      state_parts)\n  if grads_volatility_fn is None:\n    [\n        _,\n        grads_volatility_fn,\n    ] = diag_jacobian(\n        xs=state_parts,\n        ys=volatility_fn_results,\n        sample_shape=sample_shape,\n        parallel_iterations=parallel_iterations,\n        fn=volatility_fn)\n\n  # Compute gradient of `volatility_parts**2`\n  if needs_volatility_fn_gradients:\n    grads_volatility_fn = [\n        2. * g * volatility if g is not None else tf.zeros_like(\n            fn_arg, dtype=fn_arg.dtype.base_dtype)\n        for g, volatility, fn_arg in zip(\n            grads_volatility_fn, volatility_fn_results, state_parts)\n    ]\n\n  return volatility_fn_results, grads_volatility_fn", "language": "python", "code": "def _maybe_call_volatility_fn_and_grads(volatility_fn,\n                                        state,\n                                        volatility_fn_results=None,\n                                        grads_volatility_fn=None,\n                                        sample_shape=None,\n                                        parallel_iterations=10):\n  \"\"\"Helper which computes `volatility_fn` results and grads, if needed.\"\"\"\n  state_parts = list(state) if mcmc_util.is_list_like(state) else [state]\n  needs_volatility_fn_gradients = grads_volatility_fn is None\n\n  # Convert `volatility_fn_results` to a list\n  if volatility_fn_results is None:\n    volatility_fn_results = volatility_fn(*state_parts)\n\n  volatility_fn_results = (list(volatility_fn_results)\n                           if mcmc_util.is_list_like(volatility_fn_results)\n                           else [volatility_fn_results])\n  if len(volatility_fn_results) == 1:\n    volatility_fn_results *= len(state_parts)\n  if len(state_parts) != len(volatility_fn_results):\n    raise ValueError('`volatility_fn` should return a tensor or a list '\n                     'of the same length as `current_state`.')\n\n  # The shape of 'volatility_parts' needs to have the number of chains as a\n  # leading dimension. For determinism we broadcast 'volatility_parts' to the\n  # shape of `state_parts` since each dimension of `state_parts` could have a\n  # different volatility value.\n\n  volatility_fn_results = _maybe_broadcast_volatility(volatility_fn_results,\n                                                      state_parts)\n  if grads_volatility_fn is None:\n    [\n        _,\n        grads_volatility_fn,\n    ] = diag_jacobian(\n        xs=state_parts,\n        ys=volatility_fn_results,\n        sample_shape=sample_shape,\n        parallel_iterations=parallel_iterations,\n        fn=volatility_fn)\n\n  # Compute gradient of `volatility_parts**2`\n  if needs_volatility_fn_gradients:\n    grads_volatility_fn = [\n        2. * g * volatility if g is not None else tf.zeros_like(\n            fn_arg, dtype=fn_arg.dtype.base_dtype)\n        for g, volatility, fn_arg in zip(\n            grads_volatility_fn, volatility_fn_results, state_parts)\n    ]\n\n  return volatility_fn_results, grads_volatility_fn", "code_tokens": ["def", "_maybe_call_volatility_fn_and_grads", "(", "volatility_fn", ",", "state", ",", "volatility_fn_results", "=", "None", ",", "grads_volatility_fn", "=", "None", ",", "sample_shape", "=", "None", ",", "parallel_iterations", "=", "10", ")", ":", "state_parts", "=", "list", "(", "state", ")", "if", "mcmc_util", ".", "is_list_like", "(", "state", ")", "else", "[", "state", "]", "needs_volatility_fn_gradients", "=", "grads_volatility_fn", "is", "None", "# Convert `volatility_fn_results` to a list", "if", "volatility_fn_results", "is", "None", ":", "volatility_fn_results", "=", "volatility_fn", "(", "*", "state_parts", ")", "volatility_fn_results", "=", "(", "list", "(", "volatility_fn_results", ")", "if", "mcmc_util", ".", "is_list_like", "(", "volatility_fn_results", ")", "else", "[", "volatility_fn_results", "]", ")", "if", "len", "(", "volatility_fn_results", ")", "==", "1", ":", "volatility_fn_results", "*=", "len", "(", "state_parts", ")", "if", "len", "(", "state_parts", ")", "!=", "len", "(", "volatility_fn_results", ")", ":", "raise", "ValueError", "(", "'`volatility_fn` should return a tensor or a list '", "'of the same length as `current_state`.'", ")", "# The shape of 'volatility_parts' needs to have the number of chains as a", "# leading dimension. For determinism we broadcast 'volatility_parts' to the", "# shape of `state_parts` since each dimension of `state_parts` could have a", "# different volatility value.", "volatility_fn_results", "=", "_maybe_broadcast_volatility", "(", "volatility_fn_results", ",", "state_parts", ")", "if", "grads_volatility_fn", "is", "None", ":", "[", "_", ",", "grads_volatility_fn", ",", "]", "=", "diag_jacobian", "(", "xs", "=", "state_parts", ",", "ys", "=", "volatility_fn_results", ",", "sample_shape", "=", "sample_shape", ",", "parallel_iterations", "=", "parallel_iterations", ",", "fn", "=", "volatility_fn", ")", "# Compute gradient of `volatility_parts**2`", "if", "needs_volatility_fn_gradients", ":", "grads_volatility_fn", "=", "[", "2.", "*", "g", "*", "volatility", "if", "g", "is", "not", "None", "else", "tf", ".", "zeros_like", "(", "fn_arg", ",", "dtype", "=", "fn_arg", ".", "dtype", ".", "base_dtype", ")", "for", "g", ",", "volatility", ",", "fn_arg", "in", "zip", "(", "grads_volatility_fn", ",", "volatility_fn_results", ",", "state_parts", ")", "]", "return", "volatility_fn_results", ",", "grads_volatility_fn"], "docstring": "Helper which computes `volatility_fn` results and grads, if needed.", "docstring_tokens": ["Helper", "which", "computes", "volatility_fn", "results", "and", "grads", "if", "needed", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/langevin.py#L872-L922", "partition": "test", "index": 830, "time": "2018-05-01 08:49:03"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/langevin.py", "func_name": "_maybe_broadcast_volatility", "original_string": "def _maybe_broadcast_volatility(volatility_parts,\n                                state_parts):\n  \"\"\"Helper to broadcast `volatility_parts` to the shape of `state_parts`.\"\"\"\n  return [v + tf.zeros_like(sp, dtype=sp.dtype.base_dtype)\n          for v, sp in zip(volatility_parts, state_parts)]", "language": "python", "code": "def _maybe_broadcast_volatility(volatility_parts,\n                                state_parts):\n  \"\"\"Helper to broadcast `volatility_parts` to the shape of `state_parts`.\"\"\"\n  return [v + tf.zeros_like(sp, dtype=sp.dtype.base_dtype)\n          for v, sp in zip(volatility_parts, state_parts)]", "code_tokens": ["def", "_maybe_broadcast_volatility", "(", "volatility_parts", ",", "state_parts", ")", ":", "return", "[", "v", "+", "tf", ".", "zeros_like", "(", "sp", ",", "dtype", "=", "sp", ".", "dtype", ".", "base_dtype", ")", "for", "v", ",", "sp", "in", "zip", "(", "volatility_parts", ",", "state_parts", ")", "]"], "docstring": "Helper to broadcast `volatility_parts` to the shape of `state_parts`.", "docstring_tokens": ["Helper", "to", "broadcast", "volatility_parts", "to", "the", "shape", "of", "state_parts", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/langevin.py#L925-L929", "partition": "test", "index": 831, "time": "2018-05-01 08:49:03"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/langevin.py", "func_name": "_euler_method", "original_string": "def _euler_method(random_draw_parts,\n                  state_parts,\n                  drift_parts,\n                  step_size_parts,\n                  volatility_parts,\n                  name=None):\n  \"\"\"Applies one step of Euler-Maruyama method.\n\n  Generates proposal of the form:\n\n  ```python\n  tfd.Normal(loc=state_parts + _get_drift(state_parts, ...),\n             scale=tf.sqrt(step_size * volatility_fn(current_state)))\n  ```\n\n  `_get_drift(state_parts, ..)` is a diffusion drift value at `state_parts`.\n\n\n  Args:\n    random_draw_parts: Python `list` of `Tensor`s containing the value(s) of the\n      random perturbation variable(s). Must broadcast with the shape of\n      `state_parts`.\n    state_parts: Python `list` of `Tensor`s representing the current\n      state(s) of the Markov chain(s).\n    drift_parts: Python `list` of `Tensor`s representing value of the drift\n      `_get_drift(*state_parts, ..)`. Must broadcast with the shape of\n      `state_parts`.\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      the Euler-Maruyama method. Must broadcast with the shape of\n      `state_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    volatility_parts: Python `list` of `Tensor`s representing the value of\n      `volatility_fn(*state_parts)`. Must broadcast with the shape of\n      `state_parts`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'mala_euler_method').\n\n  Returns:\n    proposed_state_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'mala_euler_method', [\n      random_draw_parts, state_parts, drift_parts, step_size_parts,\n      volatility_parts\n  ]):\n    proposed_state_parts = []\n    for random_draw, state, drift, step_size, volatility in zip(\n        random_draw_parts,\n        state_parts,\n        drift_parts,\n        step_size_parts,\n        volatility_parts):\n      proposal = state + drift + volatility * tf.sqrt(step_size) * random_draw\n      proposed_state_parts.append(proposal)\n\n    return proposed_state_parts", "language": "python", "code": "def _euler_method(random_draw_parts,\n                  state_parts,\n                  drift_parts,\n                  step_size_parts,\n                  volatility_parts,\n                  name=None):\n  \"\"\"Applies one step of Euler-Maruyama method.\n\n  Generates proposal of the form:\n\n  ```python\n  tfd.Normal(loc=state_parts + _get_drift(state_parts, ...),\n             scale=tf.sqrt(step_size * volatility_fn(current_state)))\n  ```\n\n  `_get_drift(state_parts, ..)` is a diffusion drift value at `state_parts`.\n\n\n  Args:\n    random_draw_parts: Python `list` of `Tensor`s containing the value(s) of the\n      random perturbation variable(s). Must broadcast with the shape of\n      `state_parts`.\n    state_parts: Python `list` of `Tensor`s representing the current\n      state(s) of the Markov chain(s).\n    drift_parts: Python `list` of `Tensor`s representing value of the drift\n      `_get_drift(*state_parts, ..)`. Must broadcast with the shape of\n      `state_parts`.\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      the Euler-Maruyama method. Must broadcast with the shape of\n      `state_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    volatility_parts: Python `list` of `Tensor`s representing the value of\n      `volatility_fn(*state_parts)`. Must broadcast with the shape of\n      `state_parts`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'mala_euler_method').\n\n  Returns:\n    proposed_state_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'mala_euler_method', [\n      random_draw_parts, state_parts, drift_parts, step_size_parts,\n      volatility_parts\n  ]):\n    proposed_state_parts = []\n    for random_draw, state, drift, step_size, volatility in zip(\n        random_draw_parts,\n        state_parts,\n        drift_parts,\n        step_size_parts,\n        volatility_parts):\n      proposal = state + drift + volatility * tf.sqrt(step_size) * random_draw\n      proposed_state_parts.append(proposal)\n\n    return proposed_state_parts", "code_tokens": ["def", "_euler_method", "(", "random_draw_parts", ",", "state_parts", ",", "drift_parts", ",", "step_size_parts", ",", "volatility_parts", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'mala_euler_method'", ",", "[", "random_draw_parts", ",", "state_parts", ",", "drift_parts", ",", "step_size_parts", ",", "volatility_parts", "]", ")", ":", "proposed_state_parts", "=", "[", "]", "for", "random_draw", ",", "state", ",", "drift", ",", "step_size", ",", "volatility", "in", "zip", "(", "random_draw_parts", ",", "state_parts", ",", "drift_parts", ",", "step_size_parts", ",", "volatility_parts", ")", ":", "proposal", "=", "state", "+", "drift", "+", "volatility", "*", "tf", ".", "sqrt", "(", "step_size", ")", "*", "random_draw", "proposed_state_parts", ".", "append", "(", "proposal", ")", "return", "proposed_state_parts"], "docstring": "Applies one step of Euler-Maruyama method.\n\n  Generates proposal of the form:\n\n  ```python\n  tfd.Normal(loc=state_parts + _get_drift(state_parts, ...),\n             scale=tf.sqrt(step_size * volatility_fn(current_state)))\n  ```\n\n  `_get_drift(state_parts, ..)` is a diffusion drift value at `state_parts`.\n\n\n  Args:\n    random_draw_parts: Python `list` of `Tensor`s containing the value(s) of the\n      random perturbation variable(s). Must broadcast with the shape of\n      `state_parts`.\n    state_parts: Python `list` of `Tensor`s representing the current\n      state(s) of the Markov chain(s).\n    drift_parts: Python `list` of `Tensor`s representing value of the drift\n      `_get_drift(*state_parts, ..)`. Must broadcast with the shape of\n      `state_parts`.\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      the Euler-Maruyama method. Must broadcast with the shape of\n      `state_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    volatility_parts: Python `list` of `Tensor`s representing the value of\n      `volatility_fn(*state_parts)`. Must broadcast with the shape of\n      `state_parts`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'mala_euler_method').\n\n  Returns:\n    proposed_state_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.", "docstring_tokens": ["Applies", "one", "step", "of", "Euler", "-", "Maruyama", "method", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/langevin.py#L630-L688", "partition": "test", "index": 827, "time": "2018-05-01 08:49:03"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/util.py", "func_name": "_value_and_gradients", "original_string": "def _value_and_gradients(fn, fn_arg_list, result=None, grads=None, name=None):\n  \"\"\"Helper to `maybe_call_fn_and_grads`.\"\"\"\n  with tf.compat.v1.name_scope(name, 'value_and_gradients',\n                               [fn_arg_list, result, grads]):\n\n    def _convert_to_tensor(x, name):\n      ctt = lambda x_: x_ if x_ is None else tf.convert_to_tensor(\n          value=x_, name=name)\n      return [ctt(x_) for x_ in x] if is_list_like(x) else ctt(x)\n\n    fn_arg_list = (list(fn_arg_list) if is_list_like(fn_arg_list)\n                   else [fn_arg_list])\n    fn_arg_list = _convert_to_tensor(fn_arg_list, 'fn_arg')\n\n    if result is None:\n      result = fn(*fn_arg_list)\n      if grads is None and tf.executing_eagerly():\n        # Ensure we disable bijector cacheing in eager mode.\n        # TODO(b/72831017): Remove this once bijector cacheing is fixed for\n        # eager mode.\n        fn_arg_list = [0 + x for x in fn_arg_list]\n\n    result = _convert_to_tensor(result, 'fn_result')\n\n    if grads is not None:\n      grads = _convert_to_tensor(grads, 'fn_grad')\n      return result, grads\n\n    if is_list_like(result) and len(result) == len(fn_arg_list):\n      # Compute the block diagonal of Jacobian.\n      # TODO(b/79158574): Guard this calculation by an arg which explicitly\n      # requests block diagonal Jacobian calculation.\n      def fn_slice(i):\n        \"\"\"Needed to prevent `cell-var-from-loop` pylint warning.\"\"\"\n        return lambda x: fn(*(fn_arg_list[:i] + [x] + fn_arg_list[i+1:]))\n      grads = [\n          tfp_math_value_and_gradients(fn_slice(i), fn_arg_list[i])[1]\n          for i in range(len(result))\n      ]\n    else:\n      _, grads = tfp_math_value_and_gradients(fn, fn_arg_list)\n\n    return result, grads", "language": "python", "code": "def _value_and_gradients(fn, fn_arg_list, result=None, grads=None, name=None):\n  \"\"\"Helper to `maybe_call_fn_and_grads`.\"\"\"\n  with tf.compat.v1.name_scope(name, 'value_and_gradients',\n                               [fn_arg_list, result, grads]):\n\n    def _convert_to_tensor(x, name):\n      ctt = lambda x_: x_ if x_ is None else tf.convert_to_tensor(\n          value=x_, name=name)\n      return [ctt(x_) for x_ in x] if is_list_like(x) else ctt(x)\n\n    fn_arg_list = (list(fn_arg_list) if is_list_like(fn_arg_list)\n                   else [fn_arg_list])\n    fn_arg_list = _convert_to_tensor(fn_arg_list, 'fn_arg')\n\n    if result is None:\n      result = fn(*fn_arg_list)\n      if grads is None and tf.executing_eagerly():\n        # Ensure we disable bijector cacheing in eager mode.\n        # TODO(b/72831017): Remove this once bijector cacheing is fixed for\n        # eager mode.\n        fn_arg_list = [0 + x for x in fn_arg_list]\n\n    result = _convert_to_tensor(result, 'fn_result')\n\n    if grads is not None:\n      grads = _convert_to_tensor(grads, 'fn_grad')\n      return result, grads\n\n    if is_list_like(result) and len(result) == len(fn_arg_list):\n      # Compute the block diagonal of Jacobian.\n      # TODO(b/79158574): Guard this calculation by an arg which explicitly\n      # requests block diagonal Jacobian calculation.\n      def fn_slice(i):\n        \"\"\"Needed to prevent `cell-var-from-loop` pylint warning.\"\"\"\n        return lambda x: fn(*(fn_arg_list[:i] + [x] + fn_arg_list[i+1:]))\n      grads = [\n          tfp_math_value_and_gradients(fn_slice(i), fn_arg_list[i])[1]\n          for i in range(len(result))\n      ]\n    else:\n      _, grads = tfp_math_value_and_gradients(fn, fn_arg_list)\n\n    return result, grads", "code_tokens": ["def", "_value_and_gradients", "(", "fn", ",", "fn_arg_list", ",", "result", "=", "None", ",", "grads", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'value_and_gradients'", ",", "[", "fn_arg_list", ",", "result", ",", "grads", "]", ")", ":", "def", "_convert_to_tensor", "(", "x", ",", "name", ")", ":", "ctt", "=", "lambda", "x_", ":", "x_", "if", "x_", "is", "None", "else", "tf", ".", "convert_to_tensor", "(", "value", "=", "x_", ",", "name", "=", "name", ")", "return", "[", "ctt", "(", "x_", ")", "for", "x_", "in", "x", "]", "if", "is_list_like", "(", "x", ")", "else", "ctt", "(", "x", ")", "fn_arg_list", "=", "(", "list", "(", "fn_arg_list", ")", "if", "is_list_like", "(", "fn_arg_list", ")", "else", "[", "fn_arg_list", "]", ")", "fn_arg_list", "=", "_convert_to_tensor", "(", "fn_arg_list", ",", "'fn_arg'", ")", "if", "result", "is", "None", ":", "result", "=", "fn", "(", "*", "fn_arg_list", ")", "if", "grads", "is", "None", "and", "tf", ".", "executing_eagerly", "(", ")", ":", "# Ensure we disable bijector cacheing in eager mode.", "# TODO(b/72831017): Remove this once bijector cacheing is fixed for", "# eager mode.", "fn_arg_list", "=", "[", "0", "+", "x", "for", "x", "in", "fn_arg_list", "]", "result", "=", "_convert_to_tensor", "(", "result", ",", "'fn_result'", ")", "if", "grads", "is", "not", "None", ":", "grads", "=", "_convert_to_tensor", "(", "grads", ",", "'fn_grad'", ")", "return", "result", ",", "grads", "if", "is_list_like", "(", "result", ")", "and", "len", "(", "result", ")", "==", "len", "(", "fn_arg_list", ")", ":", "# Compute the block diagonal of Jacobian.", "# TODO(b/79158574): Guard this calculation by an arg which explicitly", "# requests block diagonal Jacobian calculation.", "def", "fn_slice", "(", "i", ")", ":", "\"\"\"Needed to prevent `cell-var-from-loop` pylint warning.\"\"\"", "return", "lambda", "x", ":", "fn", "(", "*", "(", "fn_arg_list", "[", ":", "i", "]", "+", "[", "x", "]", "+", "fn_arg_list", "[", "i", "+", "1", ":", "]", ")", ")", "grads", "=", "[", "tfp_math_value_and_gradients", "(", "fn_slice", "(", "i", ")", ",", "fn_arg_list", "[", "i", "]", ")", "[", "1", "]", "for", "i", "in", "range", "(", "len", "(", "result", ")", ")", "]", "else", ":", "_", ",", "grads", "=", "tfp_math_value_and_gradients", "(", "fn", ",", "fn_arg_list", ")", "return", "result", ",", "grads"], "docstring": "Helper to `maybe_call_fn_and_grads`.", "docstring_tokens": ["Helper", "to", "maybe_call_fn_and_grads", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/util.py#L176-L218", "partition": "test", "index": 968, "time": "2018-05-02 16:07:50"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/util.py", "func_name": "maybe_call_fn_and_grads", "original_string": "def maybe_call_fn_and_grads(fn,\n                            fn_arg_list,\n                            result=None,\n                            grads=None,\n                            check_non_none_grads=True,\n                            name=None):\n  \"\"\"Calls `fn` and computes the gradient of the result wrt `args_list`.\"\"\"\n  with tf.compat.v1.name_scope(name, 'maybe_call_fn_and_grads',\n                               [fn_arg_list, result, grads]):\n    fn_arg_list = (list(fn_arg_list) if is_list_like(fn_arg_list)\n                   else [fn_arg_list])\n    result, grads = _value_and_gradients(fn, fn_arg_list, result, grads)\n    if not all(r.dtype.is_floating\n               for r in (result if is_list_like(result) else [result])):  # pylint: disable=superfluous-parens\n      raise TypeError('Function result must be a `Tensor` with `float` '\n                      '`dtype`.')\n    if len(fn_arg_list) != len(grads):\n      raise ValueError('Function args must be in one-to-one correspondence '\n                       'with grads.')\n    if check_non_none_grads and any(g is None for g in grads):\n      raise ValueError('Encountered `None` gradient.\\n'\n                       '  fn_arg_list: {}\\n'\n                       '  grads: {}'.format(fn_arg_list, grads))\n    return result, grads", "language": "python", "code": "def maybe_call_fn_and_grads(fn,\n                            fn_arg_list,\n                            result=None,\n                            grads=None,\n                            check_non_none_grads=True,\n                            name=None):\n  \"\"\"Calls `fn` and computes the gradient of the result wrt `args_list`.\"\"\"\n  with tf.compat.v1.name_scope(name, 'maybe_call_fn_and_grads',\n                               [fn_arg_list, result, grads]):\n    fn_arg_list = (list(fn_arg_list) if is_list_like(fn_arg_list)\n                   else [fn_arg_list])\n    result, grads = _value_and_gradients(fn, fn_arg_list, result, grads)\n    if not all(r.dtype.is_floating\n               for r in (result if is_list_like(result) else [result])):  # pylint: disable=superfluous-parens\n      raise TypeError('Function result must be a `Tensor` with `float` '\n                      '`dtype`.')\n    if len(fn_arg_list) != len(grads):\n      raise ValueError('Function args must be in one-to-one correspondence '\n                       'with grads.')\n    if check_non_none_grads and any(g is None for g in grads):\n      raise ValueError('Encountered `None` gradient.\\n'\n                       '  fn_arg_list: {}\\n'\n                       '  grads: {}'.format(fn_arg_list, grads))\n    return result, grads", "code_tokens": ["def", "maybe_call_fn_and_grads", "(", "fn", ",", "fn_arg_list", ",", "result", "=", "None", ",", "grads", "=", "None", ",", "check_non_none_grads", "=", "True", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'maybe_call_fn_and_grads'", ",", "[", "fn_arg_list", ",", "result", ",", "grads", "]", ")", ":", "fn_arg_list", "=", "(", "list", "(", "fn_arg_list", ")", "if", "is_list_like", "(", "fn_arg_list", ")", "else", "[", "fn_arg_list", "]", ")", "result", ",", "grads", "=", "_value_and_gradients", "(", "fn", ",", "fn_arg_list", ",", "result", ",", "grads", ")", "if", "not", "all", "(", "r", ".", "dtype", ".", "is_floating", "for", "r", "in", "(", "result", "if", "is_list_like", "(", "result", ")", "else", "[", "result", "]", ")", ")", ":", "# pylint: disable=superfluous-parens", "raise", "TypeError", "(", "'Function result must be a `Tensor` with `float` '", "'`dtype`.'", ")", "if", "len", "(", "fn_arg_list", ")", "!=", "len", "(", "grads", ")", ":", "raise", "ValueError", "(", "'Function args must be in one-to-one correspondence '", "'with grads.'", ")", "if", "check_non_none_grads", "and", "any", "(", "g", "is", "None", "for", "g", "in", "grads", ")", ":", "raise", "ValueError", "(", "'Encountered `None` gradient.\\n'", "'  fn_arg_list: {}\\n'", "'  grads: {}'", ".", "format", "(", "fn_arg_list", ",", "grads", ")", ")", "return", "result", ",", "grads"], "docstring": "Calls `fn` and computes the gradient of the result wrt `args_list`.", "docstring_tokens": ["Calls", "fn", "and", "computes", "the", "gradient", "of", "the", "result", "wrt", "args_list", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/util.py#L221-L244", "partition": "test", "index": 969, "time": "2018-05-02 16:07:50"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/glm/fisher_scoring.py", "func_name": "num_cols", "original_string": "def num_cols(x):\n  \"\"\"Returns number of cols in a given `Tensor`.\"\"\"\n  if tf.compat.dimension_value(x.shape[-1]) is not None:\n    return tf.compat.dimension_value(x.shape[-1])\n  return tf.shape(input=x)[-1]", "language": "python", "code": "def num_cols(x):\n  \"\"\"Returns number of cols in a given `Tensor`.\"\"\"\n  if tf.compat.dimension_value(x.shape[-1]) is not None:\n    return tf.compat.dimension_value(x.shape[-1])\n  return tf.shape(input=x)[-1]", "code_tokens": ["def", "num_cols", "(", "x", ")", ":", "if", "tf", ".", "compat", ".", "dimension_value", "(", "x", ".", "shape", "[", "-", "1", "]", ")", "is", "not", "None", ":", "return", "tf", ".", "compat", ".", "dimension_value", "(", "x", ".", "shape", "[", "-", "1", "]", ")", "return", "tf", ".", "shape", "(", "input", "=", "x", ")", "[", "-", "1", "]"], "docstring": "Returns number of cols in a given `Tensor`.", "docstring_tokens": ["Returns", "number", "of", "cols", "in", "a", "given", "Tensor", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/glm/fisher_scoring.py#L635-L639", "partition": "test", "index": 895, "time": "2018-05-07 20:39:11"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/glm/fisher_scoring.py", "func_name": "fit", "original_string": "def fit(\n    model_matrix,\n    response,\n    model,\n    model_coefficients_start=None,\n    predicted_linear_response_start=None,\n    l2_regularizer=None,\n    dispersion=None,\n    offset=None,\n    convergence_criteria_fn=None,\n    learning_rate=None,\n    fast_unsafe_numerics=True,\n    maximum_iterations=None,\n    name=None):\n  \"\"\"Runs multiple Fisher scoring steps.\n\n  Args:\n    model_matrix: (Batch of) `float`-like, matrix-shaped `Tensor` where each row\n      represents a sample's features.\n    response: (Batch of) vector-shaped `Tensor` where each element represents a\n      sample's observed response (to the corresponding row of features). Must\n      have same `dtype` as `model_matrix`.\n    model: `tfp.glm.ExponentialFamily`-like instance which implicitly\n      characterizes a negative log-likelihood loss by specifying the\n      distribuion's `mean`, `gradient_mean`, and `variance`.\n    model_coefficients_start: Optional (batch of) vector-shaped `Tensor`\n      representing the initial model coefficients, one for each column in\n      `model_matrix`. Must have same `dtype` as `model_matrix`.\n      Default value: Zeros.\n    predicted_linear_response_start: Optional `Tensor` with `shape`, `dtype`\n      matching `response`; represents `offset` shifted initial linear\n      predictions based on `model_coefficients_start`.\n      Default value: `offset` if `model_coefficients is None`, and\n      `tf.linalg.matvec(model_matrix, model_coefficients_start) + offset`\n      otherwise.\n    l2_regularizer: Optional scalar `Tensor` representing L2 regularization\n      penalty, i.e.,\n      `loss(w) = sum{-log p(y[i]|x[i],w) : i=1..n} + l2_regularizer ||w||_2^2`.\n      Default value: `None` (i.e., no L2 regularization).\n    dispersion: Optional (batch of) `Tensor` representing `response` dispersion,\n      i.e., as in, `p(y|theta) := exp((y theta - A(theta)) / dispersion)`.\n      Must broadcast with rows of `model_matrix`.\n      Default value: `None` (i.e., \"no dispersion\").\n    offset: Optional `Tensor` representing constant shift applied to\n      `predicted_linear_response`.  Must broadcast to `response`.\n      Default value: `None` (i.e., `tf.zeros_like(response)`).\n    convergence_criteria_fn: Python `callable` taking:\n      `is_converged_previous`, `iter_`, `model_coefficients_previous`,\n      `predicted_linear_response_previous`, `model_coefficients_next`,\n      `predicted_linear_response_next`, `response`, `model`, `dispersion` and\n      returning a `bool` `Tensor` indicating that Fisher scoring has converged.\n      See `convergence_criteria_small_relative_norm_weights_change` as an\n      example function.\n      Default value: `None` (i.e.,\n      `convergence_criteria_small_relative_norm_weights_change`).\n    learning_rate: Optional (batch of) scalar `Tensor` used to dampen iterative\n      progress. Typically only needed if optimization diverges, should be no\n      larger than `1` and typically very close to `1`.\n      Default value: `None` (i.e., `1`).\n    fast_unsafe_numerics: Optional Python `bool` indicating if faster, less\n      numerically accurate methods can be employed for computing the weighted\n      least-squares solution.\n      Default value: `True` (i.e., \"fast but possibly diminished accuracy\").\n    maximum_iterations: Optional maximum number of iterations of Fisher scoring\n      to run; \"and-ed\" with result of `convergence_criteria_fn`.\n      Default value: `None` (i.e., `infinity`).\n    name: Python `str` used as name prefix to ops created by this function.\n      Default value: `\"fit\"`.\n\n  Returns:\n    model_coefficients: (Batch of) vector-shaped `Tensor`; represents the\n      fitted model coefficients, one for each column in `model_matrix`.\n    predicted_linear_response: `response`-shaped `Tensor` representing linear\n      predictions based on new `model_coefficients`, i.e.,\n      `tf.linalg.matvec(model_matrix, model_coefficients) + offset`.\n    is_converged: `bool` `Tensor` indicating that the returned\n      `model_coefficients` met the `convergence_criteria_fn` criteria within the\n      `maximum_iterations` limit.\n    iter_: `int32` `Tensor` indicating the number of iterations taken.\n\n  #### Example\n\n  ```python\n  from __future__ import print_function\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  def make_dataset(n, d, link, scale=1., dtype=np.float32):\n    model_coefficients = tfd.Uniform(\n        low=np.array(-1, dtype),\n        high=np.array(1, dtype)).sample(d, seed=42)\n    radius = np.sqrt(2.)\n    model_coefficients *= radius / tf.linalg.norm(model_coefficients)\n    model_matrix = tfd.Normal(\n        loc=np.array(0, dtype),\n        scale=np.array(1, dtype)).sample([n, d], seed=43)\n    scale = tf.convert_to_tensor(scale, dtype)\n    linear_response = tf.tensordot(\n        model_matrix, model_coefficients, axes=[[1], [0]])\n    if link == 'linear':\n      response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)\n    elif link == 'probit':\n      response = tf.cast(\n          tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,\n          dtype)\n    elif link == 'logit':\n      response = tfd.Bernoulli(logits=linear_response).sample(seed=44)\n    else:\n      raise ValueError('unrecognized true link: {}'.format(link))\n    return model_matrix, response, model_coefficients\n\n  X, Y, w_true = make_dataset(n=int(1e6), d=100, link='probit')\n\n  w, linear_response, is_converged, num_iter = tfp.glm.fit(\n      model_matrix=X,\n      response=Y,\n      model=tfp.glm.BernoulliNormalCDF())\n  log_likelihood = tfp.glm.BernoulliNormalCDF().log_prob(Y, linear_response)\n\n  with tf.Session() as sess:\n    [w_, linear_response_, is_converged_, num_iter_, Y_, w_true_,\n     log_likelihood_] = sess.run([\n        w, linear_response, is_converged, num_iter, Y, w_true,\n        log_likelihood])\n\n  print('is_converged: ', is_converged_)\n  print('    num_iter: ', num_iter_)\n  print('    accuracy: ', np.mean((linear_response_ > 0.) == Y_))\n  print('    deviance: ', 2. * np.mean(log_likelihood_))\n  print('||w0-w1||_2 / (1+||w0||_2): ', (np.linalg.norm(w_true_ - w_, ord=2) /\n                                         (1. + np.linalg.norm(w_true_, ord=2))))\n\n  # ==>\n  # is_converged:  True\n  #     num_iter:  6\n  #     accuracy:  0.804382\n  #     deviance:  -0.820746600628\n  # ||w0-w1||_2 / (1+||w0||_2):  0.00619245105309\n  ```\n\n  \"\"\"\n  graph_deps = [model_matrix, response, model_coefficients_start,\n                predicted_linear_response_start, dispersion, offset,\n                learning_rate, maximum_iterations]\n  with tf.compat.v1.name_scope(name, 'fit', graph_deps):\n    [\n        model_matrix,\n        response,\n        model_coefficients_start,\n        predicted_linear_response_start,\n        offset,\n    ] = prepare_args(\n        model_matrix,\n        response,\n        model_coefficients_start,\n        predicted_linear_response_start,\n        offset)\n    if convergence_criteria_fn is None:\n      convergence_criteria_fn = (\n          convergence_criteria_small_relative_norm_weights_change())\n\n    def _body(\n        is_converged_previous,\n        iter_,\n        model_coefficients_previous,\n        predicted_linear_response_previous):\n      \"\"\"`tf.while_loop` body.\"\"\"\n      model_coefficients_next, predicted_linear_response_next = fit_one_step(\n          model_matrix,\n          response,\n          model,\n          model_coefficients_previous,\n          predicted_linear_response_previous,\n          l2_regularizer,\n          dispersion,\n          offset,\n          learning_rate,\n          fast_unsafe_numerics)\n      is_converged_next = convergence_criteria_fn(\n          is_converged_previous=is_converged_previous,\n          iter_=iter_,\n          model_coefficients_previous=model_coefficients_previous,\n          predicted_linear_response_previous=predicted_linear_response_previous,\n          model_coefficients_next=model_coefficients_next,\n          predicted_linear_response_next=predicted_linear_response_next,\n          response=response,\n          model=model,\n          dispersion=dispersion)\n      return [\n          is_converged_next,\n          iter_ + 1,\n          model_coefficients_next,\n          predicted_linear_response_next,\n      ]\n\n    # while not converged:\n    #   fit_one_step\n    [\n        is_converged,\n        iter_,\n        model_coefficients,\n        predicted_linear_response,\n    ] = tf.while_loop(\n        cond=lambda is_converged, *args: tf.logical_not(is_converged),\n        body=_body,\n        loop_vars=[\n            tf.zeros([], np.bool),   # is_converged\n            tf.zeros([], np.int32),  # iter_\n            model_coefficients_start,\n            predicted_linear_response_start,\n        ],\n        maximum_iterations=maximum_iterations)\n\n    return [\n        model_coefficients,\n        predicted_linear_response,\n        is_converged,\n        iter_\n    ]", "language": "python", "code": "def fit(\n    model_matrix,\n    response,\n    model,\n    model_coefficients_start=None,\n    predicted_linear_response_start=None,\n    l2_regularizer=None,\n    dispersion=None,\n    offset=None,\n    convergence_criteria_fn=None,\n    learning_rate=None,\n    fast_unsafe_numerics=True,\n    maximum_iterations=None,\n    name=None):\n  \"\"\"Runs multiple Fisher scoring steps.\n\n  Args:\n    model_matrix: (Batch of) `float`-like, matrix-shaped `Tensor` where each row\n      represents a sample's features.\n    response: (Batch of) vector-shaped `Tensor` where each element represents a\n      sample's observed response (to the corresponding row of features). Must\n      have same `dtype` as `model_matrix`.\n    model: `tfp.glm.ExponentialFamily`-like instance which implicitly\n      characterizes a negative log-likelihood loss by specifying the\n      distribuion's `mean`, `gradient_mean`, and `variance`.\n    model_coefficients_start: Optional (batch of) vector-shaped `Tensor`\n      representing the initial model coefficients, one for each column in\n      `model_matrix`. Must have same `dtype` as `model_matrix`.\n      Default value: Zeros.\n    predicted_linear_response_start: Optional `Tensor` with `shape`, `dtype`\n      matching `response`; represents `offset` shifted initial linear\n      predictions based on `model_coefficients_start`.\n      Default value: `offset` if `model_coefficients is None`, and\n      `tf.linalg.matvec(model_matrix, model_coefficients_start) + offset`\n      otherwise.\n    l2_regularizer: Optional scalar `Tensor` representing L2 regularization\n      penalty, i.e.,\n      `loss(w) = sum{-log p(y[i]|x[i],w) : i=1..n} + l2_regularizer ||w||_2^2`.\n      Default value: `None` (i.e., no L2 regularization).\n    dispersion: Optional (batch of) `Tensor` representing `response` dispersion,\n      i.e., as in, `p(y|theta) := exp((y theta - A(theta)) / dispersion)`.\n      Must broadcast with rows of `model_matrix`.\n      Default value: `None` (i.e., \"no dispersion\").\n    offset: Optional `Tensor` representing constant shift applied to\n      `predicted_linear_response`.  Must broadcast to `response`.\n      Default value: `None` (i.e., `tf.zeros_like(response)`).\n    convergence_criteria_fn: Python `callable` taking:\n      `is_converged_previous`, `iter_`, `model_coefficients_previous`,\n      `predicted_linear_response_previous`, `model_coefficients_next`,\n      `predicted_linear_response_next`, `response`, `model`, `dispersion` and\n      returning a `bool` `Tensor` indicating that Fisher scoring has converged.\n      See `convergence_criteria_small_relative_norm_weights_change` as an\n      example function.\n      Default value: `None` (i.e.,\n      `convergence_criteria_small_relative_norm_weights_change`).\n    learning_rate: Optional (batch of) scalar `Tensor` used to dampen iterative\n      progress. Typically only needed if optimization diverges, should be no\n      larger than `1` and typically very close to `1`.\n      Default value: `None` (i.e., `1`).\n    fast_unsafe_numerics: Optional Python `bool` indicating if faster, less\n      numerically accurate methods can be employed for computing the weighted\n      least-squares solution.\n      Default value: `True` (i.e., \"fast but possibly diminished accuracy\").\n    maximum_iterations: Optional maximum number of iterations of Fisher scoring\n      to run; \"and-ed\" with result of `convergence_criteria_fn`.\n      Default value: `None` (i.e., `infinity`).\n    name: Python `str` used as name prefix to ops created by this function.\n      Default value: `\"fit\"`.\n\n  Returns:\n    model_coefficients: (Batch of) vector-shaped `Tensor`; represents the\n      fitted model coefficients, one for each column in `model_matrix`.\n    predicted_linear_response: `response`-shaped `Tensor` representing linear\n      predictions based on new `model_coefficients`, i.e.,\n      `tf.linalg.matvec(model_matrix, model_coefficients) + offset`.\n    is_converged: `bool` `Tensor` indicating that the returned\n      `model_coefficients` met the `convergence_criteria_fn` criteria within the\n      `maximum_iterations` limit.\n    iter_: `int32` `Tensor` indicating the number of iterations taken.\n\n  #### Example\n\n  ```python\n  from __future__ import print_function\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  def make_dataset(n, d, link, scale=1., dtype=np.float32):\n    model_coefficients = tfd.Uniform(\n        low=np.array(-1, dtype),\n        high=np.array(1, dtype)).sample(d, seed=42)\n    radius = np.sqrt(2.)\n    model_coefficients *= radius / tf.linalg.norm(model_coefficients)\n    model_matrix = tfd.Normal(\n        loc=np.array(0, dtype),\n        scale=np.array(1, dtype)).sample([n, d], seed=43)\n    scale = tf.convert_to_tensor(scale, dtype)\n    linear_response = tf.tensordot(\n        model_matrix, model_coefficients, axes=[[1], [0]])\n    if link == 'linear':\n      response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)\n    elif link == 'probit':\n      response = tf.cast(\n          tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,\n          dtype)\n    elif link == 'logit':\n      response = tfd.Bernoulli(logits=linear_response).sample(seed=44)\n    else:\n      raise ValueError('unrecognized true link: {}'.format(link))\n    return model_matrix, response, model_coefficients\n\n  X, Y, w_true = make_dataset(n=int(1e6), d=100, link='probit')\n\n  w, linear_response, is_converged, num_iter = tfp.glm.fit(\n      model_matrix=X,\n      response=Y,\n      model=tfp.glm.BernoulliNormalCDF())\n  log_likelihood = tfp.glm.BernoulliNormalCDF().log_prob(Y, linear_response)\n\n  with tf.Session() as sess:\n    [w_, linear_response_, is_converged_, num_iter_, Y_, w_true_,\n     log_likelihood_] = sess.run([\n        w, linear_response, is_converged, num_iter, Y, w_true,\n        log_likelihood])\n\n  print('is_converged: ', is_converged_)\n  print('    num_iter: ', num_iter_)\n  print('    accuracy: ', np.mean((linear_response_ > 0.) == Y_))\n  print('    deviance: ', 2. * np.mean(log_likelihood_))\n  print('||w0-w1||_2 / (1+||w0||_2): ', (np.linalg.norm(w_true_ - w_, ord=2) /\n                                         (1. + np.linalg.norm(w_true_, ord=2))))\n\n  # ==>\n  # is_converged:  True\n  #     num_iter:  6\n  #     accuracy:  0.804382\n  #     deviance:  -0.820746600628\n  # ||w0-w1||_2 / (1+||w0||_2):  0.00619245105309\n  ```\n\n  \"\"\"\n  graph_deps = [model_matrix, response, model_coefficients_start,\n                predicted_linear_response_start, dispersion, offset,\n                learning_rate, maximum_iterations]\n  with tf.compat.v1.name_scope(name, 'fit', graph_deps):\n    [\n        model_matrix,\n        response,\n        model_coefficients_start,\n        predicted_linear_response_start,\n        offset,\n    ] = prepare_args(\n        model_matrix,\n        response,\n        model_coefficients_start,\n        predicted_linear_response_start,\n        offset)\n    if convergence_criteria_fn is None:\n      convergence_criteria_fn = (\n          convergence_criteria_small_relative_norm_weights_change())\n\n    def _body(\n        is_converged_previous,\n        iter_,\n        model_coefficients_previous,\n        predicted_linear_response_previous):\n      \"\"\"`tf.while_loop` body.\"\"\"\n      model_coefficients_next, predicted_linear_response_next = fit_one_step(\n          model_matrix,\n          response,\n          model,\n          model_coefficients_previous,\n          predicted_linear_response_previous,\n          l2_regularizer,\n          dispersion,\n          offset,\n          learning_rate,\n          fast_unsafe_numerics)\n      is_converged_next = convergence_criteria_fn(\n          is_converged_previous=is_converged_previous,\n          iter_=iter_,\n          model_coefficients_previous=model_coefficients_previous,\n          predicted_linear_response_previous=predicted_linear_response_previous,\n          model_coefficients_next=model_coefficients_next,\n          predicted_linear_response_next=predicted_linear_response_next,\n          response=response,\n          model=model,\n          dispersion=dispersion)\n      return [\n          is_converged_next,\n          iter_ + 1,\n          model_coefficients_next,\n          predicted_linear_response_next,\n      ]\n\n    # while not converged:\n    #   fit_one_step\n    [\n        is_converged,\n        iter_,\n        model_coefficients,\n        predicted_linear_response,\n    ] = tf.while_loop(\n        cond=lambda is_converged, *args: tf.logical_not(is_converged),\n        body=_body,\n        loop_vars=[\n            tf.zeros([], np.bool),   # is_converged\n            tf.zeros([], np.int32),  # iter_\n            model_coefficients_start,\n            predicted_linear_response_start,\n        ],\n        maximum_iterations=maximum_iterations)\n\n    return [\n        model_coefficients,\n        predicted_linear_response,\n        is_converged,\n        iter_\n    ]", "code_tokens": ["def", "fit", "(", "model_matrix", ",", "response", ",", "model", ",", "model_coefficients_start", "=", "None", ",", "predicted_linear_response_start", "=", "None", ",", "l2_regularizer", "=", "None", ",", "dispersion", "=", "None", ",", "offset", "=", "None", ",", "convergence_criteria_fn", "=", "None", ",", "learning_rate", "=", "None", ",", "fast_unsafe_numerics", "=", "True", ",", "maximum_iterations", "=", "None", ",", "name", "=", "None", ")", ":", "graph_deps", "=", "[", "model_matrix", ",", "response", ",", "model_coefficients_start", ",", "predicted_linear_response_start", ",", "dispersion", ",", "offset", ",", "learning_rate", ",", "maximum_iterations", "]", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'fit'", ",", "graph_deps", ")", ":", "[", "model_matrix", ",", "response", ",", "model_coefficients_start", ",", "predicted_linear_response_start", ",", "offset", ",", "]", "=", "prepare_args", "(", "model_matrix", ",", "response", ",", "model_coefficients_start", ",", "predicted_linear_response_start", ",", "offset", ")", "if", "convergence_criteria_fn", "is", "None", ":", "convergence_criteria_fn", "=", "(", "convergence_criteria_small_relative_norm_weights_change", "(", ")", ")", "def", "_body", "(", "is_converged_previous", ",", "iter_", ",", "model_coefficients_previous", ",", "predicted_linear_response_previous", ")", ":", "\"\"\"`tf.while_loop` body.\"\"\"", "model_coefficients_next", ",", "predicted_linear_response_next", "=", "fit_one_step", "(", "model_matrix", ",", "response", ",", "model", ",", "model_coefficients_previous", ",", "predicted_linear_response_previous", ",", "l2_regularizer", ",", "dispersion", ",", "offset", ",", "learning_rate", ",", "fast_unsafe_numerics", ")", "is_converged_next", "=", "convergence_criteria_fn", "(", "is_converged_previous", "=", "is_converged_previous", ",", "iter_", "=", "iter_", ",", "model_coefficients_previous", "=", "model_coefficients_previous", ",", "predicted_linear_response_previous", "=", "predicted_linear_response_previous", ",", "model_coefficients_next", "=", "model_coefficients_next", ",", "predicted_linear_response_next", "=", "predicted_linear_response_next", ",", "response", "=", "response", ",", "model", "=", "model", ",", "dispersion", "=", "dispersion", ")", "return", "[", "is_converged_next", ",", "iter_", "+", "1", ",", "model_coefficients_next", ",", "predicted_linear_response_next", ",", "]", "# while not converged:", "#   fit_one_step", "[", "is_converged", ",", "iter_", ",", "model_coefficients", ",", "predicted_linear_response", ",", "]", "=", "tf", ".", "while_loop", "(", "cond", "=", "lambda", "is_converged", ",", "*", "args", ":", "tf", ".", "logical_not", "(", "is_converged", ")", ",", "body", "=", "_body", ",", "loop_vars", "=", "[", "tf", ".", "zeros", "(", "[", "]", ",", "np", ".", "bool", ")", ",", "# is_converged", "tf", ".", "zeros", "(", "[", "]", ",", "np", ".", "int32", ")", ",", "# iter_", "model_coefficients_start", ",", "predicted_linear_response_start", ",", "]", ",", "maximum_iterations", "=", "maximum_iterations", ")", "return", "[", "model_coefficients", ",", "predicted_linear_response", ",", "is_converged", ",", "iter_", "]"], "docstring": "Runs multiple Fisher scoring steps.\n\n  Args:\n    model_matrix: (Batch of) `float`-like, matrix-shaped `Tensor` where each row\n      represents a sample's features.\n    response: (Batch of) vector-shaped `Tensor` where each element represents a\n      sample's observed response (to the corresponding row of features). Must\n      have same `dtype` as `model_matrix`.\n    model: `tfp.glm.ExponentialFamily`-like instance which implicitly\n      characterizes a negative log-likelihood loss by specifying the\n      distribuion's `mean`, `gradient_mean`, and `variance`.\n    model_coefficients_start: Optional (batch of) vector-shaped `Tensor`\n      representing the initial model coefficients, one for each column in\n      `model_matrix`. Must have same `dtype` as `model_matrix`.\n      Default value: Zeros.\n    predicted_linear_response_start: Optional `Tensor` with `shape`, `dtype`\n      matching `response`; represents `offset` shifted initial linear\n      predictions based on `model_coefficients_start`.\n      Default value: `offset` if `model_coefficients is None`, and\n      `tf.linalg.matvec(model_matrix, model_coefficients_start) + offset`\n      otherwise.\n    l2_regularizer: Optional scalar `Tensor` representing L2 regularization\n      penalty, i.e.,\n      `loss(w) = sum{-log p(y[i]|x[i],w) : i=1..n} + l2_regularizer ||w||_2^2`.\n      Default value: `None` (i.e., no L2 regularization).\n    dispersion: Optional (batch of) `Tensor` representing `response` dispersion,\n      i.e., as in, `p(y|theta) := exp((y theta - A(theta)) / dispersion)`.\n      Must broadcast with rows of `model_matrix`.\n      Default value: `None` (i.e., \"no dispersion\").\n    offset: Optional `Tensor` representing constant shift applied to\n      `predicted_linear_response`.  Must broadcast to `response`.\n      Default value: `None` (i.e., `tf.zeros_like(response)`).\n    convergence_criteria_fn: Python `callable` taking:\n      `is_converged_previous`, `iter_`, `model_coefficients_previous`,\n      `predicted_linear_response_previous`, `model_coefficients_next`,\n      `predicted_linear_response_next`, `response`, `model`, `dispersion` and\n      returning a `bool` `Tensor` indicating that Fisher scoring has converged.\n      See `convergence_criteria_small_relative_norm_weights_change` as an\n      example function.\n      Default value: `None` (i.e.,\n      `convergence_criteria_small_relative_norm_weights_change`).\n    learning_rate: Optional (batch of) scalar `Tensor` used to dampen iterative\n      progress. Typically only needed if optimization diverges, should be no\n      larger than `1` and typically very close to `1`.\n      Default value: `None` (i.e., `1`).\n    fast_unsafe_numerics: Optional Python `bool` indicating if faster, less\n      numerically accurate methods can be employed for computing the weighted\n      least-squares solution.\n      Default value: `True` (i.e., \"fast but possibly diminished accuracy\").\n    maximum_iterations: Optional maximum number of iterations of Fisher scoring\n      to run; \"and-ed\" with result of `convergence_criteria_fn`.\n      Default value: `None` (i.e., `infinity`).\n    name: Python `str` used as name prefix to ops created by this function.\n      Default value: `\"fit\"`.\n\n  Returns:\n    model_coefficients: (Batch of) vector-shaped `Tensor`; represents the\n      fitted model coefficients, one for each column in `model_matrix`.\n    predicted_linear_response: `response`-shaped `Tensor` representing linear\n      predictions based on new `model_coefficients`, i.e.,\n      `tf.linalg.matvec(model_matrix, model_coefficients) + offset`.\n    is_converged: `bool` `Tensor` indicating that the returned\n      `model_coefficients` met the `convergence_criteria_fn` criteria within the\n      `maximum_iterations` limit.\n    iter_: `int32` `Tensor` indicating the number of iterations taken.\n\n  #### Example\n\n  ```python\n  from __future__ import print_function\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  def make_dataset(n, d, link, scale=1., dtype=np.float32):\n    model_coefficients = tfd.Uniform(\n        low=np.array(-1, dtype),\n        high=np.array(1, dtype)).sample(d, seed=42)\n    radius = np.sqrt(2.)\n    model_coefficients *= radius / tf.linalg.norm(model_coefficients)\n    model_matrix = tfd.Normal(\n        loc=np.array(0, dtype),\n        scale=np.array(1, dtype)).sample([n, d], seed=43)\n    scale = tf.convert_to_tensor(scale, dtype)\n    linear_response = tf.tensordot(\n        model_matrix, model_coefficients, axes=[[1], [0]])\n    if link == 'linear':\n      response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)\n    elif link == 'probit':\n      response = tf.cast(\n          tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,\n          dtype)\n    elif link == 'logit':\n      response = tfd.Bernoulli(logits=linear_response).sample(seed=44)\n    else:\n      raise ValueError('unrecognized true link: {}'.format(link))\n    return model_matrix, response, model_coefficients\n\n  X, Y, w_true = make_dataset(n=int(1e6), d=100, link='probit')\n\n  w, linear_response, is_converged, num_iter = tfp.glm.fit(\n      model_matrix=X,\n      response=Y,\n      model=tfp.glm.BernoulliNormalCDF())\n  log_likelihood = tfp.glm.BernoulliNormalCDF().log_prob(Y, linear_response)\n\n  with tf.Session() as sess:\n    [w_, linear_response_, is_converged_, num_iter_, Y_, w_true_,\n     log_likelihood_] = sess.run([\n        w, linear_response, is_converged, num_iter, Y, w_true,\n        log_likelihood])\n\n  print('is_converged: ', is_converged_)\n  print('    num_iter: ', num_iter_)\n  print('    accuracy: ', np.mean((linear_response_ > 0.) == Y_))\n  print('    deviance: ', 2. * np.mean(log_likelihood_))\n  print('||w0-w1||_2 / (1+||w0||_2): ', (np.linalg.norm(w_true_ - w_, ord=2) /\n                                         (1. + np.linalg.norm(w_true_, ord=2))))\n\n  # ==>\n  # is_converged:  True\n  #     num_iter:  6\n  #     accuracy:  0.804382\n  #     deviance:  -0.820746600628\n  # ||w0-w1||_2 / (1+||w0||_2):  0.00619245105309\n  ```", "docstring_tokens": ["Runs", "multiple", "Fisher", "scoring", "steps", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/glm/fisher_scoring.py#L36-L256", "partition": "test", "index": 892, "time": "2018-05-07 20:39:11"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/glm/fisher_scoring.py", "func_name": "convergence_criteria_small_relative_norm_weights_change", "original_string": "def convergence_criteria_small_relative_norm_weights_change(\n    tolerance=1e-5,\n    norm_order=2):\n  \"\"\"Returns Python `callable` which indicates fitting procedure has converged.\n\n  Writing old, new `model_coefficients` as `w0`, `w1`, this function\n  defines convergence as,\n\n  ```python\n  relative_euclidean_norm = (tf.norm(w0 - w1, ord=2, axis=-1) /\n                             (1. + tf.norm(w0, ord=2, axis=-1)))\n  reduce_all(relative_euclidean_norm < tolerance)\n  ```\n\n  where `tf.norm(x, ord=2)` denotes the [Euclidean norm](\n  https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm) of `x`.\n\n  Args:\n    tolerance: `float`-like `Tensor` indicating convergence, i.e., when\n      max relative Euclidean norm weights difference < tolerance`.\n      Default value: `1e-5`.\n    norm_order: Order of the norm. Default value: `2` (i.e., \"Euclidean norm\".)\n\n  Returns:\n    convergence_criteria_fn: Python `callable` which returns `bool` `Tensor`\n      indicated fitting procedure has converged. (See inner function\n      specification for argument signature.)\n      Default value: `1e-5`.\n  \"\"\"\n  def convergence_criteria_fn(\n      is_converged_previous,  # pylint: disable=unused-argument\n      iter_,\n      model_coefficients_previous,\n      predicted_linear_response_previous,  # pylint: disable=unused-argument\n      model_coefficients_next,\n      predicted_linear_response_next,  # pylint: disable=unused-argument\n      response,  # pylint: disable=unused-argument\n      model,  # pylint: disable=unused-argument\n      dispersion):  # pylint: disable=unused-argument\n    \"\"\"Returns `bool` `Tensor` indicating if fitting procedure has converged.\n\n    Args:\n      is_converged_previous: \"old\" convergence results.\n      iter_: Iteration number.\n      model_coefficients_previous: \"old\" `model_coefficients`.\n      predicted_linear_response_previous: \"old\" `predicted_linear_response`.\n      model_coefficients_next: \"new\" `model_coefficients`.\n      predicted_linear_response_next: \"new: `predicted_linear_response`.\n      response: (Batch of) vector-shaped `Tensor` where each element represents\n        a sample's observed response (to the corresponding row of features).\n        Must have same `dtype` as `model_matrix`.\n      model: `tfp.glm.ExponentialFamily`-like instance used to construct the\n        negative log-likelihood loss, gradient, and expected Hessian (i.e., the\n        Fisher information matrix).\n      dispersion: `Tensor` representing `response` dispersion, i.e., as in:\n        `p(y|theta) := exp((y theta - A(theta)) / dispersion)`. Must broadcast\n        with rows of `model_matrix`.\n        Default value: `None` (i.e., \"no dispersion\").\n\n    Returns:\n      is_converged: `bool` `Tensor`.\n    \"\"\"\n    relative_euclidean_norm = (\n        tf.norm(\n            tensor=model_coefficients_previous - model_coefficients_next,\n            ord=norm_order,\n            axis=-1) /\n        (1. +\n         tf.norm(tensor=model_coefficients_previous, ord=norm_order, axis=-1)))\n    return (iter_ > 0) & tf.reduce_all(\n        input_tensor=relative_euclidean_norm < tolerance)\n\n  return convergence_criteria_fn", "language": "python", "code": "def convergence_criteria_small_relative_norm_weights_change(\n    tolerance=1e-5,\n    norm_order=2):\n  \"\"\"Returns Python `callable` which indicates fitting procedure has converged.\n\n  Writing old, new `model_coefficients` as `w0`, `w1`, this function\n  defines convergence as,\n\n  ```python\n  relative_euclidean_norm = (tf.norm(w0 - w1, ord=2, axis=-1) /\n                             (1. + tf.norm(w0, ord=2, axis=-1)))\n  reduce_all(relative_euclidean_norm < tolerance)\n  ```\n\n  where `tf.norm(x, ord=2)` denotes the [Euclidean norm](\n  https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm) of `x`.\n\n  Args:\n    tolerance: `float`-like `Tensor` indicating convergence, i.e., when\n      max relative Euclidean norm weights difference < tolerance`.\n      Default value: `1e-5`.\n    norm_order: Order of the norm. Default value: `2` (i.e., \"Euclidean norm\".)\n\n  Returns:\n    convergence_criteria_fn: Python `callable` which returns `bool` `Tensor`\n      indicated fitting procedure has converged. (See inner function\n      specification for argument signature.)\n      Default value: `1e-5`.\n  \"\"\"\n  def convergence_criteria_fn(\n      is_converged_previous,  # pylint: disable=unused-argument\n      iter_,\n      model_coefficients_previous,\n      predicted_linear_response_previous,  # pylint: disable=unused-argument\n      model_coefficients_next,\n      predicted_linear_response_next,  # pylint: disable=unused-argument\n      response,  # pylint: disable=unused-argument\n      model,  # pylint: disable=unused-argument\n      dispersion):  # pylint: disable=unused-argument\n    \"\"\"Returns `bool` `Tensor` indicating if fitting procedure has converged.\n\n    Args:\n      is_converged_previous: \"old\" convergence results.\n      iter_: Iteration number.\n      model_coefficients_previous: \"old\" `model_coefficients`.\n      predicted_linear_response_previous: \"old\" `predicted_linear_response`.\n      model_coefficients_next: \"new\" `model_coefficients`.\n      predicted_linear_response_next: \"new: `predicted_linear_response`.\n      response: (Batch of) vector-shaped `Tensor` where each element represents\n        a sample's observed response (to the corresponding row of features).\n        Must have same `dtype` as `model_matrix`.\n      model: `tfp.glm.ExponentialFamily`-like instance used to construct the\n        negative log-likelihood loss, gradient, and expected Hessian (i.e., the\n        Fisher information matrix).\n      dispersion: `Tensor` representing `response` dispersion, i.e., as in:\n        `p(y|theta) := exp((y theta - A(theta)) / dispersion)`. Must broadcast\n        with rows of `model_matrix`.\n        Default value: `None` (i.e., \"no dispersion\").\n\n    Returns:\n      is_converged: `bool` `Tensor`.\n    \"\"\"\n    relative_euclidean_norm = (\n        tf.norm(\n            tensor=model_coefficients_previous - model_coefficients_next,\n            ord=norm_order,\n            axis=-1) /\n        (1. +\n         tf.norm(tensor=model_coefficients_previous, ord=norm_order, axis=-1)))\n    return (iter_ > 0) & tf.reduce_all(\n        input_tensor=relative_euclidean_norm < tolerance)\n\n  return convergence_criteria_fn", "code_tokens": ["def", "convergence_criteria_small_relative_norm_weights_change", "(", "tolerance", "=", "1e-5", ",", "norm_order", "=", "2", ")", ":", "def", "convergence_criteria_fn", "(", "is_converged_previous", ",", "# pylint: disable=unused-argument", "iter_", ",", "model_coefficients_previous", ",", "predicted_linear_response_previous", ",", "# pylint: disable=unused-argument", "model_coefficients_next", ",", "predicted_linear_response_next", ",", "# pylint: disable=unused-argument", "response", ",", "# pylint: disable=unused-argument", "model", ",", "# pylint: disable=unused-argument", "dispersion", ")", ":", "# pylint: disable=unused-argument", "\"\"\"Returns `bool` `Tensor` indicating if fitting procedure has converged.\n\n    Args:\n      is_converged_previous: \"old\" convergence results.\n      iter_: Iteration number.\n      model_coefficients_previous: \"old\" `model_coefficients`.\n      predicted_linear_response_previous: \"old\" `predicted_linear_response`.\n      model_coefficients_next: \"new\" `model_coefficients`.\n      predicted_linear_response_next: \"new: `predicted_linear_response`.\n      response: (Batch of) vector-shaped `Tensor` where each element represents\n        a sample's observed response (to the corresponding row of features).\n        Must have same `dtype` as `model_matrix`.\n      model: `tfp.glm.ExponentialFamily`-like instance used to construct the\n        negative log-likelihood loss, gradient, and expected Hessian (i.e., the\n        Fisher information matrix).\n      dispersion: `Tensor` representing `response` dispersion, i.e., as in:\n        `p(y|theta) := exp((y theta - A(theta)) / dispersion)`. Must broadcast\n        with rows of `model_matrix`.\n        Default value: `None` (i.e., \"no dispersion\").\n\n    Returns:\n      is_converged: `bool` `Tensor`.\n    \"\"\"", "relative_euclidean_norm", "=", "(", "tf", ".", "norm", "(", "tensor", "=", "model_coefficients_previous", "-", "model_coefficients_next", ",", "ord", "=", "norm_order", ",", "axis", "=", "-", "1", ")", "/", "(", "1.", "+", "tf", ".", "norm", "(", "tensor", "=", "model_coefficients_previous", ",", "ord", "=", "norm_order", ",", "axis", "=", "-", "1", ")", ")", ")", "return", "(", "iter_", ">", "0", ")", "&", "tf", ".", "reduce_all", "(", "input_tensor", "=", "relative_euclidean_norm", "<", "tolerance", ")", "return", "convergence_criteria_fn"], "docstring": "Returns Python `callable` which indicates fitting procedure has converged.\n\n  Writing old, new `model_coefficients` as `w0`, `w1`, this function\n  defines convergence as,\n\n  ```python\n  relative_euclidean_norm = (tf.norm(w0 - w1, ord=2, axis=-1) /\n                             (1. + tf.norm(w0, ord=2, axis=-1)))\n  reduce_all(relative_euclidean_norm < tolerance)\n  ```\n\n  where `tf.norm(x, ord=2)` denotes the [Euclidean norm](\n  https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm) of `x`.\n\n  Args:\n    tolerance: `float`-like `Tensor` indicating convergence, i.e., when\n      max relative Euclidean norm weights difference < tolerance`.\n      Default value: `1e-5`.\n    norm_order: Order of the norm. Default value: `2` (i.e., \"Euclidean norm\".)\n\n  Returns:\n    convergence_criteria_fn: Python `callable` which returns `bool` `Tensor`\n      indicated fitting procedure has converged. (See inner function\n      specification for argument signature.)\n      Default value: `1e-5`.", "docstring_tokens": ["Returns", "Python", "callable", "which", "indicates", "fitting", "procedure", "has", "converged", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/glm/fisher_scoring.py#L442-L514", "partition": "test", "index": 893, "time": "2018-05-07 20:39:11"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/glm/fisher_scoring.py", "func_name": "prepare_args", "original_string": "def prepare_args(model_matrix,\n                 response,\n                 model_coefficients,\n                 predicted_linear_response,\n                 offset,\n                 name=None):\n  \"\"\"Helper to `fit` which sanitizes input args.\n\n  Args:\n    model_matrix: (Batch of) `float`-like, matrix-shaped `Tensor` where each row\n      represents a sample's features.\n    response: (Batch of) vector-shaped `Tensor` where each element represents a\n      sample's observed response (to the corresponding row of features). Must\n      have same `dtype` as `model_matrix`.\n    model_coefficients: Optional (batch of) vector-shaped `Tensor` representing\n      the model coefficients, one for each column in `model_matrix`. Must have\n      same `dtype` as `model_matrix`.\n      Default value: `tf.zeros(tf.shape(model_matrix)[-1], model_matrix.dtype)`.\n    predicted_linear_response: Optional `Tensor` with `shape`, `dtype` matching\n      `response`; represents `offset` shifted initial linear predictions based\n      on current `model_coefficients`.\n      Default value: `offset` if `model_coefficients is None`, and\n      `tf.linalg.matvec(model_matrix, model_coefficients_start) + offset`\n      otherwise.\n    offset: Optional `Tensor` with `shape`, `dtype` matching `response`;\n      represents constant shift applied to `predicted_linear_response`.\n      Default value: `None` (i.e., `tf.zeros_like(response)`).\n    name: Python `str` used as name prefix to ops created by this function.\n      Default value: `\"prepare_args\"`.\n\n  Returns:\n    model_matrix: A `Tensor` with `shape`, `dtype` and values of the\n      `model_matrix` argument.\n    response: A `Tensor` with `shape`, `dtype` and values of the\n      `response` argument.\n    model_coefficients_start: A `Tensor` with `shape`, `dtype` and\n      values of the `model_coefficients_start` argument if specified.\n      A (batch of) vector-shaped `Tensors` with `dtype` matching `model_matrix`\n      containing the default starting point otherwise.\n    predicted_linear_response:  A `Tensor` with `shape`, `dtype` and\n      values of the `predicted_linear_response` argument if specified.\n      A `Tensor` with `shape`, `dtype` matching `response` containing the\n      default value otherwise.\n    offset: A `Tensor` with `shape`, `dtype` and values of the `offset` argument\n      if specified or `None` otherwise.\n  \"\"\"\n  graph_deps = [model_matrix, response, model_coefficients,\n                predicted_linear_response, offset]\n  with tf.compat.v1.name_scope(name, 'prepare_args', graph_deps):\n    dtype = dtype_util.common_dtype(graph_deps, np.float32)\n\n    model_matrix = tf.convert_to_tensor(\n        value=model_matrix, dtype=dtype, name='model_matrix')\n\n    if offset is not None:\n      offset = tf.convert_to_tensor(value=offset, dtype=dtype, name='offset')\n\n    response = tf.convert_to_tensor(\n        value=response, dtype=dtype, name='response')\n\n    use_default_model_coefficients = model_coefficients is None\n    if use_default_model_coefficients:\n      # User did not supply model coefficients; assume they're all zero.\n      batch_shape = tf.shape(input=model_matrix)[:-2]\n      num_columns = tf.shape(input=model_matrix)[-1]\n      model_coefficients = tf.zeros(\n          shape=tf.concat([batch_shape, [num_columns]], axis=0),\n          dtype=dtype, name='model_coefficients')\n    else:\n      # User did supply model coefficients; convert to Tensor in case it's\n      # numpy or literal.\n      model_coefficients = tf.convert_to_tensor(\n          value=model_coefficients, dtype=dtype, name='model_coefficients')\n\n    if predicted_linear_response is None:\n      if use_default_model_coefficients:\n        # Since we're using zeros for model_coefficients, we know the predicted\n        # linear response will also be all zeros.\n        if offset is None:\n          predicted_linear_response = tf.zeros_like(\n              response, dtype, name='predicted_linear_response')\n        else:\n          predicted_linear_response = tf.broadcast_to(\n              offset,\n              tf.shape(input=response),\n              name='predicted_linear_response')\n      else:\n        # We were given model_coefficients but not the predicted linear\n        # response.\n        predicted_linear_response = calculate_linear_predictor(\n            model_matrix, model_coefficients, offset)\n    else:\n      predicted_linear_response = tf.convert_to_tensor(\n          value=predicted_linear_response,\n          dtype=dtype,\n          name='predicted_linear_response')\n\n  return [\n      model_matrix,\n      response,\n      model_coefficients,\n      predicted_linear_response,\n      offset,\n  ]", "language": "python", "code": "def prepare_args(model_matrix,\n                 response,\n                 model_coefficients,\n                 predicted_linear_response,\n                 offset,\n                 name=None):\n  \"\"\"Helper to `fit` which sanitizes input args.\n\n  Args:\n    model_matrix: (Batch of) `float`-like, matrix-shaped `Tensor` where each row\n      represents a sample's features.\n    response: (Batch of) vector-shaped `Tensor` where each element represents a\n      sample's observed response (to the corresponding row of features). Must\n      have same `dtype` as `model_matrix`.\n    model_coefficients: Optional (batch of) vector-shaped `Tensor` representing\n      the model coefficients, one for each column in `model_matrix`. Must have\n      same `dtype` as `model_matrix`.\n      Default value: `tf.zeros(tf.shape(model_matrix)[-1], model_matrix.dtype)`.\n    predicted_linear_response: Optional `Tensor` with `shape`, `dtype` matching\n      `response`; represents `offset` shifted initial linear predictions based\n      on current `model_coefficients`.\n      Default value: `offset` if `model_coefficients is None`, and\n      `tf.linalg.matvec(model_matrix, model_coefficients_start) + offset`\n      otherwise.\n    offset: Optional `Tensor` with `shape`, `dtype` matching `response`;\n      represents constant shift applied to `predicted_linear_response`.\n      Default value: `None` (i.e., `tf.zeros_like(response)`).\n    name: Python `str` used as name prefix to ops created by this function.\n      Default value: `\"prepare_args\"`.\n\n  Returns:\n    model_matrix: A `Tensor` with `shape`, `dtype` and values of the\n      `model_matrix` argument.\n    response: A `Tensor` with `shape`, `dtype` and values of the\n      `response` argument.\n    model_coefficients_start: A `Tensor` with `shape`, `dtype` and\n      values of the `model_coefficients_start` argument if specified.\n      A (batch of) vector-shaped `Tensors` with `dtype` matching `model_matrix`\n      containing the default starting point otherwise.\n    predicted_linear_response:  A `Tensor` with `shape`, `dtype` and\n      values of the `predicted_linear_response` argument if specified.\n      A `Tensor` with `shape`, `dtype` matching `response` containing the\n      default value otherwise.\n    offset: A `Tensor` with `shape`, `dtype` and values of the `offset` argument\n      if specified or `None` otherwise.\n  \"\"\"\n  graph_deps = [model_matrix, response, model_coefficients,\n                predicted_linear_response, offset]\n  with tf.compat.v1.name_scope(name, 'prepare_args', graph_deps):\n    dtype = dtype_util.common_dtype(graph_deps, np.float32)\n\n    model_matrix = tf.convert_to_tensor(\n        value=model_matrix, dtype=dtype, name='model_matrix')\n\n    if offset is not None:\n      offset = tf.convert_to_tensor(value=offset, dtype=dtype, name='offset')\n\n    response = tf.convert_to_tensor(\n        value=response, dtype=dtype, name='response')\n\n    use_default_model_coefficients = model_coefficients is None\n    if use_default_model_coefficients:\n      # User did not supply model coefficients; assume they're all zero.\n      batch_shape = tf.shape(input=model_matrix)[:-2]\n      num_columns = tf.shape(input=model_matrix)[-1]\n      model_coefficients = tf.zeros(\n          shape=tf.concat([batch_shape, [num_columns]], axis=0),\n          dtype=dtype, name='model_coefficients')\n    else:\n      # User did supply model coefficients; convert to Tensor in case it's\n      # numpy or literal.\n      model_coefficients = tf.convert_to_tensor(\n          value=model_coefficients, dtype=dtype, name='model_coefficients')\n\n    if predicted_linear_response is None:\n      if use_default_model_coefficients:\n        # Since we're using zeros for model_coefficients, we know the predicted\n        # linear response will also be all zeros.\n        if offset is None:\n          predicted_linear_response = tf.zeros_like(\n              response, dtype, name='predicted_linear_response')\n        else:\n          predicted_linear_response = tf.broadcast_to(\n              offset,\n              tf.shape(input=response),\n              name='predicted_linear_response')\n      else:\n        # We were given model_coefficients but not the predicted linear\n        # response.\n        predicted_linear_response = calculate_linear_predictor(\n            model_matrix, model_coefficients, offset)\n    else:\n      predicted_linear_response = tf.convert_to_tensor(\n          value=predicted_linear_response,\n          dtype=dtype,\n          name='predicted_linear_response')\n\n  return [\n      model_matrix,\n      response,\n      model_coefficients,\n      predicted_linear_response,\n      offset,\n  ]", "code_tokens": ["def", "prepare_args", "(", "model_matrix", ",", "response", ",", "model_coefficients", ",", "predicted_linear_response", ",", "offset", ",", "name", "=", "None", ")", ":", "graph_deps", "=", "[", "model_matrix", ",", "response", ",", "model_coefficients", ",", "predicted_linear_response", ",", "offset", "]", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'prepare_args'", ",", "graph_deps", ")", ":", "dtype", "=", "dtype_util", ".", "common_dtype", "(", "graph_deps", ",", "np", ".", "float32", ")", "model_matrix", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "model_matrix", ",", "dtype", "=", "dtype", ",", "name", "=", "'model_matrix'", ")", "if", "offset", "is", "not", "None", ":", "offset", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "offset", ",", "dtype", "=", "dtype", ",", "name", "=", "'offset'", ")", "response", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "response", ",", "dtype", "=", "dtype", ",", "name", "=", "'response'", ")", "use_default_model_coefficients", "=", "model_coefficients", "is", "None", "if", "use_default_model_coefficients", ":", "# User did not supply model coefficients; assume they're all zero.", "batch_shape", "=", "tf", ".", "shape", "(", "input", "=", "model_matrix", ")", "[", ":", "-", "2", "]", "num_columns", "=", "tf", ".", "shape", "(", "input", "=", "model_matrix", ")", "[", "-", "1", "]", "model_coefficients", "=", "tf", ".", "zeros", "(", "shape", "=", "tf", ".", "concat", "(", "[", "batch_shape", ",", "[", "num_columns", "]", "]", ",", "axis", "=", "0", ")", ",", "dtype", "=", "dtype", ",", "name", "=", "'model_coefficients'", ")", "else", ":", "# User did supply model coefficients; convert to Tensor in case it's", "# numpy or literal.", "model_coefficients", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "model_coefficients", ",", "dtype", "=", "dtype", ",", "name", "=", "'model_coefficients'", ")", "if", "predicted_linear_response", "is", "None", ":", "if", "use_default_model_coefficients", ":", "# Since we're using zeros for model_coefficients, we know the predicted", "# linear response will also be all zeros.", "if", "offset", "is", "None", ":", "predicted_linear_response", "=", "tf", ".", "zeros_like", "(", "response", ",", "dtype", ",", "name", "=", "'predicted_linear_response'", ")", "else", ":", "predicted_linear_response", "=", "tf", ".", "broadcast_to", "(", "offset", ",", "tf", ".", "shape", "(", "input", "=", "response", ")", ",", "name", "=", "'predicted_linear_response'", ")", "else", ":", "# We were given model_coefficients but not the predicted linear", "# response.", "predicted_linear_response", "=", "calculate_linear_predictor", "(", "model_matrix", ",", "model_coefficients", ",", "offset", ")", "else", ":", "predicted_linear_response", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "predicted_linear_response", ",", "dtype", "=", "dtype", ",", "name", "=", "'predicted_linear_response'", ")", "return", "[", "model_matrix", ",", "response", ",", "model_coefficients", ",", "predicted_linear_response", ",", "offset", ",", "]"], "docstring": "Helper to `fit` which sanitizes input args.\n\n  Args:\n    model_matrix: (Batch of) `float`-like, matrix-shaped `Tensor` where each row\n      represents a sample's features.\n    response: (Batch of) vector-shaped `Tensor` where each element represents a\n      sample's observed response (to the corresponding row of features). Must\n      have same `dtype` as `model_matrix`.\n    model_coefficients: Optional (batch of) vector-shaped `Tensor` representing\n      the model coefficients, one for each column in `model_matrix`. Must have\n      same `dtype` as `model_matrix`.\n      Default value: `tf.zeros(tf.shape(model_matrix)[-1], model_matrix.dtype)`.\n    predicted_linear_response: Optional `Tensor` with `shape`, `dtype` matching\n      `response`; represents `offset` shifted initial linear predictions based\n      on current `model_coefficients`.\n      Default value: `offset` if `model_coefficients is None`, and\n      `tf.linalg.matvec(model_matrix, model_coefficients_start) + offset`\n      otherwise.\n    offset: Optional `Tensor` with `shape`, `dtype` matching `response`;\n      represents constant shift applied to `predicted_linear_response`.\n      Default value: `None` (i.e., `tf.zeros_like(response)`).\n    name: Python `str` used as name prefix to ops created by this function.\n      Default value: `\"prepare_args\"`.\n\n  Returns:\n    model_matrix: A `Tensor` with `shape`, `dtype` and values of the\n      `model_matrix` argument.\n    response: A `Tensor` with `shape`, `dtype` and values of the\n      `response` argument.\n    model_coefficients_start: A `Tensor` with `shape`, `dtype` and\n      values of the `model_coefficients_start` argument if specified.\n      A (batch of) vector-shaped `Tensors` with `dtype` matching `model_matrix`\n      containing the default starting point otherwise.\n    predicted_linear_response:  A `Tensor` with `shape`, `dtype` and\n      values of the `predicted_linear_response` argument if specified.\n      A `Tensor` with `shape`, `dtype` matching `response` containing the\n      default value otherwise.\n    offset: A `Tensor` with `shape`, `dtype` and values of the `offset` argument\n      if specified or `None` otherwise.", "docstring_tokens": ["Helper", "to", "fit", "which", "sanitizes", "input", "args", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/glm/fisher_scoring.py#L517-L620", "partition": "test", "index": 894, "time": "2018-05-07 20:39:11"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/glm/family.py", "func_name": "ExponentialFamily._name_scope", "original_string": "def _name_scope(self, name=None, default_name=None, values=None):\n    \"\"\"Helper function to standardize op scope.\"\"\"\n    with tf.compat.v1.name_scope(self.name):\n      with tf.compat.v1.name_scope(\n          name, default_name, values=values or []) as scope:\n        yield scope", "language": "python", "code": "def _name_scope(self, name=None, default_name=None, values=None):\n    \"\"\"Helper function to standardize op scope.\"\"\"\n    with tf.compat.v1.name_scope(self.name):\n      with tf.compat.v1.name_scope(\n          name, default_name, values=values or []) as scope:\n        yield scope", "code_tokens": ["def", "_name_scope", "(", "self", ",", "name", "=", "None", ",", "default_name", "=", "None", ",", "values", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "self", ".", "name", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "default_name", ",", "values", "=", "values", "or", "[", "]", ")", "as", "scope", ":", "yield", "scope"], "docstring": "Helper function to standardize op scope.", "docstring_tokens": ["Helper", "function", "to", "standardize", "op", "scope", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/glm/family.py#L174-L179", "partition": "test", "index": 901, "time": "2018-05-07 20:39:11"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/random_ops.py", "func_name": "random_rademacher", "original_string": "def random_rademacher(shape, dtype=tf.float32, seed=None, name=None):\n  \"\"\"Generates `Tensor` consisting of `-1` or `+1`, chosen uniformly at random.\n\n  For more details, see [Rademacher distribution](\n  https://en.wikipedia.org/wiki/Rademacher_distribution).\n\n  Args:\n    shape: Vector-shaped, `int` `Tensor` representing shape of output.\n    dtype: (Optional) TF `dtype` representing `dtype` of output.\n    seed: (Optional) Python integer to seed the random number generator.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'random_rademacher').\n\n  Returns:\n    rademacher: `Tensor` with specified `shape` and `dtype` consisting of `-1`\n      or `+1` chosen uniformly-at-random.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'random_rademacher', [shape, seed]):\n    # Choose the dtype to cause `2 * random_bernoulli - 1` to run in the same\n    # memory (host or device) as the downstream cast will want to put it.  The\n    # convention on GPU is that int32 are in host memory and int64 are in device\n    # memory.\n    generation_dtype = tf.int64 if tf.as_dtype(dtype) != tf.int32 else tf.int32\n    random_bernoulli = tf.random.uniform(\n        shape, minval=0, maxval=2, dtype=generation_dtype, seed=seed)\n    return tf.cast(2 * random_bernoulli - 1, dtype)", "language": "python", "code": "def random_rademacher(shape, dtype=tf.float32, seed=None, name=None):\n  \"\"\"Generates `Tensor` consisting of `-1` or `+1`, chosen uniformly at random.\n\n  For more details, see [Rademacher distribution](\n  https://en.wikipedia.org/wiki/Rademacher_distribution).\n\n  Args:\n    shape: Vector-shaped, `int` `Tensor` representing shape of output.\n    dtype: (Optional) TF `dtype` representing `dtype` of output.\n    seed: (Optional) Python integer to seed the random number generator.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'random_rademacher').\n\n  Returns:\n    rademacher: `Tensor` with specified `shape` and `dtype` consisting of `-1`\n      or `+1` chosen uniformly-at-random.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'random_rademacher', [shape, seed]):\n    # Choose the dtype to cause `2 * random_bernoulli - 1` to run in the same\n    # memory (host or device) as the downstream cast will want to put it.  The\n    # convention on GPU is that int32 are in host memory and int64 are in device\n    # memory.\n    generation_dtype = tf.int64 if tf.as_dtype(dtype) != tf.int32 else tf.int32\n    random_bernoulli = tf.random.uniform(\n        shape, minval=0, maxval=2, dtype=generation_dtype, seed=seed)\n    return tf.cast(2 * random_bernoulli - 1, dtype)", "code_tokens": ["def", "random_rademacher", "(", "shape", ",", "dtype", "=", "tf", ".", "float32", ",", "seed", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'random_rademacher'", ",", "[", "shape", ",", "seed", "]", ")", ":", "# Choose the dtype to cause `2 * random_bernoulli - 1` to run in the same", "# memory (host or device) as the downstream cast will want to put it.  The", "# convention on GPU is that int32 are in host memory and int64 are in device", "# memory.", "generation_dtype", "=", "tf", ".", "int64", "if", "tf", ".", "as_dtype", "(", "dtype", ")", "!=", "tf", ".", "int32", "else", "tf", ".", "int32", "random_bernoulli", "=", "tf", ".", "random", ".", "uniform", "(", "shape", ",", "minval", "=", "0", ",", "maxval", "=", "2", ",", "dtype", "=", "generation_dtype", ",", "seed", "=", "seed", ")", "return", "tf", ".", "cast", "(", "2", "*", "random_bernoulli", "-", "1", ",", "dtype", ")"], "docstring": "Generates `Tensor` consisting of `-1` or `+1`, chosen uniformly at random.\n\n  For more details, see [Rademacher distribution](\n  https://en.wikipedia.org/wiki/Rademacher_distribution).\n\n  Args:\n    shape: Vector-shaped, `int` `Tensor` representing shape of output.\n    dtype: (Optional) TF `dtype` representing `dtype` of output.\n    seed: (Optional) Python integer to seed the random number generator.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'random_rademacher').\n\n  Returns:\n    rademacher: `Tensor` with specified `shape` and `dtype` consisting of `-1`\n      or `+1` chosen uniformly-at-random.", "docstring_tokens": ["Generates", "Tensor", "consisting", "of", "-", "1", "or", "+", "1", "chosen", "uniformly", "at", "random", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/random_ops.py#L33-L58", "partition": "test", "index": 879, "time": "2018-05-22 13:25:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/generated_random_variables.py", "func_name": "as_random_variable", "original_string": "def as_random_variable(distribution,\n                       sample_shape=(),\n                       value=None):\n  \"\"\"Wrap an existing distribution as a traceable random variable.\n\n  This enables the use of custom or user-provided distributions in\n  Edward models. Unlike a bare `RandomVariable` object, this method\n  wraps the constructor so it is included in the Edward trace and its\n  values can be properly intercepted and overridden.\n\n  Where possible, you should prefer the built-in constructors\n  (`ed.Normal`, etc); these simultaneously construct a Distribution\n  and a RandomVariable object so that the distribution parameters\n  themselves may be intercepted and overridden. RVs constructed via\n  `as_random_variable()` have a fixed distribution and may not support\n  program transformations (e.g, conjugate marginalization) that rely\n  on overriding distribution parameters.\n\n  Args:\n    distribution: tfd.Distribution governing the distribution of the random\n      variable, such as sampling and log-probabilities.\n    sample_shape: tf.TensorShape of samples to draw from the random variable.\n      Default is `()` corresponding to a single sample.\n    value: Fixed tf.Tensor to associate with random variable. Must have shape\n      `sample_shape + distribution.batch_shape + distribution.event_shape`.\n      Default is to sample from random variable according to `sample_shape`.\n\n  Returns:\n    rv: a `RandomVariable` wrapping the provided distribution.\n\n  #### Example\n\n  ```python\n  from tensorflow_probability import distributions as tfd\n  from tensorflow_probability import edward2 as ed\n\n  def model():\n    # equivalent to ed.Normal(0., 1., name='x')\n    return ed.as_random_variable(tfd.Normal(0., 1., name='x'))\n\n  log_joint = ed.make_log_joint_fn(model)\n  output = log_joint(x=2.)\n  ```\n  \"\"\"\n\n  return _build_custom_rv(distribution=distribution,\n                          sample_shape=sample_shape,\n                          value=value,\n                          name=_simple_name(distribution))", "language": "python", "code": "def as_random_variable(distribution,\n                       sample_shape=(),\n                       value=None):\n  \"\"\"Wrap an existing distribution as a traceable random variable.\n\n  This enables the use of custom or user-provided distributions in\n  Edward models. Unlike a bare `RandomVariable` object, this method\n  wraps the constructor so it is included in the Edward trace and its\n  values can be properly intercepted and overridden.\n\n  Where possible, you should prefer the built-in constructors\n  (`ed.Normal`, etc); these simultaneously construct a Distribution\n  and a RandomVariable object so that the distribution parameters\n  themselves may be intercepted and overridden. RVs constructed via\n  `as_random_variable()` have a fixed distribution and may not support\n  program transformations (e.g, conjugate marginalization) that rely\n  on overriding distribution parameters.\n\n  Args:\n    distribution: tfd.Distribution governing the distribution of the random\n      variable, such as sampling and log-probabilities.\n    sample_shape: tf.TensorShape of samples to draw from the random variable.\n      Default is `()` corresponding to a single sample.\n    value: Fixed tf.Tensor to associate with random variable. Must have shape\n      `sample_shape + distribution.batch_shape + distribution.event_shape`.\n      Default is to sample from random variable according to `sample_shape`.\n\n  Returns:\n    rv: a `RandomVariable` wrapping the provided distribution.\n\n  #### Example\n\n  ```python\n  from tensorflow_probability import distributions as tfd\n  from tensorflow_probability import edward2 as ed\n\n  def model():\n    # equivalent to ed.Normal(0., 1., name='x')\n    return ed.as_random_variable(tfd.Normal(0., 1., name='x'))\n\n  log_joint = ed.make_log_joint_fn(model)\n  output = log_joint(x=2.)\n  ```\n  \"\"\"\n\n  return _build_custom_rv(distribution=distribution,\n                          sample_shape=sample_shape,\n                          value=value,\n                          name=_simple_name(distribution))", "code_tokens": ["def", "as_random_variable", "(", "distribution", ",", "sample_shape", "=", "(", ")", ",", "value", "=", "None", ")", ":", "return", "_build_custom_rv", "(", "distribution", "=", "distribution", ",", "sample_shape", "=", "sample_shape", ",", "value", "=", "value", ",", "name", "=", "_simple_name", "(", "distribution", ")", ")"], "docstring": "Wrap an existing distribution as a traceable random variable.\n\n  This enables the use of custom or user-provided distributions in\n  Edward models. Unlike a bare `RandomVariable` object, this method\n  wraps the constructor so it is included in the Edward trace and its\n  values can be properly intercepted and overridden.\n\n  Where possible, you should prefer the built-in constructors\n  (`ed.Normal`, etc); these simultaneously construct a Distribution\n  and a RandomVariable object so that the distribution parameters\n  themselves may be intercepted and overridden. RVs constructed via\n  `as_random_variable()` have a fixed distribution and may not support\n  program transformations (e.g, conjugate marginalization) that rely\n  on overriding distribution parameters.\n\n  Args:\n    distribution: tfd.Distribution governing the distribution of the random\n      variable, such as sampling and log-probabilities.\n    sample_shape: tf.TensorShape of samples to draw from the random variable.\n      Default is `()` corresponding to a single sample.\n    value: Fixed tf.Tensor to associate with random variable. Must have shape\n      `sample_shape + distribution.batch_shape + distribution.event_shape`.\n      Default is to sample from random variable according to `sample_shape`.\n\n  Returns:\n    rv: a `RandomVariable` wrapping the provided distribution.\n\n  #### Example\n\n  ```python\n  from tensorflow_probability import distributions as tfd\n  from tensorflow_probability import edward2 as ed\n\n  def model():\n    # equivalent to ed.Normal(0., 1., name='x')\n    return ed.as_random_variable(tfd.Normal(0., 1., name='x'))\n\n  log_joint = ed.make_log_joint_fn(model)\n  output = log_joint(x=2.)\n  ```", "docstring_tokens": ["Wrap", "an", "existing", "distribution", "as", "a", "traceable", "random", "variable", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/generated_random_variables.py#L97-L145", "partition": "test", "index": 617, "time": "2018-06-06 16:04:56"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/generated_random_variables.py", "func_name": "_build_custom_rv", "original_string": "def _build_custom_rv(distribution, sample_shape, value, name):\n  \"\"\"RandomVariable constructor with a dummy name argument.\"\"\"\n  # Program transformations (e.g., `make_log_joint_fn`) assume that\n  # the traced constructor has `name` and `value` kwargs, enabling\n  # them to override the value of an RV according to its name.\n  # User-defined RVs inherit their name from the provided\n  # distribution; this helper method exposes the name as a dummy kwarg\n  # so that it's visible to program transformations.\n  del name  # unused\n  return RandomVariable(distribution=distribution,\n                        sample_shape=sample_shape,\n                        value=value)", "language": "python", "code": "def _build_custom_rv(distribution, sample_shape, value, name):\n  \"\"\"RandomVariable constructor with a dummy name argument.\"\"\"\n  # Program transformations (e.g., `make_log_joint_fn`) assume that\n  # the traced constructor has `name` and `value` kwargs, enabling\n  # them to override the value of an RV according to its name.\n  # User-defined RVs inherit their name from the provided\n  # distribution; this helper method exposes the name as a dummy kwarg\n  # so that it's visible to program transformations.\n  del name  # unused\n  return RandomVariable(distribution=distribution,\n                        sample_shape=sample_shape,\n                        value=value)", "code_tokens": ["def", "_build_custom_rv", "(", "distribution", ",", "sample_shape", ",", "value", ",", "name", ")", ":", "# Program transformations (e.g., `make_log_joint_fn`) assume that", "# the traced constructor has `name` and `value` kwargs, enabling", "# them to override the value of an RV according to its name.", "# User-defined RVs inherit their name from the provided", "# distribution; this helper method exposes the name as a dummy kwarg", "# so that it's visible to program transformations.", "del", "name", "# unused", "return", "RandomVariable", "(", "distribution", "=", "distribution", ",", "sample_shape", "=", "sample_shape", ",", "value", "=", "value", ")"], "docstring": "RandomVariable constructor with a dummy name argument.", "docstring_tokens": ["RandomVariable", "constructor", "with", "a", "dummy", "name", "argument", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/generated_random_variables.py#L83-L94", "partition": "test", "index": 616, "time": "2018-06-06 16:04:56"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/generated_random_variables.py", "func_name": "_simple_name", "original_string": "def _simple_name(distribution):\n  \"\"\"Infer the original name passed into a distribution constructor.\n\n  Distributions typically follow the pattern of\n  with.name_scope(name) as name:\n    super(name=name)\n  so we attempt to reverse the name-scope transformation to allow\n  addressing of RVs by the distribution's original, user-visible\n  name kwarg.\n\n  Args:\n    distribution: a tfd.Distribution instance.\n  Returns:\n    simple_name: the original name passed into the Distribution.\n\n  #### Example\n\n  ```\n  d1 = tfd.Normal(0., 1., name='x') # d1.name = 'x/'\n  d2 = tfd.Normal(0., 1., name='x') # d2.name = 'x_2/'\n  _simple_name(d2) # returns 'x'\n\n  ```\n\n  \"\"\"\n  simple_name = distribution.name\n\n  # turn 'scope/x/' into 'x'\n  if simple_name.endswith('/'):\n    simple_name = simple_name.split('/')[-2]\n\n  # turn 'x_3' into 'x'\n  parts = simple_name.split('_')\n  if parts[-1].isdigit():\n    simple_name = '_'.join(parts[:-1])\n\n  return simple_name", "language": "python", "code": "def _simple_name(distribution):\n  \"\"\"Infer the original name passed into a distribution constructor.\n\n  Distributions typically follow the pattern of\n  with.name_scope(name) as name:\n    super(name=name)\n  so we attempt to reverse the name-scope transformation to allow\n  addressing of RVs by the distribution's original, user-visible\n  name kwarg.\n\n  Args:\n    distribution: a tfd.Distribution instance.\n  Returns:\n    simple_name: the original name passed into the Distribution.\n\n  #### Example\n\n  ```\n  d1 = tfd.Normal(0., 1., name='x') # d1.name = 'x/'\n  d2 = tfd.Normal(0., 1., name='x') # d2.name = 'x_2/'\n  _simple_name(d2) # returns 'x'\n\n  ```\n\n  \"\"\"\n  simple_name = distribution.name\n\n  # turn 'scope/x/' into 'x'\n  if simple_name.endswith('/'):\n    simple_name = simple_name.split('/')[-2]\n\n  # turn 'x_3' into 'x'\n  parts = simple_name.split('_')\n  if parts[-1].isdigit():\n    simple_name = '_'.join(parts[:-1])\n\n  return simple_name", "code_tokens": ["def", "_simple_name", "(", "distribution", ")", ":", "simple_name", "=", "distribution", ".", "name", "# turn 'scope/x/' into 'x'", "if", "simple_name", ".", "endswith", "(", "'/'", ")", ":", "simple_name", "=", "simple_name", ".", "split", "(", "'/'", ")", "[", "-", "2", "]", "# turn 'x_3' into 'x'", "parts", "=", "simple_name", ".", "split", "(", "'_'", ")", "if", "parts", "[", "-", "1", "]", ".", "isdigit", "(", ")", ":", "simple_name", "=", "'_'", ".", "join", "(", "parts", "[", ":", "-", "1", "]", ")", "return", "simple_name"], "docstring": "Infer the original name passed into a distribution constructor.\n\n  Distributions typically follow the pattern of\n  with.name_scope(name) as name:\n    super(name=name)\n  so we attempt to reverse the name-scope transformation to allow\n  addressing of RVs by the distribution's original, user-visible\n  name kwarg.\n\n  Args:\n    distribution: a tfd.Distribution instance.\n  Returns:\n    simple_name: the original name passed into the Distribution.\n\n  #### Example\n\n  ```\n  d1 = tfd.Normal(0., 1., name='x') # d1.name = 'x/'\n  d2 = tfd.Normal(0., 1., name='x') # d2.name = 'x_2/'\n  _simple_name(d2) # returns 'x'\n\n  ```", "docstring_tokens": ["Infer", "the", "original", "name", "passed", "into", "a", "distribution", "constructor", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/generated_random_variables.py#L43-L79", "partition": "test", "index": 615, "time": "2018-06-06 16:04:56"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/random_variable.py", "func_name": "RandomVariable.sample_shape_tensor", "original_string": "def sample_shape_tensor(self, name=\"sample_shape_tensor\"):\n    \"\"\"Sample shape of random variable as a 1-D `Tensor`.\n\n    Args:\n      name: name to give to the op\n\n    Returns:\n      sample_shape: `Tensor`.\n    \"\"\"\n    with tf.compat.v1.name_scope(name):\n      if isinstance(self._sample_shape, tf.Tensor):\n        return self._sample_shape\n      return tf.convert_to_tensor(\n          value=self.sample_shape.as_list(), dtype=tf.int32)", "language": "python", "code": "def sample_shape_tensor(self, name=\"sample_shape_tensor\"):\n    \"\"\"Sample shape of random variable as a 1-D `Tensor`.\n\n    Args:\n      name: name to give to the op\n\n    Returns:\n      sample_shape: `Tensor`.\n    \"\"\"\n    with tf.compat.v1.name_scope(name):\n      if isinstance(self._sample_shape, tf.Tensor):\n        return self._sample_shape\n      return tf.convert_to_tensor(\n          value=self.sample_shape.as_list(), dtype=tf.int32)", "code_tokens": ["def", "sample_shape_tensor", "(", "self", ",", "name", "=", "\"sample_shape_tensor\"", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ")", ":", "if", "isinstance", "(", "self", ".", "_sample_shape", ",", "tf", ".", "Tensor", ")", ":", "return", "self", ".", "_sample_shape", "return", "tf", ".", "convert_to_tensor", "(", "value", "=", "self", ".", "sample_shape", ".", "as_list", "(", ")", ",", "dtype", "=", "tf", ".", "int32", ")"], "docstring": "Sample shape of random variable as a 1-D `Tensor`.\n\n    Args:\n      name: name to give to the op\n\n    Returns:\n      sample_shape: `Tensor`.", "docstring_tokens": ["Sample", "shape", "of", "random", "variable", "as", "a", "1", "-", "D", "Tensor", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/random_variable.py#L139-L152", "partition": "test", "index": 768, "time": "2018-06-07 19:53:53"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/transformed_kernel.py", "func_name": "forward_log_det_jacobian_fn", "original_string": "def forward_log_det_jacobian_fn(bijector):\n  \"\"\"Makes a function which applies a list of Bijectors' `log_det_jacobian`s.\"\"\"\n  if not mcmc_util.is_list_like(bijector):\n    bijector = [bijector]\n\n  def fn(transformed_state_parts, event_ndims):\n    return sum([\n        b.forward_log_det_jacobian(sp, event_ndims=e)\n        for b, e, sp in zip(bijector, event_ndims, transformed_state_parts)\n    ])\n\n  return fn", "language": "python", "code": "def forward_log_det_jacobian_fn(bijector):\n  \"\"\"Makes a function which applies a list of Bijectors' `log_det_jacobian`s.\"\"\"\n  if not mcmc_util.is_list_like(bijector):\n    bijector = [bijector]\n\n  def fn(transformed_state_parts, event_ndims):\n    return sum([\n        b.forward_log_det_jacobian(sp, event_ndims=e)\n        for b, e, sp in zip(bijector, event_ndims, transformed_state_parts)\n    ])\n\n  return fn", "code_tokens": ["def", "forward_log_det_jacobian_fn", "(", "bijector", ")", ":", "if", "not", "mcmc_util", ".", "is_list_like", "(", "bijector", ")", ":", "bijector", "=", "[", "bijector", "]", "def", "fn", "(", "transformed_state_parts", ",", "event_ndims", ")", ":", "return", "sum", "(", "[", "b", ".", "forward_log_det_jacobian", "(", "sp", ",", "event_ndims", "=", "e", ")", "for", "b", ",", "e", ",", "sp", "in", "zip", "(", "bijector", ",", "event_ndims", ",", "transformed_state_parts", ")", "]", ")", "return", "fn"], "docstring": "Makes a function which applies a list of Bijectors' `log_det_jacobian`s.", "docstring_tokens": ["Makes", "a", "function", "which", "applies", "a", "list", "of", "Bijectors", "log_det_jacobian", "s", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/transformed_kernel.py#L42-L53", "partition": "test", "index": 990, "time": "2018-06-11 13:48:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/transformed_kernel.py", "func_name": "forward_transform_fn", "original_string": "def forward_transform_fn(bijector):\n  \"\"\"Makes a function which applies a list of Bijectors' `forward`s.\"\"\"\n  if not mcmc_util.is_list_like(bijector):\n    bijector = [bijector]\n\n  def fn(transformed_state_parts):\n    return [b.forward(sp) for b, sp in zip(bijector, transformed_state_parts)]\n\n  return fn", "language": "python", "code": "def forward_transform_fn(bijector):\n  \"\"\"Makes a function which applies a list of Bijectors' `forward`s.\"\"\"\n  if not mcmc_util.is_list_like(bijector):\n    bijector = [bijector]\n\n  def fn(transformed_state_parts):\n    return [b.forward(sp) for b, sp in zip(bijector, transformed_state_parts)]\n\n  return fn", "code_tokens": ["def", "forward_transform_fn", "(", "bijector", ")", ":", "if", "not", "mcmc_util", ".", "is_list_like", "(", "bijector", ")", ":", "bijector", "=", "[", "bijector", "]", "def", "fn", "(", "transformed_state_parts", ")", ":", "return", "[", "b", ".", "forward", "(", "sp", ")", "for", "b", ",", "sp", "in", "zip", "(", "bijector", ",", "transformed_state_parts", ")", "]", "return", "fn"], "docstring": "Makes a function which applies a list of Bijectors' `forward`s.", "docstring_tokens": ["Makes", "a", "function", "which", "applies", "a", "list", "of", "Bijectors", "forward", "s", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/transformed_kernel.py#L56-L64", "partition": "test", "index": 991, "time": "2018-06-11 13:48:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/transformed_kernel.py", "func_name": "inverse_transform_fn", "original_string": "def inverse_transform_fn(bijector):\n  \"\"\"Makes a function which applies a list of Bijectors' `inverse`s.\"\"\"\n  if not mcmc_util.is_list_like(bijector):\n    bijector = [bijector]\n  def fn(state_parts):\n    return [b.inverse(sp)\n            for b, sp in zip(bijector, state_parts)]\n  return fn", "language": "python", "code": "def inverse_transform_fn(bijector):\n  \"\"\"Makes a function which applies a list of Bijectors' `inverse`s.\"\"\"\n  if not mcmc_util.is_list_like(bijector):\n    bijector = [bijector]\n  def fn(state_parts):\n    return [b.inverse(sp)\n            for b, sp in zip(bijector, state_parts)]\n  return fn", "code_tokens": ["def", "inverse_transform_fn", "(", "bijector", ")", ":", "if", "not", "mcmc_util", ".", "is_list_like", "(", "bijector", ")", ":", "bijector", "=", "[", "bijector", "]", "def", "fn", "(", "state_parts", ")", ":", "return", "[", "b", ".", "inverse", "(", "sp", ")", "for", "b", ",", "sp", "in", "zip", "(", "bijector", ",", "state_parts", ")", "]", "return", "fn"], "docstring": "Makes a function which applies a list of Bijectors' `inverse`s.", "docstring_tokens": ["Makes", "a", "function", "which", "applies", "a", "list", "of", "Bijectors", "inverse", "s", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/transformed_kernel.py#L67-L74", "partition": "test", "index": 992, "time": "2018-06-11 13:48:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/transformed_kernel.py", "func_name": "TransformedTransitionKernel.one_step", "original_string": "def one_step(self, current_state, previous_kernel_results):\n    \"\"\"Runs one iteration of the Transformed Kernel.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s\n        representing the current state(s) of the Markov chain(s),\n        _after_ application of `bijector.forward`. The first `r`\n        dimensions index independent chains,\n        `r = tf.rank(target_log_prob_fn(*current_state))`. The\n        `inner_kernel.one_step` does not actually use `current_state`,\n        rather it takes as input\n        `previous_kernel_results.transformed_state` (because\n        `TransformedTransitionKernel` creates a copy of the input\n        inner_kernel with a modified `target_log_prob_fn` which\n        internally applies the `bijector.forward`).\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.\n    \"\"\"\n    with tf.compat.v1.name_scope(\n        name=mcmc_util.make_name(self.name, 'transformed_kernel', 'one_step'),\n        values=[previous_kernel_results]):\n      transformed_next_state, kernel_results = self._inner_kernel.one_step(\n          previous_kernel_results.transformed_state,\n          previous_kernel_results.inner_results)\n      transformed_next_state_parts = (\n          transformed_next_state\n          if mcmc_util.is_list_like(transformed_next_state) else\n          [transformed_next_state])\n      next_state_parts = self._forward_transform(transformed_next_state_parts)\n      next_state = (\n          next_state_parts if mcmc_util.is_list_like(transformed_next_state)\n          else next_state_parts[0])\n      kernel_results = TransformedTransitionKernelResults(\n          transformed_state=transformed_next_state,\n          inner_results=kernel_results)\n      return next_state, kernel_results", "language": "python", "code": "def one_step(self, current_state, previous_kernel_results):\n    \"\"\"Runs one iteration of the Transformed Kernel.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s\n        representing the current state(s) of the Markov chain(s),\n        _after_ application of `bijector.forward`. The first `r`\n        dimensions index independent chains,\n        `r = tf.rank(target_log_prob_fn(*current_state))`. The\n        `inner_kernel.one_step` does not actually use `current_state`,\n        rather it takes as input\n        `previous_kernel_results.transformed_state` (because\n        `TransformedTransitionKernel` creates a copy of the input\n        inner_kernel with a modified `target_log_prob_fn` which\n        internally applies the `bijector.forward`).\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.\n    \"\"\"\n    with tf.compat.v1.name_scope(\n        name=mcmc_util.make_name(self.name, 'transformed_kernel', 'one_step'),\n        values=[previous_kernel_results]):\n      transformed_next_state, kernel_results = self._inner_kernel.one_step(\n          previous_kernel_results.transformed_state,\n          previous_kernel_results.inner_results)\n      transformed_next_state_parts = (\n          transformed_next_state\n          if mcmc_util.is_list_like(transformed_next_state) else\n          [transformed_next_state])\n      next_state_parts = self._forward_transform(transformed_next_state_parts)\n      next_state = (\n          next_state_parts if mcmc_util.is_list_like(transformed_next_state)\n          else next_state_parts[0])\n      kernel_results = TransformedTransitionKernelResults(\n          transformed_state=transformed_next_state,\n          inner_results=kernel_results)\n      return next_state, kernel_results", "code_tokens": ["def", "one_step", "(", "self", ",", "current_state", ",", "previous_kernel_results", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", "=", "mcmc_util", ".", "make_name", "(", "self", ".", "name", ",", "'transformed_kernel'", ",", "'one_step'", ")", ",", "values", "=", "[", "previous_kernel_results", "]", ")", ":", "transformed_next_state", ",", "kernel_results", "=", "self", ".", "_inner_kernel", ".", "one_step", "(", "previous_kernel_results", ".", "transformed_state", ",", "previous_kernel_results", ".", "inner_results", ")", "transformed_next_state_parts", "=", "(", "transformed_next_state", "if", "mcmc_util", ".", "is_list_like", "(", "transformed_next_state", ")", "else", "[", "transformed_next_state", "]", ")", "next_state_parts", "=", "self", ".", "_forward_transform", "(", "transformed_next_state_parts", ")", "next_state", "=", "(", "next_state_parts", "if", "mcmc_util", ".", "is_list_like", "(", "transformed_next_state", ")", "else", "next_state_parts", "[", "0", "]", ")", "kernel_results", "=", "TransformedTransitionKernelResults", "(", "transformed_state", "=", "transformed_next_state", ",", "inner_results", "=", "kernel_results", ")", "return", "next_state", ",", "kernel_results"], "docstring": "Runs one iteration of the Transformed Kernel.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s\n        representing the current state(s) of the Markov chain(s),\n        _after_ application of `bijector.forward`. The first `r`\n        dimensions index independent chains,\n        `r = tf.rank(target_log_prob_fn(*current_state))`. The\n        `inner_kernel.one_step` does not actually use `current_state`,\n        rather it takes as input\n        `previous_kernel_results.transformed_state` (because\n        `TransformedTransitionKernel` creates a copy of the input\n        inner_kernel with a modified `target_log_prob_fn` which\n        internally applies the `bijector.forward`).\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.", "docstring_tokens": ["Runs", "one", "iteration", "of", "the", "Transformed", "Kernel", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/transformed_kernel.py#L230-L273", "partition": "test", "index": 993, "time": "2018-06-11 13:48:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", "func_name": "hager_zhang", "original_string": "def hager_zhang(value_and_gradients_function,\n                initial_step_size=None,\n                value_at_initial_step=None,\n                value_at_zero=None,\n                converged=None,\n                threshold_use_approximate_wolfe_condition=1e-6,\n                shrinkage_param=0.66,\n                expansion_param=5.0,\n                sufficient_decrease_param=0.1,\n                curvature_param=0.9,\n                step_size_shrink_param=0.1,\n                max_iterations=50,\n                name=None):\n  \"\"\"The Hager Zhang line search algorithm.\n\n  Performs an inexact line search based on the algorithm of\n  [Hager and Zhang (2006)][2].\n  The univariate objective function `value_and_gradients_function` is typically\n  generated by projecting a multivariate objective function along a search\n  direction. Suppose the multivariate function to be minimized is\n  `g(x1,x2, .. xn)`. Let (d1, d2, ..., dn) be the direction along which we wish\n  to perform a line search. Then the projected univariate function to be used\n  for line search is\n\n  ```None\n    f(a) = g(x1 + d1 * a, x2 + d2 * a, ..., xn + dn * a)\n  ```\n\n  The directional derivative along (d1, d2, ..., dn) is needed for this\n  procedure. This also corresponds to the derivative of the projected function\n  `f(a)` with respect to `a`. Note that this derivative must be negative for\n  `a = 0` if the direction is a descent direction.\n\n  The usual stopping criteria for the line search is the satisfaction of the\n  (weak) Wolfe conditions. For details of the Wolfe conditions, see\n  ref. [3]. On a finite precision machine, the exact Wolfe conditions can\n  be difficult to satisfy when one is very close to the minimum and as argued\n  by [Hager and Zhang (2005)][1], one can only expect the minimum to be\n  determined within square root of machine precision. To improve the situation,\n  they propose to replace the Wolfe conditions with an approximate version\n  depending on the derivative of the function which is applied only when one\n  is very close to the minimum. The following algorithm implements this\n  enhanced scheme.\n\n  ### Usage:\n\n  Primary use of line search methods is as an internal component of a class of\n  optimization algorithms (called line search based methods as opposed to\n  trust region methods). Hence, the end user will typically not want to access\n  line search directly. In particular, inexact line search should not be\n  confused with a univariate minimization method. The stopping criteria of line\n  search is the satisfaction of Wolfe conditions and not the discovery of the\n  minimum of the function.\n\n  With this caveat in mind, the following example illustrates the standalone\n  usage of the line search.\n\n  ```python\n    # Define value and gradient namedtuple\n    ValueAndGradient = namedtuple('ValueAndGradient', ['x', 'f', 'df'])\n    # Define a quadratic target with minimum at 1.3.\n    def value_and_gradients_function(x):\n      return ValueAndGradient(x=x, f=(x - 1.3) ** 2, df=2 * (x-1.3))\n    # Set initial step size.\n    step_size = tf.constant(0.1)\n    ls_result = tfp.optimizer.linesearch.hager_zhang(\n        value_and_gradients_function, initial_step_size=step_size)\n    # Evaluate the results.\n    with tf.Session() as session:\n      results = session.run(ls_result)\n      # Ensure convergence.\n      assert results.converged\n      # If the line search converged, the left and the right ends of the\n      # bracketing interval are identical.\n      assert results.left.x == result.right.x\n      # Print the number of evaluations and the final step size.\n      print (\"Final Step Size: %f, Evaluations: %d\" % (results.left.x,\n                                                       results.func_evals))\n  ```\n\n  ### References:\n  [1]: William Hager, Hongchao Zhang. A new conjugate gradient method with\n    guaranteed descent and an efficient line search. SIAM J. Optim., Vol 16. 1,\n    pp. 170-172. 2005.\n    https://www.math.lsu.edu/~hozhang/papers/cg_descent.pdf\n\n  [2]: William Hager, Hongchao Zhang. Algorithm 851: CG_DESCENT, a conjugate\n    gradient method with guaranteed descent. ACM Transactions on Mathematical\n    Software, Vol 32., 1, pp. 113-137. 2006.\n    http://users.clas.ufl.edu/hager/papers/CG/cg_compare.pdf\n\n  [3]: Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series in\n    Operations Research. pp 33-36. 2006\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    initial_step_size: (Optional) Scalar positive `Tensor` of real dtype, or\n      a tensor of shape [n] in batching mode. The initial value (or values) to\n      try to bracket the minimum. Default is `1.` as a float32.\n      Note that this point need not necessarily bracket the minimum for the line\n      search to work correctly but the supplied value must be greater than 0.\n      A good initial value will make the search converge faster.\n    value_at_initial_step: (Optional) The full return value of evaluating\n      value_and_gradients_function at initial_step_size, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If supplied the value of\n      `initial_step_size` will be ignored, otherwise the tuple will be computed\n      by evaluating value_and_gradients_function.\n    value_at_zero: (Optional) The full return value of\n      value_and_gradients_function at `0.`, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If not supplied the tuple\n      will be computed by evaluating value_and_gradients_function.\n    converged: (Optional) In batching mode a tensor of shape [n], indicating\n      batch members which have already converged and no further search should\n      be performed. These batch members are also reported as converged in the\n      output, and both their `left` and `right` are set to the\n      `value_at_initial_step`.\n    threshold_use_approximate_wolfe_condition: Scalar positive `Tensor`\n      of real dtype. Corresponds to the parameter 'epsilon' in\n      [Hager and Zhang (2006)][2]. Used to estimate the\n      threshold at which the line search switches to approximate Wolfe\n      conditions.\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in\n      [Hager and Zhang (2006)][2].\n      If the secant**2 step does not shrink the bracketing interval by this\n      proportion, a bisection step is performed to reduce the interval width.\n    expansion_param: Scalar positive `Tensor` of real dtype. Must be greater\n      than `1.`. Used to expand the initial interval in case it does not bracket\n      a minimum. Corresponds to `rho` in [Hager and Zhang (2006)][2].\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    step_size_shrink_param: Positive scalar `Tensor` of real dtype. Bounded\n      above by `1`. If the supplied step size is too big (i.e. either the\n      objective value or the gradient at that point is infinite), this factor\n      is used to shrink the step size until it is finite.\n    max_iterations: Positive scalar `Tensor` of integral dtype or None. The\n      maximum number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'hager_zhang' is used.\n\n  Returns:\n    results: A namedtuple containing the following attributes.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the final bracketing interval. Values are\n        equal to those of `right` on batch members where converged is True.\n        Otherwise, it corresponds to the last interval computed.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the final bracketing interval. Values are\n        equal to those of `left` on batch members where converged is True.\n        Otherwise, it corresponds to the last interval computed.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'hager_zhang', [\n      initial_step_size, value_at_initial_step, value_at_zero, converged,\n      threshold_use_approximate_wolfe_condition, shrinkage_param,\n      expansion_param, sufficient_decrease_param, curvature_param]):\n    val_0, val_initial, f_lim, prepare_evals = _prepare_args(\n        value_and_gradients_function,\n        initial_step_size,\n        value_at_initial_step,\n        value_at_zero,\n        threshold_use_approximate_wolfe_condition)\n\n    valid_inputs = (hzl.is_finite(val_0) & (val_0.df < 0) &\n                    tf.math.is_finite(val_initial.x) & (val_initial.x > 0))\n\n    if converged is None:\n      init_converged = tf.zeros_like(valid_inputs)  # i.e. all false.\n    else:\n      init_converged = tf.convert_to_tensor(value=converged)\n\n    failed = ~init_converged & ~valid_inputs\n    active = ~init_converged & valid_inputs\n\n    # Note: _fix_step_size returns immediately if either all inputs are invalid\n    # or none of the active ones need fixing.\n    fix_step_evals, val_c, fix_failed = _fix_step_size(\n        value_and_gradients_function, val_initial, active,\n        step_size_shrink_param)\n\n    init_interval = HagerZhangLineSearchResult(\n        converged=init_converged,\n        failed=failed | fix_failed,\n        func_evals=prepare_evals + fix_step_evals,\n        iterations=tf.convert_to_tensor(value=0),\n        left=val_0,\n        right=hzl.val_where(init_converged, val_0, val_c))\n\n    def _apply_bracket_and_search():\n      \"\"\"Bracketing and searching to do for valid inputs.\"\"\"\n      return _bracket_and_search(\n          value_and_gradients_function, init_interval, f_lim, max_iterations,\n          shrinkage_param, expansion_param, sufficient_decrease_param,\n          curvature_param)\n\n    init_active = ~init_interval.failed & ~init_interval.converged\n    return prefer_static.cond(\n        tf.reduce_any(input_tensor=init_active),\n        _apply_bracket_and_search,\n        lambda: init_interval)", "language": "python", "code": "def hager_zhang(value_and_gradients_function,\n                initial_step_size=None,\n                value_at_initial_step=None,\n                value_at_zero=None,\n                converged=None,\n                threshold_use_approximate_wolfe_condition=1e-6,\n                shrinkage_param=0.66,\n                expansion_param=5.0,\n                sufficient_decrease_param=0.1,\n                curvature_param=0.9,\n                step_size_shrink_param=0.1,\n                max_iterations=50,\n                name=None):\n  \"\"\"The Hager Zhang line search algorithm.\n\n  Performs an inexact line search based on the algorithm of\n  [Hager and Zhang (2006)][2].\n  The univariate objective function `value_and_gradients_function` is typically\n  generated by projecting a multivariate objective function along a search\n  direction. Suppose the multivariate function to be minimized is\n  `g(x1,x2, .. xn)`. Let (d1, d2, ..., dn) be the direction along which we wish\n  to perform a line search. Then the projected univariate function to be used\n  for line search is\n\n  ```None\n    f(a) = g(x1 + d1 * a, x2 + d2 * a, ..., xn + dn * a)\n  ```\n\n  The directional derivative along (d1, d2, ..., dn) is needed for this\n  procedure. This also corresponds to the derivative of the projected function\n  `f(a)` with respect to `a`. Note that this derivative must be negative for\n  `a = 0` if the direction is a descent direction.\n\n  The usual stopping criteria for the line search is the satisfaction of the\n  (weak) Wolfe conditions. For details of the Wolfe conditions, see\n  ref. [3]. On a finite precision machine, the exact Wolfe conditions can\n  be difficult to satisfy when one is very close to the minimum and as argued\n  by [Hager and Zhang (2005)][1], one can only expect the minimum to be\n  determined within square root of machine precision. To improve the situation,\n  they propose to replace the Wolfe conditions with an approximate version\n  depending on the derivative of the function which is applied only when one\n  is very close to the minimum. The following algorithm implements this\n  enhanced scheme.\n\n  ### Usage:\n\n  Primary use of line search methods is as an internal component of a class of\n  optimization algorithms (called line search based methods as opposed to\n  trust region methods). Hence, the end user will typically not want to access\n  line search directly. In particular, inexact line search should not be\n  confused with a univariate minimization method. The stopping criteria of line\n  search is the satisfaction of Wolfe conditions and not the discovery of the\n  minimum of the function.\n\n  With this caveat in mind, the following example illustrates the standalone\n  usage of the line search.\n\n  ```python\n    # Define value and gradient namedtuple\n    ValueAndGradient = namedtuple('ValueAndGradient', ['x', 'f', 'df'])\n    # Define a quadratic target with minimum at 1.3.\n    def value_and_gradients_function(x):\n      return ValueAndGradient(x=x, f=(x - 1.3) ** 2, df=2 * (x-1.3))\n    # Set initial step size.\n    step_size = tf.constant(0.1)\n    ls_result = tfp.optimizer.linesearch.hager_zhang(\n        value_and_gradients_function, initial_step_size=step_size)\n    # Evaluate the results.\n    with tf.Session() as session:\n      results = session.run(ls_result)\n      # Ensure convergence.\n      assert results.converged\n      # If the line search converged, the left and the right ends of the\n      # bracketing interval are identical.\n      assert results.left.x == result.right.x\n      # Print the number of evaluations and the final step size.\n      print (\"Final Step Size: %f, Evaluations: %d\" % (results.left.x,\n                                                       results.func_evals))\n  ```\n\n  ### References:\n  [1]: William Hager, Hongchao Zhang. A new conjugate gradient method with\n    guaranteed descent and an efficient line search. SIAM J. Optim., Vol 16. 1,\n    pp. 170-172. 2005.\n    https://www.math.lsu.edu/~hozhang/papers/cg_descent.pdf\n\n  [2]: William Hager, Hongchao Zhang. Algorithm 851: CG_DESCENT, a conjugate\n    gradient method with guaranteed descent. ACM Transactions on Mathematical\n    Software, Vol 32., 1, pp. 113-137. 2006.\n    http://users.clas.ufl.edu/hager/papers/CG/cg_compare.pdf\n\n  [3]: Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series in\n    Operations Research. pp 33-36. 2006\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    initial_step_size: (Optional) Scalar positive `Tensor` of real dtype, or\n      a tensor of shape [n] in batching mode. The initial value (or values) to\n      try to bracket the minimum. Default is `1.` as a float32.\n      Note that this point need not necessarily bracket the minimum for the line\n      search to work correctly but the supplied value must be greater than 0.\n      A good initial value will make the search converge faster.\n    value_at_initial_step: (Optional) The full return value of evaluating\n      value_and_gradients_function at initial_step_size, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If supplied the value of\n      `initial_step_size` will be ignored, otherwise the tuple will be computed\n      by evaluating value_and_gradients_function.\n    value_at_zero: (Optional) The full return value of\n      value_and_gradients_function at `0.`, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If not supplied the tuple\n      will be computed by evaluating value_and_gradients_function.\n    converged: (Optional) In batching mode a tensor of shape [n], indicating\n      batch members which have already converged and no further search should\n      be performed. These batch members are also reported as converged in the\n      output, and both their `left` and `right` are set to the\n      `value_at_initial_step`.\n    threshold_use_approximate_wolfe_condition: Scalar positive `Tensor`\n      of real dtype. Corresponds to the parameter 'epsilon' in\n      [Hager and Zhang (2006)][2]. Used to estimate the\n      threshold at which the line search switches to approximate Wolfe\n      conditions.\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in\n      [Hager and Zhang (2006)][2].\n      If the secant**2 step does not shrink the bracketing interval by this\n      proportion, a bisection step is performed to reduce the interval width.\n    expansion_param: Scalar positive `Tensor` of real dtype. Must be greater\n      than `1.`. Used to expand the initial interval in case it does not bracket\n      a minimum. Corresponds to `rho` in [Hager and Zhang (2006)][2].\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    step_size_shrink_param: Positive scalar `Tensor` of real dtype. Bounded\n      above by `1`. If the supplied step size is too big (i.e. either the\n      objective value or the gradient at that point is infinite), this factor\n      is used to shrink the step size until it is finite.\n    max_iterations: Positive scalar `Tensor` of integral dtype or None. The\n      maximum number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'hager_zhang' is used.\n\n  Returns:\n    results: A namedtuple containing the following attributes.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the final bracketing interval. Values are\n        equal to those of `right` on batch members where converged is True.\n        Otherwise, it corresponds to the last interval computed.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the final bracketing interval. Values are\n        equal to those of `left` on batch members where converged is True.\n        Otherwise, it corresponds to the last interval computed.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'hager_zhang', [\n      initial_step_size, value_at_initial_step, value_at_zero, converged,\n      threshold_use_approximate_wolfe_condition, shrinkage_param,\n      expansion_param, sufficient_decrease_param, curvature_param]):\n    val_0, val_initial, f_lim, prepare_evals = _prepare_args(\n        value_and_gradients_function,\n        initial_step_size,\n        value_at_initial_step,\n        value_at_zero,\n        threshold_use_approximate_wolfe_condition)\n\n    valid_inputs = (hzl.is_finite(val_0) & (val_0.df < 0) &\n                    tf.math.is_finite(val_initial.x) & (val_initial.x > 0))\n\n    if converged is None:\n      init_converged = tf.zeros_like(valid_inputs)  # i.e. all false.\n    else:\n      init_converged = tf.convert_to_tensor(value=converged)\n\n    failed = ~init_converged & ~valid_inputs\n    active = ~init_converged & valid_inputs\n\n    # Note: _fix_step_size returns immediately if either all inputs are invalid\n    # or none of the active ones need fixing.\n    fix_step_evals, val_c, fix_failed = _fix_step_size(\n        value_and_gradients_function, val_initial, active,\n        step_size_shrink_param)\n\n    init_interval = HagerZhangLineSearchResult(\n        converged=init_converged,\n        failed=failed | fix_failed,\n        func_evals=prepare_evals + fix_step_evals,\n        iterations=tf.convert_to_tensor(value=0),\n        left=val_0,\n        right=hzl.val_where(init_converged, val_0, val_c))\n\n    def _apply_bracket_and_search():\n      \"\"\"Bracketing and searching to do for valid inputs.\"\"\"\n      return _bracket_and_search(\n          value_and_gradients_function, init_interval, f_lim, max_iterations,\n          shrinkage_param, expansion_param, sufficient_decrease_param,\n          curvature_param)\n\n    init_active = ~init_interval.failed & ~init_interval.converged\n    return prefer_static.cond(\n        tf.reduce_any(input_tensor=init_active),\n        _apply_bracket_and_search,\n        lambda: init_interval)", "code_tokens": ["def", "hager_zhang", "(", "value_and_gradients_function", ",", "initial_step_size", "=", "None", ",", "value_at_initial_step", "=", "None", ",", "value_at_zero", "=", "None", ",", "converged", "=", "None", ",", "threshold_use_approximate_wolfe_condition", "=", "1e-6", ",", "shrinkage_param", "=", "0.66", ",", "expansion_param", "=", "5.0", ",", "sufficient_decrease_param", "=", "0.1", ",", "curvature_param", "=", "0.9", ",", "step_size_shrink_param", "=", "0.1", ",", "max_iterations", "=", "50", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'hager_zhang'", ",", "[", "initial_step_size", ",", "value_at_initial_step", ",", "value_at_zero", ",", "converged", ",", "threshold_use_approximate_wolfe_condition", ",", "shrinkage_param", ",", "expansion_param", ",", "sufficient_decrease_param", ",", "curvature_param", "]", ")", ":", "val_0", ",", "val_initial", ",", "f_lim", ",", "prepare_evals", "=", "_prepare_args", "(", "value_and_gradients_function", ",", "initial_step_size", ",", "value_at_initial_step", ",", "value_at_zero", ",", "threshold_use_approximate_wolfe_condition", ")", "valid_inputs", "=", "(", "hzl", ".", "is_finite", "(", "val_0", ")", "&", "(", "val_0", ".", "df", "<", "0", ")", "&", "tf", ".", "math", ".", "is_finite", "(", "val_initial", ".", "x", ")", "&", "(", "val_initial", ".", "x", ">", "0", ")", ")", "if", "converged", "is", "None", ":", "init_converged", "=", "tf", ".", "zeros_like", "(", "valid_inputs", ")", "# i.e. all false.", "else", ":", "init_converged", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "converged", ")", "failed", "=", "~", "init_converged", "&", "~", "valid_inputs", "active", "=", "~", "init_converged", "&", "valid_inputs", "# Note: _fix_step_size returns immediately if either all inputs are invalid", "# or none of the active ones need fixing.", "fix_step_evals", ",", "val_c", ",", "fix_failed", "=", "_fix_step_size", "(", "value_and_gradients_function", ",", "val_initial", ",", "active", ",", "step_size_shrink_param", ")", "init_interval", "=", "HagerZhangLineSearchResult", "(", "converged", "=", "init_converged", ",", "failed", "=", "failed", "|", "fix_failed", ",", "func_evals", "=", "prepare_evals", "+", "fix_step_evals", ",", "iterations", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "0", ")", ",", "left", "=", "val_0", ",", "right", "=", "hzl", ".", "val_where", "(", "init_converged", ",", "val_0", ",", "val_c", ")", ")", "def", "_apply_bracket_and_search", "(", ")", ":", "\"\"\"Bracketing and searching to do for valid inputs.\"\"\"", "return", "_bracket_and_search", "(", "value_and_gradients_function", ",", "init_interval", ",", "f_lim", ",", "max_iterations", ",", "shrinkage_param", ",", "expansion_param", ",", "sufficient_decrease_param", ",", "curvature_param", ")", "init_active", "=", "~", "init_interval", ".", "failed", "&", "~", "init_interval", ".", "converged", "return", "prefer_static", ".", "cond", "(", "tf", ".", "reduce_any", "(", "input_tensor", "=", "init_active", ")", ",", "_apply_bracket_and_search", ",", "lambda", ":", "init_interval", ")"], "docstring": "The Hager Zhang line search algorithm.\n\n  Performs an inexact line search based on the algorithm of\n  [Hager and Zhang (2006)][2].\n  The univariate objective function `value_and_gradients_function` is typically\n  generated by projecting a multivariate objective function along a search\n  direction. Suppose the multivariate function to be minimized is\n  `g(x1,x2, .. xn)`. Let (d1, d2, ..., dn) be the direction along which we wish\n  to perform a line search. Then the projected univariate function to be used\n  for line search is\n\n  ```None\n    f(a) = g(x1 + d1 * a, x2 + d2 * a, ..., xn + dn * a)\n  ```\n\n  The directional derivative along (d1, d2, ..., dn) is needed for this\n  procedure. This also corresponds to the derivative of the projected function\n  `f(a)` with respect to `a`. Note that this derivative must be negative for\n  `a = 0` if the direction is a descent direction.\n\n  The usual stopping criteria for the line search is the satisfaction of the\n  (weak) Wolfe conditions. For details of the Wolfe conditions, see\n  ref. [3]. On a finite precision machine, the exact Wolfe conditions can\n  be difficult to satisfy when one is very close to the minimum and as argued\n  by [Hager and Zhang (2005)][1], one can only expect the minimum to be\n  determined within square root of machine precision. To improve the situation,\n  they propose to replace the Wolfe conditions with an approximate version\n  depending on the derivative of the function which is applied only when one\n  is very close to the minimum. The following algorithm implements this\n  enhanced scheme.\n\n  ### Usage:\n\n  Primary use of line search methods is as an internal component of a class of\n  optimization algorithms (called line search based methods as opposed to\n  trust region methods). Hence, the end user will typically not want to access\n  line search directly. In particular, inexact line search should not be\n  confused with a univariate minimization method. The stopping criteria of line\n  search is the satisfaction of Wolfe conditions and not the discovery of the\n  minimum of the function.\n\n  With this caveat in mind, the following example illustrates the standalone\n  usage of the line search.\n\n  ```python\n    # Define value and gradient namedtuple\n    ValueAndGradient = namedtuple('ValueAndGradient', ['x', 'f', 'df'])\n    # Define a quadratic target with minimum at 1.3.\n    def value_and_gradients_function(x):\n      return ValueAndGradient(x=x, f=(x - 1.3) ** 2, df=2 * (x-1.3))\n    # Set initial step size.\n    step_size = tf.constant(0.1)\n    ls_result = tfp.optimizer.linesearch.hager_zhang(\n        value_and_gradients_function, initial_step_size=step_size)\n    # Evaluate the results.\n    with tf.Session() as session:\n      results = session.run(ls_result)\n      # Ensure convergence.\n      assert results.converged\n      # If the line search converged, the left and the right ends of the\n      # bracketing interval are identical.\n      assert results.left.x == result.right.x\n      # Print the number of evaluations and the final step size.\n      print (\"Final Step Size: %f, Evaluations: %d\" % (results.left.x,\n                                                       results.func_evals))\n  ```\n\n  ### References:\n  [1]: William Hager, Hongchao Zhang. A new conjugate gradient method with\n    guaranteed descent and an efficient line search. SIAM J. Optim., Vol 16. 1,\n    pp. 170-172. 2005.\n    https://www.math.lsu.edu/~hozhang/papers/cg_descent.pdf\n\n  [2]: William Hager, Hongchao Zhang. Algorithm 851: CG_DESCENT, a conjugate\n    gradient method with guaranteed descent. ACM Transactions on Mathematical\n    Software, Vol 32., 1, pp. 113-137. 2006.\n    http://users.clas.ufl.edu/hager/papers/CG/cg_compare.pdf\n\n  [3]: Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series in\n    Operations Research. pp 33-36. 2006\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    initial_step_size: (Optional) Scalar positive `Tensor` of real dtype, or\n      a tensor of shape [n] in batching mode. The initial value (or values) to\n      try to bracket the minimum. Default is `1.` as a float32.\n      Note that this point need not necessarily bracket the minimum for the line\n      search to work correctly but the supplied value must be greater than 0.\n      A good initial value will make the search converge faster.\n    value_at_initial_step: (Optional) The full return value of evaluating\n      value_and_gradients_function at initial_step_size, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If supplied the value of\n      `initial_step_size` will be ignored, otherwise the tuple will be computed\n      by evaluating value_and_gradients_function.\n    value_at_zero: (Optional) The full return value of\n      value_and_gradients_function at `0.`, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If not supplied the tuple\n      will be computed by evaluating value_and_gradients_function.\n    converged: (Optional) In batching mode a tensor of shape [n], indicating\n      batch members which have already converged and no further search should\n      be performed. These batch members are also reported as converged in the\n      output, and both their `left` and `right` are set to the\n      `value_at_initial_step`.\n    threshold_use_approximate_wolfe_condition: Scalar positive `Tensor`\n      of real dtype. Corresponds to the parameter 'epsilon' in\n      [Hager and Zhang (2006)][2]. Used to estimate the\n      threshold at which the line search switches to approximate Wolfe\n      conditions.\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in\n      [Hager and Zhang (2006)][2].\n      If the secant**2 step does not shrink the bracketing interval by this\n      proportion, a bisection step is performed to reduce the interval width.\n    expansion_param: Scalar positive `Tensor` of real dtype. Must be greater\n      than `1.`. Used to expand the initial interval in case it does not bracket\n      a minimum. Corresponds to `rho` in [Hager and Zhang (2006)][2].\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    step_size_shrink_param: Positive scalar `Tensor` of real dtype. Bounded\n      above by `1`. If the supplied step size is too big (i.e. either the\n      objective value or the gradient at that point is infinite), this factor\n      is used to shrink the step size until it is finite.\n    max_iterations: Positive scalar `Tensor` of integral dtype or None. The\n      maximum number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'hager_zhang' is used.\n\n  Returns:\n    results: A namedtuple containing the following attributes.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the final bracketing interval. Values are\n        equal to those of `right` on batch members where converged is True.\n        Otherwise, it corresponds to the last interval computed.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the final bracketing interval. Values are\n        equal to those of `left` on batch members where converged is True.\n        Otherwise, it corresponds to the last interval computed.", "docstring_tokens": ["The", "Hager", "Zhang", "line", "search", "algorithm", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py#L69-L296", "partition": "test", "index": 671, "time": "2018-06-12 10:44:51"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", "func_name": "_line_search_after_bracketing", "original_string": "def _line_search_after_bracketing(\n    value_and_gradients_function,\n    search_interval,\n    val_0,\n    f_lim,\n    max_iterations,\n    sufficient_decrease_param,\n    curvature_param,\n    shrinkage_param):\n  \"\"\"The main loop of line search after the minimum has been bracketed.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    search_interval: Instance of `HagerZhangLineSearchResults` containing\n      the current line search interval.\n    val_0: A namedtuple as returned by value_and_gradients_function evaluated\n      at `0.`. The gradient must be negative (i.e. must be a descent direction).\n    f_lim: Scalar `Tensor` of float dtype.\n    max_iterations: Positive scalar `Tensor` of integral dtype. The maximum\n      number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in [Hager and Zhang (2006)][2].\n\n  Returns:\n    A namedtuple containing the following fields.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the updated bracketing interval.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the updated bracketing interval.\n  \"\"\"\n\n  def _loop_cond(curr_interval):\n    \"\"\"Loop condition.\"\"\"\n    active = ~(curr_interval.converged | curr_interval.failed)\n    return (curr_interval.iterations <\n            max_iterations) & tf.reduce_any(input_tensor=active)\n\n  def _loop_body(curr_interval):\n    \"\"\"The loop body.\"\"\"\n    secant2_raw_result = hzl.secant2(\n        value_and_gradients_function, val_0, curr_interval, f_lim,\n        sufficient_decrease_param, curvature_param)\n    secant2_result = HagerZhangLineSearchResult(\n        converged=secant2_raw_result.converged,\n        failed=secant2_raw_result.failed,\n        iterations=curr_interval.iterations + 1,\n        func_evals=secant2_raw_result.num_evals,\n        left=secant2_raw_result.left,\n        right=secant2_raw_result.right)\n\n    should_check_shrinkage = ~(secant2_result.converged | secant2_result.failed)\n\n    def _do_check_shrinkage():\n      \"\"\"Check if interval has shrinked enough.\"\"\"\n      old_width = curr_interval.right.x - curr_interval.left.x\n      new_width = secant2_result.right.x - secant2_result.left.x\n      sufficient_shrinkage = new_width < old_width * shrinkage_param\n      func_is_flat = (\n          _very_close(curr_interval.left.f, curr_interval.right.f) &\n          _very_close(secant2_result.left.f, secant2_result.right.f))\n\n      new_converged = (\n          should_check_shrinkage & sufficient_shrinkage & func_is_flat)\n      needs_inner_bisect = should_check_shrinkage & ~sufficient_shrinkage\n\n      inner_bisect_args = secant2_result._replace(\n          converged=secant2_result.converged | new_converged)\n\n      def _apply_inner_bisect():\n        return _line_search_inner_bisection(\n            value_and_gradients_function, inner_bisect_args,\n            needs_inner_bisect, f_lim)\n\n      return prefer_static.cond(\n          tf.reduce_any(input_tensor=needs_inner_bisect),\n          _apply_inner_bisect,\n          lambda: inner_bisect_args)\n\n    next_args = prefer_static.cond(\n        tf.reduce_any(input_tensor=should_check_shrinkage),\n        _do_check_shrinkage,\n        lambda: secant2_result)\n\n    interval_shrunk = (\n        ~next_args.failed & _very_close(next_args.left.x, next_args.right.x))\n    return [next_args._replace(converged=next_args.converged | interval_shrunk)]\n\n  return tf.while_loop(\n      cond=_loop_cond,\n      body=_loop_body,\n      loop_vars=[search_interval],\n      parallel_iterations=1)[0]", "language": "python", "code": "def _line_search_after_bracketing(\n    value_and_gradients_function,\n    search_interval,\n    val_0,\n    f_lim,\n    max_iterations,\n    sufficient_decrease_param,\n    curvature_param,\n    shrinkage_param):\n  \"\"\"The main loop of line search after the minimum has been bracketed.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    search_interval: Instance of `HagerZhangLineSearchResults` containing\n      the current line search interval.\n    val_0: A namedtuple as returned by value_and_gradients_function evaluated\n      at `0.`. The gradient must be negative (i.e. must be a descent direction).\n    f_lim: Scalar `Tensor` of float dtype.\n    max_iterations: Positive scalar `Tensor` of integral dtype. The maximum\n      number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in [Hager and Zhang (2006)][2].\n\n  Returns:\n    A namedtuple containing the following fields.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the updated bracketing interval.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the updated bracketing interval.\n  \"\"\"\n\n  def _loop_cond(curr_interval):\n    \"\"\"Loop condition.\"\"\"\n    active = ~(curr_interval.converged | curr_interval.failed)\n    return (curr_interval.iterations <\n            max_iterations) & tf.reduce_any(input_tensor=active)\n\n  def _loop_body(curr_interval):\n    \"\"\"The loop body.\"\"\"\n    secant2_raw_result = hzl.secant2(\n        value_and_gradients_function, val_0, curr_interval, f_lim,\n        sufficient_decrease_param, curvature_param)\n    secant2_result = HagerZhangLineSearchResult(\n        converged=secant2_raw_result.converged,\n        failed=secant2_raw_result.failed,\n        iterations=curr_interval.iterations + 1,\n        func_evals=secant2_raw_result.num_evals,\n        left=secant2_raw_result.left,\n        right=secant2_raw_result.right)\n\n    should_check_shrinkage = ~(secant2_result.converged | secant2_result.failed)\n\n    def _do_check_shrinkage():\n      \"\"\"Check if interval has shrinked enough.\"\"\"\n      old_width = curr_interval.right.x - curr_interval.left.x\n      new_width = secant2_result.right.x - secant2_result.left.x\n      sufficient_shrinkage = new_width < old_width * shrinkage_param\n      func_is_flat = (\n          _very_close(curr_interval.left.f, curr_interval.right.f) &\n          _very_close(secant2_result.left.f, secant2_result.right.f))\n\n      new_converged = (\n          should_check_shrinkage & sufficient_shrinkage & func_is_flat)\n      needs_inner_bisect = should_check_shrinkage & ~sufficient_shrinkage\n\n      inner_bisect_args = secant2_result._replace(\n          converged=secant2_result.converged | new_converged)\n\n      def _apply_inner_bisect():\n        return _line_search_inner_bisection(\n            value_and_gradients_function, inner_bisect_args,\n            needs_inner_bisect, f_lim)\n\n      return prefer_static.cond(\n          tf.reduce_any(input_tensor=needs_inner_bisect),\n          _apply_inner_bisect,\n          lambda: inner_bisect_args)\n\n    next_args = prefer_static.cond(\n        tf.reduce_any(input_tensor=should_check_shrinkage),\n        _do_check_shrinkage,\n        lambda: secant2_result)\n\n    interval_shrunk = (\n        ~next_args.failed & _very_close(next_args.left.x, next_args.right.x))\n    return [next_args._replace(converged=next_args.converged | interval_shrunk)]\n\n  return tf.while_loop(\n      cond=_loop_cond,\n      body=_loop_body,\n      loop_vars=[search_interval],\n      parallel_iterations=1)[0]", "code_tokens": ["def", "_line_search_after_bracketing", "(", "value_and_gradients_function", ",", "search_interval", ",", "val_0", ",", "f_lim", ",", "max_iterations", ",", "sufficient_decrease_param", ",", "curvature_param", ",", "shrinkage_param", ")", ":", "def", "_loop_cond", "(", "curr_interval", ")", ":", "\"\"\"Loop condition.\"\"\"", "active", "=", "~", "(", "curr_interval", ".", "converged", "|", "curr_interval", ".", "failed", ")", "return", "(", "curr_interval", ".", "iterations", "<", "max_iterations", ")", "&", "tf", ".", "reduce_any", "(", "input_tensor", "=", "active", ")", "def", "_loop_body", "(", "curr_interval", ")", ":", "\"\"\"The loop body.\"\"\"", "secant2_raw_result", "=", "hzl", ".", "secant2", "(", "value_and_gradients_function", ",", "val_0", ",", "curr_interval", ",", "f_lim", ",", "sufficient_decrease_param", ",", "curvature_param", ")", "secant2_result", "=", "HagerZhangLineSearchResult", "(", "converged", "=", "secant2_raw_result", ".", "converged", ",", "failed", "=", "secant2_raw_result", ".", "failed", ",", "iterations", "=", "curr_interval", ".", "iterations", "+", "1", ",", "func_evals", "=", "secant2_raw_result", ".", "num_evals", ",", "left", "=", "secant2_raw_result", ".", "left", ",", "right", "=", "secant2_raw_result", ".", "right", ")", "should_check_shrinkage", "=", "~", "(", "secant2_result", ".", "converged", "|", "secant2_result", ".", "failed", ")", "def", "_do_check_shrinkage", "(", ")", ":", "\"\"\"Check if interval has shrinked enough.\"\"\"", "old_width", "=", "curr_interval", ".", "right", ".", "x", "-", "curr_interval", ".", "left", ".", "x", "new_width", "=", "secant2_result", ".", "right", ".", "x", "-", "secant2_result", ".", "left", ".", "x", "sufficient_shrinkage", "=", "new_width", "<", "old_width", "*", "shrinkage_param", "func_is_flat", "=", "(", "_very_close", "(", "curr_interval", ".", "left", ".", "f", ",", "curr_interval", ".", "right", ".", "f", ")", "&", "_very_close", "(", "secant2_result", ".", "left", ".", "f", ",", "secant2_result", ".", "right", ".", "f", ")", ")", "new_converged", "=", "(", "should_check_shrinkage", "&", "sufficient_shrinkage", "&", "func_is_flat", ")", "needs_inner_bisect", "=", "should_check_shrinkage", "&", "~", "sufficient_shrinkage", "inner_bisect_args", "=", "secant2_result", ".", "_replace", "(", "converged", "=", "secant2_result", ".", "converged", "|", "new_converged", ")", "def", "_apply_inner_bisect", "(", ")", ":", "return", "_line_search_inner_bisection", "(", "value_and_gradients_function", ",", "inner_bisect_args", ",", "needs_inner_bisect", ",", "f_lim", ")", "return", "prefer_static", ".", "cond", "(", "tf", ".", "reduce_any", "(", "input_tensor", "=", "needs_inner_bisect", ")", ",", "_apply_inner_bisect", ",", "lambda", ":", "inner_bisect_args", ")", "next_args", "=", "prefer_static", ".", "cond", "(", "tf", ".", "reduce_any", "(", "input_tensor", "=", "should_check_shrinkage", ")", ",", "_do_check_shrinkage", ",", "lambda", ":", "secant2_result", ")", "interval_shrunk", "=", "(", "~", "next_args", ".", "failed", "&", "_very_close", "(", "next_args", ".", "left", ".", "x", ",", "next_args", ".", "right", ".", "x", ")", ")", "return", "[", "next_args", ".", "_replace", "(", "converged", "=", "next_args", ".", "converged", "|", "interval_shrunk", ")", "]", "return", "tf", ".", "while_loop", "(", "cond", "=", "_loop_cond", ",", "body", "=", "_loop_body", ",", "loop_vars", "=", "[", "search_interval", "]", ",", "parallel_iterations", "=", "1", ")", "[", "0", "]"], "docstring": "The main loop of line search after the minimum has been bracketed.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    search_interval: Instance of `HagerZhangLineSearchResults` containing\n      the current line search interval.\n    val_0: A namedtuple as returned by value_and_gradients_function evaluated\n      at `0.`. The gradient must be negative (i.e. must be a descent direction).\n    f_lim: Scalar `Tensor` of float dtype.\n    max_iterations: Positive scalar `Tensor` of integral dtype. The maximum\n      number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in [Hager and Zhang (2006)][2].\n\n  Returns:\n    A namedtuple containing the following fields.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the updated bracketing interval.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the updated bracketing interval.", "docstring_tokens": ["The", "main", "loop", "of", "line", "search", "after", "the", "minimum", "has", "been", "bracketed", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py#L420-L542", "partition": "test", "index": 674, "time": "2018-06-12 10:44:51"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", "func_name": "_prepare_args", "original_string": "def _prepare_args(value_and_gradients_function,\n                  initial_step_size,\n                  val_initial,\n                  val_0,\n                  approximate_wolfe_threshold):\n  \"\"\"Prepares the arguments for the line search initialization.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    initial_step_size: Scalar positive `Tensor` of real dtype, or a tensor of\n      shape [n] in batching mode. The initial value (or values) to try to\n      bracket the minimum. Default is `1.` as a float32.\n      Note that this point need not necessarily bracket the minimum for the line\n      search to work correctly but the supplied value must be greater than 0.\n      A good initial value will make the search converge faster.\n    val_initial: The full return value of evaluating\n      value_and_gradients_function at initial_step_size, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If not None the value of\n      `initial_step_size` will be ignored, otherwise the tuple will be computed\n      by evaluating value_and_gradients_function.\n    val_0: The full return value of value_and_gradients_function at `0.`, i.e.\n      a namedtuple with 'x', 'f', 'df', if already known by the caller. If None\n      the tuple will be computed by evaluating value_and_gradients_function.\n    approximate_wolfe_threshold: Scalar positive `Tensor` of\n      real dtype. Corresponds to the parameter 'epsilon' in\n      [Hager and Zhang (2006)][2]. Used to estimate the\n      threshold at which the line search switches to approximate Wolfe\n      conditions.\n\n  Returns:\n    left: A namedtuple, as returned by value_and_gradients_function,\n      containing the value and derivative of the function at `0.`.\n    val_initial: A namedtuple, as returned by value_and_gradients_function,\n      containing the value and derivative of the function at\n      `initial_step_size`.\n    f_lim: Real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    eval_count: Scalar int32 `Tensor`. The number of target function\n      evaluations made by this function.\n  \"\"\"\n  eval_count = 0\n  if val_initial is None:\n    if initial_step_size is not None:\n      initial_step_size = tf.convert_to_tensor(value=initial_step_size)\n    else:\n      initial_step_size = tf.convert_to_tensor(value=1.0, dtype=tf.float32)\n    val_initial = value_and_gradients_function(initial_step_size)\n    eval_count += 1\n\n  if val_0 is None:\n    x_0 = tf.zeros_like(val_initial.x)\n    val_0 = value_and_gradients_function(x_0)\n    eval_count += 1\n\n  f_lim = val_0.f + (approximate_wolfe_threshold * tf.abs(val_0.f))\n  return val_0, val_initial, f_lim, tf.convert_to_tensor(value=eval_count)", "language": "python", "code": "def _prepare_args(value_and_gradients_function,\n                  initial_step_size,\n                  val_initial,\n                  val_0,\n                  approximate_wolfe_threshold):\n  \"\"\"Prepares the arguments for the line search initialization.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    initial_step_size: Scalar positive `Tensor` of real dtype, or a tensor of\n      shape [n] in batching mode. The initial value (or values) to try to\n      bracket the minimum. Default is `1.` as a float32.\n      Note that this point need not necessarily bracket the minimum for the line\n      search to work correctly but the supplied value must be greater than 0.\n      A good initial value will make the search converge faster.\n    val_initial: The full return value of evaluating\n      value_and_gradients_function at initial_step_size, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If not None the value of\n      `initial_step_size` will be ignored, otherwise the tuple will be computed\n      by evaluating value_and_gradients_function.\n    val_0: The full return value of value_and_gradients_function at `0.`, i.e.\n      a namedtuple with 'x', 'f', 'df', if already known by the caller. If None\n      the tuple will be computed by evaluating value_and_gradients_function.\n    approximate_wolfe_threshold: Scalar positive `Tensor` of\n      real dtype. Corresponds to the parameter 'epsilon' in\n      [Hager and Zhang (2006)][2]. Used to estimate the\n      threshold at which the line search switches to approximate Wolfe\n      conditions.\n\n  Returns:\n    left: A namedtuple, as returned by value_and_gradients_function,\n      containing the value and derivative of the function at `0.`.\n    val_initial: A namedtuple, as returned by value_and_gradients_function,\n      containing the value and derivative of the function at\n      `initial_step_size`.\n    f_lim: Real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    eval_count: Scalar int32 `Tensor`. The number of target function\n      evaluations made by this function.\n  \"\"\"\n  eval_count = 0\n  if val_initial is None:\n    if initial_step_size is not None:\n      initial_step_size = tf.convert_to_tensor(value=initial_step_size)\n    else:\n      initial_step_size = tf.convert_to_tensor(value=1.0, dtype=tf.float32)\n    val_initial = value_and_gradients_function(initial_step_size)\n    eval_count += 1\n\n  if val_0 is None:\n    x_0 = tf.zeros_like(val_initial.x)\n    val_0 = value_and_gradients_function(x_0)\n    eval_count += 1\n\n  f_lim = val_0.f + (approximate_wolfe_threshold * tf.abs(val_0.f))\n  return val_0, val_initial, f_lim, tf.convert_to_tensor(value=eval_count)", "code_tokens": ["def", "_prepare_args", "(", "value_and_gradients_function", ",", "initial_step_size", ",", "val_initial", ",", "val_0", ",", "approximate_wolfe_threshold", ")", ":", "eval_count", "=", "0", "if", "val_initial", "is", "None", ":", "if", "initial_step_size", "is", "not", "None", ":", "initial_step_size", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "initial_step_size", ")", "else", ":", "initial_step_size", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "1.0", ",", "dtype", "=", "tf", ".", "float32", ")", "val_initial", "=", "value_and_gradients_function", "(", "initial_step_size", ")", "eval_count", "+=", "1", "if", "val_0", "is", "None", ":", "x_0", "=", "tf", ".", "zeros_like", "(", "val_initial", ".", "x", ")", "val_0", "=", "value_and_gradients_function", "(", "x_0", ")", "eval_count", "+=", "1", "f_lim", "=", "val_0", ".", "f", "+", "(", "approximate_wolfe_threshold", "*", "tf", ".", "abs", "(", "val_0", ".", "f", ")", ")", "return", "val_0", ",", "val_initial", ",", "f_lim", ",", "tf", ".", "convert_to_tensor", "(", "value", "=", "eval_count", ")"], "docstring": "Prepares the arguments for the line search initialization.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    initial_step_size: Scalar positive `Tensor` of real dtype, or a tensor of\n      shape [n] in batching mode. The initial value (or values) to try to\n      bracket the minimum. Default is `1.` as a float32.\n      Note that this point need not necessarily bracket the minimum for the line\n      search to work correctly but the supplied value must be greater than 0.\n      A good initial value will make the search converge faster.\n    val_initial: The full return value of evaluating\n      value_and_gradients_function at initial_step_size, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If not None the value of\n      `initial_step_size` will be ignored, otherwise the tuple will be computed\n      by evaluating value_and_gradients_function.\n    val_0: The full return value of value_and_gradients_function at `0.`, i.e.\n      a namedtuple with 'x', 'f', 'df', if already known by the caller. If None\n      the tuple will be computed by evaluating value_and_gradients_function.\n    approximate_wolfe_threshold: Scalar positive `Tensor` of\n      real dtype. Corresponds to the parameter 'epsilon' in\n      [Hager and Zhang (2006)][2]. Used to estimate the\n      threshold at which the line search switches to approximate Wolfe\n      conditions.\n\n  Returns:\n    left: A namedtuple, as returned by value_and_gradients_function,\n      containing the value and derivative of the function at `0.`.\n    val_initial: A namedtuple, as returned by value_and_gradients_function,\n      containing the value and derivative of the function at\n      `initial_step_size`.\n    f_lim: Real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    eval_count: Scalar int32 `Tensor`. The number of target function\n      evaluations made by this function.", "docstring_tokens": ["Prepares", "the", "arguments", "for", "the", "line", "search", "initialization", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py#L579-L650", "partition": "test", "index": 676, "time": "2018-06-12 10:44:51"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", "func_name": "_bracket_and_search", "original_string": "def _bracket_and_search(\n    value_and_gradients_function,\n    init_interval,\n    f_lim,\n    max_iterations,\n    shrinkage_param,\n    expansion_param,\n    sufficient_decrease_param,\n    curvature_param):\n  \"\"\"Brackets the minimum and performs a line search.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    init_interval: Instance of `HagerZhangLineSearchResults` containing\n      the initial line search interval. The gradient of init_interval.left must\n      be negative (i.e. must be a descent direction), while init_interval.right\n      must be positive and finite.\n    f_lim: Scalar `Tensor` of float dtype.\n    max_iterations: Positive scalar `Tensor` of integral dtype. The maximum\n      number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in [Hager and Zhang (2006)][2].\n    expansion_param: Scalar positive `Tensor` of real dtype. Must be greater\n      than `1.`. Used to expand the initial interval in case it does not bracket\n      a minimum. Corresponds to `rho` in [Hager and Zhang (2006)][2].\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n\n  Returns:\n    A namedtuple containing the following fields.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the updated bracketing interval.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the updated bracketing interval.\n  \"\"\"\n  bracket_result = hzl.bracket(value_and_gradients_function, init_interval,\n                               f_lim, max_iterations, expansion_param)\n\n  converged = init_interval.converged | _very_close(\n      bracket_result.left.x, bracket_result.right.x)\n\n  # We fail if we have not yet converged but already exhausted all iterations.\n  exhausted_iterations = ~converged & tf.greater_equal(\n      bracket_result.iteration, max_iterations)\n\n  line_search_args = HagerZhangLineSearchResult(\n      converged=converged,\n      failed=bracket_result.failed | exhausted_iterations,\n      iterations=bracket_result.iteration,\n      func_evals=bracket_result.num_evals,\n      left=bracket_result.left,\n      right=bracket_result.right)\n\n  return _line_search_after_bracketing(\n      value_and_gradients_function, line_search_args, init_interval.left,\n      f_lim, max_iterations, sufficient_decrease_param, curvature_param,\n      shrinkage_param)", "language": "python", "code": "def _bracket_and_search(\n    value_and_gradients_function,\n    init_interval,\n    f_lim,\n    max_iterations,\n    shrinkage_param,\n    expansion_param,\n    sufficient_decrease_param,\n    curvature_param):\n  \"\"\"Brackets the minimum and performs a line search.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    init_interval: Instance of `HagerZhangLineSearchResults` containing\n      the initial line search interval. The gradient of init_interval.left must\n      be negative (i.e. must be a descent direction), while init_interval.right\n      must be positive and finite.\n    f_lim: Scalar `Tensor` of float dtype.\n    max_iterations: Positive scalar `Tensor` of integral dtype. The maximum\n      number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in [Hager and Zhang (2006)][2].\n    expansion_param: Scalar positive `Tensor` of real dtype. Must be greater\n      than `1.`. Used to expand the initial interval in case it does not bracket\n      a minimum. Corresponds to `rho` in [Hager and Zhang (2006)][2].\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n\n  Returns:\n    A namedtuple containing the following fields.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the updated bracketing interval.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the updated bracketing interval.\n  \"\"\"\n  bracket_result = hzl.bracket(value_and_gradients_function, init_interval,\n                               f_lim, max_iterations, expansion_param)\n\n  converged = init_interval.converged | _very_close(\n      bracket_result.left.x, bracket_result.right.x)\n\n  # We fail if we have not yet converged but already exhausted all iterations.\n  exhausted_iterations = ~converged & tf.greater_equal(\n      bracket_result.iteration, max_iterations)\n\n  line_search_args = HagerZhangLineSearchResult(\n      converged=converged,\n      failed=bracket_result.failed | exhausted_iterations,\n      iterations=bracket_result.iteration,\n      func_evals=bracket_result.num_evals,\n      left=bracket_result.left,\n      right=bracket_result.right)\n\n  return _line_search_after_bracketing(\n      value_and_gradients_function, line_search_args, init_interval.left,\n      f_lim, max_iterations, sufficient_decrease_param, curvature_param,\n      shrinkage_param)", "code_tokens": ["def", "_bracket_and_search", "(", "value_and_gradients_function", ",", "init_interval", ",", "f_lim", ",", "max_iterations", ",", "shrinkage_param", ",", "expansion_param", ",", "sufficient_decrease_param", ",", "curvature_param", ")", ":", "bracket_result", "=", "hzl", ".", "bracket", "(", "value_and_gradients_function", ",", "init_interval", ",", "f_lim", ",", "max_iterations", ",", "expansion_param", ")", "converged", "=", "init_interval", ".", "converged", "|", "_very_close", "(", "bracket_result", ".", "left", ".", "x", ",", "bracket_result", ".", "right", ".", "x", ")", "# We fail if we have not yet converged but already exhausted all iterations.", "exhausted_iterations", "=", "~", "converged", "&", "tf", ".", "greater_equal", "(", "bracket_result", ".", "iteration", ",", "max_iterations", ")", "line_search_args", "=", "HagerZhangLineSearchResult", "(", "converged", "=", "converged", ",", "failed", "=", "bracket_result", ".", "failed", "|", "exhausted_iterations", ",", "iterations", "=", "bracket_result", ".", "iteration", ",", "func_evals", "=", "bracket_result", ".", "num_evals", ",", "left", "=", "bracket_result", ".", "left", ",", "right", "=", "bracket_result", ".", "right", ")", "return", "_line_search_after_bracketing", "(", "value_and_gradients_function", ",", "line_search_args", ",", "init_interval", ".", "left", ",", "f_lim", ",", "max_iterations", ",", "sufficient_decrease_param", ",", "curvature_param", ",", "shrinkage_param", ")"], "docstring": "Brackets the minimum and performs a line search.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    init_interval: Instance of `HagerZhangLineSearchResults` containing\n      the initial line search interval. The gradient of init_interval.left must\n      be negative (i.e. must be a descent direction), while init_interval.right\n      must be positive and finite.\n    f_lim: Scalar `Tensor` of float dtype.\n    max_iterations: Positive scalar `Tensor` of integral dtype. The maximum\n      number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in [Hager and Zhang (2006)][2].\n    expansion_param: Scalar positive `Tensor` of real dtype. Must be greater\n      than `1.`. Used to expand the initial interval in case it does not bracket\n      a minimum. Corresponds to `rho` in [Hager and Zhang (2006)][2].\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n\n  Returns:\n    A namedtuple containing the following fields.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the updated bracketing interval.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the updated bracketing interval.", "docstring_tokens": ["Brackets", "the", "minimum", "and", "performs", "a", "line", "search", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py#L332-L417", "partition": "test", "index": 673, "time": "2018-06-12 10:44:51"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/positive_semidefinite_kernels/internal/util.py", "func_name": "pad_shape_right_with_ones", "original_string": "def pad_shape_right_with_ones(x, ndims):\n  \"\"\"Maybe add `ndims` ones to `x.shape` on the right.\n\n  If `ndims` is zero, this is a no-op; otherwise, we will create and return a\n  new `Tensor` whose shape is that of `x` with `ndims` ones concatenated on the\n  right side. If the shape of `x` is known statically, the shape of the return\n  value will be as well.\n\n  Args:\n    x: The `Tensor` we'll return a reshaping of.\n    ndims: Python `integer` number of ones to pad onto `x.shape`.\n  Returns:\n    If `ndims` is zero, `x`; otherwise, a `Tensor` whose shape is that of `x`\n    with `ndims` ones concatenated on the right side. If possible, returns a\n    `Tensor` whose shape is known statically.\n  Raises:\n    ValueError: if `ndims` is not a Python `integer` greater than or equal to\n    zero.\n  \"\"\"\n  if not (isinstance(ndims, int) and ndims >= 0):\n    raise ValueError(\n        '`ndims` must be a Python `integer` greater than zero. Got: {}'\n        .format(ndims))\n  if ndims == 0:\n    return x\n  x = tf.convert_to_tensor(value=x)\n  original_shape = x.shape\n  new_shape = distribution_util.pad(\n      tf.shape(input=x), axis=0, back=True, value=1, count=ndims)\n  x = tf.reshape(x, new_shape)\n  x.set_shape(original_shape.concatenate([1]*ndims))\n  return x", "language": "python", "code": "def pad_shape_right_with_ones(x, ndims):\n  \"\"\"Maybe add `ndims` ones to `x.shape` on the right.\n\n  If `ndims` is zero, this is a no-op; otherwise, we will create and return a\n  new `Tensor` whose shape is that of `x` with `ndims` ones concatenated on the\n  right side. If the shape of `x` is known statically, the shape of the return\n  value will be as well.\n\n  Args:\n    x: The `Tensor` we'll return a reshaping of.\n    ndims: Python `integer` number of ones to pad onto `x.shape`.\n  Returns:\n    If `ndims` is zero, `x`; otherwise, a `Tensor` whose shape is that of `x`\n    with `ndims` ones concatenated on the right side. If possible, returns a\n    `Tensor` whose shape is known statically.\n  Raises:\n    ValueError: if `ndims` is not a Python `integer` greater than or equal to\n    zero.\n  \"\"\"\n  if not (isinstance(ndims, int) and ndims >= 0):\n    raise ValueError(\n        '`ndims` must be a Python `integer` greater than zero. Got: {}'\n        .format(ndims))\n  if ndims == 0:\n    return x\n  x = tf.convert_to_tensor(value=x)\n  original_shape = x.shape\n  new_shape = distribution_util.pad(\n      tf.shape(input=x), axis=0, back=True, value=1, count=ndims)\n  x = tf.reshape(x, new_shape)\n  x.set_shape(original_shape.concatenate([1]*ndims))\n  return x", "code_tokens": ["def", "pad_shape_right_with_ones", "(", "x", ",", "ndims", ")", ":", "if", "not", "(", "isinstance", "(", "ndims", ",", "int", ")", "and", "ndims", ">=", "0", ")", ":", "raise", "ValueError", "(", "'`ndims` must be a Python `integer` greater than zero. Got: {}'", ".", "format", "(", "ndims", ")", ")", "if", "ndims", "==", "0", ":", "return", "x", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ")", "original_shape", "=", "x", ".", "shape", "new_shape", "=", "distribution_util", ".", "pad", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "axis", "=", "0", ",", "back", "=", "True", ",", "value", "=", "1", ",", "count", "=", "ndims", ")", "x", "=", "tf", ".", "reshape", "(", "x", ",", "new_shape", ")", "x", ".", "set_shape", "(", "original_shape", ".", "concatenate", "(", "[", "1", "]", "*", "ndims", ")", ")", "return", "x"], "docstring": "Maybe add `ndims` ones to `x.shape` on the right.\n\n  If `ndims` is zero, this is a no-op; otherwise, we will create and return a\n  new `Tensor` whose shape is that of `x` with `ndims` ones concatenated on the\n  right side. If the shape of `x` is known statically, the shape of the return\n  value will be as well.\n\n  Args:\n    x: The `Tensor` we'll return a reshaping of.\n    ndims: Python `integer` number of ones to pad onto `x.shape`.\n  Returns:\n    If `ndims` is zero, `x`; otherwise, a `Tensor` whose shape is that of `x`\n    with `ndims` ones concatenated on the right side. If possible, returns a\n    `Tensor` whose shape is known statically.\n  Raises:\n    ValueError: if `ndims` is not a Python `integer` greater than or equal to\n    zero.", "docstring_tokens": ["Maybe", "add", "ndims", "ones", "to", "x", ".", "shape", "on", "the", "right", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/positive_semidefinite_kernels/internal/util.py#L34-L65", "partition": "test", "index": 1068, "time": "2018-06-12 12:52:48"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/batch_reshape.py", "func_name": "BatchReshape._call_reshape_input_output", "original_string": "def _call_reshape_input_output(self, fn, x, extra_kwargs=None):\n    \"\"\"Calls `fn`, appropriately reshaping its input `x` and output.\"\"\"\n    # Note: we take `extra_kwargs` as a dict rather than `**extra_kwargs`\n    # because it is possible the user provided extra kwargs would itself\n    # have `fn` and/or `x` as a key.\n    with tf.control_dependencies(self._runtime_assertions +\n                                 self._validate_sample_arg(x)):\n      sample_shape, static_sample_shape = self._sample_shape(x)\n      old_shape = tf.concat(\n          [\n              sample_shape,\n              self.distribution.batch_shape_tensor(),\n              self.event_shape_tensor(),\n          ],\n          axis=0)\n      x_reshape = tf.reshape(x, old_shape)\n      result = fn(x_reshape, **extra_kwargs) if extra_kwargs else fn(x_reshape)\n      new_shape = tf.concat(\n          [\n              sample_shape,\n              self._batch_shape_unexpanded,\n          ], axis=0)\n      result = tf.reshape(result, new_shape)\n      if (tensorshape_util.rank(static_sample_shape) is not None and\n          tensorshape_util.rank(self.batch_shape) is not None):\n        new_shape = tensorshape_util.concatenate(static_sample_shape,\n                                                 self.batch_shape)\n        tensorshape_util.set_shape(result, new_shape)\n      return result", "language": "python", "code": "def _call_reshape_input_output(self, fn, x, extra_kwargs=None):\n    \"\"\"Calls `fn`, appropriately reshaping its input `x` and output.\"\"\"\n    # Note: we take `extra_kwargs` as a dict rather than `**extra_kwargs`\n    # because it is possible the user provided extra kwargs would itself\n    # have `fn` and/or `x` as a key.\n    with tf.control_dependencies(self._runtime_assertions +\n                                 self._validate_sample_arg(x)):\n      sample_shape, static_sample_shape = self._sample_shape(x)\n      old_shape = tf.concat(\n          [\n              sample_shape,\n              self.distribution.batch_shape_tensor(),\n              self.event_shape_tensor(),\n          ],\n          axis=0)\n      x_reshape = tf.reshape(x, old_shape)\n      result = fn(x_reshape, **extra_kwargs) if extra_kwargs else fn(x_reshape)\n      new_shape = tf.concat(\n          [\n              sample_shape,\n              self._batch_shape_unexpanded,\n          ], axis=0)\n      result = tf.reshape(result, new_shape)\n      if (tensorshape_util.rank(static_sample_shape) is not None and\n          tensorshape_util.rank(self.batch_shape) is not None):\n        new_shape = tensorshape_util.concatenate(static_sample_shape,\n                                                 self.batch_shape)\n        tensorshape_util.set_shape(result, new_shape)\n      return result", "code_tokens": ["def", "_call_reshape_input_output", "(", "self", ",", "fn", ",", "x", ",", "extra_kwargs", "=", "None", ")", ":", "# Note: we take `extra_kwargs` as a dict rather than `**extra_kwargs`", "# because it is possible the user provided extra kwargs would itself", "# have `fn` and/or `x` as a key.", "with", "tf", ".", "control_dependencies", "(", "self", ".", "_runtime_assertions", "+", "self", ".", "_validate_sample_arg", "(", "x", ")", ")", ":", "sample_shape", ",", "static_sample_shape", "=", "self", ".", "_sample_shape", "(", "x", ")", "old_shape", "=", "tf", ".", "concat", "(", "[", "sample_shape", ",", "self", ".", "distribution", ".", "batch_shape_tensor", "(", ")", ",", "self", ".", "event_shape_tensor", "(", ")", ",", "]", ",", "axis", "=", "0", ")", "x_reshape", "=", "tf", ".", "reshape", "(", "x", ",", "old_shape", ")", "result", "=", "fn", "(", "x_reshape", ",", "*", "*", "extra_kwargs", ")", "if", "extra_kwargs", "else", "fn", "(", "x_reshape", ")", "new_shape", "=", "tf", ".", "concat", "(", "[", "sample_shape", ",", "self", ".", "_batch_shape_unexpanded", ",", "]", ",", "axis", "=", "0", ")", "result", "=", "tf", ".", "reshape", "(", "result", ",", "new_shape", ")", "if", "(", "tensorshape_util", ".", "rank", "(", "static_sample_shape", ")", "is", "not", "None", "and", "tensorshape_util", ".", "rank", "(", "self", ".", "batch_shape", ")", "is", "not", "None", ")", ":", "new_shape", "=", "tensorshape_util", ".", "concatenate", "(", "static_sample_shape", ",", "self", ".", "batch_shape", ")", "tensorshape_util", ".", "set_shape", "(", "result", ",", "new_shape", ")", "return", "result"], "docstring": "Calls `fn`, appropriately reshaping its input `x` and output.", "docstring_tokens": ["Calls", "fn", "appropriately", "reshaping", "its", "input", "x", "and", "output", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/batch_reshape.py#L234-L262", "partition": "test", "index": 834, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/batch_reshape.py", "func_name": "BatchReshape._sample_shape", "original_string": "def _sample_shape(self, x):\n    \"\"\"Computes graph and static `sample_shape`.\"\"\"\n    x_ndims = (\n        tf.rank(x) if tensorshape_util.rank(x.shape) is None else\n        tensorshape_util.rank(x.shape))\n    event_ndims = (\n        tf.size(input=self.event_shape_tensor())\n        if tensorshape_util.rank(self.event_shape) is None else\n        tensorshape_util.rank(self.event_shape))\n    batch_ndims = (\n        tf.size(input=self._batch_shape_unexpanded)\n        if tensorshape_util.rank(self.batch_shape) is None else\n        tensorshape_util.rank(self.batch_shape))\n    sample_ndims = x_ndims - batch_ndims - event_ndims\n    if isinstance(sample_ndims, int):\n      static_sample_shape = x.shape[:sample_ndims]\n    else:\n      static_sample_shape = tf.TensorShape(None)\n    if tensorshape_util.is_fully_defined(static_sample_shape):\n      sample_shape = np.int32(static_sample_shape)\n    else:\n      sample_shape = tf.shape(input=x)[:sample_ndims]\n    return sample_shape, static_sample_shape", "language": "python", "code": "def _sample_shape(self, x):\n    \"\"\"Computes graph and static `sample_shape`.\"\"\"\n    x_ndims = (\n        tf.rank(x) if tensorshape_util.rank(x.shape) is None else\n        tensorshape_util.rank(x.shape))\n    event_ndims = (\n        tf.size(input=self.event_shape_tensor())\n        if tensorshape_util.rank(self.event_shape) is None else\n        tensorshape_util.rank(self.event_shape))\n    batch_ndims = (\n        tf.size(input=self._batch_shape_unexpanded)\n        if tensorshape_util.rank(self.batch_shape) is None else\n        tensorshape_util.rank(self.batch_shape))\n    sample_ndims = x_ndims - batch_ndims - event_ndims\n    if isinstance(sample_ndims, int):\n      static_sample_shape = x.shape[:sample_ndims]\n    else:\n      static_sample_shape = tf.TensorShape(None)\n    if tensorshape_util.is_fully_defined(static_sample_shape):\n      sample_shape = np.int32(static_sample_shape)\n    else:\n      sample_shape = tf.shape(input=x)[:sample_ndims]\n    return sample_shape, static_sample_shape", "code_tokens": ["def", "_sample_shape", "(", "self", ",", "x", ")", ":", "x_ndims", "=", "(", "tf", ".", "rank", "(", "x", ")", "if", "tensorshape_util", ".", "rank", "(", "x", ".", "shape", ")", "is", "None", "else", "tensorshape_util", ".", "rank", "(", "x", ".", "shape", ")", ")", "event_ndims", "=", "(", "tf", ".", "size", "(", "input", "=", "self", ".", "event_shape_tensor", "(", ")", ")", "if", "tensorshape_util", ".", "rank", "(", "self", ".", "event_shape", ")", "is", "None", "else", "tensorshape_util", ".", "rank", "(", "self", ".", "event_shape", ")", ")", "batch_ndims", "=", "(", "tf", ".", "size", "(", "input", "=", "self", ".", "_batch_shape_unexpanded", ")", "if", "tensorshape_util", ".", "rank", "(", "self", ".", "batch_shape", ")", "is", "None", "else", "tensorshape_util", ".", "rank", "(", "self", ".", "batch_shape", ")", ")", "sample_ndims", "=", "x_ndims", "-", "batch_ndims", "-", "event_ndims", "if", "isinstance", "(", "sample_ndims", ",", "int", ")", ":", "static_sample_shape", "=", "x", ".", "shape", "[", ":", "sample_ndims", "]", "else", ":", "static_sample_shape", "=", "tf", ".", "TensorShape", "(", "None", ")", "if", "tensorshape_util", ".", "is_fully_defined", "(", "static_sample_shape", ")", ":", "sample_shape", "=", "np", ".", "int32", "(", "static_sample_shape", ")", "else", ":", "sample_shape", "=", "tf", ".", "shape", "(", "input", "=", "x", ")", "[", ":", "sample_ndims", "]", "return", "sample_shape", ",", "static_sample_shape"], "docstring": "Computes graph and static `sample_shape`.", "docstring_tokens": ["Computes", "graph", "and", "static", "sample_shape", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/batch_reshape.py#L210-L232", "partition": "test", "index": 833, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/vector_diffeomixture.py", "func_name": "interpolate_loc", "original_string": "def interpolate_loc(grid, loc):\n  \"\"\"Helper which interpolates between two locs.\"\"\"\n  if len(loc) != 2:\n    raise NotImplementedError(\"Currently only bimixtures are supported; \"\n                              \"len(scale)={} is not 2.\".format(len(loc)))\n  deg = tf.compat.dimension_value(\n      tensorshape_util.with_rank_at_least(grid.shape, 1)[-1])\n  if deg is None:\n    raise ValueError(\"Num quadrature grid points must be known prior \"\n                     \"to graph execution.\")\n  with tf.name_scope(\"interpolate_loc\"):\n    if loc is None or loc[0] is None and loc[1] is None:\n      return [None]*deg\n    # shape: [B, 1, k, deg]\n    w = grid[..., tf.newaxis, :, :]\n    loc = [\n        x[..., tf.newaxis]  # shape: [B, e, 1]\n        if x is not None else None for x in loc\n    ]\n    if loc[0] is None:\n      x = w[..., 1, :] * loc[1]                        # shape: [B, e, deg]\n    elif loc[1] is None:\n      x = w[..., 0, :] * loc[0]                        # shape: [B, e, deg]\n    else:\n      delta = loc[0] - loc[1]\n      x = w[..., 0, :] * delta + loc[1]                # shape: [B, e, deg]\n    return [x[..., k] for k in range(deg)]", "language": "python", "code": "def interpolate_loc(grid, loc):\n  \"\"\"Helper which interpolates between two locs.\"\"\"\n  if len(loc) != 2:\n    raise NotImplementedError(\"Currently only bimixtures are supported; \"\n                              \"len(scale)={} is not 2.\".format(len(loc)))\n  deg = tf.compat.dimension_value(\n      tensorshape_util.with_rank_at_least(grid.shape, 1)[-1])\n  if deg is None:\n    raise ValueError(\"Num quadrature grid points must be known prior \"\n                     \"to graph execution.\")\n  with tf.name_scope(\"interpolate_loc\"):\n    if loc is None or loc[0] is None and loc[1] is None:\n      return [None]*deg\n    # shape: [B, 1, k, deg]\n    w = grid[..., tf.newaxis, :, :]\n    loc = [\n        x[..., tf.newaxis]  # shape: [B, e, 1]\n        if x is not None else None for x in loc\n    ]\n    if loc[0] is None:\n      x = w[..., 1, :] * loc[1]                        # shape: [B, e, deg]\n    elif loc[1] is None:\n      x = w[..., 0, :] * loc[0]                        # shape: [B, e, deg]\n    else:\n      delta = loc[0] - loc[1]\n      x = w[..., 0, :] * delta + loc[1]                # shape: [B, e, deg]\n    return [x[..., k] for k in range(deg)]", "code_tokens": ["def", "interpolate_loc", "(", "grid", ",", "loc", ")", ":", "if", "len", "(", "loc", ")", "!=", "2", ":", "raise", "NotImplementedError", "(", "\"Currently only bimixtures are supported; \"", "\"len(scale)={} is not 2.\"", ".", "format", "(", "len", "(", "loc", ")", ")", ")", "deg", "=", "tf", ".", "compat", ".", "dimension_value", "(", "tensorshape_util", ".", "with_rank_at_least", "(", "grid", ".", "shape", ",", "1", ")", "[", "-", "1", "]", ")", "if", "deg", "is", "None", ":", "raise", "ValueError", "(", "\"Num quadrature grid points must be known prior \"", "\"to graph execution.\"", ")", "with", "tf", ".", "name_scope", "(", "\"interpolate_loc\"", ")", ":", "if", "loc", "is", "None", "or", "loc", "[", "0", "]", "is", "None", "and", "loc", "[", "1", "]", "is", "None", ":", "return", "[", "None", "]", "*", "deg", "# shape: [B, 1, k, deg]", "w", "=", "grid", "[", "...", ",", "tf", ".", "newaxis", ",", ":", ",", ":", "]", "loc", "=", "[", "x", "[", "...", ",", "tf", ".", "newaxis", "]", "# shape: [B, e, 1]", "if", "x", "is", "not", "None", "else", "None", "for", "x", "in", "loc", "]", "if", "loc", "[", "0", "]", "is", "None", ":", "x", "=", "w", "[", "...", ",", "1", ",", ":", "]", "*", "loc", "[", "1", "]", "# shape: [B, e, deg]", "elif", "loc", "[", "1", "]", "is", "None", ":", "x", "=", "w", "[", "...", ",", "0", ",", ":", "]", "*", "loc", "[", "0", "]", "# shape: [B, e, deg]", "else", ":", "delta", "=", "loc", "[", "0", "]", "-", "loc", "[", "1", "]", "x", "=", "w", "[", "...", ",", "0", ",", ":", "]", "*", "delta", "+", "loc", "[", "1", "]", "# shape: [B, e, deg]", "return", "[", "x", "[", "...", ",", "k", "]", "for", "k", "in", "range", "(", "deg", ")", "]"], "docstring": "Helper which interpolates between two locs.", "docstring_tokens": ["Helper", "which", "interpolates", "between", "two", "locs", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/vector_diffeomixture.py#L853-L879", "partition": "test", "index": 682, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/vector_diffeomixture.py", "func_name": "determine_batch_event_shapes", "original_string": "def determine_batch_event_shapes(grid, endpoint_affine):\n  \"\"\"Helper to infer batch_shape and event_shape.\"\"\"\n  with tf.name_scope(\"determine_batch_event_shapes\"):\n    # grid  # shape: [B, k, q]\n    # endpoint_affine     # len=k, shape: [B, d, d]\n    batch_shape = grid.shape[:-2]\n    batch_shape_tensor = tf.shape(input=grid)[:-2]\n    event_shape = None\n    event_shape_tensor = None\n\n    def _set_event_shape(shape, shape_tensor):\n      if event_shape is None:\n        return shape, shape_tensor\n      return (tf.broadcast_static_shape(event_shape, shape),\n              tf.broadcast_dynamic_shape(event_shape_tensor, shape_tensor))\n\n    for aff in endpoint_affine:\n      if aff.shift is not None:\n        batch_shape = tf.broadcast_static_shape(batch_shape,\n                                                aff.shift.shape[:-1])\n        batch_shape_tensor = tf.broadcast_dynamic_shape(\n            batch_shape_tensor,\n            tf.shape(input=aff.shift)[:-1])\n        event_shape, event_shape_tensor = _set_event_shape(\n            aff.shift.shape[-1:],\n            tf.shape(input=aff.shift)[-1:])\n\n      if aff.scale is not None:\n        batch_shape = tf.broadcast_static_shape(batch_shape,\n                                                aff.scale.batch_shape)\n        batch_shape_tensor = tf.broadcast_dynamic_shape(\n            batch_shape_tensor, aff.scale.batch_shape_tensor())\n        event_shape, event_shape_tensor = _set_event_shape(\n            tf.TensorShape([aff.scale.range_dimension]),\n            aff.scale.range_dimension_tensor()[tf.newaxis])\n\n    return batch_shape, batch_shape_tensor, event_shape, event_shape_tensor", "language": "python", "code": "def determine_batch_event_shapes(grid, endpoint_affine):\n  \"\"\"Helper to infer batch_shape and event_shape.\"\"\"\n  with tf.name_scope(\"determine_batch_event_shapes\"):\n    # grid  # shape: [B, k, q]\n    # endpoint_affine     # len=k, shape: [B, d, d]\n    batch_shape = grid.shape[:-2]\n    batch_shape_tensor = tf.shape(input=grid)[:-2]\n    event_shape = None\n    event_shape_tensor = None\n\n    def _set_event_shape(shape, shape_tensor):\n      if event_shape is None:\n        return shape, shape_tensor\n      return (tf.broadcast_static_shape(event_shape, shape),\n              tf.broadcast_dynamic_shape(event_shape_tensor, shape_tensor))\n\n    for aff in endpoint_affine:\n      if aff.shift is not None:\n        batch_shape = tf.broadcast_static_shape(batch_shape,\n                                                aff.shift.shape[:-1])\n        batch_shape_tensor = tf.broadcast_dynamic_shape(\n            batch_shape_tensor,\n            tf.shape(input=aff.shift)[:-1])\n        event_shape, event_shape_tensor = _set_event_shape(\n            aff.shift.shape[-1:],\n            tf.shape(input=aff.shift)[-1:])\n\n      if aff.scale is not None:\n        batch_shape = tf.broadcast_static_shape(batch_shape,\n                                                aff.scale.batch_shape)\n        batch_shape_tensor = tf.broadcast_dynamic_shape(\n            batch_shape_tensor, aff.scale.batch_shape_tensor())\n        event_shape, event_shape_tensor = _set_event_shape(\n            tf.TensorShape([aff.scale.range_dimension]),\n            aff.scale.range_dimension_tensor()[tf.newaxis])\n\n    return batch_shape, batch_shape_tensor, event_shape, event_shape_tensor", "code_tokens": ["def", "determine_batch_event_shapes", "(", "grid", ",", "endpoint_affine", ")", ":", "with", "tf", ".", "name_scope", "(", "\"determine_batch_event_shapes\"", ")", ":", "# grid  # shape: [B, k, q]", "# endpoint_affine     # len=k, shape: [B, d, d]", "batch_shape", "=", "grid", ".", "shape", "[", ":", "-", "2", "]", "batch_shape_tensor", "=", "tf", ".", "shape", "(", "input", "=", "grid", ")", "[", ":", "-", "2", "]", "event_shape", "=", "None", "event_shape_tensor", "=", "None", "def", "_set_event_shape", "(", "shape", ",", "shape_tensor", ")", ":", "if", "event_shape", "is", "None", ":", "return", "shape", ",", "shape_tensor", "return", "(", "tf", ".", "broadcast_static_shape", "(", "event_shape", ",", "shape", ")", ",", "tf", ".", "broadcast_dynamic_shape", "(", "event_shape_tensor", ",", "shape_tensor", ")", ")", "for", "aff", "in", "endpoint_affine", ":", "if", "aff", ".", "shift", "is", "not", "None", ":", "batch_shape", "=", "tf", ".", "broadcast_static_shape", "(", "batch_shape", ",", "aff", ".", "shift", ".", "shape", "[", ":", "-", "1", "]", ")", "batch_shape_tensor", "=", "tf", ".", "broadcast_dynamic_shape", "(", "batch_shape_tensor", ",", "tf", ".", "shape", "(", "input", "=", "aff", ".", "shift", ")", "[", ":", "-", "1", "]", ")", "event_shape", ",", "event_shape_tensor", "=", "_set_event_shape", "(", "aff", ".", "shift", ".", "shape", "[", "-", "1", ":", "]", ",", "tf", ".", "shape", "(", "input", "=", "aff", ".", "shift", ")", "[", "-", "1", ":", "]", ")", "if", "aff", ".", "scale", "is", "not", "None", ":", "batch_shape", "=", "tf", ".", "broadcast_static_shape", "(", "batch_shape", ",", "aff", ".", "scale", ".", "batch_shape", ")", "batch_shape_tensor", "=", "tf", ".", "broadcast_dynamic_shape", "(", "batch_shape_tensor", ",", "aff", ".", "scale", ".", "batch_shape_tensor", "(", ")", ")", "event_shape", ",", "event_shape_tensor", "=", "_set_event_shape", "(", "tf", ".", "TensorShape", "(", "[", "aff", ".", "scale", ".", "range_dimension", "]", ")", ",", "aff", ".", "scale", ".", "range_dimension_tensor", "(", ")", "[", "tf", ".", "newaxis", "]", ")", "return", "batch_shape", ",", "batch_shape_tensor", ",", "event_shape", ",", "event_shape_tensor"], "docstring": "Helper to infer batch_shape and event_shape.", "docstring_tokens": ["Helper", "to", "infer", "batch_shape", "and", "event_shape", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/vector_diffeomixture.py#L814-L850", "partition": "test", "index": 681, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/vector_diffeomixture.py", "func_name": "maybe_check_quadrature_param", "original_string": "def maybe_check_quadrature_param(param, name, validate_args):\n  \"\"\"Helper which checks validity of `loc` and `scale` init args.\"\"\"\n  with tf.name_scope(\"check_\" + name):\n    assertions = []\n    if tensorshape_util.rank(param.shape) is not None:\n      if tensorshape_util.rank(param.shape) == 0:\n        raise ValueError(\"Mixing params must be a (batch of) vector; \"\n                         \"{}.rank={} is not at least one.\".format(\n                             name, tensorshape_util.rank(param.shape)))\n    elif validate_args:\n      assertions.append(\n          assert_util.assert_rank_at_least(\n              param,\n              1,\n              message=(\"Mixing params must be a (batch of) vector; \"\n                       \"{}.rank is not at least one.\".format(name))))\n\n    # TODO(jvdillon): Remove once we support k-mixtures.\n    if tensorshape_util.with_rank_at_least(param.shape, 1)[-1] is not None:\n      if tf.compat.dimension_value(param.shape[-1]) != 1:\n        raise NotImplementedError(\"Currently only bimixtures are supported; \"\n                                  \"{}.shape[-1]={} is not 1.\".format(\n                                      name,\n                                      tf.compat.dimension_value(\n                                          param.shape[-1])))\n    elif validate_args:\n      assertions.append(\n          assert_util.assert_equal(\n              tf.shape(input=param)[-1],\n              1,\n              message=(\"Currently only bimixtures are supported; \"\n                       \"{}.shape[-1] is not 1.\".format(name))))\n\n    if assertions:\n      return distribution_util.with_dependencies(assertions, param)\n    return param", "language": "python", "code": "def maybe_check_quadrature_param(param, name, validate_args):\n  \"\"\"Helper which checks validity of `loc` and `scale` init args.\"\"\"\n  with tf.name_scope(\"check_\" + name):\n    assertions = []\n    if tensorshape_util.rank(param.shape) is not None:\n      if tensorshape_util.rank(param.shape) == 0:\n        raise ValueError(\"Mixing params must be a (batch of) vector; \"\n                         \"{}.rank={} is not at least one.\".format(\n                             name, tensorshape_util.rank(param.shape)))\n    elif validate_args:\n      assertions.append(\n          assert_util.assert_rank_at_least(\n              param,\n              1,\n              message=(\"Mixing params must be a (batch of) vector; \"\n                       \"{}.rank is not at least one.\".format(name))))\n\n    # TODO(jvdillon): Remove once we support k-mixtures.\n    if tensorshape_util.with_rank_at_least(param.shape, 1)[-1] is not None:\n      if tf.compat.dimension_value(param.shape[-1]) != 1:\n        raise NotImplementedError(\"Currently only bimixtures are supported; \"\n                                  \"{}.shape[-1]={} is not 1.\".format(\n                                      name,\n                                      tf.compat.dimension_value(\n                                          param.shape[-1])))\n    elif validate_args:\n      assertions.append(\n          assert_util.assert_equal(\n              tf.shape(input=param)[-1],\n              1,\n              message=(\"Currently only bimixtures are supported; \"\n                       \"{}.shape[-1] is not 1.\".format(name))))\n\n    if assertions:\n      return distribution_util.with_dependencies(assertions, param)\n    return param", "code_tokens": ["def", "maybe_check_quadrature_param", "(", "param", ",", "name", ",", "validate_args", ")", ":", "with", "tf", ".", "name_scope", "(", "\"check_\"", "+", "name", ")", ":", "assertions", "=", "[", "]", "if", "tensorshape_util", ".", "rank", "(", "param", ".", "shape", ")", "is", "not", "None", ":", "if", "tensorshape_util", ".", "rank", "(", "param", ".", "shape", ")", "==", "0", ":", "raise", "ValueError", "(", "\"Mixing params must be a (batch of) vector; \"", "\"{}.rank={} is not at least one.\"", ".", "format", "(", "name", ",", "tensorshape_util", ".", "rank", "(", "param", ".", "shape", ")", ")", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "assert_util", ".", "assert_rank_at_least", "(", "param", ",", "1", ",", "message", "=", "(", "\"Mixing params must be a (batch of) vector; \"", "\"{}.rank is not at least one.\"", ".", "format", "(", "name", ")", ")", ")", ")", "# TODO(jvdillon): Remove once we support k-mixtures.", "if", "tensorshape_util", ".", "with_rank_at_least", "(", "param", ".", "shape", ",", "1", ")", "[", "-", "1", "]", "is", "not", "None", ":", "if", "tf", ".", "compat", ".", "dimension_value", "(", "param", ".", "shape", "[", "-", "1", "]", ")", "!=", "1", ":", "raise", "NotImplementedError", "(", "\"Currently only bimixtures are supported; \"", "\"{}.shape[-1]={} is not 1.\"", ".", "format", "(", "name", ",", "tf", ".", "compat", ".", "dimension_value", "(", "param", ".", "shape", "[", "-", "1", "]", ")", ")", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "assert_util", ".", "assert_equal", "(", "tf", ".", "shape", "(", "input", "=", "param", ")", "[", "-", "1", "]", ",", "1", ",", "message", "=", "(", "\"Currently only bimixtures are supported; \"", "\"{}.shape[-1] is not 1.\"", ".", "format", "(", "name", ")", ")", ")", ")", "if", "assertions", ":", "return", "distribution_util", ".", "with_dependencies", "(", "assertions", ",", "param", ")", "return", "param"], "docstring": "Helper which checks validity of `loc` and `scale` init args.", "docstring_tokens": ["Helper", "which", "checks", "validity", "of", "loc", "and", "scale", "init", "args", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/vector_diffeomixture.py#L776-L811", "partition": "test", "index": 680, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/kumaraswamy.py", "func_name": "_harmonic_number", "original_string": "def _harmonic_number(x):\n  \"\"\"Compute the harmonic number from its analytic continuation.\n\n  Derivation from [here](\n  https://en.wikipedia.org/wiki/Digamma_function#Relation_to_harmonic_numbers)\n  and [Euler's constant](\n  https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant).\n\n  Args:\n    x: input float.\n\n  Returns:\n    z: The analytic continuation of the harmonic number for the input.\n  \"\"\"\n  one = tf.ones([], dtype=x.dtype)\n  return tf.math.digamma(x + one) - tf.math.digamma(one)", "language": "python", "code": "def _harmonic_number(x):\n  \"\"\"Compute the harmonic number from its analytic continuation.\n\n  Derivation from [here](\n  https://en.wikipedia.org/wiki/Digamma_function#Relation_to_harmonic_numbers)\n  and [Euler's constant](\n  https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant).\n\n  Args:\n    x: input float.\n\n  Returns:\n    z: The analytic continuation of the harmonic number for the input.\n  \"\"\"\n  one = tf.ones([], dtype=x.dtype)\n  return tf.math.digamma(x + one) - tf.math.digamma(one)", "code_tokens": ["def", "_harmonic_number", "(", "x", ")", ":", "one", "=", "tf", ".", "ones", "(", "[", "]", ",", "dtype", "=", "x", ".", "dtype", ")", "return", "tf", ".", "math", ".", "digamma", "(", "x", "+", "one", ")", "-", "tf", ".", "math", ".", "digamma", "(", "one", ")"], "docstring": "Compute the harmonic number from its analytic continuation.\n\n  Derivation from [here](\n  https://en.wikipedia.org/wiki/Digamma_function#Relation_to_harmonic_numbers)\n  and [Euler's constant](\n  https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant).\n\n  Args:\n    x: input float.\n\n  Returns:\n    z: The analytic continuation of the harmonic number for the input.", "docstring_tokens": ["Compute", "the", "harmonic", "number", "from", "its", "analytic", "continuation", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/kumaraswamy.py#L40-L55", "partition": "test", "index": 985, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/chain.py", "func_name": "_compute_min_event_ndims", "original_string": "def _compute_min_event_ndims(bijector_list, compute_forward=True):\n  \"\"\"Computes the min_event_ndims associated with the give list of bijectors.\n\n  Given a list `bijector_list` of bijectors, compute the min_event_ndims that is\n  associated with the composition of bijectors in that list.\n\n  min_event_ndims is the # of right most dimensions for which the bijector has\n  done necessary computation on (i.e. the non-broadcastable part of the\n  computation).\n\n  We can derive the min_event_ndims for a chain of bijectors as follows:\n\n  In the case where there are no rank changing bijectors, this will simply be\n  `max(b.forward_min_event_ndims for b in bijector_list)`. This is because the\n  bijector with the most forward_min_event_ndims requires the most dimensions,\n  and hence the chain also requires operating on those dimensions.\n\n  However in the case of rank changing, more care is needed in determining the\n  exact amount of dimensions. Padding dimensions causes subsequent bijectors to\n  operate on the padded dimensions, and Removing dimensions causes bijectors to\n  operate more left.\n\n  Args:\n    bijector_list: List of bijectors to be composed by chain.\n    compute_forward: Boolean. If True, computes the min_event_ndims associated\n      with a forward call to Chain, and otherwise computes the min_event_ndims\n      associated with an inverse call to Chain. The latter is the same as the\n      min_event_ndims associated with a forward call to Invert(Chain(....)).\n\n  Returns:\n    min_event_ndims\n  \"\"\"\n  min_event_ndims = 0\n  # This is a mouthful, but what this encapsulates is that if not for rank\n  # changing bijectors, we'd only need to compute the largest of the min\n  # required ndims. Hence \"max_min\". Due to rank changing bijectors, we need to\n  # account for synthetic rank growth / synthetic rank decrease from a rank\n  # changing bijector.\n  rank_changed_adjusted_max_min_event_ndims = 0\n\n  if compute_forward:\n    bijector_list = reversed(bijector_list)\n\n  for b in bijector_list:\n    if compute_forward:\n      current_min_event_ndims = b.forward_min_event_ndims\n      current_inverse_min_event_ndims = b.inverse_min_event_ndims\n    else:\n      current_min_event_ndims = b.inverse_min_event_ndims\n      current_inverse_min_event_ndims = b.forward_min_event_ndims\n\n    # New dimensions were touched.\n    if rank_changed_adjusted_max_min_event_ndims < current_min_event_ndims:\n      min_event_ndims += (\n          current_min_event_ndims - rank_changed_adjusted_max_min_event_ndims)\n    rank_changed_adjusted_max_min_event_ndims = max(\n        current_min_event_ndims, rank_changed_adjusted_max_min_event_ndims)\n\n    # If the number of dimensions has increased via forward, then\n    # inverse_min_event_ndims > forward_min_event_ndims, and hence the\n    # dimensions we computed on, have moved left (so we have operated\n    # on additional dimensions).\n    # Conversely, if the number of dimensions has decreased via forward,\n    # then we have inverse_min_event_ndims < forward_min_event_ndims,\n    # and so we will have operated on fewer right most dimensions.\n\n    number_of_changed_dimensions = (\n        current_min_event_ndims - current_inverse_min_event_ndims)\n    rank_changed_adjusted_max_min_event_ndims -= number_of_changed_dimensions\n  return min_event_ndims", "language": "python", "code": "def _compute_min_event_ndims(bijector_list, compute_forward=True):\n  \"\"\"Computes the min_event_ndims associated with the give list of bijectors.\n\n  Given a list `bijector_list` of bijectors, compute the min_event_ndims that is\n  associated with the composition of bijectors in that list.\n\n  min_event_ndims is the # of right most dimensions for which the bijector has\n  done necessary computation on (i.e. the non-broadcastable part of the\n  computation).\n\n  We can derive the min_event_ndims for a chain of bijectors as follows:\n\n  In the case where there are no rank changing bijectors, this will simply be\n  `max(b.forward_min_event_ndims for b in bijector_list)`. This is because the\n  bijector with the most forward_min_event_ndims requires the most dimensions,\n  and hence the chain also requires operating on those dimensions.\n\n  However in the case of rank changing, more care is needed in determining the\n  exact amount of dimensions. Padding dimensions causes subsequent bijectors to\n  operate on the padded dimensions, and Removing dimensions causes bijectors to\n  operate more left.\n\n  Args:\n    bijector_list: List of bijectors to be composed by chain.\n    compute_forward: Boolean. If True, computes the min_event_ndims associated\n      with a forward call to Chain, and otherwise computes the min_event_ndims\n      associated with an inverse call to Chain. The latter is the same as the\n      min_event_ndims associated with a forward call to Invert(Chain(....)).\n\n  Returns:\n    min_event_ndims\n  \"\"\"\n  min_event_ndims = 0\n  # This is a mouthful, but what this encapsulates is that if not for rank\n  # changing bijectors, we'd only need to compute the largest of the min\n  # required ndims. Hence \"max_min\". Due to rank changing bijectors, we need to\n  # account for synthetic rank growth / synthetic rank decrease from a rank\n  # changing bijector.\n  rank_changed_adjusted_max_min_event_ndims = 0\n\n  if compute_forward:\n    bijector_list = reversed(bijector_list)\n\n  for b in bijector_list:\n    if compute_forward:\n      current_min_event_ndims = b.forward_min_event_ndims\n      current_inverse_min_event_ndims = b.inverse_min_event_ndims\n    else:\n      current_min_event_ndims = b.inverse_min_event_ndims\n      current_inverse_min_event_ndims = b.forward_min_event_ndims\n\n    # New dimensions were touched.\n    if rank_changed_adjusted_max_min_event_ndims < current_min_event_ndims:\n      min_event_ndims += (\n          current_min_event_ndims - rank_changed_adjusted_max_min_event_ndims)\n    rank_changed_adjusted_max_min_event_ndims = max(\n        current_min_event_ndims, rank_changed_adjusted_max_min_event_ndims)\n\n    # If the number of dimensions has increased via forward, then\n    # inverse_min_event_ndims > forward_min_event_ndims, and hence the\n    # dimensions we computed on, have moved left (so we have operated\n    # on additional dimensions).\n    # Conversely, if the number of dimensions has decreased via forward,\n    # then we have inverse_min_event_ndims < forward_min_event_ndims,\n    # and so we will have operated on fewer right most dimensions.\n\n    number_of_changed_dimensions = (\n        current_min_event_ndims - current_inverse_min_event_ndims)\n    rank_changed_adjusted_max_min_event_ndims -= number_of_changed_dimensions\n  return min_event_ndims", "code_tokens": ["def", "_compute_min_event_ndims", "(", "bijector_list", ",", "compute_forward", "=", "True", ")", ":", "min_event_ndims", "=", "0", "# This is a mouthful, but what this encapsulates is that if not for rank", "# changing bijectors, we'd only need to compute the largest of the min", "# required ndims. Hence \"max_min\". Due to rank changing bijectors, we need to", "# account for synthetic rank growth / synthetic rank decrease from a rank", "# changing bijector.", "rank_changed_adjusted_max_min_event_ndims", "=", "0", "if", "compute_forward", ":", "bijector_list", "=", "reversed", "(", "bijector_list", ")", "for", "b", "in", "bijector_list", ":", "if", "compute_forward", ":", "current_min_event_ndims", "=", "b", ".", "forward_min_event_ndims", "current_inverse_min_event_ndims", "=", "b", ".", "inverse_min_event_ndims", "else", ":", "current_min_event_ndims", "=", "b", ".", "inverse_min_event_ndims", "current_inverse_min_event_ndims", "=", "b", ".", "forward_min_event_ndims", "# New dimensions were touched.", "if", "rank_changed_adjusted_max_min_event_ndims", "<", "current_min_event_ndims", ":", "min_event_ndims", "+=", "(", "current_min_event_ndims", "-", "rank_changed_adjusted_max_min_event_ndims", ")", "rank_changed_adjusted_max_min_event_ndims", "=", "max", "(", "current_min_event_ndims", ",", "rank_changed_adjusted_max_min_event_ndims", ")", "# If the number of dimensions has increased via forward, then", "# inverse_min_event_ndims > forward_min_event_ndims, and hence the", "# dimensions we computed on, have moved left (so we have operated", "# on additional dimensions).", "# Conversely, if the number of dimensions has decreased via forward,", "# then we have inverse_min_event_ndims < forward_min_event_ndims,", "# and so we will have operated on fewer right most dimensions.", "number_of_changed_dimensions", "=", "(", "current_min_event_ndims", "-", "current_inverse_min_event_ndims", ")", "rank_changed_adjusted_max_min_event_ndims", "-=", "number_of_changed_dimensions", "return", "min_event_ndims"], "docstring": "Computes the min_event_ndims associated with the give list of bijectors.\n\n  Given a list `bijector_list` of bijectors, compute the min_event_ndims that is\n  associated with the composition of bijectors in that list.\n\n  min_event_ndims is the # of right most dimensions for which the bijector has\n  done necessary computation on (i.e. the non-broadcastable part of the\n  computation).\n\n  We can derive the min_event_ndims for a chain of bijectors as follows:\n\n  In the case where there are no rank changing bijectors, this will simply be\n  `max(b.forward_min_event_ndims for b in bijector_list)`. This is because the\n  bijector with the most forward_min_event_ndims requires the most dimensions,\n  and hence the chain also requires operating on those dimensions.\n\n  However in the case of rank changing, more care is needed in determining the\n  exact amount of dimensions. Padding dimensions causes subsequent bijectors to\n  operate on the padded dimensions, and Removing dimensions causes bijectors to\n  operate more left.\n\n  Args:\n    bijector_list: List of bijectors to be composed by chain.\n    compute_forward: Boolean. If True, computes the min_event_ndims associated\n      with a forward call to Chain, and otherwise computes the min_event_ndims\n      associated with an inverse call to Chain. The latter is the same as the\n      min_event_ndims associated with a forward call to Invert(Chain(....)).\n\n  Returns:\n    min_event_ndims", "docstring_tokens": ["Computes", "the", "min_event_ndims", "associated", "with", "the", "give", "list", "of", "bijectors", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/chain.py#L40-L109", "partition": "test", "index": 737, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/fill_triangular.py", "func_name": "vector_size_to_square_matrix_size", "original_string": "def vector_size_to_square_matrix_size(d, validate_args, name=None):\n  \"\"\"Convert a vector size to a matrix size.\"\"\"\n  if isinstance(d, (float, int, np.generic, np.ndarray)):\n    n = (-1 + np.sqrt(1 + 8 * d)) / 2.\n    if float(int(n)) != n:\n      raise ValueError(\"Vector length is not a triangular number.\")\n    return int(n)\n  else:\n    with tf.name_scope(name or \"vector_size_to_square_matrix_size\") as name:\n      n = (-1. + tf.sqrt(1 + 8. * tf.cast(d, dtype=tf.float32))) / 2.\n      if validate_args:\n        with tf.control_dependencies([\n            assert_util.assert_equal(\n                tf.cast(tf.cast(n, dtype=tf.int32), dtype=tf.float32),\n                n,\n                message=\"Vector length is not a triangular number\")\n        ]):\n          n = tf.identity(n)\n      return tf.cast(n, d.dtype)", "language": "python", "code": "def vector_size_to_square_matrix_size(d, validate_args, name=None):\n  \"\"\"Convert a vector size to a matrix size.\"\"\"\n  if isinstance(d, (float, int, np.generic, np.ndarray)):\n    n = (-1 + np.sqrt(1 + 8 * d)) / 2.\n    if float(int(n)) != n:\n      raise ValueError(\"Vector length is not a triangular number.\")\n    return int(n)\n  else:\n    with tf.name_scope(name or \"vector_size_to_square_matrix_size\") as name:\n      n = (-1. + tf.sqrt(1 + 8. * tf.cast(d, dtype=tf.float32))) / 2.\n      if validate_args:\n        with tf.control_dependencies([\n            assert_util.assert_equal(\n                tf.cast(tf.cast(n, dtype=tf.int32), dtype=tf.float32),\n                n,\n                message=\"Vector length is not a triangular number\")\n        ]):\n          n = tf.identity(n)\n      return tf.cast(n, d.dtype)", "code_tokens": ["def", "vector_size_to_square_matrix_size", "(", "d", ",", "validate_args", ",", "name", "=", "None", ")", ":", "if", "isinstance", "(", "d", ",", "(", "float", ",", "int", ",", "np", ".", "generic", ",", "np", ".", "ndarray", ")", ")", ":", "n", "=", "(", "-", "1", "+", "np", ".", "sqrt", "(", "1", "+", "8", "*", "d", ")", ")", "/", "2.", "if", "float", "(", "int", "(", "n", ")", ")", "!=", "n", ":", "raise", "ValueError", "(", "\"Vector length is not a triangular number.\"", ")", "return", "int", "(", "n", ")", "else", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "\"vector_size_to_square_matrix_size\"", ")", "as", "name", ":", "n", "=", "(", "-", "1.", "+", "tf", ".", "sqrt", "(", "1", "+", "8.", "*", "tf", ".", "cast", "(", "d", ",", "dtype", "=", "tf", ".", "float32", ")", ")", ")", "/", "2.", "if", "validate_args", ":", "with", "tf", ".", "control_dependencies", "(", "[", "assert_util", ".", "assert_equal", "(", "tf", ".", "cast", "(", "tf", ".", "cast", "(", "n", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "n", ",", "message", "=", "\"Vector length is not a triangular number\"", ")", "]", ")", ":", "n", "=", "tf", ".", "identity", "(", "n", ")", "return", "tf", ".", "cast", "(", "n", ",", "d", ".", "dtype", ")"], "docstring": "Convert a vector size to a matrix size.", "docstring_tokens": ["Convert", "a", "vector", "size", "to", "a", "matrix", "size", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/fill_triangular.py#L135-L153", "partition": "test", "index": 738, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/affine.py", "func_name": "_as_tensor", "original_string": "def _as_tensor(x, name, dtype):\n  \"\"\"Convenience to convert to `Tensor` or leave as `None`.\"\"\"\n  return None if x is None else tf.convert_to_tensor(\n      value=x, name=name, dtype=dtype)", "language": "python", "code": "def _as_tensor(x, name, dtype):\n  \"\"\"Convenience to convert to `Tensor` or leave as `None`.\"\"\"\n  return None if x is None else tf.convert_to_tensor(\n      value=x, name=name, dtype=dtype)", "code_tokens": ["def", "_as_tensor", "(", "x", ",", "name", ",", "dtype", ")", ":", "return", "None", "if", "x", "is", "None", "else", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "name", ",", "dtype", "=", "dtype", ")"], "docstring": "Convenience to convert to `Tensor` or leave as `None`.", "docstring_tokens": ["Convenience", "to", "convert", "to", "Tensor", "or", "leave", "as", "None", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/affine.py#L34-L37", "partition": "test", "index": 645, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/mixture.py", "func_name": "Mixture.entropy_lower_bound", "original_string": "def entropy_lower_bound(self, name=\"entropy_lower_bound\"):\n    r\"\"\"A lower bound on the entropy of this mixture model.\n\n    The bound below is not always very tight, and its usefulness depends\n    on the mixture probabilities and the components in use.\n\n    A lower bound is useful for ELBO when the `Mixture` is the variational\n    distribution:\n\n    \\\\(\n    \\log p(x) >= ELBO = \\int q(z) \\log p(x, z) dz + H[q]\n    \\\\)\n\n    where \\\\( p \\\\) is the prior distribution, \\\\( q \\\\) is the variational,\n    and \\\\( H[q] \\\\) is the entropy of \\\\( q \\\\). If there is a lower bound\n    \\\\( G[q] \\\\) such that \\\\( H[q] \\geq G[q] \\\\) then it can be used in\n    place of \\\\( H[q] \\\\).\n\n    For a mixture of distributions \\\\( q(Z) = \\sum_i c_i q_i(Z) \\\\) with\n    \\\\( \\sum_i c_i = 1 \\\\), by the concavity of \\\\( f(x) = -x \\log x \\\\), a\n    simple lower bound is:\n\n    \\\\(\n    \\begin{align}\n    H[q] & = - \\int q(z) \\log q(z) dz \\\\\\\n       & = - \\int (\\sum_i c_i q_i(z)) \\log(\\sum_i c_i q_i(z)) dz \\\\\\\n       & \\geq - \\sum_i c_i \\int q_i(z) \\log q_i(z) dz \\\\\\\n       & = \\sum_i c_i H[q_i]\n    \\end{align}\n    \\\\)\n\n    This is the term we calculate below for \\\\( G[q] \\\\).\n\n    Args:\n      name: A name for this operation (optional).\n\n    Returns:\n      A lower bound on the Mixture's entropy.\n    \"\"\"\n    with self._name_scope(name):\n      with tf.control_dependencies(self._assertions):\n        distribution_entropies = [d.entropy() for d in self.components]\n        cat_probs = self._cat_probs(log_probs=False)\n        partial_entropies = [\n            c_p * m for (c_p, m) in zip(cat_probs, distribution_entropies)\n        ]\n        # These are all the same shape by virtue of matching batch_shape\n        return tf.add_n(partial_entropies)", "language": "python", "code": "def entropy_lower_bound(self, name=\"entropy_lower_bound\"):\n    r\"\"\"A lower bound on the entropy of this mixture model.\n\n    The bound below is not always very tight, and its usefulness depends\n    on the mixture probabilities and the components in use.\n\n    A lower bound is useful for ELBO when the `Mixture` is the variational\n    distribution:\n\n    \\\\(\n    \\log p(x) >= ELBO = \\int q(z) \\log p(x, z) dz + H[q]\n    \\\\)\n\n    where \\\\( p \\\\) is the prior distribution, \\\\( q \\\\) is the variational,\n    and \\\\( H[q] \\\\) is the entropy of \\\\( q \\\\). If there is a lower bound\n    \\\\( G[q] \\\\) such that \\\\( H[q] \\geq G[q] \\\\) then it can be used in\n    place of \\\\( H[q] \\\\).\n\n    For a mixture of distributions \\\\( q(Z) = \\sum_i c_i q_i(Z) \\\\) with\n    \\\\( \\sum_i c_i = 1 \\\\), by the concavity of \\\\( f(x) = -x \\log x \\\\), a\n    simple lower bound is:\n\n    \\\\(\n    \\begin{align}\n    H[q] & = - \\int q(z) \\log q(z) dz \\\\\\\n       & = - \\int (\\sum_i c_i q_i(z)) \\log(\\sum_i c_i q_i(z)) dz \\\\\\\n       & \\geq - \\sum_i c_i \\int q_i(z) \\log q_i(z) dz \\\\\\\n       & = \\sum_i c_i H[q_i]\n    \\end{align}\n    \\\\)\n\n    This is the term we calculate below for \\\\( G[q] \\\\).\n\n    Args:\n      name: A name for this operation (optional).\n\n    Returns:\n      A lower bound on the Mixture's entropy.\n    \"\"\"\n    with self._name_scope(name):\n      with tf.control_dependencies(self._assertions):\n        distribution_entropies = [d.entropy() for d in self.components]\n        cat_probs = self._cat_probs(log_probs=False)\n        partial_entropies = [\n            c_p * m for (c_p, m) in zip(cat_probs, distribution_entropies)\n        ]\n        # These are all the same shape by virtue of matching batch_shape\n        return tf.add_n(partial_entropies)", "code_tokens": ["def", "entropy_lower_bound", "(", "self", ",", "name", "=", "\"entropy_lower_bound\"", ")", ":", "with", "self", ".", "_name_scope", "(", "name", ")", ":", "with", "tf", ".", "control_dependencies", "(", "self", ".", "_assertions", ")", ":", "distribution_entropies", "=", "[", "d", ".", "entropy", "(", ")", "for", "d", "in", "self", ".", "components", "]", "cat_probs", "=", "self", ".", "_cat_probs", "(", "log_probs", "=", "False", ")", "partial_entropies", "=", "[", "c_p", "*", "m", "for", "(", "c_p", ",", "m", ")", "in", "zip", "(", "cat_probs", ",", "distribution_entropies", ")", "]", "# These are all the same shape by virtue of matching batch_shape", "return", "tf", ".", "add_n", "(", "partial_entropies", ")"], "docstring": "r\"\"\"A lower bound on the entropy of this mixture model.\n\n    The bound below is not always very tight, and its usefulness depends\n    on the mixture probabilities and the components in use.\n\n    A lower bound is useful for ELBO when the `Mixture` is the variational\n    distribution:\n\n    \\\\(\n    \\log p(x) >= ELBO = \\int q(z) \\log p(x, z) dz + H[q]\n    \\\\)\n\n    where \\\\( p \\\\) is the prior distribution, \\\\( q \\\\) is the variational,\n    and \\\\( H[q] \\\\) is the entropy of \\\\( q \\\\). If there is a lower bound\n    \\\\( G[q] \\\\) such that \\\\( H[q] \\geq G[q] \\\\) then it can be used in\n    place of \\\\( H[q] \\\\).\n\n    For a mixture of distributions \\\\( q(Z) = \\sum_i c_i q_i(Z) \\\\) with\n    \\\\( \\sum_i c_i = 1 \\\\), by the concavity of \\\\( f(x) = -x \\log x \\\\), a\n    simple lower bound is:\n\n    \\\\(\n    \\begin{align}\n    H[q] & = - \\int q(z) \\log q(z) dz \\\\\\\n       & = - \\int (\\sum_i c_i q_i(z)) \\log(\\sum_i c_i q_i(z)) dz \\\\\\\n       & \\geq - \\sum_i c_i \\int q_i(z) \\log q_i(z) dz \\\\\\\n       & = \\sum_i c_i H[q_i]\n    \\end{align}\n    \\\\)\n\n    This is the term we calculate below for \\\\( G[q] \\\\).\n\n    Args:\n      name: A name for this operation (optional).\n\n    Returns:\n      A lower bound on the Mixture's entropy.", "docstring_tokens": ["r", "A", "lower", "bound", "on", "the", "entropy", "of", "this", "mixture", "model", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/mixture.py#L448-L495", "partition": "test", "index": 650, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/batch_reshape.py", "func_name": "BatchReshape._call_and_reshape_output", "original_string": "def _call_and_reshape_output(\n      self,\n      fn,\n      event_shape_list=None,\n      static_event_shape_list=None,\n      extra_kwargs=None):\n    \"\"\"Calls `fn` and appropriately reshapes its output.\"\"\"\n    # Note: we take `extra_kwargs` as a dict rather than `**extra_kwargs`\n    # because it is possible the user provided extra kwargs would itself\n    # have `fn`, `event_shape_list`, `static_event_shape_list` and/or\n    # `extra_kwargs` as keys.\n    with tf.control_dependencies(self._runtime_assertions):\n      if event_shape_list is None:\n        event_shape_list = [self._event_shape_tensor()]\n      if static_event_shape_list is None:\n        static_event_shape_list = [self.event_shape]\n      new_shape = tf.concat(\n          [self._batch_shape_unexpanded] + event_shape_list, axis=0)\n      result = tf.reshape(fn(**extra_kwargs) if extra_kwargs else fn(),\n                          new_shape)\n      if (tensorshape_util.rank(self.batch_shape) is not None and\n          tensorshape_util.rank(self.event_shape) is not None):\n        event_shape = tf.TensorShape([])\n        for rss in static_event_shape_list:\n          event_shape = tensorshape_util.concatenate(event_shape, rss)\n        static_shape = tensorshape_util.concatenate(\n            self.batch_shape, event_shape)\n        tensorshape_util.set_shape(result, static_shape)\n      return result", "language": "python", "code": "def _call_and_reshape_output(\n      self,\n      fn,\n      event_shape_list=None,\n      static_event_shape_list=None,\n      extra_kwargs=None):\n    \"\"\"Calls `fn` and appropriately reshapes its output.\"\"\"\n    # Note: we take `extra_kwargs` as a dict rather than `**extra_kwargs`\n    # because it is possible the user provided extra kwargs would itself\n    # have `fn`, `event_shape_list`, `static_event_shape_list` and/or\n    # `extra_kwargs` as keys.\n    with tf.control_dependencies(self._runtime_assertions):\n      if event_shape_list is None:\n        event_shape_list = [self._event_shape_tensor()]\n      if static_event_shape_list is None:\n        static_event_shape_list = [self.event_shape]\n      new_shape = tf.concat(\n          [self._batch_shape_unexpanded] + event_shape_list, axis=0)\n      result = tf.reshape(fn(**extra_kwargs) if extra_kwargs else fn(),\n                          new_shape)\n      if (tensorshape_util.rank(self.batch_shape) is not None and\n          tensorshape_util.rank(self.event_shape) is not None):\n        event_shape = tf.TensorShape([])\n        for rss in static_event_shape_list:\n          event_shape = tensorshape_util.concatenate(event_shape, rss)\n        static_shape = tensorshape_util.concatenate(\n            self.batch_shape, event_shape)\n        tensorshape_util.set_shape(result, static_shape)\n      return result", "code_tokens": ["def", "_call_and_reshape_output", "(", "self", ",", "fn", ",", "event_shape_list", "=", "None", ",", "static_event_shape_list", "=", "None", ",", "extra_kwargs", "=", "None", ")", ":", "# Note: we take `extra_kwargs` as a dict rather than `**extra_kwargs`", "# because it is possible the user provided extra kwargs would itself", "# have `fn`, `event_shape_list`, `static_event_shape_list` and/or", "# `extra_kwargs` as keys.", "with", "tf", ".", "control_dependencies", "(", "self", ".", "_runtime_assertions", ")", ":", "if", "event_shape_list", "is", "None", ":", "event_shape_list", "=", "[", "self", ".", "_event_shape_tensor", "(", ")", "]", "if", "static_event_shape_list", "is", "None", ":", "static_event_shape_list", "=", "[", "self", ".", "event_shape", "]", "new_shape", "=", "tf", ".", "concat", "(", "[", "self", ".", "_batch_shape_unexpanded", "]", "+", "event_shape_list", ",", "axis", "=", "0", ")", "result", "=", "tf", ".", "reshape", "(", "fn", "(", "*", "*", "extra_kwargs", ")", "if", "extra_kwargs", "else", "fn", "(", ")", ",", "new_shape", ")", "if", "(", "tensorshape_util", ".", "rank", "(", "self", ".", "batch_shape", ")", "is", "not", "None", "and", "tensorshape_util", ".", "rank", "(", "self", ".", "event_shape", ")", "is", "not", "None", ")", ":", "event_shape", "=", "tf", ".", "TensorShape", "(", "[", "]", ")", "for", "rss", "in", "static_event_shape_list", ":", "event_shape", "=", "tensorshape_util", ".", "concatenate", "(", "event_shape", ",", "rss", ")", "static_shape", "=", "tensorshape_util", ".", "concatenate", "(", "self", ".", "batch_shape", ",", "event_shape", ")", "tensorshape_util", ".", "set_shape", "(", "result", ",", "static_shape", ")", "return", "result"], "docstring": "Calls `fn` and appropriately reshapes its output.", "docstring_tokens": ["Calls", "fn", "and", "appropriately", "reshapes", "its", "output", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/batch_reshape.py#L264-L292", "partition": "test", "index": 835, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/mixture.py", "func_name": "Mixture._cat_probs", "original_string": "def _cat_probs(self, log_probs):\n    \"\"\"Get a list of num_components batchwise probabilities.\"\"\"\n    which_softmax = tf.nn.log_softmax if log_probs else tf.nn.softmax\n    cat_probs = which_softmax(self.cat.logits)\n    cat_probs = tf.unstack(cat_probs, num=self.num_components, axis=-1)\n    return cat_probs", "language": "python", "code": "def _cat_probs(self, log_probs):\n    \"\"\"Get a list of num_components batchwise probabilities.\"\"\"\n    which_softmax = tf.nn.log_softmax if log_probs else tf.nn.softmax\n    cat_probs = which_softmax(self.cat.logits)\n    cat_probs = tf.unstack(cat_probs, num=self.num_components, axis=-1)\n    return cat_probs", "code_tokens": ["def", "_cat_probs", "(", "self", ",", "log_probs", ")", ":", "which_softmax", "=", "tf", ".", "nn", ".", "log_softmax", "if", "log_probs", "else", "tf", ".", "nn", ".", "softmax", "cat_probs", "=", "which_softmax", "(", "self", ".", "cat", ".", "logits", ")", "cat_probs", "=", "tf", ".", "unstack", "(", "cat_probs", ",", "num", "=", "self", ".", "num_components", ",", "axis", "=", "-", "1", ")", "return", "cat_probs"], "docstring": "Get a list of num_components batchwise probabilities.", "docstring_tokens": ["Get", "a", "list", "of", "num_components", "batchwise", "probabilities", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/mixture.py#L497-L502", "partition": "test", "index": 651, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/poisson_lognormal.py", "func_name": "quadrature_scheme_lognormal_gauss_hermite", "original_string": "def quadrature_scheme_lognormal_gauss_hermite(\n    loc, scale, quadrature_size,\n    validate_args=False, name=None):  # pylint: disable=unused-argument\n  \"\"\"Use Gauss-Hermite quadrature to form quadrature on positive-reals.\n\n  Note: for a given `quadrature_size`, this method is generally less accurate\n  than `quadrature_scheme_lognormal_quantiles`.\n\n  Args:\n    loc: `float`-like (batch of) scalar `Tensor`; the location parameter of\n      the LogNormal prior.\n    scale: `float`-like (batch of) scalar `Tensor`; the scale parameter of\n      the LogNormal prior.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: (Batch of) length-`quadrature_size` vectors representing the\n      `log_rate` parameters of a `Poisson`.\n    probs: (Batch of) length-`quadrature_size` vectors representing the\n      weight associate with each `grid` value.\n  \"\"\"\n  with tf.name_scope(\n      name or \"vector_diffeomixture_quadrature_gauss_hermite\"):\n    grid, probs = np.polynomial.hermite.hermgauss(deg=quadrature_size)\n    npdt = dtype_util.as_numpy_dtype(loc.dtype)\n    grid = grid.astype(npdt)\n    probs = probs.astype(npdt)\n    probs /= np.linalg.norm(probs, ord=1, keepdims=True)\n    probs = tf.convert_to_tensor(value=probs, name=\"probs\", dtype=loc.dtype)\n    # The following maps the broadcast of `loc` and `scale` to each grid\n    # point, i.e., we are creating several log-rates that correspond to the\n    # different Gauss-Hermite quadrature points and (possible) batches of\n    # `loc` and `scale`.\n    grid = (loc[..., tf.newaxis] + np.sqrt(2.) * scale[..., tf.newaxis] * grid)\n    return grid, probs", "language": "python", "code": "def quadrature_scheme_lognormal_gauss_hermite(\n    loc, scale, quadrature_size,\n    validate_args=False, name=None):  # pylint: disable=unused-argument\n  \"\"\"Use Gauss-Hermite quadrature to form quadrature on positive-reals.\n\n  Note: for a given `quadrature_size`, this method is generally less accurate\n  than `quadrature_scheme_lognormal_quantiles`.\n\n  Args:\n    loc: `float`-like (batch of) scalar `Tensor`; the location parameter of\n      the LogNormal prior.\n    scale: `float`-like (batch of) scalar `Tensor`; the scale parameter of\n      the LogNormal prior.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: (Batch of) length-`quadrature_size` vectors representing the\n      `log_rate` parameters of a `Poisson`.\n    probs: (Batch of) length-`quadrature_size` vectors representing the\n      weight associate with each `grid` value.\n  \"\"\"\n  with tf.name_scope(\n      name or \"vector_diffeomixture_quadrature_gauss_hermite\"):\n    grid, probs = np.polynomial.hermite.hermgauss(deg=quadrature_size)\n    npdt = dtype_util.as_numpy_dtype(loc.dtype)\n    grid = grid.astype(npdt)\n    probs = probs.astype(npdt)\n    probs /= np.linalg.norm(probs, ord=1, keepdims=True)\n    probs = tf.convert_to_tensor(value=probs, name=\"probs\", dtype=loc.dtype)\n    # The following maps the broadcast of `loc` and `scale` to each grid\n    # point, i.e., we are creating several log-rates that correspond to the\n    # different Gauss-Hermite quadrature points and (possible) batches of\n    # `loc` and `scale`.\n    grid = (loc[..., tf.newaxis] + np.sqrt(2.) * scale[..., tf.newaxis] * grid)\n    return grid, probs", "code_tokens": ["def", "quadrature_scheme_lognormal_gauss_hermite", "(", "loc", ",", "scale", ",", "quadrature_size", ",", "validate_args", "=", "False", ",", "name", "=", "None", ")", ":", "# pylint: disable=unused-argument", "with", "tf", ".", "name_scope", "(", "name", "or", "\"vector_diffeomixture_quadrature_gauss_hermite\"", ")", ":", "grid", ",", "probs", "=", "np", ".", "polynomial", ".", "hermite", ".", "hermgauss", "(", "deg", "=", "quadrature_size", ")", "npdt", "=", "dtype_util", ".", "as_numpy_dtype", "(", "loc", ".", "dtype", ")", "grid", "=", "grid", ".", "astype", "(", "npdt", ")", "probs", "=", "probs", ".", "astype", "(", "npdt", ")", "probs", "/=", "np", ".", "linalg", ".", "norm", "(", "probs", ",", "ord", "=", "1", ",", "keepdims", "=", "True", ")", "probs", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "probs", ",", "name", "=", "\"probs\"", ",", "dtype", "=", "loc", ".", "dtype", ")", "# The following maps the broadcast of `loc` and `scale` to each grid", "# point, i.e., we are creating several log-rates that correspond to the", "# different Gauss-Hermite quadrature points and (possible) batches of", "# `loc` and `scale`.", "grid", "=", "(", "loc", "[", "...", ",", "tf", ".", "newaxis", "]", "+", "np", ".", "sqrt", "(", "2.", ")", "*", "scale", "[", "...", ",", "tf", ".", "newaxis", "]", "*", "grid", ")", "return", "grid", ",", "probs"], "docstring": "Use Gauss-Hermite quadrature to form quadrature on positive-reals.\n\n  Note: for a given `quadrature_size`, this method is generally less accurate\n  than `quadrature_scheme_lognormal_quantiles`.\n\n  Args:\n    loc: `float`-like (batch of) scalar `Tensor`; the location parameter of\n      the LogNormal prior.\n    scale: `float`-like (batch of) scalar `Tensor`; the scale parameter of\n      the LogNormal prior.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: (Batch of) length-`quadrature_size` vectors representing the\n      `log_rate` parameters of a `Poisson`.\n    probs: (Batch of) length-`quadrature_size` vectors representing the\n      weight associate with each `grid` value.", "docstring_tokens": ["Use", "Gauss", "-", "Hermite", "quadrature", "to", "form", "quadrature", "on", "positive", "-", "reals", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/poisson_lognormal.py#L45-L85", "partition": "test", "index": 786, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/poisson_lognormal.py", "func_name": "quadrature_scheme_lognormal_quantiles", "original_string": "def quadrature_scheme_lognormal_quantiles(\n    loc, scale, quadrature_size,\n    validate_args=False, name=None):\n  \"\"\"Use LogNormal quantiles to form quadrature on positive-reals.\n\n  Args:\n    loc: `float`-like (batch of) scalar `Tensor`; the location parameter of\n      the LogNormal prior.\n    scale: `float`-like (batch of) scalar `Tensor`; the scale parameter of\n      the LogNormal prior.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: (Batch of) length-`quadrature_size` vectors representing the\n      `log_rate` parameters of a `Poisson`.\n    probs: (Batch of) length-`quadrature_size` vectors representing the\n      weight associate with each `grid` value.\n  \"\"\"\n  with tf.name_scope(name or \"quadrature_scheme_lognormal_quantiles\"):\n    # Create a LogNormal distribution.\n    dist = transformed_distribution.TransformedDistribution(\n        distribution=normal.Normal(loc=loc, scale=scale),\n        bijector=exp_bijector.Exp(),\n        validate_args=validate_args)\n    batch_ndims = tensorshape_util.rank(dist.batch_shape)\n    if batch_ndims is None:\n      batch_ndims = tf.shape(input=dist.batch_shape_tensor())[0]\n\n    def _compute_quantiles():\n      \"\"\"Helper to build quantiles.\"\"\"\n      # Omit {0, 1} since they might lead to Inf/NaN.\n      zero = tf.zeros([], dtype=dist.dtype)\n      edges = tf.linspace(zero, 1., quadrature_size + 3)[1:-1]\n      # Expand edges so its broadcast across batch dims.\n      edges = tf.reshape(\n          edges,\n          shape=tf.concat(\n              [[-1], tf.ones([batch_ndims], dtype=tf.int32)], axis=0))\n      quantiles = dist.quantile(edges)\n      # Cyclically permute left by one.\n      perm = tf.concat([tf.range(1, 1 + batch_ndims), [0]], axis=0)\n      quantiles = tf.transpose(a=quantiles, perm=perm)\n      return quantiles\n    quantiles = _compute_quantiles()\n\n    # Compute grid as quantile midpoints.\n    grid = (quantiles[..., :-1] + quantiles[..., 1:]) / 2.\n    # Set shape hints.\n    new_shape = tensorshape_util.concatenate(dist.batch_shape,\n                                             [quadrature_size])\n    tensorshape_util.set_shape(grid, new_shape)\n\n    # By construction probs is constant, i.e., `1 / quadrature_size`. This is\n    # important, because non-constant probs leads to non-reparameterizable\n    # samples.\n    probs = tf.fill(\n        dims=[quadrature_size], value=1. / tf.cast(quadrature_size, dist.dtype))\n\n    return grid, probs", "language": "python", "code": "def quadrature_scheme_lognormal_quantiles(\n    loc, scale, quadrature_size,\n    validate_args=False, name=None):\n  \"\"\"Use LogNormal quantiles to form quadrature on positive-reals.\n\n  Args:\n    loc: `float`-like (batch of) scalar `Tensor`; the location parameter of\n      the LogNormal prior.\n    scale: `float`-like (batch of) scalar `Tensor`; the scale parameter of\n      the LogNormal prior.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: (Batch of) length-`quadrature_size` vectors representing the\n      `log_rate` parameters of a `Poisson`.\n    probs: (Batch of) length-`quadrature_size` vectors representing the\n      weight associate with each `grid` value.\n  \"\"\"\n  with tf.name_scope(name or \"quadrature_scheme_lognormal_quantiles\"):\n    # Create a LogNormal distribution.\n    dist = transformed_distribution.TransformedDistribution(\n        distribution=normal.Normal(loc=loc, scale=scale),\n        bijector=exp_bijector.Exp(),\n        validate_args=validate_args)\n    batch_ndims = tensorshape_util.rank(dist.batch_shape)\n    if batch_ndims is None:\n      batch_ndims = tf.shape(input=dist.batch_shape_tensor())[0]\n\n    def _compute_quantiles():\n      \"\"\"Helper to build quantiles.\"\"\"\n      # Omit {0, 1} since they might lead to Inf/NaN.\n      zero = tf.zeros([], dtype=dist.dtype)\n      edges = tf.linspace(zero, 1., quadrature_size + 3)[1:-1]\n      # Expand edges so its broadcast across batch dims.\n      edges = tf.reshape(\n          edges,\n          shape=tf.concat(\n              [[-1], tf.ones([batch_ndims], dtype=tf.int32)], axis=0))\n      quantiles = dist.quantile(edges)\n      # Cyclically permute left by one.\n      perm = tf.concat([tf.range(1, 1 + batch_ndims), [0]], axis=0)\n      quantiles = tf.transpose(a=quantiles, perm=perm)\n      return quantiles\n    quantiles = _compute_quantiles()\n\n    # Compute grid as quantile midpoints.\n    grid = (quantiles[..., :-1] + quantiles[..., 1:]) / 2.\n    # Set shape hints.\n    new_shape = tensorshape_util.concatenate(dist.batch_shape,\n                                             [quadrature_size])\n    tensorshape_util.set_shape(grid, new_shape)\n\n    # By construction probs is constant, i.e., `1 / quadrature_size`. This is\n    # important, because non-constant probs leads to non-reparameterizable\n    # samples.\n    probs = tf.fill(\n        dims=[quadrature_size], value=1. / tf.cast(quadrature_size, dist.dtype))\n\n    return grid, probs", "code_tokens": ["def", "quadrature_scheme_lognormal_quantiles", "(", "loc", ",", "scale", ",", "quadrature_size", ",", "validate_args", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "\"quadrature_scheme_lognormal_quantiles\"", ")", ":", "# Create a LogNormal distribution.", "dist", "=", "transformed_distribution", ".", "TransformedDistribution", "(", "distribution", "=", "normal", ".", "Normal", "(", "loc", "=", "loc", ",", "scale", "=", "scale", ")", ",", "bijector", "=", "exp_bijector", ".", "Exp", "(", ")", ",", "validate_args", "=", "validate_args", ")", "batch_ndims", "=", "tensorshape_util", ".", "rank", "(", "dist", ".", "batch_shape", ")", "if", "batch_ndims", "is", "None", ":", "batch_ndims", "=", "tf", ".", "shape", "(", "input", "=", "dist", ".", "batch_shape_tensor", "(", ")", ")", "[", "0", "]", "def", "_compute_quantiles", "(", ")", ":", "\"\"\"Helper to build quantiles.\"\"\"", "# Omit {0, 1} since they might lead to Inf/NaN.", "zero", "=", "tf", ".", "zeros", "(", "[", "]", ",", "dtype", "=", "dist", ".", "dtype", ")", "edges", "=", "tf", ".", "linspace", "(", "zero", ",", "1.", ",", "quadrature_size", "+", "3", ")", "[", "1", ":", "-", "1", "]", "# Expand edges so its broadcast across batch dims.", "edges", "=", "tf", ".", "reshape", "(", "edges", ",", "shape", "=", "tf", ".", "concat", "(", "[", "[", "-", "1", "]", ",", "tf", ".", "ones", "(", "[", "batch_ndims", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "]", ",", "axis", "=", "0", ")", ")", "quantiles", "=", "dist", ".", "quantile", "(", "edges", ")", "# Cyclically permute left by one.", "perm", "=", "tf", ".", "concat", "(", "[", "tf", ".", "range", "(", "1", ",", "1", "+", "batch_ndims", ")", ",", "[", "0", "]", "]", ",", "axis", "=", "0", ")", "quantiles", "=", "tf", ".", "transpose", "(", "a", "=", "quantiles", ",", "perm", "=", "perm", ")", "return", "quantiles", "quantiles", "=", "_compute_quantiles", "(", ")", "# Compute grid as quantile midpoints.", "grid", "=", "(", "quantiles", "[", "...", ",", ":", "-", "1", "]", "+", "quantiles", "[", "...", ",", "1", ":", "]", ")", "/", "2.", "# Set shape hints.", "new_shape", "=", "tensorshape_util", ".", "concatenate", "(", "dist", ".", "batch_shape", ",", "[", "quadrature_size", "]", ")", "tensorshape_util", ".", "set_shape", "(", "grid", ",", "new_shape", ")", "# By construction probs is constant, i.e., `1 / quadrature_size`. This is", "# important, because non-constant probs leads to non-reparameterizable", "# samples.", "probs", "=", "tf", ".", "fill", "(", "dims", "=", "[", "quadrature_size", "]", ",", "value", "=", "1.", "/", "tf", ".", "cast", "(", "quadrature_size", ",", "dist", ".", "dtype", ")", ")", "return", "grid", ",", "probs"], "docstring": "Use LogNormal quantiles to form quadrature on positive-reals.\n\n  Args:\n    loc: `float`-like (batch of) scalar `Tensor`; the location parameter of\n      the LogNormal prior.\n    scale: `float`-like (batch of) scalar `Tensor`; the scale parameter of\n      the LogNormal prior.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: (Batch of) length-`quadrature_size` vectors representing the\n      `log_rate` parameters of a `Poisson`.\n    probs: (Batch of) length-`quadrature_size` vectors representing the\n      weight associate with each `grid` value.", "docstring_tokens": ["Use", "LogNormal", "quantiles", "to", "form", "quadrature", "on", "positive", "-", "reals", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/poisson_lognormal.py#L88-L152", "partition": "test", "index": 787, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/vector_diffeomixture.py", "func_name": "quadrature_scheme_softmaxnormal_gauss_hermite", "original_string": "def quadrature_scheme_softmaxnormal_gauss_hermite(\n    normal_loc, normal_scale, quadrature_size,\n    validate_args=False, name=None):\n  \"\"\"Use Gauss-Hermite quadrature to form quadrature on `K - 1` simplex.\n\n  A `SoftmaxNormal` random variable `Y` may be generated via\n\n  ```\n  Y = SoftmaxCentered(X),\n  X = Normal(normal_loc, normal_scale)\n  ```\n\n  Note: for a given `quadrature_size`, this method is generally less accurate\n  than `quadrature_scheme_softmaxnormal_quantiles`.\n\n  Args:\n    normal_loc: `float`-like `Tensor` with shape `[b1, ..., bB, K-1]`, B>=0.\n      The location parameter of the Normal used to construct the SoftmaxNormal.\n    normal_scale: `float`-like `Tensor`. Broadcastable with `normal_loc`.\n      The scale parameter of the Normal used to construct the SoftmaxNormal.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      convex combination of affine parameters for `K` components.\n      `grid[..., :, n]` is the `n`-th grid point, living in the `K - 1` simplex.\n    probs:  Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      associated with each grid point.\n  \"\"\"\n  with tf.name_scope(\n      name or \"quadrature_scheme_softmaxnormal_gauss_hermite\"):\n    normal_loc = tf.convert_to_tensor(value=normal_loc, name=\"normal_loc\")\n    npdt = dtype_util.as_numpy_dtype(normal_loc.dtype)\n    normal_scale = tf.convert_to_tensor(\n        value=normal_scale, dtype=npdt, name=\"normal_scale\")\n\n    normal_scale = maybe_check_quadrature_param(\n        normal_scale, \"normal_scale\", validate_args)\n\n    grid, probs = np.polynomial.hermite.hermgauss(deg=quadrature_size)\n    grid = grid.astype(npdt)\n    probs = probs.astype(npdt)\n    probs /= np.linalg.norm(probs, ord=1, keepdims=True)\n    probs = tf.convert_to_tensor(value=probs, name=\"probs\", dtype=npdt)\n\n    grid = softmax(\n        -distribution_util.pad(\n            (normal_loc[..., tf.newaxis] +\n             np.sqrt(2.) * normal_scale[..., tf.newaxis] * grid),\n            axis=-2,\n            front=True),\n        axis=-2)  # shape: [B, components, deg]\n\n    return grid, probs", "language": "python", "code": "def quadrature_scheme_softmaxnormal_gauss_hermite(\n    normal_loc, normal_scale, quadrature_size,\n    validate_args=False, name=None):\n  \"\"\"Use Gauss-Hermite quadrature to form quadrature on `K - 1` simplex.\n\n  A `SoftmaxNormal` random variable `Y` may be generated via\n\n  ```\n  Y = SoftmaxCentered(X),\n  X = Normal(normal_loc, normal_scale)\n  ```\n\n  Note: for a given `quadrature_size`, this method is generally less accurate\n  than `quadrature_scheme_softmaxnormal_quantiles`.\n\n  Args:\n    normal_loc: `float`-like `Tensor` with shape `[b1, ..., bB, K-1]`, B>=0.\n      The location parameter of the Normal used to construct the SoftmaxNormal.\n    normal_scale: `float`-like `Tensor`. Broadcastable with `normal_loc`.\n      The scale parameter of the Normal used to construct the SoftmaxNormal.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      convex combination of affine parameters for `K` components.\n      `grid[..., :, n]` is the `n`-th grid point, living in the `K - 1` simplex.\n    probs:  Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      associated with each grid point.\n  \"\"\"\n  with tf.name_scope(\n      name or \"quadrature_scheme_softmaxnormal_gauss_hermite\"):\n    normal_loc = tf.convert_to_tensor(value=normal_loc, name=\"normal_loc\")\n    npdt = dtype_util.as_numpy_dtype(normal_loc.dtype)\n    normal_scale = tf.convert_to_tensor(\n        value=normal_scale, dtype=npdt, name=\"normal_scale\")\n\n    normal_scale = maybe_check_quadrature_param(\n        normal_scale, \"normal_scale\", validate_args)\n\n    grid, probs = np.polynomial.hermite.hermgauss(deg=quadrature_size)\n    grid = grid.astype(npdt)\n    probs = probs.astype(npdt)\n    probs /= np.linalg.norm(probs, ord=1, keepdims=True)\n    probs = tf.convert_to_tensor(value=probs, name=\"probs\", dtype=npdt)\n\n    grid = softmax(\n        -distribution_util.pad(\n            (normal_loc[..., tf.newaxis] +\n             np.sqrt(2.) * normal_scale[..., tf.newaxis] * grid),\n            axis=-2,\n            front=True),\n        axis=-2)  # shape: [B, components, deg]\n\n    return grid, probs", "code_tokens": ["def", "quadrature_scheme_softmaxnormal_gauss_hermite", "(", "normal_loc", ",", "normal_scale", ",", "quadrature_size", ",", "validate_args", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "\"quadrature_scheme_softmaxnormal_gauss_hermite\"", ")", ":", "normal_loc", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "normal_loc", ",", "name", "=", "\"normal_loc\"", ")", "npdt", "=", "dtype_util", ".", "as_numpy_dtype", "(", "normal_loc", ".", "dtype", ")", "normal_scale", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "normal_scale", ",", "dtype", "=", "npdt", ",", "name", "=", "\"normal_scale\"", ")", "normal_scale", "=", "maybe_check_quadrature_param", "(", "normal_scale", ",", "\"normal_scale\"", ",", "validate_args", ")", "grid", ",", "probs", "=", "np", ".", "polynomial", ".", "hermite", ".", "hermgauss", "(", "deg", "=", "quadrature_size", ")", "grid", "=", "grid", ".", "astype", "(", "npdt", ")", "probs", "=", "probs", ".", "astype", "(", "npdt", ")", "probs", "/=", "np", ".", "linalg", ".", "norm", "(", "probs", ",", "ord", "=", "1", ",", "keepdims", "=", "True", ")", "probs", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "probs", ",", "name", "=", "\"probs\"", ",", "dtype", "=", "npdt", ")", "grid", "=", "softmax", "(", "-", "distribution_util", ".", "pad", "(", "(", "normal_loc", "[", "...", ",", "tf", ".", "newaxis", "]", "+", "np", ".", "sqrt", "(", "2.", ")", "*", "normal_scale", "[", "...", ",", "tf", ".", "newaxis", "]", "*", "grid", ")", ",", "axis", "=", "-", "2", ",", "front", "=", "True", ")", ",", "axis", "=", "-", "2", ")", "# shape: [B, components, deg]", "return", "grid", ",", "probs"], "docstring": "Use Gauss-Hermite quadrature to form quadrature on `K - 1` simplex.\n\n  A `SoftmaxNormal` random variable `Y` may be generated via\n\n  ```\n  Y = SoftmaxCentered(X),\n  X = Normal(normal_loc, normal_scale)\n  ```\n\n  Note: for a given `quadrature_size`, this method is generally less accurate\n  than `quadrature_scheme_softmaxnormal_quantiles`.\n\n  Args:\n    normal_loc: `float`-like `Tensor` with shape `[b1, ..., bB, K-1]`, B>=0.\n      The location parameter of the Normal used to construct the SoftmaxNormal.\n    normal_scale: `float`-like `Tensor`. Broadcastable with `normal_loc`.\n      The scale parameter of the Normal used to construct the SoftmaxNormal.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      convex combination of affine parameters for `K` components.\n      `grid[..., :, n]` is the `n`-th grid point, living in the `K - 1` simplex.\n    probs:  Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      associated with each grid point.", "docstring_tokens": ["Use", "Gauss", "-", "Hermite", "quadrature", "to", "form", "quadrature", "on", "K", "-", "1", "simplex", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/vector_diffeomixture.py#L46-L105", "partition": "test", "index": 678, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/vector_diffeomixture.py", "func_name": "quadrature_scheme_softmaxnormal_quantiles", "original_string": "def quadrature_scheme_softmaxnormal_quantiles(\n    normal_loc, normal_scale, quadrature_size,\n    validate_args=False, name=None):\n  \"\"\"Use SoftmaxNormal quantiles to form quadrature on `K - 1` simplex.\n\n  A `SoftmaxNormal` random variable `Y` may be generated via\n\n  ```\n  Y = SoftmaxCentered(X),\n  X = Normal(normal_loc, normal_scale)\n  ```\n\n  Args:\n    normal_loc: `float`-like `Tensor` with shape `[b1, ..., bB, K-1]`, B>=0.\n      The location parameter of the Normal used to construct the SoftmaxNormal.\n    normal_scale: `float`-like `Tensor`. Broadcastable with `normal_loc`.\n      The scale parameter of the Normal used to construct the SoftmaxNormal.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      convex combination of affine parameters for `K` components.\n      `grid[..., :, n]` is the `n`-th grid point, living in the `K - 1` simplex.\n    probs:  Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      associated with each grid point.\n  \"\"\"\n  with tf.name_scope(name or \"softmax_normal_grid_and_probs\"):\n    normal_loc = tf.convert_to_tensor(value=normal_loc, name=\"normal_loc\")\n    dt = dtype_util.base_dtype(normal_loc.dtype)\n    normal_scale = tf.convert_to_tensor(\n        value=normal_scale, dtype=dt, name=\"normal_scale\")\n\n    normal_scale = maybe_check_quadrature_param(\n        normal_scale, \"normal_scale\", validate_args)\n\n    dist = normal.Normal(loc=normal_loc, scale=normal_scale)\n\n    def _get_batch_ndims():\n      \"\"\"Helper to get rank(dist.batch_shape), statically if possible.\"\"\"\n      ndims = tensorshape_util.rank(dist.batch_shape)\n      if ndims is None:\n        ndims = tf.shape(input=dist.batch_shape_tensor())[0]\n      return ndims\n    batch_ndims = _get_batch_ndims()\n\n    def _get_final_shape(qs):\n      \"\"\"Helper to build `TensorShape`.\"\"\"\n      bs = tensorshape_util.with_rank_at_least(dist.batch_shape, 1)\n      num_components = tf.compat.dimension_value(bs[-1])\n      if num_components is not None:\n        num_components += 1\n      tail = tf.TensorShape([num_components, qs])\n      return bs[:-1].concatenate(tail)\n\n    def _compute_quantiles():\n      \"\"\"Helper to build quantiles.\"\"\"\n      # Omit {0, 1} since they might lead to Inf/NaN.\n      zero = tf.zeros([], dtype=dist.dtype)\n      edges = tf.linspace(zero, 1., quadrature_size + 3)[1:-1]\n      # Expand edges so its broadcast across batch dims.\n      edges = tf.reshape(\n          edges,\n          shape=tf.concat(\n              [[-1], tf.ones([batch_ndims], dtype=tf.int32)], axis=0))\n      quantiles = dist.quantile(edges)\n      quantiles = softmax_centered_bijector.SoftmaxCentered().forward(quantiles)\n      # Cyclically permute left by one.\n      perm = tf.concat([tf.range(1, 1 + batch_ndims), [0]], axis=0)\n      quantiles = tf.transpose(a=quantiles, perm=perm)\n      tensorshape_util.set_shape(\n          quantiles, _get_final_shape(quadrature_size + 1))\n      return quantiles\n    quantiles = _compute_quantiles()\n\n    # Compute grid as quantile midpoints.\n    grid = (quantiles[..., :-1] + quantiles[..., 1:]) / 2.\n    # Set shape hints.\n    tensorshape_util.set_shape(grid, _get_final_shape(quadrature_size))\n\n    # By construction probs is constant, i.e., `1 / quadrature_size`. This is\n    # important, because non-constant probs leads to non-reparameterizable\n    # samples.\n    probs = tf.fill(\n        dims=[quadrature_size], value=1. / tf.cast(quadrature_size, dist.dtype))\n\n    return grid, probs", "language": "python", "code": "def quadrature_scheme_softmaxnormal_quantiles(\n    normal_loc, normal_scale, quadrature_size,\n    validate_args=False, name=None):\n  \"\"\"Use SoftmaxNormal quantiles to form quadrature on `K - 1` simplex.\n\n  A `SoftmaxNormal` random variable `Y` may be generated via\n\n  ```\n  Y = SoftmaxCentered(X),\n  X = Normal(normal_loc, normal_scale)\n  ```\n\n  Args:\n    normal_loc: `float`-like `Tensor` with shape `[b1, ..., bB, K-1]`, B>=0.\n      The location parameter of the Normal used to construct the SoftmaxNormal.\n    normal_scale: `float`-like `Tensor`. Broadcastable with `normal_loc`.\n      The scale parameter of the Normal used to construct the SoftmaxNormal.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      convex combination of affine parameters for `K` components.\n      `grid[..., :, n]` is the `n`-th grid point, living in the `K - 1` simplex.\n    probs:  Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      associated with each grid point.\n  \"\"\"\n  with tf.name_scope(name or \"softmax_normal_grid_and_probs\"):\n    normal_loc = tf.convert_to_tensor(value=normal_loc, name=\"normal_loc\")\n    dt = dtype_util.base_dtype(normal_loc.dtype)\n    normal_scale = tf.convert_to_tensor(\n        value=normal_scale, dtype=dt, name=\"normal_scale\")\n\n    normal_scale = maybe_check_quadrature_param(\n        normal_scale, \"normal_scale\", validate_args)\n\n    dist = normal.Normal(loc=normal_loc, scale=normal_scale)\n\n    def _get_batch_ndims():\n      \"\"\"Helper to get rank(dist.batch_shape), statically if possible.\"\"\"\n      ndims = tensorshape_util.rank(dist.batch_shape)\n      if ndims is None:\n        ndims = tf.shape(input=dist.batch_shape_tensor())[0]\n      return ndims\n    batch_ndims = _get_batch_ndims()\n\n    def _get_final_shape(qs):\n      \"\"\"Helper to build `TensorShape`.\"\"\"\n      bs = tensorshape_util.with_rank_at_least(dist.batch_shape, 1)\n      num_components = tf.compat.dimension_value(bs[-1])\n      if num_components is not None:\n        num_components += 1\n      tail = tf.TensorShape([num_components, qs])\n      return bs[:-1].concatenate(tail)\n\n    def _compute_quantiles():\n      \"\"\"Helper to build quantiles.\"\"\"\n      # Omit {0, 1} since they might lead to Inf/NaN.\n      zero = tf.zeros([], dtype=dist.dtype)\n      edges = tf.linspace(zero, 1., quadrature_size + 3)[1:-1]\n      # Expand edges so its broadcast across batch dims.\n      edges = tf.reshape(\n          edges,\n          shape=tf.concat(\n              [[-1], tf.ones([batch_ndims], dtype=tf.int32)], axis=0))\n      quantiles = dist.quantile(edges)\n      quantiles = softmax_centered_bijector.SoftmaxCentered().forward(quantiles)\n      # Cyclically permute left by one.\n      perm = tf.concat([tf.range(1, 1 + batch_ndims), [0]], axis=0)\n      quantiles = tf.transpose(a=quantiles, perm=perm)\n      tensorshape_util.set_shape(\n          quantiles, _get_final_shape(quadrature_size + 1))\n      return quantiles\n    quantiles = _compute_quantiles()\n\n    # Compute grid as quantile midpoints.\n    grid = (quantiles[..., :-1] + quantiles[..., 1:]) / 2.\n    # Set shape hints.\n    tensorshape_util.set_shape(grid, _get_final_shape(quadrature_size))\n\n    # By construction probs is constant, i.e., `1 / quadrature_size`. This is\n    # important, because non-constant probs leads to non-reparameterizable\n    # samples.\n    probs = tf.fill(\n        dims=[quadrature_size], value=1. / tf.cast(quadrature_size, dist.dtype))\n\n    return grid, probs", "code_tokens": ["def", "quadrature_scheme_softmaxnormal_quantiles", "(", "normal_loc", ",", "normal_scale", ",", "quadrature_size", ",", "validate_args", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "\"softmax_normal_grid_and_probs\"", ")", ":", "normal_loc", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "normal_loc", ",", "name", "=", "\"normal_loc\"", ")", "dt", "=", "dtype_util", ".", "base_dtype", "(", "normal_loc", ".", "dtype", ")", "normal_scale", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "normal_scale", ",", "dtype", "=", "dt", ",", "name", "=", "\"normal_scale\"", ")", "normal_scale", "=", "maybe_check_quadrature_param", "(", "normal_scale", ",", "\"normal_scale\"", ",", "validate_args", ")", "dist", "=", "normal", ".", "Normal", "(", "loc", "=", "normal_loc", ",", "scale", "=", "normal_scale", ")", "def", "_get_batch_ndims", "(", ")", ":", "\"\"\"Helper to get rank(dist.batch_shape), statically if possible.\"\"\"", "ndims", "=", "tensorshape_util", ".", "rank", "(", "dist", ".", "batch_shape", ")", "if", "ndims", "is", "None", ":", "ndims", "=", "tf", ".", "shape", "(", "input", "=", "dist", ".", "batch_shape_tensor", "(", ")", ")", "[", "0", "]", "return", "ndims", "batch_ndims", "=", "_get_batch_ndims", "(", ")", "def", "_get_final_shape", "(", "qs", ")", ":", "\"\"\"Helper to build `TensorShape`.\"\"\"", "bs", "=", "tensorshape_util", ".", "with_rank_at_least", "(", "dist", ".", "batch_shape", ",", "1", ")", "num_components", "=", "tf", ".", "compat", ".", "dimension_value", "(", "bs", "[", "-", "1", "]", ")", "if", "num_components", "is", "not", "None", ":", "num_components", "+=", "1", "tail", "=", "tf", ".", "TensorShape", "(", "[", "num_components", ",", "qs", "]", ")", "return", "bs", "[", ":", "-", "1", "]", ".", "concatenate", "(", "tail", ")", "def", "_compute_quantiles", "(", ")", ":", "\"\"\"Helper to build quantiles.\"\"\"", "# Omit {0, 1} since they might lead to Inf/NaN.", "zero", "=", "tf", ".", "zeros", "(", "[", "]", ",", "dtype", "=", "dist", ".", "dtype", ")", "edges", "=", "tf", ".", "linspace", "(", "zero", ",", "1.", ",", "quadrature_size", "+", "3", ")", "[", "1", ":", "-", "1", "]", "# Expand edges so its broadcast across batch dims.", "edges", "=", "tf", ".", "reshape", "(", "edges", ",", "shape", "=", "tf", ".", "concat", "(", "[", "[", "-", "1", "]", ",", "tf", ".", "ones", "(", "[", "batch_ndims", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "]", ",", "axis", "=", "0", ")", ")", "quantiles", "=", "dist", ".", "quantile", "(", "edges", ")", "quantiles", "=", "softmax_centered_bijector", ".", "SoftmaxCentered", "(", ")", ".", "forward", "(", "quantiles", ")", "# Cyclically permute left by one.", "perm", "=", "tf", ".", "concat", "(", "[", "tf", ".", "range", "(", "1", ",", "1", "+", "batch_ndims", ")", ",", "[", "0", "]", "]", ",", "axis", "=", "0", ")", "quantiles", "=", "tf", ".", "transpose", "(", "a", "=", "quantiles", ",", "perm", "=", "perm", ")", "tensorshape_util", ".", "set_shape", "(", "quantiles", ",", "_get_final_shape", "(", "quadrature_size", "+", "1", ")", ")", "return", "quantiles", "quantiles", "=", "_compute_quantiles", "(", ")", "# Compute grid as quantile midpoints.", "grid", "=", "(", "quantiles", "[", "...", ",", ":", "-", "1", "]", "+", "quantiles", "[", "...", ",", "1", ":", "]", ")", "/", "2.", "# Set shape hints.", "tensorshape_util", ".", "set_shape", "(", "grid", ",", "_get_final_shape", "(", "quadrature_size", ")", ")", "# By construction probs is constant, i.e., `1 / quadrature_size`. This is", "# important, because non-constant probs leads to non-reparameterizable", "# samples.", "probs", "=", "tf", ".", "fill", "(", "dims", "=", "[", "quadrature_size", "]", ",", "value", "=", "1.", "/", "tf", ".", "cast", "(", "quadrature_size", ",", "dist", ".", "dtype", ")", ")", "return", "grid", ",", "probs"], "docstring": "Use SoftmaxNormal quantiles to form quadrature on `K - 1` simplex.\n\n  A `SoftmaxNormal` random variable `Y` may be generated via\n\n  ```\n  Y = SoftmaxCentered(X),\n  X = Normal(normal_loc, normal_scale)\n  ```\n\n  Args:\n    normal_loc: `float`-like `Tensor` with shape `[b1, ..., bB, K-1]`, B>=0.\n      The location parameter of the Normal used to construct the SoftmaxNormal.\n    normal_scale: `float`-like `Tensor`. Broadcastable with `normal_loc`.\n      The scale parameter of the Normal used to construct the SoftmaxNormal.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      convex combination of affine parameters for `K` components.\n      `grid[..., :, n]` is the `n`-th grid point, living in the `K - 1` simplex.\n    probs:  Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      associated with each grid point.", "docstring_tokens": ["Use", "SoftmaxNormal", "quantiles", "to", "form", "quadrature", "on", "K", "-", "1", "simplex", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/vector_diffeomixture.py#L108-L199", "partition": "test", "index": 679, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/vector_diffeomixture.py", "func_name": "interpolate_scale", "original_string": "def interpolate_scale(grid, scale):\n  \"\"\"Helper which interpolates between two scales.\"\"\"\n  if len(scale) != 2:\n    raise NotImplementedError(\"Currently only bimixtures are supported; \"\n                              \"len(scale)={} is not 2.\".format(len(scale)))\n  deg = tf.compat.dimension_value(\n      tensorshape_util.with_rank_at_least(grid.shape, 1)[-1])\n  if deg is None:\n    raise ValueError(\"Num quadrature grid points must be known prior \"\n                     \"to graph execution.\")\n  with tf.name_scope(\"interpolate_scale\"):\n    return [linop_add_lib.add_operators([\n        linop_scale(grid[..., k, q], s)\n        for k, s in enumerate(scale)\n    ])[0] for q in range(deg)]", "language": "python", "code": "def interpolate_scale(grid, scale):\n  \"\"\"Helper which interpolates between two scales.\"\"\"\n  if len(scale) != 2:\n    raise NotImplementedError(\"Currently only bimixtures are supported; \"\n                              \"len(scale)={} is not 2.\".format(len(scale)))\n  deg = tf.compat.dimension_value(\n      tensorshape_util.with_rank_at_least(grid.shape, 1)[-1])\n  if deg is None:\n    raise ValueError(\"Num quadrature grid points must be known prior \"\n                     \"to graph execution.\")\n  with tf.name_scope(\"interpolate_scale\"):\n    return [linop_add_lib.add_operators([\n        linop_scale(grid[..., k, q], s)\n        for k, s in enumerate(scale)\n    ])[0] for q in range(deg)]", "code_tokens": ["def", "interpolate_scale", "(", "grid", ",", "scale", ")", ":", "if", "len", "(", "scale", ")", "!=", "2", ":", "raise", "NotImplementedError", "(", "\"Currently only bimixtures are supported; \"", "\"len(scale)={} is not 2.\"", ".", "format", "(", "len", "(", "scale", ")", ")", ")", "deg", "=", "tf", ".", "compat", ".", "dimension_value", "(", "tensorshape_util", ".", "with_rank_at_least", "(", "grid", ".", "shape", ",", "1", ")", "[", "-", "1", "]", ")", "if", "deg", "is", "None", ":", "raise", "ValueError", "(", "\"Num quadrature grid points must be known prior \"", "\"to graph execution.\"", ")", "with", "tf", ".", "name_scope", "(", "\"interpolate_scale\"", ")", ":", "return", "[", "linop_add_lib", ".", "add_operators", "(", "[", "linop_scale", "(", "grid", "[", "...", ",", "k", ",", "q", "]", ",", "s", ")", "for", "k", ",", "s", "in", "enumerate", "(", "scale", ")", "]", ")", "[", "0", "]", "for", "q", "in", "range", "(", "deg", ")", "]"], "docstring": "Helper which interpolates between two scales.", "docstring_tokens": ["Helper", "which", "interpolates", "between", "two", "scales", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/vector_diffeomixture.py#L882-L896", "partition": "test", "index": 683, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/real_nvp.py", "func_name": "real_nvp_default_template", "original_string": "def real_nvp_default_template(hidden_layers,\n                              shift_only=False,\n                              activation=tf.nn.relu,\n                              name=None,\n                              *args,  # pylint: disable=keyword-arg-before-vararg\n                              **kwargs):\n  \"\"\"Build a scale-and-shift function using a multi-layer neural network.\n\n  This will be wrapped in a make_template to ensure the variables are only\n  created once. It takes the `d`-dimensional input x[0:d] and returns the `D-d`\n  dimensional outputs `loc` (\"mu\") and `log_scale` (\"alpha\").\n\n  The default template does not support conditioning and will raise an\n  exception if `condition_kwargs` are passed to it. To use conditioning in\n  real nvp bijector, implement a conditioned shift/scale template that\n  handles the `condition_kwargs`.\n\n  Arguments:\n    hidden_layers: Python `list`-like of non-negative integer, scalars\n      indicating the number of units in each hidden layer. Default: `[512, 512].\n    shift_only: Python `bool` indicating if only the `shift` term shall be\n      computed (i.e. NICE bijector). Default: `False`.\n    activation: Activation function (callable). Explicitly setting to `None`\n      implies a linear activation.\n    name: A name for ops managed by this function. Default:\n      \"real_nvp_default_template\".\n    *args: `tf.layers.dense` arguments.\n    **kwargs: `tf.layers.dense` keyword arguments.\n\n  Returns:\n    shift: `Float`-like `Tensor` of shift terms (\"mu\" in\n      [Papamakarios et al.  (2016)][1]).\n    log_scale: `Float`-like `Tensor` of log(scale) terms (\"alpha\" in\n      [Papamakarios et al. (2016)][1]).\n\n  Raises:\n    NotImplementedError: if rightmost dimension of `inputs` is unknown prior to\n      graph execution, or if `condition_kwargs` is not empty.\n\n  #### References\n\n  [1]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked\n       Autoregressive Flow for Density Estimation. In _Neural Information\n       Processing Systems_, 2017. https://arxiv.org/abs/1705.07057\n  \"\"\"\n\n  with tf.compat.v2.name_scope(name or \"real_nvp_default_template\"):\n\n    def _fn(x, output_units, **condition_kwargs):\n      \"\"\"Fully connected MLP parameterized via `real_nvp_template`.\"\"\"\n      if condition_kwargs:\n        raise NotImplementedError(\n            \"Conditioning not implemented in the default template.\")\n\n      if tensorshape_util.rank(x.shape) == 1:\n        x = x[tf.newaxis, ...]\n        reshape_output = lambda x: x[0]\n      else:\n        reshape_output = lambda x: x\n      for units in hidden_layers:\n        x = tf.compat.v1.layers.dense(\n            inputs=x,\n            units=units,\n            activation=activation,\n            *args,  # pylint: disable=keyword-arg-before-vararg\n            **kwargs)\n      x = tf.compat.v1.layers.dense(\n          inputs=x,\n          units=(1 if shift_only else 2) * output_units,\n          activation=None,\n          *args,  # pylint: disable=keyword-arg-before-vararg\n          **kwargs)\n      if shift_only:\n        return reshape_output(x), None\n      shift, log_scale = tf.split(x, 2, axis=-1)\n      return reshape_output(shift), reshape_output(log_scale)\n\n    return tf.compat.v1.make_template(\"real_nvp_default_template\", _fn)", "language": "python", "code": "def real_nvp_default_template(hidden_layers,\n                              shift_only=False,\n                              activation=tf.nn.relu,\n                              name=None,\n                              *args,  # pylint: disable=keyword-arg-before-vararg\n                              **kwargs):\n  \"\"\"Build a scale-and-shift function using a multi-layer neural network.\n\n  This will be wrapped in a make_template to ensure the variables are only\n  created once. It takes the `d`-dimensional input x[0:d] and returns the `D-d`\n  dimensional outputs `loc` (\"mu\") and `log_scale` (\"alpha\").\n\n  The default template does not support conditioning and will raise an\n  exception if `condition_kwargs` are passed to it. To use conditioning in\n  real nvp bijector, implement a conditioned shift/scale template that\n  handles the `condition_kwargs`.\n\n  Arguments:\n    hidden_layers: Python `list`-like of non-negative integer, scalars\n      indicating the number of units in each hidden layer. Default: `[512, 512].\n    shift_only: Python `bool` indicating if only the `shift` term shall be\n      computed (i.e. NICE bijector). Default: `False`.\n    activation: Activation function (callable). Explicitly setting to `None`\n      implies a linear activation.\n    name: A name for ops managed by this function. Default:\n      \"real_nvp_default_template\".\n    *args: `tf.layers.dense` arguments.\n    **kwargs: `tf.layers.dense` keyword arguments.\n\n  Returns:\n    shift: `Float`-like `Tensor` of shift terms (\"mu\" in\n      [Papamakarios et al.  (2016)][1]).\n    log_scale: `Float`-like `Tensor` of log(scale) terms (\"alpha\" in\n      [Papamakarios et al. (2016)][1]).\n\n  Raises:\n    NotImplementedError: if rightmost dimension of `inputs` is unknown prior to\n      graph execution, or if `condition_kwargs` is not empty.\n\n  #### References\n\n  [1]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked\n       Autoregressive Flow for Density Estimation. In _Neural Information\n       Processing Systems_, 2017. https://arxiv.org/abs/1705.07057\n  \"\"\"\n\n  with tf.compat.v2.name_scope(name or \"real_nvp_default_template\"):\n\n    def _fn(x, output_units, **condition_kwargs):\n      \"\"\"Fully connected MLP parameterized via `real_nvp_template`.\"\"\"\n      if condition_kwargs:\n        raise NotImplementedError(\n            \"Conditioning not implemented in the default template.\")\n\n      if tensorshape_util.rank(x.shape) == 1:\n        x = x[tf.newaxis, ...]\n        reshape_output = lambda x: x[0]\n      else:\n        reshape_output = lambda x: x\n      for units in hidden_layers:\n        x = tf.compat.v1.layers.dense(\n            inputs=x,\n            units=units,\n            activation=activation,\n            *args,  # pylint: disable=keyword-arg-before-vararg\n            **kwargs)\n      x = tf.compat.v1.layers.dense(\n          inputs=x,\n          units=(1 if shift_only else 2) * output_units,\n          activation=None,\n          *args,  # pylint: disable=keyword-arg-before-vararg\n          **kwargs)\n      if shift_only:\n        return reshape_output(x), None\n      shift, log_scale = tf.split(x, 2, axis=-1)\n      return reshape_output(shift), reshape_output(log_scale)\n\n    return tf.compat.v1.make_template(\"real_nvp_default_template\", _fn)", "code_tokens": ["def", "real_nvp_default_template", "(", "hidden_layers", ",", "shift_only", "=", "False", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "name", "=", "None", ",", "*", "args", ",", "# pylint: disable=keyword-arg-before-vararg", "*", "*", "kwargs", ")", ":", "with", "tf", ".", "compat", ".", "v2", ".", "name_scope", "(", "name", "or", "\"real_nvp_default_template\"", ")", ":", "def", "_fn", "(", "x", ",", "output_units", ",", "*", "*", "condition_kwargs", ")", ":", "\"\"\"Fully connected MLP parameterized via `real_nvp_template`.\"\"\"", "if", "condition_kwargs", ":", "raise", "NotImplementedError", "(", "\"Conditioning not implemented in the default template.\"", ")", "if", "tensorshape_util", ".", "rank", "(", "x", ".", "shape", ")", "==", "1", ":", "x", "=", "x", "[", "tf", ".", "newaxis", ",", "...", "]", "reshape_output", "=", "lambda", "x", ":", "x", "[", "0", "]", "else", ":", "reshape_output", "=", "lambda", "x", ":", "x", "for", "units", "in", "hidden_layers", ":", "x", "=", "tf", ".", "compat", ".", "v1", ".", "layers", ".", "dense", "(", "inputs", "=", "x", ",", "units", "=", "units", ",", "activation", "=", "activation", ",", "*", "args", ",", "# pylint: disable=keyword-arg-before-vararg", "*", "*", "kwargs", ")", "x", "=", "tf", ".", "compat", ".", "v1", ".", "layers", ".", "dense", "(", "inputs", "=", "x", ",", "units", "=", "(", "1", "if", "shift_only", "else", "2", ")", "*", "output_units", ",", "activation", "=", "None", ",", "*", "args", ",", "# pylint: disable=keyword-arg-before-vararg", "*", "*", "kwargs", ")", "if", "shift_only", ":", "return", "reshape_output", "(", "x", ")", ",", "None", "shift", ",", "log_scale", "=", "tf", ".", "split", "(", "x", ",", "2", ",", "axis", "=", "-", "1", ")", "return", "reshape_output", "(", "shift", ")", ",", "reshape_output", "(", "log_scale", ")", "return", "tf", ".", "compat", ".", "v1", ".", "make_template", "(", "\"real_nvp_default_template\"", ",", "_fn", ")"], "docstring": "Build a scale-and-shift function using a multi-layer neural network.\n\n  This will be wrapped in a make_template to ensure the variables are only\n  created once. It takes the `d`-dimensional input x[0:d] and returns the `D-d`\n  dimensional outputs `loc` (\"mu\") and `log_scale` (\"alpha\").\n\n  The default template does not support conditioning and will raise an\n  exception if `condition_kwargs` are passed to it. To use conditioning in\n  real nvp bijector, implement a conditioned shift/scale template that\n  handles the `condition_kwargs`.\n\n  Arguments:\n    hidden_layers: Python `list`-like of non-negative integer, scalars\n      indicating the number of units in each hidden layer. Default: `[512, 512].\n    shift_only: Python `bool` indicating if only the `shift` term shall be\n      computed (i.e. NICE bijector). Default: `False`.\n    activation: Activation function (callable). Explicitly setting to `None`\n      implies a linear activation.\n    name: A name for ops managed by this function. Default:\n      \"real_nvp_default_template\".\n    *args: `tf.layers.dense` arguments.\n    **kwargs: `tf.layers.dense` keyword arguments.\n\n  Returns:\n    shift: `Float`-like `Tensor` of shift terms (\"mu\" in\n      [Papamakarios et al.  (2016)][1]).\n    log_scale: `Float`-like `Tensor` of log(scale) terms (\"alpha\" in\n      [Papamakarios et al. (2016)][1]).\n\n  Raises:\n    NotImplementedError: if rightmost dimension of `inputs` is unknown prior to\n      graph execution, or if `condition_kwargs` is not empty.\n\n  #### References\n\n  [1]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked\n       Autoregressive Flow for Density Estimation. In _Neural Information\n       Processing Systems_, 2017. https://arxiv.org/abs/1705.07057", "docstring_tokens": ["Build", "a", "scale", "-", "and", "-", "shift", "function", "using", "a", "multi", "-", "layer", "neural", "network", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/real_nvp.py#L228-L305", "partition": "test", "index": 773, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "move_dimension", "original_string": "def move_dimension(x, source_idx, dest_idx):\n  \"\"\"Move a single tensor dimension within its shape.\n\n  This is a special case of `tf.transpose()`, which applies\n  arbitrary permutations to tensor dimensions.\n\n  Args:\n    x: Tensor of rank `ndims`.\n    source_idx: Integer index into `x.shape` (negative indexing is supported).\n    dest_idx: Integer index into `x.shape` (negative indexing is supported).\n\n  Returns:\n    x_perm: Tensor of rank `ndims`, in which the dimension at original\n     index `source_idx` has been moved to new index `dest_idx`, with\n     all other dimensions retained in their original order.\n\n  Example:\n\n  ```python\n  x = tf.placeholder(shape=[200, 30, 4, 1, 6])\n  x_perm = _move_dimension(x, 1, 1) # no-op\n  x_perm = _move_dimension(x, 0, 3) # result shape [30, 4, 1, 200, 6]\n  x_perm = _move_dimension(x, 0, -2) # equivalent to previous\n  x_perm = _move_dimension(x, 4, 2) # result shape [200, 30, 6, 4, 1]\n  ```\n  \"\"\"\n  ndims = prefer_static_rank(x)\n  dtype = dtype_util.common_dtype([source_idx, dest_idx],\n                                  preferred_dtype=tf.int32)\n  source_idx = tf.convert_to_tensor(value=source_idx, dtype=dtype)\n  dest_idx = tf.convert_to_tensor(value=dest_idx, dtype=dtype)\n\n  # Handle negative indexing.\n  source_idx = pick_scalar_condition(source_idx < 0, ndims + source_idx,\n                                     source_idx)\n  dest_idx = pick_scalar_condition(dest_idx < 0, ndims + dest_idx, dest_idx)\n\n  # Construct the appropriate permutation of dimensions, depending\n  # whether the source is before or after the destination.\n  def move_left_permutation():\n    return prefer_static_value(\n        tf.concat([\n            tf.range(0, dest_idx, dtype=dtype), [source_idx],\n            tf.range(dest_idx, source_idx, dtype=dtype),\n            tf.range(source_idx + 1, ndims, dtype=dtype)\n        ],\n                  axis=0))\n\n  def move_right_permutation():\n    return prefer_static_value(\n        tf.concat([\n            tf.range(0, source_idx, dtype=dtype),\n            tf.range(source_idx + 1, dest_idx + 1, dtype=dtype), [source_idx],\n            tf.range(dest_idx + 1, ndims, dtype=dtype)\n        ],\n                  axis=0))\n\n  def x_permuted():\n    return tf.transpose(\n        a=x,\n        perm=prefer_static.cond(source_idx < dest_idx,\n                                move_right_permutation,\n                                move_left_permutation))\n\n  # One final conditional to handle the special case where source\n  # and destination indices are equal.\n  return prefer_static.cond(tf.equal(source_idx, dest_idx),\n                            lambda: x, x_permuted)", "language": "python", "code": "def move_dimension(x, source_idx, dest_idx):\n  \"\"\"Move a single tensor dimension within its shape.\n\n  This is a special case of `tf.transpose()`, which applies\n  arbitrary permutations to tensor dimensions.\n\n  Args:\n    x: Tensor of rank `ndims`.\n    source_idx: Integer index into `x.shape` (negative indexing is supported).\n    dest_idx: Integer index into `x.shape` (negative indexing is supported).\n\n  Returns:\n    x_perm: Tensor of rank `ndims`, in which the dimension at original\n     index `source_idx` has been moved to new index `dest_idx`, with\n     all other dimensions retained in their original order.\n\n  Example:\n\n  ```python\n  x = tf.placeholder(shape=[200, 30, 4, 1, 6])\n  x_perm = _move_dimension(x, 1, 1) # no-op\n  x_perm = _move_dimension(x, 0, 3) # result shape [30, 4, 1, 200, 6]\n  x_perm = _move_dimension(x, 0, -2) # equivalent to previous\n  x_perm = _move_dimension(x, 4, 2) # result shape [200, 30, 6, 4, 1]\n  ```\n  \"\"\"\n  ndims = prefer_static_rank(x)\n  dtype = dtype_util.common_dtype([source_idx, dest_idx],\n                                  preferred_dtype=tf.int32)\n  source_idx = tf.convert_to_tensor(value=source_idx, dtype=dtype)\n  dest_idx = tf.convert_to_tensor(value=dest_idx, dtype=dtype)\n\n  # Handle negative indexing.\n  source_idx = pick_scalar_condition(source_idx < 0, ndims + source_idx,\n                                     source_idx)\n  dest_idx = pick_scalar_condition(dest_idx < 0, ndims + dest_idx, dest_idx)\n\n  # Construct the appropriate permutation of dimensions, depending\n  # whether the source is before or after the destination.\n  def move_left_permutation():\n    return prefer_static_value(\n        tf.concat([\n            tf.range(0, dest_idx, dtype=dtype), [source_idx],\n            tf.range(dest_idx, source_idx, dtype=dtype),\n            tf.range(source_idx + 1, ndims, dtype=dtype)\n        ],\n                  axis=0))\n\n  def move_right_permutation():\n    return prefer_static_value(\n        tf.concat([\n            tf.range(0, source_idx, dtype=dtype),\n            tf.range(source_idx + 1, dest_idx + 1, dtype=dtype), [source_idx],\n            tf.range(dest_idx + 1, ndims, dtype=dtype)\n        ],\n                  axis=0))\n\n  def x_permuted():\n    return tf.transpose(\n        a=x,\n        perm=prefer_static.cond(source_idx < dest_idx,\n                                move_right_permutation,\n                                move_left_permutation))\n\n  # One final conditional to handle the special case where source\n  # and destination indices are equal.\n  return prefer_static.cond(tf.equal(source_idx, dest_idx),\n                            lambda: x, x_permuted)", "code_tokens": ["def", "move_dimension", "(", "x", ",", "source_idx", ",", "dest_idx", ")", ":", "ndims", "=", "prefer_static_rank", "(", "x", ")", "dtype", "=", "dtype_util", ".", "common_dtype", "(", "[", "source_idx", ",", "dest_idx", "]", ",", "preferred_dtype", "=", "tf", ".", "int32", ")", "source_idx", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "source_idx", ",", "dtype", "=", "dtype", ")", "dest_idx", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "dest_idx", ",", "dtype", "=", "dtype", ")", "# Handle negative indexing.", "source_idx", "=", "pick_scalar_condition", "(", "source_idx", "<", "0", ",", "ndims", "+", "source_idx", ",", "source_idx", ")", "dest_idx", "=", "pick_scalar_condition", "(", "dest_idx", "<", "0", ",", "ndims", "+", "dest_idx", ",", "dest_idx", ")", "# Construct the appropriate permutation of dimensions, depending", "# whether the source is before or after the destination.", "def", "move_left_permutation", "(", ")", ":", "return", "prefer_static_value", "(", "tf", ".", "concat", "(", "[", "tf", ".", "range", "(", "0", ",", "dest_idx", ",", "dtype", "=", "dtype", ")", ",", "[", "source_idx", "]", ",", "tf", ".", "range", "(", "dest_idx", ",", "source_idx", ",", "dtype", "=", "dtype", ")", ",", "tf", ".", "range", "(", "source_idx", "+", "1", ",", "ndims", ",", "dtype", "=", "dtype", ")", "]", ",", "axis", "=", "0", ")", ")", "def", "move_right_permutation", "(", ")", ":", "return", "prefer_static_value", "(", "tf", ".", "concat", "(", "[", "tf", ".", "range", "(", "0", ",", "source_idx", ",", "dtype", "=", "dtype", ")", ",", "tf", ".", "range", "(", "source_idx", "+", "1", ",", "dest_idx", "+", "1", ",", "dtype", "=", "dtype", ")", ",", "[", "source_idx", "]", ",", "tf", ".", "range", "(", "dest_idx", "+", "1", ",", "ndims", ",", "dtype", "=", "dtype", ")", "]", ",", "axis", "=", "0", ")", ")", "def", "x_permuted", "(", ")", ":", "return", "tf", ".", "transpose", "(", "a", "=", "x", ",", "perm", "=", "prefer_static", ".", "cond", "(", "source_idx", "<", "dest_idx", ",", "move_right_permutation", ",", "move_left_permutation", ")", ")", "# One final conditional to handle the special case where source", "# and destination indices are equal.", "return", "prefer_static", ".", "cond", "(", "tf", ".", "equal", "(", "source_idx", ",", "dest_idx", ")", ",", "lambda", ":", "x", ",", "x_permuted", ")"], "docstring": "Move a single tensor dimension within its shape.\n\n  This is a special case of `tf.transpose()`, which applies\n  arbitrary permutations to tensor dimensions.\n\n  Args:\n    x: Tensor of rank `ndims`.\n    source_idx: Integer index into `x.shape` (negative indexing is supported).\n    dest_idx: Integer index into `x.shape` (negative indexing is supported).\n\n  Returns:\n    x_perm: Tensor of rank `ndims`, in which the dimension at original\n     index `source_idx` has been moved to new index `dest_idx`, with\n     all other dimensions retained in their original order.\n\n  Example:\n\n  ```python\n  x = tf.placeholder(shape=[200, 30, 4, 1, 6])\n  x_perm = _move_dimension(x, 1, 1) # no-op\n  x_perm = _move_dimension(x, 0, 3) # result shape [30, 4, 1, 200, 6]\n  x_perm = _move_dimension(x, 0, -2) # equivalent to previous\n  x_perm = _move_dimension(x, 4, 2) # result shape [200, 30, 6, 4, 1]\n  ```", "docstring_tokens": ["Move", "a", "single", "tensor", "dimension", "within", "its", "shape", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L572-L639", "partition": "test", "index": 910, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "pad_mixture_dimensions", "original_string": "def pad_mixture_dimensions(x, mixture_distribution, categorical_distribution,\n                           event_ndims):\n  \"\"\"Pad dimensions of event tensors for mixture distributions.\n\n  See `Mixture._sample_n` and `MixtureSameFamily._sample_n` for usage examples.\n\n  Args:\n    x: event tensor to pad.\n    mixture_distribution: Base distribution of the mixture.\n    categorical_distribution: `Categorical` distribution that mixes the base\n      distribution.\n    event_ndims: Integer specifying the number of event dimensions in the event\n      tensor.\n\n  Returns:\n    A padded version of `x` that can broadcast with `categorical_distribution`.\n  \"\"\"\n  with tf.name_scope(\"pad_mix_dims\"):\n\n    def _get_ndims(d):\n      if tensorshape_util.rank(d.batch_shape) is not None:\n        return tensorshape_util.rank(d.batch_shape)\n      return tf.shape(input=d.batch_shape_tensor())[0]\n\n    dist_batch_ndims = _get_ndims(mixture_distribution)\n    cat_batch_ndims = _get_ndims(categorical_distribution)\n    pad_ndims = tf.where(categorical_distribution.is_scalar_batch(),\n                         dist_batch_ndims, dist_batch_ndims - cat_batch_ndims)\n    s = tf.shape(input=x)\n    x = tf.reshape(\n        x,\n        shape=tf.concat([\n            s[:-1],\n            tf.ones([pad_ndims], dtype=tf.int32),\n            s[-1:],\n            tf.ones([event_ndims], dtype=tf.int32),\n        ],\n                        axis=0))\n    return x", "language": "python", "code": "def pad_mixture_dimensions(x, mixture_distribution, categorical_distribution,\n                           event_ndims):\n  \"\"\"Pad dimensions of event tensors for mixture distributions.\n\n  See `Mixture._sample_n` and `MixtureSameFamily._sample_n` for usage examples.\n\n  Args:\n    x: event tensor to pad.\n    mixture_distribution: Base distribution of the mixture.\n    categorical_distribution: `Categorical` distribution that mixes the base\n      distribution.\n    event_ndims: Integer specifying the number of event dimensions in the event\n      tensor.\n\n  Returns:\n    A padded version of `x` that can broadcast with `categorical_distribution`.\n  \"\"\"\n  with tf.name_scope(\"pad_mix_dims\"):\n\n    def _get_ndims(d):\n      if tensorshape_util.rank(d.batch_shape) is not None:\n        return tensorshape_util.rank(d.batch_shape)\n      return tf.shape(input=d.batch_shape_tensor())[0]\n\n    dist_batch_ndims = _get_ndims(mixture_distribution)\n    cat_batch_ndims = _get_ndims(categorical_distribution)\n    pad_ndims = tf.where(categorical_distribution.is_scalar_batch(),\n                         dist_batch_ndims, dist_batch_ndims - cat_batch_ndims)\n    s = tf.shape(input=x)\n    x = tf.reshape(\n        x,\n        shape=tf.concat([\n            s[:-1],\n            tf.ones([pad_ndims], dtype=tf.int32),\n            s[-1:],\n            tf.ones([event_ndims], dtype=tf.int32),\n        ],\n                        axis=0))\n    return x", "code_tokens": ["def", "pad_mixture_dimensions", "(", "x", ",", "mixture_distribution", ",", "categorical_distribution", ",", "event_ndims", ")", ":", "with", "tf", ".", "name_scope", "(", "\"pad_mix_dims\"", ")", ":", "def", "_get_ndims", "(", "d", ")", ":", "if", "tensorshape_util", ".", "rank", "(", "d", ".", "batch_shape", ")", "is", "not", "None", ":", "return", "tensorshape_util", ".", "rank", "(", "d", ".", "batch_shape", ")", "return", "tf", ".", "shape", "(", "input", "=", "d", ".", "batch_shape_tensor", "(", ")", ")", "[", "0", "]", "dist_batch_ndims", "=", "_get_ndims", "(", "mixture_distribution", ")", "cat_batch_ndims", "=", "_get_ndims", "(", "categorical_distribution", ")", "pad_ndims", "=", "tf", ".", "where", "(", "categorical_distribution", ".", "is_scalar_batch", "(", ")", ",", "dist_batch_ndims", ",", "dist_batch_ndims", "-", "cat_batch_ndims", ")", "s", "=", "tf", ".", "shape", "(", "input", "=", "x", ")", "x", "=", "tf", ".", "reshape", "(", "x", ",", "shape", "=", "tf", ".", "concat", "(", "[", "s", "[", ":", "-", "1", "]", ",", "tf", ".", "ones", "(", "[", "pad_ndims", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "s", "[", "-", "1", ":", "]", ",", "tf", ".", "ones", "(", "[", "event_ndims", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "]", ",", "axis", "=", "0", ")", ")", "return", "x"], "docstring": "Pad dimensions of event tensors for mixture distributions.\n\n  See `Mixture._sample_n` and `MixtureSameFamily._sample_n` for usage examples.\n\n  Args:\n    x: event tensor to pad.\n    mixture_distribution: Base distribution of the mixture.\n    categorical_distribution: `Categorical` distribution that mixes the base\n      distribution.\n    event_ndims: Integer specifying the number of event dimensions in the event\n      tensor.\n\n  Returns:\n    A padded version of `x` that can broadcast with `categorical_distribution`.", "docstring_tokens": ["Pad", "dimensions", "of", "event", "tensors", "for", "mixture", "distributions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L465-L503", "partition": "test", "index": 908, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "maybe_check_scalar_distribution", "original_string": "def maybe_check_scalar_distribution(distribution, expected_base_dtype,\n                                    validate_args):\n  \"\"\"Helper which checks validity of a scalar `distribution` init arg.\n\n  Valid here means:\n\n  * `distribution` has scalar batch and event shapes.\n  * `distribution` is `FULLY_REPARAMETERIZED`\n  * `distribution` has expected dtype.\n\n  Args:\n    distribution:  `Distribution`-like object.\n    expected_base_dtype:  `TensorFlow` `dtype`.\n    validate_args:  Python `bool`.  Whether to do additional checks: (i)  check\n      that reparameterization_type is `FULLY_REPARAMETERIZED`. (ii) add\n      `tf.Assert` ops to the graph to enforce that distribution is scalar in the\n      event that this cannot be determined statically.\n\n  Returns:\n    List of `tf.Assert` ops to run to enforce validity checks that could not\n      be statically determined.  Empty if `not validate_args`.\n\n  Raises:\n    ValueError:  If validate_args and distribution is not FULLY_REPARAMETERIZED\n    ValueError:  If distribution is statically determined to not have both\n      scalar batch and scalar event shapes.\n  \"\"\"\n  if distribution.dtype != expected_base_dtype:\n    raise TypeError(\"dtype mismatch; \"\n                    \"distribution.dtype=\\\"{}\\\" is not \\\"{}\\\"\".format(\n                        dtype_util.name(distribution.dtype),\n                        dtype_util.name(expected_base_dtype)))\n\n  # Although `reparameterization_type` is a static property, we guard it by\n  # `validate_args`. This allows users to use a `distribution` which is not\n  # reparameterized itself. However, we tacitly assume that although the\n  # distribution is not reparameterized, it only depends on non-trainable\n  # variables.\n  if validate_args and (distribution.reparameterization_type !=\n                        reparameterization.FULLY_REPARAMETERIZED):\n    raise ValueError(\"Base distribution should be reparameterized or be \"\n                     \"a function of non-trainable variables; \"\n                     \"distribution.reparameterization_type = \\\"{}\\\" \"\n                     \"!= \\\"FULLY_REPARAMETERIZED\\\".\".format(\n                         distribution.reparameterization_type))\n  with tf.name_scope(\"check_distribution\"):\n    assertions = []\n\n    def check_is_scalar(is_scalar, name):\n      is_scalar_ = tf.get_static_value(is_scalar)\n      if is_scalar_ is not None:\n        if not is_scalar_:\n          raise ValueError(\"distribution must be scalar; \"\n                           \"distribution.{}=False is not True\".format(name))\n      elif validate_args:\n        assertions.append(\n            assert_util.assert_equal(\n                is_scalar,\n                True,\n                message=(\"distribution must be scalar; \"\n                         \"distribution.{}=False is not True\".format(name))))\n\n    check_is_scalar(distribution.is_scalar_event(), \"is_scalar_event\")\n    check_is_scalar(distribution.is_scalar_batch(), \"is_scalar_batch\")\n    return assertions", "language": "python", "code": "def maybe_check_scalar_distribution(distribution, expected_base_dtype,\n                                    validate_args):\n  \"\"\"Helper which checks validity of a scalar `distribution` init arg.\n\n  Valid here means:\n\n  * `distribution` has scalar batch and event shapes.\n  * `distribution` is `FULLY_REPARAMETERIZED`\n  * `distribution` has expected dtype.\n\n  Args:\n    distribution:  `Distribution`-like object.\n    expected_base_dtype:  `TensorFlow` `dtype`.\n    validate_args:  Python `bool`.  Whether to do additional checks: (i)  check\n      that reparameterization_type is `FULLY_REPARAMETERIZED`. (ii) add\n      `tf.Assert` ops to the graph to enforce that distribution is scalar in the\n      event that this cannot be determined statically.\n\n  Returns:\n    List of `tf.Assert` ops to run to enforce validity checks that could not\n      be statically determined.  Empty if `not validate_args`.\n\n  Raises:\n    ValueError:  If validate_args and distribution is not FULLY_REPARAMETERIZED\n    ValueError:  If distribution is statically determined to not have both\n      scalar batch and scalar event shapes.\n  \"\"\"\n  if distribution.dtype != expected_base_dtype:\n    raise TypeError(\"dtype mismatch; \"\n                    \"distribution.dtype=\\\"{}\\\" is not \\\"{}\\\"\".format(\n                        dtype_util.name(distribution.dtype),\n                        dtype_util.name(expected_base_dtype)))\n\n  # Although `reparameterization_type` is a static property, we guard it by\n  # `validate_args`. This allows users to use a `distribution` which is not\n  # reparameterized itself. However, we tacitly assume that although the\n  # distribution is not reparameterized, it only depends on non-trainable\n  # variables.\n  if validate_args and (distribution.reparameterization_type !=\n                        reparameterization.FULLY_REPARAMETERIZED):\n    raise ValueError(\"Base distribution should be reparameterized or be \"\n                     \"a function of non-trainable variables; \"\n                     \"distribution.reparameterization_type = \\\"{}\\\" \"\n                     \"!= \\\"FULLY_REPARAMETERIZED\\\".\".format(\n                         distribution.reparameterization_type))\n  with tf.name_scope(\"check_distribution\"):\n    assertions = []\n\n    def check_is_scalar(is_scalar, name):\n      is_scalar_ = tf.get_static_value(is_scalar)\n      if is_scalar_ is not None:\n        if not is_scalar_:\n          raise ValueError(\"distribution must be scalar; \"\n                           \"distribution.{}=False is not True\".format(name))\n      elif validate_args:\n        assertions.append(\n            assert_util.assert_equal(\n                is_scalar,\n                True,\n                message=(\"distribution must be scalar; \"\n                         \"distribution.{}=False is not True\".format(name))))\n\n    check_is_scalar(distribution.is_scalar_event(), \"is_scalar_event\")\n    check_is_scalar(distribution.is_scalar_batch(), \"is_scalar_batch\")\n    return assertions", "code_tokens": ["def", "maybe_check_scalar_distribution", "(", "distribution", ",", "expected_base_dtype", ",", "validate_args", ")", ":", "if", "distribution", ".", "dtype", "!=", "expected_base_dtype", ":", "raise", "TypeError", "(", "\"dtype mismatch; \"", "\"distribution.dtype=\\\"{}\\\" is not \\\"{}\\\"\"", ".", "format", "(", "dtype_util", ".", "name", "(", "distribution", ".", "dtype", ")", ",", "dtype_util", ".", "name", "(", "expected_base_dtype", ")", ")", ")", "# Although `reparameterization_type` is a static property, we guard it by", "# `validate_args`. This allows users to use a `distribution` which is not", "# reparameterized itself. However, we tacitly assume that although the", "# distribution is not reparameterized, it only depends on non-trainable", "# variables.", "if", "validate_args", "and", "(", "distribution", ".", "reparameterization_type", "!=", "reparameterization", ".", "FULLY_REPARAMETERIZED", ")", ":", "raise", "ValueError", "(", "\"Base distribution should be reparameterized or be \"", "\"a function of non-trainable variables; \"", "\"distribution.reparameterization_type = \\\"{}\\\" \"", "\"!= \\\"FULLY_REPARAMETERIZED\\\".\"", ".", "format", "(", "distribution", ".", "reparameterization_type", ")", ")", "with", "tf", ".", "name_scope", "(", "\"check_distribution\"", ")", ":", "assertions", "=", "[", "]", "def", "check_is_scalar", "(", "is_scalar", ",", "name", ")", ":", "is_scalar_", "=", "tf", ".", "get_static_value", "(", "is_scalar", ")", "if", "is_scalar_", "is", "not", "None", ":", "if", "not", "is_scalar_", ":", "raise", "ValueError", "(", "\"distribution must be scalar; \"", "\"distribution.{}=False is not True\"", ".", "format", "(", "name", ")", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "assert_util", ".", "assert_equal", "(", "is_scalar", ",", "True", ",", "message", "=", "(", "\"distribution must be scalar; \"", "\"distribution.{}=False is not True\"", ".", "format", "(", "name", ")", ")", ")", ")", "check_is_scalar", "(", "distribution", ".", "is_scalar_event", "(", ")", ",", "\"is_scalar_event\"", ")", "check_is_scalar", "(", "distribution", ".", "is_scalar_batch", "(", ")", ",", "\"is_scalar_batch\"", ")", "return", "assertions"], "docstring": "Helper which checks validity of a scalar `distribution` init arg.\n\n  Valid here means:\n\n  * `distribution` has scalar batch and event shapes.\n  * `distribution` is `FULLY_REPARAMETERIZED`\n  * `distribution` has expected dtype.\n\n  Args:\n    distribution:  `Distribution`-like object.\n    expected_base_dtype:  `TensorFlow` `dtype`.\n    validate_args:  Python `bool`.  Whether to do additional checks: (i)  check\n      that reparameterization_type is `FULLY_REPARAMETERIZED`. (ii) add\n      `tf.Assert` ops to the graph to enforce that distribution is scalar in the\n      event that this cannot be determined statically.\n\n  Returns:\n    List of `tf.Assert` ops to run to enforce validity checks that could not\n      be statically determined.  Empty if `not validate_args`.\n\n  Raises:\n    ValueError:  If validate_args and distribution is not FULLY_REPARAMETERIZED\n    ValueError:  If distribution is statically determined to not have both\n      scalar batch and scalar event shapes.", "docstring_tokens": ["Helper", "which", "checks", "validity", "of", "a", "scalar", "distribution", "init", "arg", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L398-L462", "partition": "test", "index": 907, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "is_diagonal_scale", "original_string": "def is_diagonal_scale(scale):\n  \"\"\"Returns `True` if `scale` is a `LinearOperator` that is known to be diag.\n\n  Args:\n    scale:  `LinearOperator` instance.\n\n  Returns:\n    Python `bool`.\n\n  Raises:\n    TypeError:  If `scale` is not a `LinearOperator`.\n  \"\"\"\n  if not isinstance(scale, tf.linalg.LinearOperator):\n    raise TypeError(\"Expected argument 'scale' to be instance of LinearOperator\"\n                    \". Found: %s\" % scale)\n  return (isinstance(scale, tf.linalg.LinearOperatorIdentity) or\n          isinstance(scale, tf.linalg.LinearOperatorScaledIdentity) or\n          isinstance(scale, tf.linalg.LinearOperatorDiag))", "language": "python", "code": "def is_diagonal_scale(scale):\n  \"\"\"Returns `True` if `scale` is a `LinearOperator` that is known to be diag.\n\n  Args:\n    scale:  `LinearOperator` instance.\n\n  Returns:\n    Python `bool`.\n\n  Raises:\n    TypeError:  If `scale` is not a `LinearOperator`.\n  \"\"\"\n  if not isinstance(scale, tf.linalg.LinearOperator):\n    raise TypeError(\"Expected argument 'scale' to be instance of LinearOperator\"\n                    \". Found: %s\" % scale)\n  return (isinstance(scale, tf.linalg.LinearOperatorIdentity) or\n          isinstance(scale, tf.linalg.LinearOperatorScaledIdentity) or\n          isinstance(scale, tf.linalg.LinearOperatorDiag))", "code_tokens": ["def", "is_diagonal_scale", "(", "scale", ")", ":", "if", "not", "isinstance", "(", "scale", ",", "tf", ".", "linalg", ".", "LinearOperator", ")", ":", "raise", "TypeError", "(", "\"Expected argument 'scale' to be instance of LinearOperator\"", "\". Found: %s\"", "%", "scale", ")", "return", "(", "isinstance", "(", "scale", ",", "tf", ".", "linalg", ".", "LinearOperatorIdentity", ")", "or", "isinstance", "(", "scale", ",", "tf", ".", "linalg", ".", "LinearOperatorScaledIdentity", ")", "or", "isinstance", "(", "scale", ",", "tf", ".", "linalg", ".", "LinearOperatorDiag", ")", ")"], "docstring": "Returns `True` if `scale` is a `LinearOperator` that is known to be diag.\n\n  Args:\n    scale:  `LinearOperator` instance.\n\n  Returns:\n    Python `bool`.\n\n  Raises:\n    TypeError:  If `scale` is not a `LinearOperator`.", "docstring_tokens": ["Returns", "True", "if", "scale", "is", "a", "LinearOperator", "that", "is", "known", "to", "be", "diag", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L378-L395", "partition": "test", "index": 906, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "shapes_from_loc_and_scale", "original_string": "def shapes_from_loc_and_scale(loc, scale, name=\"shapes_from_loc_and_scale\"):\n  \"\"\"Infer distribution batch and event shapes from a location and scale.\n\n  Location and scale family distributions determine their batch/event shape by\n  broadcasting the `loc` and `scale` args.  This helper does that broadcast,\n  statically if possible.\n\n  Batch shape broadcasts as per the normal rules.\n  We allow the `loc` event shape to broadcast up to that of `scale`.  We do not\n  allow `scale`'s event shape to change.  Therefore, the last dimension of `loc`\n  must either be size `1`, or the same as `scale.range_dimension`.\n\n  See `MultivariateNormalLinearOperator` for a usage example.\n\n  Args:\n    loc: `Tensor` (already converted to tensor) or `None`. If `None`, or\n      `rank(loc)==0`, both batch and event shape are determined by `scale`.\n    scale:  A `LinearOperator` instance.\n    name:  A string name to prepend to created ops.\n\n  Returns:\n    batch_shape:  `TensorShape` (if broadcast is done statically), or `Tensor`.\n    event_shape:  `TensorShape` (if broadcast is done statically), or `Tensor`.\n\n  Raises:\n    ValueError:  If the last dimension of `loc` is determined statically to be\n      different than the range of `scale`.\n  \"\"\"\n  if loc is not None and tensorshape_util.rank(loc.shape) == 0:\n    loc = None  # scalar loc is irrelevant to determining batch/event shape.\n  with tf.name_scope(name):\n    # Get event shape.\n    event_size = scale.range_dimension_tensor()\n    event_size_ = tf.get_static_value(event_size)\n    loc_event_size_ = (None if loc is None\n                       else tf.compat.dimension_value(loc.shape[-1]))\n\n    if event_size_ is not None and loc_event_size_ is not None:\n      # Static check that event shapes match.\n      if loc_event_size_ != 1 and loc_event_size_ != event_size_:\n        raise ValueError(\n            \"Event size of 'scale' ({}) could not be broadcast up to that \"\n            \"of 'loc' ({}).\".format(event_size_, loc_event_size_))\n    elif loc_event_size_ is not None and loc_event_size_ != 1:\n      event_size_ = loc_event_size_\n\n    if event_size_ is None:\n      event_shape = event_size[tf.newaxis]\n    else:\n      event_shape = tf.convert_to_tensor(\n          value=np.reshape(event_size_, [1]),\n          dtype=tf.int32,\n          name=\"event_shape\")\n\n    # Get batch shape.\n    batch_shape = scale.batch_shape_tensor()\n    if loc is not None:\n      loc_batch_shape = tensorshape_util.with_rank_at_least(loc.shape, 1)[:-1]\n      if tensorshape_util.rank(\n          loc.shape) is None or not tensorshape_util.is_fully_defined(\n              loc_batch_shape):\n        loc_batch_shape = tf.shape(input=loc)[:-1]\n      else:\n        loc_batch_shape = tf.convert_to_tensor(\n            value=loc_batch_shape, dtype=tf.int32, name=\"loc_batch_shape\")\n      # This is defined in the core util module.\n      batch_shape = prefer_static_broadcast_shape(batch_shape, loc_batch_shape)  # pylint: disable=undefined-variable\n      batch_shape = tf.convert_to_tensor(\n          value=batch_shape, dtype=tf.int32, name=\"batch_shape\")\n\n    return batch_shape, event_shape", "language": "python", "code": "def shapes_from_loc_and_scale(loc, scale, name=\"shapes_from_loc_and_scale\"):\n  \"\"\"Infer distribution batch and event shapes from a location and scale.\n\n  Location and scale family distributions determine their batch/event shape by\n  broadcasting the `loc` and `scale` args.  This helper does that broadcast,\n  statically if possible.\n\n  Batch shape broadcasts as per the normal rules.\n  We allow the `loc` event shape to broadcast up to that of `scale`.  We do not\n  allow `scale`'s event shape to change.  Therefore, the last dimension of `loc`\n  must either be size `1`, or the same as `scale.range_dimension`.\n\n  See `MultivariateNormalLinearOperator` for a usage example.\n\n  Args:\n    loc: `Tensor` (already converted to tensor) or `None`. If `None`, or\n      `rank(loc)==0`, both batch and event shape are determined by `scale`.\n    scale:  A `LinearOperator` instance.\n    name:  A string name to prepend to created ops.\n\n  Returns:\n    batch_shape:  `TensorShape` (if broadcast is done statically), or `Tensor`.\n    event_shape:  `TensorShape` (if broadcast is done statically), or `Tensor`.\n\n  Raises:\n    ValueError:  If the last dimension of `loc` is determined statically to be\n      different than the range of `scale`.\n  \"\"\"\n  if loc is not None and tensorshape_util.rank(loc.shape) == 0:\n    loc = None  # scalar loc is irrelevant to determining batch/event shape.\n  with tf.name_scope(name):\n    # Get event shape.\n    event_size = scale.range_dimension_tensor()\n    event_size_ = tf.get_static_value(event_size)\n    loc_event_size_ = (None if loc is None\n                       else tf.compat.dimension_value(loc.shape[-1]))\n\n    if event_size_ is not None and loc_event_size_ is not None:\n      # Static check that event shapes match.\n      if loc_event_size_ != 1 and loc_event_size_ != event_size_:\n        raise ValueError(\n            \"Event size of 'scale' ({}) could not be broadcast up to that \"\n            \"of 'loc' ({}).\".format(event_size_, loc_event_size_))\n    elif loc_event_size_ is not None and loc_event_size_ != 1:\n      event_size_ = loc_event_size_\n\n    if event_size_ is None:\n      event_shape = event_size[tf.newaxis]\n    else:\n      event_shape = tf.convert_to_tensor(\n          value=np.reshape(event_size_, [1]),\n          dtype=tf.int32,\n          name=\"event_shape\")\n\n    # Get batch shape.\n    batch_shape = scale.batch_shape_tensor()\n    if loc is not None:\n      loc_batch_shape = tensorshape_util.with_rank_at_least(loc.shape, 1)[:-1]\n      if tensorshape_util.rank(\n          loc.shape) is None or not tensorshape_util.is_fully_defined(\n              loc_batch_shape):\n        loc_batch_shape = tf.shape(input=loc)[:-1]\n      else:\n        loc_batch_shape = tf.convert_to_tensor(\n            value=loc_batch_shape, dtype=tf.int32, name=\"loc_batch_shape\")\n      # This is defined in the core util module.\n      batch_shape = prefer_static_broadcast_shape(batch_shape, loc_batch_shape)  # pylint: disable=undefined-variable\n      batch_shape = tf.convert_to_tensor(\n          value=batch_shape, dtype=tf.int32, name=\"batch_shape\")\n\n    return batch_shape, event_shape", "code_tokens": ["def", "shapes_from_loc_and_scale", "(", "loc", ",", "scale", ",", "name", "=", "\"shapes_from_loc_and_scale\"", ")", ":", "if", "loc", "is", "not", "None", "and", "tensorshape_util", ".", "rank", "(", "loc", ".", "shape", ")", "==", "0", ":", "loc", "=", "None", "# scalar loc is irrelevant to determining batch/event shape.", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "# Get event shape.", "event_size", "=", "scale", ".", "range_dimension_tensor", "(", ")", "event_size_", "=", "tf", ".", "get_static_value", "(", "event_size", ")", "loc_event_size_", "=", "(", "None", "if", "loc", "is", "None", "else", "tf", ".", "compat", ".", "dimension_value", "(", "loc", ".", "shape", "[", "-", "1", "]", ")", ")", "if", "event_size_", "is", "not", "None", "and", "loc_event_size_", "is", "not", "None", ":", "# Static check that event shapes match.", "if", "loc_event_size_", "!=", "1", "and", "loc_event_size_", "!=", "event_size_", ":", "raise", "ValueError", "(", "\"Event size of 'scale' ({}) could not be broadcast up to that \"", "\"of 'loc' ({}).\"", ".", "format", "(", "event_size_", ",", "loc_event_size_", ")", ")", "elif", "loc_event_size_", "is", "not", "None", "and", "loc_event_size_", "!=", "1", ":", "event_size_", "=", "loc_event_size_", "if", "event_size_", "is", "None", ":", "event_shape", "=", "event_size", "[", "tf", ".", "newaxis", "]", "else", ":", "event_shape", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "np", ".", "reshape", "(", "event_size_", ",", "[", "1", "]", ")", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"event_shape\"", ")", "# Get batch shape.", "batch_shape", "=", "scale", ".", "batch_shape_tensor", "(", ")", "if", "loc", "is", "not", "None", ":", "loc_batch_shape", "=", "tensorshape_util", ".", "with_rank_at_least", "(", "loc", ".", "shape", ",", "1", ")", "[", ":", "-", "1", "]", "if", "tensorshape_util", ".", "rank", "(", "loc", ".", "shape", ")", "is", "None", "or", "not", "tensorshape_util", ".", "is_fully_defined", "(", "loc_batch_shape", ")", ":", "loc_batch_shape", "=", "tf", ".", "shape", "(", "input", "=", "loc", ")", "[", ":", "-", "1", "]", "else", ":", "loc_batch_shape", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "loc_batch_shape", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"loc_batch_shape\"", ")", "# This is defined in the core util module.", "batch_shape", "=", "prefer_static_broadcast_shape", "(", "batch_shape", ",", "loc_batch_shape", ")", "# pylint: disable=undefined-variable", "batch_shape", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "batch_shape", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"batch_shape\"", ")", "return", "batch_shape", ",", "event_shape"], "docstring": "Infer distribution batch and event shapes from a location and scale.\n\n  Location and scale family distributions determine their batch/event shape by\n  broadcasting the `loc` and `scale` args.  This helper does that broadcast,\n  statically if possible.\n\n  Batch shape broadcasts as per the normal rules.\n  We allow the `loc` event shape to broadcast up to that of `scale`.  We do not\n  allow `scale`'s event shape to change.  Therefore, the last dimension of `loc`\n  must either be size `1`, or the same as `scale.range_dimension`.\n\n  See `MultivariateNormalLinearOperator` for a usage example.\n\n  Args:\n    loc: `Tensor` (already converted to tensor) or `None`. If `None`, or\n      `rank(loc)==0`, both batch and event shape are determined by `scale`.\n    scale:  A `LinearOperator` instance.\n    name:  A string name to prepend to created ops.\n\n  Returns:\n    batch_shape:  `TensorShape` (if broadcast is done statically), or `Tensor`.\n    event_shape:  `TensorShape` (if broadcast is done statically), or `Tensor`.\n\n  Raises:\n    ValueError:  If the last dimension of `loc` is determined statically to be\n      different than the range of `scale`.", "docstring_tokens": ["Infer", "distribution", "batch", "and", "event", "shapes", "from", "a", "location", "and", "scale", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L281-L351", "partition": "test", "index": 905, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "make_diag_scale", "original_string": "def make_diag_scale(loc=None,\n                    scale_diag=None,\n                    scale_identity_multiplier=None,\n                    shape_hint=None,\n                    validate_args=False,\n                    assert_positive=False,\n                    name=None,\n                    dtype=None):\n  \"\"\"Creates a LinearOperator representing a diagonal matrix.\n\n  Args:\n    loc: Floating-point `Tensor`. This is used for inferring shape in the case\n      where only `scale_identity_multiplier` is set.\n    scale_diag: Floating-point `Tensor` representing the diagonal matrix.\n      `scale_diag` has shape [N1, N2, ...  k], which represents a k x k diagonal\n      matrix. When `None` no diagonal term is added to the LinearOperator.\n    scale_identity_multiplier: floating point rank 0 `Tensor` representing a\n      scaling done to the identity matrix. When `scale_identity_multiplier =\n      scale_diag = scale_tril = None` then `scale += IdentityMatrix`. Otherwise\n      no scaled-identity-matrix is added to `scale`.\n    shape_hint: scalar integer `Tensor` representing a hint at the dimension of\n      the identity matrix when only `scale_identity_multiplier` is set.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness.\n    assert_positive: Python `bool` indicating whether LinearOperator should be\n      checked for being positive definite.\n    name: Python `str` name given to ops managed by this object.\n    dtype: TF `DType` to prefer when converting args to `Tensor`s. Else, we fall\n      back to a compatible dtype across all of `loc`, `scale_diag`, and\n      `scale_identity_multiplier`.\n\n  Returns:\n    `LinearOperator` representing a lower triangular matrix.\n\n  Raises:\n    ValueError:  If only `scale_identity_multiplier` is set and `loc` and\n      `shape_hint` are both None.\n  \"\"\"\n\n  def _maybe_attach_assertion(x):\n    if not validate_args:\n      return x\n    if assert_positive:\n      return with_dependencies([\n          assert_util.assert_positive(\n              x, message=\"diagonal part must be positive\"),\n      ], x)\n    return with_dependencies([\n        assert_util.assert_none_equal(\n            x, tf.zeros([], x.dtype), message=\"diagonal part must be non-zero\")\n    ], x)\n\n  with tf.name_scope(name or \"make_diag_scale\"):\n    if dtype is None:\n      dtype = dtype_util.common_dtype(\n          [loc, scale_diag, scale_identity_multiplier],\n          preferred_dtype=tf.float32)\n    loc = _convert_to_tensor(loc, name=\"loc\", dtype=dtype)\n    scale_diag = _convert_to_tensor(scale_diag, name=\"scale_diag\", dtype=dtype)\n    scale_identity_multiplier = _convert_to_tensor(\n        scale_identity_multiplier,\n        name=\"scale_identity_multiplier\",\n        dtype=dtype)\n\n    if scale_diag is not None:\n      if scale_identity_multiplier is not None:\n        scale_diag += scale_identity_multiplier[..., tf.newaxis]\n      return tf.linalg.LinearOperatorDiag(\n          diag=_maybe_attach_assertion(scale_diag),\n          is_non_singular=True,\n          is_self_adjoint=True,\n          is_positive_definite=assert_positive)\n\n    if loc is None and shape_hint is None:\n      raise ValueError(\"Cannot infer `event_shape` unless `loc` or \"\n                       \"`shape_hint` is specified.\")\n\n    num_rows = shape_hint\n    del shape_hint\n    if num_rows is None:\n      num_rows = tf.compat.dimension_value(loc.shape[-1])\n      if num_rows is None:\n        num_rows = tf.shape(input=loc)[-1]\n\n    if scale_identity_multiplier is None:\n      return tf.linalg.LinearOperatorIdentity(\n          num_rows=num_rows,\n          dtype=dtype,\n          is_self_adjoint=True,\n          is_positive_definite=True,\n          assert_proper_shapes=validate_args)\n\n    return tf.linalg.LinearOperatorScaledIdentity(\n        num_rows=num_rows,\n        multiplier=_maybe_attach_assertion(scale_identity_multiplier),\n        is_non_singular=True,\n        is_self_adjoint=True,\n        is_positive_definite=assert_positive,\n        assert_proper_shapes=validate_args)", "language": "python", "code": "def make_diag_scale(loc=None,\n                    scale_diag=None,\n                    scale_identity_multiplier=None,\n                    shape_hint=None,\n                    validate_args=False,\n                    assert_positive=False,\n                    name=None,\n                    dtype=None):\n  \"\"\"Creates a LinearOperator representing a diagonal matrix.\n\n  Args:\n    loc: Floating-point `Tensor`. This is used for inferring shape in the case\n      where only `scale_identity_multiplier` is set.\n    scale_diag: Floating-point `Tensor` representing the diagonal matrix.\n      `scale_diag` has shape [N1, N2, ...  k], which represents a k x k diagonal\n      matrix. When `None` no diagonal term is added to the LinearOperator.\n    scale_identity_multiplier: floating point rank 0 `Tensor` representing a\n      scaling done to the identity matrix. When `scale_identity_multiplier =\n      scale_diag = scale_tril = None` then `scale += IdentityMatrix`. Otherwise\n      no scaled-identity-matrix is added to `scale`.\n    shape_hint: scalar integer `Tensor` representing a hint at the dimension of\n      the identity matrix when only `scale_identity_multiplier` is set.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness.\n    assert_positive: Python `bool` indicating whether LinearOperator should be\n      checked for being positive definite.\n    name: Python `str` name given to ops managed by this object.\n    dtype: TF `DType` to prefer when converting args to `Tensor`s. Else, we fall\n      back to a compatible dtype across all of `loc`, `scale_diag`, and\n      `scale_identity_multiplier`.\n\n  Returns:\n    `LinearOperator` representing a lower triangular matrix.\n\n  Raises:\n    ValueError:  If only `scale_identity_multiplier` is set and `loc` and\n      `shape_hint` are both None.\n  \"\"\"\n\n  def _maybe_attach_assertion(x):\n    if not validate_args:\n      return x\n    if assert_positive:\n      return with_dependencies([\n          assert_util.assert_positive(\n              x, message=\"diagonal part must be positive\"),\n      ], x)\n    return with_dependencies([\n        assert_util.assert_none_equal(\n            x, tf.zeros([], x.dtype), message=\"diagonal part must be non-zero\")\n    ], x)\n\n  with tf.name_scope(name or \"make_diag_scale\"):\n    if dtype is None:\n      dtype = dtype_util.common_dtype(\n          [loc, scale_diag, scale_identity_multiplier],\n          preferred_dtype=tf.float32)\n    loc = _convert_to_tensor(loc, name=\"loc\", dtype=dtype)\n    scale_diag = _convert_to_tensor(scale_diag, name=\"scale_diag\", dtype=dtype)\n    scale_identity_multiplier = _convert_to_tensor(\n        scale_identity_multiplier,\n        name=\"scale_identity_multiplier\",\n        dtype=dtype)\n\n    if scale_diag is not None:\n      if scale_identity_multiplier is not None:\n        scale_diag += scale_identity_multiplier[..., tf.newaxis]\n      return tf.linalg.LinearOperatorDiag(\n          diag=_maybe_attach_assertion(scale_diag),\n          is_non_singular=True,\n          is_self_adjoint=True,\n          is_positive_definite=assert_positive)\n\n    if loc is None and shape_hint is None:\n      raise ValueError(\"Cannot infer `event_shape` unless `loc` or \"\n                       \"`shape_hint` is specified.\")\n\n    num_rows = shape_hint\n    del shape_hint\n    if num_rows is None:\n      num_rows = tf.compat.dimension_value(loc.shape[-1])\n      if num_rows is None:\n        num_rows = tf.shape(input=loc)[-1]\n\n    if scale_identity_multiplier is None:\n      return tf.linalg.LinearOperatorIdentity(\n          num_rows=num_rows,\n          dtype=dtype,\n          is_self_adjoint=True,\n          is_positive_definite=True,\n          assert_proper_shapes=validate_args)\n\n    return tf.linalg.LinearOperatorScaledIdentity(\n        num_rows=num_rows,\n        multiplier=_maybe_attach_assertion(scale_identity_multiplier),\n        is_non_singular=True,\n        is_self_adjoint=True,\n        is_positive_definite=assert_positive,\n        assert_proper_shapes=validate_args)", "code_tokens": ["def", "make_diag_scale", "(", "loc", "=", "None", ",", "scale_diag", "=", "None", ",", "scale_identity_multiplier", "=", "None", ",", "shape_hint", "=", "None", ",", "validate_args", "=", "False", ",", "assert_positive", "=", "False", ",", "name", "=", "None", ",", "dtype", "=", "None", ")", ":", "def", "_maybe_attach_assertion", "(", "x", ")", ":", "if", "not", "validate_args", ":", "return", "x", "if", "assert_positive", ":", "return", "with_dependencies", "(", "[", "assert_util", ".", "assert_positive", "(", "x", ",", "message", "=", "\"diagonal part must be positive\"", ")", ",", "]", ",", "x", ")", "return", "with_dependencies", "(", "[", "assert_util", ".", "assert_none_equal", "(", "x", ",", "tf", ".", "zeros", "(", "[", "]", ",", "x", ".", "dtype", ")", ",", "message", "=", "\"diagonal part must be non-zero\"", ")", "]", ",", "x", ")", "with", "tf", ".", "name_scope", "(", "name", "or", "\"make_diag_scale\"", ")", ":", "if", "dtype", "is", "None", ":", "dtype", "=", "dtype_util", ".", "common_dtype", "(", "[", "loc", ",", "scale_diag", ",", "scale_identity_multiplier", "]", ",", "preferred_dtype", "=", "tf", ".", "float32", ")", "loc", "=", "_convert_to_tensor", "(", "loc", ",", "name", "=", "\"loc\"", ",", "dtype", "=", "dtype", ")", "scale_diag", "=", "_convert_to_tensor", "(", "scale_diag", ",", "name", "=", "\"scale_diag\"", ",", "dtype", "=", "dtype", ")", "scale_identity_multiplier", "=", "_convert_to_tensor", "(", "scale_identity_multiplier", ",", "name", "=", "\"scale_identity_multiplier\"", ",", "dtype", "=", "dtype", ")", "if", "scale_diag", "is", "not", "None", ":", "if", "scale_identity_multiplier", "is", "not", "None", ":", "scale_diag", "+=", "scale_identity_multiplier", "[", "...", ",", "tf", ".", "newaxis", "]", "return", "tf", ".", "linalg", ".", "LinearOperatorDiag", "(", "diag", "=", "_maybe_attach_assertion", "(", "scale_diag", ")", ",", "is_non_singular", "=", "True", ",", "is_self_adjoint", "=", "True", ",", "is_positive_definite", "=", "assert_positive", ")", "if", "loc", "is", "None", "and", "shape_hint", "is", "None", ":", "raise", "ValueError", "(", "\"Cannot infer `event_shape` unless `loc` or \"", "\"`shape_hint` is specified.\"", ")", "num_rows", "=", "shape_hint", "del", "shape_hint", "if", "num_rows", "is", "None", ":", "num_rows", "=", "tf", ".", "compat", ".", "dimension_value", "(", "loc", ".", "shape", "[", "-", "1", "]", ")", "if", "num_rows", "is", "None", ":", "num_rows", "=", "tf", ".", "shape", "(", "input", "=", "loc", ")", "[", "-", "1", "]", "if", "scale_identity_multiplier", "is", "None", ":", "return", "tf", ".", "linalg", ".", "LinearOperatorIdentity", "(", "num_rows", "=", "num_rows", ",", "dtype", "=", "dtype", ",", "is_self_adjoint", "=", "True", ",", "is_positive_definite", "=", "True", ",", "assert_proper_shapes", "=", "validate_args", ")", "return", "tf", ".", "linalg", ".", "LinearOperatorScaledIdentity", "(", "num_rows", "=", "num_rows", ",", "multiplier", "=", "_maybe_attach_assertion", "(", "scale_identity_multiplier", ")", ",", "is_non_singular", "=", "True", ",", "is_self_adjoint", "=", "True", ",", "is_positive_definite", "=", "assert_positive", ",", "assert_proper_shapes", "=", "validate_args", ")"], "docstring": "Creates a LinearOperator representing a diagonal matrix.\n\n  Args:\n    loc: Floating-point `Tensor`. This is used for inferring shape in the case\n      where only `scale_identity_multiplier` is set.\n    scale_diag: Floating-point `Tensor` representing the diagonal matrix.\n      `scale_diag` has shape [N1, N2, ...  k], which represents a k x k diagonal\n      matrix. When `None` no diagonal term is added to the LinearOperator.\n    scale_identity_multiplier: floating point rank 0 `Tensor` representing a\n      scaling done to the identity matrix. When `scale_identity_multiplier =\n      scale_diag = scale_tril = None` then `scale += IdentityMatrix`. Otherwise\n      no scaled-identity-matrix is added to `scale`.\n    shape_hint: scalar integer `Tensor` representing a hint at the dimension of\n      the identity matrix when only `scale_identity_multiplier` is set.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness.\n    assert_positive: Python `bool` indicating whether LinearOperator should be\n      checked for being positive definite.\n    name: Python `str` name given to ops managed by this object.\n    dtype: TF `DType` to prefer when converting args to `Tensor`s. Else, we fall\n      back to a compatible dtype across all of `loc`, `scale_diag`, and\n      `scale_identity_multiplier`.\n\n  Returns:\n    `LinearOperator` representing a lower triangular matrix.\n\n  Raises:\n    ValueError:  If only `scale_identity_multiplier` is set and `loc` and\n      `shape_hint` are both None.", "docstring_tokens": ["Creates", "a", "LinearOperator", "representing", "a", "diagonal", "matrix", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L180-L278", "partition": "test", "index": 904, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "make_tril_scale", "original_string": "def make_tril_scale(loc=None,\n                    scale_tril=None,\n                    scale_diag=None,\n                    scale_identity_multiplier=None,\n                    shape_hint=None,\n                    validate_args=False,\n                    assert_positive=False,\n                    name=None):\n  \"\"\"Creates a LinearOperator representing a lower triangular matrix.\n\n  Args:\n    loc: Floating-point `Tensor`. This is used for inferring shape in the case\n      where only `scale_identity_multiplier` is set.\n    scale_tril: Floating-point `Tensor` representing the diagonal matrix.\n      `scale_diag` has shape [N1, N2, ...  k, k], which represents a k x k lower\n      triangular matrix. When `None` no `scale_tril` term is added to the\n      LinearOperator. The upper triangular elements above the diagonal are\n      ignored.\n    scale_diag: Floating-point `Tensor` representing the diagonal matrix.\n      `scale_diag` has shape [N1, N2, ...  k], which represents a k x k diagonal\n      matrix. When `None` no diagonal term is added to the LinearOperator.\n    scale_identity_multiplier: floating point rank 0 `Tensor` representing a\n      scaling done to the identity matrix. When `scale_identity_multiplier =\n      scale_diag = scale_tril = None` then `scale += IdentityMatrix`. Otherwise\n      no scaled-identity-matrix is added to `scale`.\n    shape_hint: scalar integer `Tensor` representing a hint at the dimension of\n      the identity matrix when only `scale_identity_multiplier` is set.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness.\n    assert_positive: Python `bool` indicating whether LinearOperator should be\n      checked for being positive definite.\n    name: Python `str` name given to ops managed by this object.\n\n  Returns:\n    `LinearOperator` representing a lower triangular matrix.\n\n  Raises:\n    ValueError:  If only `scale_identity_multiplier` is set and `loc` and\n      `shape_hint` are both None.\n  \"\"\"\n\n  def _maybe_attach_assertion(x):\n    if not validate_args:\n      return x\n    if assert_positive:\n      return with_dependencies([\n          assert_util.assert_positive(\n              tf.linalg.diag_part(x), message=\"diagonal part must be positive\"),\n      ], x)\n    return with_dependencies([\n        assert_util.assert_none_equal(\n            tf.linalg.diag_part(x),\n            tf.zeros([], x.dtype),\n            message=\"diagonal part must be non-zero\"),\n    ], x)\n\n  with tf.name_scope(name or \"make_tril_scale\"):\n\n    dtype = dtype_util.common_dtype(\n        [loc, scale_tril, scale_diag, scale_identity_multiplier],\n        preferred_dtype=tf.float32)\n    loc = _convert_to_tensor(loc, name=\"loc\", dtype=dtype)\n    scale_tril = _convert_to_tensor(scale_tril, name=\"scale_tril\", dtype=dtype)\n    scale_diag = _convert_to_tensor(scale_diag, name=\"scale_diag\", dtype=dtype)\n    scale_identity_multiplier = _convert_to_tensor(\n        scale_identity_multiplier,\n        name=\"scale_identity_multiplier\",\n        dtype=dtype)\n\n  if scale_tril is not None:\n    scale_tril = tf.linalg.band_part(scale_tril, -1, 0)  # Zero out TriU.\n    tril_diag = tf.linalg.diag_part(scale_tril)\n    if scale_diag is not None:\n      tril_diag += scale_diag\n    if scale_identity_multiplier is not None:\n      tril_diag += scale_identity_multiplier[..., tf.newaxis]\n\n    scale_tril = tf.linalg.set_diag(scale_tril, tril_diag)\n\n    return tf.linalg.LinearOperatorLowerTriangular(\n        tril=_maybe_attach_assertion(scale_tril),\n        is_non_singular=True,\n        is_self_adjoint=False,\n        is_positive_definite=assert_positive)\n\n  return make_diag_scale(\n      loc=loc,\n      scale_diag=scale_diag,\n      scale_identity_multiplier=scale_identity_multiplier,\n      shape_hint=shape_hint,\n      validate_args=validate_args,\n      assert_positive=assert_positive,\n      name=name)", "language": "python", "code": "def make_tril_scale(loc=None,\n                    scale_tril=None,\n                    scale_diag=None,\n                    scale_identity_multiplier=None,\n                    shape_hint=None,\n                    validate_args=False,\n                    assert_positive=False,\n                    name=None):\n  \"\"\"Creates a LinearOperator representing a lower triangular matrix.\n\n  Args:\n    loc: Floating-point `Tensor`. This is used for inferring shape in the case\n      where only `scale_identity_multiplier` is set.\n    scale_tril: Floating-point `Tensor` representing the diagonal matrix.\n      `scale_diag` has shape [N1, N2, ...  k, k], which represents a k x k lower\n      triangular matrix. When `None` no `scale_tril` term is added to the\n      LinearOperator. The upper triangular elements above the diagonal are\n      ignored.\n    scale_diag: Floating-point `Tensor` representing the diagonal matrix.\n      `scale_diag` has shape [N1, N2, ...  k], which represents a k x k diagonal\n      matrix. When `None` no diagonal term is added to the LinearOperator.\n    scale_identity_multiplier: floating point rank 0 `Tensor` representing a\n      scaling done to the identity matrix. When `scale_identity_multiplier =\n      scale_diag = scale_tril = None` then `scale += IdentityMatrix`. Otherwise\n      no scaled-identity-matrix is added to `scale`.\n    shape_hint: scalar integer `Tensor` representing a hint at the dimension of\n      the identity matrix when only `scale_identity_multiplier` is set.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness.\n    assert_positive: Python `bool` indicating whether LinearOperator should be\n      checked for being positive definite.\n    name: Python `str` name given to ops managed by this object.\n\n  Returns:\n    `LinearOperator` representing a lower triangular matrix.\n\n  Raises:\n    ValueError:  If only `scale_identity_multiplier` is set and `loc` and\n      `shape_hint` are both None.\n  \"\"\"\n\n  def _maybe_attach_assertion(x):\n    if not validate_args:\n      return x\n    if assert_positive:\n      return with_dependencies([\n          assert_util.assert_positive(\n              tf.linalg.diag_part(x), message=\"diagonal part must be positive\"),\n      ], x)\n    return with_dependencies([\n        assert_util.assert_none_equal(\n            tf.linalg.diag_part(x),\n            tf.zeros([], x.dtype),\n            message=\"diagonal part must be non-zero\"),\n    ], x)\n\n  with tf.name_scope(name or \"make_tril_scale\"):\n\n    dtype = dtype_util.common_dtype(\n        [loc, scale_tril, scale_diag, scale_identity_multiplier],\n        preferred_dtype=tf.float32)\n    loc = _convert_to_tensor(loc, name=\"loc\", dtype=dtype)\n    scale_tril = _convert_to_tensor(scale_tril, name=\"scale_tril\", dtype=dtype)\n    scale_diag = _convert_to_tensor(scale_diag, name=\"scale_diag\", dtype=dtype)\n    scale_identity_multiplier = _convert_to_tensor(\n        scale_identity_multiplier,\n        name=\"scale_identity_multiplier\",\n        dtype=dtype)\n\n  if scale_tril is not None:\n    scale_tril = tf.linalg.band_part(scale_tril, -1, 0)  # Zero out TriU.\n    tril_diag = tf.linalg.diag_part(scale_tril)\n    if scale_diag is not None:\n      tril_diag += scale_diag\n    if scale_identity_multiplier is not None:\n      tril_diag += scale_identity_multiplier[..., tf.newaxis]\n\n    scale_tril = tf.linalg.set_diag(scale_tril, tril_diag)\n\n    return tf.linalg.LinearOperatorLowerTriangular(\n        tril=_maybe_attach_assertion(scale_tril),\n        is_non_singular=True,\n        is_self_adjoint=False,\n        is_positive_definite=assert_positive)\n\n  return make_diag_scale(\n      loc=loc,\n      scale_diag=scale_diag,\n      scale_identity_multiplier=scale_identity_multiplier,\n      shape_hint=shape_hint,\n      validate_args=validate_args,\n      assert_positive=assert_positive,\n      name=name)", "code_tokens": ["def", "make_tril_scale", "(", "loc", "=", "None", ",", "scale_tril", "=", "None", ",", "scale_diag", "=", "None", ",", "scale_identity_multiplier", "=", "None", ",", "shape_hint", "=", "None", ",", "validate_args", "=", "False", ",", "assert_positive", "=", "False", ",", "name", "=", "None", ")", ":", "def", "_maybe_attach_assertion", "(", "x", ")", ":", "if", "not", "validate_args", ":", "return", "x", "if", "assert_positive", ":", "return", "with_dependencies", "(", "[", "assert_util", ".", "assert_positive", "(", "tf", ".", "linalg", ".", "diag_part", "(", "x", ")", ",", "message", "=", "\"diagonal part must be positive\"", ")", ",", "]", ",", "x", ")", "return", "with_dependencies", "(", "[", "assert_util", ".", "assert_none_equal", "(", "tf", ".", "linalg", ".", "diag_part", "(", "x", ")", ",", "tf", ".", "zeros", "(", "[", "]", ",", "x", ".", "dtype", ")", ",", "message", "=", "\"diagonal part must be non-zero\"", ")", ",", "]", ",", "x", ")", "with", "tf", ".", "name_scope", "(", "name", "or", "\"make_tril_scale\"", ")", ":", "dtype", "=", "dtype_util", ".", "common_dtype", "(", "[", "loc", ",", "scale_tril", ",", "scale_diag", ",", "scale_identity_multiplier", "]", ",", "preferred_dtype", "=", "tf", ".", "float32", ")", "loc", "=", "_convert_to_tensor", "(", "loc", ",", "name", "=", "\"loc\"", ",", "dtype", "=", "dtype", ")", "scale_tril", "=", "_convert_to_tensor", "(", "scale_tril", ",", "name", "=", "\"scale_tril\"", ",", "dtype", "=", "dtype", ")", "scale_diag", "=", "_convert_to_tensor", "(", "scale_diag", ",", "name", "=", "\"scale_diag\"", ",", "dtype", "=", "dtype", ")", "scale_identity_multiplier", "=", "_convert_to_tensor", "(", "scale_identity_multiplier", ",", "name", "=", "\"scale_identity_multiplier\"", ",", "dtype", "=", "dtype", ")", "if", "scale_tril", "is", "not", "None", ":", "scale_tril", "=", "tf", ".", "linalg", ".", "band_part", "(", "scale_tril", ",", "-", "1", ",", "0", ")", "# Zero out TriU.", "tril_diag", "=", "tf", ".", "linalg", ".", "diag_part", "(", "scale_tril", ")", "if", "scale_diag", "is", "not", "None", ":", "tril_diag", "+=", "scale_diag", "if", "scale_identity_multiplier", "is", "not", "None", ":", "tril_diag", "+=", "scale_identity_multiplier", "[", "...", ",", "tf", ".", "newaxis", "]", "scale_tril", "=", "tf", ".", "linalg", ".", "set_diag", "(", "scale_tril", ",", "tril_diag", ")", "return", "tf", ".", "linalg", ".", "LinearOperatorLowerTriangular", "(", "tril", "=", "_maybe_attach_assertion", "(", "scale_tril", ")", ",", "is_non_singular", "=", "True", ",", "is_self_adjoint", "=", "False", ",", "is_positive_definite", "=", "assert_positive", ")", "return", "make_diag_scale", "(", "loc", "=", "loc", ",", "scale_diag", "=", "scale_diag", ",", "scale_identity_multiplier", "=", "scale_identity_multiplier", ",", "shape_hint", "=", "shape_hint", ",", "validate_args", "=", "validate_args", ",", "assert_positive", "=", "assert_positive", ",", "name", "=", "name", ")"], "docstring": "Creates a LinearOperator representing a lower triangular matrix.\n\n  Args:\n    loc: Floating-point `Tensor`. This is used for inferring shape in the case\n      where only `scale_identity_multiplier` is set.\n    scale_tril: Floating-point `Tensor` representing the diagonal matrix.\n      `scale_diag` has shape [N1, N2, ...  k, k], which represents a k x k lower\n      triangular matrix. When `None` no `scale_tril` term is added to the\n      LinearOperator. The upper triangular elements above the diagonal are\n      ignored.\n    scale_diag: Floating-point `Tensor` representing the diagonal matrix.\n      `scale_diag` has shape [N1, N2, ...  k], which represents a k x k diagonal\n      matrix. When `None` no diagonal term is added to the LinearOperator.\n    scale_identity_multiplier: floating point rank 0 `Tensor` representing a\n      scaling done to the identity matrix. When `scale_identity_multiplier =\n      scale_diag = scale_tril = None` then `scale += IdentityMatrix`. Otherwise\n      no scaled-identity-matrix is added to `scale`.\n    shape_hint: scalar integer `Tensor` representing a hint at the dimension of\n      the identity matrix when only `scale_identity_multiplier` is set.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness.\n    assert_positive: Python `bool` indicating whether LinearOperator should be\n      checked for being positive definite.\n    name: Python `str` name given to ops managed by this object.\n\n  Returns:\n    `LinearOperator` representing a lower triangular matrix.\n\n  Raises:\n    ValueError:  If only `scale_identity_multiplier` is set and `loc` and\n      `shape_hint` are both None.", "docstring_tokens": ["Creates", "a", "LinearOperator", "representing", "a", "lower", "triangular", "matrix", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L85-L177", "partition": "test", "index": 903, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "mixture_stddev", "original_string": "def mixture_stddev(mixture_weight_vector, mean_vector, stddev_vector):\n  \"\"\"Computes the standard deviation of a mixture distribution.\n\n  This function works regardless of the component distribution, so long as\n  each component's mean and standard deviation can be provided.\n\n  Args:\n    mixture_weight_vector: A 2D tensor with shape [batch_size, num_components]\n    mean_vector: A 2D tensor of mixture component means. Has shape `[batch_size,\n      num_components]`.\n    stddev_vector: A 2D tensor of mixture component standard deviations. Has\n      shape `[batch_size, num_components]`.\n\n  Returns:\n    A 1D tensor of shape `[batch_size]` representing the standard deviation of\n    the mixture distribution with given weights and component means and standard\n    deviations.\n  Raises:\n    ValueError: If the shapes of the input tensors are not as expected.\n  \"\"\"\n  tensorshape_util.assert_has_rank(mixture_weight_vector.shape, 2)\n  if not tensorshape_util.is_compatible_with(mean_vector.shape,\n                                             mixture_weight_vector.shape):\n    raise ValueError(\"Expecting means to have same shape as mixture weights.\")\n  if not tensorshape_util.is_compatible_with(stddev_vector.shape,\n                                             mixture_weight_vector.shape):\n    raise ValueError(\"Expecting stddevs to have same shape as mixture weights.\")\n\n  # Reshape the distribution parameters for batched vectorized dot products.\n  pi_for_dot_prod = tf.expand_dims(mixture_weight_vector, axis=1)\n  mu_for_dot_prod = tf.expand_dims(mean_vector, axis=2)\n  sigma_for_dot_prod = tf.expand_dims(stddev_vector, axis=2)\n\n  # weighted average of component means under mixture distribution.\n  mean_wa = tf.matmul(pi_for_dot_prod, mu_for_dot_prod)\n  mean_wa = tf.reshape(mean_wa, (-1,))\n  # weighted average of component variances under mixture distribution.\n  var_wa = tf.matmul(pi_for_dot_prod, tf.square(sigma_for_dot_prod))\n  var_wa = tf.reshape(var_wa, (-1,))\n  # weighted average of component squared means under mixture distribution.\n  sq_mean_wa = tf.matmul(pi_for_dot_prod, tf.square(mu_for_dot_prod))\n  sq_mean_wa = tf.reshape(sq_mean_wa, (-1,))\n  mixture_variance = var_wa + sq_mean_wa - tf.square(mean_wa)\n  return tf.sqrt(mixture_variance)", "language": "python", "code": "def mixture_stddev(mixture_weight_vector, mean_vector, stddev_vector):\n  \"\"\"Computes the standard deviation of a mixture distribution.\n\n  This function works regardless of the component distribution, so long as\n  each component's mean and standard deviation can be provided.\n\n  Args:\n    mixture_weight_vector: A 2D tensor with shape [batch_size, num_components]\n    mean_vector: A 2D tensor of mixture component means. Has shape `[batch_size,\n      num_components]`.\n    stddev_vector: A 2D tensor of mixture component standard deviations. Has\n      shape `[batch_size, num_components]`.\n\n  Returns:\n    A 1D tensor of shape `[batch_size]` representing the standard deviation of\n    the mixture distribution with given weights and component means and standard\n    deviations.\n  Raises:\n    ValueError: If the shapes of the input tensors are not as expected.\n  \"\"\"\n  tensorshape_util.assert_has_rank(mixture_weight_vector.shape, 2)\n  if not tensorshape_util.is_compatible_with(mean_vector.shape,\n                                             mixture_weight_vector.shape):\n    raise ValueError(\"Expecting means to have same shape as mixture weights.\")\n  if not tensorshape_util.is_compatible_with(stddev_vector.shape,\n                                             mixture_weight_vector.shape):\n    raise ValueError(\"Expecting stddevs to have same shape as mixture weights.\")\n\n  # Reshape the distribution parameters for batched vectorized dot products.\n  pi_for_dot_prod = tf.expand_dims(mixture_weight_vector, axis=1)\n  mu_for_dot_prod = tf.expand_dims(mean_vector, axis=2)\n  sigma_for_dot_prod = tf.expand_dims(stddev_vector, axis=2)\n\n  # weighted average of component means under mixture distribution.\n  mean_wa = tf.matmul(pi_for_dot_prod, mu_for_dot_prod)\n  mean_wa = tf.reshape(mean_wa, (-1,))\n  # weighted average of component variances under mixture distribution.\n  var_wa = tf.matmul(pi_for_dot_prod, tf.square(sigma_for_dot_prod))\n  var_wa = tf.reshape(var_wa, (-1,))\n  # weighted average of component squared means under mixture distribution.\n  sq_mean_wa = tf.matmul(pi_for_dot_prod, tf.square(mu_for_dot_prod))\n  sq_mean_wa = tf.reshape(sq_mean_wa, (-1,))\n  mixture_variance = var_wa + sq_mean_wa - tf.square(mean_wa)\n  return tf.sqrt(mixture_variance)", "code_tokens": ["def", "mixture_stddev", "(", "mixture_weight_vector", ",", "mean_vector", ",", "stddev_vector", ")", ":", "tensorshape_util", ".", "assert_has_rank", "(", "mixture_weight_vector", ".", "shape", ",", "2", ")", "if", "not", "tensorshape_util", ".", "is_compatible_with", "(", "mean_vector", ".", "shape", ",", "mixture_weight_vector", ".", "shape", ")", ":", "raise", "ValueError", "(", "\"Expecting means to have same shape as mixture weights.\"", ")", "if", "not", "tensorshape_util", ".", "is_compatible_with", "(", "stddev_vector", ".", "shape", ",", "mixture_weight_vector", ".", "shape", ")", ":", "raise", "ValueError", "(", "\"Expecting stddevs to have same shape as mixture weights.\"", ")", "# Reshape the distribution parameters for batched vectorized dot products.", "pi_for_dot_prod", "=", "tf", ".", "expand_dims", "(", "mixture_weight_vector", ",", "axis", "=", "1", ")", "mu_for_dot_prod", "=", "tf", ".", "expand_dims", "(", "mean_vector", ",", "axis", "=", "2", ")", "sigma_for_dot_prod", "=", "tf", ".", "expand_dims", "(", "stddev_vector", ",", "axis", "=", "2", ")", "# weighted average of component means under mixture distribution.", "mean_wa", "=", "tf", ".", "matmul", "(", "pi_for_dot_prod", ",", "mu_for_dot_prod", ")", "mean_wa", "=", "tf", ".", "reshape", "(", "mean_wa", ",", "(", "-", "1", ",", ")", ")", "# weighted average of component variances under mixture distribution.", "var_wa", "=", "tf", ".", "matmul", "(", "pi_for_dot_prod", ",", "tf", ".", "square", "(", "sigma_for_dot_prod", ")", ")", "var_wa", "=", "tf", ".", "reshape", "(", "var_wa", ",", "(", "-", "1", ",", ")", ")", "# weighted average of component squared means under mixture distribution.", "sq_mean_wa", "=", "tf", ".", "matmul", "(", "pi_for_dot_prod", ",", "tf", ".", "square", "(", "mu_for_dot_prod", ")", ")", "sq_mean_wa", "=", "tf", ".", "reshape", "(", "sq_mean_wa", ",", "(", "-", "1", ",", ")", ")", "mixture_variance", "=", "var_wa", "+", "sq_mean_wa", "-", "tf", ".", "square", "(", "mean_wa", ")", "return", "tf", ".", "sqrt", "(", "mixture_variance", ")"], "docstring": "Computes the standard deviation of a mixture distribution.\n\n  This function works regardless of the component distribution, so long as\n  each component's mean and standard deviation can be provided.\n\n  Args:\n    mixture_weight_vector: A 2D tensor with shape [batch_size, num_components]\n    mean_vector: A 2D tensor of mixture component means. Has shape `[batch_size,\n      num_components]`.\n    stddev_vector: A 2D tensor of mixture component standard deviations. Has\n      shape `[batch_size, num_components]`.\n\n  Returns:\n    A 1D tensor of shape `[batch_size]` representing the standard deviation of\n    the mixture distribution with given weights and component means and standard\n    deviations.\n  Raises:\n    ValueError: If the shapes of the input tensors are not as expected.", "docstring_tokens": ["Computes", "the", "standard", "deviation", "of", "a", "mixture", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L39-L82", "partition": "test", "index": 902, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/normal_conjugate_posteriors.py", "func_name": "normal_conjugates_known_scale_posterior", "original_string": "def normal_conjugates_known_scale_posterior(prior, scale, s, n):\n  \"\"\"Posterior Normal distribution with conjugate prior on the mean.\n\n  This model assumes that `n` observations (with sum `s`) come from a\n  Normal with unknown mean `loc` (described by the Normal `prior`)\n  and known variance `scale**2`. The \"known scale posterior\" is\n  the distribution of the unknown `loc`.\n\n  Accepts a prior Normal distribution object, having parameters\n  `loc0` and `scale0`, as well as known `scale` values of the predictive\n  distribution(s) (also assumed Normal),\n  and statistical estimates `s` (the sum(s) of the observations) and\n  `n` (the number(s) of observations).\n\n  Returns a posterior (also Normal) distribution object, with parameters\n  `(loc', scale'**2)`, where:\n\n  ```\n  mu ~ N(mu', sigma'**2)\n  sigma'**2 = 1/(1/sigma0**2 + n/sigma**2),\n  mu' = (mu0/sigma0**2 + s/sigma**2) * sigma'**2.\n  ```\n\n  Distribution parameters from `prior`, as well as `scale`, `s`, and `n`.\n  will broadcast in the case of multidimensional sets of parameters.\n\n  Args:\n    prior: `Normal` object of type `dtype`:\n      the prior distribution having parameters `(loc0, scale0)`.\n    scale: tensor of type `dtype`, taking values `scale > 0`.\n      The known stddev parameter(s).\n    s: Tensor of type `dtype`. The sum(s) of observations.\n    n: Tensor of type `int`. The number(s) of observations.\n\n  Returns:\n    A new Normal posterior distribution object for the unknown observation\n    mean `loc`.\n\n  Raises:\n    TypeError: if dtype of `s` does not match `dtype`, or `prior` is not a\n      Normal object.\n  \"\"\"\n  if not isinstance(prior, normal.Normal):\n    raise TypeError(\"Expected prior to be an instance of type Normal\")\n\n  if s.dtype != prior.dtype:\n    raise TypeError(\n        \"Observation sum s.dtype does not match prior dtype: %s vs. %s\"\n        % (s.dtype, prior.dtype))\n\n  n = tf.cast(n, prior.dtype)\n  scale0_2 = tf.square(prior.scale)\n  scale_2 = tf.square(scale)\n  scalep_2 = 1.0/(1/scale0_2 + n/scale_2)\n  return normal.Normal(\n      loc=(prior.loc / scale0_2 + s / scale_2) * scalep_2,\n      scale=tf.sqrt(scalep_2))", "language": "python", "code": "def normal_conjugates_known_scale_posterior(prior, scale, s, n):\n  \"\"\"Posterior Normal distribution with conjugate prior on the mean.\n\n  This model assumes that `n` observations (with sum `s`) come from a\n  Normal with unknown mean `loc` (described by the Normal `prior`)\n  and known variance `scale**2`. The \"known scale posterior\" is\n  the distribution of the unknown `loc`.\n\n  Accepts a prior Normal distribution object, having parameters\n  `loc0` and `scale0`, as well as known `scale` values of the predictive\n  distribution(s) (also assumed Normal),\n  and statistical estimates `s` (the sum(s) of the observations) and\n  `n` (the number(s) of observations).\n\n  Returns a posterior (also Normal) distribution object, with parameters\n  `(loc', scale'**2)`, where:\n\n  ```\n  mu ~ N(mu', sigma'**2)\n  sigma'**2 = 1/(1/sigma0**2 + n/sigma**2),\n  mu' = (mu0/sigma0**2 + s/sigma**2) * sigma'**2.\n  ```\n\n  Distribution parameters from `prior`, as well as `scale`, `s`, and `n`.\n  will broadcast in the case of multidimensional sets of parameters.\n\n  Args:\n    prior: `Normal` object of type `dtype`:\n      the prior distribution having parameters `(loc0, scale0)`.\n    scale: tensor of type `dtype`, taking values `scale > 0`.\n      The known stddev parameter(s).\n    s: Tensor of type `dtype`. The sum(s) of observations.\n    n: Tensor of type `int`. The number(s) of observations.\n\n  Returns:\n    A new Normal posterior distribution object for the unknown observation\n    mean `loc`.\n\n  Raises:\n    TypeError: if dtype of `s` does not match `dtype`, or `prior` is not a\n      Normal object.\n  \"\"\"\n  if not isinstance(prior, normal.Normal):\n    raise TypeError(\"Expected prior to be an instance of type Normal\")\n\n  if s.dtype != prior.dtype:\n    raise TypeError(\n        \"Observation sum s.dtype does not match prior dtype: %s vs. %s\"\n        % (s.dtype, prior.dtype))\n\n  n = tf.cast(n, prior.dtype)\n  scale0_2 = tf.square(prior.scale)\n  scale_2 = tf.square(scale)\n  scalep_2 = 1.0/(1/scale0_2 + n/scale_2)\n  return normal.Normal(\n      loc=(prior.loc / scale0_2 + s / scale_2) * scalep_2,\n      scale=tf.sqrt(scalep_2))", "code_tokens": ["def", "normal_conjugates_known_scale_posterior", "(", "prior", ",", "scale", ",", "s", ",", "n", ")", ":", "if", "not", "isinstance", "(", "prior", ",", "normal", ".", "Normal", ")", ":", "raise", "TypeError", "(", "\"Expected prior to be an instance of type Normal\"", ")", "if", "s", ".", "dtype", "!=", "prior", ".", "dtype", ":", "raise", "TypeError", "(", "\"Observation sum s.dtype does not match prior dtype: %s vs. %s\"", "%", "(", "s", ".", "dtype", ",", "prior", ".", "dtype", ")", ")", "n", "=", "tf", ".", "cast", "(", "n", ",", "prior", ".", "dtype", ")", "scale0_2", "=", "tf", ".", "square", "(", "prior", ".", "scale", ")", "scale_2", "=", "tf", ".", "square", "(", "scale", ")", "scalep_2", "=", "1.0", "/", "(", "1", "/", "scale0_2", "+", "n", "/", "scale_2", ")", "return", "normal", ".", "Normal", "(", "loc", "=", "(", "prior", ".", "loc", "/", "scale0_2", "+", "s", "/", "scale_2", ")", "*", "scalep_2", ",", "scale", "=", "tf", ".", "sqrt", "(", "scalep_2", ")", ")"], "docstring": "Posterior Normal distribution with conjugate prior on the mean.\n\n  This model assumes that `n` observations (with sum `s`) come from a\n  Normal with unknown mean `loc` (described by the Normal `prior`)\n  and known variance `scale**2`. The \"known scale posterior\" is\n  the distribution of the unknown `loc`.\n\n  Accepts a prior Normal distribution object, having parameters\n  `loc0` and `scale0`, as well as known `scale` values of the predictive\n  distribution(s) (also assumed Normal),\n  and statistical estimates `s` (the sum(s) of the observations) and\n  `n` (the number(s) of observations).\n\n  Returns a posterior (also Normal) distribution object, with parameters\n  `(loc', scale'**2)`, where:\n\n  ```\n  mu ~ N(mu', sigma'**2)\n  sigma'**2 = 1/(1/sigma0**2 + n/sigma**2),\n  mu' = (mu0/sigma0**2 + s/sigma**2) * sigma'**2.\n  ```\n\n  Distribution parameters from `prior`, as well as `scale`, `s`, and `n`.\n  will broadcast in the case of multidimensional sets of parameters.\n\n  Args:\n    prior: `Normal` object of type `dtype`:\n      the prior distribution having parameters `(loc0, scale0)`.\n    scale: tensor of type `dtype`, taking values `scale > 0`.\n      The known stddev parameter(s).\n    s: Tensor of type `dtype`. The sum(s) of observations.\n    n: Tensor of type `int`. The number(s) of observations.\n\n  Returns:\n    A new Normal posterior distribution object for the unknown observation\n    mean `loc`.\n\n  Raises:\n    TypeError: if dtype of `s` does not match `dtype`, or `prior` is not a\n      Normal object.", "docstring_tokens": ["Posterior", "Normal", "distribution", "with", "conjugate", "prior", "on", "the", "mean", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/normal_conjugate_posteriors.py#L25-L81", "partition": "test", "index": 772, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/binomial.py", "func_name": "_bdtr", "original_string": "def _bdtr(k, n, p):\n  \"\"\"The binomial cumulative distribution function.\n\n  Args:\n    k: floating point `Tensor`.\n    n: floating point `Tensor`.\n    p: floating point `Tensor`.\n\n  Returns:\n    `sum_{j=0}^k p^j (1 - p)^(n - j)`.\n  \"\"\"\n  # Trick for getting safe backprop/gradients into n, k when\n  #   betainc(a = 0, ..) = nan\n  # Write:\n  #   where(unsafe, safe_output, betainc(where(unsafe, safe_input, input)))\n  ones = tf.ones_like(n - k)\n  k_eq_n = tf.equal(k, n)\n  safe_dn = tf.where(k_eq_n, ones, n - k)\n  dk = tf.math.betainc(a=safe_dn, b=k + 1, x=1 - p)\n  return tf.where(k_eq_n, ones, dk)", "language": "python", "code": "def _bdtr(k, n, p):\n  \"\"\"The binomial cumulative distribution function.\n\n  Args:\n    k: floating point `Tensor`.\n    n: floating point `Tensor`.\n    p: floating point `Tensor`.\n\n  Returns:\n    `sum_{j=0}^k p^j (1 - p)^(n - j)`.\n  \"\"\"\n  # Trick for getting safe backprop/gradients into n, k when\n  #   betainc(a = 0, ..) = nan\n  # Write:\n  #   where(unsafe, safe_output, betainc(where(unsafe, safe_input, input)))\n  ones = tf.ones_like(n - k)\n  k_eq_n = tf.equal(k, n)\n  safe_dn = tf.where(k_eq_n, ones, n - k)\n  dk = tf.math.betainc(a=safe_dn, b=k + 1, x=1 - p)\n  return tf.where(k_eq_n, ones, dk)", "code_tokens": ["def", "_bdtr", "(", "k", ",", "n", ",", "p", ")", ":", "# Trick for getting safe backprop/gradients into n, k when", "#   betainc(a = 0, ..) = nan", "# Write:", "#   where(unsafe, safe_output, betainc(where(unsafe, safe_input, input)))", "ones", "=", "tf", ".", "ones_like", "(", "n", "-", "k", ")", "k_eq_n", "=", "tf", ".", "equal", "(", "k", ",", "n", ")", "safe_dn", "=", "tf", ".", "where", "(", "k_eq_n", ",", "ones", ",", "n", "-", "k", ")", "dk", "=", "tf", ".", "math", ".", "betainc", "(", "a", "=", "safe_dn", ",", "b", "=", "k", "+", "1", ",", "x", "=", "1", "-", "p", ")", "return", "tf", ".", "where", "(", "k_eq_n", ",", "ones", ",", "dk", ")"], "docstring": "The binomial cumulative distribution function.\n\n  Args:\n    k: floating point `Tensor`.\n    n: floating point `Tensor`.\n    p: floating point `Tensor`.\n\n  Returns:\n    `sum_{j=0}^k p^j (1 - p)^(n - j)`.", "docstring_tokens": ["The", "binomial", "cumulative", "distribution", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/binomial.py#L43-L62", "partition": "test", "index": 836, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/affine.py", "func_name": "Affine._create_scale_operator", "original_string": "def _create_scale_operator(self, identity_multiplier, diag, tril,\n                             perturb_diag, perturb_factor, shift, validate_args,\n                             dtype):\n    \"\"\"Construct `scale` from various components.\n\n    Args:\n      identity_multiplier: floating point rank 0 `Tensor` representing a scaling\n        done to the identity matrix.\n      diag: Floating-point `Tensor` representing the diagonal matrix.`diag` has\n        shape `[N1, N2, ...  k]`, which represents a k x k diagonal matrix.\n      tril: Floating-point `Tensor` representing the lower triangular matrix.\n       `tril` has shape `[N1, N2, ...  k, k]`, which represents a k x k lower\n       triangular matrix.\n      perturb_diag: Floating-point `Tensor` representing the diagonal matrix of\n        the low rank update.\n      perturb_factor: Floating-point `Tensor` representing factor matrix.\n      shift: Floating-point `Tensor` representing `shift in `scale @ X + shift`.\n      validate_args: Python `bool` indicating whether arguments should be\n        checked for correctness.\n      dtype: `DType` for arg `Tensor` conversions.\n\n    Returns:\n      scale. In the case of scaling by a constant, scale is a\n      floating point `Tensor`. Otherwise, scale is a `LinearOperator`.\n\n    Raises:\n      ValueError: if all of `tril`, `diag` and `identity_multiplier` are `None`.\n    \"\"\"\n    identity_multiplier = _as_tensor(identity_multiplier, \"identity_multiplier\",\n                                     dtype)\n    diag = _as_tensor(diag, \"diag\", dtype)\n    tril = _as_tensor(tril, \"tril\", dtype)\n    perturb_diag = _as_tensor(perturb_diag, \"perturb_diag\", dtype)\n    perturb_factor = _as_tensor(perturb_factor, \"perturb_factor\", dtype)\n\n    # If possible, use the low rank update to infer the shape of\n    # the identity matrix, when scale represents a scaled identity matrix\n    # with a low rank update.\n    shape_hint = None\n    if perturb_factor is not None:\n      shape_hint = distribution_util.dimension_size(perturb_factor, axis=-2)\n\n    if self._is_only_identity_multiplier:\n      if validate_args:\n        return distribution_util.with_dependencies([\n            assert_util.assert_none_equal(\n                identity_multiplier, tf.zeros([], identity_multiplier.dtype),\n                [\"identity_multiplier should be non-zero.\"])\n        ], identity_multiplier)\n      return identity_multiplier\n\n    scale = distribution_util.make_tril_scale(\n        loc=shift,\n        scale_tril=tril,\n        scale_diag=diag,\n        scale_identity_multiplier=identity_multiplier,\n        validate_args=validate_args,\n        assert_positive=False,\n        shape_hint=shape_hint)\n\n    if perturb_factor is not None:\n      return tf.linalg.LinearOperatorLowRankUpdate(\n          scale,\n          u=perturb_factor,\n          diag_update=perturb_diag,\n          is_diag_update_positive=perturb_diag is None,\n          is_non_singular=True,  # Implied by is_positive_definite=True.\n          is_self_adjoint=True,\n          is_positive_definite=True,\n          is_square=True)\n\n    return scale", "language": "python", "code": "def _create_scale_operator(self, identity_multiplier, diag, tril,\n                             perturb_diag, perturb_factor, shift, validate_args,\n                             dtype):\n    \"\"\"Construct `scale` from various components.\n\n    Args:\n      identity_multiplier: floating point rank 0 `Tensor` representing a scaling\n        done to the identity matrix.\n      diag: Floating-point `Tensor` representing the diagonal matrix.`diag` has\n        shape `[N1, N2, ...  k]`, which represents a k x k diagonal matrix.\n      tril: Floating-point `Tensor` representing the lower triangular matrix.\n       `tril` has shape `[N1, N2, ...  k, k]`, which represents a k x k lower\n       triangular matrix.\n      perturb_diag: Floating-point `Tensor` representing the diagonal matrix of\n        the low rank update.\n      perturb_factor: Floating-point `Tensor` representing factor matrix.\n      shift: Floating-point `Tensor` representing `shift in `scale @ X + shift`.\n      validate_args: Python `bool` indicating whether arguments should be\n        checked for correctness.\n      dtype: `DType` for arg `Tensor` conversions.\n\n    Returns:\n      scale. In the case of scaling by a constant, scale is a\n      floating point `Tensor`. Otherwise, scale is a `LinearOperator`.\n\n    Raises:\n      ValueError: if all of `tril`, `diag` and `identity_multiplier` are `None`.\n    \"\"\"\n    identity_multiplier = _as_tensor(identity_multiplier, \"identity_multiplier\",\n                                     dtype)\n    diag = _as_tensor(diag, \"diag\", dtype)\n    tril = _as_tensor(tril, \"tril\", dtype)\n    perturb_diag = _as_tensor(perturb_diag, \"perturb_diag\", dtype)\n    perturb_factor = _as_tensor(perturb_factor, \"perturb_factor\", dtype)\n\n    # If possible, use the low rank update to infer the shape of\n    # the identity matrix, when scale represents a scaled identity matrix\n    # with a low rank update.\n    shape_hint = None\n    if perturb_factor is not None:\n      shape_hint = distribution_util.dimension_size(perturb_factor, axis=-2)\n\n    if self._is_only_identity_multiplier:\n      if validate_args:\n        return distribution_util.with_dependencies([\n            assert_util.assert_none_equal(\n                identity_multiplier, tf.zeros([], identity_multiplier.dtype),\n                [\"identity_multiplier should be non-zero.\"])\n        ], identity_multiplier)\n      return identity_multiplier\n\n    scale = distribution_util.make_tril_scale(\n        loc=shift,\n        scale_tril=tril,\n        scale_diag=diag,\n        scale_identity_multiplier=identity_multiplier,\n        validate_args=validate_args,\n        assert_positive=False,\n        shape_hint=shape_hint)\n\n    if perturb_factor is not None:\n      return tf.linalg.LinearOperatorLowRankUpdate(\n          scale,\n          u=perturb_factor,\n          diag_update=perturb_diag,\n          is_diag_update_positive=perturb_diag is None,\n          is_non_singular=True,  # Implied by is_positive_definite=True.\n          is_self_adjoint=True,\n          is_positive_definite=True,\n          is_square=True)\n\n    return scale", "code_tokens": ["def", "_create_scale_operator", "(", "self", ",", "identity_multiplier", ",", "diag", ",", "tril", ",", "perturb_diag", ",", "perturb_factor", ",", "shift", ",", "validate_args", ",", "dtype", ")", ":", "identity_multiplier", "=", "_as_tensor", "(", "identity_multiplier", ",", "\"identity_multiplier\"", ",", "dtype", ")", "diag", "=", "_as_tensor", "(", "diag", ",", "\"diag\"", ",", "dtype", ")", "tril", "=", "_as_tensor", "(", "tril", ",", "\"tril\"", ",", "dtype", ")", "perturb_diag", "=", "_as_tensor", "(", "perturb_diag", ",", "\"perturb_diag\"", ",", "dtype", ")", "perturb_factor", "=", "_as_tensor", "(", "perturb_factor", ",", "\"perturb_factor\"", ",", "dtype", ")", "# If possible, use the low rank update to infer the shape of", "# the identity matrix, when scale represents a scaled identity matrix", "# with a low rank update.", "shape_hint", "=", "None", "if", "perturb_factor", "is", "not", "None", ":", "shape_hint", "=", "distribution_util", ".", "dimension_size", "(", "perturb_factor", ",", "axis", "=", "-", "2", ")", "if", "self", ".", "_is_only_identity_multiplier", ":", "if", "validate_args", ":", "return", "distribution_util", ".", "with_dependencies", "(", "[", "assert_util", ".", "assert_none_equal", "(", "identity_multiplier", ",", "tf", ".", "zeros", "(", "[", "]", ",", "identity_multiplier", ".", "dtype", ")", ",", "[", "\"identity_multiplier should be non-zero.\"", "]", ")", "]", ",", "identity_multiplier", ")", "return", "identity_multiplier", "scale", "=", "distribution_util", ".", "make_tril_scale", "(", "loc", "=", "shift", ",", "scale_tril", "=", "tril", ",", "scale_diag", "=", "diag", ",", "scale_identity_multiplier", "=", "identity_multiplier", ",", "validate_args", "=", "validate_args", ",", "assert_positive", "=", "False", ",", "shape_hint", "=", "shape_hint", ")", "if", "perturb_factor", "is", "not", "None", ":", "return", "tf", ".", "linalg", ".", "LinearOperatorLowRankUpdate", "(", "scale", ",", "u", "=", "perturb_factor", ",", "diag_update", "=", "perturb_diag", ",", "is_diag_update_positive", "=", "perturb_diag", "is", "None", ",", "is_non_singular", "=", "True", ",", "# Implied by is_positive_definite=True.", "is_self_adjoint", "=", "True", ",", "is_positive_definite", "=", "True", ",", "is_square", "=", "True", ")", "return", "scale"], "docstring": "Construct `scale` from various components.\n\n    Args:\n      identity_multiplier: floating point rank 0 `Tensor` representing a scaling\n        done to the identity matrix.\n      diag: Floating-point `Tensor` representing the diagonal matrix.`diag` has\n        shape `[N1, N2, ...  k]`, which represents a k x k diagonal matrix.\n      tril: Floating-point `Tensor` representing the lower triangular matrix.\n       `tril` has shape `[N1, N2, ...  k, k]`, which represents a k x k lower\n       triangular matrix.\n      perturb_diag: Floating-point `Tensor` representing the diagonal matrix of\n        the low rank update.\n      perturb_factor: Floating-point `Tensor` representing factor matrix.\n      shift: Floating-point `Tensor` representing `shift in `scale @ X + shift`.\n      validate_args: Python `bool` indicating whether arguments should be\n        checked for correctness.\n      dtype: `DType` for arg `Tensor` conversions.\n\n    Returns:\n      scale. In the case of scaling by a constant, scale is a\n      floating point `Tensor`. Otherwise, scale is a `LinearOperator`.\n\n    Raises:\n      ValueError: if all of `tril`, `diag` and `identity_multiplier` are `None`.", "docstring_tokens": ["Construct", "scale", "from", "various", "components", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/affine.py#L238-L309", "partition": "test", "index": 646, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/batch_normalization.py", "func_name": "BatchNormalization._validate_bn_layer", "original_string": "def _validate_bn_layer(self, layer):\n    \"\"\"Check for valid BatchNormalization layer.\n\n    Args:\n      layer: Instance of `tf.layers.BatchNormalization`.\n    Raises:\n      ValueError: If batchnorm_layer argument is not an instance of\n      `tf.layers.BatchNormalization`, or if `batchnorm_layer.renorm=True` or\n      if `batchnorm_layer.virtual_batch_size` is specified.\n    \"\"\"\n    if (not isinstance(layer, tf.keras.layers.BatchNormalization) and\n        not isinstance(layer, tf.compat.v1.layers.BatchNormalization)):\n      raise ValueError(\n          \"batchnorm_layer must be an instance of BatchNormalization layer.\")\n    if layer.renorm:\n      raise ValueError(\"BatchNorm Bijector does not support renormalization.\")\n    if layer.virtual_batch_size:\n      raise ValueError(\n          \"BatchNorm Bijector does not support virtual batch sizes.\")", "language": "python", "code": "def _validate_bn_layer(self, layer):\n    \"\"\"Check for valid BatchNormalization layer.\n\n    Args:\n      layer: Instance of `tf.layers.BatchNormalization`.\n    Raises:\n      ValueError: If batchnorm_layer argument is not an instance of\n      `tf.layers.BatchNormalization`, or if `batchnorm_layer.renorm=True` or\n      if `batchnorm_layer.virtual_batch_size` is specified.\n    \"\"\"\n    if (not isinstance(layer, tf.keras.layers.BatchNormalization) and\n        not isinstance(layer, tf.compat.v1.layers.BatchNormalization)):\n      raise ValueError(\n          \"batchnorm_layer must be an instance of BatchNormalization layer.\")\n    if layer.renorm:\n      raise ValueError(\"BatchNorm Bijector does not support renormalization.\")\n    if layer.virtual_batch_size:\n      raise ValueError(\n          \"BatchNorm Bijector does not support virtual batch sizes.\")", "code_tokens": ["def", "_validate_bn_layer", "(", "self", ",", "layer", ")", ":", "if", "(", "not", "isinstance", "(", "layer", ",", "tf", ".", "keras", ".", "layers", ".", "BatchNormalization", ")", "and", "not", "isinstance", "(", "layer", ",", "tf", ".", "compat", ".", "v1", ".", "layers", ".", "BatchNormalization", ")", ")", ":", "raise", "ValueError", "(", "\"batchnorm_layer must be an instance of BatchNormalization layer.\"", ")", "if", "layer", ".", "renorm", ":", "raise", "ValueError", "(", "\"BatchNorm Bijector does not support renormalization.\"", ")", "if", "layer", ".", "virtual_batch_size", ":", "raise", "ValueError", "(", "\"BatchNorm Bijector does not support virtual batch sizes.\"", ")"], "docstring": "Check for valid BatchNormalization layer.\n\n    Args:\n      layer: Instance of `tf.layers.BatchNormalization`.\n    Raises:\n      ValueError: If batchnorm_layer argument is not an instance of\n      `tf.layers.BatchNormalization`, or if `batchnorm_layer.renorm=True` or\n      if `batchnorm_layer.virtual_batch_size` is specified.", "docstring_tokens": ["Check", "for", "valid", "BatchNormalization", "layer", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/batch_normalization.py#L164-L182", "partition": "test", "index": 886, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/batch_normalization.py", "func_name": "_undo_batch_normalization", "original_string": "def _undo_batch_normalization(x,\n                              mean,\n                              variance,\n                              offset,\n                              scale,\n                              variance_epsilon,\n                              name=None):\n  r\"\"\"Inverse of tf.nn.batch_normalization.\n\n  Args:\n    x: Input `Tensor` of arbitrary dimensionality.\n    mean: A mean `Tensor`.\n    variance: A variance `Tensor`.\n    offset: An offset `Tensor`, often denoted `beta` in equations, or\n      None. If present, will be added to the normalized tensor.\n    scale: A scale `Tensor`, often denoted `gamma` in equations, or\n      `None`. If present, the scale is applied to the normalized tensor.\n    variance_epsilon: A small `float` added to the minibatch `variance` to\n      prevent dividing by zero.\n    name: A name for this operation (optional).\n\n  Returns:\n    batch_unnormalized: The de-normalized, de-scaled, de-offset `Tensor`.\n  \"\"\"\n  with tf.compat.v2.name_scope(name or \"undo_batchnorm\"):\n    # inv = tf.rsqrt(variance + variance_epsilon)\n    # if scale is not None:\n    #   inv *= scale\n    # return x * inv + (\n    #     offset - mean * inv if offset is not None else -mean * inv)\n    rescale = tf.sqrt(variance + variance_epsilon)\n    if scale is not None:\n      rescale /= scale\n    batch_unnormalized = x * rescale + (\n        mean - offset * rescale if offset is not None else mean)\n    return batch_unnormalized", "language": "python", "code": "def _undo_batch_normalization(x,\n                              mean,\n                              variance,\n                              offset,\n                              scale,\n                              variance_epsilon,\n                              name=None):\n  r\"\"\"Inverse of tf.nn.batch_normalization.\n\n  Args:\n    x: Input `Tensor` of arbitrary dimensionality.\n    mean: A mean `Tensor`.\n    variance: A variance `Tensor`.\n    offset: An offset `Tensor`, often denoted `beta` in equations, or\n      None. If present, will be added to the normalized tensor.\n    scale: A scale `Tensor`, often denoted `gamma` in equations, or\n      `None`. If present, the scale is applied to the normalized tensor.\n    variance_epsilon: A small `float` added to the minibatch `variance` to\n      prevent dividing by zero.\n    name: A name for this operation (optional).\n\n  Returns:\n    batch_unnormalized: The de-normalized, de-scaled, de-offset `Tensor`.\n  \"\"\"\n  with tf.compat.v2.name_scope(name or \"undo_batchnorm\"):\n    # inv = tf.rsqrt(variance + variance_epsilon)\n    # if scale is not None:\n    #   inv *= scale\n    # return x * inv + (\n    #     offset - mean * inv if offset is not None else -mean * inv)\n    rescale = tf.sqrt(variance + variance_epsilon)\n    if scale is not None:\n      rescale /= scale\n    batch_unnormalized = x * rescale + (\n        mean - offset * rescale if offset is not None else mean)\n    return batch_unnormalized", "code_tokens": ["def", "_undo_batch_normalization", "(", "x", ",", "mean", ",", "variance", ",", "offset", ",", "scale", ",", "variance_epsilon", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v2", ".", "name_scope", "(", "name", "or", "\"undo_batchnorm\"", ")", ":", "# inv = tf.rsqrt(variance + variance_epsilon)", "# if scale is not None:", "#   inv *= scale", "# return x * inv + (", "#     offset - mean * inv if offset is not None else -mean * inv)", "rescale", "=", "tf", ".", "sqrt", "(", "variance", "+", "variance_epsilon", ")", "if", "scale", "is", "not", "None", ":", "rescale", "/=", "scale", "batch_unnormalized", "=", "x", "*", "rescale", "+", "(", "mean", "-", "offset", "*", "rescale", "if", "offset", "is", "not", "None", "else", "mean", ")", "return", "batch_unnormalized"], "docstring": "r\"\"\"Inverse of tf.nn.batch_normalization.\n\n  Args:\n    x: Input `Tensor` of arbitrary dimensionality.\n    mean: A mean `Tensor`.\n    variance: A variance `Tensor`.\n    offset: An offset `Tensor`, often denoted `beta` in equations, or\n      None. If present, will be added to the normalized tensor.\n    scale: A scale `Tensor`, often denoted `gamma` in equations, or\n      `None`. If present, the scale is applied to the normalized tensor.\n    variance_epsilon: A small `float` added to the minibatch `variance` to\n      prevent dividing by zero.\n    name: A name for this operation (optional).\n\n  Returns:\n    batch_unnormalized: The de-normalized, de-scaled, de-offset `Tensor`.", "docstring_tokens": ["r", "Inverse", "of", "tf", ".", "nn", ".", "batch_normalization", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/batch_normalization.py#L33-L68", "partition": "test", "index": 885, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/cholesky_outer_product.py", "func_name": "CholeskyOuterProduct._make_columnar", "original_string": "def _make_columnar(self, x):\n    \"\"\"Ensures non-scalar input has at least one column.\n\n    Example:\n      If `x = [1, 2, 3]` then the output is `[[1], [2], [3]]`.\n\n      If `x = [[1, 2, 3], [4, 5, 6]]` then the output is unchanged.\n\n      If `x = 1` then the output is unchanged.\n\n    Args:\n      x: `Tensor`.\n\n    Returns:\n      columnar_x: `Tensor` with at least two dimensions.\n    \"\"\"\n    if tensorshape_util.rank(x.shape) is not None:\n      if tensorshape_util.rank(x.shape) == 1:\n        x = x[tf.newaxis, :]\n      return x\n    shape = tf.shape(input=x)\n    maybe_expanded_shape = tf.concat([\n        shape[:-1],\n        distribution_util.pick_vector(\n            tf.equal(tf.rank(x), 1), [1], np.array([], dtype=np.int32)),\n        shape[-1:],\n    ], 0)\n    return tf.reshape(x, maybe_expanded_shape)", "language": "python", "code": "def _make_columnar(self, x):\n    \"\"\"Ensures non-scalar input has at least one column.\n\n    Example:\n      If `x = [1, 2, 3]` then the output is `[[1], [2], [3]]`.\n\n      If `x = [[1, 2, 3], [4, 5, 6]]` then the output is unchanged.\n\n      If `x = 1` then the output is unchanged.\n\n    Args:\n      x: `Tensor`.\n\n    Returns:\n      columnar_x: `Tensor` with at least two dimensions.\n    \"\"\"\n    if tensorshape_util.rank(x.shape) is not None:\n      if tensorshape_util.rank(x.shape) == 1:\n        x = x[tf.newaxis, :]\n      return x\n    shape = tf.shape(input=x)\n    maybe_expanded_shape = tf.concat([\n        shape[:-1],\n        distribution_util.pick_vector(\n            tf.equal(tf.rank(x), 1), [1], np.array([], dtype=np.int32)),\n        shape[-1:],\n    ], 0)\n    return tf.reshape(x, maybe_expanded_shape)", "code_tokens": ["def", "_make_columnar", "(", "self", ",", "x", ")", ":", "if", "tensorshape_util", ".", "rank", "(", "x", ".", "shape", ")", "is", "not", "None", ":", "if", "tensorshape_util", ".", "rank", "(", "x", ".", "shape", ")", "==", "1", ":", "x", "=", "x", "[", "tf", ".", "newaxis", ",", ":", "]", "return", "x", "shape", "=", "tf", ".", "shape", "(", "input", "=", "x", ")", "maybe_expanded_shape", "=", "tf", ".", "concat", "(", "[", "shape", "[", ":", "-", "1", "]", ",", "distribution_util", ".", "pick_vector", "(", "tf", ".", "equal", "(", "tf", ".", "rank", "(", "x", ")", ",", "1", ")", ",", "[", "1", "]", ",", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "np", ".", "int32", ")", ")", ",", "shape", "[", "-", "1", ":", "]", ",", "]", ",", "0", ")", "return", "tf", ".", "reshape", "(", "x", ",", "maybe_expanded_shape", ")"], "docstring": "Ensures non-scalar input has at least one column.\n\n    Example:\n      If `x = [1, 2, 3]` then the output is `[[1], [2], [3]]`.\n\n      If `x = [[1, 2, 3], [4, 5, 6]]` then the output is unchanged.\n\n      If `x = 1` then the output is unchanged.\n\n    Args:\n      x: `Tensor`.\n\n    Returns:\n      columnar_x: `Tensor` with at least two dimensions.", "docstring_tokens": ["Ensures", "non", "-", "scalar", "input", "has", "at", "least", "one", "column", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/cholesky_outer_product.py#L190-L217", "partition": "test", "index": 878, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/masked_autoregressive.py", "func_name": "masked_dense", "original_string": "def masked_dense(inputs,\n                 units,\n                 num_blocks=None,\n                 exclusive=False,\n                 kernel_initializer=None,\n                 reuse=None,\n                 name=None,\n                 *args,  # pylint: disable=keyword-arg-before-vararg\n                 **kwargs):\n  \"\"\"A autoregressively masked dense layer. Analogous to `tf.layers.dense`.\n\n  See [Germain et al. (2015)][1] for detailed explanation.\n\n  Arguments:\n    inputs: Tensor input.\n    units: Python `int` scalar representing the dimensionality of the output\n      space.\n    num_blocks: Python `int` scalar representing the number of blocks for the\n      MADE masks.\n    exclusive: Python `bool` scalar representing whether to zero the diagonal of\n      the mask, used for the first layer of a MADE.\n    kernel_initializer: Initializer function for the weight matrix.\n      If `None` (default), weights are initialized using the\n      `tf.glorot_random_initializer`.\n    reuse: Python `bool` scalar representing whether to reuse the weights of a\n      previous layer by the same name.\n    name: Python `str` used to describe ops managed by this function.\n    *args: `tf.layers.dense` arguments.\n    **kwargs: `tf.layers.dense` keyword arguments.\n\n  Returns:\n    Output tensor.\n\n  Raises:\n    NotImplementedError: if rightmost dimension of `inputs` is unknown prior to\n      graph execution.\n\n  #### References\n\n  [1]: Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE:\n       Masked Autoencoder for Distribution Estimation. In _International\n       Conference on Machine Learning_, 2015. https://arxiv.org/abs/1502.03509\n  \"\"\"\n  # TODO(b/67594795): Better support of dynamic shape.\n  input_depth = tf.compat.dimension_value(\n      tensorshape_util.with_rank_at_least(inputs.shape, 1)[-1])\n  if input_depth is None:\n    raise NotImplementedError(\n        \"Rightmost dimension must be known prior to graph execution.\")\n\n  mask = _gen_mask(num_blocks, input_depth, units,\n                   MASK_EXCLUSIVE if exclusive else MASK_INCLUSIVE).T\n\n  if kernel_initializer is None:\n    kernel_initializer = tf.compat.v1.glorot_normal_initializer()\n\n  def masked_initializer(shape, dtype=None, partition_info=None):\n    return mask * kernel_initializer(shape, dtype, partition_info)\n\n  with tf.compat.v2.name_scope(name or \"masked_dense\"):\n    layer = tf.compat.v1.layers.Dense(\n        units,\n        kernel_initializer=masked_initializer,\n        kernel_constraint=lambda x: mask * x,\n        name=name,\n        dtype=dtype_util.base_dtype(inputs.dtype),\n        _scope=name,\n        _reuse=reuse,\n        *args,  # pylint: disable=keyword-arg-before-vararg\n        **kwargs)\n    return layer.apply(inputs)", "language": "python", "code": "def masked_dense(inputs,\n                 units,\n                 num_blocks=None,\n                 exclusive=False,\n                 kernel_initializer=None,\n                 reuse=None,\n                 name=None,\n                 *args,  # pylint: disable=keyword-arg-before-vararg\n                 **kwargs):\n  \"\"\"A autoregressively masked dense layer. Analogous to `tf.layers.dense`.\n\n  See [Germain et al. (2015)][1] for detailed explanation.\n\n  Arguments:\n    inputs: Tensor input.\n    units: Python `int` scalar representing the dimensionality of the output\n      space.\n    num_blocks: Python `int` scalar representing the number of blocks for the\n      MADE masks.\n    exclusive: Python `bool` scalar representing whether to zero the diagonal of\n      the mask, used for the first layer of a MADE.\n    kernel_initializer: Initializer function for the weight matrix.\n      If `None` (default), weights are initialized using the\n      `tf.glorot_random_initializer`.\n    reuse: Python `bool` scalar representing whether to reuse the weights of a\n      previous layer by the same name.\n    name: Python `str` used to describe ops managed by this function.\n    *args: `tf.layers.dense` arguments.\n    **kwargs: `tf.layers.dense` keyword arguments.\n\n  Returns:\n    Output tensor.\n\n  Raises:\n    NotImplementedError: if rightmost dimension of `inputs` is unknown prior to\n      graph execution.\n\n  #### References\n\n  [1]: Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE:\n       Masked Autoencoder for Distribution Estimation. In _International\n       Conference on Machine Learning_, 2015. https://arxiv.org/abs/1502.03509\n  \"\"\"\n  # TODO(b/67594795): Better support of dynamic shape.\n  input_depth = tf.compat.dimension_value(\n      tensorshape_util.with_rank_at_least(inputs.shape, 1)[-1])\n  if input_depth is None:\n    raise NotImplementedError(\n        \"Rightmost dimension must be known prior to graph execution.\")\n\n  mask = _gen_mask(num_blocks, input_depth, units,\n                   MASK_EXCLUSIVE if exclusive else MASK_INCLUSIVE).T\n\n  if kernel_initializer is None:\n    kernel_initializer = tf.compat.v1.glorot_normal_initializer()\n\n  def masked_initializer(shape, dtype=None, partition_info=None):\n    return mask * kernel_initializer(shape, dtype, partition_info)\n\n  with tf.compat.v2.name_scope(name or \"masked_dense\"):\n    layer = tf.compat.v1.layers.Dense(\n        units,\n        kernel_initializer=masked_initializer,\n        kernel_constraint=lambda x: mask * x,\n        name=name,\n        dtype=dtype_util.base_dtype(inputs.dtype),\n        _scope=name,\n        _reuse=reuse,\n        *args,  # pylint: disable=keyword-arg-before-vararg\n        **kwargs)\n    return layer.apply(inputs)", "code_tokens": ["def", "masked_dense", "(", "inputs", ",", "units", ",", "num_blocks", "=", "None", ",", "exclusive", "=", "False", ",", "kernel_initializer", "=", "None", ",", "reuse", "=", "None", ",", "name", "=", "None", ",", "*", "args", ",", "# pylint: disable=keyword-arg-before-vararg", "*", "*", "kwargs", ")", ":", "# TODO(b/67594795): Better support of dynamic shape.", "input_depth", "=", "tf", ".", "compat", ".", "dimension_value", "(", "tensorshape_util", ".", "with_rank_at_least", "(", "inputs", ".", "shape", ",", "1", ")", "[", "-", "1", "]", ")", "if", "input_depth", "is", "None", ":", "raise", "NotImplementedError", "(", "\"Rightmost dimension must be known prior to graph execution.\"", ")", "mask", "=", "_gen_mask", "(", "num_blocks", ",", "input_depth", ",", "units", ",", "MASK_EXCLUSIVE", "if", "exclusive", "else", "MASK_INCLUSIVE", ")", ".", "T", "if", "kernel_initializer", "is", "None", ":", "kernel_initializer", "=", "tf", ".", "compat", ".", "v1", ".", "glorot_normal_initializer", "(", ")", "def", "masked_initializer", "(", "shape", ",", "dtype", "=", "None", ",", "partition_info", "=", "None", ")", ":", "return", "mask", "*", "kernel_initializer", "(", "shape", ",", "dtype", ",", "partition_info", ")", "with", "tf", ".", "compat", ".", "v2", ".", "name_scope", "(", "name", "or", "\"masked_dense\"", ")", ":", "layer", "=", "tf", ".", "compat", ".", "v1", ".", "layers", ".", "Dense", "(", "units", ",", "kernel_initializer", "=", "masked_initializer", ",", "kernel_constraint", "=", "lambda", "x", ":", "mask", "*", "x", ",", "name", "=", "name", ",", "dtype", "=", "dtype_util", ".", "base_dtype", "(", "inputs", ".", "dtype", ")", ",", "_scope", "=", "name", ",", "_reuse", "=", "reuse", ",", "*", "args", ",", "# pylint: disable=keyword-arg-before-vararg", "*", "*", "kwargs", ")", "return", "layer", ".", "apply", "(", "inputs", ")"], "docstring": "A autoregressively masked dense layer. Analogous to `tf.layers.dense`.\n\n  See [Germain et al. (2015)][1] for detailed explanation.\n\n  Arguments:\n    inputs: Tensor input.\n    units: Python `int` scalar representing the dimensionality of the output\n      space.\n    num_blocks: Python `int` scalar representing the number of blocks for the\n      MADE masks.\n    exclusive: Python `bool` scalar representing whether to zero the diagonal of\n      the mask, used for the first layer of a MADE.\n    kernel_initializer: Initializer function for the weight matrix.\n      If `None` (default), weights are initialized using the\n      `tf.glorot_random_initializer`.\n    reuse: Python `bool` scalar representing whether to reuse the weights of a\n      previous layer by the same name.\n    name: Python `str` used to describe ops managed by this function.\n    *args: `tf.layers.dense` arguments.\n    **kwargs: `tf.layers.dense` keyword arguments.\n\n  Returns:\n    Output tensor.\n\n  Raises:\n    NotImplementedError: if rightmost dimension of `inputs` is unknown prior to\n      graph execution.\n\n  #### References\n\n  [1]: Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE:\n       Masked Autoencoder for Distribution Estimation. In _International\n       Conference on Machine Learning_, 2015. https://arxiv.org/abs/1502.03509", "docstring_tokens": ["A", "autoregressively", "masked", "dense", "layer", ".", "Analogous", "to", "tf", ".", "layers", ".", "dense", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/masked_autoregressive.py#L337-L407", "partition": "test", "index": 1117, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/masked_autoregressive.py", "func_name": "_gen_mask", "original_string": "def _gen_mask(num_blocks,\n              n_in,\n              n_out,\n              mask_type=MASK_EXCLUSIVE,\n              dtype=tf.float32):\n  \"\"\"Generate the mask for building an autoregressive dense layer.\"\"\"\n  # TODO(b/67594795): Better support of dynamic shape.\n  mask = np.zeros([n_out, n_in], dtype=dtype.as_numpy_dtype())\n  slices = _gen_slices(num_blocks, n_in, n_out, mask_type=mask_type)\n  for [row_slice, col_slice] in slices:\n    mask[row_slice, col_slice] = 1\n  return mask", "language": "python", "code": "def _gen_mask(num_blocks,\n              n_in,\n              n_out,\n              mask_type=MASK_EXCLUSIVE,\n              dtype=tf.float32):\n  \"\"\"Generate the mask for building an autoregressive dense layer.\"\"\"\n  # TODO(b/67594795): Better support of dynamic shape.\n  mask = np.zeros([n_out, n_in], dtype=dtype.as_numpy_dtype())\n  slices = _gen_slices(num_blocks, n_in, n_out, mask_type=mask_type)\n  for [row_slice, col_slice] in slices:\n    mask[row_slice, col_slice] = 1\n  return mask", "code_tokens": ["def", "_gen_mask", "(", "num_blocks", ",", "n_in", ",", "n_out", ",", "mask_type", "=", "MASK_EXCLUSIVE", ",", "dtype", "=", "tf", ".", "float32", ")", ":", "# TODO(b/67594795): Better support of dynamic shape.", "mask", "=", "np", ".", "zeros", "(", "[", "n_out", ",", "n_in", "]", ",", "dtype", "=", "dtype", ".", "as_numpy_dtype", "(", ")", ")", "slices", "=", "_gen_slices", "(", "num_blocks", ",", "n_in", ",", "n_out", ",", "mask_type", "=", "mask_type", ")", "for", "[", "row_slice", ",", "col_slice", "]", "in", "slices", ":", "mask", "[", "row_slice", ",", "col_slice", "]", "=", "1", "return", "mask"], "docstring": "Generate the mask for building an autoregressive dense layer.", "docstring_tokens": ["Generate", "the", "mask", "for", "building", "an", "autoregressive", "dense", "layer", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/masked_autoregressive.py#L323-L334", "partition": "test", "index": 1116, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/masked_autoregressive.py", "func_name": "_gen_slices", "original_string": "def _gen_slices(num_blocks, n_in, n_out, mask_type=MASK_EXCLUSIVE):\n  \"\"\"Generate the slices for building an autoregressive mask.\"\"\"\n  # TODO(b/67594795): Better support of dynamic shape.\n  slices = []\n  col = 0\n  d_in = n_in // num_blocks\n  d_out = n_out // num_blocks\n  row = d_out if mask_type == MASK_EXCLUSIVE else 0\n  for _ in range(num_blocks):\n    row_slice = slice(row, None)\n    col_slice = slice(col, col + d_in)\n    slices.append([row_slice, col_slice])\n    col += d_in\n    row += d_out\n  return slices", "language": "python", "code": "def _gen_slices(num_blocks, n_in, n_out, mask_type=MASK_EXCLUSIVE):\n  \"\"\"Generate the slices for building an autoregressive mask.\"\"\"\n  # TODO(b/67594795): Better support of dynamic shape.\n  slices = []\n  col = 0\n  d_in = n_in // num_blocks\n  d_out = n_out // num_blocks\n  row = d_out if mask_type == MASK_EXCLUSIVE else 0\n  for _ in range(num_blocks):\n    row_slice = slice(row, None)\n    col_slice = slice(col, col + d_in)\n    slices.append([row_slice, col_slice])\n    col += d_in\n    row += d_out\n  return slices", "code_tokens": ["def", "_gen_slices", "(", "num_blocks", ",", "n_in", ",", "n_out", ",", "mask_type", "=", "MASK_EXCLUSIVE", ")", ":", "# TODO(b/67594795): Better support of dynamic shape.", "slices", "=", "[", "]", "col", "=", "0", "d_in", "=", "n_in", "//", "num_blocks", "d_out", "=", "n_out", "//", "num_blocks", "row", "=", "d_out", "if", "mask_type", "==", "MASK_EXCLUSIVE", "else", "0", "for", "_", "in", "range", "(", "num_blocks", ")", ":", "row_slice", "=", "slice", "(", "row", ",", "None", ")", "col_slice", "=", "slice", "(", "col", ",", "col", "+", "d_in", ")", "slices", ".", "append", "(", "[", "row_slice", ",", "col_slice", "]", ")", "col", "+=", "d_in", "row", "+=", "d_out", "return", "slices"], "docstring": "Generate the slices for building an autoregressive mask.", "docstring_tokens": ["Generate", "the", "slices", "for", "building", "an", "autoregressive", "mask", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/masked_autoregressive.py#L306-L320", "partition": "test", "index": 1115, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/moving_stats.py", "func_name": "assign_log_moving_mean_exp", "original_string": "def assign_log_moving_mean_exp(\n    log_mean_exp_var, log_value, decay, name=None):\n  \"\"\"Compute the log of the exponentially weighted moving mean of the exp.\n\n  If `log_value` is a draw from a stationary random variable, this function\n  approximates `log(E[exp(log_value)])`, i.e., a weighted log-sum-exp. More\n  precisely, a `tf.Variable`, `log_mean_exp_var`, is updated by `log_value`\n  using the following identity:\n\n  ```none\n  log_mean_exp_var =\n  = log(decay exp(log_mean_exp_var) + (1 - decay) exp(log_value))\n  = log(exp(log_mean_exp_var + log(decay)) + exp(log_value + log1p(-decay)))\n  = log_mean_exp_var\n    + log(  exp(log_mean_exp_var   - log_mean_exp_var + log(decay))\n          + exp(log_value - log_mean_exp_var + log1p(-decay)))\n  = log_mean_exp_var\n    + log_sum_exp([log(decay), log_value - log_mean_exp_var + log1p(-decay)]).\n  ```\n\n  In addition to numerical stability, this formulation is advantageous because\n  `log_mean_exp_var` can be updated in a lock-free manner, i.e., using\n  `assign_add`. (Note: the updates are not thread-safe; it's just that the\n  update to the tf.Variable is presumed efficient due to being lock-free.)\n\n  Args:\n    log_mean_exp_var: `float`-like `Variable` representing the log of the\n      exponentially weighted moving mean of the exp. Same shape as `log_value`.\n    log_value: `float`-like `Tensor` representing a new (streaming) observation.\n      Same shape as `log_mean_exp_var`.\n    decay: A `float`-like `Tensor`. The moving mean decay. Typically close to\n      `1.`, e.g., `0.999`.\n    name: Optional name of the returned operation.\n\n  Returns:\n    log_mean_exp_var: A reference to the input 'Variable' tensor with the\n      `log_value`-updated log of the exponentially weighted moving mean of exp.\n\n  Raises:\n    TypeError: if `log_mean_exp_var` does not have float type `dtype`.\n    TypeError: if `log_mean_exp_var`, `log_value`, `decay` have different\n      `base_dtype`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, \"assign_log_moving_mean_exp\",\n                               [log_mean_exp_var, log_value, decay]):\n    # We want to update the variable in a numerically stable and lock-free way.\n    # To do this, observe that variable `x` updated by `v` is:\n    # x = log(w exp(x) + (1-w) exp(v))\n    #   = log(exp(x + log(w)) + exp(v + log1p(-w)))\n    #   = x + log(exp(x - x + log(w)) + exp(v - x + log1p(-w)))\n    #   = x + lse([log(w), v - x + log1p(-w)])\n    with tf.compat.v1.colocate_with(log_mean_exp_var):\n      base_dtype = log_mean_exp_var.dtype.base_dtype\n      if not base_dtype.is_floating:\n        raise TypeError(\n            \"log_mean_exp_var.base_dtype({}) does not have float type \"\n            \"`dtype`.\".format(base_dtype.name))\n      log_value = tf.convert_to_tensor(\n          value=log_value, dtype=base_dtype, name=\"log_value\")\n      decay = tf.convert_to_tensor(value=decay, dtype=base_dtype, name=\"decay\")\n      delta = (log_value - log_mean_exp_var)[tf.newaxis, ...]\n      x = tf.concat([\n          tf.math.log(decay) * tf.ones_like(delta),\n          delta + tf.math.log1p(-decay)\n      ],\n                    axis=0)\n      x = tf.reduce_logsumexp(input_tensor=x, axis=0)\n      return log_mean_exp_var.assign_add(x)", "language": "python", "code": "def assign_log_moving_mean_exp(\n    log_mean_exp_var, log_value, decay, name=None):\n  \"\"\"Compute the log of the exponentially weighted moving mean of the exp.\n\n  If `log_value` is a draw from a stationary random variable, this function\n  approximates `log(E[exp(log_value)])`, i.e., a weighted log-sum-exp. More\n  precisely, a `tf.Variable`, `log_mean_exp_var`, is updated by `log_value`\n  using the following identity:\n\n  ```none\n  log_mean_exp_var =\n  = log(decay exp(log_mean_exp_var) + (1 - decay) exp(log_value))\n  = log(exp(log_mean_exp_var + log(decay)) + exp(log_value + log1p(-decay)))\n  = log_mean_exp_var\n    + log(  exp(log_mean_exp_var   - log_mean_exp_var + log(decay))\n          + exp(log_value - log_mean_exp_var + log1p(-decay)))\n  = log_mean_exp_var\n    + log_sum_exp([log(decay), log_value - log_mean_exp_var + log1p(-decay)]).\n  ```\n\n  In addition to numerical stability, this formulation is advantageous because\n  `log_mean_exp_var` can be updated in a lock-free manner, i.e., using\n  `assign_add`. (Note: the updates are not thread-safe; it's just that the\n  update to the tf.Variable is presumed efficient due to being lock-free.)\n\n  Args:\n    log_mean_exp_var: `float`-like `Variable` representing the log of the\n      exponentially weighted moving mean of the exp. Same shape as `log_value`.\n    log_value: `float`-like `Tensor` representing a new (streaming) observation.\n      Same shape as `log_mean_exp_var`.\n    decay: A `float`-like `Tensor`. The moving mean decay. Typically close to\n      `1.`, e.g., `0.999`.\n    name: Optional name of the returned operation.\n\n  Returns:\n    log_mean_exp_var: A reference to the input 'Variable' tensor with the\n      `log_value`-updated log of the exponentially weighted moving mean of exp.\n\n  Raises:\n    TypeError: if `log_mean_exp_var` does not have float type `dtype`.\n    TypeError: if `log_mean_exp_var`, `log_value`, `decay` have different\n      `base_dtype`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, \"assign_log_moving_mean_exp\",\n                               [log_mean_exp_var, log_value, decay]):\n    # We want to update the variable in a numerically stable and lock-free way.\n    # To do this, observe that variable `x` updated by `v` is:\n    # x = log(w exp(x) + (1-w) exp(v))\n    #   = log(exp(x + log(w)) + exp(v + log1p(-w)))\n    #   = x + log(exp(x - x + log(w)) + exp(v - x + log1p(-w)))\n    #   = x + lse([log(w), v - x + log1p(-w)])\n    with tf.compat.v1.colocate_with(log_mean_exp_var):\n      base_dtype = log_mean_exp_var.dtype.base_dtype\n      if not base_dtype.is_floating:\n        raise TypeError(\n            \"log_mean_exp_var.base_dtype({}) does not have float type \"\n            \"`dtype`.\".format(base_dtype.name))\n      log_value = tf.convert_to_tensor(\n          value=log_value, dtype=base_dtype, name=\"log_value\")\n      decay = tf.convert_to_tensor(value=decay, dtype=base_dtype, name=\"decay\")\n      delta = (log_value - log_mean_exp_var)[tf.newaxis, ...]\n      x = tf.concat([\n          tf.math.log(decay) * tf.ones_like(delta),\n          delta + tf.math.log1p(-decay)\n      ],\n                    axis=0)\n      x = tf.reduce_logsumexp(input_tensor=x, axis=0)\n      return log_mean_exp_var.assign_add(x)", "code_tokens": ["def", "assign_log_moving_mean_exp", "(", "log_mean_exp_var", ",", "log_value", ",", "decay", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "\"assign_log_moving_mean_exp\"", ",", "[", "log_mean_exp_var", ",", "log_value", ",", "decay", "]", ")", ":", "# We want to update the variable in a numerically stable and lock-free way.", "# To do this, observe that variable `x` updated by `v` is:", "# x = log(w exp(x) + (1-w) exp(v))", "#   = log(exp(x + log(w)) + exp(v + log1p(-w)))", "#   = x + log(exp(x - x + log(w)) + exp(v - x + log1p(-w)))", "#   = x + lse([log(w), v - x + log1p(-w)])", "with", "tf", ".", "compat", ".", "v1", ".", "colocate_with", "(", "log_mean_exp_var", ")", ":", "base_dtype", "=", "log_mean_exp_var", ".", "dtype", ".", "base_dtype", "if", "not", "base_dtype", ".", "is_floating", ":", "raise", "TypeError", "(", "\"log_mean_exp_var.base_dtype({}) does not have float type \"", "\"`dtype`.\"", ".", "format", "(", "base_dtype", ".", "name", ")", ")", "log_value", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "log_value", ",", "dtype", "=", "base_dtype", ",", "name", "=", "\"log_value\"", ")", "decay", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "decay", ",", "dtype", "=", "base_dtype", ",", "name", "=", "\"decay\"", ")", "delta", "=", "(", "log_value", "-", "log_mean_exp_var", ")", "[", "tf", ".", "newaxis", ",", "...", "]", "x", "=", "tf", ".", "concat", "(", "[", "tf", ".", "math", ".", "log", "(", "decay", ")", "*", "tf", ".", "ones_like", "(", "delta", ")", ",", "delta", "+", "tf", ".", "math", ".", "log1p", "(", "-", "decay", ")", "]", ",", "axis", "=", "0", ")", "x", "=", "tf", ".", "reduce_logsumexp", "(", "input_tensor", "=", "x", ",", "axis", "=", "0", ")", "return", "log_mean_exp_var", ".", "assign_add", "(", "x", ")"], "docstring": "Compute the log of the exponentially weighted moving mean of the exp.\n\n  If `log_value` is a draw from a stationary random variable, this function\n  approximates `log(E[exp(log_value)])`, i.e., a weighted log-sum-exp. More\n  precisely, a `tf.Variable`, `log_mean_exp_var`, is updated by `log_value`\n  using the following identity:\n\n  ```none\n  log_mean_exp_var =\n  = log(decay exp(log_mean_exp_var) + (1 - decay) exp(log_value))\n  = log(exp(log_mean_exp_var + log(decay)) + exp(log_value + log1p(-decay)))\n  = log_mean_exp_var\n    + log(  exp(log_mean_exp_var   - log_mean_exp_var + log(decay))\n          + exp(log_value - log_mean_exp_var + log1p(-decay)))\n  = log_mean_exp_var\n    + log_sum_exp([log(decay), log_value - log_mean_exp_var + log1p(-decay)]).\n  ```\n\n  In addition to numerical stability, this formulation is advantageous because\n  `log_mean_exp_var` can be updated in a lock-free manner, i.e., using\n  `assign_add`. (Note: the updates are not thread-safe; it's just that the\n  update to the tf.Variable is presumed efficient due to being lock-free.)\n\n  Args:\n    log_mean_exp_var: `float`-like `Variable` representing the log of the\n      exponentially weighted moving mean of the exp. Same shape as `log_value`.\n    log_value: `float`-like `Tensor` representing a new (streaming) observation.\n      Same shape as `log_mean_exp_var`.\n    decay: A `float`-like `Tensor`. The moving mean decay. Typically close to\n      `1.`, e.g., `0.999`.\n    name: Optional name of the returned operation.\n\n  Returns:\n    log_mean_exp_var: A reference to the input 'Variable' tensor with the\n      `log_value`-updated log of the exponentially weighted moving mean of exp.\n\n  Raises:\n    TypeError: if `log_mean_exp_var` does not have float type `dtype`.\n    TypeError: if `log_mean_exp_var`, `log_value`, `decay` have different\n      `base_dtype`.", "docstring_tokens": ["Compute", "the", "log", "of", "the", "exponentially", "weighted", "moving", "mean", "of", "the", "exp", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/moving_stats.py#L113-L180", "partition": "test", "index": 877, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/vector_diffeomixture.py", "func_name": "concat_vectors", "original_string": "def concat_vectors(*args):\n  \"\"\"Concatenates input vectors, statically if possible.\"\"\"\n  args_ = [tf.get_static_value(x) for x in args]\n  if any(vec is None for vec in args_):\n    return tf.concat(args, axis=0)\n  return [val for vec in args_ for val in vec]", "language": "python", "code": "def concat_vectors(*args):\n  \"\"\"Concatenates input vectors, statically if possible.\"\"\"\n  args_ = [tf.get_static_value(x) for x in args]\n  if any(vec is None for vec in args_):\n    return tf.concat(args, axis=0)\n  return [val for vec in args_ for val in vec]", "code_tokens": ["def", "concat_vectors", "(", "*", "args", ")", ":", "args_", "=", "[", "tf", ".", "get_static_value", "(", "x", ")", "for", "x", "in", "args", "]", "if", "any", "(", "vec", "is", "None", "for", "vec", "in", "args_", ")", ":", "return", "tf", ".", "concat", "(", "args", ",", "axis", "=", "0", ")", "return", "[", "val", "for", "vec", "in", "args_", "for", "val", "in", "vec", "]"], "docstring": "Concatenates input vectors, statically if possible.", "docstring_tokens": ["Concatenates", "input", "vectors", "statically", "if", "possible", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/vector_diffeomixture.py#L936-L941", "partition": "test", "index": 685, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/vector_diffeomixture.py", "func_name": "linop_scale", "original_string": "def linop_scale(w, op):\n  \"\"\"Creates weighted `LinOp` from existing `LinOp`.\"\"\"\n  # We assume w > 0. (This assumption only relates to the is_* attributes.)\n  with tf.name_scope(\"linop_scale\"):\n    # TODO(b/35301104): LinearOperatorComposition doesn't combine operators, so\n    # special case combinations here. Once it does, this function can be\n    # replaced by:\n    #     return linop_composition_lib.LinearOperatorComposition([\n    #         scaled_identity(w), op])\n    def scaled_identity(w):\n      return tf.linalg.LinearOperatorScaledIdentity(\n          num_rows=op.range_dimension_tensor(),\n          multiplier=w,\n          is_non_singular=op.is_non_singular,\n          is_self_adjoint=op.is_self_adjoint,\n          is_positive_definite=op.is_positive_definite)\n\n    if isinstance(op, tf.linalg.LinearOperatorIdentity):\n      return scaled_identity(w)\n    if isinstance(op, tf.linalg.LinearOperatorScaledIdentity):\n      return scaled_identity(w * op.multiplier)\n    if isinstance(op, tf.linalg.LinearOperatorDiag):\n      return tf.linalg.LinearOperatorDiag(\n          diag=w[..., tf.newaxis] * op.diag_part(),\n          is_non_singular=op.is_non_singular,\n          is_self_adjoint=op.is_self_adjoint,\n          is_positive_definite=op.is_positive_definite)\n    if isinstance(op, tf.linalg.LinearOperatorLowerTriangular):\n      return tf.linalg.LinearOperatorLowerTriangular(\n          tril=w[..., tf.newaxis, tf.newaxis] * op.to_dense(),\n          is_non_singular=op.is_non_singular,\n          is_self_adjoint=op.is_self_adjoint,\n          is_positive_definite=op.is_positive_definite)\n    raise NotImplementedError(\n        \"Unsupported Linop type ({})\".format(type(op).__name__))", "language": "python", "code": "def linop_scale(w, op):\n  \"\"\"Creates weighted `LinOp` from existing `LinOp`.\"\"\"\n  # We assume w > 0. (This assumption only relates to the is_* attributes.)\n  with tf.name_scope(\"linop_scale\"):\n    # TODO(b/35301104): LinearOperatorComposition doesn't combine operators, so\n    # special case combinations here. Once it does, this function can be\n    # replaced by:\n    #     return linop_composition_lib.LinearOperatorComposition([\n    #         scaled_identity(w), op])\n    def scaled_identity(w):\n      return tf.linalg.LinearOperatorScaledIdentity(\n          num_rows=op.range_dimension_tensor(),\n          multiplier=w,\n          is_non_singular=op.is_non_singular,\n          is_self_adjoint=op.is_self_adjoint,\n          is_positive_definite=op.is_positive_definite)\n\n    if isinstance(op, tf.linalg.LinearOperatorIdentity):\n      return scaled_identity(w)\n    if isinstance(op, tf.linalg.LinearOperatorScaledIdentity):\n      return scaled_identity(w * op.multiplier)\n    if isinstance(op, tf.linalg.LinearOperatorDiag):\n      return tf.linalg.LinearOperatorDiag(\n          diag=w[..., tf.newaxis] * op.diag_part(),\n          is_non_singular=op.is_non_singular,\n          is_self_adjoint=op.is_self_adjoint,\n          is_positive_definite=op.is_positive_definite)\n    if isinstance(op, tf.linalg.LinearOperatorLowerTriangular):\n      return tf.linalg.LinearOperatorLowerTriangular(\n          tril=w[..., tf.newaxis, tf.newaxis] * op.to_dense(),\n          is_non_singular=op.is_non_singular,\n          is_self_adjoint=op.is_self_adjoint,\n          is_positive_definite=op.is_positive_definite)\n    raise NotImplementedError(\n        \"Unsupported Linop type ({})\".format(type(op).__name__))", "code_tokens": ["def", "linop_scale", "(", "w", ",", "op", ")", ":", "# We assume w > 0. (This assumption only relates to the is_* attributes.)", "with", "tf", ".", "name_scope", "(", "\"linop_scale\"", ")", ":", "# TODO(b/35301104): LinearOperatorComposition doesn't combine operators, so", "# special case combinations here. Once it does, this function can be", "# replaced by:", "#     return linop_composition_lib.LinearOperatorComposition([", "#         scaled_identity(w), op])", "def", "scaled_identity", "(", "w", ")", ":", "return", "tf", ".", "linalg", ".", "LinearOperatorScaledIdentity", "(", "num_rows", "=", "op", ".", "range_dimension_tensor", "(", ")", ",", "multiplier", "=", "w", ",", "is_non_singular", "=", "op", ".", "is_non_singular", ",", "is_self_adjoint", "=", "op", ".", "is_self_adjoint", ",", "is_positive_definite", "=", "op", ".", "is_positive_definite", ")", "if", "isinstance", "(", "op", ",", "tf", ".", "linalg", ".", "LinearOperatorIdentity", ")", ":", "return", "scaled_identity", "(", "w", ")", "if", "isinstance", "(", "op", ",", "tf", ".", "linalg", ".", "LinearOperatorScaledIdentity", ")", ":", "return", "scaled_identity", "(", "w", "*", "op", ".", "multiplier", ")", "if", "isinstance", "(", "op", ",", "tf", ".", "linalg", ".", "LinearOperatorDiag", ")", ":", "return", "tf", ".", "linalg", ".", "LinearOperatorDiag", "(", "diag", "=", "w", "[", "...", ",", "tf", ".", "newaxis", "]", "*", "op", ".", "diag_part", "(", ")", ",", "is_non_singular", "=", "op", ".", "is_non_singular", ",", "is_self_adjoint", "=", "op", ".", "is_self_adjoint", ",", "is_positive_definite", "=", "op", ".", "is_positive_definite", ")", "if", "isinstance", "(", "op", ",", "tf", ".", "linalg", ".", "LinearOperatorLowerTriangular", ")", ":", "return", "tf", ".", "linalg", ".", "LinearOperatorLowerTriangular", "(", "tril", "=", "w", "[", "...", ",", "tf", ".", "newaxis", ",", "tf", ".", "newaxis", "]", "*", "op", ".", "to_dense", "(", ")", ",", "is_non_singular", "=", "op", ".", "is_non_singular", ",", "is_self_adjoint", "=", "op", ".", "is_self_adjoint", ",", "is_positive_definite", "=", "op", ".", "is_positive_definite", ")", "raise", "NotImplementedError", "(", "\"Unsupported Linop type ({})\"", ".", "format", "(", "type", "(", "op", ")", ".", "__name__", ")", ")"], "docstring": "Creates weighted `LinOp` from existing `LinOp`.", "docstring_tokens": ["Creates", "weighted", "LinOp", "from", "existing", "LinOp", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/vector_diffeomixture.py#L899-L933", "partition": "test", "index": 684, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/mixture_same_family.py", "func_name": "_outer_squared_difference", "original_string": "def _outer_squared_difference(x, y):\n  \"\"\"Convenience function analogous to tf.squared_difference.\"\"\"\n  z = x - y\n  return z[..., tf.newaxis, :] * z[..., tf.newaxis]", "language": "python", "code": "def _outer_squared_difference(x, y):\n  \"\"\"Convenience function analogous to tf.squared_difference.\"\"\"\n  z = x - y\n  return z[..., tf.newaxis, :] * z[..., tf.newaxis]", "code_tokens": ["def", "_outer_squared_difference", "(", "x", ",", "y", ")", ":", "z", "=", "x", "-", "y", "return", "z", "[", "...", ",", "tf", ".", "newaxis", ",", ":", "]", "*", "z", "[", "...", ",", "tf", ".", "newaxis", "]"], "docstring": "Convenience function analogous to tf.squared_difference.", "docstring_tokens": ["Convenience", "function", "analogous", "to", "tf", ".", "squared_difference", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/mixture_same_family.py#L536-L539", "partition": "test", "index": 756, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/mixture.py", "func_name": "Mixture._expand_to_event_rank", "original_string": "def _expand_to_event_rank(self, x):\n    \"\"\"Expand the rank of x up to static_event_rank times for broadcasting.\n\n    The static event rank was checked to not be None at construction time.\n\n    Args:\n      x: A tensor to expand.\n    Returns:\n      The expanded tensor.\n    \"\"\"\n    expanded_x = x\n    for _ in range(tensorshape_util.rank(self.event_shape)):\n      expanded_x = tf.expand_dims(expanded_x, -1)\n    return expanded_x", "language": "python", "code": "def _expand_to_event_rank(self, x):\n    \"\"\"Expand the rank of x up to static_event_rank times for broadcasting.\n\n    The static event rank was checked to not be None at construction time.\n\n    Args:\n      x: A tensor to expand.\n    Returns:\n      The expanded tensor.\n    \"\"\"\n    expanded_x = x\n    for _ in range(tensorshape_util.rank(self.event_shape)):\n      expanded_x = tf.expand_dims(expanded_x, -1)\n    return expanded_x", "code_tokens": ["def", "_expand_to_event_rank", "(", "self", ",", "x", ")", ":", "expanded_x", "=", "x", "for", "_", "in", "range", "(", "tensorshape_util", ".", "rank", "(", "self", ".", "event_shape", ")", ")", ":", "expanded_x", "=", "tf", ".", "expand_dims", "(", "expanded_x", ",", "-", "1", ")", "return", "expanded_x"], "docstring": "Expand the rank of x up to static_event_rank times for broadcasting.\n\n    The static event rank was checked to not be None at construction time.\n\n    Args:\n      x: A tensor to expand.\n    Returns:\n      The expanded tensor.", "docstring_tokens": ["Expand", "the", "rank", "of", "x", "up", "to", "static_event_rank", "times", "for", "broadcasting", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/mixture.py#L235-L248", "partition": "test", "index": 649, "time": "2018-06-12 21:47:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "build_kalman_filter_step", "original_string": "def build_kalman_filter_step(get_transition_matrix_for_timestep,\n                             get_transition_noise_for_timestep,\n                             get_observation_matrix_for_timestep,\n                             get_observation_noise_for_timestep):\n  \"\"\"Build a callable that performs one step of Kalman filtering.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n    get_transition_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[latent_size]`.\n    get_observation_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[observation_size, observation_size]`.\n    get_observation_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[observation_size]`.\n\n  Returns:\n    kalman_filter_step: a callable that updates a KalmanFilterState\n      from timestep `t-1` to `t`.\n  \"\"\"\n\n  def kalman_filter_step(state, elems_t):\n    \"\"\"Run a single step of Kalman filtering.\n\n    Args:\n      state: A `KalmanFilterState` object representing the previous\n        filter state at time `t-1`.\n      elems_t: A tuple of Tensors `(x_t, mask_t)`, or a `Tensor` `x_t`.\n        `x_t` is a `Tensor` with rightmost shape dimensions\n        `[observation_size, 1]` representing the vector observed at time `t`,\n        and `mask_t` is a `Tensor` with rightmost dimensions`[1, 1]`\n        representing the observation mask at time `t`. Both `x_t` and `mask_t`\n        may have batch dimensions, which must be compatible with the batch\n        dimensions of `state.predicted_mean` and `state.predictived_cov`\n        respectively. If `mask_t` is not provided, it is assumed to be `None`.\n\n    Returns:\n      new_state: A `KalmanFilterState` object representing the new\n        filter state at time `t`.\n    \"\"\"\n\n    if isinstance(elems_t, tuple):\n      x_t, mask_t = elems_t\n    else:\n      x_t = elems_t\n      mask_t = None\n\n    observation_matrix = get_observation_matrix_for_timestep(state.timestep)\n    observation_noise = get_observation_noise_for_timestep(state.timestep)\n    if mask_t is not None:\n      # Before running the update, fill in masked observations using the prior\n      # expectation. The precise filled value shouldn't matter since updates\n      # from masked elements will not be selected below, but we need to ensure\n      # that any results we incidently compute on masked values are at least\n      # finite (not inf or NaN) so that they don't screw up gradient propagation\n      # through `tf.where`, as described in\n      #  https://github.com/tensorflow/tensorflow/issues/2540.\n      # We fill with the prior expectation because any fixed value such as zero\n      # might be arbitrarily unlikely under the prior, leading to overflow in\n      # the updates, but the prior expectation should always be a\n      # 'reasonable' observation.\n      x_expected = _propagate_mean(state.predicted_mean,\n                                   observation_matrix,\n                                   observation_noise) * tf.ones_like(x_t)\n      x_t = tf.where(\n          tf.broadcast_to(mask_t, tf.shape(input=x_expected)), x_expected,\n          tf.broadcast_to(x_t, tf.shape(input=x_expected)))\n\n    # Given predicted mean u_{t|t-1} and covariance P_{t|t-1} from the\n    # previous step, incorporate the observation x_t, producing the\n    # filtered mean u_t and covariance P_t.\n    (filtered_mean,\n     filtered_cov,\n     observation_dist) = linear_gaussian_update(\n         state.predicted_mean, state.predicted_cov,\n         observation_matrix, observation_noise,\n         x_t)\n\n    # Compute the marginal likelihood p(x_{t} | x_{:t-1}) for this\n    # observation.\n    log_marginal_likelihood = observation_dist.log_prob(x_t[..., 0])\n\n    if mask_t is not None:\n      filtered_mean = tf.where(\n          tf.broadcast_to(mask_t, tf.shape(input=filtered_mean)),\n          state.predicted_mean, filtered_mean)\n      filtered_cov = tf.where(\n          tf.broadcast_to(mask_t, tf.shape(input=filtered_cov)),\n          state.predicted_cov, filtered_cov)\n      log_marginal_likelihood = tf.where(\n          tf.broadcast_to(mask_t[..., 0, 0],\n                          tf.shape(input=log_marginal_likelihood)),\n          tf.zeros_like(log_marginal_likelihood),\n          log_marginal_likelihood)\n\n    # Run the filtered posterior through the transition\n    # model to predict the next time step:\n    #  u_{t|t-1} = F_t u_{t-1} + b_t\n    #  P_{t|t-1} = F_t P_{t-1} F_t' + Q_t\n    predicted_mean, predicted_cov = kalman_transition(\n        filtered_mean,\n        filtered_cov,\n        get_transition_matrix_for_timestep(state.timestep),\n        get_transition_noise_for_timestep(state.timestep))\n\n    return KalmanFilterState(\n        filtered_mean, filtered_cov,\n        predicted_mean, predicted_cov,\n        observation_dist.mean()[..., tf.newaxis],\n        observation_dist.covariance(),\n        log_marginal_likelihood,\n        state.timestep+1)\n\n  return kalman_filter_step", "language": "python", "code": "def build_kalman_filter_step(get_transition_matrix_for_timestep,\n                             get_transition_noise_for_timestep,\n                             get_observation_matrix_for_timestep,\n                             get_observation_noise_for_timestep):\n  \"\"\"Build a callable that performs one step of Kalman filtering.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n    get_transition_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[latent_size]`.\n    get_observation_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[observation_size, observation_size]`.\n    get_observation_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[observation_size]`.\n\n  Returns:\n    kalman_filter_step: a callable that updates a KalmanFilterState\n      from timestep `t-1` to `t`.\n  \"\"\"\n\n  def kalman_filter_step(state, elems_t):\n    \"\"\"Run a single step of Kalman filtering.\n\n    Args:\n      state: A `KalmanFilterState` object representing the previous\n        filter state at time `t-1`.\n      elems_t: A tuple of Tensors `(x_t, mask_t)`, or a `Tensor` `x_t`.\n        `x_t` is a `Tensor` with rightmost shape dimensions\n        `[observation_size, 1]` representing the vector observed at time `t`,\n        and `mask_t` is a `Tensor` with rightmost dimensions`[1, 1]`\n        representing the observation mask at time `t`. Both `x_t` and `mask_t`\n        may have batch dimensions, which must be compatible with the batch\n        dimensions of `state.predicted_mean` and `state.predictived_cov`\n        respectively. If `mask_t` is not provided, it is assumed to be `None`.\n\n    Returns:\n      new_state: A `KalmanFilterState` object representing the new\n        filter state at time `t`.\n    \"\"\"\n\n    if isinstance(elems_t, tuple):\n      x_t, mask_t = elems_t\n    else:\n      x_t = elems_t\n      mask_t = None\n\n    observation_matrix = get_observation_matrix_for_timestep(state.timestep)\n    observation_noise = get_observation_noise_for_timestep(state.timestep)\n    if mask_t is not None:\n      # Before running the update, fill in masked observations using the prior\n      # expectation. The precise filled value shouldn't matter since updates\n      # from masked elements will not be selected below, but we need to ensure\n      # that any results we incidently compute on masked values are at least\n      # finite (not inf or NaN) so that they don't screw up gradient propagation\n      # through `tf.where`, as described in\n      #  https://github.com/tensorflow/tensorflow/issues/2540.\n      # We fill with the prior expectation because any fixed value such as zero\n      # might be arbitrarily unlikely under the prior, leading to overflow in\n      # the updates, but the prior expectation should always be a\n      # 'reasonable' observation.\n      x_expected = _propagate_mean(state.predicted_mean,\n                                   observation_matrix,\n                                   observation_noise) * tf.ones_like(x_t)\n      x_t = tf.where(\n          tf.broadcast_to(mask_t, tf.shape(input=x_expected)), x_expected,\n          tf.broadcast_to(x_t, tf.shape(input=x_expected)))\n\n    # Given predicted mean u_{t|t-1} and covariance P_{t|t-1} from the\n    # previous step, incorporate the observation x_t, producing the\n    # filtered mean u_t and covariance P_t.\n    (filtered_mean,\n     filtered_cov,\n     observation_dist) = linear_gaussian_update(\n         state.predicted_mean, state.predicted_cov,\n         observation_matrix, observation_noise,\n         x_t)\n\n    # Compute the marginal likelihood p(x_{t} | x_{:t-1}) for this\n    # observation.\n    log_marginal_likelihood = observation_dist.log_prob(x_t[..., 0])\n\n    if mask_t is not None:\n      filtered_mean = tf.where(\n          tf.broadcast_to(mask_t, tf.shape(input=filtered_mean)),\n          state.predicted_mean, filtered_mean)\n      filtered_cov = tf.where(\n          tf.broadcast_to(mask_t, tf.shape(input=filtered_cov)),\n          state.predicted_cov, filtered_cov)\n      log_marginal_likelihood = tf.where(\n          tf.broadcast_to(mask_t[..., 0, 0],\n                          tf.shape(input=log_marginal_likelihood)),\n          tf.zeros_like(log_marginal_likelihood),\n          log_marginal_likelihood)\n\n    # Run the filtered posterior through the transition\n    # model to predict the next time step:\n    #  u_{t|t-1} = F_t u_{t-1} + b_t\n    #  P_{t|t-1} = F_t P_{t-1} F_t' + Q_t\n    predicted_mean, predicted_cov = kalman_transition(\n        filtered_mean,\n        filtered_cov,\n        get_transition_matrix_for_timestep(state.timestep),\n        get_transition_noise_for_timestep(state.timestep))\n\n    return KalmanFilterState(\n        filtered_mean, filtered_cov,\n        predicted_mean, predicted_cov,\n        observation_dist.mean()[..., tf.newaxis],\n        observation_dist.covariance(),\n        log_marginal_likelihood,\n        state.timestep+1)\n\n  return kalman_filter_step", "code_tokens": ["def", "build_kalman_filter_step", "(", "get_transition_matrix_for_timestep", ",", "get_transition_noise_for_timestep", ",", "get_observation_matrix_for_timestep", ",", "get_observation_noise_for_timestep", ")", ":", "def", "kalman_filter_step", "(", "state", ",", "elems_t", ")", ":", "\"\"\"Run a single step of Kalman filtering.\n\n    Args:\n      state: A `KalmanFilterState` object representing the previous\n        filter state at time `t-1`.\n      elems_t: A tuple of Tensors `(x_t, mask_t)`, or a `Tensor` `x_t`.\n        `x_t` is a `Tensor` with rightmost shape dimensions\n        `[observation_size, 1]` representing the vector observed at time `t`,\n        and `mask_t` is a `Tensor` with rightmost dimensions`[1, 1]`\n        representing the observation mask at time `t`. Both `x_t` and `mask_t`\n        may have batch dimensions, which must be compatible with the batch\n        dimensions of `state.predicted_mean` and `state.predictived_cov`\n        respectively. If `mask_t` is not provided, it is assumed to be `None`.\n\n    Returns:\n      new_state: A `KalmanFilterState` object representing the new\n        filter state at time `t`.\n    \"\"\"", "if", "isinstance", "(", "elems_t", ",", "tuple", ")", ":", "x_t", ",", "mask_t", "=", "elems_t", "else", ":", "x_t", "=", "elems_t", "mask_t", "=", "None", "observation_matrix", "=", "get_observation_matrix_for_timestep", "(", "state", ".", "timestep", ")", "observation_noise", "=", "get_observation_noise_for_timestep", "(", "state", ".", "timestep", ")", "if", "mask_t", "is", "not", "None", ":", "# Before running the update, fill in masked observations using the prior", "# expectation. The precise filled value shouldn't matter since updates", "# from masked elements will not be selected below, but we need to ensure", "# that any results we incidently compute on masked values are at least", "# finite (not inf or NaN) so that they don't screw up gradient propagation", "# through `tf.where`, as described in", "#  https://github.com/tensorflow/tensorflow/issues/2540.", "# We fill with the prior expectation because any fixed value such as zero", "# might be arbitrarily unlikely under the prior, leading to overflow in", "# the updates, but the prior expectation should always be a", "# 'reasonable' observation.", "x_expected", "=", "_propagate_mean", "(", "state", ".", "predicted_mean", ",", "observation_matrix", ",", "observation_noise", ")", "*", "tf", ".", "ones_like", "(", "x_t", ")", "x_t", "=", "tf", ".", "where", "(", "tf", ".", "broadcast_to", "(", "mask_t", ",", "tf", ".", "shape", "(", "input", "=", "x_expected", ")", ")", ",", "x_expected", ",", "tf", ".", "broadcast_to", "(", "x_t", ",", "tf", ".", "shape", "(", "input", "=", "x_expected", ")", ")", ")", "# Given predicted mean u_{t|t-1} and covariance P_{t|t-1} from the", "# previous step, incorporate the observation x_t, producing the", "# filtered mean u_t and covariance P_t.", "(", "filtered_mean", ",", "filtered_cov", ",", "observation_dist", ")", "=", "linear_gaussian_update", "(", "state", ".", "predicted_mean", ",", "state", ".", "predicted_cov", ",", "observation_matrix", ",", "observation_noise", ",", "x_t", ")", "# Compute the marginal likelihood p(x_{t} | x_{:t-1}) for this", "# observation.", "log_marginal_likelihood", "=", "observation_dist", ".", "log_prob", "(", "x_t", "[", "...", ",", "0", "]", ")", "if", "mask_t", "is", "not", "None", ":", "filtered_mean", "=", "tf", ".", "where", "(", "tf", ".", "broadcast_to", "(", "mask_t", ",", "tf", ".", "shape", "(", "input", "=", "filtered_mean", ")", ")", ",", "state", ".", "predicted_mean", ",", "filtered_mean", ")", "filtered_cov", "=", "tf", ".", "where", "(", "tf", ".", "broadcast_to", "(", "mask_t", ",", "tf", ".", "shape", "(", "input", "=", "filtered_cov", ")", ")", ",", "state", ".", "predicted_cov", ",", "filtered_cov", ")", "log_marginal_likelihood", "=", "tf", ".", "where", "(", "tf", ".", "broadcast_to", "(", "mask_t", "[", "...", ",", "0", ",", "0", "]", ",", "tf", ".", "shape", "(", "input", "=", "log_marginal_likelihood", ")", ")", ",", "tf", ".", "zeros_like", "(", "log_marginal_likelihood", ")", ",", "log_marginal_likelihood", ")", "# Run the filtered posterior through the transition", "# model to predict the next time step:", "#  u_{t|t-1} = F_t u_{t-1} + b_t", "#  P_{t|t-1} = F_t P_{t-1} F_t' + Q_t", "predicted_mean", ",", "predicted_cov", "=", "kalman_transition", "(", "filtered_mean", ",", "filtered_cov", ",", "get_transition_matrix_for_timestep", "(", "state", ".", "timestep", ")", ",", "get_transition_noise_for_timestep", "(", "state", ".", "timestep", ")", ")", "return", "KalmanFilterState", "(", "filtered_mean", ",", "filtered_cov", ",", "predicted_mean", ",", "predicted_cov", ",", "observation_dist", ".", "mean", "(", ")", "[", "...", ",", "tf", ".", "newaxis", "]", ",", "observation_dist", ".", "covariance", "(", ")", ",", "log_marginal_likelihood", ",", "state", ".", "timestep", "+", "1", ")", "return", "kalman_filter_step"], "docstring": "Build a callable that performs one step of Kalman filtering.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n    get_transition_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[latent_size]`.\n    get_observation_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[observation_size, observation_size]`.\n    get_observation_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[observation_size]`.\n\n  Returns:\n    kalman_filter_step: a callable that updates a KalmanFilterState\n      from timestep `t-1` to `t`.", "docstring_tokens": ["Build", "a", "callable", "that", "performs", "one", "step", "of", "Kalman", "filtering", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L1275-L1394", "partition": "test", "index": 942, "time": "2018-06-13 17:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "LinearGaussianStateSpaceModel._joint_mean", "original_string": "def _joint_mean(self):\n    \"\"\"Compute prior means for all variables via dynamic programming.\n\n    Returns:\n      latent_means: Prior means of latent states `z_t`, as a `Tensor`\n        of shape `batch_shape + [num_timesteps, latent_size]`\n      observation_means: Prior covariance matrices of observations\n        `x_t`, as a `Tensor` of shape `batch_shape + [num_timesteps,\n        observation_size]`\n    \"\"\"\n\n    with tf.name_scope(\"mean_joint\"):\n\n      # The initial timestep is a special case, since we sample the\n      # latent state from the prior rather than the transition model.\n\n      with tf.control_dependencies(self.runtime_assertions):\n        # Broadcast to ensure we represent the full batch shape.\n        initial_latent_mean = _broadcast_to_shape(\n            self.initial_state_prior.mean()[..., tf.newaxis],\n            tf.concat([self.batch_shape_tensor(),\n                       [self.latent_size, 1]], axis=0))\n\n      initial_observation_mean = _propagate_mean(\n          initial_latent_mean,\n          self.get_observation_matrix_for_timestep(self.initial_step),\n          self.get_observation_noise_for_timestep(self.initial_step))\n\n      mean_step = build_kalman_mean_step(\n          self.get_transition_matrix_for_timestep,\n          self.get_transition_noise_for_timestep,\n          self.get_observation_matrix_for_timestep,\n          self.get_observation_noise_for_timestep)\n\n      # Scan over all timesteps following the initial step.\n      (latent_means, observation_means) = tf.scan(\n          mean_step,\n          elems=tf.range(self.initial_step+1, self.final_step),\n          initializer=(initial_latent_mean, initial_observation_mean))\n\n      # Squish the initial step back on top of the other (scanned) timesteps\n      latent_means = tf.concat([initial_latent_mean[tf.newaxis, ...],\n                                latent_means], axis=0)\n      observation_means = tf.concat([initial_observation_mean[tf.newaxis, ...],\n                                     observation_means], axis=0)\n\n      # Put dimensions back in order. The samples we've computed have\n      # shape `[num_timesteps, batch_shape, size, 1]`, where `size`\n      # is the dimension of the latent or observation spaces\n      # respectively, but we want to return values with shape\n      # `[batch_shape, num_timesteps, size]`.\n      latent_means = tf.squeeze(latent_means, -1)\n      latent_means = distribution_util.move_dimension(latent_means, 0, -2)\n      observation_means = tf.squeeze(observation_means, -1)\n      observation_means = distribution_util.move_dimension(\n          observation_means, 0, -2)\n\n      return latent_means, observation_means", "language": "python", "code": "def _joint_mean(self):\n    \"\"\"Compute prior means for all variables via dynamic programming.\n\n    Returns:\n      latent_means: Prior means of latent states `z_t`, as a `Tensor`\n        of shape `batch_shape + [num_timesteps, latent_size]`\n      observation_means: Prior covariance matrices of observations\n        `x_t`, as a `Tensor` of shape `batch_shape + [num_timesteps,\n        observation_size]`\n    \"\"\"\n\n    with tf.name_scope(\"mean_joint\"):\n\n      # The initial timestep is a special case, since we sample the\n      # latent state from the prior rather than the transition model.\n\n      with tf.control_dependencies(self.runtime_assertions):\n        # Broadcast to ensure we represent the full batch shape.\n        initial_latent_mean = _broadcast_to_shape(\n            self.initial_state_prior.mean()[..., tf.newaxis],\n            tf.concat([self.batch_shape_tensor(),\n                       [self.latent_size, 1]], axis=0))\n\n      initial_observation_mean = _propagate_mean(\n          initial_latent_mean,\n          self.get_observation_matrix_for_timestep(self.initial_step),\n          self.get_observation_noise_for_timestep(self.initial_step))\n\n      mean_step = build_kalman_mean_step(\n          self.get_transition_matrix_for_timestep,\n          self.get_transition_noise_for_timestep,\n          self.get_observation_matrix_for_timestep,\n          self.get_observation_noise_for_timestep)\n\n      # Scan over all timesteps following the initial step.\n      (latent_means, observation_means) = tf.scan(\n          mean_step,\n          elems=tf.range(self.initial_step+1, self.final_step),\n          initializer=(initial_latent_mean, initial_observation_mean))\n\n      # Squish the initial step back on top of the other (scanned) timesteps\n      latent_means = tf.concat([initial_latent_mean[tf.newaxis, ...],\n                                latent_means], axis=0)\n      observation_means = tf.concat([initial_observation_mean[tf.newaxis, ...],\n                                     observation_means], axis=0)\n\n      # Put dimensions back in order. The samples we've computed have\n      # shape `[num_timesteps, batch_shape, size, 1]`, where `size`\n      # is the dimension of the latent or observation spaces\n      # respectively, but we want to return values with shape\n      # `[batch_shape, num_timesteps, size]`.\n      latent_means = tf.squeeze(latent_means, -1)\n      latent_means = distribution_util.move_dimension(latent_means, 0, -2)\n      observation_means = tf.squeeze(observation_means, -1)\n      observation_means = distribution_util.move_dimension(\n          observation_means, 0, -2)\n\n      return latent_means, observation_means", "code_tokens": ["def", "_joint_mean", "(", "self", ")", ":", "with", "tf", ".", "name_scope", "(", "\"mean_joint\"", ")", ":", "# The initial timestep is a special case, since we sample the", "# latent state from the prior rather than the transition model.", "with", "tf", ".", "control_dependencies", "(", "self", ".", "runtime_assertions", ")", ":", "# Broadcast to ensure we represent the full batch shape.", "initial_latent_mean", "=", "_broadcast_to_shape", "(", "self", ".", "initial_state_prior", ".", "mean", "(", ")", "[", "...", ",", "tf", ".", "newaxis", "]", ",", "tf", ".", "concat", "(", "[", "self", ".", "batch_shape_tensor", "(", ")", ",", "[", "self", ".", "latent_size", ",", "1", "]", "]", ",", "axis", "=", "0", ")", ")", "initial_observation_mean", "=", "_propagate_mean", "(", "initial_latent_mean", ",", "self", ".", "get_observation_matrix_for_timestep", "(", "self", ".", "initial_step", ")", ",", "self", ".", "get_observation_noise_for_timestep", "(", "self", ".", "initial_step", ")", ")", "mean_step", "=", "build_kalman_mean_step", "(", "self", ".", "get_transition_matrix_for_timestep", ",", "self", ".", "get_transition_noise_for_timestep", ",", "self", ".", "get_observation_matrix_for_timestep", ",", "self", ".", "get_observation_noise_for_timestep", ")", "# Scan over all timesteps following the initial step.", "(", "latent_means", ",", "observation_means", ")", "=", "tf", ".", "scan", "(", "mean_step", ",", "elems", "=", "tf", ".", "range", "(", "self", ".", "initial_step", "+", "1", ",", "self", ".", "final_step", ")", ",", "initializer", "=", "(", "initial_latent_mean", ",", "initial_observation_mean", ")", ")", "# Squish the initial step back on top of the other (scanned) timesteps", "latent_means", "=", "tf", ".", "concat", "(", "[", "initial_latent_mean", "[", "tf", ".", "newaxis", ",", "...", "]", ",", "latent_means", "]", ",", "axis", "=", "0", ")", "observation_means", "=", "tf", ".", "concat", "(", "[", "initial_observation_mean", "[", "tf", ".", "newaxis", ",", "...", "]", ",", "observation_means", "]", ",", "axis", "=", "0", ")", "# Put dimensions back in order. The samples we've computed have", "# shape `[num_timesteps, batch_shape, size, 1]`, where `size`", "# is the dimension of the latent or observation spaces", "# respectively, but we want to return values with shape", "# `[batch_shape, num_timesteps, size]`.", "latent_means", "=", "tf", ".", "squeeze", "(", "latent_means", ",", "-", "1", ")", "latent_means", "=", "distribution_util", ".", "move_dimension", "(", "latent_means", ",", "0", ",", "-", "2", ")", "observation_means", "=", "tf", ".", "squeeze", "(", "observation_means", ",", "-", "1", ")", "observation_means", "=", "distribution_util", ".", "move_dimension", "(", "observation_means", ",", "0", ",", "-", "2", ")", "return", "latent_means", ",", "observation_means"], "docstring": "Compute prior means for all variables via dynamic programming.\n\n    Returns:\n      latent_means: Prior means of latent states `z_t`, as a `Tensor`\n        of shape `batch_shape + [num_timesteps, latent_size]`\n      observation_means: Prior covariance matrices of observations\n        `x_t`, as a `Tensor` of shape `batch_shape + [num_timesteps,\n        observation_size]`", "docstring_tokens": ["Compute", "prior", "means", "for", "all", "variables", "via", "dynamic", "programming", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L981-L1038", "partition": "test", "index": 953, "time": "2018-06-13 17:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "_augment_sample_shape", "original_string": "def _augment_sample_shape(partial_batch_dist,\n                          full_sample_and_batch_shape,\n                          validate_args=False):\n  \"\"\"Augment a sample shape to broadcast batch dimensions.\n\n  Computes an augmented sample shape, so that any batch dimensions not\n  part of the distribution `partial_batch_dist` are treated as identical\n  distributions.\n\n  # partial_batch_dist.batch_shape  = [      7]\n  # full_sample_and_batch_shape     = [3, 4, 7]\n  # => return an augmented sample shape of [3, 4] so that\n  #    partial_batch_dist.sample(augmented_sample_shape) has combined\n  #    sample and batch shape of [3, 4, 7].\n\n  Args:\n    partial_batch_dist: `tfd.Distribution` instance with batch shape a\n      prefix of `full_sample_and_batch_shape`.\n    full_sample_and_batch_shape: a Tensor or Tensor-like shape.\n    validate_args: if True, check for shape errors at runtime.\n  Returns:\n    augmented_sample_shape: sample shape such that\n      `partial_batch_dist.sample(augmented_sample_shape)` has combined\n      sample and batch shape of `full_sample_and_batch_shape`.\n\n  Raises:\n    ValueError: if `partial_batch_dist.batch_shape` has more dimensions than\n      `full_sample_and_batch_shape`.\n    NotImplementedError: if broadcasting would be required to make\n      `partial_batch_dist.batch_shape` into a prefix of\n      `full_sample_and_batch_shape` .\n  \"\"\"\n  full_ndims = distribution_util.prefer_static_shape(\n      full_sample_and_batch_shape)[0]\n  partial_batch_ndims = (\n      tensorshape_util.rank(partial_batch_dist.batch_shape)  # pylint: disable=g-long-ternary\n      if tensorshape_util.rank(partial_batch_dist.batch_shape) is not None\n      else distribution_util.prefer_static_shape(\n          partial_batch_dist.batch_shape_tensor())[0])\n\n  num_broadcast_dims = full_ndims - partial_batch_ndims\n\n  expected_partial_batch_shape = (\n      full_sample_and_batch_shape[num_broadcast_dims:])\n  expected_partial_batch_shape_static = tf.get_static_value(\n      full_sample_and_batch_shape[num_broadcast_dims:])\n\n  # Raise errors statically if possible.\n  num_broadcast_dims_static = tf.get_static_value(num_broadcast_dims)\n  if num_broadcast_dims_static is not None:\n    if num_broadcast_dims_static < 0:\n      raise ValueError(\"Cannot broadcast distribution {} batch shape to \"\n                       \"target batch shape with fewer dimensions\"\n                       .format(partial_batch_dist))\n  if (expected_partial_batch_shape_static is not None and\n      tensorshape_util.is_fully_defined(partial_batch_dist.batch_shape)):\n    if (partial_batch_dist.batch_shape and\n        any(expected_partial_batch_shape_static != tensorshape_util.as_list(\n            partial_batch_dist.batch_shape))):\n      raise NotImplementedError(\"Broadcasting is not supported; \"\n                                \"unexpected batch shape \"\n                                \"(expected {}, saw {}).\".format(\n                                    expected_partial_batch_shape_static,\n                                    partial_batch_dist.batch_shape\n                                ))\n  runtime_assertions = []\n  if validate_args:\n    runtime_assertions.append(\n        assert_util.assert_greater_equal(\n            tf.convert_to_tensor(value=num_broadcast_dims, dtype=tf.int32),\n            tf.zeros((), dtype=tf.int32),\n            message=(\"Cannot broadcast distribution {} batch shape to \"\n                     \"target batch shape with fewer dimensions.\".format(\n                         partial_batch_dist))))\n    runtime_assertions.append(\n        assert_util.assert_equal(\n            expected_partial_batch_shape,\n            partial_batch_dist.batch_shape_tensor(),\n            message=(\"Broadcasting is not supported; \"\n                     \"unexpected batch shape.\"),\n            name=\"assert_batch_shape_same\"))\n\n  with tf.control_dependencies(runtime_assertions):\n    return full_sample_and_batch_shape[:num_broadcast_dims]", "language": "python", "code": "def _augment_sample_shape(partial_batch_dist,\n                          full_sample_and_batch_shape,\n                          validate_args=False):\n  \"\"\"Augment a sample shape to broadcast batch dimensions.\n\n  Computes an augmented sample shape, so that any batch dimensions not\n  part of the distribution `partial_batch_dist` are treated as identical\n  distributions.\n\n  # partial_batch_dist.batch_shape  = [      7]\n  # full_sample_and_batch_shape     = [3, 4, 7]\n  # => return an augmented sample shape of [3, 4] so that\n  #    partial_batch_dist.sample(augmented_sample_shape) has combined\n  #    sample and batch shape of [3, 4, 7].\n\n  Args:\n    partial_batch_dist: `tfd.Distribution` instance with batch shape a\n      prefix of `full_sample_and_batch_shape`.\n    full_sample_and_batch_shape: a Tensor or Tensor-like shape.\n    validate_args: if True, check for shape errors at runtime.\n  Returns:\n    augmented_sample_shape: sample shape such that\n      `partial_batch_dist.sample(augmented_sample_shape)` has combined\n      sample and batch shape of `full_sample_and_batch_shape`.\n\n  Raises:\n    ValueError: if `partial_batch_dist.batch_shape` has more dimensions than\n      `full_sample_and_batch_shape`.\n    NotImplementedError: if broadcasting would be required to make\n      `partial_batch_dist.batch_shape` into a prefix of\n      `full_sample_and_batch_shape` .\n  \"\"\"\n  full_ndims = distribution_util.prefer_static_shape(\n      full_sample_and_batch_shape)[0]\n  partial_batch_ndims = (\n      tensorshape_util.rank(partial_batch_dist.batch_shape)  # pylint: disable=g-long-ternary\n      if tensorshape_util.rank(partial_batch_dist.batch_shape) is not None\n      else distribution_util.prefer_static_shape(\n          partial_batch_dist.batch_shape_tensor())[0])\n\n  num_broadcast_dims = full_ndims - partial_batch_ndims\n\n  expected_partial_batch_shape = (\n      full_sample_and_batch_shape[num_broadcast_dims:])\n  expected_partial_batch_shape_static = tf.get_static_value(\n      full_sample_and_batch_shape[num_broadcast_dims:])\n\n  # Raise errors statically if possible.\n  num_broadcast_dims_static = tf.get_static_value(num_broadcast_dims)\n  if num_broadcast_dims_static is not None:\n    if num_broadcast_dims_static < 0:\n      raise ValueError(\"Cannot broadcast distribution {} batch shape to \"\n                       \"target batch shape with fewer dimensions\"\n                       .format(partial_batch_dist))\n  if (expected_partial_batch_shape_static is not None and\n      tensorshape_util.is_fully_defined(partial_batch_dist.batch_shape)):\n    if (partial_batch_dist.batch_shape and\n        any(expected_partial_batch_shape_static != tensorshape_util.as_list(\n            partial_batch_dist.batch_shape))):\n      raise NotImplementedError(\"Broadcasting is not supported; \"\n                                \"unexpected batch shape \"\n                                \"(expected {}, saw {}).\".format(\n                                    expected_partial_batch_shape_static,\n                                    partial_batch_dist.batch_shape\n                                ))\n  runtime_assertions = []\n  if validate_args:\n    runtime_assertions.append(\n        assert_util.assert_greater_equal(\n            tf.convert_to_tensor(value=num_broadcast_dims, dtype=tf.int32),\n            tf.zeros((), dtype=tf.int32),\n            message=(\"Cannot broadcast distribution {} batch shape to \"\n                     \"target batch shape with fewer dimensions.\".format(\n                         partial_batch_dist))))\n    runtime_assertions.append(\n        assert_util.assert_equal(\n            expected_partial_batch_shape,\n            partial_batch_dist.batch_shape_tensor(),\n            message=(\"Broadcasting is not supported; \"\n                     \"unexpected batch shape.\"),\n            name=\"assert_batch_shape_same\"))\n\n  with tf.control_dependencies(runtime_assertions):\n    return full_sample_and_batch_shape[:num_broadcast_dims]", "code_tokens": ["def", "_augment_sample_shape", "(", "partial_batch_dist", ",", "full_sample_and_batch_shape", ",", "validate_args", "=", "False", ")", ":", "full_ndims", "=", "distribution_util", ".", "prefer_static_shape", "(", "full_sample_and_batch_shape", ")", "[", "0", "]", "partial_batch_ndims", "=", "(", "tensorshape_util", ".", "rank", "(", "partial_batch_dist", ".", "batch_shape", ")", "# pylint: disable=g-long-ternary", "if", "tensorshape_util", ".", "rank", "(", "partial_batch_dist", ".", "batch_shape", ")", "is", "not", "None", "else", "distribution_util", ".", "prefer_static_shape", "(", "partial_batch_dist", ".", "batch_shape_tensor", "(", ")", ")", "[", "0", "]", ")", "num_broadcast_dims", "=", "full_ndims", "-", "partial_batch_ndims", "expected_partial_batch_shape", "=", "(", "full_sample_and_batch_shape", "[", "num_broadcast_dims", ":", "]", ")", "expected_partial_batch_shape_static", "=", "tf", ".", "get_static_value", "(", "full_sample_and_batch_shape", "[", "num_broadcast_dims", ":", "]", ")", "# Raise errors statically if possible.", "num_broadcast_dims_static", "=", "tf", ".", "get_static_value", "(", "num_broadcast_dims", ")", "if", "num_broadcast_dims_static", "is", "not", "None", ":", "if", "num_broadcast_dims_static", "<", "0", ":", "raise", "ValueError", "(", "\"Cannot broadcast distribution {} batch shape to \"", "\"target batch shape with fewer dimensions\"", ".", "format", "(", "partial_batch_dist", ")", ")", "if", "(", "expected_partial_batch_shape_static", "is", "not", "None", "and", "tensorshape_util", ".", "is_fully_defined", "(", "partial_batch_dist", ".", "batch_shape", ")", ")", ":", "if", "(", "partial_batch_dist", ".", "batch_shape", "and", "any", "(", "expected_partial_batch_shape_static", "!=", "tensorshape_util", ".", "as_list", "(", "partial_batch_dist", ".", "batch_shape", ")", ")", ")", ":", "raise", "NotImplementedError", "(", "\"Broadcasting is not supported; \"", "\"unexpected batch shape \"", "\"(expected {}, saw {}).\"", ".", "format", "(", "expected_partial_batch_shape_static", ",", "partial_batch_dist", ".", "batch_shape", ")", ")", "runtime_assertions", "=", "[", "]", "if", "validate_args", ":", "runtime_assertions", ".", "append", "(", "assert_util", ".", "assert_greater_equal", "(", "tf", ".", "convert_to_tensor", "(", "value", "=", "num_broadcast_dims", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "tf", ".", "zeros", "(", "(", ")", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "message", "=", "(", "\"Cannot broadcast distribution {} batch shape to \"", "\"target batch shape with fewer dimensions.\"", ".", "format", "(", "partial_batch_dist", ")", ")", ")", ")", "runtime_assertions", ".", "append", "(", "assert_util", ".", "assert_equal", "(", "expected_partial_batch_shape", ",", "partial_batch_dist", ".", "batch_shape_tensor", "(", ")", ",", "message", "=", "(", "\"Broadcasting is not supported; \"", "\"unexpected batch shape.\"", ")", ",", "name", "=", "\"assert_batch_shape_same\"", ")", ")", "with", "tf", ".", "control_dependencies", "(", "runtime_assertions", ")", ":", "return", "full_sample_and_batch_shape", "[", ":", "num_broadcast_dims", "]"], "docstring": "Augment a sample shape to broadcast batch dimensions.\n\n  Computes an augmented sample shape, so that any batch dimensions not\n  part of the distribution `partial_batch_dist` are treated as identical\n  distributions.\n\n  # partial_batch_dist.batch_shape  = [      7]\n  # full_sample_and_batch_shape     = [3, 4, 7]\n  # => return an augmented sample shape of [3, 4] so that\n  #    partial_batch_dist.sample(augmented_sample_shape) has combined\n  #    sample and batch shape of [3, 4, 7].\n\n  Args:\n    partial_batch_dist: `tfd.Distribution` instance with batch shape a\n      prefix of `full_sample_and_batch_shape`.\n    full_sample_and_batch_shape: a Tensor or Tensor-like shape.\n    validate_args: if True, check for shape errors at runtime.\n  Returns:\n    augmented_sample_shape: sample shape such that\n      `partial_batch_dist.sample(augmented_sample_shape)` has combined\n      sample and batch shape of `full_sample_and_batch_shape`.\n\n  Raises:\n    ValueError: if `partial_batch_dist.batch_shape` has more dimensions than\n      `full_sample_and_batch_shape`.\n    NotImplementedError: if broadcasting would be required to make\n      `partial_batch_dist.batch_shape` into a prefix of\n      `full_sample_and_batch_shape` .", "docstring_tokens": ["Augment", "a", "sample", "shape", "to", "broadcast", "batch", "dimensions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L72-L155", "partition": "test", "index": 939, "time": "2018-06-13 17:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "_check_equal_shape", "original_string": "def _check_equal_shape(name,\n                       static_shape,\n                       dynamic_shape,\n                       static_target_shape,\n                       dynamic_target_shape=None):\n  \"\"\"Check that source and target shape match, statically if possible.\"\"\"\n\n  static_target_shape = tf.TensorShape(static_target_shape)\n  if tensorshape_util.is_fully_defined(\n      static_shape) and tensorshape_util.is_fully_defined(static_target_shape):\n    if static_shape != static_target_shape:\n      raise ValueError(\"{}: required shape {} but found {}\".\n                       format(name, static_target_shape, static_shape))\n    return None\n  else:\n    if dynamic_target_shape is None:\n      if tensorshape_util.is_fully_defined(static_target_shape):\n        dynamic_target_shape = tensorshape_util.as_list(static_target_shape)\n      else:\n        raise ValueError(\"{}: cannot infer target shape: no dynamic shape \"\n                         \"specified and static shape {} is not fully defined\".\n                         format(name, static_target_shape))\n    return assert_util.assert_equal(\n        dynamic_shape,\n        dynamic_target_shape,\n        message=(\"{}: required shape {}\".format(name, static_target_shape)))", "language": "python", "code": "def _check_equal_shape(name,\n                       static_shape,\n                       dynamic_shape,\n                       static_target_shape,\n                       dynamic_target_shape=None):\n  \"\"\"Check that source and target shape match, statically if possible.\"\"\"\n\n  static_target_shape = tf.TensorShape(static_target_shape)\n  if tensorshape_util.is_fully_defined(\n      static_shape) and tensorshape_util.is_fully_defined(static_target_shape):\n    if static_shape != static_target_shape:\n      raise ValueError(\"{}: required shape {} but found {}\".\n                       format(name, static_target_shape, static_shape))\n    return None\n  else:\n    if dynamic_target_shape is None:\n      if tensorshape_util.is_fully_defined(static_target_shape):\n        dynamic_target_shape = tensorshape_util.as_list(static_target_shape)\n      else:\n        raise ValueError(\"{}: cannot infer target shape: no dynamic shape \"\n                         \"specified and static shape {} is not fully defined\".\n                         format(name, static_target_shape))\n    return assert_util.assert_equal(\n        dynamic_shape,\n        dynamic_target_shape,\n        message=(\"{}: required shape {}\".format(name, static_target_shape)))", "code_tokens": ["def", "_check_equal_shape", "(", "name", ",", "static_shape", ",", "dynamic_shape", ",", "static_target_shape", ",", "dynamic_target_shape", "=", "None", ")", ":", "static_target_shape", "=", "tf", ".", "TensorShape", "(", "static_target_shape", ")", "if", "tensorshape_util", ".", "is_fully_defined", "(", "static_shape", ")", "and", "tensorshape_util", ".", "is_fully_defined", "(", "static_target_shape", ")", ":", "if", "static_shape", "!=", "static_target_shape", ":", "raise", "ValueError", "(", "\"{}: required shape {} but found {}\"", ".", "format", "(", "name", ",", "static_target_shape", ",", "static_shape", ")", ")", "return", "None", "else", ":", "if", "dynamic_target_shape", "is", "None", ":", "if", "tensorshape_util", ".", "is_fully_defined", "(", "static_target_shape", ")", ":", "dynamic_target_shape", "=", "tensorshape_util", ".", "as_list", "(", "static_target_shape", ")", "else", ":", "raise", "ValueError", "(", "\"{}: cannot infer target shape: no dynamic shape \"", "\"specified and static shape {} is not fully defined\"", ".", "format", "(", "name", ",", "static_target_shape", ")", ")", "return", "assert_util", ".", "assert_equal", "(", "dynamic_shape", ",", "dynamic_target_shape", ",", "message", "=", "(", "\"{}: required shape {}\"", ".", "format", "(", "name", ",", "static_target_shape", ")", ")", ")"], "docstring": "Check that source and target shape match, statically if possible.", "docstring_tokens": ["Check", "that", "source", "and", "target", "shape", "match", "statically", "if", "possible", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L44-L69", "partition": "test", "index": 938, "time": "2018-06-13 17:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "kalman_transition", "original_string": "def kalman_transition(filtered_mean, filtered_cov,\n                      transition_matrix, transition_noise):\n  \"\"\"Propagate a filtered distribution through a transition model.\"\"\"\n\n  predicted_mean = _propagate_mean(filtered_mean,\n                                   transition_matrix,\n                                   transition_noise)\n  predicted_cov = _propagate_cov(filtered_cov,\n                                 transition_matrix,\n                                 transition_noise)\n  return predicted_mean, predicted_cov", "language": "python", "code": "def kalman_transition(filtered_mean, filtered_cov,\n                      transition_matrix, transition_noise):\n  \"\"\"Propagate a filtered distribution through a transition model.\"\"\"\n\n  predicted_mean = _propagate_mean(filtered_mean,\n                                   transition_matrix,\n                                   transition_noise)\n  predicted_cov = _propagate_cov(filtered_cov,\n                                 transition_matrix,\n                                 transition_noise)\n  return predicted_mean, predicted_cov", "code_tokens": ["def", "kalman_transition", "(", "filtered_mean", ",", "filtered_cov", ",", "transition_matrix", ",", "transition_noise", ")", ":", "predicted_mean", "=", "_propagate_mean", "(", "filtered_mean", ",", "transition_matrix", ",", "transition_noise", ")", "predicted_cov", "=", "_propagate_cov", "(", "filtered_cov", ",", "transition_matrix", ",", "transition_noise", ")", "return", "predicted_mean", ",", "predicted_cov"], "docstring": "Propagate a filtered distribution through a transition model.", "docstring_tokens": ["Propagate", "a", "filtered", "distribution", "through", "a", "transition", "model", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L1522-L1532", "partition": "test", "index": 944, "time": "2018-06-13 17:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "build_kalman_mean_step", "original_string": "def build_kalman_mean_step(get_transition_matrix_for_timestep,\n                           get_transition_noise_for_timestep,\n                           get_observation_matrix_for_timestep,\n                           get_observation_noise_for_timestep):\n  \"\"\"Build a callable that performs one step of Kalman mean recursion.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n    get_transition_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[latent_size]`.\n    get_observation_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[observation_size, observation_size]`.\n    get_observation_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[observation_size]`.\n\n  Returns:\n    kalman_mean_step: a callable that computes latent state and\n      observation means at time `t`, given latent mean at time `t-1`.\n  \"\"\"\n\n  def mean_step(previous_means, t):\n    \"\"\"Single step of prior mean recursion.\"\"\"\n    previous_latent_mean, _ = previous_means\n\n    latent_mean = _propagate_mean(previous_latent_mean,\n                                  get_transition_matrix_for_timestep(t - 1),\n                                  get_transition_noise_for_timestep(t - 1))\n    observation_mean = _propagate_mean(latent_mean,\n                                       get_observation_matrix_for_timestep(t),\n                                       get_observation_noise_for_timestep(t))\n    return (latent_mean, observation_mean)\n\n  return mean_step", "language": "python", "code": "def build_kalman_mean_step(get_transition_matrix_for_timestep,\n                           get_transition_noise_for_timestep,\n                           get_observation_matrix_for_timestep,\n                           get_observation_noise_for_timestep):\n  \"\"\"Build a callable that performs one step of Kalman mean recursion.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n    get_transition_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[latent_size]`.\n    get_observation_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[observation_size, observation_size]`.\n    get_observation_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[observation_size]`.\n\n  Returns:\n    kalman_mean_step: a callable that computes latent state and\n      observation means at time `t`, given latent mean at time `t-1`.\n  \"\"\"\n\n  def mean_step(previous_means, t):\n    \"\"\"Single step of prior mean recursion.\"\"\"\n    previous_latent_mean, _ = previous_means\n\n    latent_mean = _propagate_mean(previous_latent_mean,\n                                  get_transition_matrix_for_timestep(t - 1),\n                                  get_transition_noise_for_timestep(t - 1))\n    observation_mean = _propagate_mean(latent_mean,\n                                       get_observation_matrix_for_timestep(t),\n                                       get_observation_noise_for_timestep(t))\n    return (latent_mean, observation_mean)\n\n  return mean_step", "code_tokens": ["def", "build_kalman_mean_step", "(", "get_transition_matrix_for_timestep", ",", "get_transition_noise_for_timestep", ",", "get_observation_matrix_for_timestep", ",", "get_observation_noise_for_timestep", ")", ":", "def", "mean_step", "(", "previous_means", ",", "t", ")", ":", "\"\"\"Single step of prior mean recursion.\"\"\"", "previous_latent_mean", ",", "_", "=", "previous_means", "latent_mean", "=", "_propagate_mean", "(", "previous_latent_mean", ",", "get_transition_matrix_for_timestep", "(", "t", "-", "1", ")", ",", "get_transition_noise_for_timestep", "(", "t", "-", "1", ")", ")", "observation_mean", "=", "_propagate_mean", "(", "latent_mean", ",", "get_observation_matrix_for_timestep", "(", "t", ")", ",", "get_observation_noise_for_timestep", "(", "t", ")", ")", "return", "(", "latent_mean", ",", "observation_mean", ")", "return", "mean_step"], "docstring": "Build a callable that performs one step of Kalman mean recursion.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n    get_transition_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[latent_size]`.\n    get_observation_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[observation_size, observation_size]`.\n    get_observation_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[observation_size]`.\n\n  Returns:\n    kalman_mean_step: a callable that computes latent state and\n      observation means at time `t`, given latent mean at time `t-1`.", "docstring_tokens": ["Build", "a", "callable", "that", "performs", "one", "step", "of", "Kalman", "mean", "recursion", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L1535-L1574", "partition": "test", "index": 945, "time": "2018-06-13 17:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "build_kalman_cov_step", "original_string": "def build_kalman_cov_step(get_transition_matrix_for_timestep,\n                          get_transition_noise_for_timestep,\n                          get_observation_matrix_for_timestep,\n                          get_observation_noise_for_timestep):\n  \"\"\"Build a callable for one step of Kalman covariance recursion.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n    get_transition_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[latent_size]`.\n    get_observation_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[observation_size, observation_size]`.\n    get_observation_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[observation_size]`.\n\n  Returns:\n    cov_step: a callable that computes latent state and observation\n      covariance at time `t`, given latent covariance at time `t-1`.\n  \"\"\"\n\n  def cov_step(previous_covs, t):\n    \"\"\"Single step of prior covariance recursion.\"\"\"\n    previous_latent_cov, _ = previous_covs\n\n    latent_cov = _propagate_cov(\n        previous_latent_cov,\n        get_transition_matrix_for_timestep(t - 1),\n        get_transition_noise_for_timestep(t - 1))\n    observation_cov = _propagate_cov(\n        latent_cov,\n        get_observation_matrix_for_timestep(t),\n        get_observation_noise_for_timestep(t))\n\n    return (latent_cov, observation_cov)\n\n  return cov_step", "language": "python", "code": "def build_kalman_cov_step(get_transition_matrix_for_timestep,\n                          get_transition_noise_for_timestep,\n                          get_observation_matrix_for_timestep,\n                          get_observation_noise_for_timestep):\n  \"\"\"Build a callable for one step of Kalman covariance recursion.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n    get_transition_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[latent_size]`.\n    get_observation_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[observation_size, observation_size]`.\n    get_observation_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[observation_size]`.\n\n  Returns:\n    cov_step: a callable that computes latent state and observation\n      covariance at time `t`, given latent covariance at time `t-1`.\n  \"\"\"\n\n  def cov_step(previous_covs, t):\n    \"\"\"Single step of prior covariance recursion.\"\"\"\n    previous_latent_cov, _ = previous_covs\n\n    latent_cov = _propagate_cov(\n        previous_latent_cov,\n        get_transition_matrix_for_timestep(t - 1),\n        get_transition_noise_for_timestep(t - 1))\n    observation_cov = _propagate_cov(\n        latent_cov,\n        get_observation_matrix_for_timestep(t),\n        get_observation_noise_for_timestep(t))\n\n    return (latent_cov, observation_cov)\n\n  return cov_step", "code_tokens": ["def", "build_kalman_cov_step", "(", "get_transition_matrix_for_timestep", ",", "get_transition_noise_for_timestep", ",", "get_observation_matrix_for_timestep", ",", "get_observation_noise_for_timestep", ")", ":", "def", "cov_step", "(", "previous_covs", ",", "t", ")", ":", "\"\"\"Single step of prior covariance recursion.\"\"\"", "previous_latent_cov", ",", "_", "=", "previous_covs", "latent_cov", "=", "_propagate_cov", "(", "previous_latent_cov", ",", "get_transition_matrix_for_timestep", "(", "t", "-", "1", ")", ",", "get_transition_noise_for_timestep", "(", "t", "-", "1", ")", ")", "observation_cov", "=", "_propagate_cov", "(", "latent_cov", ",", "get_observation_matrix_for_timestep", "(", "t", ")", ",", "get_observation_noise_for_timestep", "(", "t", ")", ")", "return", "(", "latent_cov", ",", "observation_cov", ")", "return", "cov_step"], "docstring": "Build a callable for one step of Kalman covariance recursion.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n    get_transition_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[latent_size]`.\n    get_observation_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[observation_size, observation_size]`.\n    get_observation_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[observation_size]`.\n\n  Returns:\n    cov_step: a callable that computes latent state and observation\n      covariance at time `t`, given latent covariance at time `t-1`.", "docstring_tokens": ["Build", "a", "callable", "for", "one", "step", "of", "Kalman", "covariance", "recursion", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L1577-L1619", "partition": "test", "index": 946, "time": "2018-06-13 17:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "_propagate_mean", "original_string": "def _propagate_mean(mean, linop, dist):\n  \"\"\"Propagate a mean through linear Gaussian transformation.\"\"\"\n  return linop.matmul(mean) + dist.mean()[..., tf.newaxis]", "language": "python", "code": "def _propagate_mean(mean, linop, dist):\n  \"\"\"Propagate a mean through linear Gaussian transformation.\"\"\"\n  return linop.matmul(mean) + dist.mean()[..., tf.newaxis]", "code_tokens": ["def", "_propagate_mean", "(", "mean", ",", "linop", ",", "dist", ")", ":", "return", "linop", ".", "matmul", "(", "mean", ")", "+", "dist", ".", "mean", "(", ")", "[", "...", ",", "tf", ".", "newaxis", "]"], "docstring": "Propagate a mean through linear Gaussian transformation.", "docstring_tokens": ["Propagate", "a", "mean", "through", "linear", "Gaussian", "transformation", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L1724-L1726", "partition": "test", "index": 948, "time": "2018-06-13 17:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "LinearGaussianStateSpaceModel._joint_covariances", "original_string": "def _joint_covariances(self):\n    \"\"\"Compute prior covariances for all variables via dynamic programming.\n\n    Returns:\n      latent_covs: Prior covariance matrices of latent states `z_t`, as\n        a `Tensor` of shape `batch_shape + [num_timesteps,\n        latent_size, latent_size]`\n      observation_covs: Prior covariance matrices of observations\n        `x_t`, as a `Tensor` of shape `batch_shape + [num_timesteps,\n        observation_size, observation_size]`\n    \"\"\"\n\n    with tf.name_scope(\"covariance_joint\"):\n\n      with tf.control_dependencies(self.runtime_assertions):\n        initial_latent_cov = _broadcast_to_shape(\n            self.initial_state_prior.covariance(),\n            tf.concat([self.batch_shape_tensor(),\n                       [self.latent_size, self.latent_size]], axis=0))\n\n      initial_observation_cov = _propagate_cov(\n          initial_latent_cov,\n          self.get_observation_matrix_for_timestep(self.initial_step),\n          self.get_observation_noise_for_timestep(self.initial_step))\n\n      cov_step = build_kalman_cov_step(\n          self.get_transition_matrix_for_timestep,\n          self.get_transition_noise_for_timestep,\n          self.get_observation_matrix_for_timestep,\n          self.get_observation_noise_for_timestep)\n\n      # Scan over all timesteps following the initial step.\n      (latent_covs, observation_covs) = tf.scan(\n          cov_step,\n          elems=tf.range(self.initial_step+1, self.final_step),\n          initializer=(initial_latent_cov, initial_observation_cov))\n\n      # Squish the initial step back on top of the other (scanned) timesteps\n      latent_covs = tf.concat([initial_latent_cov[tf.newaxis, ...],\n                               latent_covs], axis=0)\n      observation_covs = tf.concat([initial_observation_cov[tf.newaxis, ...],\n                                    observation_covs], axis=0)\n\n      # Put dimensions back in order. The samples we've computed have\n      # shape `[num_timesteps, batch_shape, size, size]`, where `size`\n      # is the dimension of the state or observation spaces\n      # respectively, but we want to return values with shape\n      # `[batch_shape, num_timesteps, size, size]`.\n      latent_covs = distribution_util.move_dimension(latent_covs, 0, -3)\n      observation_covs = distribution_util.move_dimension(\n          observation_covs, 0, -3)\n      return latent_covs, observation_covs", "language": "python", "code": "def _joint_covariances(self):\n    \"\"\"Compute prior covariances for all variables via dynamic programming.\n\n    Returns:\n      latent_covs: Prior covariance matrices of latent states `z_t`, as\n        a `Tensor` of shape `batch_shape + [num_timesteps,\n        latent_size, latent_size]`\n      observation_covs: Prior covariance matrices of observations\n        `x_t`, as a `Tensor` of shape `batch_shape + [num_timesteps,\n        observation_size, observation_size]`\n    \"\"\"\n\n    with tf.name_scope(\"covariance_joint\"):\n\n      with tf.control_dependencies(self.runtime_assertions):\n        initial_latent_cov = _broadcast_to_shape(\n            self.initial_state_prior.covariance(),\n            tf.concat([self.batch_shape_tensor(),\n                       [self.latent_size, self.latent_size]], axis=0))\n\n      initial_observation_cov = _propagate_cov(\n          initial_latent_cov,\n          self.get_observation_matrix_for_timestep(self.initial_step),\n          self.get_observation_noise_for_timestep(self.initial_step))\n\n      cov_step = build_kalman_cov_step(\n          self.get_transition_matrix_for_timestep,\n          self.get_transition_noise_for_timestep,\n          self.get_observation_matrix_for_timestep,\n          self.get_observation_noise_for_timestep)\n\n      # Scan over all timesteps following the initial step.\n      (latent_covs, observation_covs) = tf.scan(\n          cov_step,\n          elems=tf.range(self.initial_step+1, self.final_step),\n          initializer=(initial_latent_cov, initial_observation_cov))\n\n      # Squish the initial step back on top of the other (scanned) timesteps\n      latent_covs = tf.concat([initial_latent_cov[tf.newaxis, ...],\n                               latent_covs], axis=0)\n      observation_covs = tf.concat([initial_observation_cov[tf.newaxis, ...],\n                                    observation_covs], axis=0)\n\n      # Put dimensions back in order. The samples we've computed have\n      # shape `[num_timesteps, batch_shape, size, size]`, where `size`\n      # is the dimension of the state or observation spaces\n      # respectively, but we want to return values with shape\n      # `[batch_shape, num_timesteps, size, size]`.\n      latent_covs = distribution_util.move_dimension(latent_covs, 0, -3)\n      observation_covs = distribution_util.move_dimension(\n          observation_covs, 0, -3)\n      return latent_covs, observation_covs", "code_tokens": ["def", "_joint_covariances", "(", "self", ")", ":", "with", "tf", ".", "name_scope", "(", "\"covariance_joint\"", ")", ":", "with", "tf", ".", "control_dependencies", "(", "self", ".", "runtime_assertions", ")", ":", "initial_latent_cov", "=", "_broadcast_to_shape", "(", "self", ".", "initial_state_prior", ".", "covariance", "(", ")", ",", "tf", ".", "concat", "(", "[", "self", ".", "batch_shape_tensor", "(", ")", ",", "[", "self", ".", "latent_size", ",", "self", ".", "latent_size", "]", "]", ",", "axis", "=", "0", ")", ")", "initial_observation_cov", "=", "_propagate_cov", "(", "initial_latent_cov", ",", "self", ".", "get_observation_matrix_for_timestep", "(", "self", ".", "initial_step", ")", ",", "self", ".", "get_observation_noise_for_timestep", "(", "self", ".", "initial_step", ")", ")", "cov_step", "=", "build_kalman_cov_step", "(", "self", ".", "get_transition_matrix_for_timestep", ",", "self", ".", "get_transition_noise_for_timestep", ",", "self", ".", "get_observation_matrix_for_timestep", ",", "self", ".", "get_observation_noise_for_timestep", ")", "# Scan over all timesteps following the initial step.", "(", "latent_covs", ",", "observation_covs", ")", "=", "tf", ".", "scan", "(", "cov_step", ",", "elems", "=", "tf", ".", "range", "(", "self", ".", "initial_step", "+", "1", ",", "self", ".", "final_step", ")", ",", "initializer", "=", "(", "initial_latent_cov", ",", "initial_observation_cov", ")", ")", "# Squish the initial step back on top of the other (scanned) timesteps", "latent_covs", "=", "tf", ".", "concat", "(", "[", "initial_latent_cov", "[", "tf", ".", "newaxis", ",", "...", "]", ",", "latent_covs", "]", ",", "axis", "=", "0", ")", "observation_covs", "=", "tf", ".", "concat", "(", "[", "initial_observation_cov", "[", "tf", ".", "newaxis", ",", "...", "]", ",", "observation_covs", "]", ",", "axis", "=", "0", ")", "# Put dimensions back in order. The samples we've computed have", "# shape `[num_timesteps, batch_shape, size, size]`, where `size`", "# is the dimension of the state or observation spaces", "# respectively, but we want to return values with shape", "# `[batch_shape, num_timesteps, size, size]`.", "latent_covs", "=", "distribution_util", ".", "move_dimension", "(", "latent_covs", ",", "0", ",", "-", "3", ")", "observation_covs", "=", "distribution_util", ".", "move_dimension", "(", "observation_covs", ",", "0", ",", "-", "3", ")", "return", "latent_covs", ",", "observation_covs"], "docstring": "Compute prior covariances for all variables via dynamic programming.\n\n    Returns:\n      latent_covs: Prior covariance matrices of latent states `z_t`, as\n        a `Tensor` of shape `batch_shape + [num_timesteps,\n        latent_size, latent_size]`\n      observation_covs: Prior covariance matrices of observations\n        `x_t`, as a `Tensor` of shape `batch_shape + [num_timesteps,\n        observation_size, observation_size]`", "docstring_tokens": ["Compute", "prior", "covariances", "for", "all", "variables", "via", "dynamic", "programming", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L1040-L1091", "partition": "test", "index": 954, "time": "2018-06-13 17:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "build_kalman_sample_step", "original_string": "def build_kalman_sample_step(get_transition_matrix_for_timestep,\n                             get_transition_noise_for_timestep,\n                             get_observation_matrix_for_timestep,\n                             get_observation_noise_for_timestep,\n                             full_sample_and_batch_shape,\n                             stream,\n                             validate_args=False):\n  \"\"\"Build a callable for one step of Kalman sampling recursion.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n    get_transition_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[latent_size]`.\n    get_observation_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[observation_size, observation_size]`.\n    get_observation_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[observation_size]`.\n    full_sample_and_batch_shape: Desired sample and batch shape of the\n      returned samples, concatenated in a single `Tensor`.\n    stream: `tfd.SeedStream` instance used to generate a\n      sequence of random seeds.\n    validate_args: if True, perform error checking at runtime.\n\n  Returns:\n    sample_step: a callable that samples the latent state and\n      observation at time `t`, given latent state at time `t-1`.\n  \"\"\"\n\n  def sample_step(sampled_prev, t):\n    \"\"\"Sample values for a single timestep.\"\"\"\n    latent_prev, _ = sampled_prev\n\n    transition_matrix = get_transition_matrix_for_timestep(t - 1)\n    transition_noise = get_transition_noise_for_timestep(t - 1)\n\n    latent_pred = transition_matrix.matmul(latent_prev)\n    latent_sampled = latent_pred + transition_noise.sample(\n        sample_shape=_augment_sample_shape(\n            transition_noise,\n            full_sample_and_batch_shape,\n            validate_args),\n        seed=stream())[..., tf.newaxis]\n\n    observation_matrix = get_observation_matrix_for_timestep(t)\n    observation_noise = get_observation_noise_for_timestep(t)\n\n    observation_pred = observation_matrix.matmul(latent_sampled)\n    observation_sampled = observation_pred + observation_noise.sample(\n        sample_shape=_augment_sample_shape(\n            observation_noise,\n            full_sample_and_batch_shape,\n            validate_args),\n        seed=stream())[..., tf.newaxis]\n\n    return (latent_sampled, observation_sampled)\n\n  return sample_step", "language": "python", "code": "def build_kalman_sample_step(get_transition_matrix_for_timestep,\n                             get_transition_noise_for_timestep,\n                             get_observation_matrix_for_timestep,\n                             get_observation_noise_for_timestep,\n                             full_sample_and_batch_shape,\n                             stream,\n                             validate_args=False):\n  \"\"\"Build a callable for one step of Kalman sampling recursion.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n    get_transition_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[latent_size]`.\n    get_observation_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[observation_size, observation_size]`.\n    get_observation_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[observation_size]`.\n    full_sample_and_batch_shape: Desired sample and batch shape of the\n      returned samples, concatenated in a single `Tensor`.\n    stream: `tfd.SeedStream` instance used to generate a\n      sequence of random seeds.\n    validate_args: if True, perform error checking at runtime.\n\n  Returns:\n    sample_step: a callable that samples the latent state and\n      observation at time `t`, given latent state at time `t-1`.\n  \"\"\"\n\n  def sample_step(sampled_prev, t):\n    \"\"\"Sample values for a single timestep.\"\"\"\n    latent_prev, _ = sampled_prev\n\n    transition_matrix = get_transition_matrix_for_timestep(t - 1)\n    transition_noise = get_transition_noise_for_timestep(t - 1)\n\n    latent_pred = transition_matrix.matmul(latent_prev)\n    latent_sampled = latent_pred + transition_noise.sample(\n        sample_shape=_augment_sample_shape(\n            transition_noise,\n            full_sample_and_batch_shape,\n            validate_args),\n        seed=stream())[..., tf.newaxis]\n\n    observation_matrix = get_observation_matrix_for_timestep(t)\n    observation_noise = get_observation_noise_for_timestep(t)\n\n    observation_pred = observation_matrix.matmul(latent_sampled)\n    observation_sampled = observation_pred + observation_noise.sample(\n        sample_shape=_augment_sample_shape(\n            observation_noise,\n            full_sample_and_batch_shape,\n            validate_args),\n        seed=stream())[..., tf.newaxis]\n\n    return (latent_sampled, observation_sampled)\n\n  return sample_step", "code_tokens": ["def", "build_kalman_sample_step", "(", "get_transition_matrix_for_timestep", ",", "get_transition_noise_for_timestep", ",", "get_observation_matrix_for_timestep", ",", "get_observation_noise_for_timestep", ",", "full_sample_and_batch_shape", ",", "stream", ",", "validate_args", "=", "False", ")", ":", "def", "sample_step", "(", "sampled_prev", ",", "t", ")", ":", "\"\"\"Sample values for a single timestep.\"\"\"", "latent_prev", ",", "_", "=", "sampled_prev", "transition_matrix", "=", "get_transition_matrix_for_timestep", "(", "t", "-", "1", ")", "transition_noise", "=", "get_transition_noise_for_timestep", "(", "t", "-", "1", ")", "latent_pred", "=", "transition_matrix", ".", "matmul", "(", "latent_prev", ")", "latent_sampled", "=", "latent_pred", "+", "transition_noise", ".", "sample", "(", "sample_shape", "=", "_augment_sample_shape", "(", "transition_noise", ",", "full_sample_and_batch_shape", ",", "validate_args", ")", ",", "seed", "=", "stream", "(", ")", ")", "[", "...", ",", "tf", ".", "newaxis", "]", "observation_matrix", "=", "get_observation_matrix_for_timestep", "(", "t", ")", "observation_noise", "=", "get_observation_noise_for_timestep", "(", "t", ")", "observation_pred", "=", "observation_matrix", ".", "matmul", "(", "latent_sampled", ")", "observation_sampled", "=", "observation_pred", "+", "observation_noise", ".", "sample", "(", "sample_shape", "=", "_augment_sample_shape", "(", "observation_noise", ",", "full_sample_and_batch_shape", ",", "validate_args", ")", ",", "seed", "=", "stream", "(", ")", ")", "[", "...", ",", "tf", ".", "newaxis", "]", "return", "(", "latent_sampled", ",", "observation_sampled", ")", "return", "sample_step"], "docstring": "Build a callable for one step of Kalman sampling recursion.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n    get_transition_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[latent_size]`.\n    get_observation_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[observation_size, observation_size]`.\n    get_observation_noise_for_timestep: callable taking a timestep as\n      an integer `Tensor` argument, and returning a\n      `MultivariateNormalLinearOperator` of event shape\n      `[observation_size]`.\n    full_sample_and_batch_shape: Desired sample and batch shape of the\n      returned samples, concatenated in a single `Tensor`.\n    stream: `tfd.SeedStream` instance used to generate a\n      sequence of random seeds.\n    validate_args: if True, perform error checking at runtime.\n\n  Returns:\n    sample_step: a callable that samples the latent state and\n      observation at time `t`, given latent state at time `t-1`.", "docstring_tokens": ["Build", "a", "callable", "for", "one", "step", "of", "Kalman", "sampling", "recursion", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L1622-L1685", "partition": "test", "index": 947, "time": "2018-06-13 17:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "linear_gaussian_update", "original_string": "def linear_gaussian_update(\n    prior_mean, prior_cov, observation_matrix, observation_noise, x_observed):\n  \"\"\"Conjugate update for a linear Gaussian model.\n\n  Given a normal prior on a latent variable `z`,\n    `p(z) = N(prior_mean, prior_cov) = N(u, P)`,\n  for which we observe a linear Gaussian transformation `x`,\n    `p(x|z) = N(H * z + c, R)`,\n  the posterior is also normal:\n    `p(z|x) = N(u*, P*)`.\n\n  We can write this update as\n     x_expected = H * u + c # pushforward prior mean\n     S = R + H * P * H'  # pushforward prior cov\n     K = P * H' * S^{-1} # optimal Kalman gain\n     u* = u + K * (x_observed - x_expected) # posterior mean\n     P* = (I - K * H) * P (I - K * H)' + K * R * K' # posterior cov\n  (see, e.g., https://en.wikipedia.org/wiki/Kalman_filter#Update)\n\n  Args:\n    prior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      potential batch shape `B = [b1, ..., b_n]`.\n    prior_cov: `Tensor` with event shape `[latent_size, latent_size]`\n      and batch shape `B` (matching `prior_mean`).\n    observation_matrix: `LinearOperator` with shape\n      `[observation_size, latent_size]` and batch shape broadcastable\n      to `B`.\n    observation_noise: potentially-batched\n      `MultivariateNormalLinearOperator` instance with event shape\n      `[observation_size]` and batch shape broadcastable to `B`.\n    x_observed: potentially batched `Tensor` with event shape\n      `[observation_size, 1]` and batch shape `B`.\n\n  Returns:\n    posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`.\n    posterior_cov: `Tensor` with event shape `[latent_size,\n      latent_size]` and batch shape `B`.\n    predictive_dist: the prior predictive distribution `p(x|z)`,\n      as a `Distribution` instance with event\n      shape `[observation_size]` and batch shape `B`. This will\n      typically be `tfd.MultivariateNormalTriL`, but when\n      `observation_size=1` we return a `tfd.Independent(tfd.Normal)`\n      instance as an optimization.\n  \"\"\"\n\n  # If observations are scalar, we can avoid some matrix ops.\n  observation_size_is_static_and_scalar = (\n      tf.compat.dimension_value(observation_matrix.shape[-2]) == 1)\n\n  # Push the predicted mean for the latent state through the\n  # observation model\n  x_expected = _propagate_mean(prior_mean,\n                               observation_matrix,\n                               observation_noise)\n\n  # Push the predictive covariance of the latent state through the\n  # observation model:\n  #  S = R + H * P * H'.\n  # We use a temporary variable for H * P,\n  # reused below to compute Kalman gain.\n  tmp_obs_cov = observation_matrix.matmul(prior_cov)\n  predicted_obs_cov = (\n      observation_matrix.matmul(tmp_obs_cov, adjoint_arg=True)\n      + observation_noise.covariance())\n\n  # Compute optimal Kalman gain:\n  #  K = P * H' * S^{-1}\n  # Since both S and P are cov matrices, thus symmetric,\n  # we can take the transpose and reuse our previous\n  # computation:\n  #      = (S^{-1} * H * P)'\n  #      = (S^{-1} * tmp_obs_cov) '\n  #      = (S \\ tmp_obs_cov)'\n  if observation_size_is_static_and_scalar:\n    gain_transpose = tmp_obs_cov/predicted_obs_cov\n  else:\n    predicted_obs_cov_chol = tf.linalg.cholesky(predicted_obs_cov)\n    gain_transpose = tf.linalg.cholesky_solve(predicted_obs_cov_chol,\n                                              tmp_obs_cov)\n\n  # Compute the posterior mean, incorporating the observation.\n  #  u* = u + K (x_observed - x_expected)\n  posterior_mean = (prior_mean +\n                    tf.linalg.matmul(gain_transpose, x_observed - x_expected,\n                                     adjoint_a=True))\n\n  # For the posterior covariance, we could use the simple update\n  #  P* = P - K * H * P\n  # but this is prone to numerical issues because it subtracts a\n  # value from a PSD matrix.  We choose instead to use the more\n  # expensive Jordan form update\n  #  P* = (I - K H) * P * (I - K H)' + K R K'\n  # which always produces a PSD result. This uses\n  #  tmp_term = (I - K * H)'\n  # as an intermediate quantity.\n  tmp_term = -observation_matrix.matmul(gain_transpose, adjoint=True)  # -K * H\n  tmp_term = tf.linalg.set_diag(tmp_term, tf.linalg.diag_part(tmp_term) + 1)\n  posterior_cov = (\n      tf.linalg.matmul(\n          tmp_term, tf.linalg.matmul(prior_cov, tmp_term), adjoint_a=True)\n      + tf.linalg.matmul(gain_transpose,\n                         tf.linalg.matmul(\n                             observation_noise.covariance(), gain_transpose),\n                         adjoint_a=True))\n\n  if observation_size_is_static_and_scalar:\n    # A plain Normal would have event shape `[]`; wrapping with Independent\n    # ensures `event_shape=[1]` as required.\n    predictive_dist = independent.Independent(\n        normal.Normal(loc=x_expected[..., 0],\n                      scale=tf.sqrt(predicted_obs_cov[..., 0])),\n        reinterpreted_batch_ndims=1)\n\n    # Minor hack to define the covariance, so that `predictive_dist` can pass as\n    # an MVNTriL-like object.\n    predictive_dist.covariance = lambda: predicted_obs_cov\n  else:\n    predictive_dist = mvn_tril.MultivariateNormalTriL(\n        loc=x_expected[..., 0],\n        scale_tril=predicted_obs_cov_chol)\n\n  return posterior_mean, posterior_cov, predictive_dist", "language": "python", "code": "def linear_gaussian_update(\n    prior_mean, prior_cov, observation_matrix, observation_noise, x_observed):\n  \"\"\"Conjugate update for a linear Gaussian model.\n\n  Given a normal prior on a latent variable `z`,\n    `p(z) = N(prior_mean, prior_cov) = N(u, P)`,\n  for which we observe a linear Gaussian transformation `x`,\n    `p(x|z) = N(H * z + c, R)`,\n  the posterior is also normal:\n    `p(z|x) = N(u*, P*)`.\n\n  We can write this update as\n     x_expected = H * u + c # pushforward prior mean\n     S = R + H * P * H'  # pushforward prior cov\n     K = P * H' * S^{-1} # optimal Kalman gain\n     u* = u + K * (x_observed - x_expected) # posterior mean\n     P* = (I - K * H) * P (I - K * H)' + K * R * K' # posterior cov\n  (see, e.g., https://en.wikipedia.org/wiki/Kalman_filter#Update)\n\n  Args:\n    prior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      potential batch shape `B = [b1, ..., b_n]`.\n    prior_cov: `Tensor` with event shape `[latent_size, latent_size]`\n      and batch shape `B` (matching `prior_mean`).\n    observation_matrix: `LinearOperator` with shape\n      `[observation_size, latent_size]` and batch shape broadcastable\n      to `B`.\n    observation_noise: potentially-batched\n      `MultivariateNormalLinearOperator` instance with event shape\n      `[observation_size]` and batch shape broadcastable to `B`.\n    x_observed: potentially batched `Tensor` with event shape\n      `[observation_size, 1]` and batch shape `B`.\n\n  Returns:\n    posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`.\n    posterior_cov: `Tensor` with event shape `[latent_size,\n      latent_size]` and batch shape `B`.\n    predictive_dist: the prior predictive distribution `p(x|z)`,\n      as a `Distribution` instance with event\n      shape `[observation_size]` and batch shape `B`. This will\n      typically be `tfd.MultivariateNormalTriL`, but when\n      `observation_size=1` we return a `tfd.Independent(tfd.Normal)`\n      instance as an optimization.\n  \"\"\"\n\n  # If observations are scalar, we can avoid some matrix ops.\n  observation_size_is_static_and_scalar = (\n      tf.compat.dimension_value(observation_matrix.shape[-2]) == 1)\n\n  # Push the predicted mean for the latent state through the\n  # observation model\n  x_expected = _propagate_mean(prior_mean,\n                               observation_matrix,\n                               observation_noise)\n\n  # Push the predictive covariance of the latent state through the\n  # observation model:\n  #  S = R + H * P * H'.\n  # We use a temporary variable for H * P,\n  # reused below to compute Kalman gain.\n  tmp_obs_cov = observation_matrix.matmul(prior_cov)\n  predicted_obs_cov = (\n      observation_matrix.matmul(tmp_obs_cov, adjoint_arg=True)\n      + observation_noise.covariance())\n\n  # Compute optimal Kalman gain:\n  #  K = P * H' * S^{-1}\n  # Since both S and P are cov matrices, thus symmetric,\n  # we can take the transpose and reuse our previous\n  # computation:\n  #      = (S^{-1} * H * P)'\n  #      = (S^{-1} * tmp_obs_cov) '\n  #      = (S \\ tmp_obs_cov)'\n  if observation_size_is_static_and_scalar:\n    gain_transpose = tmp_obs_cov/predicted_obs_cov\n  else:\n    predicted_obs_cov_chol = tf.linalg.cholesky(predicted_obs_cov)\n    gain_transpose = tf.linalg.cholesky_solve(predicted_obs_cov_chol,\n                                              tmp_obs_cov)\n\n  # Compute the posterior mean, incorporating the observation.\n  #  u* = u + K (x_observed - x_expected)\n  posterior_mean = (prior_mean +\n                    tf.linalg.matmul(gain_transpose, x_observed - x_expected,\n                                     adjoint_a=True))\n\n  # For the posterior covariance, we could use the simple update\n  #  P* = P - K * H * P\n  # but this is prone to numerical issues because it subtracts a\n  # value from a PSD matrix.  We choose instead to use the more\n  # expensive Jordan form update\n  #  P* = (I - K H) * P * (I - K H)' + K R K'\n  # which always produces a PSD result. This uses\n  #  tmp_term = (I - K * H)'\n  # as an intermediate quantity.\n  tmp_term = -observation_matrix.matmul(gain_transpose, adjoint=True)  # -K * H\n  tmp_term = tf.linalg.set_diag(tmp_term, tf.linalg.diag_part(tmp_term) + 1)\n  posterior_cov = (\n      tf.linalg.matmul(\n          tmp_term, tf.linalg.matmul(prior_cov, tmp_term), adjoint_a=True)\n      + tf.linalg.matmul(gain_transpose,\n                         tf.linalg.matmul(\n                             observation_noise.covariance(), gain_transpose),\n                         adjoint_a=True))\n\n  if observation_size_is_static_and_scalar:\n    # A plain Normal would have event shape `[]`; wrapping with Independent\n    # ensures `event_shape=[1]` as required.\n    predictive_dist = independent.Independent(\n        normal.Normal(loc=x_expected[..., 0],\n                      scale=tf.sqrt(predicted_obs_cov[..., 0])),\n        reinterpreted_batch_ndims=1)\n\n    # Minor hack to define the covariance, so that `predictive_dist` can pass as\n    # an MVNTriL-like object.\n    predictive_dist.covariance = lambda: predicted_obs_cov\n  else:\n    predictive_dist = mvn_tril.MultivariateNormalTriL(\n        loc=x_expected[..., 0],\n        scale_tril=predicted_obs_cov_chol)\n\n  return posterior_mean, posterior_cov, predictive_dist", "code_tokens": ["def", "linear_gaussian_update", "(", "prior_mean", ",", "prior_cov", ",", "observation_matrix", ",", "observation_noise", ",", "x_observed", ")", ":", "# If observations are scalar, we can avoid some matrix ops.", "observation_size_is_static_and_scalar", "=", "(", "tf", ".", "compat", ".", "dimension_value", "(", "observation_matrix", ".", "shape", "[", "-", "2", "]", ")", "==", "1", ")", "# Push the predicted mean for the latent state through the", "# observation model", "x_expected", "=", "_propagate_mean", "(", "prior_mean", ",", "observation_matrix", ",", "observation_noise", ")", "# Push the predictive covariance of the latent state through the", "# observation model:", "#  S = R + H * P * H'.", "# We use a temporary variable for H * P,", "# reused below to compute Kalman gain.", "tmp_obs_cov", "=", "observation_matrix", ".", "matmul", "(", "prior_cov", ")", "predicted_obs_cov", "=", "(", "observation_matrix", ".", "matmul", "(", "tmp_obs_cov", ",", "adjoint_arg", "=", "True", ")", "+", "observation_noise", ".", "covariance", "(", ")", ")", "# Compute optimal Kalman gain:", "#  K = P * H' * S^{-1}", "# Since both S and P are cov matrices, thus symmetric,", "# we can take the transpose and reuse our previous", "# computation:", "#      = (S^{-1} * H * P)'", "#      = (S^{-1} * tmp_obs_cov) '", "#      = (S \\ tmp_obs_cov)'", "if", "observation_size_is_static_and_scalar", ":", "gain_transpose", "=", "tmp_obs_cov", "/", "predicted_obs_cov", "else", ":", "predicted_obs_cov_chol", "=", "tf", ".", "linalg", ".", "cholesky", "(", "predicted_obs_cov", ")", "gain_transpose", "=", "tf", ".", "linalg", ".", "cholesky_solve", "(", "predicted_obs_cov_chol", ",", "tmp_obs_cov", ")", "# Compute the posterior mean, incorporating the observation.", "#  u* = u + K (x_observed - x_expected)", "posterior_mean", "=", "(", "prior_mean", "+", "tf", ".", "linalg", ".", "matmul", "(", "gain_transpose", ",", "x_observed", "-", "x_expected", ",", "adjoint_a", "=", "True", ")", ")", "# For the posterior covariance, we could use the simple update", "#  P* = P - K * H * P", "# but this is prone to numerical issues because it subtracts a", "# value from a PSD matrix.  We choose instead to use the more", "# expensive Jordan form update", "#  P* = (I - K H) * P * (I - K H)' + K R K'", "# which always produces a PSD result. This uses", "#  tmp_term = (I - K * H)'", "# as an intermediate quantity.", "tmp_term", "=", "-", "observation_matrix", ".", "matmul", "(", "gain_transpose", ",", "adjoint", "=", "True", ")", "# -K * H", "tmp_term", "=", "tf", ".", "linalg", ".", "set_diag", "(", "tmp_term", ",", "tf", ".", "linalg", ".", "diag_part", "(", "tmp_term", ")", "+", "1", ")", "posterior_cov", "=", "(", "tf", ".", "linalg", ".", "matmul", "(", "tmp_term", ",", "tf", ".", "linalg", ".", "matmul", "(", "prior_cov", ",", "tmp_term", ")", ",", "adjoint_a", "=", "True", ")", "+", "tf", ".", "linalg", ".", "matmul", "(", "gain_transpose", ",", "tf", ".", "linalg", ".", "matmul", "(", "observation_noise", ".", "covariance", "(", ")", ",", "gain_transpose", ")", ",", "adjoint_a", "=", "True", ")", ")", "if", "observation_size_is_static_and_scalar", ":", "# A plain Normal would have event shape `[]`; wrapping with Independent", "# ensures `event_shape=[1]` as required.", "predictive_dist", "=", "independent", ".", "Independent", "(", "normal", ".", "Normal", "(", "loc", "=", "x_expected", "[", "...", ",", "0", "]", ",", "scale", "=", "tf", ".", "sqrt", "(", "predicted_obs_cov", "[", "...", ",", "0", "]", ")", ")", ",", "reinterpreted_batch_ndims", "=", "1", ")", "# Minor hack to define the covariance, so that `predictive_dist` can pass as", "# an MVNTriL-like object.", "predictive_dist", ".", "covariance", "=", "lambda", ":", "predicted_obs_cov", "else", ":", "predictive_dist", "=", "mvn_tril", ".", "MultivariateNormalTriL", "(", "loc", "=", "x_expected", "[", "...", ",", "0", "]", ",", "scale_tril", "=", "predicted_obs_cov_chol", ")", "return", "posterior_mean", ",", "posterior_cov", ",", "predictive_dist"], "docstring": "Conjugate update for a linear Gaussian model.\n\n  Given a normal prior on a latent variable `z`,\n    `p(z) = N(prior_mean, prior_cov) = N(u, P)`,\n  for which we observe a linear Gaussian transformation `x`,\n    `p(x|z) = N(H * z + c, R)`,\n  the posterior is also normal:\n    `p(z|x) = N(u*, P*)`.\n\n  We can write this update as\n     x_expected = H * u + c # pushforward prior mean\n     S = R + H * P * H'  # pushforward prior cov\n     K = P * H' * S^{-1} # optimal Kalman gain\n     u* = u + K * (x_observed - x_expected) # posterior mean\n     P* = (I - K * H) * P (I - K * H)' + K * R * K' # posterior cov\n  (see, e.g., https://en.wikipedia.org/wiki/Kalman_filter#Update)\n\n  Args:\n    prior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      potential batch shape `B = [b1, ..., b_n]`.\n    prior_cov: `Tensor` with event shape `[latent_size, latent_size]`\n      and batch shape `B` (matching `prior_mean`).\n    observation_matrix: `LinearOperator` with shape\n      `[observation_size, latent_size]` and batch shape broadcastable\n      to `B`.\n    observation_noise: potentially-batched\n      `MultivariateNormalLinearOperator` instance with event shape\n      `[observation_size]` and batch shape broadcastable to `B`.\n    x_observed: potentially batched `Tensor` with event shape\n      `[observation_size, 1]` and batch shape `B`.\n\n  Returns:\n    posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`.\n    posterior_cov: `Tensor` with event shape `[latent_size,\n      latent_size]` and batch shape `B`.\n    predictive_dist: the prior predictive distribution `p(x|z)`,\n      as a `Distribution` instance with event\n      shape `[observation_size]` and batch shape `B`. This will\n      typically be `tfd.MultivariateNormalTriL`, but when\n      `observation_size=1` we return a `tfd.Independent(tfd.Normal)`\n      instance as an optimization.", "docstring_tokens": ["Conjugate", "update", "for", "a", "linear", "Gaussian", "model", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L1397-L1519", "partition": "test", "index": 943, "time": "2018-06-13 17:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "_propagate_cov", "original_string": "def _propagate_cov(cov, linop, dist):\n  \"\"\"Propagate covariance through linear Gaussian transformation.\"\"\"\n  # For linop A and input cov P, returns `A P A' + dist.cov()`\n  return linop.matmul(linop.matmul(cov), adjoint_arg=True) + dist.covariance()", "language": "python", "code": "def _propagate_cov(cov, linop, dist):\n  \"\"\"Propagate covariance through linear Gaussian transformation.\"\"\"\n  # For linop A and input cov P, returns `A P A' + dist.cov()`\n  return linop.matmul(linop.matmul(cov), adjoint_arg=True) + dist.covariance()", "code_tokens": ["def", "_propagate_cov", "(", "cov", ",", "linop", ",", "dist", ")", ":", "# For linop A and input cov P, returns `A P A' + dist.cov()`", "return", "linop", ".", "matmul", "(", "linop", ".", "matmul", "(", "cov", ")", ",", "adjoint_arg", "=", "True", ")", "+", "dist", ".", "covariance", "(", ")"], "docstring": "Propagate covariance through linear Gaussian transformation.", "docstring_tokens": ["Propagate", "covariance", "through", "linear", "Gaussian", "transformation", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L1729-L1732", "partition": "test", "index": 949, "time": "2018-06-13 17:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "LinearGaussianStateSpaceModel._joint_sample_n", "original_string": "def _joint_sample_n(self, n, seed=None):\n    \"\"\"Draw a joint sample from the prior over latents and observations.\"\"\"\n\n    with tf.name_scope(\"sample_n_joint\"):\n      stream = seed_stream.SeedStream(\n          seed, salt=\"LinearGaussianStateSpaceModel_sample_n_joint\")\n\n      sample_and_batch_shape = distribution_util.prefer_static_value(\n          tf.concat([[n], self.batch_shape_tensor()],\n                    axis=0))\n\n      # Sample the initial timestep from the prior.  Since we want\n      # this sample to have full batch shape (not just the batch shape\n      # of the self.initial_state_prior object which might in general be\n      # smaller), we augment the sample shape to include whatever\n      # extra batch dimensions are required.\n      with tf.control_dependencies(self.runtime_assertions):\n        initial_latent = self.initial_state_prior.sample(\n            sample_shape=_augment_sample_shape(\n                self.initial_state_prior,\n                sample_and_batch_shape,\n                self.validate_args),\n            seed=stream())\n\n        # Add a dummy dimension so that matmul() does matrix-vector\n        # multiplication.\n        initial_latent = initial_latent[..., tf.newaxis]\n\n      initial_observation_matrix = (\n          self.get_observation_matrix_for_timestep(self.initial_step))\n      initial_observation_noise = (\n          self.get_observation_noise_for_timestep(self.initial_step))\n\n      initial_observation_pred = initial_observation_matrix.matmul(\n          initial_latent)\n      initial_observation = (initial_observation_pred +\n                             initial_observation_noise.sample(\n                                 sample_shape=_augment_sample_shape(\n                                     initial_observation_noise,\n                                     sample_and_batch_shape,\n                                     self.validate_args),\n                                 seed=stream())[..., tf.newaxis])\n\n      sample_step = build_kalman_sample_step(\n          self.get_transition_matrix_for_timestep,\n          self.get_transition_noise_for_timestep,\n          self.get_observation_matrix_for_timestep,\n          self.get_observation_noise_for_timestep,\n          full_sample_and_batch_shape=sample_and_batch_shape,\n          stream=stream,\n          validate_args=self.validate_args)\n\n      # Scan over all timesteps to sample latents and observations.\n      (latents, observations) = tf.scan(\n          sample_step,\n          elems=tf.range(self.initial_step+1, self.final_step),\n          initializer=(initial_latent, initial_observation))\n\n      # Combine the initial sampled timestep with the remaining timesteps.\n      latents = tf.concat([initial_latent[tf.newaxis, ...],\n                           latents], axis=0)\n      observations = tf.concat([initial_observation[tf.newaxis, ...],\n                                observations], axis=0)\n\n      # Put dimensions back in order. The samples we've computed are\n      # ordered by timestep, with shape `[num_timesteps, num_samples,\n      # batch_shape, size, 1]` where `size` represents `latent_size`\n      # or `observation_size` respectively. But timesteps are really\n      # part of each probabilistic event, so we need to return a Tensor\n      # of shape `[num_samples, batch_shape, num_timesteps, size]`.\n      latents = tf.squeeze(latents, -1)\n      latents = distribution_util.move_dimension(latents, 0, -2)\n      observations = tf.squeeze(observations, -1)\n      observations = distribution_util.move_dimension(observations, 0, -2)\n\n    return latents, observations", "language": "python", "code": "def _joint_sample_n(self, n, seed=None):\n    \"\"\"Draw a joint sample from the prior over latents and observations.\"\"\"\n\n    with tf.name_scope(\"sample_n_joint\"):\n      stream = seed_stream.SeedStream(\n          seed, salt=\"LinearGaussianStateSpaceModel_sample_n_joint\")\n\n      sample_and_batch_shape = distribution_util.prefer_static_value(\n          tf.concat([[n], self.batch_shape_tensor()],\n                    axis=0))\n\n      # Sample the initial timestep from the prior.  Since we want\n      # this sample to have full batch shape (not just the batch shape\n      # of the self.initial_state_prior object which might in general be\n      # smaller), we augment the sample shape to include whatever\n      # extra batch dimensions are required.\n      with tf.control_dependencies(self.runtime_assertions):\n        initial_latent = self.initial_state_prior.sample(\n            sample_shape=_augment_sample_shape(\n                self.initial_state_prior,\n                sample_and_batch_shape,\n                self.validate_args),\n            seed=stream())\n\n        # Add a dummy dimension so that matmul() does matrix-vector\n        # multiplication.\n        initial_latent = initial_latent[..., tf.newaxis]\n\n      initial_observation_matrix = (\n          self.get_observation_matrix_for_timestep(self.initial_step))\n      initial_observation_noise = (\n          self.get_observation_noise_for_timestep(self.initial_step))\n\n      initial_observation_pred = initial_observation_matrix.matmul(\n          initial_latent)\n      initial_observation = (initial_observation_pred +\n                             initial_observation_noise.sample(\n                                 sample_shape=_augment_sample_shape(\n                                     initial_observation_noise,\n                                     sample_and_batch_shape,\n                                     self.validate_args),\n                                 seed=stream())[..., tf.newaxis])\n\n      sample_step = build_kalman_sample_step(\n          self.get_transition_matrix_for_timestep,\n          self.get_transition_noise_for_timestep,\n          self.get_observation_matrix_for_timestep,\n          self.get_observation_noise_for_timestep,\n          full_sample_and_batch_shape=sample_and_batch_shape,\n          stream=stream,\n          validate_args=self.validate_args)\n\n      # Scan over all timesteps to sample latents and observations.\n      (latents, observations) = tf.scan(\n          sample_step,\n          elems=tf.range(self.initial_step+1, self.final_step),\n          initializer=(initial_latent, initial_observation))\n\n      # Combine the initial sampled timestep with the remaining timesteps.\n      latents = tf.concat([initial_latent[tf.newaxis, ...],\n                           latents], axis=0)\n      observations = tf.concat([initial_observation[tf.newaxis, ...],\n                                observations], axis=0)\n\n      # Put dimensions back in order. The samples we've computed are\n      # ordered by timestep, with shape `[num_timesteps, num_samples,\n      # batch_shape, size, 1]` where `size` represents `latent_size`\n      # or `observation_size` respectively. But timesteps are really\n      # part of each probabilistic event, so we need to return a Tensor\n      # of shape `[num_samples, batch_shape, num_timesteps, size]`.\n      latents = tf.squeeze(latents, -1)\n      latents = distribution_util.move_dimension(latents, 0, -2)\n      observations = tf.squeeze(observations, -1)\n      observations = distribution_util.move_dimension(observations, 0, -2)\n\n    return latents, observations", "code_tokens": ["def", "_joint_sample_n", "(", "self", ",", "n", ",", "seed", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "\"sample_n_joint\"", ")", ":", "stream", "=", "seed_stream", ".", "SeedStream", "(", "seed", ",", "salt", "=", "\"LinearGaussianStateSpaceModel_sample_n_joint\"", ")", "sample_and_batch_shape", "=", "distribution_util", ".", "prefer_static_value", "(", "tf", ".", "concat", "(", "[", "[", "n", "]", ",", "self", ".", "batch_shape_tensor", "(", ")", "]", ",", "axis", "=", "0", ")", ")", "# Sample the initial timestep from the prior.  Since we want", "# this sample to have full batch shape (not just the batch shape", "# of the self.initial_state_prior object which might in general be", "# smaller), we augment the sample shape to include whatever", "# extra batch dimensions are required.", "with", "tf", ".", "control_dependencies", "(", "self", ".", "runtime_assertions", ")", ":", "initial_latent", "=", "self", ".", "initial_state_prior", ".", "sample", "(", "sample_shape", "=", "_augment_sample_shape", "(", "self", ".", "initial_state_prior", ",", "sample_and_batch_shape", ",", "self", ".", "validate_args", ")", ",", "seed", "=", "stream", "(", ")", ")", "# Add a dummy dimension so that matmul() does matrix-vector", "# multiplication.", "initial_latent", "=", "initial_latent", "[", "...", ",", "tf", ".", "newaxis", "]", "initial_observation_matrix", "=", "(", "self", ".", "get_observation_matrix_for_timestep", "(", "self", ".", "initial_step", ")", ")", "initial_observation_noise", "=", "(", "self", ".", "get_observation_noise_for_timestep", "(", "self", ".", "initial_step", ")", ")", "initial_observation_pred", "=", "initial_observation_matrix", ".", "matmul", "(", "initial_latent", ")", "initial_observation", "=", "(", "initial_observation_pred", "+", "initial_observation_noise", ".", "sample", "(", "sample_shape", "=", "_augment_sample_shape", "(", "initial_observation_noise", ",", "sample_and_batch_shape", ",", "self", ".", "validate_args", ")", ",", "seed", "=", "stream", "(", ")", ")", "[", "...", ",", "tf", ".", "newaxis", "]", ")", "sample_step", "=", "build_kalman_sample_step", "(", "self", ".", "get_transition_matrix_for_timestep", ",", "self", ".", "get_transition_noise_for_timestep", ",", "self", ".", "get_observation_matrix_for_timestep", ",", "self", ".", "get_observation_noise_for_timestep", ",", "full_sample_and_batch_shape", "=", "sample_and_batch_shape", ",", "stream", "=", "stream", ",", "validate_args", "=", "self", ".", "validate_args", ")", "# Scan over all timesteps to sample latents and observations.", "(", "latents", ",", "observations", ")", "=", "tf", ".", "scan", "(", "sample_step", ",", "elems", "=", "tf", ".", "range", "(", "self", ".", "initial_step", "+", "1", ",", "self", ".", "final_step", ")", ",", "initializer", "=", "(", "initial_latent", ",", "initial_observation", ")", ")", "# Combine the initial sampled timestep with the remaining timesteps.", "latents", "=", "tf", ".", "concat", "(", "[", "initial_latent", "[", "tf", ".", "newaxis", ",", "...", "]", ",", "latents", "]", ",", "axis", "=", "0", ")", "observations", "=", "tf", ".", "concat", "(", "[", "initial_observation", "[", "tf", ".", "newaxis", ",", "...", "]", ",", "observations", "]", ",", "axis", "=", "0", ")", "# Put dimensions back in order. The samples we've computed are", "# ordered by timestep, with shape `[num_timesteps, num_samples,", "# batch_shape, size, 1]` where `size` represents `latent_size`", "# or `observation_size` respectively. But timesteps are really", "# part of each probabilistic event, so we need to return a Tensor", "# of shape `[num_samples, batch_shape, num_timesteps, size]`.", "latents", "=", "tf", ".", "squeeze", "(", "latents", ",", "-", "1", ")", "latents", "=", "distribution_util", ".", "move_dimension", "(", "latents", ",", "0", ",", "-", "2", ")", "observations", "=", "tf", ".", "squeeze", "(", "observations", ",", "-", "1", ")", "observations", "=", "distribution_util", ".", "move_dimension", "(", "observations", ",", "0", ",", "-", "2", ")", "return", "latents", ",", "observations"], "docstring": "Draw a joint sample from the prior over latents and observations.", "docstring_tokens": ["Draw", "a", "joint", "sample", "from", "the", "prior", "over", "latents", "and", "observations", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L590-L665", "partition": "test", "index": 951, "time": "2018-06-13 17:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/positive_semidefinite_kernels/internal/util.py", "func_name": "sum_rightmost_ndims_preserving_shape", "original_string": "def sum_rightmost_ndims_preserving_shape(x, ndims):\n  \"\"\"Return `Tensor` with right-most ndims summed.\n\n  Args:\n    x: the `Tensor` whose right-most `ndims` dimensions to sum\n    ndims: number of right-most dimensions to sum.\n\n  Returns:\n    A `Tensor` resulting from calling `reduce_sum` on the `ndims` right-most\n    dimensions. If the shape of `x` is statically known, the result will also\n    have statically known shape. Otherwise, the resulting shape will only be\n    known at runtime.\n  \"\"\"\n  x = tf.convert_to_tensor(value=x)\n  if x.shape.ndims is not None:\n    axes = tf.range(x.shape.ndims - ndims, x.shape.ndims)\n  else:\n    axes = tf.range(tf.rank(x) - ndims, tf.rank(x))\n  return tf.reduce_sum(input_tensor=x, axis=axes)", "language": "python", "code": "def sum_rightmost_ndims_preserving_shape(x, ndims):\n  \"\"\"Return `Tensor` with right-most ndims summed.\n\n  Args:\n    x: the `Tensor` whose right-most `ndims` dimensions to sum\n    ndims: number of right-most dimensions to sum.\n\n  Returns:\n    A `Tensor` resulting from calling `reduce_sum` on the `ndims` right-most\n    dimensions. If the shape of `x` is statically known, the result will also\n    have statically known shape. Otherwise, the resulting shape will only be\n    known at runtime.\n  \"\"\"\n  x = tf.convert_to_tensor(value=x)\n  if x.shape.ndims is not None:\n    axes = tf.range(x.shape.ndims - ndims, x.shape.ndims)\n  else:\n    axes = tf.range(tf.rank(x) - ndims, tf.rank(x))\n  return tf.reduce_sum(input_tensor=x, axis=axes)", "code_tokens": ["def", "sum_rightmost_ndims_preserving_shape", "(", "x", ",", "ndims", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ")", "if", "x", ".", "shape", ".", "ndims", "is", "not", "None", ":", "axes", "=", "tf", ".", "range", "(", "x", ".", "shape", ".", "ndims", "-", "ndims", ",", "x", ".", "shape", ".", "ndims", ")", "else", ":", "axes", "=", "tf", ".", "range", "(", "tf", ".", "rank", "(", "x", ")", "-", "ndims", ",", "tf", ".", "rank", "(", "x", ")", ")", "return", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "x", ",", "axis", "=", "axes", ")"], "docstring": "Return `Tensor` with right-most ndims summed.\n\n  Args:\n    x: the `Tensor` whose right-most `ndims` dimensions to sum\n    ndims: number of right-most dimensions to sum.\n\n  Returns:\n    A `Tensor` resulting from calling `reduce_sum` on the `ndims` right-most\n    dimensions. If the shape of `x` is statically known, the result will also\n    have statically known shape. Otherwise, the resulting shape will only be\n    known at runtime.", "docstring_tokens": ["Return", "Tensor", "with", "right", "-", "most", "ndims", "summed", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/positive_semidefinite_kernels/internal/util.py#L68-L86", "partition": "test", "index": 1069, "time": "2018-06-14 17:05:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/positive_semidefinite_kernels/positive_semidefinite_kernel.py", "func_name": "_flatten_summand_list", "original_string": "def _flatten_summand_list(kernels):\n  \"\"\"Flatten a list of kernels which may contain _SumKernel instances.\n\n  Args:\n    kernels: Python list of `PositiveSemidefiniteKernel` instances\n\n  Returns:\n    Python list containing the elements of kernels, with any _SumKernel\n    instances replaced by their `kernels` property contents.\n  \"\"\"\n  flattened = []\n  for k in kernels:\n    if isinstance(k, _SumKernel):\n      flattened += k.kernels\n    else:\n      flattened.append(k)\n  return flattened", "language": "python", "code": "def _flatten_summand_list(kernels):\n  \"\"\"Flatten a list of kernels which may contain _SumKernel instances.\n\n  Args:\n    kernels: Python list of `PositiveSemidefiniteKernel` instances\n\n  Returns:\n    Python list containing the elements of kernels, with any _SumKernel\n    instances replaced by their `kernels` property contents.\n  \"\"\"\n  flattened = []\n  for k in kernels:\n    if isinstance(k, _SumKernel):\n      flattened += k.kernels\n    else:\n      flattened.append(k)\n  return flattened", "code_tokens": ["def", "_flatten_summand_list", "(", "kernels", ")", ":", "flattened", "=", "[", "]", "for", "k", "in", "kernels", ":", "if", "isinstance", "(", "k", ",", "_SumKernel", ")", ":", "flattened", "+=", "k", ".", "kernels", "else", ":", "flattened", ".", "append", "(", "k", ")", "return", "flattened"], "docstring": "Flatten a list of kernels which may contain _SumKernel instances.\n\n  Args:\n    kernels: Python list of `PositiveSemidefiniteKernel` instances\n\n  Returns:\n    Python list containing the elements of kernels, with any _SumKernel\n    instances replaced by their `kernels` property contents.", "docstring_tokens": ["Flatten", "a", "list", "of", "kernels", "which", "may", "contain", "_SumKernel", "instances", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/positive_semidefinite_kernels/positive_semidefinite_kernel.py#L608-L624", "partition": "test", "index": 1029, "time": "2018-06-14 17:39:24"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/positive_semidefinite_kernels/positive_semidefinite_kernel.py", "func_name": "_flatten_multiplicand_list", "original_string": "def _flatten_multiplicand_list(kernels):\n  \"\"\"Flatten a list of kernels which may contain _ProductKernel instances.\n\n  Args:\n    kernels: Python list of `PositiveSemidefiniteKernel` instances\n\n  Returns:\n    Python list containing the elements of kernels, with any _ProductKernel\n    instances replaced by their `kernels` property contents.\n  \"\"\"\n  flattened = []\n  for k in kernels:\n    if isinstance(k, _ProductKernel):\n      flattened += k.kernels\n    else:\n      flattened.append(k)\n  return flattened", "language": "python", "code": "def _flatten_multiplicand_list(kernels):\n  \"\"\"Flatten a list of kernels which may contain _ProductKernel instances.\n\n  Args:\n    kernels: Python list of `PositiveSemidefiniteKernel` instances\n\n  Returns:\n    Python list containing the elements of kernels, with any _ProductKernel\n    instances replaced by their `kernels` property contents.\n  \"\"\"\n  flattened = []\n  for k in kernels:\n    if isinstance(k, _ProductKernel):\n      flattened += k.kernels\n    else:\n      flattened.append(k)\n  return flattened", "code_tokens": ["def", "_flatten_multiplicand_list", "(", "kernels", ")", ":", "flattened", "=", "[", "]", "for", "k", "in", "kernels", ":", "if", "isinstance", "(", "k", ",", "_ProductKernel", ")", ":", "flattened", "+=", "k", ".", "kernels", "else", ":", "flattened", ".", "append", "(", "k", ")", "return", "flattened"], "docstring": "Flatten a list of kernels which may contain _ProductKernel instances.\n\n  Args:\n    kernels: Python list of `PositiveSemidefiniteKernel` instances\n\n  Returns:\n    Python list containing the elements of kernels, with any _ProductKernel\n    instances replaced by their `kernels` property contents.", "docstring_tokens": ["Flatten", "a", "list", "of", "kernels", "which", "may", "contain", "_ProductKernel", "instances", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/positive_semidefinite_kernels/positive_semidefinite_kernel.py#L627-L643", "partition": "test", "index": 1030, "time": "2018-06-14 17:39:24"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/vae.py", "func_name": "download", "original_string": "def download(directory, filename):\n  \"\"\"Downloads a file.\"\"\"\n  filepath = os.path.join(directory, filename)\n  if tf.io.gfile.exists(filepath):\n    return filepath\n  if not tf.io.gfile.exists(directory):\n    tf.io.gfile.makedirs(directory)\n  url = os.path.join(ROOT_PATH, filename)\n  print(\"Downloading %s to %s\" % (url, filepath))\n  urllib.request.urlretrieve(url, filepath)\n  return filepath", "language": "python", "code": "def download(directory, filename):\n  \"\"\"Downloads a file.\"\"\"\n  filepath = os.path.join(directory, filename)\n  if tf.io.gfile.exists(filepath):\n    return filepath\n  if not tf.io.gfile.exists(directory):\n    tf.io.gfile.makedirs(directory)\n  url = os.path.join(ROOT_PATH, filename)\n  print(\"Downloading %s to %s\" % (url, filepath))\n  urllib.request.urlretrieve(url, filepath)\n  return filepath", "code_tokens": ["def", "download", "(", "directory", ",", "filename", ")", ":", "filepath", "=", "os", ".", "path", ".", "join", "(", "directory", ",", "filename", ")", "if", "tf", ".", "io", ".", "gfile", ".", "exists", "(", "filepath", ")", ":", "return", "filepath", "if", "not", "tf", ".", "io", ".", "gfile", ".", "exists", "(", "directory", ")", ":", "tf", ".", "io", ".", "gfile", ".", "makedirs", "(", "directory", ")", "url", "=", "os", ".", "path", ".", "join", "(", "ROOT_PATH", ",", "filename", ")", "print", "(", "\"Downloading %s to %s\"", "%", "(", "url", ",", "filepath", ")", ")", "urllib", ".", "request", ".", "urlretrieve", "(", "url", ",", "filepath", ")", "return", "filepath"], "docstring": "Downloads a file.", "docstring_tokens": ["Downloads", "a", "file", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/vae.py#L435-L445", "partition": "test", "index": 819, "time": "2018-06-15 13:53:35"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/vae.py", "func_name": "pack_images", "original_string": "def pack_images(images, rows, cols):\n  \"\"\"Helper utility to make a field of images.\"\"\"\n  shape = tf.shape(input=images)\n  width = shape[-3]\n  height = shape[-2]\n  depth = shape[-1]\n  images = tf.reshape(images, (-1, width, height, depth))\n  batch = tf.shape(input=images)[0]\n  rows = tf.minimum(rows, batch)\n  cols = tf.minimum(batch // rows, cols)\n  images = images[:rows * cols]\n  images = tf.reshape(images, (rows, cols, width, height, depth))\n  images = tf.transpose(a=images, perm=[0, 2, 1, 3, 4])\n  images = tf.reshape(images, [1, rows * width, cols * height, depth])\n  return images", "language": "python", "code": "def pack_images(images, rows, cols):\n  \"\"\"Helper utility to make a field of images.\"\"\"\n  shape = tf.shape(input=images)\n  width = shape[-3]\n  height = shape[-2]\n  depth = shape[-1]\n  images = tf.reshape(images, (-1, width, height, depth))\n  batch = tf.shape(input=images)[0]\n  rows = tf.minimum(rows, batch)\n  cols = tf.minimum(batch // rows, cols)\n  images = images[:rows * cols]\n  images = tf.reshape(images, (rows, cols, width, height, depth))\n  images = tf.transpose(a=images, perm=[0, 2, 1, 3, 4])\n  images = tf.reshape(images, [1, rows * width, cols * height, depth])\n  return images", "code_tokens": ["def", "pack_images", "(", "images", ",", "rows", ",", "cols", ")", ":", "shape", "=", "tf", ".", "shape", "(", "input", "=", "images", ")", "width", "=", "shape", "[", "-", "3", "]", "height", "=", "shape", "[", "-", "2", "]", "depth", "=", "shape", "[", "-", "1", "]", "images", "=", "tf", ".", "reshape", "(", "images", ",", "(", "-", "1", ",", "width", ",", "height", ",", "depth", ")", ")", "batch", "=", "tf", ".", "shape", "(", "input", "=", "images", ")", "[", "0", "]", "rows", "=", "tf", ".", "minimum", "(", "rows", ",", "batch", ")", "cols", "=", "tf", ".", "minimum", "(", "batch", "//", "rows", ",", "cols", ")", "images", "=", "images", "[", ":", "rows", "*", "cols", "]", "images", "=", "tf", ".", "reshape", "(", "images", ",", "(", "rows", ",", "cols", ",", "width", ",", "height", ",", "depth", ")", ")", "images", "=", "tf", ".", "transpose", "(", "a", "=", "images", ",", "perm", "=", "[", "0", ",", "2", ",", "1", ",", "3", ",", "4", "]", ")", "images", "=", "tf", ".", "reshape", "(", "images", ",", "[", "1", ",", "rows", "*", "width", ",", "cols", "*", "height", ",", "depth", "]", ")", "return", "images"], "docstring": "Helper utility to make a field of images.", "docstring_tokens": ["Helper", "utility", "to", "make", "a", "field", "of", "images", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/vae.py#L303-L317", "partition": "test", "index": 818, "time": "2018-06-15 13:53:35"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/vq_vae.py", "func_name": "add_ema_control_dependencies", "original_string": "def add_ema_control_dependencies(vector_quantizer,\n                                 one_hot_assignments,\n                                 codes,\n                                 commitment_loss,\n                                 decay):\n  \"\"\"Add control dependencies to the commmitment loss to update the codebook.\n\n  Args:\n    vector_quantizer: An instance of the VectorQuantizer class.\n    one_hot_assignments: The one-hot vectors corresponding to the matched\n      codebook entry for each code in the batch.\n    codes: A `float`-like `Tensor` containing the latent vectors to be compared\n      to the codebook.\n    commitment_loss: The commitment loss from comparing the encoder outputs to\n      their neighboring codebook entries.\n    decay: Decay factor for exponential moving average.\n\n  Returns:\n    commitment_loss: Commitment loss with control dependencies.\n  \"\"\"\n  # Use an exponential moving average to update the codebook.\n  updated_ema_count = moving_averages.assign_moving_average(\n      vector_quantizer.ema_count,\n      tf.reduce_sum(input_tensor=one_hot_assignments, axis=[0, 1]),\n      decay,\n      zero_debias=False)\n  updated_ema_means = moving_averages.assign_moving_average(\n      vector_quantizer.ema_means,\n      tf.reduce_sum(\n          input_tensor=tf.expand_dims(codes, 2) *\n          tf.expand_dims(one_hot_assignments, 3),\n          axis=[0, 1]),\n      decay,\n      zero_debias=False)\n\n  # Add small value to avoid dividing by zero.\n  perturbed_ema_count = updated_ema_count + 1e-5\n  with tf.control_dependencies([commitment_loss]):\n    update_means = tf.compat.v1.assign(\n        vector_quantizer.codebook,\n        updated_ema_means / perturbed_ema_count[..., tf.newaxis])\n    with tf.control_dependencies([update_means]):\n      return tf.identity(commitment_loss)", "language": "python", "code": "def add_ema_control_dependencies(vector_quantizer,\n                                 one_hot_assignments,\n                                 codes,\n                                 commitment_loss,\n                                 decay):\n  \"\"\"Add control dependencies to the commmitment loss to update the codebook.\n\n  Args:\n    vector_quantizer: An instance of the VectorQuantizer class.\n    one_hot_assignments: The one-hot vectors corresponding to the matched\n      codebook entry for each code in the batch.\n    codes: A `float`-like `Tensor` containing the latent vectors to be compared\n      to the codebook.\n    commitment_loss: The commitment loss from comparing the encoder outputs to\n      their neighboring codebook entries.\n    decay: Decay factor for exponential moving average.\n\n  Returns:\n    commitment_loss: Commitment loss with control dependencies.\n  \"\"\"\n  # Use an exponential moving average to update the codebook.\n  updated_ema_count = moving_averages.assign_moving_average(\n      vector_quantizer.ema_count,\n      tf.reduce_sum(input_tensor=one_hot_assignments, axis=[0, 1]),\n      decay,\n      zero_debias=False)\n  updated_ema_means = moving_averages.assign_moving_average(\n      vector_quantizer.ema_means,\n      tf.reduce_sum(\n          input_tensor=tf.expand_dims(codes, 2) *\n          tf.expand_dims(one_hot_assignments, 3),\n          axis=[0, 1]),\n      decay,\n      zero_debias=False)\n\n  # Add small value to avoid dividing by zero.\n  perturbed_ema_count = updated_ema_count + 1e-5\n  with tf.control_dependencies([commitment_loss]):\n    update_means = tf.compat.v1.assign(\n        vector_quantizer.codebook,\n        updated_ema_means / perturbed_ema_count[..., tf.newaxis])\n    with tf.control_dependencies([update_means]):\n      return tf.identity(commitment_loss)", "code_tokens": ["def", "add_ema_control_dependencies", "(", "vector_quantizer", ",", "one_hot_assignments", ",", "codes", ",", "commitment_loss", ",", "decay", ")", ":", "# Use an exponential moving average to update the codebook.", "updated_ema_count", "=", "moving_averages", ".", "assign_moving_average", "(", "vector_quantizer", ".", "ema_count", ",", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "one_hot_assignments", ",", "axis", "=", "[", "0", ",", "1", "]", ")", ",", "decay", ",", "zero_debias", "=", "False", ")", "updated_ema_means", "=", "moving_averages", ".", "assign_moving_average", "(", "vector_quantizer", ".", "ema_means", ",", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "expand_dims", "(", "codes", ",", "2", ")", "*", "tf", ".", "expand_dims", "(", "one_hot_assignments", ",", "3", ")", ",", "axis", "=", "[", "0", ",", "1", "]", ")", ",", "decay", ",", "zero_debias", "=", "False", ")", "# Add small value to avoid dividing by zero.", "perturbed_ema_count", "=", "updated_ema_count", "+", "1e-5", "with", "tf", ".", "control_dependencies", "(", "[", "commitment_loss", "]", ")", ":", "update_means", "=", "tf", ".", "compat", ".", "v1", ".", "assign", "(", "vector_quantizer", ".", "codebook", ",", "updated_ema_means", "/", "perturbed_ema_count", "[", "...", ",", "tf", ".", "newaxis", "]", ")", "with", "tf", ".", "control_dependencies", "(", "[", "update_means", "]", ")", ":", "return", "tf", ".", "identity", "(", "commitment_loss", ")"], "docstring": "Add control dependencies to the commmitment loss to update the codebook.\n\n  Args:\n    vector_quantizer: An instance of the VectorQuantizer class.\n    one_hot_assignments: The one-hot vectors corresponding to the matched\n      codebook entry for each code in the batch.\n    codes: A `float`-like `Tensor` containing the latent vectors to be compared\n      to the codebook.\n    commitment_loss: The commitment loss from comparing the encoder outputs to\n      their neighboring codebook entries.\n    decay: Decay factor for exponential moving average.\n\n  Returns:\n    commitment_loss: Commitment loss with control dependencies.", "docstring_tokens": ["Add", "control", "dependencies", "to", "the", "commmitment", "loss", "to", "update", "the", "codebook", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/vq_vae.py#L259-L301", "partition": "test", "index": 845, "time": "2018-06-15 14:51:50"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/vq_vae.py", "func_name": "save_imgs", "original_string": "def save_imgs(x, fname):\n  \"\"\"Helper method to save a grid of images to a PNG file.\n\n  Args:\n    x: A numpy array of shape [n_images, height, width].\n    fname: The filename to write to (including extension).\n  \"\"\"\n  n = x.shape[0]\n  fig = figure.Figure(figsize=(n, 1), frameon=False)\n  canvas = backend_agg.FigureCanvasAgg(fig)\n  for i in range(n):\n    ax = fig.add_subplot(1, n, i+1)\n    ax.imshow(x[i].squeeze(),\n              interpolation=\"none\",\n              cmap=cm.get_cmap(\"binary\"))\n    ax.axis(\"off\")\n  canvas.print_figure(fname, format=\"png\")\n  print(\"saved %s\" % fname)", "language": "python", "code": "def save_imgs(x, fname):\n  \"\"\"Helper method to save a grid of images to a PNG file.\n\n  Args:\n    x: A numpy array of shape [n_images, height, width].\n    fname: The filename to write to (including extension).\n  \"\"\"\n  n = x.shape[0]\n  fig = figure.Figure(figsize=(n, 1), frameon=False)\n  canvas = backend_agg.FigureCanvasAgg(fig)\n  for i in range(n):\n    ax = fig.add_subplot(1, n, i+1)\n    ax.imshow(x[i].squeeze(),\n              interpolation=\"none\",\n              cmap=cm.get_cmap(\"binary\"))\n    ax.axis(\"off\")\n  canvas.print_figure(fname, format=\"png\")\n  print(\"saved %s\" % fname)", "code_tokens": ["def", "save_imgs", "(", "x", ",", "fname", ")", ":", "n", "=", "x", ".", "shape", "[", "0", "]", "fig", "=", "figure", ".", "Figure", "(", "figsize", "=", "(", "n", ",", "1", ")", ",", "frameon", "=", "False", ")", "canvas", "=", "backend_agg", ".", "FigureCanvasAgg", "(", "fig", ")", "for", "i", "in", "range", "(", "n", ")", ":", "ax", "=", "fig", ".", "add_subplot", "(", "1", ",", "n", ",", "i", "+", "1", ")", "ax", ".", "imshow", "(", "x", "[", "i", "]", ".", "squeeze", "(", ")", ",", "interpolation", "=", "\"none\"", ",", "cmap", "=", "cm", ".", "get_cmap", "(", "\"binary\"", ")", ")", "ax", ".", "axis", "(", "\"off\"", ")", "canvas", ".", "print_figure", "(", "fname", ",", "format", "=", "\"png\"", ")", "print", "(", "\"saved %s\"", "%", "fname", ")"], "docstring": "Helper method to save a grid of images to a PNG file.\n\n  Args:\n    x: A numpy array of shape [n_images, height, width].\n    fname: The filename to write to (including extension).", "docstring_tokens": ["Helper", "method", "to", "save", "a", "grid", "of", "images", "to", "a", "PNG", "file", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/vq_vae.py#L304-L321", "partition": "test", "index": 846, "time": "2018-06-15 14:51:50"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/vq_vae.py", "func_name": "visualize_training", "original_string": "def visualize_training(images_val,\n                       reconstructed_images_val,\n                       random_images_val,\n                       log_dir, prefix, viz_n=10):\n  \"\"\"Helper method to save images visualizing model reconstructions.\n\n  Args:\n    images_val: Numpy array containing a batch of input images.\n    reconstructed_images_val: Numpy array giving the expected output\n      (mean) of the decoder.\n    random_images_val: Optionally, a Numpy array giving the expected output\n      (mean) of decoding samples from the prior, or `None`.\n    log_dir: The directory to write images (Python `str`).\n    prefix: A specific label for the saved visualizations, which\n      determines their filenames (Python `str`).\n    viz_n: The number of images from each batch to visualize (Python `int`).\n  \"\"\"\n  save_imgs(images_val[:viz_n],\n            os.path.join(log_dir, \"{}_inputs.png\".format(prefix)))\n  save_imgs(reconstructed_images_val[:viz_n],\n            os.path.join(log_dir,\n                         \"{}_reconstructions.png\".format(prefix)))\n\n  if random_images_val is not None:\n    save_imgs(random_images_val[:viz_n],\n              os.path.join(log_dir,\n                           \"{}_prior_samples.png\".format(prefix)))", "language": "python", "code": "def visualize_training(images_val,\n                       reconstructed_images_val,\n                       random_images_val,\n                       log_dir, prefix, viz_n=10):\n  \"\"\"Helper method to save images visualizing model reconstructions.\n\n  Args:\n    images_val: Numpy array containing a batch of input images.\n    reconstructed_images_val: Numpy array giving the expected output\n      (mean) of the decoder.\n    random_images_val: Optionally, a Numpy array giving the expected output\n      (mean) of decoding samples from the prior, or `None`.\n    log_dir: The directory to write images (Python `str`).\n    prefix: A specific label for the saved visualizations, which\n      determines their filenames (Python `str`).\n    viz_n: The number of images from each batch to visualize (Python `int`).\n  \"\"\"\n  save_imgs(images_val[:viz_n],\n            os.path.join(log_dir, \"{}_inputs.png\".format(prefix)))\n  save_imgs(reconstructed_images_val[:viz_n],\n            os.path.join(log_dir,\n                         \"{}_reconstructions.png\".format(prefix)))\n\n  if random_images_val is not None:\n    save_imgs(random_images_val[:viz_n],\n              os.path.join(log_dir,\n                           \"{}_prior_samples.png\".format(prefix)))", "code_tokens": ["def", "visualize_training", "(", "images_val", ",", "reconstructed_images_val", ",", "random_images_val", ",", "log_dir", ",", "prefix", ",", "viz_n", "=", "10", ")", ":", "save_imgs", "(", "images_val", "[", ":", "viz_n", "]", ",", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"{}_inputs.png\"", ".", "format", "(", "prefix", ")", ")", ")", "save_imgs", "(", "reconstructed_images_val", "[", ":", "viz_n", "]", ",", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"{}_reconstructions.png\"", ".", "format", "(", "prefix", ")", ")", ")", "if", "random_images_val", "is", "not", "None", ":", "save_imgs", "(", "random_images_val", "[", ":", "viz_n", "]", ",", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"{}_prior_samples.png\"", ".", "format", "(", "prefix", ")", ")", ")"], "docstring": "Helper method to save images visualizing model reconstructions.\n\n  Args:\n    images_val: Numpy array containing a batch of input images.\n    reconstructed_images_val: Numpy array giving the expected output\n      (mean) of the decoder.\n    random_images_val: Optionally, a Numpy array giving the expected output\n      (mean) of decoding samples from the prior, or `None`.\n    log_dir: The directory to write images (Python `str`).\n    prefix: A specific label for the saved visualizations, which\n      determines their filenames (Python `str`).\n    viz_n: The number of images from each batch to visualize (Python `int`).", "docstring_tokens": ["Helper", "method", "to", "save", "images", "visualizing", "model", "reconstructions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/vq_vae.py#L324-L350", "partition": "test", "index": 847, "time": "2018-06-15 14:51:50"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs.py", "func_name": "_batch_transpose", "original_string": "def _batch_transpose(mat):\n  \"\"\"Transpose a possibly batched matrix.\n\n  Args:\n    mat: A `tf.Tensor` of shape `[..., n, m]`.\n\n  Returns:\n    A tensor of shape `[..., m, n]` with matching batch dimensions.\n  \"\"\"\n  n = distribution_util.prefer_static_rank(mat)\n  perm = tf.range(n)\n  perm = tf.concat([perm[:-2], [perm[-1], perm[-2]]], axis=0)\n  return tf.transpose(a=mat, perm=perm)", "language": "python", "code": "def _batch_transpose(mat):\n  \"\"\"Transpose a possibly batched matrix.\n\n  Args:\n    mat: A `tf.Tensor` of shape `[..., n, m]`.\n\n  Returns:\n    A tensor of shape `[..., m, n]` with matching batch dimensions.\n  \"\"\"\n  n = distribution_util.prefer_static_rank(mat)\n  perm = tf.range(n)\n  perm = tf.concat([perm[:-2], [perm[-1], perm[-2]]], axis=0)\n  return tf.transpose(a=mat, perm=perm)", "code_tokens": ["def", "_batch_transpose", "(", "mat", ")", ":", "n", "=", "distribution_util", ".", "prefer_static_rank", "(", "mat", ")", "perm", "=", "tf", ".", "range", "(", "n", ")", "perm", "=", "tf", ".", "concat", "(", "[", "perm", "[", ":", "-", "2", "]", ",", "[", "perm", "[", "-", "1", "]", ",", "perm", "[", "-", "2", "]", "]", "]", ",", "axis", "=", "0", ")", "return", "tf", ".", "transpose", "(", "a", "=", "mat", ",", "perm", "=", "perm", ")"], "docstring": "Transpose a possibly batched matrix.\n\n  Args:\n    mat: A `tf.Tensor` of shape `[..., n, m]`.\n\n  Returns:\n    A tensor of shape `[..., m, n]` with matching batch dimensions.", "docstring_tokens": ["Transpose", "a", "possibly", "batched", "matrix", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs.py#L494-L506", "partition": "test", "index": 1067, "time": "2018-06-16 05:15:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs.py", "func_name": "minimize", "original_string": "def minimize(value_and_gradients_function,\n             initial_position,\n             tolerance=1e-8,\n             x_tolerance=0,\n             f_relative_tolerance=0,\n             initial_inverse_hessian_estimate=None,\n             max_iterations=50,\n             parallel_iterations=1,\n             stopping_condition=None,\n             name=None):\n  \"\"\"Applies the BFGS algorithm to minimize a differentiable function.\n\n  Performs unconstrained minimization of a differentiable function using the\n  BFGS scheme. For details of the algorithm, see [Nocedal and Wright(2006)][1].\n\n  ### Usage:\n\n  The following example demonstrates the BFGS optimizer attempting to find the\n  minimum for a simple two dimensional quadratic objective function.\n\n  ```python\n    minimum = np.array([1.0, 1.0])  # The center of the quadratic bowl.\n    scales = np.array([2.0, 3.0])  # The scales along the two axes.\n\n    # The objective function and the gradient.\n    def quadratic(x):\n      value = tf.reduce_sum(scales * (x - minimum) ** 2)\n      return value, tf.gradients(value, x)[0]\n\n    start = tf.constant([0.6, 0.8])  # Starting point for the search.\n    optim_results = tfp.optimizer.bfgs_minimize(\n        quadratic, initial_position=start, tolerance=1e-8)\n\n    with tf.Session() as session:\n      results = session.run(optim_results)\n      # Check that the search converged\n      assert(results.converged)\n      # Check that the argmin is close to the actual value.\n      np.testing.assert_allclose(results.position, minimum)\n      # Print out the total number of function evaluations it took. Should be 6.\n      print (\"Function evaluations: %d\" % results.num_objective_evaluations)\n  ```\n\n  ### References:\n  [1]: Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series in\n    Operations Research. pp 136-140. 2006\n    http://pages.mtu.edu/~struther/Courses/OLD/Sp2013/5630/Jorge_Nocedal_Numerical_optimization_267490.pdf\n\n  Args:\n    value_and_gradients_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a tuple of `Tensor`s of real dtype containing\n      the value of the function and its gradient at that point. The function\n      to be minimized. The input should be of shape `[..., n]`, where `n` is\n      the size of the domain of input points, and all others are batching\n      dimensions. The first component of the return value should be a real\n      `Tensor` of matching shape `[...]`. The second component (the gradient)\n      should also be of shape `[..., n]` like the input value to the function.\n    initial_position: real `Tensor` of shape `[..., n]`. The starting point, or\n      points when using batching dimensions, of the search procedure. At these\n      points the function value and the gradient norm should be finite.\n    tolerance: Scalar `Tensor` of real dtype. Specifies the gradient tolerance\n      for the procedure. If the supremum norm of the gradient vector is below\n      this number, the algorithm is stopped.\n    x_tolerance: Scalar `Tensor` of real dtype. If the absolute change in the\n      position between one iteration and the next is smaller than this number,\n      the algorithm is stopped.\n    f_relative_tolerance: Scalar `Tensor` of real dtype. If the relative change\n      in the objective value between one iteration and the next is smaller\n      than this value, the algorithm is stopped.\n    initial_inverse_hessian_estimate: Optional `Tensor` of the same dtype\n      as the components of the output of the `value_and_gradients_function`.\n      If specified, the shape should broadcastable to shape `[..., n, n]`; e.g.\n      if a single `[n, n]` matrix is provided, it will be automatically\n      broadcasted to all batches. Alternatively, one can also specify a\n      different hessian estimate for each batch member.\n      For the correctness of the algorithm, it is required that this parameter\n      be symmetric and positive definite. Specifies the starting estimate for\n      the inverse of the Hessian at the initial point. If not specified,\n      the identity matrix is used as the starting estimate for the\n      inverse Hessian.\n    max_iterations: Scalar positive int32 `Tensor`. The maximum number of\n      iterations for BFGS updates.\n    parallel_iterations: Positive integer. The number of iterations allowed to\n      run in parallel.\n    stopping_condition: (Optional) A Python function that takes as input two\n      Boolean tensors of shape `[...]`, and returns a Boolean scalar tensor.\n      The input tensors are `converged` and `failed`, indicating the current\n      status of each respective batch member; the return value states whether\n      the algorithm should stop. The default is tfp.optimizer.converged_all\n      which only stops when all batch members have either converged or failed.\n      An alternative is tfp.optimizer.converged_any which stops as soon as one\n      batch member has converged, or when all have failed.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'minimize' is used.\n\n  Returns:\n    optimizer_results: A namedtuple containing the following items:\n      converged: boolean tensor of shape `[...]` indicating for each batch\n        member whether the minimum was found within tolerance.\n      failed:  boolean tensor of shape `[...]` indicating for each batch\n        member whether a line search step failed to find a suitable step size\n        satisfying Wolfe conditions. In the absence of any constraints on the\n        number of objective evaluations permitted, this value will\n        be the complement of `converged`. However, if there is\n        a constraint and the search stopped due to available\n        evaluations being exhausted, both `failed` and `converged`\n        will be simultaneously False.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A tensor of shape `[..., n]` containing the last argument value\n        found during the search from each starting point. If the search\n        converged, then this value is the argmin of the objective function.\n      objective_value: A tensor of shape `[...]` with the value of the\n        objective function at the `position`. If the search converged, then\n        this is the (local) minimum of the objective function.\n      objective_gradient: A tensor of shape `[..., n]` containing the gradient\n        of the objective function at the `position`. If the search converged\n        the max-norm of this tensor should be below the tolerance.\n      inverse_hessian_estimate: A tensor of shape `[..., n, n]` containing the\n        inverse of the estimated Hessian.\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'minimize',\n      [initial_position, tolerance, initial_inverse_hessian_estimate]):\n    initial_position = tf.convert_to_tensor(\n        value=initial_position, name='initial_position')\n    dtype = initial_position.dtype.base_dtype\n    tolerance = tf.convert_to_tensor(\n        value=tolerance, dtype=dtype, name='grad_tolerance')\n    f_relative_tolerance = tf.convert_to_tensor(\n        value=f_relative_tolerance, dtype=dtype, name='f_relative_tolerance')\n    x_tolerance = tf.convert_to_tensor(\n        value=x_tolerance, dtype=dtype, name='x_tolerance')\n    max_iterations = tf.convert_to_tensor(\n        value=max_iterations, name='max_iterations')\n\n    input_shape = distribution_util.prefer_static_shape(initial_position)\n    batch_shape, domain_size = input_shape[:-1], input_shape[-1]\n\n    if stopping_condition is None:\n      stopping_condition = bfgs_utils.converged_all\n\n    # Control inputs are an optional list of tensors to evaluate before\n    # the start of the search procedure. These can be used to assert the\n    # validity of inputs to the search procedure.\n    control_inputs = None\n\n    if initial_inverse_hessian_estimate is None:\n      # Create a default initial inverse Hessian.\n      initial_inv_hessian = tf.eye(domain_size,\n                                   batch_shape=batch_shape,\n                                   dtype=dtype,\n                                   name='initial_inv_hessian')\n    else:\n      # If an initial inverse Hessian is supplied, compute some control inputs\n      # to ensure that it is positive definite and symmetric.\n      initial_inv_hessian = tf.convert_to_tensor(\n          value=initial_inverse_hessian_estimate,\n          dtype=dtype,\n          name='initial_inv_hessian')\n      control_inputs = _inv_hessian_control_inputs(initial_inv_hessian)\n      hessian_shape = tf.concat([batch_shape, [domain_size, domain_size]], 0)\n      initial_inv_hessian = tf.broadcast_to(initial_inv_hessian, hessian_shape)\n\n    # The `state` here is a `BfgsOptimizerResults` tuple with values for the\n    # current state of the algorithm computation.\n    def _cond(state):\n      \"\"\"Continue if iterations remain and stopping condition is not met.\"\"\"\n      return ((state.num_iterations < max_iterations) &\n              tf.logical_not(stopping_condition(state.converged, state.failed)))\n\n    def _body(state):\n      \"\"\"Main optimization loop.\"\"\"\n      search_direction = _get_search_direction(state.inverse_hessian_estimate,\n                                               state.objective_gradient)\n      derivative_at_start_pt = tf.reduce_sum(\n          input_tensor=state.objective_gradient * search_direction, axis=-1)\n\n      # If the derivative at the start point is not negative, recompute the\n      # search direction with the initial inverse Hessian.\n      needs_reset = (~state.failed & ~state.converged &\n                     (derivative_at_start_pt >= 0))\n\n      search_direction_reset = _get_search_direction(\n          initial_inv_hessian, state.objective_gradient)\n\n      actual_serch_direction = tf.where(\n          needs_reset, search_direction_reset, search_direction)\n      actual_inv_hessian = tf.where(\n          needs_reset, initial_inv_hessian, state.inverse_hessian_estimate)\n\n      # Replace the hessian estimate in the state, in case it had to be reset.\n      current_state = bfgs_utils.update_fields(\n          state, inverse_hessian_estimate=actual_inv_hessian)\n\n      next_state = bfgs_utils.line_search_step(\n          current_state,\n          value_and_gradients_function, actual_serch_direction,\n          tolerance, f_relative_tolerance, x_tolerance, stopping_condition)\n\n      # Update the inverse Hessian if needed and continue.\n      return [_update_inv_hessian(current_state, next_state)]\n\n    kwargs = bfgs_utils.get_initial_state_args(\n        value_and_gradients_function,\n        initial_position,\n        tolerance,\n        control_inputs)\n    kwargs['inverse_hessian_estimate'] = initial_inv_hessian\n    initial_state = BfgsOptimizerResults(**kwargs)\n    return tf.while_loop(\n        cond=_cond,\n        body=_body,\n        loop_vars=[initial_state],\n        parallel_iterations=parallel_iterations)[0]", "language": "python", "code": "def minimize(value_and_gradients_function,\n             initial_position,\n             tolerance=1e-8,\n             x_tolerance=0,\n             f_relative_tolerance=0,\n             initial_inverse_hessian_estimate=None,\n             max_iterations=50,\n             parallel_iterations=1,\n             stopping_condition=None,\n             name=None):\n  \"\"\"Applies the BFGS algorithm to minimize a differentiable function.\n\n  Performs unconstrained minimization of a differentiable function using the\n  BFGS scheme. For details of the algorithm, see [Nocedal and Wright(2006)][1].\n\n  ### Usage:\n\n  The following example demonstrates the BFGS optimizer attempting to find the\n  minimum for a simple two dimensional quadratic objective function.\n\n  ```python\n    minimum = np.array([1.0, 1.0])  # The center of the quadratic bowl.\n    scales = np.array([2.0, 3.0])  # The scales along the two axes.\n\n    # The objective function and the gradient.\n    def quadratic(x):\n      value = tf.reduce_sum(scales * (x - minimum) ** 2)\n      return value, tf.gradients(value, x)[0]\n\n    start = tf.constant([0.6, 0.8])  # Starting point for the search.\n    optim_results = tfp.optimizer.bfgs_minimize(\n        quadratic, initial_position=start, tolerance=1e-8)\n\n    with tf.Session() as session:\n      results = session.run(optim_results)\n      # Check that the search converged\n      assert(results.converged)\n      # Check that the argmin is close to the actual value.\n      np.testing.assert_allclose(results.position, minimum)\n      # Print out the total number of function evaluations it took. Should be 6.\n      print (\"Function evaluations: %d\" % results.num_objective_evaluations)\n  ```\n\n  ### References:\n  [1]: Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series in\n    Operations Research. pp 136-140. 2006\n    http://pages.mtu.edu/~struther/Courses/OLD/Sp2013/5630/Jorge_Nocedal_Numerical_optimization_267490.pdf\n\n  Args:\n    value_and_gradients_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a tuple of `Tensor`s of real dtype containing\n      the value of the function and its gradient at that point. The function\n      to be minimized. The input should be of shape `[..., n]`, where `n` is\n      the size of the domain of input points, and all others are batching\n      dimensions. The first component of the return value should be a real\n      `Tensor` of matching shape `[...]`. The second component (the gradient)\n      should also be of shape `[..., n]` like the input value to the function.\n    initial_position: real `Tensor` of shape `[..., n]`. The starting point, or\n      points when using batching dimensions, of the search procedure. At these\n      points the function value and the gradient norm should be finite.\n    tolerance: Scalar `Tensor` of real dtype. Specifies the gradient tolerance\n      for the procedure. If the supremum norm of the gradient vector is below\n      this number, the algorithm is stopped.\n    x_tolerance: Scalar `Tensor` of real dtype. If the absolute change in the\n      position between one iteration and the next is smaller than this number,\n      the algorithm is stopped.\n    f_relative_tolerance: Scalar `Tensor` of real dtype. If the relative change\n      in the objective value between one iteration and the next is smaller\n      than this value, the algorithm is stopped.\n    initial_inverse_hessian_estimate: Optional `Tensor` of the same dtype\n      as the components of the output of the `value_and_gradients_function`.\n      If specified, the shape should broadcastable to shape `[..., n, n]`; e.g.\n      if a single `[n, n]` matrix is provided, it will be automatically\n      broadcasted to all batches. Alternatively, one can also specify a\n      different hessian estimate for each batch member.\n      For the correctness of the algorithm, it is required that this parameter\n      be symmetric and positive definite. Specifies the starting estimate for\n      the inverse of the Hessian at the initial point. If not specified,\n      the identity matrix is used as the starting estimate for the\n      inverse Hessian.\n    max_iterations: Scalar positive int32 `Tensor`. The maximum number of\n      iterations for BFGS updates.\n    parallel_iterations: Positive integer. The number of iterations allowed to\n      run in parallel.\n    stopping_condition: (Optional) A Python function that takes as input two\n      Boolean tensors of shape `[...]`, and returns a Boolean scalar tensor.\n      The input tensors are `converged` and `failed`, indicating the current\n      status of each respective batch member; the return value states whether\n      the algorithm should stop. The default is tfp.optimizer.converged_all\n      which only stops when all batch members have either converged or failed.\n      An alternative is tfp.optimizer.converged_any which stops as soon as one\n      batch member has converged, or when all have failed.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'minimize' is used.\n\n  Returns:\n    optimizer_results: A namedtuple containing the following items:\n      converged: boolean tensor of shape `[...]` indicating for each batch\n        member whether the minimum was found within tolerance.\n      failed:  boolean tensor of shape `[...]` indicating for each batch\n        member whether a line search step failed to find a suitable step size\n        satisfying Wolfe conditions. In the absence of any constraints on the\n        number of objective evaluations permitted, this value will\n        be the complement of `converged`. However, if there is\n        a constraint and the search stopped due to available\n        evaluations being exhausted, both `failed` and `converged`\n        will be simultaneously False.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A tensor of shape `[..., n]` containing the last argument value\n        found during the search from each starting point. If the search\n        converged, then this value is the argmin of the objective function.\n      objective_value: A tensor of shape `[...]` with the value of the\n        objective function at the `position`. If the search converged, then\n        this is the (local) minimum of the objective function.\n      objective_gradient: A tensor of shape `[..., n]` containing the gradient\n        of the objective function at the `position`. If the search converged\n        the max-norm of this tensor should be below the tolerance.\n      inverse_hessian_estimate: A tensor of shape `[..., n, n]` containing the\n        inverse of the estimated Hessian.\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'minimize',\n      [initial_position, tolerance, initial_inverse_hessian_estimate]):\n    initial_position = tf.convert_to_tensor(\n        value=initial_position, name='initial_position')\n    dtype = initial_position.dtype.base_dtype\n    tolerance = tf.convert_to_tensor(\n        value=tolerance, dtype=dtype, name='grad_tolerance')\n    f_relative_tolerance = tf.convert_to_tensor(\n        value=f_relative_tolerance, dtype=dtype, name='f_relative_tolerance')\n    x_tolerance = tf.convert_to_tensor(\n        value=x_tolerance, dtype=dtype, name='x_tolerance')\n    max_iterations = tf.convert_to_tensor(\n        value=max_iterations, name='max_iterations')\n\n    input_shape = distribution_util.prefer_static_shape(initial_position)\n    batch_shape, domain_size = input_shape[:-1], input_shape[-1]\n\n    if stopping_condition is None:\n      stopping_condition = bfgs_utils.converged_all\n\n    # Control inputs are an optional list of tensors to evaluate before\n    # the start of the search procedure. These can be used to assert the\n    # validity of inputs to the search procedure.\n    control_inputs = None\n\n    if initial_inverse_hessian_estimate is None:\n      # Create a default initial inverse Hessian.\n      initial_inv_hessian = tf.eye(domain_size,\n                                   batch_shape=batch_shape,\n                                   dtype=dtype,\n                                   name='initial_inv_hessian')\n    else:\n      # If an initial inverse Hessian is supplied, compute some control inputs\n      # to ensure that it is positive definite and symmetric.\n      initial_inv_hessian = tf.convert_to_tensor(\n          value=initial_inverse_hessian_estimate,\n          dtype=dtype,\n          name='initial_inv_hessian')\n      control_inputs = _inv_hessian_control_inputs(initial_inv_hessian)\n      hessian_shape = tf.concat([batch_shape, [domain_size, domain_size]], 0)\n      initial_inv_hessian = tf.broadcast_to(initial_inv_hessian, hessian_shape)\n\n    # The `state` here is a `BfgsOptimizerResults` tuple with values for the\n    # current state of the algorithm computation.\n    def _cond(state):\n      \"\"\"Continue if iterations remain and stopping condition is not met.\"\"\"\n      return ((state.num_iterations < max_iterations) &\n              tf.logical_not(stopping_condition(state.converged, state.failed)))\n\n    def _body(state):\n      \"\"\"Main optimization loop.\"\"\"\n      search_direction = _get_search_direction(state.inverse_hessian_estimate,\n                                               state.objective_gradient)\n      derivative_at_start_pt = tf.reduce_sum(\n          input_tensor=state.objective_gradient * search_direction, axis=-1)\n\n      # If the derivative at the start point is not negative, recompute the\n      # search direction with the initial inverse Hessian.\n      needs_reset = (~state.failed & ~state.converged &\n                     (derivative_at_start_pt >= 0))\n\n      search_direction_reset = _get_search_direction(\n          initial_inv_hessian, state.objective_gradient)\n\n      actual_serch_direction = tf.where(\n          needs_reset, search_direction_reset, search_direction)\n      actual_inv_hessian = tf.where(\n          needs_reset, initial_inv_hessian, state.inverse_hessian_estimate)\n\n      # Replace the hessian estimate in the state, in case it had to be reset.\n      current_state = bfgs_utils.update_fields(\n          state, inverse_hessian_estimate=actual_inv_hessian)\n\n      next_state = bfgs_utils.line_search_step(\n          current_state,\n          value_and_gradients_function, actual_serch_direction,\n          tolerance, f_relative_tolerance, x_tolerance, stopping_condition)\n\n      # Update the inverse Hessian if needed and continue.\n      return [_update_inv_hessian(current_state, next_state)]\n\n    kwargs = bfgs_utils.get_initial_state_args(\n        value_and_gradients_function,\n        initial_position,\n        tolerance,\n        control_inputs)\n    kwargs['inverse_hessian_estimate'] = initial_inv_hessian\n    initial_state = BfgsOptimizerResults(**kwargs)\n    return tf.while_loop(\n        cond=_cond,\n        body=_body,\n        loop_vars=[initial_state],\n        parallel_iterations=parallel_iterations)[0]", "code_tokens": ["def", "minimize", "(", "value_and_gradients_function", ",", "initial_position", ",", "tolerance", "=", "1e-8", ",", "x_tolerance", "=", "0", ",", "f_relative_tolerance", "=", "0", ",", "initial_inverse_hessian_estimate", "=", "None", ",", "max_iterations", "=", "50", ",", "parallel_iterations", "=", "1", ",", "stopping_condition", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'minimize'", ",", "[", "initial_position", ",", "tolerance", ",", "initial_inverse_hessian_estimate", "]", ")", ":", "initial_position", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "initial_position", ",", "name", "=", "'initial_position'", ")", "dtype", "=", "initial_position", ".", "dtype", ".", "base_dtype", "tolerance", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "tolerance", ",", "dtype", "=", "dtype", ",", "name", "=", "'grad_tolerance'", ")", "f_relative_tolerance", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "f_relative_tolerance", ",", "dtype", "=", "dtype", ",", "name", "=", "'f_relative_tolerance'", ")", "x_tolerance", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x_tolerance", ",", "dtype", "=", "dtype", ",", "name", "=", "'x_tolerance'", ")", "max_iterations", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "max_iterations", ",", "name", "=", "'max_iterations'", ")", "input_shape", "=", "distribution_util", ".", "prefer_static_shape", "(", "initial_position", ")", "batch_shape", ",", "domain_size", "=", "input_shape", "[", ":", "-", "1", "]", ",", "input_shape", "[", "-", "1", "]", "if", "stopping_condition", "is", "None", ":", "stopping_condition", "=", "bfgs_utils", ".", "converged_all", "# Control inputs are an optional list of tensors to evaluate before", "# the start of the search procedure. These can be used to assert the", "# validity of inputs to the search procedure.", "control_inputs", "=", "None", "if", "initial_inverse_hessian_estimate", "is", "None", ":", "# Create a default initial inverse Hessian.", "initial_inv_hessian", "=", "tf", ".", "eye", "(", "domain_size", ",", "batch_shape", "=", "batch_shape", ",", "dtype", "=", "dtype", ",", "name", "=", "'initial_inv_hessian'", ")", "else", ":", "# If an initial inverse Hessian is supplied, compute some control inputs", "# to ensure that it is positive definite and symmetric.", "initial_inv_hessian", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "initial_inverse_hessian_estimate", ",", "dtype", "=", "dtype", ",", "name", "=", "'initial_inv_hessian'", ")", "control_inputs", "=", "_inv_hessian_control_inputs", "(", "initial_inv_hessian", ")", "hessian_shape", "=", "tf", ".", "concat", "(", "[", "batch_shape", ",", "[", "domain_size", ",", "domain_size", "]", "]", ",", "0", ")", "initial_inv_hessian", "=", "tf", ".", "broadcast_to", "(", "initial_inv_hessian", ",", "hessian_shape", ")", "# The `state` here is a `BfgsOptimizerResults` tuple with values for the", "# current state of the algorithm computation.", "def", "_cond", "(", "state", ")", ":", "\"\"\"Continue if iterations remain and stopping condition is not met.\"\"\"", "return", "(", "(", "state", ".", "num_iterations", "<", "max_iterations", ")", "&", "tf", ".", "logical_not", "(", "stopping_condition", "(", "state", ".", "converged", ",", "state", ".", "failed", ")", ")", ")", "def", "_body", "(", "state", ")", ":", "\"\"\"Main optimization loop.\"\"\"", "search_direction", "=", "_get_search_direction", "(", "state", ".", "inverse_hessian_estimate", ",", "state", ".", "objective_gradient", ")", "derivative_at_start_pt", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "state", ".", "objective_gradient", "*", "search_direction", ",", "axis", "=", "-", "1", ")", "# If the derivative at the start point is not negative, recompute the", "# search direction with the initial inverse Hessian.", "needs_reset", "=", "(", "~", "state", ".", "failed", "&", "~", "state", ".", "converged", "&", "(", "derivative_at_start_pt", ">=", "0", ")", ")", "search_direction_reset", "=", "_get_search_direction", "(", "initial_inv_hessian", ",", "state", ".", "objective_gradient", ")", "actual_serch_direction", "=", "tf", ".", "where", "(", "needs_reset", ",", "search_direction_reset", ",", "search_direction", ")", "actual_inv_hessian", "=", "tf", ".", "where", "(", "needs_reset", ",", "initial_inv_hessian", ",", "state", ".", "inverse_hessian_estimate", ")", "# Replace the hessian estimate in the state, in case it had to be reset.", "current_state", "=", "bfgs_utils", ".", "update_fields", "(", "state", ",", "inverse_hessian_estimate", "=", "actual_inv_hessian", ")", "next_state", "=", "bfgs_utils", ".", "line_search_step", "(", "current_state", ",", "value_and_gradients_function", ",", "actual_serch_direction", ",", "tolerance", ",", "f_relative_tolerance", ",", "x_tolerance", ",", "stopping_condition", ")", "# Update the inverse Hessian if needed and continue.", "return", "[", "_update_inv_hessian", "(", "current_state", ",", "next_state", ")", "]", "kwargs", "=", "bfgs_utils", ".", "get_initial_state_args", "(", "value_and_gradients_function", ",", "initial_position", ",", "tolerance", ",", "control_inputs", ")", "kwargs", "[", "'inverse_hessian_estimate'", "]", "=", "initial_inv_hessian", "initial_state", "=", "BfgsOptimizerResults", "(", "*", "*", "kwargs", ")", "return", "tf", ".", "while_loop", "(", "cond", "=", "_cond", ",", "body", "=", "_body", ",", "loop_vars", "=", "[", "initial_state", "]", ",", "parallel_iterations", "=", "parallel_iterations", ")", "[", "0", "]"], "docstring": "Applies the BFGS algorithm to minimize a differentiable function.\n\n  Performs unconstrained minimization of a differentiable function using the\n  BFGS scheme. For details of the algorithm, see [Nocedal and Wright(2006)][1].\n\n  ### Usage:\n\n  The following example demonstrates the BFGS optimizer attempting to find the\n  minimum for a simple two dimensional quadratic objective function.\n\n  ```python\n    minimum = np.array([1.0, 1.0])  # The center of the quadratic bowl.\n    scales = np.array([2.0, 3.0])  # The scales along the two axes.\n\n    # The objective function and the gradient.\n    def quadratic(x):\n      value = tf.reduce_sum(scales * (x - minimum) ** 2)\n      return value, tf.gradients(value, x)[0]\n\n    start = tf.constant([0.6, 0.8])  # Starting point for the search.\n    optim_results = tfp.optimizer.bfgs_minimize(\n        quadratic, initial_position=start, tolerance=1e-8)\n\n    with tf.Session() as session:\n      results = session.run(optim_results)\n      # Check that the search converged\n      assert(results.converged)\n      # Check that the argmin is close to the actual value.\n      np.testing.assert_allclose(results.position, minimum)\n      # Print out the total number of function evaluations it took. Should be 6.\n      print (\"Function evaluations: %d\" % results.num_objective_evaluations)\n  ```\n\n  ### References:\n  [1]: Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series in\n    Operations Research. pp 136-140. 2006\n    http://pages.mtu.edu/~struther/Courses/OLD/Sp2013/5630/Jorge_Nocedal_Numerical_optimization_267490.pdf\n\n  Args:\n    value_and_gradients_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a tuple of `Tensor`s of real dtype containing\n      the value of the function and its gradient at that point. The function\n      to be minimized. The input should be of shape `[..., n]`, where `n` is\n      the size of the domain of input points, and all others are batching\n      dimensions. The first component of the return value should be a real\n      `Tensor` of matching shape `[...]`. The second component (the gradient)\n      should also be of shape `[..., n]` like the input value to the function.\n    initial_position: real `Tensor` of shape `[..., n]`. The starting point, or\n      points when using batching dimensions, of the search procedure. At these\n      points the function value and the gradient norm should be finite.\n    tolerance: Scalar `Tensor` of real dtype. Specifies the gradient tolerance\n      for the procedure. If the supremum norm of the gradient vector is below\n      this number, the algorithm is stopped.\n    x_tolerance: Scalar `Tensor` of real dtype. If the absolute change in the\n      position between one iteration and the next is smaller than this number,\n      the algorithm is stopped.\n    f_relative_tolerance: Scalar `Tensor` of real dtype. If the relative change\n      in the objective value between one iteration and the next is smaller\n      than this value, the algorithm is stopped.\n    initial_inverse_hessian_estimate: Optional `Tensor` of the same dtype\n      as the components of the output of the `value_and_gradients_function`.\n      If specified, the shape should broadcastable to shape `[..., n, n]`; e.g.\n      if a single `[n, n]` matrix is provided, it will be automatically\n      broadcasted to all batches. Alternatively, one can also specify a\n      different hessian estimate for each batch member.\n      For the correctness of the algorithm, it is required that this parameter\n      be symmetric and positive definite. Specifies the starting estimate for\n      the inverse of the Hessian at the initial point. If not specified,\n      the identity matrix is used as the starting estimate for the\n      inverse Hessian.\n    max_iterations: Scalar positive int32 `Tensor`. The maximum number of\n      iterations for BFGS updates.\n    parallel_iterations: Positive integer. The number of iterations allowed to\n      run in parallel.\n    stopping_condition: (Optional) A Python function that takes as input two\n      Boolean tensors of shape `[...]`, and returns a Boolean scalar tensor.\n      The input tensors are `converged` and `failed`, indicating the current\n      status of each respective batch member; the return value states whether\n      the algorithm should stop. The default is tfp.optimizer.converged_all\n      which only stops when all batch members have either converged or failed.\n      An alternative is tfp.optimizer.converged_any which stops as soon as one\n      batch member has converged, or when all have failed.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'minimize' is used.\n\n  Returns:\n    optimizer_results: A namedtuple containing the following items:\n      converged: boolean tensor of shape `[...]` indicating for each batch\n        member whether the minimum was found within tolerance.\n      failed:  boolean tensor of shape `[...]` indicating for each batch\n        member whether a line search step failed to find a suitable step size\n        satisfying Wolfe conditions. In the absence of any constraints on the\n        number of objective evaluations permitted, this value will\n        be the complement of `converged`. However, if there is\n        a constraint and the search stopped due to available\n        evaluations being exhausted, both `failed` and `converged`\n        will be simultaneously False.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A tensor of shape `[..., n]` containing the last argument value\n        found during the search from each starting point. If the search\n        converged, then this value is the argmin of the objective function.\n      objective_value: A tensor of shape `[...]` with the value of the\n        objective function at the `position`. If the search converged, then\n        this is the (local) minimum of the objective function.\n      objective_gradient: A tensor of shape `[..., n]` containing the gradient\n        of the objective function at the `position`. If the search converged\n        the max-norm of this tensor should be below the tolerance.\n      inverse_hessian_estimate: A tensor of shape `[..., n, n]` containing the\n        inverse of the estimated Hessian.", "docstring_tokens": ["Applies", "the", "BFGS", "algorithm", "to", "minimize", "a", "differentiable", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs.py#L72-L286", "partition": "test", "index": 1061, "time": "2018-06-16 05:15:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs.py", "func_name": "_bfgs_inv_hessian_update", "original_string": "def _bfgs_inv_hessian_update(grad_delta, position_delta, normalization_factor,\n                             inv_hessian_estimate):\n  \"\"\"Applies the BFGS update to the inverse Hessian estimate.\n\n  The BFGS update rule is (note A^T denotes the transpose of a vector/matrix A).\n\n  ```None\n    rho = 1/(grad_delta^T * position_delta)\n    U = (I - rho * position_delta * grad_delta^T)\n    H_1 =  U * H_0 * U^T + rho * position_delta * position_delta^T\n  ```\n\n  Here, `H_0` is the inverse Hessian estimate at the previous iteration and\n  `H_1` is the next estimate. Note that `*` should be interpreted as the\n  matrix multiplication (with the understanding that matrix multiplication for\n  scalars is usual multiplication and for matrix with vector is the action of\n  the matrix on the vector.).\n\n  The implementation below utilizes an expanded version of the above formula\n  to avoid the matrix multiplications that would be needed otherwise. By\n  expansion it is easy to see that one only needs matrix-vector or\n  vector-vector operations. The expanded version is:\n\n  ```None\n    f = 1 + rho * (grad_delta^T * H_0 * grad_delta)\n    H_1 - H_0 = - rho * [position_delta * (H_0 * grad_delta)^T +\n                        (H_0 * grad_delta) * position_delta^T] +\n                  rho * f * [position_delta * position_delta^T]\n  ```\n\n  All the terms in square brackets are matrices and are constructed using\n  vector outer products. All the other terms on the right hand side are scalars.\n  Also worth noting that the first and second lines are both rank 1 updates\n  applied to the current inverse Hessian estimate.\n\n  Args:\n    grad_delta: Real `Tensor` of shape `[..., n]`. The difference between the\n      gradient at the new position and the old position.\n    position_delta: Real `Tensor` of shape `[..., n]`. The change in position\n      from the previous iteration to the current one.\n    normalization_factor: Real `Tensor` of shape `[...]`. Should be equal to\n      `grad_delta^T * position_delta`, i.e. `1/rho` as defined above.\n    inv_hessian_estimate: Real `Tensor` of shape `[..., n, n]`. The previous\n      estimate of the inverse Hessian. Should be positive definite and\n      symmetric.\n\n  Returns:\n    A tuple containing the following fields\n      is_valid: A Boolean `Tensor` of shape `[...]` indicating batch members\n        where the update succeeded. The update can fail if the position change\n        becomes orthogonal to the gradient change.\n      next_inv_hessian_estimate: A `Tensor` of shape `[..., n, n]`. The next\n        Hessian estimate updated using the BFGS update scheme. If the\n        `inv_hessian_estimate` is symmetric and positive definite, the\n        `next_inv_hessian_estimate` is guaranteed to satisfy the same\n        conditions.\n  \"\"\"\n  # The quadratic form: y^T.H.y; where H is the inverse Hessian and y is the\n  # gradient change.\n  conditioned_grad_delta = _mul_right(inv_hessian_estimate, grad_delta)\n  conditioned_grad_delta_norm = tf.reduce_sum(\n      input_tensor=conditioned_grad_delta * grad_delta, axis=-1)\n\n  # The first rank 1 update term requires the outer product: s.y^T.\n  cross_term = _tensor_product(position_delta, conditioned_grad_delta)\n\n  def _expand_scalar(s):\n    # Expand dimensions of a batch of scalars to multiply or divide a matrix.\n    return s[..., tf.newaxis, tf.newaxis]\n\n  # Symmetrize\n  cross_term += _tensor_product(conditioned_grad_delta, position_delta)\n  position_term = _tensor_product(position_delta, position_delta)\n  with tf.control_dependencies([position_term]):\n    position_term *= _expand_scalar(\n        1 + conditioned_grad_delta_norm / normalization_factor)\n\n  return (inv_hessian_estimate +\n          (position_term - cross_term) / _expand_scalar(normalization_factor))", "language": "python", "code": "def _bfgs_inv_hessian_update(grad_delta, position_delta, normalization_factor,\n                             inv_hessian_estimate):\n  \"\"\"Applies the BFGS update to the inverse Hessian estimate.\n\n  The BFGS update rule is (note A^T denotes the transpose of a vector/matrix A).\n\n  ```None\n    rho = 1/(grad_delta^T * position_delta)\n    U = (I - rho * position_delta * grad_delta^T)\n    H_1 =  U * H_0 * U^T + rho * position_delta * position_delta^T\n  ```\n\n  Here, `H_0` is the inverse Hessian estimate at the previous iteration and\n  `H_1` is the next estimate. Note that `*` should be interpreted as the\n  matrix multiplication (with the understanding that matrix multiplication for\n  scalars is usual multiplication and for matrix with vector is the action of\n  the matrix on the vector.).\n\n  The implementation below utilizes an expanded version of the above formula\n  to avoid the matrix multiplications that would be needed otherwise. By\n  expansion it is easy to see that one only needs matrix-vector or\n  vector-vector operations. The expanded version is:\n\n  ```None\n    f = 1 + rho * (grad_delta^T * H_0 * grad_delta)\n    H_1 - H_0 = - rho * [position_delta * (H_0 * grad_delta)^T +\n                        (H_0 * grad_delta) * position_delta^T] +\n                  rho * f * [position_delta * position_delta^T]\n  ```\n\n  All the terms in square brackets are matrices and are constructed using\n  vector outer products. All the other terms on the right hand side are scalars.\n  Also worth noting that the first and second lines are both rank 1 updates\n  applied to the current inverse Hessian estimate.\n\n  Args:\n    grad_delta: Real `Tensor` of shape `[..., n]`. The difference between the\n      gradient at the new position and the old position.\n    position_delta: Real `Tensor` of shape `[..., n]`. The change in position\n      from the previous iteration to the current one.\n    normalization_factor: Real `Tensor` of shape `[...]`. Should be equal to\n      `grad_delta^T * position_delta`, i.e. `1/rho` as defined above.\n    inv_hessian_estimate: Real `Tensor` of shape `[..., n, n]`. The previous\n      estimate of the inverse Hessian. Should be positive definite and\n      symmetric.\n\n  Returns:\n    A tuple containing the following fields\n      is_valid: A Boolean `Tensor` of shape `[...]` indicating batch members\n        where the update succeeded. The update can fail if the position change\n        becomes orthogonal to the gradient change.\n      next_inv_hessian_estimate: A `Tensor` of shape `[..., n, n]`. The next\n        Hessian estimate updated using the BFGS update scheme. If the\n        `inv_hessian_estimate` is symmetric and positive definite, the\n        `next_inv_hessian_estimate` is guaranteed to satisfy the same\n        conditions.\n  \"\"\"\n  # The quadratic form: y^T.H.y; where H is the inverse Hessian and y is the\n  # gradient change.\n  conditioned_grad_delta = _mul_right(inv_hessian_estimate, grad_delta)\n  conditioned_grad_delta_norm = tf.reduce_sum(\n      input_tensor=conditioned_grad_delta * grad_delta, axis=-1)\n\n  # The first rank 1 update term requires the outer product: s.y^T.\n  cross_term = _tensor_product(position_delta, conditioned_grad_delta)\n\n  def _expand_scalar(s):\n    # Expand dimensions of a batch of scalars to multiply or divide a matrix.\n    return s[..., tf.newaxis, tf.newaxis]\n\n  # Symmetrize\n  cross_term += _tensor_product(conditioned_grad_delta, position_delta)\n  position_term = _tensor_product(position_delta, position_delta)\n  with tf.control_dependencies([position_term]):\n    position_term *= _expand_scalar(\n        1 + conditioned_grad_delta_norm / normalization_factor)\n\n  return (inv_hessian_estimate +\n          (position_term - cross_term) / _expand_scalar(normalization_factor))", "code_tokens": ["def", "_bfgs_inv_hessian_update", "(", "grad_delta", ",", "position_delta", ",", "normalization_factor", ",", "inv_hessian_estimate", ")", ":", "# The quadratic form: y^T.H.y; where H is the inverse Hessian and y is the", "# gradient change.", "conditioned_grad_delta", "=", "_mul_right", "(", "inv_hessian_estimate", ",", "grad_delta", ")", "conditioned_grad_delta_norm", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "conditioned_grad_delta", "*", "grad_delta", ",", "axis", "=", "-", "1", ")", "# The first rank 1 update term requires the outer product: s.y^T.", "cross_term", "=", "_tensor_product", "(", "position_delta", ",", "conditioned_grad_delta", ")", "def", "_expand_scalar", "(", "s", ")", ":", "# Expand dimensions of a batch of scalars to multiply or divide a matrix.", "return", "s", "[", "...", ",", "tf", ".", "newaxis", ",", "tf", ".", "newaxis", "]", "# Symmetrize", "cross_term", "+=", "_tensor_product", "(", "conditioned_grad_delta", ",", "position_delta", ")", "position_term", "=", "_tensor_product", "(", "position_delta", ",", "position_delta", ")", "with", "tf", ".", "control_dependencies", "(", "[", "position_term", "]", ")", ":", "position_term", "*=", "_expand_scalar", "(", "1", "+", "conditioned_grad_delta_norm", "/", "normalization_factor", ")", "return", "(", "inv_hessian_estimate", "+", "(", "position_term", "-", "cross_term", ")", "/", "_expand_scalar", "(", "normalization_factor", ")", ")"], "docstring": "Applies the BFGS update to the inverse Hessian estimate.\n\n  The BFGS update rule is (note A^T denotes the transpose of a vector/matrix A).\n\n  ```None\n    rho = 1/(grad_delta^T * position_delta)\n    U = (I - rho * position_delta * grad_delta^T)\n    H_1 =  U * H_0 * U^T + rho * position_delta * position_delta^T\n  ```\n\n  Here, `H_0` is the inverse Hessian estimate at the previous iteration and\n  `H_1` is the next estimate. Note that `*` should be interpreted as the\n  matrix multiplication (with the understanding that matrix multiplication for\n  scalars is usual multiplication and for matrix with vector is the action of\n  the matrix on the vector.).\n\n  The implementation below utilizes an expanded version of the above formula\n  to avoid the matrix multiplications that would be needed otherwise. By\n  expansion it is easy to see that one only needs matrix-vector or\n  vector-vector operations. The expanded version is:\n\n  ```None\n    f = 1 + rho * (grad_delta^T * H_0 * grad_delta)\n    H_1 - H_0 = - rho * [position_delta * (H_0 * grad_delta)^T +\n                        (H_0 * grad_delta) * position_delta^T] +\n                  rho * f * [position_delta * position_delta^T]\n  ```\n\n  All the terms in square brackets are matrices and are constructed using\n  vector outer products. All the other terms on the right hand side are scalars.\n  Also worth noting that the first and second lines are both rank 1 updates\n  applied to the current inverse Hessian estimate.\n\n  Args:\n    grad_delta: Real `Tensor` of shape `[..., n]`. The difference between the\n      gradient at the new position and the old position.\n    position_delta: Real `Tensor` of shape `[..., n]`. The change in position\n      from the previous iteration to the current one.\n    normalization_factor: Real `Tensor` of shape `[...]`. Should be equal to\n      `grad_delta^T * position_delta`, i.e. `1/rho` as defined above.\n    inv_hessian_estimate: Real `Tensor` of shape `[..., n, n]`. The previous\n      estimate of the inverse Hessian. Should be positive definite and\n      symmetric.\n\n  Returns:\n    A tuple containing the following fields\n      is_valid: A Boolean `Tensor` of shape `[...]` indicating batch members\n        where the update succeeded. The update can fail if the position change\n        becomes orthogonal to the gradient change.\n      next_inv_hessian_estimate: A `Tensor` of shape `[..., n, n]`. The next\n        Hessian estimate updated using the BFGS update scheme. If the\n        `inv_hessian_estimate` is symmetric and positive definite, the\n        `next_inv_hessian_estimate` is guaranteed to satisfy the same\n        conditions.", "docstring_tokens": ["Applies", "the", "BFGS", "update", "to", "the", "inverse", "Hessian", "estimate", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs.py#L355-L433", "partition": "test", "index": 1064, "time": "2018-06-16 05:15:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs.py", "func_name": "_tensor_product", "original_string": "def _tensor_product(t1, t2):\n  \"\"\"Computes the outer product of two possibly batched vectors.\n\n  Args:\n    t1: A `tf.Tensor` of shape `[..., n]`.\n    t2: A `tf.Tensor` of shape `[..., m]`.\n\n  Returns:\n    A tensor of shape `[..., n, m]` with matching batch dimensions, let's call\n    it `r`, whose components are:\n\n    ```None\n      r[..., i, j] = t1[..., i] * t2[..., j]\n    ```\n  \"\"\"\n  return tf.matmul(tf.expand_dims(t1, axis=-1), tf.expand_dims(t2, axis=-2))", "language": "python", "code": "def _tensor_product(t1, t2):\n  \"\"\"Computes the outer product of two possibly batched vectors.\n\n  Args:\n    t1: A `tf.Tensor` of shape `[..., n]`.\n    t2: A `tf.Tensor` of shape `[..., m]`.\n\n  Returns:\n    A tensor of shape `[..., n, m]` with matching batch dimensions, let's call\n    it `r`, whose components are:\n\n    ```None\n      r[..., i, j] = t1[..., i] * t2[..., j]\n    ```\n  \"\"\"\n  return tf.matmul(tf.expand_dims(t1, axis=-1), tf.expand_dims(t2, axis=-2))", "code_tokens": ["def", "_tensor_product", "(", "t1", ",", "t2", ")", ":", "return", "tf", ".", "matmul", "(", "tf", ".", "expand_dims", "(", "t1", ",", "axis", "=", "-", "1", ")", ",", "tf", ".", "expand_dims", "(", "t2", ",", "axis", "=", "-", "2", ")", ")"], "docstring": "Computes the outer product of two possibly batched vectors.\n\n  Args:\n    t1: A `tf.Tensor` of shape `[..., n]`.\n    t2: A `tf.Tensor` of shape `[..., m]`.\n\n  Returns:\n    A tensor of shape `[..., n, m]` with matching batch dimensions, let's call\n    it `r`, whose components are:\n\n    ```None\n      r[..., i, j] = t1[..., i] * t2[..., j]\n    ```", "docstring_tokens": ["Computes", "the", "outer", "product", "of", "two", "possibly", "batched", "vectors", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs.py#L476-L491", "partition": "test", "index": 1066, "time": "2018-06-16 05:15:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/slice_sampler_kernel.py", "func_name": "_sample_next", "original_string": "def _sample_next(target_log_prob_fn,\n                 current_state_parts,\n                 step_sizes,\n                 max_doublings,\n                 current_target_log_prob,\n                 batch_rank,\n                 seed=None,\n                 name=None):\n  \"\"\"Applies a single iteration of slice sampling update.\n\n  Applies hit and run style slice sampling. Chooses a uniform random direction\n  on the unit sphere in the event space. Applies the one dimensional slice\n  sampling update along that direction.\n\n  Args:\n    target_log_prob_fn: Python callable which takes an argument like\n      `*current_state_parts` and returns its (possibly unnormalized) log-density\n      under the target distribution.\n    current_state_parts: Python `list` of `Tensor`s representing the current\n      state(s) of the Markov chain(s). The first `independent_chain_ndims` of\n      the `Tensor`(s) index different chains.\n    step_sizes: Python `list` of `Tensor`s. Provides a measure of the width\n      of the density. Used to find the slice bounds. Must broadcast with the\n      shape of `current_state_parts`.\n    max_doublings: Integer number of doublings to allow while locating the slice\n      boundaries.\n    current_target_log_prob: `Tensor` representing the value of\n      `target_log_prob_fn(*current_state_parts)`. The only reason to specify\n      this argument is to reduce TF graph size.\n    batch_rank: Integer. The number of axes in the state that correspond to\n      independent batches.\n    seed: Python integer to seed random number generators.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    proposed_state_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n    proposed_target_log_prob: `Tensor` representing the value of\n      `target_log_prob_fn` at `next_state`.\n    bounds_satisfied: Boolean `Tensor` of the same shape as the log density.\n      True indicates whether the an interval containing the slice for that\n      batch was found successfully.\n    direction: `Tensor` or Python list of `Tensors`s representing the direction\n      along which the slice was sampled. Has the same shape and dtype(s) as\n      `current_state_parts`.\n    upper_bounds: `Tensor` of batch shape and the dtype of the input state. The\n      upper bounds of the slices along the sampling direction.\n    lower_bounds: `Tensor` of batch shape and the dtype of the input state. The\n      lower bounds of the slices along the sampling direction.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'sample_next', [\n      current_state_parts, step_sizes, max_doublings, current_target_log_prob,\n      batch_rank\n  ]):\n    # First step: Choose a random direction.\n    # Direction is a list of tensors. The i'th tensor should have the same shape\n    # as the i'th state part.\n    direction = _choose_random_direction(current_state_parts,\n                                         batch_rank=batch_rank,\n                                         seed=seed)\n\n    # Interpolates the step sizes for the chosen direction.\n    # Applies an ellipsoidal interpolation to compute the step direction for\n    # the chosen direction. Suppose we are given step sizes for each direction.\n    # Label these s_1, s_2, ... s_k. These are the step sizes to use if moving\n    # in a direction parallel to one of the axes. Consider an ellipsoid which\n    # intercepts the i'th axis at s_i. The step size for a direction specified\n    # by the unit vector (n_1, n_2 ...n_k) is then defined as the intersection\n    # of the line through this vector with this ellipsoid.\n    #\n    # One can show that the length of the vector from the origin to the\n    # intersection point is given by:\n    # 1 / sqrt(n_1^2 / s_1^2  + n_2^2 / s_2^2  + ...).\n    #\n    # Proof:\n    # The equation of the ellipsoid is:\n    # Sum_i [x_i^2 / s_i^2 ] = 1. Let n be a unit direction vector. Points\n    # along the line given by n may be parameterized as alpha*n where alpha is\n    # the distance along the vector. Plugging this into the equation for the\n    # ellipsoid, we get:\n    # alpha^2 ( n_1^2 / s_1^2 + n_2^2 / s_2^2 + ...) = 1\n    # so alpha = \\sqrt { \\frac{1} { ( n_1^2 / s_1^2 + n_2^2 / s_2^2 + ...) } }\n    reduce_axes = [tf.range(batch_rank, tf.rank(dirn_part))\n                   for dirn_part in direction]\n\n    components = [\n        tf.reduce_sum(\n            input_tensor=(dirn_part / step_size)**2, axis=reduce_axes[i])\n        for i, (step_size, dirn_part) in enumerate(zip(step_sizes, direction))\n    ]\n    step_size = tf.math.rsqrt(tf.add_n(components))\n    # Computes the rank of a tensor. Uses the static rank if possible.\n    def _get_rank(x):\n      return (len(x.shape.as_list()) if x.shape.dims is not None\n              else tf.rank(x))\n    state_part_ranks = [_get_rank(part) for part in current_state_parts]\n\n    def _step_along_direction(alpha):\n      \"\"\"Converts the scalar alpha into an n-dim vector with full state info.\n\n      Computes x_0 + alpha * direction where x_0 is the current state and\n      direction is the direction chosen above.\n\n      Args:\n        alpha: A tensor of shape equal to the batch dimensions of\n          `current_state_parts`.\n\n      Returns:\n        state_parts: Tensor or Python list of `Tensor`s representing the\n          state(s) of the Markov chain(s) for a given alpha and a given chosen\n          direction. Has the same shape as `current_state_parts`.\n      \"\"\"\n      padded_alphas = [_right_pad(alpha, final_rank=part_rank)\n                       for part_rank in state_part_ranks]\n\n      state_parts = [state_part + padded_alpha\n                     * direction_part for state_part, direction_part,\n                     padded_alpha in\n                     zip(current_state_parts, direction, padded_alphas)]\n      return state_parts\n\n    def projected_target_log_prob_fn(alpha):\n      \"\"\"The target log density projected along the chosen direction.\n\n      Args:\n        alpha: A tensor of shape equal to the batch dimensions of\n          `current_state_parts`.\n\n      Returns:\n        Target log density evaluated at x_0 + alpha * direction where x_0 is the\n        current state and direction is the direction chosen above. Has the same\n        shape as `alpha`.\n      \"\"\"\n      return target_log_prob_fn(*_step_along_direction(alpha))\n\n    alpha_init = tf.zeros_like(current_target_log_prob,\n                               dtype=current_state_parts[0].dtype.base_dtype)\n    [\n        next_alpha,\n        next_target_log_prob,\n        bounds_satisfied,\n        upper_bounds,\n        lower_bounds\n    ] = ssu.slice_sampler_one_dim(projected_target_log_prob_fn,\n                                  x_initial=alpha_init,\n                                  max_doublings=max_doublings,\n                                  step_size=step_size, seed=seed)\n    return [\n        _step_along_direction(next_alpha),\n        next_target_log_prob,\n        bounds_satisfied,\n        direction,\n        upper_bounds,\n        lower_bounds\n    ]", "language": "python", "code": "def _sample_next(target_log_prob_fn,\n                 current_state_parts,\n                 step_sizes,\n                 max_doublings,\n                 current_target_log_prob,\n                 batch_rank,\n                 seed=None,\n                 name=None):\n  \"\"\"Applies a single iteration of slice sampling update.\n\n  Applies hit and run style slice sampling. Chooses a uniform random direction\n  on the unit sphere in the event space. Applies the one dimensional slice\n  sampling update along that direction.\n\n  Args:\n    target_log_prob_fn: Python callable which takes an argument like\n      `*current_state_parts` and returns its (possibly unnormalized) log-density\n      under the target distribution.\n    current_state_parts: Python `list` of `Tensor`s representing the current\n      state(s) of the Markov chain(s). The first `independent_chain_ndims` of\n      the `Tensor`(s) index different chains.\n    step_sizes: Python `list` of `Tensor`s. Provides a measure of the width\n      of the density. Used to find the slice bounds. Must broadcast with the\n      shape of `current_state_parts`.\n    max_doublings: Integer number of doublings to allow while locating the slice\n      boundaries.\n    current_target_log_prob: `Tensor` representing the value of\n      `target_log_prob_fn(*current_state_parts)`. The only reason to specify\n      this argument is to reduce TF graph size.\n    batch_rank: Integer. The number of axes in the state that correspond to\n      independent batches.\n    seed: Python integer to seed random number generators.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    proposed_state_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n    proposed_target_log_prob: `Tensor` representing the value of\n      `target_log_prob_fn` at `next_state`.\n    bounds_satisfied: Boolean `Tensor` of the same shape as the log density.\n      True indicates whether the an interval containing the slice for that\n      batch was found successfully.\n    direction: `Tensor` or Python list of `Tensors`s representing the direction\n      along which the slice was sampled. Has the same shape and dtype(s) as\n      `current_state_parts`.\n    upper_bounds: `Tensor` of batch shape and the dtype of the input state. The\n      upper bounds of the slices along the sampling direction.\n    lower_bounds: `Tensor` of batch shape and the dtype of the input state. The\n      lower bounds of the slices along the sampling direction.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'sample_next', [\n      current_state_parts, step_sizes, max_doublings, current_target_log_prob,\n      batch_rank\n  ]):\n    # First step: Choose a random direction.\n    # Direction is a list of tensors. The i'th tensor should have the same shape\n    # as the i'th state part.\n    direction = _choose_random_direction(current_state_parts,\n                                         batch_rank=batch_rank,\n                                         seed=seed)\n\n    # Interpolates the step sizes for the chosen direction.\n    # Applies an ellipsoidal interpolation to compute the step direction for\n    # the chosen direction. Suppose we are given step sizes for each direction.\n    # Label these s_1, s_2, ... s_k. These are the step sizes to use if moving\n    # in a direction parallel to one of the axes. Consider an ellipsoid which\n    # intercepts the i'th axis at s_i. The step size for a direction specified\n    # by the unit vector (n_1, n_2 ...n_k) is then defined as the intersection\n    # of the line through this vector with this ellipsoid.\n    #\n    # One can show that the length of the vector from the origin to the\n    # intersection point is given by:\n    # 1 / sqrt(n_1^2 / s_1^2  + n_2^2 / s_2^2  + ...).\n    #\n    # Proof:\n    # The equation of the ellipsoid is:\n    # Sum_i [x_i^2 / s_i^2 ] = 1. Let n be a unit direction vector. Points\n    # along the line given by n may be parameterized as alpha*n where alpha is\n    # the distance along the vector. Plugging this into the equation for the\n    # ellipsoid, we get:\n    # alpha^2 ( n_1^2 / s_1^2 + n_2^2 / s_2^2 + ...) = 1\n    # so alpha = \\sqrt { \\frac{1} { ( n_1^2 / s_1^2 + n_2^2 / s_2^2 + ...) } }\n    reduce_axes = [tf.range(batch_rank, tf.rank(dirn_part))\n                   for dirn_part in direction]\n\n    components = [\n        tf.reduce_sum(\n            input_tensor=(dirn_part / step_size)**2, axis=reduce_axes[i])\n        for i, (step_size, dirn_part) in enumerate(zip(step_sizes, direction))\n    ]\n    step_size = tf.math.rsqrt(tf.add_n(components))\n    # Computes the rank of a tensor. Uses the static rank if possible.\n    def _get_rank(x):\n      return (len(x.shape.as_list()) if x.shape.dims is not None\n              else tf.rank(x))\n    state_part_ranks = [_get_rank(part) for part in current_state_parts]\n\n    def _step_along_direction(alpha):\n      \"\"\"Converts the scalar alpha into an n-dim vector with full state info.\n\n      Computes x_0 + alpha * direction where x_0 is the current state and\n      direction is the direction chosen above.\n\n      Args:\n        alpha: A tensor of shape equal to the batch dimensions of\n          `current_state_parts`.\n\n      Returns:\n        state_parts: Tensor or Python list of `Tensor`s representing the\n          state(s) of the Markov chain(s) for a given alpha and a given chosen\n          direction. Has the same shape as `current_state_parts`.\n      \"\"\"\n      padded_alphas = [_right_pad(alpha, final_rank=part_rank)\n                       for part_rank in state_part_ranks]\n\n      state_parts = [state_part + padded_alpha\n                     * direction_part for state_part, direction_part,\n                     padded_alpha in\n                     zip(current_state_parts, direction, padded_alphas)]\n      return state_parts\n\n    def projected_target_log_prob_fn(alpha):\n      \"\"\"The target log density projected along the chosen direction.\n\n      Args:\n        alpha: A tensor of shape equal to the batch dimensions of\n          `current_state_parts`.\n\n      Returns:\n        Target log density evaluated at x_0 + alpha * direction where x_0 is the\n        current state and direction is the direction chosen above. Has the same\n        shape as `alpha`.\n      \"\"\"\n      return target_log_prob_fn(*_step_along_direction(alpha))\n\n    alpha_init = tf.zeros_like(current_target_log_prob,\n                               dtype=current_state_parts[0].dtype.base_dtype)\n    [\n        next_alpha,\n        next_target_log_prob,\n        bounds_satisfied,\n        upper_bounds,\n        lower_bounds\n    ] = ssu.slice_sampler_one_dim(projected_target_log_prob_fn,\n                                  x_initial=alpha_init,\n                                  max_doublings=max_doublings,\n                                  step_size=step_size, seed=seed)\n    return [\n        _step_along_direction(next_alpha),\n        next_target_log_prob,\n        bounds_satisfied,\n        direction,\n        upper_bounds,\n        lower_bounds\n    ]", "code_tokens": ["def", "_sample_next", "(", "target_log_prob_fn", ",", "current_state_parts", ",", "step_sizes", ",", "max_doublings", ",", "current_target_log_prob", ",", "batch_rank", ",", "seed", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'sample_next'", ",", "[", "current_state_parts", ",", "step_sizes", ",", "max_doublings", ",", "current_target_log_prob", ",", "batch_rank", "]", ")", ":", "# First step: Choose a random direction.", "# Direction is a list of tensors. The i'th tensor should have the same shape", "# as the i'th state part.", "direction", "=", "_choose_random_direction", "(", "current_state_parts", ",", "batch_rank", "=", "batch_rank", ",", "seed", "=", "seed", ")", "# Interpolates the step sizes for the chosen direction.", "# Applies an ellipsoidal interpolation to compute the step direction for", "# the chosen direction. Suppose we are given step sizes for each direction.", "# Label these s_1, s_2, ... s_k. These are the step sizes to use if moving", "# in a direction parallel to one of the axes. Consider an ellipsoid which", "# intercepts the i'th axis at s_i. The step size for a direction specified", "# by the unit vector (n_1, n_2 ...n_k) is then defined as the intersection", "# of the line through this vector with this ellipsoid.", "#", "# One can show that the length of the vector from the origin to the", "# intersection point is given by:", "# 1 / sqrt(n_1^2 / s_1^2  + n_2^2 / s_2^2  + ...).", "#", "# Proof:", "# The equation of the ellipsoid is:", "# Sum_i [x_i^2 / s_i^2 ] = 1. Let n be a unit direction vector. Points", "# along the line given by n may be parameterized as alpha*n where alpha is", "# the distance along the vector. Plugging this into the equation for the", "# ellipsoid, we get:", "# alpha^2 ( n_1^2 / s_1^2 + n_2^2 / s_2^2 + ...) = 1", "# so alpha = \\sqrt { \\frac{1} { ( n_1^2 / s_1^2 + n_2^2 / s_2^2 + ...) } }", "reduce_axes", "=", "[", "tf", ".", "range", "(", "batch_rank", ",", "tf", ".", "rank", "(", "dirn_part", ")", ")", "for", "dirn_part", "in", "direction", "]", "components", "=", "[", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "(", "dirn_part", "/", "step_size", ")", "**", "2", ",", "axis", "=", "reduce_axes", "[", "i", "]", ")", "for", "i", ",", "(", "step_size", ",", "dirn_part", ")", "in", "enumerate", "(", "zip", "(", "step_sizes", ",", "direction", ")", ")", "]", "step_size", "=", "tf", ".", "math", ".", "rsqrt", "(", "tf", ".", "add_n", "(", "components", ")", ")", "# Computes the rank of a tensor. Uses the static rank if possible.", "def", "_get_rank", "(", "x", ")", ":", "return", "(", "len", "(", "x", ".", "shape", ".", "as_list", "(", ")", ")", "if", "x", ".", "shape", ".", "dims", "is", "not", "None", "else", "tf", ".", "rank", "(", "x", ")", ")", "state_part_ranks", "=", "[", "_get_rank", "(", "part", ")", "for", "part", "in", "current_state_parts", "]", "def", "_step_along_direction", "(", "alpha", ")", ":", "\"\"\"Converts the scalar alpha into an n-dim vector with full state info.\n\n      Computes x_0 + alpha * direction where x_0 is the current state and\n      direction is the direction chosen above.\n\n      Args:\n        alpha: A tensor of shape equal to the batch dimensions of\n          `current_state_parts`.\n\n      Returns:\n        state_parts: Tensor or Python list of `Tensor`s representing the\n          state(s) of the Markov chain(s) for a given alpha and a given chosen\n          direction. Has the same shape as `current_state_parts`.\n      \"\"\"", "padded_alphas", "=", "[", "_right_pad", "(", "alpha", ",", "final_rank", "=", "part_rank", ")", "for", "part_rank", "in", "state_part_ranks", "]", "state_parts", "=", "[", "state_part", "+", "padded_alpha", "*", "direction_part", "for", "state_part", ",", "direction_part", ",", "padded_alpha", "in", "zip", "(", "current_state_parts", ",", "direction", ",", "padded_alphas", ")", "]", "return", "state_parts", "def", "projected_target_log_prob_fn", "(", "alpha", ")", ":", "\"\"\"The target log density projected along the chosen direction.\n\n      Args:\n        alpha: A tensor of shape equal to the batch dimensions of\n          `current_state_parts`.\n\n      Returns:\n        Target log density evaluated at x_0 + alpha * direction where x_0 is the\n        current state and direction is the direction chosen above. Has the same\n        shape as `alpha`.\n      \"\"\"", "return", "target_log_prob_fn", "(", "*", "_step_along_direction", "(", "alpha", ")", ")", "alpha_init", "=", "tf", ".", "zeros_like", "(", "current_target_log_prob", ",", "dtype", "=", "current_state_parts", "[", "0", "]", ".", "dtype", ".", "base_dtype", ")", "[", "next_alpha", ",", "next_target_log_prob", ",", "bounds_satisfied", ",", "upper_bounds", ",", "lower_bounds", "]", "=", "ssu", ".", "slice_sampler_one_dim", "(", "projected_target_log_prob_fn", ",", "x_initial", "=", "alpha_init", ",", "max_doublings", "=", "max_doublings", ",", "step_size", "=", "step_size", ",", "seed", "=", "seed", ")", "return", "[", "_step_along_direction", "(", "next_alpha", ")", ",", "next_target_log_prob", ",", "bounds_satisfied", ",", "direction", ",", "upper_bounds", ",", "lower_bounds", "]"], "docstring": "Applies a single iteration of slice sampling update.\n\n  Applies hit and run style slice sampling. Chooses a uniform random direction\n  on the unit sphere in the event space. Applies the one dimensional slice\n  sampling update along that direction.\n\n  Args:\n    target_log_prob_fn: Python callable which takes an argument like\n      `*current_state_parts` and returns its (possibly unnormalized) log-density\n      under the target distribution.\n    current_state_parts: Python `list` of `Tensor`s representing the current\n      state(s) of the Markov chain(s). The first `independent_chain_ndims` of\n      the `Tensor`(s) index different chains.\n    step_sizes: Python `list` of `Tensor`s. Provides a measure of the width\n      of the density. Used to find the slice bounds. Must broadcast with the\n      shape of `current_state_parts`.\n    max_doublings: Integer number of doublings to allow while locating the slice\n      boundaries.\n    current_target_log_prob: `Tensor` representing the value of\n      `target_log_prob_fn(*current_state_parts)`. The only reason to specify\n      this argument is to reduce TF graph size.\n    batch_rank: Integer. The number of axes in the state that correspond to\n      independent batches.\n    seed: Python integer to seed random number generators.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    proposed_state_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n    proposed_target_log_prob: `Tensor` representing the value of\n      `target_log_prob_fn` at `next_state`.\n    bounds_satisfied: Boolean `Tensor` of the same shape as the log density.\n      True indicates whether the an interval containing the slice for that\n      batch was found successfully.\n    direction: `Tensor` or Python list of `Tensors`s representing the direction\n      along which the slice was sampled. Has the same shape and dtype(s) as\n      `current_state_parts`.\n    upper_bounds: `Tensor` of batch shape and the dtype of the input state. The\n      upper bounds of the slices along the sampling direction.\n    lower_bounds: `Tensor` of batch shape and the dtype of the input state. The\n      lower bounds of the slices along the sampling direction.", "docstring_tokens": ["Applies", "a", "single", "iteration", "of", "slice", "sampling", "update", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/slice_sampler_kernel.py#L381-L537", "partition": "test", "index": 694, "time": "2018-06-19 06:01:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/slice_sampler_utils.py", "func_name": "slice_sampler_one_dim", "original_string": "def slice_sampler_one_dim(target_log_prob, x_initial, step_size=0.01,\n                          max_doublings=30, seed=None, name=None):\n  \"\"\"For a given x position in each Markov chain, returns the next x.\n\n  Applies the one dimensional slice sampling algorithm as defined in Neal (2003)\n  to an input tensor x of shape (num_chains,) where num_chains is the number of\n  simulataneous Markov chains, and returns the next tensor x of shape\n  (num_chains,) when these chains are evolved by the slice sampling algorithm.\n\n  Args:\n    target_log_prob: Callable accepting a tensor like `x_initial` and returning\n      a tensor containing the log density at that point of the same shape.\n    x_initial: A tensor of any shape. The initial positions of the chains. This\n      function assumes that all the dimensions of `x_initial` are batch\n      dimensions (i.e. the event shape is `[]`).\n    step_size: A tensor of shape and dtype compatible with `x_initial`. The min\n      interval size in the doubling algorithm.\n    max_doublings: Scalar tensor of dtype `tf.int32`. The maximum number of\n      doublings to try to find the slice bounds.\n    seed: (Optional) positive int. The random seed. If None, no seed is set.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    retval: A tensor of the same shape and dtype as `x_initial`. The next state\n      of the Markov chain.\n    next_target_log_prob: The target log density evaluated at `retval`.\n    bounds_satisfied: A tensor of bool dtype and shape batch dimensions.\n    upper_bounds: Tensor of the same shape and dtype as `x_initial`. The upper\n      bounds for the slice found.\n    lower_bounds: Tensor of the same shape and dtype as `x_initial`. The lower\n      bounds for the slice found.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'slice_sampler_one_dim',\n                               [x_initial, step_size, max_doublings]):\n    x_initial = tf.convert_to_tensor(value=x_initial)\n    # Obtain the input dtype of the array.\n    dtype = x_initial.dtype.base_dtype\n    # Select the height of the slice. Tensor of shape x_initial.shape.\n    log_slice_heights = target_log_prob(x_initial) - tf.random.gamma(\n        tf.shape(input=x_initial), alpha=1, dtype=dtype, seed=seed)\n    # Given the above x and slice heights, compute the bounds of the slice for\n    # each chain.\n    upper_bounds, lower_bounds, bounds_satisfied = slice_bounds_by_doubling(\n        x_initial, target_log_prob, log_slice_heights, max_doublings, step_size,\n        seed=seed)\n    retval = _sample_with_shrinkage(x_initial, target_log_prob=target_log_prob,\n                                    log_slice_heights=log_slice_heights,\n                                    step_size=step_size,\n                                    lower_bounds=lower_bounds,\n                                    upper_bounds=upper_bounds, seed=seed)\n    return (retval, target_log_prob(retval), bounds_satisfied,\n            upper_bounds, lower_bounds)", "language": "python", "code": "def slice_sampler_one_dim(target_log_prob, x_initial, step_size=0.01,\n                          max_doublings=30, seed=None, name=None):\n  \"\"\"For a given x position in each Markov chain, returns the next x.\n\n  Applies the one dimensional slice sampling algorithm as defined in Neal (2003)\n  to an input tensor x of shape (num_chains,) where num_chains is the number of\n  simulataneous Markov chains, and returns the next tensor x of shape\n  (num_chains,) when these chains are evolved by the slice sampling algorithm.\n\n  Args:\n    target_log_prob: Callable accepting a tensor like `x_initial` and returning\n      a tensor containing the log density at that point of the same shape.\n    x_initial: A tensor of any shape. The initial positions of the chains. This\n      function assumes that all the dimensions of `x_initial` are batch\n      dimensions (i.e. the event shape is `[]`).\n    step_size: A tensor of shape and dtype compatible with `x_initial`. The min\n      interval size in the doubling algorithm.\n    max_doublings: Scalar tensor of dtype `tf.int32`. The maximum number of\n      doublings to try to find the slice bounds.\n    seed: (Optional) positive int. The random seed. If None, no seed is set.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    retval: A tensor of the same shape and dtype as `x_initial`. The next state\n      of the Markov chain.\n    next_target_log_prob: The target log density evaluated at `retval`.\n    bounds_satisfied: A tensor of bool dtype and shape batch dimensions.\n    upper_bounds: Tensor of the same shape and dtype as `x_initial`. The upper\n      bounds for the slice found.\n    lower_bounds: Tensor of the same shape and dtype as `x_initial`. The lower\n      bounds for the slice found.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'slice_sampler_one_dim',\n                               [x_initial, step_size, max_doublings]):\n    x_initial = tf.convert_to_tensor(value=x_initial)\n    # Obtain the input dtype of the array.\n    dtype = x_initial.dtype.base_dtype\n    # Select the height of the slice. Tensor of shape x_initial.shape.\n    log_slice_heights = target_log_prob(x_initial) - tf.random.gamma(\n        tf.shape(input=x_initial), alpha=1, dtype=dtype, seed=seed)\n    # Given the above x and slice heights, compute the bounds of the slice for\n    # each chain.\n    upper_bounds, lower_bounds, bounds_satisfied = slice_bounds_by_doubling(\n        x_initial, target_log_prob, log_slice_heights, max_doublings, step_size,\n        seed=seed)\n    retval = _sample_with_shrinkage(x_initial, target_log_prob=target_log_prob,\n                                    log_slice_heights=log_slice_heights,\n                                    step_size=step_size,\n                                    lower_bounds=lower_bounds,\n                                    upper_bounds=upper_bounds, seed=seed)\n    return (retval, target_log_prob(retval), bounds_satisfied,\n            upper_bounds, lower_bounds)", "code_tokens": ["def", "slice_sampler_one_dim", "(", "target_log_prob", ",", "x_initial", ",", "step_size", "=", "0.01", ",", "max_doublings", "=", "30", ",", "seed", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'slice_sampler_one_dim'", ",", "[", "x_initial", ",", "step_size", ",", "max_doublings", "]", ")", ":", "x_initial", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x_initial", ")", "# Obtain the input dtype of the array.", "dtype", "=", "x_initial", ".", "dtype", ".", "base_dtype", "# Select the height of the slice. Tensor of shape x_initial.shape.", "log_slice_heights", "=", "target_log_prob", "(", "x_initial", ")", "-", "tf", ".", "random", ".", "gamma", "(", "tf", ".", "shape", "(", "input", "=", "x_initial", ")", ",", "alpha", "=", "1", ",", "dtype", "=", "dtype", ",", "seed", "=", "seed", ")", "# Given the above x and slice heights, compute the bounds of the slice for", "# each chain.", "upper_bounds", ",", "lower_bounds", ",", "bounds_satisfied", "=", "slice_bounds_by_doubling", "(", "x_initial", ",", "target_log_prob", ",", "log_slice_heights", ",", "max_doublings", ",", "step_size", ",", "seed", "=", "seed", ")", "retval", "=", "_sample_with_shrinkage", "(", "x_initial", ",", "target_log_prob", "=", "target_log_prob", ",", "log_slice_heights", "=", "log_slice_heights", ",", "step_size", "=", "step_size", ",", "lower_bounds", "=", "lower_bounds", ",", "upper_bounds", "=", "upper_bounds", ",", "seed", "=", "seed", ")", "return", "(", "retval", ",", "target_log_prob", "(", "retval", ")", ",", "bounds_satisfied", ",", "upper_bounds", ",", "lower_bounds", ")"], "docstring": "For a given x position in each Markov chain, returns the next x.\n\n  Applies the one dimensional slice sampling algorithm as defined in Neal (2003)\n  to an input tensor x of shape (num_chains,) where num_chains is the number of\n  simulataneous Markov chains, and returns the next tensor x of shape\n  (num_chains,) when these chains are evolved by the slice sampling algorithm.\n\n  Args:\n    target_log_prob: Callable accepting a tensor like `x_initial` and returning\n      a tensor containing the log density at that point of the same shape.\n    x_initial: A tensor of any shape. The initial positions of the chains. This\n      function assumes that all the dimensions of `x_initial` are batch\n      dimensions (i.e. the event shape is `[]`).\n    step_size: A tensor of shape and dtype compatible with `x_initial`. The min\n      interval size in the doubling algorithm.\n    max_doublings: Scalar tensor of dtype `tf.int32`. The maximum number of\n      doublings to try to find the slice bounds.\n    seed: (Optional) positive int. The random seed. If None, no seed is set.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    retval: A tensor of the same shape and dtype as `x_initial`. The next state\n      of the Markov chain.\n    next_target_log_prob: The target log density evaluated at `retval`.\n    bounds_satisfied: A tensor of bool dtype and shape batch dimensions.\n    upper_bounds: Tensor of the same shape and dtype as `x_initial`. The upper\n      bounds for the slice found.\n    lower_bounds: Tensor of the same shape and dtype as `x_initial`. The lower\n      bounds for the slice found.", "docstring_tokens": ["For", "a", "given", "x", "position", "in", "each", "Markov", "chain", "returns", "the", "next", "x", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/slice_sampler_utils.py#L379-L431", "partition": "test", "index": 796, "time": "2018-06-19 06:01:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/slice_sampler_utils.py", "func_name": "_sample_with_shrinkage", "original_string": "def _sample_with_shrinkage(x_initial, target_log_prob, log_slice_heights,\n                           step_size, lower_bounds, upper_bounds, seed=None,\n                           name=None):\n  \"\"\"Samples from the slice by applying shrinkage for rejected points.\n\n  Implements the one dimensional slice sampling algorithm of Neal (2003), with a\n  doubling algorithm (Neal 2003 P715 Fig. 4), which doubles the size of the\n  interval at each iteration and shrinkage (Neal 2003 P716 Fig. 5), which\n  reduces the width of the slice when a selected point is rejected, by setting\n  the relevant bound that that value. Randomly sampled points are checked for\n  two criteria: that they lie within the slice and that they pass the\n  acceptability check (Neal 2003 P717 Fig. 6), which tests that the new state\n  could have generated the previous one.\n\n  Args:\n    x_initial: A tensor of any shape. The initial positions of the chains. This\n      function assumes that all the dimensions of `x_initial` are batch\n      dimensions (i.e. the event shape is `[]`).\n    target_log_prob: Callable accepting a tensor like `x_initial` and returning\n      a tensor containing the log density at that point of the same shape.\n    log_slice_heights: Tensor of the same shape and dtype as the return value\n      of `target_log_prob` when applied to `x_initial`. The log of the height of\n      the chosen slice.\n    step_size: A tensor of shape and dtype compatible with `x_initial`. The min\n      interval size in the doubling algorithm.\n    lower_bounds: Tensor of same shape and dtype as `x_initial`. Slice lower\n      bounds for each chain.\n    upper_bounds: Tensor of same shape and dtype as `x_initial`. Slice upper\n      bounds for each chain.\n    seed: (Optional) positive int. The random seed. If None, no seed is set.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    x_proposed: A tensor of the same shape and dtype as `x_initial`. The next\n      proposed state of the chain.\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'sample_with_shrinkage',\n      [x_initial, log_slice_heights, step_size, lower_bounds, upper_bounds]):\n    seed_gen = distributions.SeedStream(seed, salt='_sample_with_shrinkage')\n    # Keeps track of whether an acceptable sample has been found for the chain.\n    found = tf.zeros_like(x_initial, dtype=tf.bool)\n    cond = lambda found, *ignored_args: ~tf.reduce_all(input_tensor=found)\n    x_next = tf.identity(x_initial)\n    x_initial_shape = tf.shape(input=x_initial)\n    x_initial_dtype = x_initial.dtype.base_dtype\n    def _body(found, left, right, x_next):\n      \"\"\"Iterates until every chain has found a suitable next state.\"\"\"\n      proportions = tf.random.uniform(\n          x_initial_shape, dtype=x_initial_dtype, seed=seed_gen())\n      x_proposed = tf.where(~found, left + proportions * (right - left), x_next)\n      accept_res = _test_acceptance(x_initial, target_log_prob=target_log_prob,\n                                    decided=found,\n                                    log_slice_heights=log_slice_heights,\n                                    x_proposed=x_proposed, step_size=step_size,\n                                    lower_bounds=left, upper_bounds=right)\n      boundary_test = log_slice_heights < target_log_prob(x_proposed)\n      can_accept = boundary_test & accept_res\n      next_found = found | can_accept\n      # Note that it might seem that we are moving the left and right end points\n      # even if the point has been accepted (which is contrary to the stated\n      # algorithm in Neal). However, this does not matter because the endpoints\n      # for points that have been already accepted are not used again so it\n      # doesn't matter what we do with them.\n      next_left = tf.where(x_proposed < x_initial, x_proposed, left)\n      next_right = tf.where(x_proposed >= x_initial, x_proposed, right)\n      return next_found, next_left, next_right, x_proposed\n\n    return tf.while_loop(\n        cond=cond,\n        body=_body,\n        loop_vars=(found, lower_bounds, upper_bounds, x_next))[-1]", "language": "python", "code": "def _sample_with_shrinkage(x_initial, target_log_prob, log_slice_heights,\n                           step_size, lower_bounds, upper_bounds, seed=None,\n                           name=None):\n  \"\"\"Samples from the slice by applying shrinkage for rejected points.\n\n  Implements the one dimensional slice sampling algorithm of Neal (2003), with a\n  doubling algorithm (Neal 2003 P715 Fig. 4), which doubles the size of the\n  interval at each iteration and shrinkage (Neal 2003 P716 Fig. 5), which\n  reduces the width of the slice when a selected point is rejected, by setting\n  the relevant bound that that value. Randomly sampled points are checked for\n  two criteria: that they lie within the slice and that they pass the\n  acceptability check (Neal 2003 P717 Fig. 6), which tests that the new state\n  could have generated the previous one.\n\n  Args:\n    x_initial: A tensor of any shape. The initial positions of the chains. This\n      function assumes that all the dimensions of `x_initial` are batch\n      dimensions (i.e. the event shape is `[]`).\n    target_log_prob: Callable accepting a tensor like `x_initial` and returning\n      a tensor containing the log density at that point of the same shape.\n    log_slice_heights: Tensor of the same shape and dtype as the return value\n      of `target_log_prob` when applied to `x_initial`. The log of the height of\n      the chosen slice.\n    step_size: A tensor of shape and dtype compatible with `x_initial`. The min\n      interval size in the doubling algorithm.\n    lower_bounds: Tensor of same shape and dtype as `x_initial`. Slice lower\n      bounds for each chain.\n    upper_bounds: Tensor of same shape and dtype as `x_initial`. Slice upper\n      bounds for each chain.\n    seed: (Optional) positive int. The random seed. If None, no seed is set.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    x_proposed: A tensor of the same shape and dtype as `x_initial`. The next\n      proposed state of the chain.\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'sample_with_shrinkage',\n      [x_initial, log_slice_heights, step_size, lower_bounds, upper_bounds]):\n    seed_gen = distributions.SeedStream(seed, salt='_sample_with_shrinkage')\n    # Keeps track of whether an acceptable sample has been found for the chain.\n    found = tf.zeros_like(x_initial, dtype=tf.bool)\n    cond = lambda found, *ignored_args: ~tf.reduce_all(input_tensor=found)\n    x_next = tf.identity(x_initial)\n    x_initial_shape = tf.shape(input=x_initial)\n    x_initial_dtype = x_initial.dtype.base_dtype\n    def _body(found, left, right, x_next):\n      \"\"\"Iterates until every chain has found a suitable next state.\"\"\"\n      proportions = tf.random.uniform(\n          x_initial_shape, dtype=x_initial_dtype, seed=seed_gen())\n      x_proposed = tf.where(~found, left + proportions * (right - left), x_next)\n      accept_res = _test_acceptance(x_initial, target_log_prob=target_log_prob,\n                                    decided=found,\n                                    log_slice_heights=log_slice_heights,\n                                    x_proposed=x_proposed, step_size=step_size,\n                                    lower_bounds=left, upper_bounds=right)\n      boundary_test = log_slice_heights < target_log_prob(x_proposed)\n      can_accept = boundary_test & accept_res\n      next_found = found | can_accept\n      # Note that it might seem that we are moving the left and right end points\n      # even if the point has been accepted (which is contrary to the stated\n      # algorithm in Neal). However, this does not matter because the endpoints\n      # for points that have been already accepted are not used again so it\n      # doesn't matter what we do with them.\n      next_left = tf.where(x_proposed < x_initial, x_proposed, left)\n      next_right = tf.where(x_proposed >= x_initial, x_proposed, right)\n      return next_found, next_left, next_right, x_proposed\n\n    return tf.while_loop(\n        cond=cond,\n        body=_body,\n        loop_vars=(found, lower_bounds, upper_bounds, x_next))[-1]", "code_tokens": ["def", "_sample_with_shrinkage", "(", "x_initial", ",", "target_log_prob", ",", "log_slice_heights", ",", "step_size", ",", "lower_bounds", ",", "upper_bounds", ",", "seed", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'sample_with_shrinkage'", ",", "[", "x_initial", ",", "log_slice_heights", ",", "step_size", ",", "lower_bounds", ",", "upper_bounds", "]", ")", ":", "seed_gen", "=", "distributions", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'_sample_with_shrinkage'", ")", "# Keeps track of whether an acceptable sample has been found for the chain.", "found", "=", "tf", ".", "zeros_like", "(", "x_initial", ",", "dtype", "=", "tf", ".", "bool", ")", "cond", "=", "lambda", "found", ",", "*", "ignored_args", ":", "~", "tf", ".", "reduce_all", "(", "input_tensor", "=", "found", ")", "x_next", "=", "tf", ".", "identity", "(", "x_initial", ")", "x_initial_shape", "=", "tf", ".", "shape", "(", "input", "=", "x_initial", ")", "x_initial_dtype", "=", "x_initial", ".", "dtype", ".", "base_dtype", "def", "_body", "(", "found", ",", "left", ",", "right", ",", "x_next", ")", ":", "\"\"\"Iterates until every chain has found a suitable next state.\"\"\"", "proportions", "=", "tf", ".", "random", ".", "uniform", "(", "x_initial_shape", ",", "dtype", "=", "x_initial_dtype", ",", "seed", "=", "seed_gen", "(", ")", ")", "x_proposed", "=", "tf", ".", "where", "(", "~", "found", ",", "left", "+", "proportions", "*", "(", "right", "-", "left", ")", ",", "x_next", ")", "accept_res", "=", "_test_acceptance", "(", "x_initial", ",", "target_log_prob", "=", "target_log_prob", ",", "decided", "=", "found", ",", "log_slice_heights", "=", "log_slice_heights", ",", "x_proposed", "=", "x_proposed", ",", "step_size", "=", "step_size", ",", "lower_bounds", "=", "left", ",", "upper_bounds", "=", "right", ")", "boundary_test", "=", "log_slice_heights", "<", "target_log_prob", "(", "x_proposed", ")", "can_accept", "=", "boundary_test", "&", "accept_res", "next_found", "=", "found", "|", "can_accept", "# Note that it might seem that we are moving the left and right end points", "# even if the point has been accepted (which is contrary to the stated", "# algorithm in Neal). However, this does not matter because the endpoints", "# for points that have been already accepted are not used again so it", "# doesn't matter what we do with them.", "next_left", "=", "tf", ".", "where", "(", "x_proposed", "<", "x_initial", ",", "x_proposed", ",", "left", ")", "next_right", "=", "tf", ".", "where", "(", "x_proposed", ">=", "x_initial", ",", "x_proposed", ",", "right", ")", "return", "next_found", ",", "next_left", ",", "next_right", ",", "x_proposed", "return", "tf", ".", "while_loop", "(", "cond", "=", "cond", ",", "body", "=", "_body", ",", "loop_vars", "=", "(", "found", ",", "lower_bounds", ",", "upper_bounds", ",", "x_next", ")", ")", "[", "-", "1", "]"], "docstring": "Samples from the slice by applying shrinkage for rejected points.\n\n  Implements the one dimensional slice sampling algorithm of Neal (2003), with a\n  doubling algorithm (Neal 2003 P715 Fig. 4), which doubles the size of the\n  interval at each iteration and shrinkage (Neal 2003 P716 Fig. 5), which\n  reduces the width of the slice when a selected point is rejected, by setting\n  the relevant bound that that value. Randomly sampled points are checked for\n  two criteria: that they lie within the slice and that they pass the\n  acceptability check (Neal 2003 P717 Fig. 6), which tests that the new state\n  could have generated the previous one.\n\n  Args:\n    x_initial: A tensor of any shape. The initial positions of the chains. This\n      function assumes that all the dimensions of `x_initial` are batch\n      dimensions (i.e. the event shape is `[]`).\n    target_log_prob: Callable accepting a tensor like `x_initial` and returning\n      a tensor containing the log density at that point of the same shape.\n    log_slice_heights: Tensor of the same shape and dtype as the return value\n      of `target_log_prob` when applied to `x_initial`. The log of the height of\n      the chosen slice.\n    step_size: A tensor of shape and dtype compatible with `x_initial`. The min\n      interval size in the doubling algorithm.\n    lower_bounds: Tensor of same shape and dtype as `x_initial`. Slice lower\n      bounds for each chain.\n    upper_bounds: Tensor of same shape and dtype as `x_initial`. Slice upper\n      bounds for each chain.\n    seed: (Optional) positive int. The random seed. If None, no seed is set.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    x_proposed: A tensor of the same shape and dtype as `x_initial`. The next\n      proposed state of the chain.", "docstring_tokens": ["Samples", "from", "the", "slice", "by", "applying", "shrinkage", "for", "rejected", "points", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/slice_sampler_utils.py#L304-L376", "partition": "test", "index": 795, "time": "2018-06-19 06:01:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/slice_sampler_kernel.py", "func_name": "_choose_random_direction", "original_string": "def _choose_random_direction(current_state_parts, batch_rank, seed=None):\n  \"\"\"Chooses a random direction in the event space.\"\"\"\n  seed_gen = distributions.SeedStream(seed, salt='_choose_random_direction')\n  # Chooses the random directions across each of the input components.\n  rnd_direction_parts = [\n      tf.random.normal(\n          tf.shape(input=current_state_part), dtype=tf.float32, seed=seed_gen())\n      for current_state_part in current_state_parts\n  ]\n\n  # Sum squares over all of the input components. Note this takes all\n  # components into account.\n  sum_squares = sum(\n      tf.reduce_sum(\n          input_tensor=rnd_direction**2.,\n          axis=tf.range(batch_rank, tf.rank(rnd_direction)),\n          keepdims=True) for rnd_direction in rnd_direction_parts)\n\n  # Normalizes the random direction fragments.\n  rnd_direction_parts = [rnd_direction / tf.sqrt(sum_squares)\n                         for rnd_direction in rnd_direction_parts]\n\n  return rnd_direction_parts", "language": "python", "code": "def _choose_random_direction(current_state_parts, batch_rank, seed=None):\n  \"\"\"Chooses a random direction in the event space.\"\"\"\n  seed_gen = distributions.SeedStream(seed, salt='_choose_random_direction')\n  # Chooses the random directions across each of the input components.\n  rnd_direction_parts = [\n      tf.random.normal(\n          tf.shape(input=current_state_part), dtype=tf.float32, seed=seed_gen())\n      for current_state_part in current_state_parts\n  ]\n\n  # Sum squares over all of the input components. Note this takes all\n  # components into account.\n  sum_squares = sum(\n      tf.reduce_sum(\n          input_tensor=rnd_direction**2.,\n          axis=tf.range(batch_rank, tf.rank(rnd_direction)),\n          keepdims=True) for rnd_direction in rnd_direction_parts)\n\n  # Normalizes the random direction fragments.\n  rnd_direction_parts = [rnd_direction / tf.sqrt(sum_squares)\n                         for rnd_direction in rnd_direction_parts]\n\n  return rnd_direction_parts", "code_tokens": ["def", "_choose_random_direction", "(", "current_state_parts", ",", "batch_rank", ",", "seed", "=", "None", ")", ":", "seed_gen", "=", "distributions", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'_choose_random_direction'", ")", "# Chooses the random directions across each of the input components.", "rnd_direction_parts", "=", "[", "tf", ".", "random", ".", "normal", "(", "tf", ".", "shape", "(", "input", "=", "current_state_part", ")", ",", "dtype", "=", "tf", ".", "float32", ",", "seed", "=", "seed_gen", "(", ")", ")", "for", "current_state_part", "in", "current_state_parts", "]", "# Sum squares over all of the input components. Note this takes all", "# components into account.", "sum_squares", "=", "sum", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "rnd_direction", "**", "2.", ",", "axis", "=", "tf", ".", "range", "(", "batch_rank", ",", "tf", ".", "rank", "(", "rnd_direction", ")", ")", ",", "keepdims", "=", "True", ")", "for", "rnd_direction", "in", "rnd_direction_parts", ")", "# Normalizes the random direction fragments.", "rnd_direction_parts", "=", "[", "rnd_direction", "/", "tf", ".", "sqrt", "(", "sum_squares", ")", "for", "rnd_direction", "in", "rnd_direction_parts", "]", "return", "rnd_direction_parts"], "docstring": "Chooses a random direction in the event space.", "docstring_tokens": ["Chooses", "a", "random", "direction", "in", "the", "event", "space", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/slice_sampler_kernel.py#L356-L378", "partition": "test", "index": 693, "time": "2018-06-19 06:01:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/slice_sampler_kernel.py", "func_name": "_right_pad", "original_string": "def _right_pad(x, final_rank):\n  \"\"\"Pads the shape of x to the right to be of rank final_rank.\n\n  Expands the dims of `x` to the right such that its rank is equal to\n  final_rank. For example, if `x` is of shape [1, 5, 7, 2] and `final_rank` is\n  7, we return padded_x, which is of shape [1, 5, 7, 2, 1, 1, 1].\n\n  Args:\n    x: The tensor whose shape is to be padded.\n    final_rank: Scalar int32 `Tensor` or Python `int`. The desired rank of x.\n\n  Returns:\n    padded_x: A tensor of rank final_rank.\n  \"\"\"\n  padded_shape = tf.concat(\n      [tf.shape(input=x),\n       tf.ones(final_rank - tf.rank(x), dtype=tf.int32)],\n      axis=0)\n  static_padded_shape = None\n  if x.shape.is_fully_defined() and isinstance(final_rank, int):\n    static_padded_shape = x.shape.as_list()\n    extra_dims = final_rank - len(static_padded_shape)\n    static_padded_shape.extend([1] * extra_dims)\n\n  padded_x = tf.reshape(x, static_padded_shape or padded_shape)\n  return padded_x", "language": "python", "code": "def _right_pad(x, final_rank):\n  \"\"\"Pads the shape of x to the right to be of rank final_rank.\n\n  Expands the dims of `x` to the right such that its rank is equal to\n  final_rank. For example, if `x` is of shape [1, 5, 7, 2] and `final_rank` is\n  7, we return padded_x, which is of shape [1, 5, 7, 2, 1, 1, 1].\n\n  Args:\n    x: The tensor whose shape is to be padded.\n    final_rank: Scalar int32 `Tensor` or Python `int`. The desired rank of x.\n\n  Returns:\n    padded_x: A tensor of rank final_rank.\n  \"\"\"\n  padded_shape = tf.concat(\n      [tf.shape(input=x),\n       tf.ones(final_rank - tf.rank(x), dtype=tf.int32)],\n      axis=0)\n  static_padded_shape = None\n  if x.shape.is_fully_defined() and isinstance(final_rank, int):\n    static_padded_shape = x.shape.as_list()\n    extra_dims = final_rank - len(static_padded_shape)\n    static_padded_shape.extend([1] * extra_dims)\n\n  padded_x = tf.reshape(x, static_padded_shape or padded_shape)\n  return padded_x", "code_tokens": ["def", "_right_pad", "(", "x", ",", "final_rank", ")", ":", "padded_shape", "=", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "tf", ".", "ones", "(", "final_rank", "-", "tf", ".", "rank", "(", "x", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "]", ",", "axis", "=", "0", ")", "static_padded_shape", "=", "None", "if", "x", ".", "shape", ".", "is_fully_defined", "(", ")", "and", "isinstance", "(", "final_rank", ",", "int", ")", ":", "static_padded_shape", "=", "x", ".", "shape", ".", "as_list", "(", ")", "extra_dims", "=", "final_rank", "-", "len", "(", "static_padded_shape", ")", "static_padded_shape", ".", "extend", "(", "[", "1", "]", "*", "extra_dims", ")", "padded_x", "=", "tf", ".", "reshape", "(", "x", ",", "static_padded_shape", "or", "padded_shape", ")", "return", "padded_x"], "docstring": "Pads the shape of x to the right to be of rank final_rank.\n\n  Expands the dims of `x` to the right such that its rank is equal to\n  final_rank. For example, if `x` is of shape [1, 5, 7, 2] and `final_rank` is\n  7, we return padded_x, which is of shape [1, 5, 7, 2, 1, 1, 1].\n\n  Args:\n    x: The tensor whose shape is to be padded.\n    final_rank: Scalar int32 `Tensor` or Python `int`. The desired rank of x.\n\n  Returns:\n    padded_x: A tensor of rank final_rank.", "docstring_tokens": ["Pads", "the", "shape", "of", "x", "to", "the", "right", "to", "be", "of", "rank", "final_rank", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/slice_sampler_kernel.py#L555-L580", "partition": "test", "index": 696, "time": "2018-06-19 06:01:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/slice_sampler_kernel.py", "func_name": "SliceSampler.one_step", "original_string": "def one_step(self, current_state, previous_kernel_results):\n    \"\"\"Runs one iteration of Slice Sampler.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s representing the\n        current state(s) of the Markov chain(s). The first `r` dimensions\n        index independent chains,\n        `r = tf.rank(target_log_prob_fn(*current_state))`.\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.\n\n    Raises:\n      ValueError: if there isn't one `step_size` or a list with same length as\n        `current_state`.\n      TypeError: if `not target_log_prob.dtype.is_floating`.\n    \"\"\"\n    with tf.compat.v1.name_scope(\n        name=mcmc_util.make_name(self.name, 'slice', 'one_step'),\n        values=[\n            self.step_size, self.max_doublings, self._seed_stream,\n            current_state, previous_kernel_results.target_log_prob\n        ]):\n      with tf.compat.v1.name_scope('initialize'):\n        [\n            current_state_parts,\n            step_sizes,\n            current_target_log_prob\n        ] = _prepare_args(\n            self.target_log_prob_fn,\n            current_state,\n            self.step_size,\n            previous_kernel_results.target_log_prob,\n            maybe_expand=True)\n\n        max_doublings = tf.convert_to_tensor(\n            value=self.max_doublings, dtype=tf.int32, name='max_doublings')\n\n      independent_chain_ndims = distribution_util.prefer_static_rank(\n          current_target_log_prob)\n\n      [\n          next_state_parts,\n          next_target_log_prob,\n          bounds_satisfied,\n          direction,\n          upper_bounds,\n          lower_bounds\n      ] = _sample_next(\n          self.target_log_prob_fn,\n          current_state_parts,\n          step_sizes,\n          max_doublings,\n          current_target_log_prob,\n          independent_chain_ndims,\n          seed=self._seed_stream()\n      )\n\n      def maybe_flatten(x):\n        return x if mcmc_util.is_list_like(current_state) else x[0]\n\n      return [\n          maybe_flatten(next_state_parts),\n          SliceSamplerKernelResults(\n              target_log_prob=next_target_log_prob,\n              bounds_satisfied=bounds_satisfied,\n              direction=direction,\n              upper_bounds=upper_bounds,\n              lower_bounds=lower_bounds\n          ),\n      ]", "language": "python", "code": "def one_step(self, current_state, previous_kernel_results):\n    \"\"\"Runs one iteration of Slice Sampler.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s representing the\n        current state(s) of the Markov chain(s). The first `r` dimensions\n        index independent chains,\n        `r = tf.rank(target_log_prob_fn(*current_state))`.\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.\n\n    Raises:\n      ValueError: if there isn't one `step_size` or a list with same length as\n        `current_state`.\n      TypeError: if `not target_log_prob.dtype.is_floating`.\n    \"\"\"\n    with tf.compat.v1.name_scope(\n        name=mcmc_util.make_name(self.name, 'slice', 'one_step'),\n        values=[\n            self.step_size, self.max_doublings, self._seed_stream,\n            current_state, previous_kernel_results.target_log_prob\n        ]):\n      with tf.compat.v1.name_scope('initialize'):\n        [\n            current_state_parts,\n            step_sizes,\n            current_target_log_prob\n        ] = _prepare_args(\n            self.target_log_prob_fn,\n            current_state,\n            self.step_size,\n            previous_kernel_results.target_log_prob,\n            maybe_expand=True)\n\n        max_doublings = tf.convert_to_tensor(\n            value=self.max_doublings, dtype=tf.int32, name='max_doublings')\n\n      independent_chain_ndims = distribution_util.prefer_static_rank(\n          current_target_log_prob)\n\n      [\n          next_state_parts,\n          next_target_log_prob,\n          bounds_satisfied,\n          direction,\n          upper_bounds,\n          lower_bounds\n      ] = _sample_next(\n          self.target_log_prob_fn,\n          current_state_parts,\n          step_sizes,\n          max_doublings,\n          current_target_log_prob,\n          independent_chain_ndims,\n          seed=self._seed_stream()\n      )\n\n      def maybe_flatten(x):\n        return x if mcmc_util.is_list_like(current_state) else x[0]\n\n      return [\n          maybe_flatten(next_state_parts),\n          SliceSamplerKernelResults(\n              target_log_prob=next_target_log_prob,\n              bounds_satisfied=bounds_satisfied,\n              direction=direction,\n              upper_bounds=upper_bounds,\n              lower_bounds=lower_bounds\n          ),\n      ]", "code_tokens": ["def", "one_step", "(", "self", ",", "current_state", ",", "previous_kernel_results", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", "=", "mcmc_util", ".", "make_name", "(", "self", ".", "name", ",", "'slice'", ",", "'one_step'", ")", ",", "values", "=", "[", "self", ".", "step_size", ",", "self", ".", "max_doublings", ",", "self", ".", "_seed_stream", ",", "current_state", ",", "previous_kernel_results", ".", "target_log_prob", "]", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'initialize'", ")", ":", "[", "current_state_parts", ",", "step_sizes", ",", "current_target_log_prob", "]", "=", "_prepare_args", "(", "self", ".", "target_log_prob_fn", ",", "current_state", ",", "self", ".", "step_size", ",", "previous_kernel_results", ".", "target_log_prob", ",", "maybe_expand", "=", "True", ")", "max_doublings", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "self", ".", "max_doublings", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "'max_doublings'", ")", "independent_chain_ndims", "=", "distribution_util", ".", "prefer_static_rank", "(", "current_target_log_prob", ")", "[", "next_state_parts", ",", "next_target_log_prob", ",", "bounds_satisfied", ",", "direction", ",", "upper_bounds", ",", "lower_bounds", "]", "=", "_sample_next", "(", "self", ".", "target_log_prob_fn", ",", "current_state_parts", ",", "step_sizes", ",", "max_doublings", ",", "current_target_log_prob", ",", "independent_chain_ndims", ",", "seed", "=", "self", ".", "_seed_stream", "(", ")", ")", "def", "maybe_flatten", "(", "x", ")", ":", "return", "x", "if", "mcmc_util", ".", "is_list_like", "(", "current_state", ")", "else", "x", "[", "0", "]", "return", "[", "maybe_flatten", "(", "next_state_parts", ")", ",", "SliceSamplerKernelResults", "(", "target_log_prob", "=", "next_target_log_prob", ",", "bounds_satisfied", "=", "bounds_satisfied", ",", "direction", "=", "direction", ",", "upper_bounds", "=", "upper_bounds", ",", "lower_bounds", "=", "lower_bounds", ")", ",", "]"], "docstring": "Runs one iteration of Slice Sampler.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s representing the\n        current state(s) of the Markov chain(s). The first `r` dimensions\n        index independent chains,\n        `r = tf.rank(target_log_prob_fn(*current_state))`.\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.\n\n    Raises:\n      ValueError: if there isn't one `step_size` or a list with same length as\n        `current_state`.\n      TypeError: if `not target_log_prob.dtype.is_floating`.", "docstring_tokens": ["Runs", "one", "iteration", "of", "Slice", "Sampler", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/slice_sampler_kernel.py#L259-L336", "partition": "test", "index": 697, "time": "2018-06-19 06:01:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/slice_sampler_utils.py", "func_name": "slice_bounds_by_doubling", "original_string": "def slice_bounds_by_doubling(x_initial,\n                             target_log_prob,\n                             log_slice_heights,\n                             max_doublings,\n                             step_size,\n                             seed=None,\n                             name=None):\n  \"\"\"Returns the bounds of the slice at each stage of doubling procedure.\n\n  Precomputes the x coordinates of the left (L) and right (R) endpoints of the\n  interval `I` produced in the \"doubling\" algorithm [Neal 2003][1] P713. Note\n  that we simultaneously compute all possible doubling values for each chain,\n  for the reason that at small-medium densities, the gains from parallel\n  evaluation might cause a speed-up, but this will be benchmarked against the\n  while loop implementation.\n\n  Args:\n    x_initial: `tf.Tensor` of any shape and any real dtype consumable by\n      `target_log_prob`. The initial points.\n    target_log_prob: A callable taking a `tf.Tensor` of shape and dtype as\n      `x_initial` and returning a tensor of the same shape. The log density of\n      the target distribution.\n    log_slice_heights: `tf.Tensor` with the same shape as `x_initial` and the\n      same dtype as returned by `target_log_prob`. The log of the height of the\n      slice for each chain. The values must be bounded above by\n      `target_log_prob(x_initial)`.\n    max_doublings: Scalar positive int32 `tf.Tensor`. The maximum number of\n      doublings to consider.\n    step_size: `tf.Tensor` with same dtype as and shape compatible with\n      `x_initial`. The size of the initial interval.\n    seed: (Optional) positive int. The random seed. If None, no seed is set.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    upper_bounds: A tensor of same shape and dtype as `x_initial`. Slice upper\n      bounds for each chain.\n    lower_bounds: A tensor of same shape and dtype as `x_initial`. Slice lower\n      bounds for each chain.\n    both_ok: A tensor of shape `x_initial` and boolean dtype. Indicates if both\n      the chosen upper and lower bound lie outside of the slice.\n\n  #### References\n\n  [1]: Radford M. Neal. Slice Sampling. The Annals of Statistics. 2003, Vol 31,\n       No. 3 , 705-767.\n       https://projecteuclid.org/download/pdf_1/euclid.aos/1056562461\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'slice_bounds_by_doubling',\n      [x_initial, log_slice_heights, max_doublings, step_size]):\n    seed_gen = distributions.SeedStream(seed, salt='slice_bounds_by_doubling')\n    x_initial = tf.convert_to_tensor(value=x_initial)\n    batch_shape = tf.shape(input=x_initial)\n    dtype = step_size.dtype.base_dtype\n    left_endpoints = x_initial + step_size * tf.random.uniform(\n        batch_shape, minval=-1.0, maxval=0.0, dtype=dtype, seed=seed_gen())\n\n    # Compute the increments by which we need to step the upper and lower bounds\n    # part of the doubling procedure.\n    left_increments, widths = _left_doubling_increments(\n        batch_shape, max_doublings, step_size, seed=seed_gen())\n    # The left and right end points. Shape (max_doublings+1,) + batch_shape.\n    left_endpoints -= left_increments\n    right_endpoints = left_endpoints + widths\n\n    # Test if these end points lie outside of the slice.\n    # Checks if the end points of the slice are outside the graph of the pdf.\n    left_ep_values = tf.map_fn(target_log_prob, left_endpoints)\n    right_ep_values = tf.map_fn(target_log_prob, right_endpoints)\n    left_ok = left_ep_values < log_slice_heights\n    right_ok = right_ep_values < log_slice_heights\n    both_ok = left_ok & right_ok\n\n    both_ok_f = tf.reshape(both_ok, [max_doublings + 1, -1])\n\n    best_interval_idx = _find_best_interval_idx(\n        tf.cast(both_ok_f, dtype=tf.int32))\n\n    # Formats the above index as required to use with gather_nd.\n    point_index_gather = tf.stack(\n        [best_interval_idx,\n         tf.range(tf.size(input=best_interval_idx))],\n        axis=1,\n        name='point_index_gather')\n    left_ep_f = tf.reshape(left_endpoints, [max_doublings + 1, -1])\n    right_ep_f = tf.reshape(right_endpoints, [max_doublings + 1, -1])\n    # The x values of the uppper and lower bounds of the slices for each chain.\n    lower_bounds = tf.reshape(tf.gather_nd(left_ep_f, point_index_gather),\n                              batch_shape)\n    upper_bounds = tf.reshape(tf.gather_nd(right_ep_f, point_index_gather),\n                              batch_shape)\n    both_ok = tf.reduce_any(input_tensor=both_ok, axis=0)\n    return upper_bounds, lower_bounds, both_ok", "language": "python", "code": "def slice_bounds_by_doubling(x_initial,\n                             target_log_prob,\n                             log_slice_heights,\n                             max_doublings,\n                             step_size,\n                             seed=None,\n                             name=None):\n  \"\"\"Returns the bounds of the slice at each stage of doubling procedure.\n\n  Precomputes the x coordinates of the left (L) and right (R) endpoints of the\n  interval `I` produced in the \"doubling\" algorithm [Neal 2003][1] P713. Note\n  that we simultaneously compute all possible doubling values for each chain,\n  for the reason that at small-medium densities, the gains from parallel\n  evaluation might cause a speed-up, but this will be benchmarked against the\n  while loop implementation.\n\n  Args:\n    x_initial: `tf.Tensor` of any shape and any real dtype consumable by\n      `target_log_prob`. The initial points.\n    target_log_prob: A callable taking a `tf.Tensor` of shape and dtype as\n      `x_initial` and returning a tensor of the same shape. The log density of\n      the target distribution.\n    log_slice_heights: `tf.Tensor` with the same shape as `x_initial` and the\n      same dtype as returned by `target_log_prob`. The log of the height of the\n      slice for each chain. The values must be bounded above by\n      `target_log_prob(x_initial)`.\n    max_doublings: Scalar positive int32 `tf.Tensor`. The maximum number of\n      doublings to consider.\n    step_size: `tf.Tensor` with same dtype as and shape compatible with\n      `x_initial`. The size of the initial interval.\n    seed: (Optional) positive int. The random seed. If None, no seed is set.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    upper_bounds: A tensor of same shape and dtype as `x_initial`. Slice upper\n      bounds for each chain.\n    lower_bounds: A tensor of same shape and dtype as `x_initial`. Slice lower\n      bounds for each chain.\n    both_ok: A tensor of shape `x_initial` and boolean dtype. Indicates if both\n      the chosen upper and lower bound lie outside of the slice.\n\n  #### References\n\n  [1]: Radford M. Neal. Slice Sampling. The Annals of Statistics. 2003, Vol 31,\n       No. 3 , 705-767.\n       https://projecteuclid.org/download/pdf_1/euclid.aos/1056562461\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'slice_bounds_by_doubling',\n      [x_initial, log_slice_heights, max_doublings, step_size]):\n    seed_gen = distributions.SeedStream(seed, salt='slice_bounds_by_doubling')\n    x_initial = tf.convert_to_tensor(value=x_initial)\n    batch_shape = tf.shape(input=x_initial)\n    dtype = step_size.dtype.base_dtype\n    left_endpoints = x_initial + step_size * tf.random.uniform(\n        batch_shape, minval=-1.0, maxval=0.0, dtype=dtype, seed=seed_gen())\n\n    # Compute the increments by which we need to step the upper and lower bounds\n    # part of the doubling procedure.\n    left_increments, widths = _left_doubling_increments(\n        batch_shape, max_doublings, step_size, seed=seed_gen())\n    # The left and right end points. Shape (max_doublings+1,) + batch_shape.\n    left_endpoints -= left_increments\n    right_endpoints = left_endpoints + widths\n\n    # Test if these end points lie outside of the slice.\n    # Checks if the end points of the slice are outside the graph of the pdf.\n    left_ep_values = tf.map_fn(target_log_prob, left_endpoints)\n    right_ep_values = tf.map_fn(target_log_prob, right_endpoints)\n    left_ok = left_ep_values < log_slice_heights\n    right_ok = right_ep_values < log_slice_heights\n    both_ok = left_ok & right_ok\n\n    both_ok_f = tf.reshape(both_ok, [max_doublings + 1, -1])\n\n    best_interval_idx = _find_best_interval_idx(\n        tf.cast(both_ok_f, dtype=tf.int32))\n\n    # Formats the above index as required to use with gather_nd.\n    point_index_gather = tf.stack(\n        [best_interval_idx,\n         tf.range(tf.size(input=best_interval_idx))],\n        axis=1,\n        name='point_index_gather')\n    left_ep_f = tf.reshape(left_endpoints, [max_doublings + 1, -1])\n    right_ep_f = tf.reshape(right_endpoints, [max_doublings + 1, -1])\n    # The x values of the uppper and lower bounds of the slices for each chain.\n    lower_bounds = tf.reshape(tf.gather_nd(left_ep_f, point_index_gather),\n                              batch_shape)\n    upper_bounds = tf.reshape(tf.gather_nd(right_ep_f, point_index_gather),\n                              batch_shape)\n    both_ok = tf.reduce_any(input_tensor=both_ok, axis=0)\n    return upper_bounds, lower_bounds, both_ok", "code_tokens": ["def", "slice_bounds_by_doubling", "(", "x_initial", ",", "target_log_prob", ",", "log_slice_heights", ",", "max_doublings", ",", "step_size", ",", "seed", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'slice_bounds_by_doubling'", ",", "[", "x_initial", ",", "log_slice_heights", ",", "max_doublings", ",", "step_size", "]", ")", ":", "seed_gen", "=", "distributions", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'slice_bounds_by_doubling'", ")", "x_initial", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x_initial", ")", "batch_shape", "=", "tf", ".", "shape", "(", "input", "=", "x_initial", ")", "dtype", "=", "step_size", ".", "dtype", ".", "base_dtype", "left_endpoints", "=", "x_initial", "+", "step_size", "*", "tf", ".", "random", ".", "uniform", "(", "batch_shape", ",", "minval", "=", "-", "1.0", ",", "maxval", "=", "0.0", ",", "dtype", "=", "dtype", ",", "seed", "=", "seed_gen", "(", ")", ")", "# Compute the increments by which we need to step the upper and lower bounds", "# part of the doubling procedure.", "left_increments", ",", "widths", "=", "_left_doubling_increments", "(", "batch_shape", ",", "max_doublings", ",", "step_size", ",", "seed", "=", "seed_gen", "(", ")", ")", "# The left and right end points. Shape (max_doublings+1,) + batch_shape.", "left_endpoints", "-=", "left_increments", "right_endpoints", "=", "left_endpoints", "+", "widths", "# Test if these end points lie outside of the slice.", "# Checks if the end points of the slice are outside the graph of the pdf.", "left_ep_values", "=", "tf", ".", "map_fn", "(", "target_log_prob", ",", "left_endpoints", ")", "right_ep_values", "=", "tf", ".", "map_fn", "(", "target_log_prob", ",", "right_endpoints", ")", "left_ok", "=", "left_ep_values", "<", "log_slice_heights", "right_ok", "=", "right_ep_values", "<", "log_slice_heights", "both_ok", "=", "left_ok", "&", "right_ok", "both_ok_f", "=", "tf", ".", "reshape", "(", "both_ok", ",", "[", "max_doublings", "+", "1", ",", "-", "1", "]", ")", "best_interval_idx", "=", "_find_best_interval_idx", "(", "tf", ".", "cast", "(", "both_ok_f", ",", "dtype", "=", "tf", ".", "int32", ")", ")", "# Formats the above index as required to use with gather_nd.", "point_index_gather", "=", "tf", ".", "stack", "(", "[", "best_interval_idx", ",", "tf", ".", "range", "(", "tf", ".", "size", "(", "input", "=", "best_interval_idx", ")", ")", "]", ",", "axis", "=", "1", ",", "name", "=", "'point_index_gather'", ")", "left_ep_f", "=", "tf", ".", "reshape", "(", "left_endpoints", ",", "[", "max_doublings", "+", "1", ",", "-", "1", "]", ")", "right_ep_f", "=", "tf", ".", "reshape", "(", "right_endpoints", ",", "[", "max_doublings", "+", "1", ",", "-", "1", "]", ")", "# The x values of the uppper and lower bounds of the slices for each chain.", "lower_bounds", "=", "tf", ".", "reshape", "(", "tf", ".", "gather_nd", "(", "left_ep_f", ",", "point_index_gather", ")", ",", "batch_shape", ")", "upper_bounds", "=", "tf", ".", "reshape", "(", "tf", ".", "gather_nd", "(", "right_ep_f", ",", "point_index_gather", ")", ",", "batch_shape", ")", "both_ok", "=", "tf", ".", "reduce_any", "(", "input_tensor", "=", "both_ok", ",", "axis", "=", "0", ")", "return", "upper_bounds", ",", "lower_bounds", ",", "both_ok"], "docstring": "Returns the bounds of the slice at each stage of doubling procedure.\n\n  Precomputes the x coordinates of the left (L) and right (R) endpoints of the\n  interval `I` produced in the \"doubling\" algorithm [Neal 2003][1] P713. Note\n  that we simultaneously compute all possible doubling values for each chain,\n  for the reason that at small-medium densities, the gains from parallel\n  evaluation might cause a speed-up, but this will be benchmarked against the\n  while loop implementation.\n\n  Args:\n    x_initial: `tf.Tensor` of any shape and any real dtype consumable by\n      `target_log_prob`. The initial points.\n    target_log_prob: A callable taking a `tf.Tensor` of shape and dtype as\n      `x_initial` and returning a tensor of the same shape. The log density of\n      the target distribution.\n    log_slice_heights: `tf.Tensor` with the same shape as `x_initial` and the\n      same dtype as returned by `target_log_prob`. The log of the height of the\n      slice for each chain. The values must be bounded above by\n      `target_log_prob(x_initial)`.\n    max_doublings: Scalar positive int32 `tf.Tensor`. The maximum number of\n      doublings to consider.\n    step_size: `tf.Tensor` with same dtype as and shape compatible with\n      `x_initial`. The size of the initial interval.\n    seed: (Optional) positive int. The random seed. If None, no seed is set.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    upper_bounds: A tensor of same shape and dtype as `x_initial`. Slice upper\n      bounds for each chain.\n    lower_bounds: A tensor of same shape and dtype as `x_initial`. Slice lower\n      bounds for each chain.\n    both_ok: A tensor of shape `x_initial` and boolean dtype. Indicates if both\n      the chosen upper and lower bound lie outside of the slice.\n\n  #### References\n\n  [1]: Radford M. Neal. Slice Sampling. The Annals of Statistics. 2003, Vol 31,\n       No. 3 , 705-767.\n       https://projecteuclid.org/download/pdf_1/euclid.aos/1056562461", "docstring_tokens": ["Returns", "the", "bounds", "of", "the", "slice", "at", "each", "stage", "of", "doubling", "procedure", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/slice_sampler_utils.py#L129-L222", "partition": "test", "index": 794, "time": "2018-06-19 06:01:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/slice_sampler_utils.py", "func_name": "_find_best_interval_idx", "original_string": "def _find_best_interval_idx(x, name=None):\n  \"\"\"Finds the index of the optimal set of bounds for each chain.\n\n  For each chain, finds the smallest set of bounds for which both edges lie\n  outside the slice. This is equivalent to the point at which a for loop\n  implementation (P715 of Neal (2003)) of the algorithm would terminate.\n\n  Performs the following calculation, where i is the number of doublings that\n  have been performed and k is the max number of doublings:\n\n  (2 * k - i) * flag + i\n\n  The argmax of the above returns the earliest index where the bounds were\n  outside the slice and if there is no such point, the widest bounds.\n\n  Args:\n    x: A tensor of shape (max_doublings+1, batch_shape). Type int32, with value\n      0 or 1. Indicates if this set of bounds is outside the slice.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    indices: A tensor of shape batch_shape. Type int32, with the index of the\n      first set of bounds outside the slice and if there are none, the index of\n      the widest set.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'find_best_interval_idx', [x]):\n    # Returns max_doublings + 1. Positive int32.\n    k = tf.shape(input=x)[0]\n    dtype = x.dtype.base_dtype\n    # Factors by which to multiply the flag. Corresponds to (2 * k - i) above.\n    mults = tf.range(2 * k, k, -1, dtype=dtype)[:, tf.newaxis]\n    # Factors by which to shift the flag. Corresponds to i above. Ensures the\n    # widest bounds are selected if there are no bounds outside the slice.\n    shifts = tf.range(k, dtype=dtype)[:, tf.newaxis]\n    indices = tf.argmax(input=mults * x + shifts, axis=0, output_type=dtype)\n    return indices", "language": "python", "code": "def _find_best_interval_idx(x, name=None):\n  \"\"\"Finds the index of the optimal set of bounds for each chain.\n\n  For each chain, finds the smallest set of bounds for which both edges lie\n  outside the slice. This is equivalent to the point at which a for loop\n  implementation (P715 of Neal (2003)) of the algorithm would terminate.\n\n  Performs the following calculation, where i is the number of doublings that\n  have been performed and k is the max number of doublings:\n\n  (2 * k - i) * flag + i\n\n  The argmax of the above returns the earliest index where the bounds were\n  outside the slice and if there is no such point, the widest bounds.\n\n  Args:\n    x: A tensor of shape (max_doublings+1, batch_shape). Type int32, with value\n      0 or 1. Indicates if this set of bounds is outside the slice.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    indices: A tensor of shape batch_shape. Type int32, with the index of the\n      first set of bounds outside the slice and if there are none, the index of\n      the widest set.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'find_best_interval_idx', [x]):\n    # Returns max_doublings + 1. Positive int32.\n    k = tf.shape(input=x)[0]\n    dtype = x.dtype.base_dtype\n    # Factors by which to multiply the flag. Corresponds to (2 * k - i) above.\n    mults = tf.range(2 * k, k, -1, dtype=dtype)[:, tf.newaxis]\n    # Factors by which to shift the flag. Corresponds to i above. Ensures the\n    # widest bounds are selected if there are no bounds outside the slice.\n    shifts = tf.range(k, dtype=dtype)[:, tf.newaxis]\n    indices = tf.argmax(input=mults * x + shifts, axis=0, output_type=dtype)\n    return indices", "code_tokens": ["def", "_find_best_interval_idx", "(", "x", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'find_best_interval_idx'", ",", "[", "x", "]", ")", ":", "# Returns max_doublings + 1. Positive int32.", "k", "=", "tf", ".", "shape", "(", "input", "=", "x", ")", "[", "0", "]", "dtype", "=", "x", ".", "dtype", ".", "base_dtype", "# Factors by which to multiply the flag. Corresponds to (2 * k - i) above.", "mults", "=", "tf", ".", "range", "(", "2", "*", "k", ",", "k", ",", "-", "1", ",", "dtype", "=", "dtype", ")", "[", ":", ",", "tf", ".", "newaxis", "]", "# Factors by which to shift the flag. Corresponds to i above. Ensures the", "# widest bounds are selected if there are no bounds outside the slice.", "shifts", "=", "tf", ".", "range", "(", "k", ",", "dtype", "=", "dtype", ")", "[", ":", ",", "tf", ".", "newaxis", "]", "indices", "=", "tf", ".", "argmax", "(", "input", "=", "mults", "*", "x", "+", "shifts", ",", "axis", "=", "0", ",", "output_type", "=", "dtype", ")", "return", "indices"], "docstring": "Finds the index of the optimal set of bounds for each chain.\n\n  For each chain, finds the smallest set of bounds for which both edges lie\n  outside the slice. This is equivalent to the point at which a for loop\n  implementation (P715 of Neal (2003)) of the algorithm would terminate.\n\n  Performs the following calculation, where i is the number of doublings that\n  have been performed and k is the max number of doublings:\n\n  (2 * k - i) * flag + i\n\n  The argmax of the above returns the earliest index where the bounds were\n  outside the slice and if there is no such point, the widest bounds.\n\n  Args:\n    x: A tensor of shape (max_doublings+1, batch_shape). Type int32, with value\n      0 or 1. Indicates if this set of bounds is outside the slice.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    indices: A tensor of shape batch_shape. Type int32, with the index of the\n      first set of bounds outside the slice and if there are none, the index of\n      the widest set.", "docstring_tokens": ["Finds", "the", "index", "of", "the", "optimal", "set", "of", "bounds", "for", "each", "chain", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/slice_sampler_utils.py#L90-L126", "partition": "test", "index": 793, "time": "2018-06-19 06:01:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/slice_sampler_utils.py", "func_name": "_left_doubling_increments", "original_string": "def _left_doubling_increments(batch_shape, max_doublings, step_size, seed=None,\n                              name=None):\n  \"\"\"Computes the doubling increments for the left end point.\n\n  The doubling procedure expands an initial interval to find a superset of the\n  true slice. At each doubling iteration, the interval width is doubled to\n  either the left or the right hand side with equal probability.\n  If, initially, the left end point is at `L(0)` and the width of the\n  interval is `w(0)`, then the left end point and the width at the\n  k-th iteration (denoted L(k) and w(k) respectively) are given by the following\n  recursions:\n\n  ```none\n  w(k) = 2 * w(k-1)\n  L(k) = L(k-1) - w(k-1) * X_k, X_k ~ Bernoulli(0.5)\n  or, L(0) - L(k) = w(0) Sum(2^i * X(i+1), 0 <= i < k)\n  ```\n\n  This function computes the sequence of `L(0)-L(k)` and `w(k)` for k between 0\n  and `max_doublings` independently for each chain.\n\n  Args:\n    batch_shape: Positive int32 `tf.Tensor`. The batch shape.\n    max_doublings: Scalar positive int32 `tf.Tensor`. The maximum number of\n      doublings to consider.\n    step_size: A real `tf.Tensor` with shape compatible with [num_chains].\n      The size of the initial interval.\n    seed: (Optional) positive int. The random seed. If None, no seed is set.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    left_increments: A tensor of shape (max_doublings+1, batch_shape). The\n      relative position of the left end point after the doublings.\n    widths: A tensor of shape (max_doublings+1, ones_like(batch_shape)). The\n      widths of the intervals at each stage of the doubling.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'left_doubling_increments',\n                               [batch_shape, max_doublings, step_size]):\n\n    step_size = tf.convert_to_tensor(value=step_size)\n    dtype = step_size.dtype.base_dtype\n    # Output shape of the left increments tensor.\n    output_shape = tf.concat(([max_doublings + 1], batch_shape), axis=0)\n    # A sample realization of X_k.\n    expand_left = distributions.Bernoulli(0.5, dtype=dtype).sample(\n        sample_shape=output_shape, seed=seed)\n\n    # The widths of the successive intervals. Starts with 1.0 and ends with\n    # 2^max_doublings.\n    width_multipliers = tf.cast(2 ** tf.range(0, max_doublings+1), dtype=dtype)\n    # Output shape of the `widths` tensor.\n    widths_shape = tf.concat(([max_doublings + 1],\n                              tf.ones_like(batch_shape)), axis=0)\n    width_multipliers = tf.reshape(width_multipliers, shape=widths_shape)\n    # Widths shape is [max_doublings + 1, 1, 1, 1...].\n    widths = width_multipliers * step_size\n\n    # Take the cumulative sum of the left side increments in slice width to give\n    # the resulting distance from the inital lower bound.\n    left_increments = tf.cumsum(widths * expand_left, exclusive=True, axis=0)\n    return left_increments, widths", "language": "python", "code": "def _left_doubling_increments(batch_shape, max_doublings, step_size, seed=None,\n                              name=None):\n  \"\"\"Computes the doubling increments for the left end point.\n\n  The doubling procedure expands an initial interval to find a superset of the\n  true slice. At each doubling iteration, the interval width is doubled to\n  either the left or the right hand side with equal probability.\n  If, initially, the left end point is at `L(0)` and the width of the\n  interval is `w(0)`, then the left end point and the width at the\n  k-th iteration (denoted L(k) and w(k) respectively) are given by the following\n  recursions:\n\n  ```none\n  w(k) = 2 * w(k-1)\n  L(k) = L(k-1) - w(k-1) * X_k, X_k ~ Bernoulli(0.5)\n  or, L(0) - L(k) = w(0) Sum(2^i * X(i+1), 0 <= i < k)\n  ```\n\n  This function computes the sequence of `L(0)-L(k)` and `w(k)` for k between 0\n  and `max_doublings` independently for each chain.\n\n  Args:\n    batch_shape: Positive int32 `tf.Tensor`. The batch shape.\n    max_doublings: Scalar positive int32 `tf.Tensor`. The maximum number of\n      doublings to consider.\n    step_size: A real `tf.Tensor` with shape compatible with [num_chains].\n      The size of the initial interval.\n    seed: (Optional) positive int. The random seed. If None, no seed is set.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    left_increments: A tensor of shape (max_doublings+1, batch_shape). The\n      relative position of the left end point after the doublings.\n    widths: A tensor of shape (max_doublings+1, ones_like(batch_shape)). The\n      widths of the intervals at each stage of the doubling.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'left_doubling_increments',\n                               [batch_shape, max_doublings, step_size]):\n\n    step_size = tf.convert_to_tensor(value=step_size)\n    dtype = step_size.dtype.base_dtype\n    # Output shape of the left increments tensor.\n    output_shape = tf.concat(([max_doublings + 1], batch_shape), axis=0)\n    # A sample realization of X_k.\n    expand_left = distributions.Bernoulli(0.5, dtype=dtype).sample(\n        sample_shape=output_shape, seed=seed)\n\n    # The widths of the successive intervals. Starts with 1.0 and ends with\n    # 2^max_doublings.\n    width_multipliers = tf.cast(2 ** tf.range(0, max_doublings+1), dtype=dtype)\n    # Output shape of the `widths` tensor.\n    widths_shape = tf.concat(([max_doublings + 1],\n                              tf.ones_like(batch_shape)), axis=0)\n    width_multipliers = tf.reshape(width_multipliers, shape=widths_shape)\n    # Widths shape is [max_doublings + 1, 1, 1, 1...].\n    widths = width_multipliers * step_size\n\n    # Take the cumulative sum of the left side increments in slice width to give\n    # the resulting distance from the inital lower bound.\n    left_increments = tf.cumsum(widths * expand_left, exclusive=True, axis=0)\n    return left_increments, widths", "code_tokens": ["def", "_left_doubling_increments", "(", "batch_shape", ",", "max_doublings", ",", "step_size", ",", "seed", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'left_doubling_increments'", ",", "[", "batch_shape", ",", "max_doublings", ",", "step_size", "]", ")", ":", "step_size", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "step_size", ")", "dtype", "=", "step_size", ".", "dtype", ".", "base_dtype", "# Output shape of the left increments tensor.", "output_shape", "=", "tf", ".", "concat", "(", "(", "[", "max_doublings", "+", "1", "]", ",", "batch_shape", ")", ",", "axis", "=", "0", ")", "# A sample realization of X_k.", "expand_left", "=", "distributions", ".", "Bernoulli", "(", "0.5", ",", "dtype", "=", "dtype", ")", ".", "sample", "(", "sample_shape", "=", "output_shape", ",", "seed", "=", "seed", ")", "# The widths of the successive intervals. Starts with 1.0 and ends with", "# 2^max_doublings.", "width_multipliers", "=", "tf", ".", "cast", "(", "2", "**", "tf", ".", "range", "(", "0", ",", "max_doublings", "+", "1", ")", ",", "dtype", "=", "dtype", ")", "# Output shape of the `widths` tensor.", "widths_shape", "=", "tf", ".", "concat", "(", "(", "[", "max_doublings", "+", "1", "]", ",", "tf", ".", "ones_like", "(", "batch_shape", ")", ")", ",", "axis", "=", "0", ")", "width_multipliers", "=", "tf", ".", "reshape", "(", "width_multipliers", ",", "shape", "=", "widths_shape", ")", "# Widths shape is [max_doublings + 1, 1, 1, 1...].", "widths", "=", "width_multipliers", "*", "step_size", "# Take the cumulative sum of the left side increments in slice width to give", "# the resulting distance from the inital lower bound.", "left_increments", "=", "tf", ".", "cumsum", "(", "widths", "*", "expand_left", ",", "exclusive", "=", "True", ",", "axis", "=", "0", ")", "return", "left_increments", ",", "widths"], "docstring": "Computes the doubling increments for the left end point.\n\n  The doubling procedure expands an initial interval to find a superset of the\n  true slice. At each doubling iteration, the interval width is doubled to\n  either the left or the right hand side with equal probability.\n  If, initially, the left end point is at `L(0)` and the width of the\n  interval is `w(0)`, then the left end point and the width at the\n  k-th iteration (denoted L(k) and w(k) respectively) are given by the following\n  recursions:\n\n  ```none\n  w(k) = 2 * w(k-1)\n  L(k) = L(k-1) - w(k-1) * X_k, X_k ~ Bernoulli(0.5)\n  or, L(0) - L(k) = w(0) Sum(2^i * X(i+1), 0 <= i < k)\n  ```\n\n  This function computes the sequence of `L(0)-L(k)` and `w(k)` for k between 0\n  and `max_doublings` independently for each chain.\n\n  Args:\n    batch_shape: Positive int32 `tf.Tensor`. The batch shape.\n    max_doublings: Scalar positive int32 `tf.Tensor`. The maximum number of\n      doublings to consider.\n    step_size: A real `tf.Tensor` with shape compatible with [num_chains].\n      The size of the initial interval.\n    seed: (Optional) positive int. The random seed. If None, no seed is set.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'find_slice_bounds').\n\n  Returns:\n    left_increments: A tensor of shape (max_doublings+1, batch_shape). The\n      relative position of the left end point after the doublings.\n    widths: A tensor of shape (max_doublings+1, ones_like(batch_shape)). The\n      widths of the intervals at each stage of the doubling.", "docstring_tokens": ["Computes", "the", "doubling", "increments", "for", "the", "left", "end", "point", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/slice_sampler_utils.py#L26-L87", "partition": "test", "index": 792, "time": "2018-06-19 06:01:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/slice_sampler_kernel.py", "func_name": "_maybe_call_fn", "original_string": "def _maybe_call_fn(fn,\n                   fn_arg_list,\n                   fn_result=None,\n                   description='target_log_prob'):\n  \"\"\"Helper which computes `fn_result` if needed.\"\"\"\n  fn_arg_list = (list(fn_arg_list) if mcmc_util.is_list_like(fn_arg_list)\n                 else [fn_arg_list])\n  if fn_result is None:\n    fn_result = fn(*fn_arg_list)\n  if not fn_result.dtype.is_floating:\n    raise TypeError('`{}` must be a `Tensor` with `float` `dtype`.'.format(\n        description))\n  return fn_result", "language": "python", "code": "def _maybe_call_fn(fn,\n                   fn_arg_list,\n                   fn_result=None,\n                   description='target_log_prob'):\n  \"\"\"Helper which computes `fn_result` if needed.\"\"\"\n  fn_arg_list = (list(fn_arg_list) if mcmc_util.is_list_like(fn_arg_list)\n                 else [fn_arg_list])\n  if fn_result is None:\n    fn_result = fn(*fn_arg_list)\n  if not fn_result.dtype.is_floating:\n    raise TypeError('`{}` must be a `Tensor` with `float` `dtype`.'.format(\n        description))\n  return fn_result", "code_tokens": ["def", "_maybe_call_fn", "(", "fn", ",", "fn_arg_list", ",", "fn_result", "=", "None", ",", "description", "=", "'target_log_prob'", ")", ":", "fn_arg_list", "=", "(", "list", "(", "fn_arg_list", ")", "if", "mcmc_util", ".", "is_list_like", "(", "fn_arg_list", ")", "else", "[", "fn_arg_list", "]", ")", "if", "fn_result", "is", "None", ":", "fn_result", "=", "fn", "(", "*", "fn_arg_list", ")", "if", "not", "fn_result", ".", "dtype", ".", "is_floating", ":", "raise", "TypeError", "(", "'`{}` must be a `Tensor` with `float` `dtype`.'", ".", "format", "(", "description", ")", ")", "return", "fn_result"], "docstring": "Helper which computes `fn_result` if needed.", "docstring_tokens": ["Helper", "which", "computes", "fn_result", "if", "needed", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/slice_sampler_kernel.py#L540-L552", "partition": "test", "index": 695, "time": "2018-06-19 06:01:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/random_ops.py", "func_name": "random_rayleigh", "original_string": "def random_rayleigh(shape, scale=None, dtype=tf.float32, seed=None, name=None):\n  \"\"\"Generates `Tensor` of positive reals drawn from a Rayleigh distributions.\n\n  The probability density function of a Rayleigh distribution with `scale`\n  parameter is given by:\n\n  ```none\n  f(x) = x scale**-2 exp(-x**2 0.5 scale**-2)\n  ```\n\n  For more details, see [Rayleigh distribution](\n  https://en.wikipedia.org/wiki/Rayleigh_distribution)\n\n  Args:\n    shape: Vector-shaped, `int` `Tensor` representing shape of output.\n    scale: (Optional) Positive `float` `Tensor` representing `Rayleigh` scale.\n      Default value: `None` (i.e., `scale = 1.`).\n    dtype: (Optional) TF `dtype` representing `dtype` of output.\n      Default value: `tf.float32`.\n    seed: (Optional) Python integer to seed the random number generator.\n      Default value: `None` (i.e., no seed).\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'random_rayleigh').\n\n  Returns:\n    rayleigh: `Tensor` with specified `shape` and `dtype` consisting of positive\n      real values drawn from a Rayleigh distribution with specified `scale`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'random_rayleigh', [shape, scale, seed]):\n    if scale is not None:\n      # Its important to expand the shape to match scale's, otherwise we won't\n      # have independent draws.\n      scale = tf.convert_to_tensor(value=scale, dtype=dtype, name='scale')\n      shape = tf.broadcast_dynamic_shape(shape, tf.shape(input=scale))\n    x = tf.sqrt(-2. * tf.math.log(\n        tf.random.uniform(shape, minval=0, maxval=1, dtype=dtype, seed=seed)))\n    if scale is None:\n      return x\n    return x * scale", "language": "python", "code": "def random_rayleigh(shape, scale=None, dtype=tf.float32, seed=None, name=None):\n  \"\"\"Generates `Tensor` of positive reals drawn from a Rayleigh distributions.\n\n  The probability density function of a Rayleigh distribution with `scale`\n  parameter is given by:\n\n  ```none\n  f(x) = x scale**-2 exp(-x**2 0.5 scale**-2)\n  ```\n\n  For more details, see [Rayleigh distribution](\n  https://en.wikipedia.org/wiki/Rayleigh_distribution)\n\n  Args:\n    shape: Vector-shaped, `int` `Tensor` representing shape of output.\n    scale: (Optional) Positive `float` `Tensor` representing `Rayleigh` scale.\n      Default value: `None` (i.e., `scale = 1.`).\n    dtype: (Optional) TF `dtype` representing `dtype` of output.\n      Default value: `tf.float32`.\n    seed: (Optional) Python integer to seed the random number generator.\n      Default value: `None` (i.e., no seed).\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'random_rayleigh').\n\n  Returns:\n    rayleigh: `Tensor` with specified `shape` and `dtype` consisting of positive\n      real values drawn from a Rayleigh distribution with specified `scale`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'random_rayleigh', [shape, scale, seed]):\n    if scale is not None:\n      # Its important to expand the shape to match scale's, otherwise we won't\n      # have independent draws.\n      scale = tf.convert_to_tensor(value=scale, dtype=dtype, name='scale')\n      shape = tf.broadcast_dynamic_shape(shape, tf.shape(input=scale))\n    x = tf.sqrt(-2. * tf.math.log(\n        tf.random.uniform(shape, minval=0, maxval=1, dtype=dtype, seed=seed)))\n    if scale is None:\n      return x\n    return x * scale", "code_tokens": ["def", "random_rayleigh", "(", "shape", ",", "scale", "=", "None", ",", "dtype", "=", "tf", ".", "float32", ",", "seed", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'random_rayleigh'", ",", "[", "shape", ",", "scale", ",", "seed", "]", ")", ":", "if", "scale", "is", "not", "None", ":", "# Its important to expand the shape to match scale's, otherwise we won't", "# have independent draws.", "scale", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "scale", ",", "dtype", "=", "dtype", ",", "name", "=", "'scale'", ")", "shape", "=", "tf", ".", "broadcast_dynamic_shape", "(", "shape", ",", "tf", ".", "shape", "(", "input", "=", "scale", ")", ")", "x", "=", "tf", ".", "sqrt", "(", "-", "2.", "*", "tf", ".", "math", ".", "log", "(", "tf", ".", "random", ".", "uniform", "(", "shape", ",", "minval", "=", "0", ",", "maxval", "=", "1", ",", "dtype", "=", "dtype", ",", "seed", "=", "seed", ")", ")", ")", "if", "scale", "is", "None", ":", "return", "x", "return", "x", "*", "scale"], "docstring": "Generates `Tensor` of positive reals drawn from a Rayleigh distributions.\n\n  The probability density function of a Rayleigh distribution with `scale`\n  parameter is given by:\n\n  ```none\n  f(x) = x scale**-2 exp(-x**2 0.5 scale**-2)\n  ```\n\n  For more details, see [Rayleigh distribution](\n  https://en.wikipedia.org/wiki/Rayleigh_distribution)\n\n  Args:\n    shape: Vector-shaped, `int` `Tensor` representing shape of output.\n    scale: (Optional) Positive `float` `Tensor` representing `Rayleigh` scale.\n      Default value: `None` (i.e., `scale = 1.`).\n    dtype: (Optional) TF `dtype` representing `dtype` of output.\n      Default value: `tf.float32`.\n    seed: (Optional) Python integer to seed the random number generator.\n      Default value: `None` (i.e., no seed).\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'random_rayleigh').\n\n  Returns:\n    rayleigh: `Tensor` with specified `shape` and `dtype` consisting of positive\n      real values drawn from a Rayleigh distribution with specified `scale`.", "docstring_tokens": ["Generates", "Tensor", "of", "positive", "reals", "drawn", "from", "a", "Rayleigh", "distributions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/random_ops.py#L61-L99", "partition": "test", "index": 880, "time": "2018-06-26 08:55:47"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/transpose.py", "func_name": "_maybe_validate_rightmost_transposed_ndims", "original_string": "def _maybe_validate_rightmost_transposed_ndims(\n    rightmost_transposed_ndims, validate_args, name=None):\n  \"\"\"Checks that `rightmost_transposed_ndims` is valid.\"\"\"\n  with tf.name_scope(name or 'maybe_validate_rightmost_transposed_ndims'):\n    assertions = []\n    if not dtype_util.is_integer(rightmost_transposed_ndims.dtype):\n      raise TypeError('`rightmost_transposed_ndims` must be integer type.')\n\n    if tensorshape_util.rank(rightmost_transposed_ndims.shape) is not None:\n      if tensorshape_util.rank(rightmost_transposed_ndims.shape) != 0:\n        raise ValueError('`rightmost_transposed_ndims` must be a scalar, '\n                         'saw rank: {}.'.format(\n                             tensorshape_util.rank(\n                                 rightmost_transposed_ndims.shape)))\n    elif validate_args:\n      assertions += [assert_util.assert_rank(rightmost_transposed_ndims, 0)]\n\n    rightmost_transposed_ndims_ = tf.get_static_value(\n        rightmost_transposed_ndims)\n    msg = '`rightmost_transposed_ndims` must be non-negative.'\n    if rightmost_transposed_ndims_ is not None:\n      if rightmost_transposed_ndims_ < 0:\n        raise ValueError(msg[:-1] + ', saw: {}.'.format(\n            rightmost_transposed_ndims_))\n    elif validate_args:\n      assertions += [\n          assert_util.assert_non_negative(\n              rightmost_transposed_ndims, message=msg)\n      ]\n\n    return assertions", "language": "python", "code": "def _maybe_validate_rightmost_transposed_ndims(\n    rightmost_transposed_ndims, validate_args, name=None):\n  \"\"\"Checks that `rightmost_transposed_ndims` is valid.\"\"\"\n  with tf.name_scope(name or 'maybe_validate_rightmost_transposed_ndims'):\n    assertions = []\n    if not dtype_util.is_integer(rightmost_transposed_ndims.dtype):\n      raise TypeError('`rightmost_transposed_ndims` must be integer type.')\n\n    if tensorshape_util.rank(rightmost_transposed_ndims.shape) is not None:\n      if tensorshape_util.rank(rightmost_transposed_ndims.shape) != 0:\n        raise ValueError('`rightmost_transposed_ndims` must be a scalar, '\n                         'saw rank: {}.'.format(\n                             tensorshape_util.rank(\n                                 rightmost_transposed_ndims.shape)))\n    elif validate_args:\n      assertions += [assert_util.assert_rank(rightmost_transposed_ndims, 0)]\n\n    rightmost_transposed_ndims_ = tf.get_static_value(\n        rightmost_transposed_ndims)\n    msg = '`rightmost_transposed_ndims` must be non-negative.'\n    if rightmost_transposed_ndims_ is not None:\n      if rightmost_transposed_ndims_ < 0:\n        raise ValueError(msg[:-1] + ', saw: {}.'.format(\n            rightmost_transposed_ndims_))\n    elif validate_args:\n      assertions += [\n          assert_util.assert_non_negative(\n              rightmost_transposed_ndims, message=msg)\n      ]\n\n    return assertions", "code_tokens": ["def", "_maybe_validate_rightmost_transposed_ndims", "(", "rightmost_transposed_ndims", ",", "validate_args", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "'maybe_validate_rightmost_transposed_ndims'", ")", ":", "assertions", "=", "[", "]", "if", "not", "dtype_util", ".", "is_integer", "(", "rightmost_transposed_ndims", ".", "dtype", ")", ":", "raise", "TypeError", "(", "'`rightmost_transposed_ndims` must be integer type.'", ")", "if", "tensorshape_util", ".", "rank", "(", "rightmost_transposed_ndims", ".", "shape", ")", "is", "not", "None", ":", "if", "tensorshape_util", ".", "rank", "(", "rightmost_transposed_ndims", ".", "shape", ")", "!=", "0", ":", "raise", "ValueError", "(", "'`rightmost_transposed_ndims` must be a scalar, '", "'saw rank: {}.'", ".", "format", "(", "tensorshape_util", ".", "rank", "(", "rightmost_transposed_ndims", ".", "shape", ")", ")", ")", "elif", "validate_args", ":", "assertions", "+=", "[", "assert_util", ".", "assert_rank", "(", "rightmost_transposed_ndims", ",", "0", ")", "]", "rightmost_transposed_ndims_", "=", "tf", ".", "get_static_value", "(", "rightmost_transposed_ndims", ")", "msg", "=", "'`rightmost_transposed_ndims` must be non-negative.'", "if", "rightmost_transposed_ndims_", "is", "not", "None", ":", "if", "rightmost_transposed_ndims_", "<", "0", ":", "raise", "ValueError", "(", "msg", "[", ":", "-", "1", "]", "+", "', saw: {}.'", ".", "format", "(", "rightmost_transposed_ndims_", ")", ")", "elif", "validate_args", ":", "assertions", "+=", "[", "assert_util", ".", "assert_non_negative", "(", "rightmost_transposed_ndims", ",", "message", "=", "msg", ")", "]", "return", "assertions"], "docstring": "Checks that `rightmost_transposed_ndims` is valid.", "docstring_tokens": ["Checks", "that", "rightmost_transposed_ndims", "is", "valid", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/transpose.py#L258-L288", "partition": "test", "index": 931, "time": "2018-06-26 12:24:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/transpose.py", "func_name": "_maybe_validate_perm", "original_string": "def _maybe_validate_perm(perm, validate_args, name=None):\n  \"\"\"Checks that `perm` is valid.\"\"\"\n  with tf.name_scope(name or 'maybe_validate_perm'):\n    assertions = []\n    if not dtype_util.is_integer(perm.dtype):\n      raise TypeError('`perm` must be integer type')\n\n    msg = '`perm` must be a vector.'\n    if tensorshape_util.rank(perm.shape) is not None:\n      if tensorshape_util.rank(perm.shape) != 1:\n        raise ValueError(\n            msg[:-1] +\n            ', saw rank: {}.'.format(tensorshape_util.rank(perm.shape)))\n    elif validate_args:\n      assertions += [assert_util.assert_rank(perm, 1, message=msg)]\n\n    perm_ = tf.get_static_value(perm)\n    msg = '`perm` must be a valid permutation vector.'\n    if perm_ is not None:\n      if not np.all(np.arange(np.size(perm_)) == np.sort(perm_)):\n        raise ValueError(msg[:-1] + ', saw: {}.'.format(perm_))\n    elif validate_args:\n      assertions += [\n          assert_util.assert_equal(\n              tf.sort(perm), tf.range(tf.size(input=perm)), message=msg)\n      ]\n\n    return assertions", "language": "python", "code": "def _maybe_validate_perm(perm, validate_args, name=None):\n  \"\"\"Checks that `perm` is valid.\"\"\"\n  with tf.name_scope(name or 'maybe_validate_perm'):\n    assertions = []\n    if not dtype_util.is_integer(perm.dtype):\n      raise TypeError('`perm` must be integer type')\n\n    msg = '`perm` must be a vector.'\n    if tensorshape_util.rank(perm.shape) is not None:\n      if tensorshape_util.rank(perm.shape) != 1:\n        raise ValueError(\n            msg[:-1] +\n            ', saw rank: {}.'.format(tensorshape_util.rank(perm.shape)))\n    elif validate_args:\n      assertions += [assert_util.assert_rank(perm, 1, message=msg)]\n\n    perm_ = tf.get_static_value(perm)\n    msg = '`perm` must be a valid permutation vector.'\n    if perm_ is not None:\n      if not np.all(np.arange(np.size(perm_)) == np.sort(perm_)):\n        raise ValueError(msg[:-1] + ', saw: {}.'.format(perm_))\n    elif validate_args:\n      assertions += [\n          assert_util.assert_equal(\n              tf.sort(perm), tf.range(tf.size(input=perm)), message=msg)\n      ]\n\n    return assertions", "code_tokens": ["def", "_maybe_validate_perm", "(", "perm", ",", "validate_args", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "'maybe_validate_perm'", ")", ":", "assertions", "=", "[", "]", "if", "not", "dtype_util", ".", "is_integer", "(", "perm", ".", "dtype", ")", ":", "raise", "TypeError", "(", "'`perm` must be integer type'", ")", "msg", "=", "'`perm` must be a vector.'", "if", "tensorshape_util", ".", "rank", "(", "perm", ".", "shape", ")", "is", "not", "None", ":", "if", "tensorshape_util", ".", "rank", "(", "perm", ".", "shape", ")", "!=", "1", ":", "raise", "ValueError", "(", "msg", "[", ":", "-", "1", "]", "+", "', saw rank: {}.'", ".", "format", "(", "tensorshape_util", ".", "rank", "(", "perm", ".", "shape", ")", ")", ")", "elif", "validate_args", ":", "assertions", "+=", "[", "assert_util", ".", "assert_rank", "(", "perm", ",", "1", ",", "message", "=", "msg", ")", "]", "perm_", "=", "tf", ".", "get_static_value", "(", "perm", ")", "msg", "=", "'`perm` must be a valid permutation vector.'", "if", "perm_", "is", "not", "None", ":", "if", "not", "np", ".", "all", "(", "np", ".", "arange", "(", "np", ".", "size", "(", "perm_", ")", ")", "==", "np", ".", "sort", "(", "perm_", ")", ")", ":", "raise", "ValueError", "(", "msg", "[", ":", "-", "1", "]", "+", "', saw: {}.'", ".", "format", "(", "perm_", ")", ")", "elif", "validate_args", ":", "assertions", "+=", "[", "assert_util", ".", "assert_equal", "(", "tf", ".", "sort", "(", "perm", ")", ",", "tf", ".", "range", "(", "tf", ".", "size", "(", "input", "=", "perm", ")", ")", ",", "message", "=", "msg", ")", "]", "return", "assertions"], "docstring": "Checks that `perm` is valid.", "docstring_tokens": ["Checks", "that", "perm", "is", "valid", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/transpose.py#L291-L318", "partition": "test", "index": 932, "time": "2018-06-26 12:24:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/reshape.py", "func_name": "_replace_event_shape_in_tensorshape", "original_string": "def _replace_event_shape_in_tensorshape(\n    input_tensorshape, event_shape_in, event_shape_out):\n  \"\"\"Replaces the event shape dims of a `TensorShape`.\n\n  Args:\n    input_tensorshape: a `TensorShape` instance in which to attempt replacing\n      event shape.\n    event_shape_in: `Tensor` shape representing the event shape expected to\n      be present in (rightmost dims of) `tensorshape_in`. Must be compatible\n      with the rightmost dims of `tensorshape_in`.\n    event_shape_out: `Tensor` shape representing the new event shape, i.e.,\n      the replacement of `event_shape_in`,\n\n  Returns:\n    output_tensorshape: `TensorShape` with the rightmost `event_shape_in`\n      replaced by `event_shape_out`. Might be partially defined, i.e.,\n      `TensorShape(None)`.\n    is_validated: Python `bool` indicating static validation happened.\n\n  Raises:\n    ValueError: if we can determine the event shape portion of\n      `tensorshape_in` as well as `event_shape_in` both statically, and they\n      are not compatible. \"Compatible\" here means that they are identical on\n      any dims that are not -1 in `event_shape_in`.\n  \"\"\"\n  event_shape_in_ndims = tensorshape_util.num_elements(event_shape_in.shape)\n  if tensorshape_util.rank(\n      input_tensorshape) is None or event_shape_in_ndims is None:\n    return tf.TensorShape(None), False  # Not is_validated.\n\n  input_non_event_ndims = tensorshape_util.rank(\n      input_tensorshape) - event_shape_in_ndims\n  if input_non_event_ndims < 0:\n    raise ValueError(\n        'Input has fewer ndims ({}) than event shape ndims ({}).'.format(\n            tensorshape_util.rank(input_tensorshape), event_shape_in_ndims))\n\n  input_non_event_tensorshape = input_tensorshape[:input_non_event_ndims]\n  input_event_tensorshape = input_tensorshape[input_non_event_ndims:]\n\n  # Check that `input_event_shape_` and `event_shape_in` are compatible in the\n  # sense that they have equal entries in any position that isn't a `-1` in\n  # `event_shape_in`. Note that our validations at construction time ensure\n  # there is at most one such entry in `event_shape_in`.\n  event_shape_in_ = tf.get_static_value(event_shape_in)\n  is_validated = (\n      tensorshape_util.is_fully_defined(input_event_tensorshape) and\n      event_shape_in_ is not None)\n  if is_validated:\n    input_event_shape_ = np.int32(input_event_tensorshape)\n    mask = event_shape_in_ >= 0\n    explicit_input_event_shape_ = input_event_shape_[mask]\n    explicit_event_shape_in_ = event_shape_in_[mask]\n    if not all(explicit_input_event_shape_ == explicit_event_shape_in_):\n      raise ValueError(\n          'Input `event_shape` does not match `event_shape_in`. '\n          '({} vs {}).'.format(input_event_shape_, event_shape_in_))\n\n  event_tensorshape_out = tensorshape_util.constant_value_as_shape(\n      event_shape_out)\n  if tensorshape_util.rank(event_tensorshape_out) is None:\n    output_tensorshape = tf.TensorShape(None)\n  else:\n    output_tensorshape = tensorshape_util.concatenate(\n        input_non_event_tensorshape, event_tensorshape_out)\n\n  return output_tensorshape, is_validated", "language": "python", "code": "def _replace_event_shape_in_tensorshape(\n    input_tensorshape, event_shape_in, event_shape_out):\n  \"\"\"Replaces the event shape dims of a `TensorShape`.\n\n  Args:\n    input_tensorshape: a `TensorShape` instance in which to attempt replacing\n      event shape.\n    event_shape_in: `Tensor` shape representing the event shape expected to\n      be present in (rightmost dims of) `tensorshape_in`. Must be compatible\n      with the rightmost dims of `tensorshape_in`.\n    event_shape_out: `Tensor` shape representing the new event shape, i.e.,\n      the replacement of `event_shape_in`,\n\n  Returns:\n    output_tensorshape: `TensorShape` with the rightmost `event_shape_in`\n      replaced by `event_shape_out`. Might be partially defined, i.e.,\n      `TensorShape(None)`.\n    is_validated: Python `bool` indicating static validation happened.\n\n  Raises:\n    ValueError: if we can determine the event shape portion of\n      `tensorshape_in` as well as `event_shape_in` both statically, and they\n      are not compatible. \"Compatible\" here means that they are identical on\n      any dims that are not -1 in `event_shape_in`.\n  \"\"\"\n  event_shape_in_ndims = tensorshape_util.num_elements(event_shape_in.shape)\n  if tensorshape_util.rank(\n      input_tensorshape) is None or event_shape_in_ndims is None:\n    return tf.TensorShape(None), False  # Not is_validated.\n\n  input_non_event_ndims = tensorshape_util.rank(\n      input_tensorshape) - event_shape_in_ndims\n  if input_non_event_ndims < 0:\n    raise ValueError(\n        'Input has fewer ndims ({}) than event shape ndims ({}).'.format(\n            tensorshape_util.rank(input_tensorshape), event_shape_in_ndims))\n\n  input_non_event_tensorshape = input_tensorshape[:input_non_event_ndims]\n  input_event_tensorshape = input_tensorshape[input_non_event_ndims:]\n\n  # Check that `input_event_shape_` and `event_shape_in` are compatible in the\n  # sense that they have equal entries in any position that isn't a `-1` in\n  # `event_shape_in`. Note that our validations at construction time ensure\n  # there is at most one such entry in `event_shape_in`.\n  event_shape_in_ = tf.get_static_value(event_shape_in)\n  is_validated = (\n      tensorshape_util.is_fully_defined(input_event_tensorshape) and\n      event_shape_in_ is not None)\n  if is_validated:\n    input_event_shape_ = np.int32(input_event_tensorshape)\n    mask = event_shape_in_ >= 0\n    explicit_input_event_shape_ = input_event_shape_[mask]\n    explicit_event_shape_in_ = event_shape_in_[mask]\n    if not all(explicit_input_event_shape_ == explicit_event_shape_in_):\n      raise ValueError(\n          'Input `event_shape` does not match `event_shape_in`. '\n          '({} vs {}).'.format(input_event_shape_, event_shape_in_))\n\n  event_tensorshape_out = tensorshape_util.constant_value_as_shape(\n      event_shape_out)\n  if tensorshape_util.rank(event_tensorshape_out) is None:\n    output_tensorshape = tf.TensorShape(None)\n  else:\n    output_tensorshape = tensorshape_util.concatenate(\n        input_non_event_tensorshape, event_tensorshape_out)\n\n  return output_tensorshape, is_validated", "code_tokens": ["def", "_replace_event_shape_in_tensorshape", "(", "input_tensorshape", ",", "event_shape_in", ",", "event_shape_out", ")", ":", "event_shape_in_ndims", "=", "tensorshape_util", ".", "num_elements", "(", "event_shape_in", ".", "shape", ")", "if", "tensorshape_util", ".", "rank", "(", "input_tensorshape", ")", "is", "None", "or", "event_shape_in_ndims", "is", "None", ":", "return", "tf", ".", "TensorShape", "(", "None", ")", ",", "False", "# Not is_validated.", "input_non_event_ndims", "=", "tensorshape_util", ".", "rank", "(", "input_tensorshape", ")", "-", "event_shape_in_ndims", "if", "input_non_event_ndims", "<", "0", ":", "raise", "ValueError", "(", "'Input has fewer ndims ({}) than event shape ndims ({}).'", ".", "format", "(", "tensorshape_util", ".", "rank", "(", "input_tensorshape", ")", ",", "event_shape_in_ndims", ")", ")", "input_non_event_tensorshape", "=", "input_tensorshape", "[", ":", "input_non_event_ndims", "]", "input_event_tensorshape", "=", "input_tensorshape", "[", "input_non_event_ndims", ":", "]", "# Check that `input_event_shape_` and `event_shape_in` are compatible in the", "# sense that they have equal entries in any position that isn't a `-1` in", "# `event_shape_in`. Note that our validations at construction time ensure", "# there is at most one such entry in `event_shape_in`.", "event_shape_in_", "=", "tf", ".", "get_static_value", "(", "event_shape_in", ")", "is_validated", "=", "(", "tensorshape_util", ".", "is_fully_defined", "(", "input_event_tensorshape", ")", "and", "event_shape_in_", "is", "not", "None", ")", "if", "is_validated", ":", "input_event_shape_", "=", "np", ".", "int32", "(", "input_event_tensorshape", ")", "mask", "=", "event_shape_in_", ">=", "0", "explicit_input_event_shape_", "=", "input_event_shape_", "[", "mask", "]", "explicit_event_shape_in_", "=", "event_shape_in_", "[", "mask", "]", "if", "not", "all", "(", "explicit_input_event_shape_", "==", "explicit_event_shape_in_", ")", ":", "raise", "ValueError", "(", "'Input `event_shape` does not match `event_shape_in`. '", "'({} vs {}).'", ".", "format", "(", "input_event_shape_", ",", "event_shape_in_", ")", ")", "event_tensorshape_out", "=", "tensorshape_util", ".", "constant_value_as_shape", "(", "event_shape_out", ")", "if", "tensorshape_util", ".", "rank", "(", "event_tensorshape_out", ")", "is", "None", ":", "output_tensorshape", "=", "tf", ".", "TensorShape", "(", "None", ")", "else", ":", "output_tensorshape", "=", "tensorshape_util", ".", "concatenate", "(", "input_non_event_tensorshape", ",", "event_tensorshape_out", ")", "return", "output_tensorshape", ",", "is_validated"], "docstring": "Replaces the event shape dims of a `TensorShape`.\n\n  Args:\n    input_tensorshape: a `TensorShape` instance in which to attempt replacing\n      event shape.\n    event_shape_in: `Tensor` shape representing the event shape expected to\n      be present in (rightmost dims of) `tensorshape_in`. Must be compatible\n      with the rightmost dims of `tensorshape_in`.\n    event_shape_out: `Tensor` shape representing the new event shape, i.e.,\n      the replacement of `event_shape_in`,\n\n  Returns:\n    output_tensorshape: `TensorShape` with the rightmost `event_shape_in`\n      replaced by `event_shape_out`. Might be partially defined, i.e.,\n      `TensorShape(None)`.\n    is_validated: Python `bool` indicating static validation happened.\n\n  Raises:\n    ValueError: if we can determine the event shape portion of\n      `tensorshape_in` as well as `event_shape_in` both statically, and they\n      are not compatible. \"Compatible\" here means that they are identical on\n      any dims that are not -1 in `event_shape_in`.", "docstring_tokens": ["Replaces", "the", "event", "shape", "dims", "of", "a", "TensorShape", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/reshape.py#L316-L382", "partition": "test", "index": 976, "time": "2018-06-28 10:16:59"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/latent_dirichlet_allocation_distributions.py", "func_name": "make_encoder", "original_string": "def make_encoder(activation, num_topics, layer_sizes):\n  \"\"\"Create the encoder function.\n\n  Args:\n    activation: Activation function to use.\n    num_topics: The number of topics.\n    layer_sizes: The number of hidden units per layer in the encoder.\n\n  Returns:\n    encoder: A `callable` mapping a bag-of-words `Tensor` to a\n      `tfd.Distribution` instance over topics.\n  \"\"\"\n  encoder_net = tf.keras.Sequential()\n  for num_hidden_units in layer_sizes:\n    encoder_net.add(\n        tf.keras.layers.Dense(\n            num_hidden_units,\n            activation=activation,\n            kernel_initializer=tf.compat.v1.glorot_normal_initializer()))\n  encoder_net.add(\n      tf.keras.layers.Dense(\n          num_topics,\n          activation=tf.nn.softplus,\n          kernel_initializer=tf.compat.v1.glorot_normal_initializer()))\n\n  def encoder(bag_of_words):\n    net = _clip_dirichlet_parameters(encoder_net(bag_of_words))\n    return tfd.Dirichlet(concentration=net,\n                         name=\"topics_posterior\")\n\n  return encoder", "language": "python", "code": "def make_encoder(activation, num_topics, layer_sizes):\n  \"\"\"Create the encoder function.\n\n  Args:\n    activation: Activation function to use.\n    num_topics: The number of topics.\n    layer_sizes: The number of hidden units per layer in the encoder.\n\n  Returns:\n    encoder: A `callable` mapping a bag-of-words `Tensor` to a\n      `tfd.Distribution` instance over topics.\n  \"\"\"\n  encoder_net = tf.keras.Sequential()\n  for num_hidden_units in layer_sizes:\n    encoder_net.add(\n        tf.keras.layers.Dense(\n            num_hidden_units,\n            activation=activation,\n            kernel_initializer=tf.compat.v1.glorot_normal_initializer()))\n  encoder_net.add(\n      tf.keras.layers.Dense(\n          num_topics,\n          activation=tf.nn.softplus,\n          kernel_initializer=tf.compat.v1.glorot_normal_initializer()))\n\n  def encoder(bag_of_words):\n    net = _clip_dirichlet_parameters(encoder_net(bag_of_words))\n    return tfd.Dirichlet(concentration=net,\n                         name=\"topics_posterior\")\n\n  return encoder", "code_tokens": ["def", "make_encoder", "(", "activation", ",", "num_topics", ",", "layer_sizes", ")", ":", "encoder_net", "=", "tf", ".", "keras", ".", "Sequential", "(", ")", "for", "num_hidden_units", "in", "layer_sizes", ":", "encoder_net", ".", "add", "(", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "num_hidden_units", ",", "activation", "=", "activation", ",", "kernel_initializer", "=", "tf", ".", "compat", ".", "v1", ".", "glorot_normal_initializer", "(", ")", ")", ")", "encoder_net", ".", "add", "(", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "num_topics", ",", "activation", "=", "tf", ".", "nn", ".", "softplus", ",", "kernel_initializer", "=", "tf", ".", "compat", ".", "v1", ".", "glorot_normal_initializer", "(", ")", ")", ")", "def", "encoder", "(", "bag_of_words", ")", ":", "net", "=", "_clip_dirichlet_parameters", "(", "encoder_net", "(", "bag_of_words", ")", ")", "return", "tfd", ".", "Dirichlet", "(", "concentration", "=", "net", ",", "name", "=", "\"topics_posterior\"", ")", "return", "encoder"], "docstring": "Create the encoder function.\n\n  Args:\n    activation: Activation function to use.\n    num_topics: The number of topics.\n    layer_sizes: The number of hidden units per layer in the encoder.\n\n  Returns:\n    encoder: A `callable` mapping a bag-of-words `Tensor` to a\n      `tfd.Distribution` instance over topics.", "docstring_tokens": ["Create", "the", "encoder", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/latent_dirichlet_allocation_distributions.py#L164-L194", "partition": "test", "index": 1012, "time": "2018-07-02 01:57:53"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/latent_dirichlet_allocation_edward2.py", "func_name": "make_lda_variational", "original_string": "def make_lda_variational(activation, num_topics, layer_sizes):\n  \"\"\"Creates the variational distribution for LDA.\n\n  Args:\n    activation: Activation function to use.\n    num_topics: The number of topics.\n    layer_sizes: The number of hidden units per layer in the encoder.\n\n  Returns:\n    lda_variational: A function that takes a bag-of-words Tensor as\n      input and returns a distribution over topics.\n  \"\"\"\n  encoder_net = tf.keras.Sequential()\n  for num_hidden_units in layer_sizes:\n    encoder_net.add(\n        tf.keras.layers.Dense(\n            num_hidden_units,\n            activation=activation,\n            kernel_initializer=tf.compat.v1.glorot_normal_initializer()))\n  encoder_net.add(\n      tf.keras.layers.Dense(\n          num_topics,\n          activation=tf.nn.softplus,\n          kernel_initializer=tf.compat.v1.glorot_normal_initializer()))\n\n  def lda_variational(bag_of_words):\n    concentration = _clip_dirichlet_parameters(encoder_net(bag_of_words))\n    return ed.Dirichlet(concentration=concentration, name=\"topics_posterior\")\n\n  return lda_variational", "language": "python", "code": "def make_lda_variational(activation, num_topics, layer_sizes):\n  \"\"\"Creates the variational distribution for LDA.\n\n  Args:\n    activation: Activation function to use.\n    num_topics: The number of topics.\n    layer_sizes: The number of hidden units per layer in the encoder.\n\n  Returns:\n    lda_variational: A function that takes a bag-of-words Tensor as\n      input and returns a distribution over topics.\n  \"\"\"\n  encoder_net = tf.keras.Sequential()\n  for num_hidden_units in layer_sizes:\n    encoder_net.add(\n        tf.keras.layers.Dense(\n            num_hidden_units,\n            activation=activation,\n            kernel_initializer=tf.compat.v1.glorot_normal_initializer()))\n  encoder_net.add(\n      tf.keras.layers.Dense(\n          num_topics,\n          activation=tf.nn.softplus,\n          kernel_initializer=tf.compat.v1.glorot_normal_initializer()))\n\n  def lda_variational(bag_of_words):\n    concentration = _clip_dirichlet_parameters(encoder_net(bag_of_words))\n    return ed.Dirichlet(concentration=concentration, name=\"topics_posterior\")\n\n  return lda_variational", "code_tokens": ["def", "make_lda_variational", "(", "activation", ",", "num_topics", ",", "layer_sizes", ")", ":", "encoder_net", "=", "tf", ".", "keras", ".", "Sequential", "(", ")", "for", "num_hidden_units", "in", "layer_sizes", ":", "encoder_net", ".", "add", "(", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "num_hidden_units", ",", "activation", "=", "activation", ",", "kernel_initializer", "=", "tf", ".", "compat", ".", "v1", ".", "glorot_normal_initializer", "(", ")", ")", ")", "encoder_net", ".", "add", "(", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "num_topics", ",", "activation", "=", "tf", ".", "nn", ".", "softplus", ",", "kernel_initializer", "=", "tf", ".", "compat", ".", "v1", ".", "glorot_normal_initializer", "(", ")", ")", ")", "def", "lda_variational", "(", "bag_of_words", ")", ":", "concentration", "=", "_clip_dirichlet_parameters", "(", "encoder_net", "(", "bag_of_words", ")", ")", "return", "ed", ".", "Dirichlet", "(", "concentration", "=", "concentration", ",", "name", "=", "\"topics_posterior\"", ")", "return", "lda_variational"], "docstring": "Creates the variational distribution for LDA.\n\n  Args:\n    activation: Activation function to use.\n    num_topics: The number of topics.\n    layer_sizes: The number of hidden units per layer in the encoder.\n\n  Returns:\n    lda_variational: A function that takes a bag-of-words Tensor as\n      input and returns a distribution over topics.", "docstring_tokens": ["Creates", "the", "variational", "distribution", "for", "LDA", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/latent_dirichlet_allocation_edward2.py#L194-L223", "partition": "test", "index": 839, "time": "2018-07-02 01:57:53"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/latent_dirichlet_allocation_edward2.py", "func_name": "get_topics_strings", "original_string": "def get_topics_strings(topics_words, alpha, vocabulary,\n                       topics_to_print=10, words_per_topic=10):\n  \"\"\"Returns the summary of the learned topics.\n\n  Arguments:\n    topics_words: KxV tensor with topics as rows and words as columns.\n    alpha: 1xK tensor of prior Dirichlet concentrations for the\n        topics.\n    vocabulary: A mapping of word's integer index to the corresponding string.\n    topics_to_print: The number of topics with highest prior weight to\n        summarize.\n    words_per_topic: Number of wodrs per topic to return.\n\n  Returns:\n    summary: A np.array with strings.\n  \"\"\"\n  alpha = np.squeeze(alpha, axis=0)\n  # Use a stable sorting algorithm so that when alpha is fixed\n  # we always get the same topics.\n  highest_weight_topics = np.argsort(-alpha, kind=\"mergesort\")\n  top_words = np.argsort(-topics_words, axis=1)\n\n  res = []\n  for topic_idx in highest_weight_topics[:topics_to_print]:\n    l = [\"index={} alpha={:.2f}\".format(topic_idx, alpha[topic_idx])]\n    l += [vocabulary[word] for word in top_words[topic_idx, :words_per_topic]]\n    res.append(\" \".join(l))\n\n  return np.array(res)", "language": "python", "code": "def get_topics_strings(topics_words, alpha, vocabulary,\n                       topics_to_print=10, words_per_topic=10):\n  \"\"\"Returns the summary of the learned topics.\n\n  Arguments:\n    topics_words: KxV tensor with topics as rows and words as columns.\n    alpha: 1xK tensor of prior Dirichlet concentrations for the\n        topics.\n    vocabulary: A mapping of word's integer index to the corresponding string.\n    topics_to_print: The number of topics with highest prior weight to\n        summarize.\n    words_per_topic: Number of wodrs per topic to return.\n\n  Returns:\n    summary: A np.array with strings.\n  \"\"\"\n  alpha = np.squeeze(alpha, axis=0)\n  # Use a stable sorting algorithm so that when alpha is fixed\n  # we always get the same topics.\n  highest_weight_topics = np.argsort(-alpha, kind=\"mergesort\")\n  top_words = np.argsort(-topics_words, axis=1)\n\n  res = []\n  for topic_idx in highest_weight_topics[:topics_to_print]:\n    l = [\"index={} alpha={:.2f}\".format(topic_idx, alpha[topic_idx])]\n    l += [vocabulary[word] for word in top_words[topic_idx, :words_per_topic]]\n    res.append(\" \".join(l))\n\n  return np.array(res)", "code_tokens": ["def", "get_topics_strings", "(", "topics_words", ",", "alpha", ",", "vocabulary", ",", "topics_to_print", "=", "10", ",", "words_per_topic", "=", "10", ")", ":", "alpha", "=", "np", ".", "squeeze", "(", "alpha", ",", "axis", "=", "0", ")", "# Use a stable sorting algorithm so that when alpha is fixed", "# we always get the same topics.", "highest_weight_topics", "=", "np", ".", "argsort", "(", "-", "alpha", ",", "kind", "=", "\"mergesort\"", ")", "top_words", "=", "np", ".", "argsort", "(", "-", "topics_words", ",", "axis", "=", "1", ")", "res", "=", "[", "]", "for", "topic_idx", "in", "highest_weight_topics", "[", ":", "topics_to_print", "]", ":", "l", "=", "[", "\"index={} alpha={:.2f}\"", ".", "format", "(", "topic_idx", ",", "alpha", "[", "topic_idx", "]", ")", "]", "l", "+=", "[", "vocabulary", "[", "word", "]", "for", "word", "in", "top_words", "[", "topic_idx", ",", ":", "words_per_topic", "]", "]", "res", ".", "append", "(", "\" \"", ".", "join", "(", "l", ")", ")", "return", "np", ".", "array", "(", "res", ")"], "docstring": "Returns the summary of the learned topics.\n\n  Arguments:\n    topics_words: KxV tensor with topics as rows and words as columns.\n    alpha: 1xK tensor of prior Dirichlet concentrations for the\n        topics.\n    vocabulary: A mapping of word's integer index to the corresponding string.\n    topics_to_print: The number of topics with highest prior weight to\n        summarize.\n    words_per_topic: Number of wodrs per topic to return.\n\n  Returns:\n    summary: A np.array with strings.", "docstring_tokens": ["Returns", "the", "summary", "of", "the", "learned", "topics", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/latent_dirichlet_allocation_edward2.py#L371-L399", "partition": "test", "index": 840, "time": "2018-07-02 01:57:53"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/latent_dirichlet_allocation_edward2.py", "func_name": "newsgroups_dataset", "original_string": "def newsgroups_dataset(directory, split_name, num_words, shuffle_and_repeat):\n  \"\"\"20 newsgroups as a tf.data.Dataset.\"\"\"\n  data = np.load(download(directory, FILE_TEMPLATE.format(split=split_name)))\n  # The last row is empty in both train and test.\n  data = data[:-1]\n\n  # Each row is a list of word ids in the document. We first convert this to\n  # sparse COO matrix (which automatically sums the repeating words). Then,\n  # we convert this COO matrix to CSR format which allows for fast querying of\n  # documents.\n  num_documents = data.shape[0]\n  indices = np.array([(row_idx, column_idx)\n                      for row_idx, row in enumerate(data)\n                      for column_idx in row])\n  sparse_matrix = scipy.sparse.coo_matrix(\n      (np.ones(indices.shape[0]), (indices[:, 0], indices[:, 1])),\n      shape=(num_documents, num_words),\n      dtype=np.float32)\n  sparse_matrix = sparse_matrix.tocsr()\n\n  dataset = tf.data.Dataset.range(num_documents)\n\n  # For training, we shuffle each epoch and repeat the epochs.\n  if shuffle_and_repeat:\n    dataset = dataset.shuffle(num_documents).repeat()\n\n  # Returns a single document as a dense TensorFlow tensor. The dataset is\n  # stored as a sparse matrix outside of the graph.\n  def get_row_py_func(idx):\n    def get_row_python(idx_py):\n      return np.squeeze(np.array(sparse_matrix[idx_py].todense()), axis=0)\n\n    py_func = tf.compat.v1.py_func(\n        get_row_python, [idx], tf.float32, stateful=False)\n    py_func.set_shape((num_words,))\n    return py_func\n\n  dataset = dataset.map(get_row_py_func)\n  return dataset", "language": "python", "code": "def newsgroups_dataset(directory, split_name, num_words, shuffle_and_repeat):\n  \"\"\"20 newsgroups as a tf.data.Dataset.\"\"\"\n  data = np.load(download(directory, FILE_TEMPLATE.format(split=split_name)))\n  # The last row is empty in both train and test.\n  data = data[:-1]\n\n  # Each row is a list of word ids in the document. We first convert this to\n  # sparse COO matrix (which automatically sums the repeating words). Then,\n  # we convert this COO matrix to CSR format which allows for fast querying of\n  # documents.\n  num_documents = data.shape[0]\n  indices = np.array([(row_idx, column_idx)\n                      for row_idx, row in enumerate(data)\n                      for column_idx in row])\n  sparse_matrix = scipy.sparse.coo_matrix(\n      (np.ones(indices.shape[0]), (indices[:, 0], indices[:, 1])),\n      shape=(num_documents, num_words),\n      dtype=np.float32)\n  sparse_matrix = sparse_matrix.tocsr()\n\n  dataset = tf.data.Dataset.range(num_documents)\n\n  # For training, we shuffle each epoch and repeat the epochs.\n  if shuffle_and_repeat:\n    dataset = dataset.shuffle(num_documents).repeat()\n\n  # Returns a single document as a dense TensorFlow tensor. The dataset is\n  # stored as a sparse matrix outside of the graph.\n  def get_row_py_func(idx):\n    def get_row_python(idx_py):\n      return np.squeeze(np.array(sparse_matrix[idx_py].todense()), axis=0)\n\n    py_func = tf.compat.v1.py_func(\n        get_row_python, [idx], tf.float32, stateful=False)\n    py_func.set_shape((num_words,))\n    return py_func\n\n  dataset = dataset.map(get_row_py_func)\n  return dataset", "code_tokens": ["def", "newsgroups_dataset", "(", "directory", ",", "split_name", ",", "num_words", ",", "shuffle_and_repeat", ")", ":", "data", "=", "np", ".", "load", "(", "download", "(", "directory", ",", "FILE_TEMPLATE", ".", "format", "(", "split", "=", "split_name", ")", ")", ")", "# The last row is empty in both train and test.", "data", "=", "data", "[", ":", "-", "1", "]", "# Each row is a list of word ids in the document. We first convert this to", "# sparse COO matrix (which automatically sums the repeating words). Then,", "# we convert this COO matrix to CSR format which allows for fast querying of", "# documents.", "num_documents", "=", "data", ".", "shape", "[", "0", "]", "indices", "=", "np", ".", "array", "(", "[", "(", "row_idx", ",", "column_idx", ")", "for", "row_idx", ",", "row", "in", "enumerate", "(", "data", ")", "for", "column_idx", "in", "row", "]", ")", "sparse_matrix", "=", "scipy", ".", "sparse", ".", "coo_matrix", "(", "(", "np", ".", "ones", "(", "indices", ".", "shape", "[", "0", "]", ")", ",", "(", "indices", "[", ":", ",", "0", "]", ",", "indices", "[", ":", ",", "1", "]", ")", ")", ",", "shape", "=", "(", "num_documents", ",", "num_words", ")", ",", "dtype", "=", "np", ".", "float32", ")", "sparse_matrix", "=", "sparse_matrix", ".", "tocsr", "(", ")", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "range", "(", "num_documents", ")", "# For training, we shuffle each epoch and repeat the epochs.", "if", "shuffle_and_repeat", ":", "dataset", "=", "dataset", ".", "shuffle", "(", "num_documents", ")", ".", "repeat", "(", ")", "# Returns a single document as a dense TensorFlow tensor. The dataset is", "# stored as a sparse matrix outside of the graph.", "def", "get_row_py_func", "(", "idx", ")", ":", "def", "get_row_python", "(", "idx_py", ")", ":", "return", "np", ".", "squeeze", "(", "np", ".", "array", "(", "sparse_matrix", "[", "idx_py", "]", ".", "todense", "(", ")", ")", ",", "axis", "=", "0", ")", "py_func", "=", "tf", ".", "compat", ".", "v1", ".", "py_func", "(", "get_row_python", ",", "[", "idx", "]", ",", "tf", ".", "float32", ",", "stateful", "=", "False", ")", "py_func", ".", "set_shape", "(", "(", "num_words", ",", ")", ")", "return", "py_func", "dataset", "=", "dataset", ".", "map", "(", "get_row_py_func", ")", "return", "dataset"], "docstring": "20 newsgroups as a tf.data.Dataset.", "docstring_tokens": ["20", "newsgroups", "as", "a", "tf", ".", "data", ".", "Dataset", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/latent_dirichlet_allocation_edward2.py#L419-L457", "partition": "test", "index": 841, "time": "2018-07-02 01:57:53"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/latent_dirichlet_allocation_distributions.py", "func_name": "make_decoder", "original_string": "def make_decoder(num_topics, num_words):\n  \"\"\"Create the decoder function.\n\n  Args:\n    num_topics: The number of topics.\n    num_words: The number of words.\n\n  Returns:\n    decoder: A `callable` mapping a `Tensor` of encodings to a\n      `tfd.Distribution` instance over words.\n  \"\"\"\n  topics_words_logits = tf.compat.v1.get_variable(\n      \"topics_words_logits\",\n      shape=[num_topics, num_words],\n      initializer=tf.compat.v1.glorot_normal_initializer())\n  topics_words = tf.nn.softmax(topics_words_logits, axis=-1)\n\n  def decoder(topics):\n    word_probs = tf.matmul(topics, topics_words)\n    # The observations are bag of words and therefore not one-hot. However,\n    # log_prob of OneHotCategorical computes the probability correctly in\n    # this case.\n    return tfd.OneHotCategorical(probs=word_probs,\n                                 name=\"bag_of_words\")\n\n  return decoder, topics_words", "language": "python", "code": "def make_decoder(num_topics, num_words):\n  \"\"\"Create the decoder function.\n\n  Args:\n    num_topics: The number of topics.\n    num_words: The number of words.\n\n  Returns:\n    decoder: A `callable` mapping a `Tensor` of encodings to a\n      `tfd.Distribution` instance over words.\n  \"\"\"\n  topics_words_logits = tf.compat.v1.get_variable(\n      \"topics_words_logits\",\n      shape=[num_topics, num_words],\n      initializer=tf.compat.v1.glorot_normal_initializer())\n  topics_words = tf.nn.softmax(topics_words_logits, axis=-1)\n\n  def decoder(topics):\n    word_probs = tf.matmul(topics, topics_words)\n    # The observations are bag of words and therefore not one-hot. However,\n    # log_prob of OneHotCategorical computes the probability correctly in\n    # this case.\n    return tfd.OneHotCategorical(probs=word_probs,\n                                 name=\"bag_of_words\")\n\n  return decoder, topics_words", "code_tokens": ["def", "make_decoder", "(", "num_topics", ",", "num_words", ")", ":", "topics_words_logits", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"topics_words_logits\"", ",", "shape", "=", "[", "num_topics", ",", "num_words", "]", ",", "initializer", "=", "tf", ".", "compat", ".", "v1", ".", "glorot_normal_initializer", "(", ")", ")", "topics_words", "=", "tf", ".", "nn", ".", "softmax", "(", "topics_words_logits", ",", "axis", "=", "-", "1", ")", "def", "decoder", "(", "topics", ")", ":", "word_probs", "=", "tf", ".", "matmul", "(", "topics", ",", "topics_words", ")", "# The observations are bag of words and therefore not one-hot. However,", "# log_prob of OneHotCategorical computes the probability correctly in", "# this case.", "return", "tfd", ".", "OneHotCategorical", "(", "probs", "=", "word_probs", ",", "name", "=", "\"bag_of_words\"", ")", "return", "decoder", ",", "topics_words"], "docstring": "Create the decoder function.\n\n  Args:\n    num_topics: The number of topics.\n    num_words: The number of words.\n\n  Returns:\n    decoder: A `callable` mapping a `Tensor` of encodings to a\n      `tfd.Distribution` instance over words.", "docstring_tokens": ["Create", "the", "decoder", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/latent_dirichlet_allocation_distributions.py#L197-L222", "partition": "test", "index": 1013, "time": "2018-07-02 01:57:53"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/latent_dirichlet_allocation_edward2.py", "func_name": "build_input_fns", "original_string": "def build_input_fns(data_dir, batch_size):\n  \"\"\"Builds iterators for train and evaluation data.\n\n  Each object is represented as a bag-of-words vector.\n\n  Arguments:\n    data_dir: Folder in which to store the data.\n    batch_size: Batch size for both train and evaluation.\n\n  Returns:\n    train_input_fn: A function that returns an iterator over the training data.\n    eval_input_fn: A function that returns an iterator over the evaluation data.\n    vocabulary: A mapping of word's integer index to the corresponding string.\n  \"\"\"\n\n  with open(download(data_dir, \"vocab.pkl\"), \"r\") as f:\n    words_to_idx = pickle.load(f)\n  num_words = len(words_to_idx)\n\n  vocabulary = [None] * num_words\n  for word, idx in words_to_idx.items():\n    vocabulary[idx] = word\n\n  # Build an iterator over training batches.\n  def train_input_fn():\n    dataset = newsgroups_dataset(\n        data_dir, \"train\", num_words, shuffle_and_repeat=True)\n    # Prefetching makes training about 1.5x faster.\n    dataset = dataset.batch(batch_size).prefetch(32)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  # Build an iterator over the heldout set.\n  def eval_input_fn():\n    dataset = newsgroups_dataset(\n        data_dir, \"test\", num_words, shuffle_and_repeat=False)\n    dataset = dataset.batch(batch_size)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return train_input_fn, eval_input_fn, vocabulary", "language": "python", "code": "def build_input_fns(data_dir, batch_size):\n  \"\"\"Builds iterators for train and evaluation data.\n\n  Each object is represented as a bag-of-words vector.\n\n  Arguments:\n    data_dir: Folder in which to store the data.\n    batch_size: Batch size for both train and evaluation.\n\n  Returns:\n    train_input_fn: A function that returns an iterator over the training data.\n    eval_input_fn: A function that returns an iterator over the evaluation data.\n    vocabulary: A mapping of word's integer index to the corresponding string.\n  \"\"\"\n\n  with open(download(data_dir, \"vocab.pkl\"), \"r\") as f:\n    words_to_idx = pickle.load(f)\n  num_words = len(words_to_idx)\n\n  vocabulary = [None] * num_words\n  for word, idx in words_to_idx.items():\n    vocabulary[idx] = word\n\n  # Build an iterator over training batches.\n  def train_input_fn():\n    dataset = newsgroups_dataset(\n        data_dir, \"train\", num_words, shuffle_and_repeat=True)\n    # Prefetching makes training about 1.5x faster.\n    dataset = dataset.batch(batch_size).prefetch(32)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  # Build an iterator over the heldout set.\n  def eval_input_fn():\n    dataset = newsgroups_dataset(\n        data_dir, \"test\", num_words, shuffle_and_repeat=False)\n    dataset = dataset.batch(batch_size)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return train_input_fn, eval_input_fn, vocabulary", "code_tokens": ["def", "build_input_fns", "(", "data_dir", ",", "batch_size", ")", ":", "with", "open", "(", "download", "(", "data_dir", ",", "\"vocab.pkl\"", ")", ",", "\"r\"", ")", "as", "f", ":", "words_to_idx", "=", "pickle", ".", "load", "(", "f", ")", "num_words", "=", "len", "(", "words_to_idx", ")", "vocabulary", "=", "[", "None", "]", "*", "num_words", "for", "word", ",", "idx", "in", "words_to_idx", ".", "items", "(", ")", ":", "vocabulary", "[", "idx", "]", "=", "word", "# Build an iterator over training batches.", "def", "train_input_fn", "(", ")", ":", "dataset", "=", "newsgroups_dataset", "(", "data_dir", ",", "\"train\"", ",", "num_words", ",", "shuffle_and_repeat", "=", "True", ")", "# Prefetching makes training about 1.5x faster.", "dataset", "=", "dataset", ".", "batch", "(", "batch_size", ")", ".", "prefetch", "(", "32", ")", "return", "tf", ".", "compat", ".", "v1", ".", "data", ".", "make_one_shot_iterator", "(", "dataset", ")", ".", "get_next", "(", ")", "# Build an iterator over the heldout set.", "def", "eval_input_fn", "(", ")", ":", "dataset", "=", "newsgroups_dataset", "(", "data_dir", ",", "\"test\"", ",", "num_words", ",", "shuffle_and_repeat", "=", "False", ")", "dataset", "=", "dataset", ".", "batch", "(", "batch_size", ")", "return", "tf", ".", "compat", ".", "v1", ".", "data", ".", "make_one_shot_iterator", "(", "dataset", ")", ".", "get_next", "(", ")", "return", "train_input_fn", ",", "eval_input_fn", ",", "vocabulary"], "docstring": "Builds iterators for train and evaluation data.\n\n  Each object is represented as a bag-of-words vector.\n\n  Arguments:\n    data_dir: Folder in which to store the data.\n    batch_size: Batch size for both train and evaluation.\n\n  Returns:\n    train_input_fn: A function that returns an iterator over the training data.\n    eval_input_fn: A function that returns an iterator over the evaluation data.\n    vocabulary: A mapping of word's integer index to the corresponding string.", "docstring_tokens": ["Builds", "iterators", "for", "train", "and", "evaluation", "data", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/latent_dirichlet_allocation_edward2.py#L481-L519", "partition": "test", "index": 843, "time": "2018-07-02 01:57:53"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/latent_dirichlet_allocation_edward2.py", "func_name": "build_fake_input_fns", "original_string": "def build_fake_input_fns(batch_size):\n  \"\"\"Builds fake data for unit testing.\"\"\"\n  num_words = 1000\n  vocabulary = [str(i) for i in range(num_words)]\n\n  random_sample = np.random.randint(\n      10, size=(batch_size, num_words)).astype(np.float32)\n\n  def train_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(random_sample)\n    dataset = dataset.batch(batch_size).repeat()\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  def eval_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(random_sample)\n    dataset = dataset.batch(batch_size)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return train_input_fn, eval_input_fn, vocabulary", "language": "python", "code": "def build_fake_input_fns(batch_size):\n  \"\"\"Builds fake data for unit testing.\"\"\"\n  num_words = 1000\n  vocabulary = [str(i) for i in range(num_words)]\n\n  random_sample = np.random.randint(\n      10, size=(batch_size, num_words)).astype(np.float32)\n\n  def train_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(random_sample)\n    dataset = dataset.batch(batch_size).repeat()\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  def eval_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(random_sample)\n    dataset = dataset.batch(batch_size)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return train_input_fn, eval_input_fn, vocabulary", "code_tokens": ["def", "build_fake_input_fns", "(", "batch_size", ")", ":", "num_words", "=", "1000", "vocabulary", "=", "[", "str", "(", "i", ")", "for", "i", "in", "range", "(", "num_words", ")", "]", "random_sample", "=", "np", ".", "random", ".", "randint", "(", "10", ",", "size", "=", "(", "batch_size", ",", "num_words", ")", ")", ".", "astype", "(", "np", ".", "float32", ")", "def", "train_input_fn", "(", ")", ":", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "random_sample", ")", "dataset", "=", "dataset", ".", "batch", "(", "batch_size", ")", ".", "repeat", "(", ")", "return", "tf", ".", "compat", ".", "v1", ".", "data", ".", "make_one_shot_iterator", "(", "dataset", ")", ".", "get_next", "(", ")", "def", "eval_input_fn", "(", ")", ":", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "random_sample", ")", "dataset", "=", "dataset", ".", "batch", "(", "batch_size", ")", "return", "tf", ".", "compat", ".", "v1", ".", "data", ".", "make_one_shot_iterator", "(", "dataset", ")", ".", "get_next", "(", ")", "return", "train_input_fn", ",", "eval_input_fn", ",", "vocabulary"], "docstring": "Builds fake data for unit testing.", "docstring_tokens": ["Builds", "fake", "data", "for", "unit", "testing", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/latent_dirichlet_allocation_edward2.py#L460-L478", "partition": "test", "index": 842, "time": "2018-07-02 01:57:53"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/latent_dirichlet_allocation_distributions.py", "func_name": "make_prior", "original_string": "def make_prior(num_topics, initial_value):\n  \"\"\"Create the prior distribution.\n\n  Args:\n    num_topics: Number of topics.\n    initial_value: The starting value for the prior parameters.\n\n  Returns:\n    prior: A `callable` that returns a `tf.distribution.Distribution`\n        instance, the prior distribution.\n    prior_variables: A `list` of `Variable` objects, the trainable parameters\n        of the prior.\n  \"\"\"\n  def _softplus_inverse(x):\n    return np.log(np.expm1(x))\n\n  logit_concentration = tf.compat.v1.get_variable(\n      \"logit_concentration\",\n      shape=[1, num_topics],\n      initializer=tf.compat.v1.initializers.constant(\n          _softplus_inverse(initial_value)))\n  concentration = _clip_dirichlet_parameters(\n      tf.nn.softplus(logit_concentration))\n\n  def prior():\n    return tfd.Dirichlet(concentration=concentration,\n                         name=\"topics_prior\")\n\n  prior_variables = [logit_concentration]\n\n  return prior, prior_variables", "language": "python", "code": "def make_prior(num_topics, initial_value):\n  \"\"\"Create the prior distribution.\n\n  Args:\n    num_topics: Number of topics.\n    initial_value: The starting value for the prior parameters.\n\n  Returns:\n    prior: A `callable` that returns a `tf.distribution.Distribution`\n        instance, the prior distribution.\n    prior_variables: A `list` of `Variable` objects, the trainable parameters\n        of the prior.\n  \"\"\"\n  def _softplus_inverse(x):\n    return np.log(np.expm1(x))\n\n  logit_concentration = tf.compat.v1.get_variable(\n      \"logit_concentration\",\n      shape=[1, num_topics],\n      initializer=tf.compat.v1.initializers.constant(\n          _softplus_inverse(initial_value)))\n  concentration = _clip_dirichlet_parameters(\n      tf.nn.softplus(logit_concentration))\n\n  def prior():\n    return tfd.Dirichlet(concentration=concentration,\n                         name=\"topics_prior\")\n\n  prior_variables = [logit_concentration]\n\n  return prior, prior_variables", "code_tokens": ["def", "make_prior", "(", "num_topics", ",", "initial_value", ")", ":", "def", "_softplus_inverse", "(", "x", ")", ":", "return", "np", ".", "log", "(", "np", ".", "expm1", "(", "x", ")", ")", "logit_concentration", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"logit_concentration\"", ",", "shape", "=", "[", "1", ",", "num_topics", "]", ",", "initializer", "=", "tf", ".", "compat", ".", "v1", ".", "initializers", ".", "constant", "(", "_softplus_inverse", "(", "initial_value", ")", ")", ")", "concentration", "=", "_clip_dirichlet_parameters", "(", "tf", ".", "nn", ".", "softplus", "(", "logit_concentration", ")", ")", "def", "prior", "(", ")", ":", "return", "tfd", ".", "Dirichlet", "(", "concentration", "=", "concentration", ",", "name", "=", "\"topics_prior\"", ")", "prior_variables", "=", "[", "logit_concentration", "]", "return", "prior", ",", "prior_variables"], "docstring": "Create the prior distribution.\n\n  Args:\n    num_topics: Number of topics.\n    initial_value: The starting value for the prior parameters.\n\n  Returns:\n    prior: A `callable` that returns a `tf.distribution.Distribution`\n        instance, the prior distribution.\n    prior_variables: A `list` of `Variable` objects, the trainable parameters\n        of the prior.", "docstring_tokens": ["Create", "the", "prior", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/latent_dirichlet_allocation_distributions.py#L225-L255", "partition": "test", "index": 1014, "time": "2018-07-02 01:57:53"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/nelder_mead.py", "func_name": "_replace_at_index", "original_string": "def _replace_at_index(x, index, replacement):\n  \"\"\"Replaces an element at supplied index.\"\"\"\n  x_new = tf.concat([x[:index], tf.expand_dims(replacement, axis=0),\n                     x[(index + 1):]], axis=0)\n  return x_new", "language": "python", "code": "def _replace_at_index(x, index, replacement):\n  \"\"\"Replaces an element at supplied index.\"\"\"\n  x_new = tf.concat([x[:index], tf.expand_dims(replacement, axis=0),\n                     x[(index + 1):]], axis=0)\n  return x_new", "code_tokens": ["def", "_replace_at_index", "(", "x", ",", "index", ",", "replacement", ")", ":", "x_new", "=", "tf", ".", "concat", "(", "[", "x", "[", ":", "index", "]", ",", "tf", ".", "expand_dims", "(", "replacement", ",", "axis", "=", "0", ")", ",", "x", "[", "(", "index", "+", "1", ")", ":", "]", "]", ",", "axis", "=", "0", ")", "return", "x_new"], "docstring": "Replaces an element at supplied index.", "docstring_tokens": ["Replaces", "an", "element", "at", "supplied", "index", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/nelder_mead.py#L602-L606", "partition": "test", "index": 864, "time": "2018-07-03 10:33:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/nelder_mead.py", "func_name": "_prepare_args_with_initial_vertex", "original_string": "def _prepare_args_with_initial_vertex(objective_function,\n                                      initial_vertex,\n                                      step_sizes,\n                                      objective_at_initial_vertex,\n                                      batch_evaluate_objective):\n  \"\"\"Constructs a standard axes aligned simplex.\"\"\"\n  dim = tf.size(input=initial_vertex)\n  num_vertices = dim + 1\n  unit_vectors_along_axes = tf.reshape(\n      tf.eye(dim, dim, dtype=initial_vertex.dtype.base_dtype),\n      tf.concat([[dim], tf.shape(input=initial_vertex)], axis=0))\n\n  # If step_sizes does not broadcast to initial_vertex, the multiplication\n  # in the second term will fail.\n  simplex_face = initial_vertex + step_sizes * unit_vectors_along_axes\n  simplex = tf.concat([tf.expand_dims(initial_vertex, axis=0),\n                       simplex_face], axis=0)\n  num_evaluations = 0\n  # Evaluate the objective function at the simplex vertices.\n  if objective_at_initial_vertex is None:\n    objective_at_initial_vertex = objective_function(initial_vertex)\n    num_evaluations += 1\n\n  objective_at_simplex_face, num_evals = _evaluate_objective_multiple(\n      objective_function, simplex_face, batch_evaluate_objective)\n  num_evaluations += num_evals\n\n  objective_at_simplex = tf.concat(\n      [\n          tf.expand_dims(objective_at_initial_vertex, axis=0),\n          objective_at_simplex_face\n      ], axis=0)\n\n  return (dim,\n          num_vertices,\n          simplex,\n          objective_at_simplex,\n          num_evaluations)", "language": "python", "code": "def _prepare_args_with_initial_vertex(objective_function,\n                                      initial_vertex,\n                                      step_sizes,\n                                      objective_at_initial_vertex,\n                                      batch_evaluate_objective):\n  \"\"\"Constructs a standard axes aligned simplex.\"\"\"\n  dim = tf.size(input=initial_vertex)\n  num_vertices = dim + 1\n  unit_vectors_along_axes = tf.reshape(\n      tf.eye(dim, dim, dtype=initial_vertex.dtype.base_dtype),\n      tf.concat([[dim], tf.shape(input=initial_vertex)], axis=0))\n\n  # If step_sizes does not broadcast to initial_vertex, the multiplication\n  # in the second term will fail.\n  simplex_face = initial_vertex + step_sizes * unit_vectors_along_axes\n  simplex = tf.concat([tf.expand_dims(initial_vertex, axis=0),\n                       simplex_face], axis=0)\n  num_evaluations = 0\n  # Evaluate the objective function at the simplex vertices.\n  if objective_at_initial_vertex is None:\n    objective_at_initial_vertex = objective_function(initial_vertex)\n    num_evaluations += 1\n\n  objective_at_simplex_face, num_evals = _evaluate_objective_multiple(\n      objective_function, simplex_face, batch_evaluate_objective)\n  num_evaluations += num_evals\n\n  objective_at_simplex = tf.concat(\n      [\n          tf.expand_dims(objective_at_initial_vertex, axis=0),\n          objective_at_simplex_face\n      ], axis=0)\n\n  return (dim,\n          num_vertices,\n          simplex,\n          objective_at_simplex,\n          num_evaluations)", "code_tokens": ["def", "_prepare_args_with_initial_vertex", "(", "objective_function", ",", "initial_vertex", ",", "step_sizes", ",", "objective_at_initial_vertex", ",", "batch_evaluate_objective", ")", ":", "dim", "=", "tf", ".", "size", "(", "input", "=", "initial_vertex", ")", "num_vertices", "=", "dim", "+", "1", "unit_vectors_along_axes", "=", "tf", ".", "reshape", "(", "tf", ".", "eye", "(", "dim", ",", "dim", ",", "dtype", "=", "initial_vertex", ".", "dtype", ".", "base_dtype", ")", ",", "tf", ".", "concat", "(", "[", "[", "dim", "]", ",", "tf", ".", "shape", "(", "input", "=", "initial_vertex", ")", "]", ",", "axis", "=", "0", ")", ")", "# If step_sizes does not broadcast to initial_vertex, the multiplication", "# in the second term will fail.", "simplex_face", "=", "initial_vertex", "+", "step_sizes", "*", "unit_vectors_along_axes", "simplex", "=", "tf", ".", "concat", "(", "[", "tf", ".", "expand_dims", "(", "initial_vertex", ",", "axis", "=", "0", ")", ",", "simplex_face", "]", ",", "axis", "=", "0", ")", "num_evaluations", "=", "0", "# Evaluate the objective function at the simplex vertices.", "if", "objective_at_initial_vertex", "is", "None", ":", "objective_at_initial_vertex", "=", "objective_function", "(", "initial_vertex", ")", "num_evaluations", "+=", "1", "objective_at_simplex_face", ",", "num_evals", "=", "_evaluate_objective_multiple", "(", "objective_function", ",", "simplex_face", ",", "batch_evaluate_objective", ")", "num_evaluations", "+=", "num_evals", "objective_at_simplex", "=", "tf", ".", "concat", "(", "[", "tf", ".", "expand_dims", "(", "objective_at_initial_vertex", ",", "axis", "=", "0", ")", ",", "objective_at_simplex_face", "]", ",", "axis", "=", "0", ")", "return", "(", "dim", ",", "num_vertices", ",", "simplex", ",", "objective_at_simplex", ",", "num_evaluations", ")"], "docstring": "Constructs a standard axes aligned simplex.", "docstring_tokens": ["Constructs", "a", "standard", "axes", "aligned", "simplex", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/nelder_mead.py#L792-L829", "partition": "test", "index": 868, "time": "2018-07-03 10:33:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/nelder_mead.py", "func_name": "_prepare_args_with_initial_simplex", "original_string": "def _prepare_args_with_initial_simplex(objective_function,\n                                       initial_simplex,\n                                       objective_at_initial_simplex,\n                                       batch_evaluate_objective):\n  \"\"\"Evaluates the objective function at the specified initial simplex.\"\"\"\n  initial_simplex = tf.convert_to_tensor(value=initial_simplex)\n\n  # If d is the dimension of the problem, the number of vertices in the\n  # simplex should be d+1. From this, we can infer the number of dimensions\n  # as n - 1 where n is the number of vertices specified.\n  num_vertices = tf.shape(input=initial_simplex)[0]\n  dim = num_vertices - 1\n  num_evaluations = 0\n\n  if objective_at_initial_simplex is None:\n    objective_at_initial_simplex, n_evals = _evaluate_objective_multiple(\n        objective_function, initial_simplex, batch_evaluate_objective)\n    num_evaluations += n_evals\n  objective_at_initial_simplex = tf.convert_to_tensor(\n      value=objective_at_initial_simplex)\n  return (dim,\n          num_vertices,\n          initial_simplex,\n          objective_at_initial_simplex,\n          num_evaluations)", "language": "python", "code": "def _prepare_args_with_initial_simplex(objective_function,\n                                       initial_simplex,\n                                       objective_at_initial_simplex,\n                                       batch_evaluate_objective):\n  \"\"\"Evaluates the objective function at the specified initial simplex.\"\"\"\n  initial_simplex = tf.convert_to_tensor(value=initial_simplex)\n\n  # If d is the dimension of the problem, the number of vertices in the\n  # simplex should be d+1. From this, we can infer the number of dimensions\n  # as n - 1 where n is the number of vertices specified.\n  num_vertices = tf.shape(input=initial_simplex)[0]\n  dim = num_vertices - 1\n  num_evaluations = 0\n\n  if objective_at_initial_simplex is None:\n    objective_at_initial_simplex, n_evals = _evaluate_objective_multiple(\n        objective_function, initial_simplex, batch_evaluate_objective)\n    num_evaluations += n_evals\n  objective_at_initial_simplex = tf.convert_to_tensor(\n      value=objective_at_initial_simplex)\n  return (dim,\n          num_vertices,\n          initial_simplex,\n          objective_at_initial_simplex,\n          num_evaluations)", "code_tokens": ["def", "_prepare_args_with_initial_simplex", "(", "objective_function", ",", "initial_simplex", ",", "objective_at_initial_simplex", ",", "batch_evaluate_objective", ")", ":", "initial_simplex", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "initial_simplex", ")", "# If d is the dimension of the problem, the number of vertices in the", "# simplex should be d+1. From this, we can infer the number of dimensions", "# as n - 1 where n is the number of vertices specified.", "num_vertices", "=", "tf", ".", "shape", "(", "input", "=", "initial_simplex", ")", "[", "0", "]", "dim", "=", "num_vertices", "-", "1", "num_evaluations", "=", "0", "if", "objective_at_initial_simplex", "is", "None", ":", "objective_at_initial_simplex", ",", "n_evals", "=", "_evaluate_objective_multiple", "(", "objective_function", ",", "initial_simplex", ",", "batch_evaluate_objective", ")", "num_evaluations", "+=", "n_evals", "objective_at_initial_simplex", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "objective_at_initial_simplex", ")", "return", "(", "dim", ",", "num_vertices", ",", "initial_simplex", ",", "objective_at_initial_simplex", ",", "num_evaluations", ")"], "docstring": "Evaluates the objective function at the specified initial simplex.", "docstring_tokens": ["Evaluates", "the", "objective", "function", "at", "the", "specified", "initial", "simplex", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/nelder_mead.py#L765-L789", "partition": "test", "index": 867, "time": "2018-07-03 10:33:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/nelder_mead.py", "func_name": "_prepare_args", "original_string": "def _prepare_args(objective_function,\n                  initial_simplex,\n                  initial_vertex,\n                  step_sizes,\n                  objective_at_initial_simplex,\n                  objective_at_initial_vertex,\n                  batch_evaluate_objective):\n  \"\"\"Computes the initial simplex and the objective values at the simplex.\n\n  Args:\n    objective_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a `Tensor` of real dtype containing\n      the value of the function at that point. The function\n      to be evaluated at the simplex. If `batch_evaluate_objective` is `True`,\n      the callable may be evaluated on a `Tensor` of shape `[n+1] + s `\n      where `n` is the dimension of the problem and `s` is the shape of a\n      single point in the domain (so `n` is the size of a `Tensor`\n      representing a single point).\n      In this case, the expected return value is a `Tensor` of shape `[n+1]`.\n    initial_simplex: None or `Tensor` of real dtype. The initial simplex to\n      start the search. If supplied, should be a `Tensor` of shape `[n+1] + s`\n      where `n` is the dimension of the problem and `s` is the shape of a\n      single point in the domain. Each row (i.e. the `Tensor` with a given\n      value of the first index) is interpreted as a vertex of a simplex and\n      hence the rows must be affinely independent. If not supplied, an axes\n      aligned simplex is constructed using the `initial_vertex` and\n      `step_sizes`. Only one and at least one of `initial_simplex` and\n      `initial_vertex` must be supplied.\n    initial_vertex: None or `Tensor` of real dtype and any shape that can\n      be consumed by the `objective_function`. A single point in the domain that\n      will be used to construct an axes aligned initial simplex.\n    step_sizes: None or `Tensor` of real dtype and shape broadcasting\n      compatible with `initial_vertex`. Supplies the simplex scale along each\n      axes. Only used if `initial_simplex` is not supplied. See the docstring\n      of `minimize` for more details.\n    objective_at_initial_simplex: None or rank `1` `Tensor` of real dtype.\n      The value of the objective function at the initial simplex.\n      May be supplied only if `initial_simplex` is\n      supplied. If not supplied, it will be computed.\n    objective_at_initial_vertex: None or scalar `Tensor` of real dtype. The\n      value of the objective function at the initial vertex. May be supplied\n      only if the `initial_vertex` is also supplied.\n    batch_evaluate_objective: Python `bool`. If True, the objective function\n      will be evaluated on all the vertices of the simplex packed into a\n      single tensor. If False, the objective will be mapped across each\n      vertex separately.\n\n  Returns:\n    prepared_args: A tuple containing the following elements:\n      dimension: Scalar `Tensor` of `int32` dtype. The dimension of the problem\n        as inferred from the supplied arguments.\n      num_vertices: Scalar `Tensor` of `int32` dtype. The number of vertices\n        in the simplex.\n      simplex: A `Tensor` of same dtype as `initial_simplex`\n        (or `initial_vertex`). The first component of the shape of the\n        `Tensor` is `num_vertices` and each element represents a vertex of\n        the simplex.\n      objective_at_simplex: A `Tensor` of same dtype as the dtype of the\n        return value of objective_function. The shape is a vector of size\n        `num_vertices`. The objective function evaluated at the simplex.\n      num_evaluations: An `int32` scalar `Tensor`. The number of points on\n        which the objective function was evaluated.\n\n  Raises:\n    ValueError: If any of the following conditions hold\n      1. If none or more than one of `initial_simplex` and `initial_vertex` are\n        supplied.\n      2. If `initial_simplex` and `step_sizes` are both specified.\n  \"\"\"\n  if objective_at_initial_simplex is not None and initial_simplex is None:\n    raise ValueError('`objective_at_initial_simplex` specified but the'\n                     '`initial_simplex` was not.')\n\n  if objective_at_initial_vertex is not None and initial_vertex is None:\n    raise ValueError('`objective_at_initial_vertex` specified but the'\n                     '`initial_vertex` was not.')\n\n  # The full simplex was specified.\n  if initial_simplex is not None:\n    if initial_vertex is not None:\n      raise ValueError('Both `initial_simplex` and `initial_vertex` specified.'\n                       ' Only one of the two should be specified.')\n\n    if step_sizes is not None:\n      raise ValueError('`step_sizes` must not be specified when an'\n                       ' `initial_simplex` has been specified.')\n    return _prepare_args_with_initial_simplex(objective_function,\n                                              initial_simplex,\n                                              objective_at_initial_simplex,\n                                              batch_evaluate_objective)\n\n  if initial_vertex is None:\n    raise ValueError('One of `initial_simplex` or `initial_vertex`'\n                     ' must be supplied')\n\n  if step_sizes is None:\n    step_sizes = _default_step_sizes(initial_vertex)\n\n  return _prepare_args_with_initial_vertex(objective_function,\n                                           initial_vertex,\n                                           step_sizes,\n                                           objective_at_initial_vertex,\n                                           batch_evaluate_objective)", "language": "python", "code": "def _prepare_args(objective_function,\n                  initial_simplex,\n                  initial_vertex,\n                  step_sizes,\n                  objective_at_initial_simplex,\n                  objective_at_initial_vertex,\n                  batch_evaluate_objective):\n  \"\"\"Computes the initial simplex and the objective values at the simplex.\n\n  Args:\n    objective_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a `Tensor` of real dtype containing\n      the value of the function at that point. The function\n      to be evaluated at the simplex. If `batch_evaluate_objective` is `True`,\n      the callable may be evaluated on a `Tensor` of shape `[n+1] + s `\n      where `n` is the dimension of the problem and `s` is the shape of a\n      single point in the domain (so `n` is the size of a `Tensor`\n      representing a single point).\n      In this case, the expected return value is a `Tensor` of shape `[n+1]`.\n    initial_simplex: None or `Tensor` of real dtype. The initial simplex to\n      start the search. If supplied, should be a `Tensor` of shape `[n+1] + s`\n      where `n` is the dimension of the problem and `s` is the shape of a\n      single point in the domain. Each row (i.e. the `Tensor` with a given\n      value of the first index) is interpreted as a vertex of a simplex and\n      hence the rows must be affinely independent. If not supplied, an axes\n      aligned simplex is constructed using the `initial_vertex` and\n      `step_sizes`. Only one and at least one of `initial_simplex` and\n      `initial_vertex` must be supplied.\n    initial_vertex: None or `Tensor` of real dtype and any shape that can\n      be consumed by the `objective_function`. A single point in the domain that\n      will be used to construct an axes aligned initial simplex.\n    step_sizes: None or `Tensor` of real dtype and shape broadcasting\n      compatible with `initial_vertex`. Supplies the simplex scale along each\n      axes. Only used if `initial_simplex` is not supplied. See the docstring\n      of `minimize` for more details.\n    objective_at_initial_simplex: None or rank `1` `Tensor` of real dtype.\n      The value of the objective function at the initial simplex.\n      May be supplied only if `initial_simplex` is\n      supplied. If not supplied, it will be computed.\n    objective_at_initial_vertex: None or scalar `Tensor` of real dtype. The\n      value of the objective function at the initial vertex. May be supplied\n      only if the `initial_vertex` is also supplied.\n    batch_evaluate_objective: Python `bool`. If True, the objective function\n      will be evaluated on all the vertices of the simplex packed into a\n      single tensor. If False, the objective will be mapped across each\n      vertex separately.\n\n  Returns:\n    prepared_args: A tuple containing the following elements:\n      dimension: Scalar `Tensor` of `int32` dtype. The dimension of the problem\n        as inferred from the supplied arguments.\n      num_vertices: Scalar `Tensor` of `int32` dtype. The number of vertices\n        in the simplex.\n      simplex: A `Tensor` of same dtype as `initial_simplex`\n        (or `initial_vertex`). The first component of the shape of the\n        `Tensor` is `num_vertices` and each element represents a vertex of\n        the simplex.\n      objective_at_simplex: A `Tensor` of same dtype as the dtype of the\n        return value of objective_function. The shape is a vector of size\n        `num_vertices`. The objective function evaluated at the simplex.\n      num_evaluations: An `int32` scalar `Tensor`. The number of points on\n        which the objective function was evaluated.\n\n  Raises:\n    ValueError: If any of the following conditions hold\n      1. If none or more than one of `initial_simplex` and `initial_vertex` are\n        supplied.\n      2. If `initial_simplex` and `step_sizes` are both specified.\n  \"\"\"\n  if objective_at_initial_simplex is not None and initial_simplex is None:\n    raise ValueError('`objective_at_initial_simplex` specified but the'\n                     '`initial_simplex` was not.')\n\n  if objective_at_initial_vertex is not None and initial_vertex is None:\n    raise ValueError('`objective_at_initial_vertex` specified but the'\n                     '`initial_vertex` was not.')\n\n  # The full simplex was specified.\n  if initial_simplex is not None:\n    if initial_vertex is not None:\n      raise ValueError('Both `initial_simplex` and `initial_vertex` specified.'\n                       ' Only one of the two should be specified.')\n\n    if step_sizes is not None:\n      raise ValueError('`step_sizes` must not be specified when an'\n                       ' `initial_simplex` has been specified.')\n    return _prepare_args_with_initial_simplex(objective_function,\n                                              initial_simplex,\n                                              objective_at_initial_simplex,\n                                              batch_evaluate_objective)\n\n  if initial_vertex is None:\n    raise ValueError('One of `initial_simplex` or `initial_vertex`'\n                     ' must be supplied')\n\n  if step_sizes is None:\n    step_sizes = _default_step_sizes(initial_vertex)\n\n  return _prepare_args_with_initial_vertex(objective_function,\n                                           initial_vertex,\n                                           step_sizes,\n                                           objective_at_initial_vertex,\n                                           batch_evaluate_objective)", "code_tokens": ["def", "_prepare_args", "(", "objective_function", ",", "initial_simplex", ",", "initial_vertex", ",", "step_sizes", ",", "objective_at_initial_simplex", ",", "objective_at_initial_vertex", ",", "batch_evaluate_objective", ")", ":", "if", "objective_at_initial_simplex", "is", "not", "None", "and", "initial_simplex", "is", "None", ":", "raise", "ValueError", "(", "'`objective_at_initial_simplex` specified but the'", "'`initial_simplex` was not.'", ")", "if", "objective_at_initial_vertex", "is", "not", "None", "and", "initial_vertex", "is", "None", ":", "raise", "ValueError", "(", "'`objective_at_initial_vertex` specified but the'", "'`initial_vertex` was not.'", ")", "# The full simplex was specified.", "if", "initial_simplex", "is", "not", "None", ":", "if", "initial_vertex", "is", "not", "None", ":", "raise", "ValueError", "(", "'Both `initial_simplex` and `initial_vertex` specified.'", "' Only one of the two should be specified.'", ")", "if", "step_sizes", "is", "not", "None", ":", "raise", "ValueError", "(", "'`step_sizes` must not be specified when an'", "' `initial_simplex` has been specified.'", ")", "return", "_prepare_args_with_initial_simplex", "(", "objective_function", ",", "initial_simplex", ",", "objective_at_initial_simplex", ",", "batch_evaluate_objective", ")", "if", "initial_vertex", "is", "None", ":", "raise", "ValueError", "(", "'One of `initial_simplex` or `initial_vertex`'", "' must be supplied'", ")", "if", "step_sizes", "is", "None", ":", "step_sizes", "=", "_default_step_sizes", "(", "initial_vertex", ")", "return", "_prepare_args_with_initial_vertex", "(", "objective_function", ",", "initial_vertex", ",", "step_sizes", ",", "objective_at_initial_vertex", ",", "batch_evaluate_objective", ")"], "docstring": "Computes the initial simplex and the objective values at the simplex.\n\n  Args:\n    objective_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a `Tensor` of real dtype containing\n      the value of the function at that point. The function\n      to be evaluated at the simplex. If `batch_evaluate_objective` is `True`,\n      the callable may be evaluated on a `Tensor` of shape `[n+1] + s `\n      where `n` is the dimension of the problem and `s` is the shape of a\n      single point in the domain (so `n` is the size of a `Tensor`\n      representing a single point).\n      In this case, the expected return value is a `Tensor` of shape `[n+1]`.\n    initial_simplex: None or `Tensor` of real dtype. The initial simplex to\n      start the search. If supplied, should be a `Tensor` of shape `[n+1] + s`\n      where `n` is the dimension of the problem and `s` is the shape of a\n      single point in the domain. Each row (i.e. the `Tensor` with a given\n      value of the first index) is interpreted as a vertex of a simplex and\n      hence the rows must be affinely independent. If not supplied, an axes\n      aligned simplex is constructed using the `initial_vertex` and\n      `step_sizes`. Only one and at least one of `initial_simplex` and\n      `initial_vertex` must be supplied.\n    initial_vertex: None or `Tensor` of real dtype and any shape that can\n      be consumed by the `objective_function`. A single point in the domain that\n      will be used to construct an axes aligned initial simplex.\n    step_sizes: None or `Tensor` of real dtype and shape broadcasting\n      compatible with `initial_vertex`. Supplies the simplex scale along each\n      axes. Only used if `initial_simplex` is not supplied. See the docstring\n      of `minimize` for more details.\n    objective_at_initial_simplex: None or rank `1` `Tensor` of real dtype.\n      The value of the objective function at the initial simplex.\n      May be supplied only if `initial_simplex` is\n      supplied. If not supplied, it will be computed.\n    objective_at_initial_vertex: None or scalar `Tensor` of real dtype. The\n      value of the objective function at the initial vertex. May be supplied\n      only if the `initial_vertex` is also supplied.\n    batch_evaluate_objective: Python `bool`. If True, the objective function\n      will be evaluated on all the vertices of the simplex packed into a\n      single tensor. If False, the objective will be mapped across each\n      vertex separately.\n\n  Returns:\n    prepared_args: A tuple containing the following elements:\n      dimension: Scalar `Tensor` of `int32` dtype. The dimension of the problem\n        as inferred from the supplied arguments.\n      num_vertices: Scalar `Tensor` of `int32` dtype. The number of vertices\n        in the simplex.\n      simplex: A `Tensor` of same dtype as `initial_simplex`\n        (or `initial_vertex`). The first component of the shape of the\n        `Tensor` is `num_vertices` and each element represents a vertex of\n        the simplex.\n      objective_at_simplex: A `Tensor` of same dtype as the dtype of the\n        return value of objective_function. The shape is a vector of size\n        `num_vertices`. The objective function evaluated at the simplex.\n      num_evaluations: An `int32` scalar `Tensor`. The number of points on\n        which the objective function was evaluated.\n\n  Raises:\n    ValueError: If any of the following conditions hold\n      1. If none or more than one of `initial_simplex` and `initial_vertex` are\n        supplied.\n      2. If `initial_simplex` and `step_sizes` are both specified.", "docstring_tokens": ["Computes", "the", "initial", "simplex", "and", "the", "objective", "values", "at", "the", "simplex", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/nelder_mead.py#L649-L751", "partition": "test", "index": 866, "time": "2018-07-03 10:33:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/nelder_mead.py", "func_name": "_check_convergence", "original_string": "def _check_convergence(simplex,\n                       best_vertex,\n                       best_objective,\n                       worst_objective,\n                       func_tolerance,\n                       position_tolerance):\n  \"\"\"Returns True if the simplex has converged.\n\n  If the simplex size is smaller than the `position_tolerance` or the variation\n  of the function value over the vertices of the simplex is smaller than the\n  `func_tolerance` return True else False.\n\n  Args:\n    simplex: `Tensor` of real dtype. The simplex to test for convergence. For\n      more details, see the docstring for `initial_simplex` argument\n      of `minimize`.\n    best_vertex: `Tensor` of real dtype and rank one less than `simplex`. The\n      vertex with the best (i.e. smallest) objective value.\n    best_objective: Scalar `Tensor` of real dtype. The best (i.e. smallest)\n      value of the objective function at a vertex.\n    worst_objective: Scalar `Tensor` of same dtype as `best_objective`. The\n      worst (i.e. largest) value of the objective function at a vertex.\n    func_tolerance: Scalar positive `Tensor`. The tolerance for the variation\n      of the objective function value over the simplex. If the variation over\n      the simplex vertices is below this threshold, convergence is True.\n    position_tolerance: Scalar positive `Tensor`. The algorithm stops if the\n      lengths (under the supremum norm) of edges connecting to the best vertex\n      are below this threshold.\n\n  Returns:\n    has_converged: A scalar boolean `Tensor` indicating whether the algorithm\n      is deemed to have converged.\n  \"\"\"\n  objective_convergence = tf.abs(worst_objective -\n                                 best_objective) < func_tolerance\n  simplex_degeneracy = tf.reduce_max(\n      input_tensor=tf.abs(simplex - best_vertex)) < position_tolerance\n  return objective_convergence | simplex_degeneracy", "language": "python", "code": "def _check_convergence(simplex,\n                       best_vertex,\n                       best_objective,\n                       worst_objective,\n                       func_tolerance,\n                       position_tolerance):\n  \"\"\"Returns True if the simplex has converged.\n\n  If the simplex size is smaller than the `position_tolerance` or the variation\n  of the function value over the vertices of the simplex is smaller than the\n  `func_tolerance` return True else False.\n\n  Args:\n    simplex: `Tensor` of real dtype. The simplex to test for convergence. For\n      more details, see the docstring for `initial_simplex` argument\n      of `minimize`.\n    best_vertex: `Tensor` of real dtype and rank one less than `simplex`. The\n      vertex with the best (i.e. smallest) objective value.\n    best_objective: Scalar `Tensor` of real dtype. The best (i.e. smallest)\n      value of the objective function at a vertex.\n    worst_objective: Scalar `Tensor` of same dtype as `best_objective`. The\n      worst (i.e. largest) value of the objective function at a vertex.\n    func_tolerance: Scalar positive `Tensor`. The tolerance for the variation\n      of the objective function value over the simplex. If the variation over\n      the simplex vertices is below this threshold, convergence is True.\n    position_tolerance: Scalar positive `Tensor`. The algorithm stops if the\n      lengths (under the supremum norm) of edges connecting to the best vertex\n      are below this threshold.\n\n  Returns:\n    has_converged: A scalar boolean `Tensor` indicating whether the algorithm\n      is deemed to have converged.\n  \"\"\"\n  objective_convergence = tf.abs(worst_objective -\n                                 best_objective) < func_tolerance\n  simplex_degeneracy = tf.reduce_max(\n      input_tensor=tf.abs(simplex - best_vertex)) < position_tolerance\n  return objective_convergence | simplex_degeneracy", "code_tokens": ["def", "_check_convergence", "(", "simplex", ",", "best_vertex", ",", "best_objective", ",", "worst_objective", ",", "func_tolerance", ",", "position_tolerance", ")", ":", "objective_convergence", "=", "tf", ".", "abs", "(", "worst_objective", "-", "best_objective", ")", "<", "func_tolerance", "simplex_degeneracy", "=", "tf", ".", "reduce_max", "(", "input_tensor", "=", "tf", ".", "abs", "(", "simplex", "-", "best_vertex", ")", ")", "<", "position_tolerance", "return", "objective_convergence", "|", "simplex_degeneracy"], "docstring": "Returns True if the simplex has converged.\n\n  If the simplex size is smaller than the `position_tolerance` or the variation\n  of the function value over the vertices of the simplex is smaller than the\n  `func_tolerance` return True else False.\n\n  Args:\n    simplex: `Tensor` of real dtype. The simplex to test for convergence. For\n      more details, see the docstring for `initial_simplex` argument\n      of `minimize`.\n    best_vertex: `Tensor` of real dtype and rank one less than `simplex`. The\n      vertex with the best (i.e. smallest) objective value.\n    best_objective: Scalar `Tensor` of real dtype. The best (i.e. smallest)\n      value of the objective function at a vertex.\n    worst_objective: Scalar `Tensor` of same dtype as `best_objective`. The\n      worst (i.e. largest) value of the objective function at a vertex.\n    func_tolerance: Scalar positive `Tensor`. The tolerance for the variation\n      of the objective function value over the simplex. If the variation over\n      the simplex vertices is below this threshold, convergence is True.\n    position_tolerance: Scalar positive `Tensor`. The algorithm stops if the\n      lengths (under the supremum norm) of edges connecting to the best vertex\n      are below this threshold.\n\n  Returns:\n    has_converged: A scalar boolean `Tensor` indicating whether the algorithm\n      is deemed to have converged.", "docstring_tokens": ["Returns", "True", "if", "the", "simplex", "has", "converged", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/nelder_mead.py#L609-L646", "partition": "test", "index": 865, "time": "2018-07-03 10:33:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/nelder_mead.py", "func_name": "minimize", "original_string": "def minimize(objective_function,\n             initial_simplex=None,\n             initial_vertex=None,\n             step_sizes=None,\n             objective_at_initial_simplex=None,\n             objective_at_initial_vertex=None,\n             batch_evaluate_objective=False,\n             func_tolerance=1e-8,\n             position_tolerance=1e-8,\n             parallel_iterations=1,\n             max_iterations=None,\n             reflection=None,\n             expansion=None,\n             contraction=None,\n             shrinkage=None,\n             name=None):\n  \"\"\"Minimum of the objective function using the Nelder Mead simplex algorithm.\n\n  Performs an unconstrained minimization of a (possibly non-smooth) function\n  using the Nelder Mead simplex method. Nelder Mead method does not support\n  univariate functions. Hence the dimensions of the domain must be 2 or greater.\n  For details of the algorithm, see\n  [Press, Teukolsky, Vetterling and Flannery(2007)][1].\n\n  Points in the domain of the objective function may be represented as a\n  `Tensor` of general shape but with rank at least 1. The algorithm proceeds\n  by modifying a full rank simplex in the domain. The initial simplex may\n  either be specified by the user or can be constructed using a single vertex\n  supplied by the user. In the latter case, if `v0` is the supplied vertex,\n  the simplex is the convex hull of the set:\n\n  ```None\n  S = {v0} + {v0 + step_i * e_i}\n  ```\n\n  Here `e_i` is a vector which is `1` along the `i`-th axis and zero elsewhere\n  and `step_i` is a characteristic length scale along the `i`-th axis. If the\n  step size is not supplied by the user, a unit step size is used in every axis.\n  Alternately, a single step size may be specified which is used for every\n  axis. The most flexible option is to supply a bespoke step size for every\n  axis.\n\n  ### Usage:\n\n  The following example demonstrates the usage of the Nelder Mead minimzation\n  on a two dimensional problem with the minimum located at a non-differentiable\n  point.\n\n  ```python\n    # The objective function\n    def sqrt_quadratic(x):\n      return tf.sqrt(tf.reduce_sum(x ** 2, axis=-1))\n\n    start = tf.constant([6.0, -21.0])  # Starting point for the search.\n    optim_results = tfp.optimizer.nelder_mead_minimize(\n        sqrt_quadratic, initial_vertex=start, func_tolerance=1e-8,\n        batch_evaluate_objective=True)\n\n    with tf.Session() as session:\n      results = session.run(optim_results)\n      # Check that the search converged\n      assert(results.converged)\n      # Check that the argmin is close to the actual value.\n      np.testing.assert_allclose(results.position, np.array([0.0, 0.0]),\n                                 atol=1e-7)\n      # Print out the total number of function evaluations it took.\n      print (\"Function evaluations: %d\" % results.num_objective_evaluations)\n  ```\n\n  ### References:\n  [1]: William Press, Saul Teukolsky, William Vetterling and Brian Flannery.\n    Numerical Recipes in C++, third edition. pp. 502-507. (2007).\n    http://numerical.recipes/cpppages/chap0sel.pdf\n\n  [2]: Jeffrey Lagarias, James Reeds, Margaret Wright and Paul Wright.\n    Convergence properties of the Nelder-Mead simplex method in low dimensions,\n    Siam J. Optim., Vol 9, No. 1, pp. 112-147. (1998).\n    http://www.math.kent.edu/~reichel/courses/Opt/reading.material.2/nelder.mead.pdf\n\n  [3]: Fuchang Gao and Lixing Han. Implementing the Nelder-Mead simplex\n    algorithm with adaptive parameters. Computational Optimization and\n    Applications, Vol 51, Issue 1, pp 259-277. (2012).\n    https://pdfs.semanticscholar.org/15b4/c4aa7437df4d032c6ee6ce98d6030dd627be.pdf\n\n  Args:\n    objective_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a `Tensor` of real dtype containing\n      the value of the function at that point. The function\n      to be minimized. If `batch_evaluate_objective` is `True`, the callable\n      may be evaluated on a `Tensor` of shape `[n+1] + s ` where `n` is\n      the dimension of the problem and `s` is the shape of a single point\n      in the domain (so `n` is the size of a `Tensor` representing a\n      single point).\n      In this case, the expected return value is a `Tensor` of shape `[n+1]`.\n      Note that this method does not support univariate functions so the problem\n      dimension `n` must be strictly greater than 1.\n    initial_simplex: (Optional) `Tensor` of real dtype. The initial simplex to\n      start the search. If supplied, should be a `Tensor` of shape `[n+1] + s`\n      where `n` is the dimension of the problem and `s` is the shape of a\n      single point in the domain. Each row (i.e. the `Tensor` with a given\n      value of the first index) is interpreted as a vertex of a simplex and\n      hence the rows must be affinely independent. If not supplied, an axes\n      aligned simplex is constructed using the `initial_vertex` and\n      `step_sizes`. Only one and at least one of `initial_simplex` and\n      `initial_vertex` must be supplied.\n    initial_vertex: (Optional) `Tensor` of real dtype and any shape that can\n      be consumed by the `objective_function`. A single point in the domain that\n      will be used to construct an axes aligned initial simplex.\n    step_sizes: (Optional) `Tensor` of real dtype and shape broadcasting\n      compatible with `initial_vertex`. Supplies the simplex scale along each\n      axes. Only used if `initial_simplex` is not supplied. See description\n      above for details on how step sizes and initial vertex are used to\n      construct the initial simplex.\n    objective_at_initial_simplex: (Optional) Rank `1` `Tensor` of real dtype\n      of a rank `1` `Tensor`. The value of the objective function at the\n      initial simplex. May be supplied only if `initial_simplex` is\n      supplied. If not supplied, it will be computed.\n    objective_at_initial_vertex: (Optional) Scalar `Tensor` of real dtype. The\n      value of the objective function at the initial vertex. May be supplied\n      only if the `initial_vertex` is also supplied.\n    batch_evaluate_objective: (Optional) Python `bool`. If True, the objective\n      function will be evaluated on all the vertices of the simplex packed\n      into a single tensor. If False, the objective will be mapped across each\n      vertex separately. Evaluating the objective function in a batch allows\n      use of vectorization and should be preferred if the objective function\n      allows it.\n    func_tolerance: (Optional) Scalar `Tensor` of real dtype. The algorithm\n      stops if the absolute difference between the largest and the smallest\n      function value on the vertices of the simplex is below this number.\n    position_tolerance: (Optional) Scalar `Tensor` of real dtype. The\n      algorithm stops if the largest absolute difference between the\n      coordinates of the vertices is below this threshold.\n    parallel_iterations: (Optional) Positive integer. The number of iterations\n      allowed to run in parallel.\n    max_iterations: (Optional) Scalar positive `Tensor` of dtype `int32`.\n      The maximum number of iterations allowed. If `None` then no limit is\n      applied.\n    reflection: (Optional) Positive Scalar `Tensor` of same dtype as\n      `initial_vertex`. This parameter controls the scaling of the reflected\n      vertex. See, [Press et al(2007)][1] for details. If not specified,\n      uses the dimension dependent prescription of [Gao and Han(2012)][3].\n    expansion: (Optional) Positive Scalar `Tensor` of same dtype as\n      `initial_vertex`. Should be greater than `1` and `reflection`. This\n      parameter controls the expanded scaling of a reflected vertex.\n      See, [Press et al(2007)][1] for details. If not specified, uses the\n      dimension dependent prescription of [Gao and Han(2012)][3].\n    contraction: (Optional) Positive scalar `Tensor` of same dtype as\n      `initial_vertex`. Must be between `0` and `1`. This parameter controls\n      the contraction of the reflected vertex when the objective function at\n      the reflected point fails to show sufficient decrease.\n      See, [Press et al(2007)][1] for more details. If not specified, uses\n      the dimension dependent prescription of [Gao and Han(2012][3].\n    shrinkage: (Optional) Positive scalar `Tensor` of same dtype as\n      `initial_vertex`. Must be between `0` and `1`. This parameter is the scale\n      by which the simplex is shrunk around the best point when the other\n      steps fail to produce improvements.\n      See, [Press et al(2007)][1] for more details. If not specified, uses\n      the dimension dependent prescription of [Gao and Han(2012][3].\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'minimize' is used.\n\n  Returns:\n    optimizer_results: A namedtuple containing the following items:\n      converged: Scalar boolean tensor indicating whether the minimum was\n        found within tolerance.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A `Tensor` containing the last argument value found\n        during the search. If the search converged, then\n        this value is the argmin of the objective function.\n      objective_value: A tensor containing the value of the objective\n        function at the `position`. If the search\n        converged, then this is the (local) minimum of\n        the objective function.\n      final_simplex: The last simplex constructed before stopping.\n      final_objective_values: The objective function evaluated at the\n        vertices of the final simplex.\n      initial_simplex: The starting simplex.\n      initial_objective_values: The objective function evaluated at the\n        vertices of the initial simplex.\n      num_iterations: The number of iterations of the main algorithm body.\n\n  Raises:\n    ValueError: If any of the following conditions hold\n      1. If none or more than one of `initial_simplex` and `initial_vertex` are\n        supplied.\n      2. If `initial_simplex` and `step_sizes` are both specified.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'minimize', [\n      initial_simplex, initial_vertex, step_sizes, objective_at_initial_simplex,\n      objective_at_initial_vertex, func_tolerance, position_tolerance\n  ]):\n    (\n        dim,\n        _,\n        simplex,\n        objective_at_simplex,\n        num_evaluations\n    ) = _prepare_args(objective_function,\n                      initial_simplex,\n                      initial_vertex,\n                      step_sizes,\n                      objective_at_initial_simplex,\n                      objective_at_initial_vertex,\n                      batch_evaluate_objective)\n    domain_dtype = simplex.dtype\n    (\n        reflection,\n        expansion,\n        contraction,\n        shrinkage\n    ) = _resolve_parameters(dim,\n                            reflection,\n                            expansion,\n                            contraction,\n                            shrinkage,\n                            domain_dtype)\n\n    closure_kwargs = dict(\n        objective_function=objective_function,\n        dim=dim,\n        func_tolerance=func_tolerance,\n        position_tolerance=position_tolerance,\n        batch_evaluate_objective=batch_evaluate_objective,\n        reflection=reflection,\n        expansion=expansion,\n        contraction=contraction,\n        shrinkage=shrinkage)\n\n    def _loop_body(_, iterations, simplex, objective_at_simplex,\n                   num_evaluations):\n      (\n          converged,\n          next_simplex,\n          next_objective,\n          evaluations\n      ) = nelder_mead_one_step(simplex, objective_at_simplex, **closure_kwargs)\n\n      return (converged, iterations + 1, next_simplex, next_objective,\n              num_evaluations + evaluations)\n\n    initial_args = (False, 0, simplex, objective_at_simplex,\n                    num_evaluations)\n    # Loop until either we have converged or if the max iterations are supplied\n    # then until we have converged or exhausted the available iteration budget.\n    def _is_converged(converged, num_iterations, *ignored_args):  # pylint:disable=unused-argument\n      # It is important to ensure that not_converged is a tensor. If\n      # converged is not a tensor but a Python bool, then the overloaded\n      # op '~' acts as bitwise complement so ~True = -2 and ~False = -1.\n      # In that case, the loop will never terminate.\n      not_converged = tf.logical_not(converged)\n      return (not_converged if max_iterations is None\n              else (not_converged & (num_iterations < max_iterations)))\n\n    (converged, num_iterations, final_simplex, final_objective_values,\n     final_evaluations) = tf.while_loop(\n         cond=_is_converged,\n         body=_loop_body,\n         loop_vars=initial_args,\n         parallel_iterations=parallel_iterations)\n    order = tf.argsort(\n        final_objective_values, direction='ASCENDING', stable=True)\n    best_index = order[0]\n    # The explicit cast to Tensor below is done to avoid returning a mixture\n    # of Python types and Tensors which cause problems with session.run.\n    # In the eager mode, converged may remain a Python bool. Trying to evaluate\n    # the whole tuple in one evaluate call will raise an exception because\n    # of the presence of non-tensors. This is very annoying so we explicitly\n    # cast those arguments to Tensors.\n    return NelderMeadOptimizerResults(\n        converged=tf.convert_to_tensor(value=converged),\n        num_objective_evaluations=final_evaluations,\n        position=final_simplex[best_index],\n        objective_value=final_objective_values[best_index],\n        final_simplex=final_simplex,\n        final_objective_values=final_objective_values,\n        num_iterations=tf.convert_to_tensor(value=num_iterations),\n        initial_simplex=simplex,\n        initial_objective_values=objective_at_simplex)", "language": "python", "code": "def minimize(objective_function,\n             initial_simplex=None,\n             initial_vertex=None,\n             step_sizes=None,\n             objective_at_initial_simplex=None,\n             objective_at_initial_vertex=None,\n             batch_evaluate_objective=False,\n             func_tolerance=1e-8,\n             position_tolerance=1e-8,\n             parallel_iterations=1,\n             max_iterations=None,\n             reflection=None,\n             expansion=None,\n             contraction=None,\n             shrinkage=None,\n             name=None):\n  \"\"\"Minimum of the objective function using the Nelder Mead simplex algorithm.\n\n  Performs an unconstrained minimization of a (possibly non-smooth) function\n  using the Nelder Mead simplex method. Nelder Mead method does not support\n  univariate functions. Hence the dimensions of the domain must be 2 or greater.\n  For details of the algorithm, see\n  [Press, Teukolsky, Vetterling and Flannery(2007)][1].\n\n  Points in the domain of the objective function may be represented as a\n  `Tensor` of general shape but with rank at least 1. The algorithm proceeds\n  by modifying a full rank simplex in the domain. The initial simplex may\n  either be specified by the user or can be constructed using a single vertex\n  supplied by the user. In the latter case, if `v0` is the supplied vertex,\n  the simplex is the convex hull of the set:\n\n  ```None\n  S = {v0} + {v0 + step_i * e_i}\n  ```\n\n  Here `e_i` is a vector which is `1` along the `i`-th axis and zero elsewhere\n  and `step_i` is a characteristic length scale along the `i`-th axis. If the\n  step size is not supplied by the user, a unit step size is used in every axis.\n  Alternately, a single step size may be specified which is used for every\n  axis. The most flexible option is to supply a bespoke step size for every\n  axis.\n\n  ### Usage:\n\n  The following example demonstrates the usage of the Nelder Mead minimzation\n  on a two dimensional problem with the minimum located at a non-differentiable\n  point.\n\n  ```python\n    # The objective function\n    def sqrt_quadratic(x):\n      return tf.sqrt(tf.reduce_sum(x ** 2, axis=-1))\n\n    start = tf.constant([6.0, -21.0])  # Starting point for the search.\n    optim_results = tfp.optimizer.nelder_mead_minimize(\n        sqrt_quadratic, initial_vertex=start, func_tolerance=1e-8,\n        batch_evaluate_objective=True)\n\n    with tf.Session() as session:\n      results = session.run(optim_results)\n      # Check that the search converged\n      assert(results.converged)\n      # Check that the argmin is close to the actual value.\n      np.testing.assert_allclose(results.position, np.array([0.0, 0.0]),\n                                 atol=1e-7)\n      # Print out the total number of function evaluations it took.\n      print (\"Function evaluations: %d\" % results.num_objective_evaluations)\n  ```\n\n  ### References:\n  [1]: William Press, Saul Teukolsky, William Vetterling and Brian Flannery.\n    Numerical Recipes in C++, third edition. pp. 502-507. (2007).\n    http://numerical.recipes/cpppages/chap0sel.pdf\n\n  [2]: Jeffrey Lagarias, James Reeds, Margaret Wright and Paul Wright.\n    Convergence properties of the Nelder-Mead simplex method in low dimensions,\n    Siam J. Optim., Vol 9, No. 1, pp. 112-147. (1998).\n    http://www.math.kent.edu/~reichel/courses/Opt/reading.material.2/nelder.mead.pdf\n\n  [3]: Fuchang Gao and Lixing Han. Implementing the Nelder-Mead simplex\n    algorithm with adaptive parameters. Computational Optimization and\n    Applications, Vol 51, Issue 1, pp 259-277. (2012).\n    https://pdfs.semanticscholar.org/15b4/c4aa7437df4d032c6ee6ce98d6030dd627be.pdf\n\n  Args:\n    objective_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a `Tensor` of real dtype containing\n      the value of the function at that point. The function\n      to be minimized. If `batch_evaluate_objective` is `True`, the callable\n      may be evaluated on a `Tensor` of shape `[n+1] + s ` where `n` is\n      the dimension of the problem and `s` is the shape of a single point\n      in the domain (so `n` is the size of a `Tensor` representing a\n      single point).\n      In this case, the expected return value is a `Tensor` of shape `[n+1]`.\n      Note that this method does not support univariate functions so the problem\n      dimension `n` must be strictly greater than 1.\n    initial_simplex: (Optional) `Tensor` of real dtype. The initial simplex to\n      start the search. If supplied, should be a `Tensor` of shape `[n+1] + s`\n      where `n` is the dimension of the problem and `s` is the shape of a\n      single point in the domain. Each row (i.e. the `Tensor` with a given\n      value of the first index) is interpreted as a vertex of a simplex and\n      hence the rows must be affinely independent. If not supplied, an axes\n      aligned simplex is constructed using the `initial_vertex` and\n      `step_sizes`. Only one and at least one of `initial_simplex` and\n      `initial_vertex` must be supplied.\n    initial_vertex: (Optional) `Tensor` of real dtype and any shape that can\n      be consumed by the `objective_function`. A single point in the domain that\n      will be used to construct an axes aligned initial simplex.\n    step_sizes: (Optional) `Tensor` of real dtype and shape broadcasting\n      compatible with `initial_vertex`. Supplies the simplex scale along each\n      axes. Only used if `initial_simplex` is not supplied. See description\n      above for details on how step sizes and initial vertex are used to\n      construct the initial simplex.\n    objective_at_initial_simplex: (Optional) Rank `1` `Tensor` of real dtype\n      of a rank `1` `Tensor`. The value of the objective function at the\n      initial simplex. May be supplied only if `initial_simplex` is\n      supplied. If not supplied, it will be computed.\n    objective_at_initial_vertex: (Optional) Scalar `Tensor` of real dtype. The\n      value of the objective function at the initial vertex. May be supplied\n      only if the `initial_vertex` is also supplied.\n    batch_evaluate_objective: (Optional) Python `bool`. If True, the objective\n      function will be evaluated on all the vertices of the simplex packed\n      into a single tensor. If False, the objective will be mapped across each\n      vertex separately. Evaluating the objective function in a batch allows\n      use of vectorization and should be preferred if the objective function\n      allows it.\n    func_tolerance: (Optional) Scalar `Tensor` of real dtype. The algorithm\n      stops if the absolute difference between the largest and the smallest\n      function value on the vertices of the simplex is below this number.\n    position_tolerance: (Optional) Scalar `Tensor` of real dtype. The\n      algorithm stops if the largest absolute difference between the\n      coordinates of the vertices is below this threshold.\n    parallel_iterations: (Optional) Positive integer. The number of iterations\n      allowed to run in parallel.\n    max_iterations: (Optional) Scalar positive `Tensor` of dtype `int32`.\n      The maximum number of iterations allowed. If `None` then no limit is\n      applied.\n    reflection: (Optional) Positive Scalar `Tensor` of same dtype as\n      `initial_vertex`. This parameter controls the scaling of the reflected\n      vertex. See, [Press et al(2007)][1] for details. If not specified,\n      uses the dimension dependent prescription of [Gao and Han(2012)][3].\n    expansion: (Optional) Positive Scalar `Tensor` of same dtype as\n      `initial_vertex`. Should be greater than `1` and `reflection`. This\n      parameter controls the expanded scaling of a reflected vertex.\n      See, [Press et al(2007)][1] for details. If not specified, uses the\n      dimension dependent prescription of [Gao and Han(2012)][3].\n    contraction: (Optional) Positive scalar `Tensor` of same dtype as\n      `initial_vertex`. Must be between `0` and `1`. This parameter controls\n      the contraction of the reflected vertex when the objective function at\n      the reflected point fails to show sufficient decrease.\n      See, [Press et al(2007)][1] for more details. If not specified, uses\n      the dimension dependent prescription of [Gao and Han(2012][3].\n    shrinkage: (Optional) Positive scalar `Tensor` of same dtype as\n      `initial_vertex`. Must be between `0` and `1`. This parameter is the scale\n      by which the simplex is shrunk around the best point when the other\n      steps fail to produce improvements.\n      See, [Press et al(2007)][1] for more details. If not specified, uses\n      the dimension dependent prescription of [Gao and Han(2012][3].\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'minimize' is used.\n\n  Returns:\n    optimizer_results: A namedtuple containing the following items:\n      converged: Scalar boolean tensor indicating whether the minimum was\n        found within tolerance.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A `Tensor` containing the last argument value found\n        during the search. If the search converged, then\n        this value is the argmin of the objective function.\n      objective_value: A tensor containing the value of the objective\n        function at the `position`. If the search\n        converged, then this is the (local) minimum of\n        the objective function.\n      final_simplex: The last simplex constructed before stopping.\n      final_objective_values: The objective function evaluated at the\n        vertices of the final simplex.\n      initial_simplex: The starting simplex.\n      initial_objective_values: The objective function evaluated at the\n        vertices of the initial simplex.\n      num_iterations: The number of iterations of the main algorithm body.\n\n  Raises:\n    ValueError: If any of the following conditions hold\n      1. If none or more than one of `initial_simplex` and `initial_vertex` are\n        supplied.\n      2. If `initial_simplex` and `step_sizes` are both specified.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'minimize', [\n      initial_simplex, initial_vertex, step_sizes, objective_at_initial_simplex,\n      objective_at_initial_vertex, func_tolerance, position_tolerance\n  ]):\n    (\n        dim,\n        _,\n        simplex,\n        objective_at_simplex,\n        num_evaluations\n    ) = _prepare_args(objective_function,\n                      initial_simplex,\n                      initial_vertex,\n                      step_sizes,\n                      objective_at_initial_simplex,\n                      objective_at_initial_vertex,\n                      batch_evaluate_objective)\n    domain_dtype = simplex.dtype\n    (\n        reflection,\n        expansion,\n        contraction,\n        shrinkage\n    ) = _resolve_parameters(dim,\n                            reflection,\n                            expansion,\n                            contraction,\n                            shrinkage,\n                            domain_dtype)\n\n    closure_kwargs = dict(\n        objective_function=objective_function,\n        dim=dim,\n        func_tolerance=func_tolerance,\n        position_tolerance=position_tolerance,\n        batch_evaluate_objective=batch_evaluate_objective,\n        reflection=reflection,\n        expansion=expansion,\n        contraction=contraction,\n        shrinkage=shrinkage)\n\n    def _loop_body(_, iterations, simplex, objective_at_simplex,\n                   num_evaluations):\n      (\n          converged,\n          next_simplex,\n          next_objective,\n          evaluations\n      ) = nelder_mead_one_step(simplex, objective_at_simplex, **closure_kwargs)\n\n      return (converged, iterations + 1, next_simplex, next_objective,\n              num_evaluations + evaluations)\n\n    initial_args = (False, 0, simplex, objective_at_simplex,\n                    num_evaluations)\n    # Loop until either we have converged or if the max iterations are supplied\n    # then until we have converged or exhausted the available iteration budget.\n    def _is_converged(converged, num_iterations, *ignored_args):  # pylint:disable=unused-argument\n      # It is important to ensure that not_converged is a tensor. If\n      # converged is not a tensor but a Python bool, then the overloaded\n      # op '~' acts as bitwise complement so ~True = -2 and ~False = -1.\n      # In that case, the loop will never terminate.\n      not_converged = tf.logical_not(converged)\n      return (not_converged if max_iterations is None\n              else (not_converged & (num_iterations < max_iterations)))\n\n    (converged, num_iterations, final_simplex, final_objective_values,\n     final_evaluations) = tf.while_loop(\n         cond=_is_converged,\n         body=_loop_body,\n         loop_vars=initial_args,\n         parallel_iterations=parallel_iterations)\n    order = tf.argsort(\n        final_objective_values, direction='ASCENDING', stable=True)\n    best_index = order[0]\n    # The explicit cast to Tensor below is done to avoid returning a mixture\n    # of Python types and Tensors which cause problems with session.run.\n    # In the eager mode, converged may remain a Python bool. Trying to evaluate\n    # the whole tuple in one evaluate call will raise an exception because\n    # of the presence of non-tensors. This is very annoying so we explicitly\n    # cast those arguments to Tensors.\n    return NelderMeadOptimizerResults(\n        converged=tf.convert_to_tensor(value=converged),\n        num_objective_evaluations=final_evaluations,\n        position=final_simplex[best_index],\n        objective_value=final_objective_values[best_index],\n        final_simplex=final_simplex,\n        final_objective_values=final_objective_values,\n        num_iterations=tf.convert_to_tensor(value=num_iterations),\n        initial_simplex=simplex,\n        initial_objective_values=objective_at_simplex)", "code_tokens": ["def", "minimize", "(", "objective_function", ",", "initial_simplex", "=", "None", ",", "initial_vertex", "=", "None", ",", "step_sizes", "=", "None", ",", "objective_at_initial_simplex", "=", "None", ",", "objective_at_initial_vertex", "=", "None", ",", "batch_evaluate_objective", "=", "False", ",", "func_tolerance", "=", "1e-8", ",", "position_tolerance", "=", "1e-8", ",", "parallel_iterations", "=", "1", ",", "max_iterations", "=", "None", ",", "reflection", "=", "None", ",", "expansion", "=", "None", ",", "contraction", "=", "None", ",", "shrinkage", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'minimize'", ",", "[", "initial_simplex", ",", "initial_vertex", ",", "step_sizes", ",", "objective_at_initial_simplex", ",", "objective_at_initial_vertex", ",", "func_tolerance", ",", "position_tolerance", "]", ")", ":", "(", "dim", ",", "_", ",", "simplex", ",", "objective_at_simplex", ",", "num_evaluations", ")", "=", "_prepare_args", "(", "objective_function", ",", "initial_simplex", ",", "initial_vertex", ",", "step_sizes", ",", "objective_at_initial_simplex", ",", "objective_at_initial_vertex", ",", "batch_evaluate_objective", ")", "domain_dtype", "=", "simplex", ".", "dtype", "(", "reflection", ",", "expansion", ",", "contraction", ",", "shrinkage", ")", "=", "_resolve_parameters", "(", "dim", ",", "reflection", ",", "expansion", ",", "contraction", ",", "shrinkage", ",", "domain_dtype", ")", "closure_kwargs", "=", "dict", "(", "objective_function", "=", "objective_function", ",", "dim", "=", "dim", ",", "func_tolerance", "=", "func_tolerance", ",", "position_tolerance", "=", "position_tolerance", ",", "batch_evaluate_objective", "=", "batch_evaluate_objective", ",", "reflection", "=", "reflection", ",", "expansion", "=", "expansion", ",", "contraction", "=", "contraction", ",", "shrinkage", "=", "shrinkage", ")", "def", "_loop_body", "(", "_", ",", "iterations", ",", "simplex", ",", "objective_at_simplex", ",", "num_evaluations", ")", ":", "(", "converged", ",", "next_simplex", ",", "next_objective", ",", "evaluations", ")", "=", "nelder_mead_one_step", "(", "simplex", ",", "objective_at_simplex", ",", "*", "*", "closure_kwargs", ")", "return", "(", "converged", ",", "iterations", "+", "1", ",", "next_simplex", ",", "next_objective", ",", "num_evaluations", "+", "evaluations", ")", "initial_args", "=", "(", "False", ",", "0", ",", "simplex", ",", "objective_at_simplex", ",", "num_evaluations", ")", "# Loop until either we have converged or if the max iterations are supplied", "# then until we have converged or exhausted the available iteration budget.", "def", "_is_converged", "(", "converged", ",", "num_iterations", ",", "*", "ignored_args", ")", ":", "# pylint:disable=unused-argument", "# It is important to ensure that not_converged is a tensor. If", "# converged is not a tensor but a Python bool, then the overloaded", "# op '~' acts as bitwise complement so ~True = -2 and ~False = -1.", "# In that case, the loop will never terminate.", "not_converged", "=", "tf", ".", "logical_not", "(", "converged", ")", "return", "(", "not_converged", "if", "max_iterations", "is", "None", "else", "(", "not_converged", "&", "(", "num_iterations", "<", "max_iterations", ")", ")", ")", "(", "converged", ",", "num_iterations", ",", "final_simplex", ",", "final_objective_values", ",", "final_evaluations", ")", "=", "tf", ".", "while_loop", "(", "cond", "=", "_is_converged", ",", "body", "=", "_loop_body", ",", "loop_vars", "=", "initial_args", ",", "parallel_iterations", "=", "parallel_iterations", ")", "order", "=", "tf", ".", "argsort", "(", "final_objective_values", ",", "direction", "=", "'ASCENDING'", ",", "stable", "=", "True", ")", "best_index", "=", "order", "[", "0", "]", "# The explicit cast to Tensor below is done to avoid returning a mixture", "# of Python types and Tensors which cause problems with session.run.", "# In the eager mode, converged may remain a Python bool. Trying to evaluate", "# the whole tuple in one evaluate call will raise an exception because", "# of the presence of non-tensors. This is very annoying so we explicitly", "# cast those arguments to Tensors.", "return", "NelderMeadOptimizerResults", "(", "converged", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "converged", ")", ",", "num_objective_evaluations", "=", "final_evaluations", ",", "position", "=", "final_simplex", "[", "best_index", "]", ",", "objective_value", "=", "final_objective_values", "[", "best_index", "]", ",", "final_simplex", "=", "final_simplex", ",", "final_objective_values", "=", "final_objective_values", ",", "num_iterations", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "num_iterations", ")", ",", "initial_simplex", "=", "simplex", ",", "initial_objective_values", "=", "objective_at_simplex", ")"], "docstring": "Minimum of the objective function using the Nelder Mead simplex algorithm.\n\n  Performs an unconstrained minimization of a (possibly non-smooth) function\n  using the Nelder Mead simplex method. Nelder Mead method does not support\n  univariate functions. Hence the dimensions of the domain must be 2 or greater.\n  For details of the algorithm, see\n  [Press, Teukolsky, Vetterling and Flannery(2007)][1].\n\n  Points in the domain of the objective function may be represented as a\n  `Tensor` of general shape but with rank at least 1. The algorithm proceeds\n  by modifying a full rank simplex in the domain. The initial simplex may\n  either be specified by the user or can be constructed using a single vertex\n  supplied by the user. In the latter case, if `v0` is the supplied vertex,\n  the simplex is the convex hull of the set:\n\n  ```None\n  S = {v0} + {v0 + step_i * e_i}\n  ```\n\n  Here `e_i` is a vector which is `1` along the `i`-th axis and zero elsewhere\n  and `step_i` is a characteristic length scale along the `i`-th axis. If the\n  step size is not supplied by the user, a unit step size is used in every axis.\n  Alternately, a single step size may be specified which is used for every\n  axis. The most flexible option is to supply a bespoke step size for every\n  axis.\n\n  ### Usage:\n\n  The following example demonstrates the usage of the Nelder Mead minimzation\n  on a two dimensional problem with the minimum located at a non-differentiable\n  point.\n\n  ```python\n    # The objective function\n    def sqrt_quadratic(x):\n      return tf.sqrt(tf.reduce_sum(x ** 2, axis=-1))\n\n    start = tf.constant([6.0, -21.0])  # Starting point for the search.\n    optim_results = tfp.optimizer.nelder_mead_minimize(\n        sqrt_quadratic, initial_vertex=start, func_tolerance=1e-8,\n        batch_evaluate_objective=True)\n\n    with tf.Session() as session:\n      results = session.run(optim_results)\n      # Check that the search converged\n      assert(results.converged)\n      # Check that the argmin is close to the actual value.\n      np.testing.assert_allclose(results.position, np.array([0.0, 0.0]),\n                                 atol=1e-7)\n      # Print out the total number of function evaluations it took.\n      print (\"Function evaluations: %d\" % results.num_objective_evaluations)\n  ```\n\n  ### References:\n  [1]: William Press, Saul Teukolsky, William Vetterling and Brian Flannery.\n    Numerical Recipes in C++, third edition. pp. 502-507. (2007).\n    http://numerical.recipes/cpppages/chap0sel.pdf\n\n  [2]: Jeffrey Lagarias, James Reeds, Margaret Wright and Paul Wright.\n    Convergence properties of the Nelder-Mead simplex method in low dimensions,\n    Siam J. Optim., Vol 9, No. 1, pp. 112-147. (1998).\n    http://www.math.kent.edu/~reichel/courses/Opt/reading.material.2/nelder.mead.pdf\n\n  [3]: Fuchang Gao and Lixing Han. Implementing the Nelder-Mead simplex\n    algorithm with adaptive parameters. Computational Optimization and\n    Applications, Vol 51, Issue 1, pp 259-277. (2012).\n    https://pdfs.semanticscholar.org/15b4/c4aa7437df4d032c6ee6ce98d6030dd627be.pdf\n\n  Args:\n    objective_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a `Tensor` of real dtype containing\n      the value of the function at that point. The function\n      to be minimized. If `batch_evaluate_objective` is `True`, the callable\n      may be evaluated on a `Tensor` of shape `[n+1] + s ` where `n` is\n      the dimension of the problem and `s` is the shape of a single point\n      in the domain (so `n` is the size of a `Tensor` representing a\n      single point).\n      In this case, the expected return value is a `Tensor` of shape `[n+1]`.\n      Note that this method does not support univariate functions so the problem\n      dimension `n` must be strictly greater than 1.\n    initial_simplex: (Optional) `Tensor` of real dtype. The initial simplex to\n      start the search. If supplied, should be a `Tensor` of shape `[n+1] + s`\n      where `n` is the dimension of the problem and `s` is the shape of a\n      single point in the domain. Each row (i.e. the `Tensor` with a given\n      value of the first index) is interpreted as a vertex of a simplex and\n      hence the rows must be affinely independent. If not supplied, an axes\n      aligned simplex is constructed using the `initial_vertex` and\n      `step_sizes`. Only one and at least one of `initial_simplex` and\n      `initial_vertex` must be supplied.\n    initial_vertex: (Optional) `Tensor` of real dtype and any shape that can\n      be consumed by the `objective_function`. A single point in the domain that\n      will be used to construct an axes aligned initial simplex.\n    step_sizes: (Optional) `Tensor` of real dtype and shape broadcasting\n      compatible with `initial_vertex`. Supplies the simplex scale along each\n      axes. Only used if `initial_simplex` is not supplied. See description\n      above for details on how step sizes and initial vertex are used to\n      construct the initial simplex.\n    objective_at_initial_simplex: (Optional) Rank `1` `Tensor` of real dtype\n      of a rank `1` `Tensor`. The value of the objective function at the\n      initial simplex. May be supplied only if `initial_simplex` is\n      supplied. If not supplied, it will be computed.\n    objective_at_initial_vertex: (Optional) Scalar `Tensor` of real dtype. The\n      value of the objective function at the initial vertex. May be supplied\n      only if the `initial_vertex` is also supplied.\n    batch_evaluate_objective: (Optional) Python `bool`. If True, the objective\n      function will be evaluated on all the vertices of the simplex packed\n      into a single tensor. If False, the objective will be mapped across each\n      vertex separately. Evaluating the objective function in a batch allows\n      use of vectorization and should be preferred if the objective function\n      allows it.\n    func_tolerance: (Optional) Scalar `Tensor` of real dtype. The algorithm\n      stops if the absolute difference between the largest and the smallest\n      function value on the vertices of the simplex is below this number.\n    position_tolerance: (Optional) Scalar `Tensor` of real dtype. The\n      algorithm stops if the largest absolute difference between the\n      coordinates of the vertices is below this threshold.\n    parallel_iterations: (Optional) Positive integer. The number of iterations\n      allowed to run in parallel.\n    max_iterations: (Optional) Scalar positive `Tensor` of dtype `int32`.\n      The maximum number of iterations allowed. If `None` then no limit is\n      applied.\n    reflection: (Optional) Positive Scalar `Tensor` of same dtype as\n      `initial_vertex`. This parameter controls the scaling of the reflected\n      vertex. See, [Press et al(2007)][1] for details. If not specified,\n      uses the dimension dependent prescription of [Gao and Han(2012)][3].\n    expansion: (Optional) Positive Scalar `Tensor` of same dtype as\n      `initial_vertex`. Should be greater than `1` and `reflection`. This\n      parameter controls the expanded scaling of a reflected vertex.\n      See, [Press et al(2007)][1] for details. If not specified, uses the\n      dimension dependent prescription of [Gao and Han(2012)][3].\n    contraction: (Optional) Positive scalar `Tensor` of same dtype as\n      `initial_vertex`. Must be between `0` and `1`. This parameter controls\n      the contraction of the reflected vertex when the objective function at\n      the reflected point fails to show sufficient decrease.\n      See, [Press et al(2007)][1] for more details. If not specified, uses\n      the dimension dependent prescription of [Gao and Han(2012][3].\n    shrinkage: (Optional) Positive scalar `Tensor` of same dtype as\n      `initial_vertex`. Must be between `0` and `1`. This parameter is the scale\n      by which the simplex is shrunk around the best point when the other\n      steps fail to produce improvements.\n      See, [Press et al(2007)][1] for more details. If not specified, uses\n      the dimension dependent prescription of [Gao and Han(2012][3].\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'minimize' is used.\n\n  Returns:\n    optimizer_results: A namedtuple containing the following items:\n      converged: Scalar boolean tensor indicating whether the minimum was\n        found within tolerance.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A `Tensor` containing the last argument value found\n        during the search. If the search converged, then\n        this value is the argmin of the objective function.\n      objective_value: A tensor containing the value of the objective\n        function at the `position`. If the search\n        converged, then this is the (local) minimum of\n        the objective function.\n      final_simplex: The last simplex constructed before stopping.\n      final_objective_values: The objective function evaluated at the\n        vertices of the final simplex.\n      initial_simplex: The starting simplex.\n      initial_objective_values: The objective function evaluated at the\n        vertices of the initial simplex.\n      num_iterations: The number of iterations of the main algorithm body.\n\n  Raises:\n    ValueError: If any of the following conditions hold\n      1. If none or more than one of `initial_simplex` and `initial_vertex` are\n        supplied.\n      2. If `initial_simplex` and `step_sizes` are both specified.", "docstring_tokens": ["Minimum", "of", "the", "objective", "function", "using", "the", "Nelder", "Mead", "simplex", "algorithm", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/nelder_mead.py#L62-L340", "partition": "test", "index": 858, "time": "2018-07-03 10:33:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/nelder_mead.py", "func_name": "nelder_mead_one_step", "original_string": "def nelder_mead_one_step(current_simplex,\n                         current_objective_values,\n                         objective_function=None,\n                         dim=None,\n                         func_tolerance=None,\n                         position_tolerance=None,\n                         batch_evaluate_objective=False,\n                         reflection=None,\n                         expansion=None,\n                         contraction=None,\n                         shrinkage=None,\n                         name=None):\n  \"\"\"A single iteration of the Nelder Mead algorithm.\"\"\"\n  with tf.compat.v1.name_scope(name, 'nelder_mead_one_step'):\n    domain_dtype = current_simplex.dtype.base_dtype\n    order = tf.argsort(\n        current_objective_values, direction='ASCENDING', stable=True)\n    (\n        best_index,\n        worst_index,\n        second_worst_index\n    ) = order[0], order[-1], order[-2]\n\n    worst_vertex = current_simplex[worst_index]\n\n    (\n        best_objective_value,\n        worst_objective_value,\n        second_worst_objective_value\n    ) = (\n        current_objective_values[best_index],\n        current_objective_values[worst_index],\n        current_objective_values[second_worst_index]\n    )\n\n    # Compute the centroid of the face opposite the worst vertex.\n    face_centroid = tf.reduce_sum(\n        input_tensor=current_simplex, axis=0) - worst_vertex\n    face_centroid /= tf.cast(dim, domain_dtype)\n\n    # Reflect the worst vertex through the opposite face.\n    reflected = face_centroid + reflection * (face_centroid - worst_vertex)\n    objective_at_reflected = objective_function(reflected)\n\n    num_evaluations = 1\n    has_converged = _check_convergence(current_simplex,\n                                       current_simplex[best_index],\n                                       best_objective_value,\n                                       worst_objective_value,\n                                       func_tolerance,\n                                       position_tolerance)\n    def _converged_fn():\n      return (True, current_simplex, current_objective_values, 0)\n    case0 = has_converged, _converged_fn\n    accept_reflected = (\n        (objective_at_reflected < second_worst_objective_value) &\n        (objective_at_reflected >= best_objective_value))\n    accept_reflected_fn = _accept_reflected_fn(current_simplex,\n                                               current_objective_values,\n                                               worst_index,\n                                               reflected,\n                                               objective_at_reflected)\n    case1 = accept_reflected, accept_reflected_fn\n    do_expansion = objective_at_reflected < best_objective_value\n    expansion_fn = _expansion_fn(objective_function,\n                                 current_simplex,\n                                 current_objective_values,\n                                 worst_index,\n                                 reflected,\n                                 objective_at_reflected,\n                                 face_centroid,\n                                 expansion)\n    case2 = do_expansion, expansion_fn\n    do_outside_contraction = (\n        (objective_at_reflected < worst_objective_value) &\n        (objective_at_reflected >= second_worst_objective_value)\n    )\n    outside_contraction_fn = _outside_contraction_fn(\n        objective_function,\n        current_simplex,\n        current_objective_values,\n        face_centroid,\n        best_index,\n        worst_index,\n        reflected,\n        objective_at_reflected,\n        contraction,\n        shrinkage,\n        batch_evaluate_objective)\n    case3 = do_outside_contraction, outside_contraction_fn\n    default_fn = _inside_contraction_fn(objective_function,\n                                        current_simplex,\n                                        current_objective_values,\n                                        face_centroid,\n                                        best_index,\n                                        worst_index,\n                                        worst_objective_value,\n                                        contraction,\n                                        shrinkage,\n                                        batch_evaluate_objective)\n    (\n        converged,\n        next_simplex,\n        next_objective_at_simplex,\n        case_evals) = prefer_static.case([case0, case1, case2, case3],\n                                         default=default_fn, exclusive=False)\n    next_simplex.set_shape(current_simplex.shape)\n    next_objective_at_simplex.set_shape(current_objective_values.shape)\n    return (\n        converged,\n        next_simplex,\n        next_objective_at_simplex,\n        num_evaluations + case_evals\n    )", "language": "python", "code": "def nelder_mead_one_step(current_simplex,\n                         current_objective_values,\n                         objective_function=None,\n                         dim=None,\n                         func_tolerance=None,\n                         position_tolerance=None,\n                         batch_evaluate_objective=False,\n                         reflection=None,\n                         expansion=None,\n                         contraction=None,\n                         shrinkage=None,\n                         name=None):\n  \"\"\"A single iteration of the Nelder Mead algorithm.\"\"\"\n  with tf.compat.v1.name_scope(name, 'nelder_mead_one_step'):\n    domain_dtype = current_simplex.dtype.base_dtype\n    order = tf.argsort(\n        current_objective_values, direction='ASCENDING', stable=True)\n    (\n        best_index,\n        worst_index,\n        second_worst_index\n    ) = order[0], order[-1], order[-2]\n\n    worst_vertex = current_simplex[worst_index]\n\n    (\n        best_objective_value,\n        worst_objective_value,\n        second_worst_objective_value\n    ) = (\n        current_objective_values[best_index],\n        current_objective_values[worst_index],\n        current_objective_values[second_worst_index]\n    )\n\n    # Compute the centroid of the face opposite the worst vertex.\n    face_centroid = tf.reduce_sum(\n        input_tensor=current_simplex, axis=0) - worst_vertex\n    face_centroid /= tf.cast(dim, domain_dtype)\n\n    # Reflect the worst vertex through the opposite face.\n    reflected = face_centroid + reflection * (face_centroid - worst_vertex)\n    objective_at_reflected = objective_function(reflected)\n\n    num_evaluations = 1\n    has_converged = _check_convergence(current_simplex,\n                                       current_simplex[best_index],\n                                       best_objective_value,\n                                       worst_objective_value,\n                                       func_tolerance,\n                                       position_tolerance)\n    def _converged_fn():\n      return (True, current_simplex, current_objective_values, 0)\n    case0 = has_converged, _converged_fn\n    accept_reflected = (\n        (objective_at_reflected < second_worst_objective_value) &\n        (objective_at_reflected >= best_objective_value))\n    accept_reflected_fn = _accept_reflected_fn(current_simplex,\n                                               current_objective_values,\n                                               worst_index,\n                                               reflected,\n                                               objective_at_reflected)\n    case1 = accept_reflected, accept_reflected_fn\n    do_expansion = objective_at_reflected < best_objective_value\n    expansion_fn = _expansion_fn(objective_function,\n                                 current_simplex,\n                                 current_objective_values,\n                                 worst_index,\n                                 reflected,\n                                 objective_at_reflected,\n                                 face_centroid,\n                                 expansion)\n    case2 = do_expansion, expansion_fn\n    do_outside_contraction = (\n        (objective_at_reflected < worst_objective_value) &\n        (objective_at_reflected >= second_worst_objective_value)\n    )\n    outside_contraction_fn = _outside_contraction_fn(\n        objective_function,\n        current_simplex,\n        current_objective_values,\n        face_centroid,\n        best_index,\n        worst_index,\n        reflected,\n        objective_at_reflected,\n        contraction,\n        shrinkage,\n        batch_evaluate_objective)\n    case3 = do_outside_contraction, outside_contraction_fn\n    default_fn = _inside_contraction_fn(objective_function,\n                                        current_simplex,\n                                        current_objective_values,\n                                        face_centroid,\n                                        best_index,\n                                        worst_index,\n                                        worst_objective_value,\n                                        contraction,\n                                        shrinkage,\n                                        batch_evaluate_objective)\n    (\n        converged,\n        next_simplex,\n        next_objective_at_simplex,\n        case_evals) = prefer_static.case([case0, case1, case2, case3],\n                                         default=default_fn, exclusive=False)\n    next_simplex.set_shape(current_simplex.shape)\n    next_objective_at_simplex.set_shape(current_objective_values.shape)\n    return (\n        converged,\n        next_simplex,\n        next_objective_at_simplex,\n        num_evaluations + case_evals\n    )", "code_tokens": ["def", "nelder_mead_one_step", "(", "current_simplex", ",", "current_objective_values", ",", "objective_function", "=", "None", ",", "dim", "=", "None", ",", "func_tolerance", "=", "None", ",", "position_tolerance", "=", "None", ",", "batch_evaluate_objective", "=", "False", ",", "reflection", "=", "None", ",", "expansion", "=", "None", ",", "contraction", "=", "None", ",", "shrinkage", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'nelder_mead_one_step'", ")", ":", "domain_dtype", "=", "current_simplex", ".", "dtype", ".", "base_dtype", "order", "=", "tf", ".", "argsort", "(", "current_objective_values", ",", "direction", "=", "'ASCENDING'", ",", "stable", "=", "True", ")", "(", "best_index", ",", "worst_index", ",", "second_worst_index", ")", "=", "order", "[", "0", "]", ",", "order", "[", "-", "1", "]", ",", "order", "[", "-", "2", "]", "worst_vertex", "=", "current_simplex", "[", "worst_index", "]", "(", "best_objective_value", ",", "worst_objective_value", ",", "second_worst_objective_value", ")", "=", "(", "current_objective_values", "[", "best_index", "]", ",", "current_objective_values", "[", "worst_index", "]", ",", "current_objective_values", "[", "second_worst_index", "]", ")", "# Compute the centroid of the face opposite the worst vertex.", "face_centroid", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "current_simplex", ",", "axis", "=", "0", ")", "-", "worst_vertex", "face_centroid", "/=", "tf", ".", "cast", "(", "dim", ",", "domain_dtype", ")", "# Reflect the worst vertex through the opposite face.", "reflected", "=", "face_centroid", "+", "reflection", "*", "(", "face_centroid", "-", "worst_vertex", ")", "objective_at_reflected", "=", "objective_function", "(", "reflected", ")", "num_evaluations", "=", "1", "has_converged", "=", "_check_convergence", "(", "current_simplex", ",", "current_simplex", "[", "best_index", "]", ",", "best_objective_value", ",", "worst_objective_value", ",", "func_tolerance", ",", "position_tolerance", ")", "def", "_converged_fn", "(", ")", ":", "return", "(", "True", ",", "current_simplex", ",", "current_objective_values", ",", "0", ")", "case0", "=", "has_converged", ",", "_converged_fn", "accept_reflected", "=", "(", "(", "objective_at_reflected", "<", "second_worst_objective_value", ")", "&", "(", "objective_at_reflected", ">=", "best_objective_value", ")", ")", "accept_reflected_fn", "=", "_accept_reflected_fn", "(", "current_simplex", ",", "current_objective_values", ",", "worst_index", ",", "reflected", ",", "objective_at_reflected", ")", "case1", "=", "accept_reflected", ",", "accept_reflected_fn", "do_expansion", "=", "objective_at_reflected", "<", "best_objective_value", "expansion_fn", "=", "_expansion_fn", "(", "objective_function", ",", "current_simplex", ",", "current_objective_values", ",", "worst_index", ",", "reflected", ",", "objective_at_reflected", ",", "face_centroid", ",", "expansion", ")", "case2", "=", "do_expansion", ",", "expansion_fn", "do_outside_contraction", "=", "(", "(", "objective_at_reflected", "<", "worst_objective_value", ")", "&", "(", "objective_at_reflected", ">=", "second_worst_objective_value", ")", ")", "outside_contraction_fn", "=", "_outside_contraction_fn", "(", "objective_function", ",", "current_simplex", ",", "current_objective_values", ",", "face_centroid", ",", "best_index", ",", "worst_index", ",", "reflected", ",", "objective_at_reflected", ",", "contraction", ",", "shrinkage", ",", "batch_evaluate_objective", ")", "case3", "=", "do_outside_contraction", ",", "outside_contraction_fn", "default_fn", "=", "_inside_contraction_fn", "(", "objective_function", ",", "current_simplex", ",", "current_objective_values", ",", "face_centroid", ",", "best_index", ",", "worst_index", ",", "worst_objective_value", ",", "contraction", ",", "shrinkage", ",", "batch_evaluate_objective", ")", "(", "converged", ",", "next_simplex", ",", "next_objective_at_simplex", ",", "case_evals", ")", "=", "prefer_static", ".", "case", "(", "[", "case0", ",", "case1", ",", "case2", ",", "case3", "]", ",", "default", "=", "default_fn", ",", "exclusive", "=", "False", ")", "next_simplex", ".", "set_shape", "(", "current_simplex", ".", "shape", ")", "next_objective_at_simplex", ".", "set_shape", "(", "current_objective_values", ".", "shape", ")", "return", "(", "converged", ",", "next_simplex", ",", "next_objective_at_simplex", ",", "num_evaluations", "+", "case_evals", ")"], "docstring": "A single iteration of the Nelder Mead algorithm.", "docstring_tokens": ["A", "single", "iteration", "of", "the", "Nelder", "Mead", "algorithm", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/nelder_mead.py#L343-L456", "partition": "test", "index": 859, "time": "2018-07-03 10:33:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/nelder_mead.py", "func_name": "_accept_reflected_fn", "original_string": "def _accept_reflected_fn(simplex,\n                         objective_values,\n                         worst_index,\n                         reflected,\n                         objective_at_reflected):\n  \"\"\"Creates the condition function pair for a reflection to be accepted.\"\"\"\n  def _replace_worst_with_reflected():\n    next_simplex = _replace_at_index(simplex, worst_index, reflected)\n    next_objective_values = _replace_at_index(objective_values, worst_index,\n                                              objective_at_reflected)\n    return False, next_simplex, next_objective_values, 0\n  return _replace_worst_with_reflected", "language": "python", "code": "def _accept_reflected_fn(simplex,\n                         objective_values,\n                         worst_index,\n                         reflected,\n                         objective_at_reflected):\n  \"\"\"Creates the condition function pair for a reflection to be accepted.\"\"\"\n  def _replace_worst_with_reflected():\n    next_simplex = _replace_at_index(simplex, worst_index, reflected)\n    next_objective_values = _replace_at_index(objective_values, worst_index,\n                                              objective_at_reflected)\n    return False, next_simplex, next_objective_values, 0\n  return _replace_worst_with_reflected", "code_tokens": ["def", "_accept_reflected_fn", "(", "simplex", ",", "objective_values", ",", "worst_index", ",", "reflected", ",", "objective_at_reflected", ")", ":", "def", "_replace_worst_with_reflected", "(", ")", ":", "next_simplex", "=", "_replace_at_index", "(", "simplex", ",", "worst_index", ",", "reflected", ")", "next_objective_values", "=", "_replace_at_index", "(", "objective_values", ",", "worst_index", ",", "objective_at_reflected", ")", "return", "False", ",", "next_simplex", ",", "next_objective_values", ",", "0", "return", "_replace_worst_with_reflected"], "docstring": "Creates the condition function pair for a reflection to be accepted.", "docstring_tokens": ["Creates", "the", "condition", "function", "pair", "for", "a", "reflection", "to", "be", "accepted", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/nelder_mead.py#L459-L470", "partition": "test", "index": 860, "time": "2018-07-03 10:33:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/nelder_mead.py", "func_name": "_expansion_fn", "original_string": "def _expansion_fn(objective_function,\n                  simplex,\n                  objective_values,\n                  worst_index,\n                  reflected,\n                  objective_at_reflected,\n                  face_centroid,\n                  expansion):\n  \"\"\"Creates the condition function pair for an expansion.\"\"\"\n  def _expand_and_maybe_replace():\n    \"\"\"Performs the expansion step.\"\"\"\n    expanded = face_centroid + expansion * (reflected - face_centroid)\n    expanded_objective_value = objective_function(expanded)\n    expanded_is_better = (expanded_objective_value <\n                          objective_at_reflected)\n    accept_expanded_fn = lambda: (expanded, expanded_objective_value)\n    accept_reflected_fn = lambda: (reflected, objective_at_reflected)\n    next_pt, next_objective_value = prefer_static.cond(\n        expanded_is_better, accept_expanded_fn, accept_reflected_fn)\n    next_simplex = _replace_at_index(simplex, worst_index, next_pt)\n    next_objective_at_simplex = _replace_at_index(objective_values,\n                                                  worst_index,\n                                                  next_objective_value)\n    return False, next_simplex, next_objective_at_simplex, 1\n  return _expand_and_maybe_replace", "language": "python", "code": "def _expansion_fn(objective_function,\n                  simplex,\n                  objective_values,\n                  worst_index,\n                  reflected,\n                  objective_at_reflected,\n                  face_centroid,\n                  expansion):\n  \"\"\"Creates the condition function pair for an expansion.\"\"\"\n  def _expand_and_maybe_replace():\n    \"\"\"Performs the expansion step.\"\"\"\n    expanded = face_centroid + expansion * (reflected - face_centroid)\n    expanded_objective_value = objective_function(expanded)\n    expanded_is_better = (expanded_objective_value <\n                          objective_at_reflected)\n    accept_expanded_fn = lambda: (expanded, expanded_objective_value)\n    accept_reflected_fn = lambda: (reflected, objective_at_reflected)\n    next_pt, next_objective_value = prefer_static.cond(\n        expanded_is_better, accept_expanded_fn, accept_reflected_fn)\n    next_simplex = _replace_at_index(simplex, worst_index, next_pt)\n    next_objective_at_simplex = _replace_at_index(objective_values,\n                                                  worst_index,\n                                                  next_objective_value)\n    return False, next_simplex, next_objective_at_simplex, 1\n  return _expand_and_maybe_replace", "code_tokens": ["def", "_expansion_fn", "(", "objective_function", ",", "simplex", ",", "objective_values", ",", "worst_index", ",", "reflected", ",", "objective_at_reflected", ",", "face_centroid", ",", "expansion", ")", ":", "def", "_expand_and_maybe_replace", "(", ")", ":", "\"\"\"Performs the expansion step.\"\"\"", "expanded", "=", "face_centroid", "+", "expansion", "*", "(", "reflected", "-", "face_centroid", ")", "expanded_objective_value", "=", "objective_function", "(", "expanded", ")", "expanded_is_better", "=", "(", "expanded_objective_value", "<", "objective_at_reflected", ")", "accept_expanded_fn", "=", "lambda", ":", "(", "expanded", ",", "expanded_objective_value", ")", "accept_reflected_fn", "=", "lambda", ":", "(", "reflected", ",", "objective_at_reflected", ")", "next_pt", ",", "next_objective_value", "=", "prefer_static", ".", "cond", "(", "expanded_is_better", ",", "accept_expanded_fn", ",", "accept_reflected_fn", ")", "next_simplex", "=", "_replace_at_index", "(", "simplex", ",", "worst_index", ",", "next_pt", ")", "next_objective_at_simplex", "=", "_replace_at_index", "(", "objective_values", ",", "worst_index", ",", "next_objective_value", ")", "return", "False", ",", "next_simplex", ",", "next_objective_at_simplex", ",", "1", "return", "_expand_and_maybe_replace"], "docstring": "Creates the condition function pair for an expansion.", "docstring_tokens": ["Creates", "the", "condition", "function", "pair", "for", "an", "expansion", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/nelder_mead.py#L473-L497", "partition": "test", "index": 861, "time": "2018-07-03 10:33:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/nelder_mead.py", "func_name": "_shrink_towards_best", "original_string": "def _shrink_towards_best(objective_function,\n                         simplex,\n                         best_index,\n                         shrinkage,\n                         batch_evaluate_objective):\n  \"\"\"Shrinks the simplex around the best vertex.\"\"\"\n\n  # If the contraction step fails to improve the average objective enough,\n  # the simplex is shrunk towards the best vertex.\n  best_vertex = simplex[best_index]\n  shrunk_simplex = best_vertex + shrinkage * (simplex - best_vertex)\n  objective_at_shrunk_simplex, evals = _evaluate_objective_multiple(\n      objective_function,\n      shrunk_simplex,\n      batch_evaluate_objective)\n  return (False,\n          shrunk_simplex,\n          objective_at_shrunk_simplex,\n          evals)", "language": "python", "code": "def _shrink_towards_best(objective_function,\n                         simplex,\n                         best_index,\n                         shrinkage,\n                         batch_evaluate_objective):\n  \"\"\"Shrinks the simplex around the best vertex.\"\"\"\n\n  # If the contraction step fails to improve the average objective enough,\n  # the simplex is shrunk towards the best vertex.\n  best_vertex = simplex[best_index]\n  shrunk_simplex = best_vertex + shrinkage * (simplex - best_vertex)\n  objective_at_shrunk_simplex, evals = _evaluate_objective_multiple(\n      objective_function,\n      shrunk_simplex,\n      batch_evaluate_objective)\n  return (False,\n          shrunk_simplex,\n          objective_at_shrunk_simplex,\n          evals)", "code_tokens": ["def", "_shrink_towards_best", "(", "objective_function", ",", "simplex", ",", "best_index", ",", "shrinkage", ",", "batch_evaluate_objective", ")", ":", "# If the contraction step fails to improve the average objective enough,", "# the simplex is shrunk towards the best vertex.", "best_vertex", "=", "simplex", "[", "best_index", "]", "shrunk_simplex", "=", "best_vertex", "+", "shrinkage", "*", "(", "simplex", "-", "best_vertex", ")", "objective_at_shrunk_simplex", ",", "evals", "=", "_evaluate_objective_multiple", "(", "objective_function", ",", "shrunk_simplex", ",", "batch_evaluate_objective", ")", "return", "(", "False", ",", "shrunk_simplex", ",", "objective_at_shrunk_simplex", ",", "evals", ")"], "docstring": "Shrinks the simplex around the best vertex.", "docstring_tokens": ["Shrinks", "the", "simplex", "around", "the", "best", "vertex", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/nelder_mead.py#L581-L599", "partition": "test", "index": 863, "time": "2018-07-03 10:33:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/nelder_mead.py", "func_name": "_outside_contraction_fn", "original_string": "def _outside_contraction_fn(objective_function,\n                            simplex,\n                            objective_values,\n                            face_centroid,\n                            best_index,\n                            worst_index,\n                            reflected,\n                            objective_at_reflected,\n                            contraction,\n                            shrinkage,\n                            batch_evaluate_objective):\n  \"\"\"Creates the condition function pair for an outside contraction.\"\"\"\n  def _contraction():\n    \"\"\"Performs a contraction.\"\"\"\n    contracted = face_centroid + contraction * (reflected - face_centroid)\n    objective_at_contracted = objective_function(contracted)\n    is_contracted_acceptable = objective_at_contracted <= objective_at_reflected\n    def _accept_contraction():\n      next_simplex = _replace_at_index(simplex, worst_index, contracted)\n      objective_at_next_simplex = _replace_at_index(\n          objective_values,\n          worst_index,\n          objective_at_contracted)\n      return (False,\n              next_simplex,\n              objective_at_next_simplex,\n              1)\n\n    def _reject_contraction():\n      return _shrink_towards_best(objective_function,\n                                  simplex,\n                                  best_index,\n                                  shrinkage,\n                                  batch_evaluate_objective)\n\n    return prefer_static.cond(is_contracted_acceptable,\n                              _accept_contraction,\n                              _reject_contraction)\n  return _contraction", "language": "python", "code": "def _outside_contraction_fn(objective_function,\n                            simplex,\n                            objective_values,\n                            face_centroid,\n                            best_index,\n                            worst_index,\n                            reflected,\n                            objective_at_reflected,\n                            contraction,\n                            shrinkage,\n                            batch_evaluate_objective):\n  \"\"\"Creates the condition function pair for an outside contraction.\"\"\"\n  def _contraction():\n    \"\"\"Performs a contraction.\"\"\"\n    contracted = face_centroid + contraction * (reflected - face_centroid)\n    objective_at_contracted = objective_function(contracted)\n    is_contracted_acceptable = objective_at_contracted <= objective_at_reflected\n    def _accept_contraction():\n      next_simplex = _replace_at_index(simplex, worst_index, contracted)\n      objective_at_next_simplex = _replace_at_index(\n          objective_values,\n          worst_index,\n          objective_at_contracted)\n      return (False,\n              next_simplex,\n              objective_at_next_simplex,\n              1)\n\n    def _reject_contraction():\n      return _shrink_towards_best(objective_function,\n                                  simplex,\n                                  best_index,\n                                  shrinkage,\n                                  batch_evaluate_objective)\n\n    return prefer_static.cond(is_contracted_acceptable,\n                              _accept_contraction,\n                              _reject_contraction)\n  return _contraction", "code_tokens": ["def", "_outside_contraction_fn", "(", "objective_function", ",", "simplex", ",", "objective_values", ",", "face_centroid", ",", "best_index", ",", "worst_index", ",", "reflected", ",", "objective_at_reflected", ",", "contraction", ",", "shrinkage", ",", "batch_evaluate_objective", ")", ":", "def", "_contraction", "(", ")", ":", "\"\"\"Performs a contraction.\"\"\"", "contracted", "=", "face_centroid", "+", "contraction", "*", "(", "reflected", "-", "face_centroid", ")", "objective_at_contracted", "=", "objective_function", "(", "contracted", ")", "is_contracted_acceptable", "=", "objective_at_contracted", "<=", "objective_at_reflected", "def", "_accept_contraction", "(", ")", ":", "next_simplex", "=", "_replace_at_index", "(", "simplex", ",", "worst_index", ",", "contracted", ")", "objective_at_next_simplex", "=", "_replace_at_index", "(", "objective_values", ",", "worst_index", ",", "objective_at_contracted", ")", "return", "(", "False", ",", "next_simplex", ",", "objective_at_next_simplex", ",", "1", ")", "def", "_reject_contraction", "(", ")", ":", "return", "_shrink_towards_best", "(", "objective_function", ",", "simplex", ",", "best_index", ",", "shrinkage", ",", "batch_evaluate_objective", ")", "return", "prefer_static", ".", "cond", "(", "is_contracted_acceptable", ",", "_accept_contraction", ",", "_reject_contraction", ")", "return", "_contraction"], "docstring": "Creates the condition function pair for an outside contraction.", "docstring_tokens": ["Creates", "the", "condition", "function", "pair", "for", "an", "outside", "contraction", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/nelder_mead.py#L500-L538", "partition": "test", "index": 862, "time": "2018-07-03 10:33:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/nelder_mead.py", "func_name": "_evaluate_objective_multiple", "original_string": "def _evaluate_objective_multiple(objective_function, arg_batch,\n                                 batch_evaluate_objective):\n  \"\"\"Evaluates the objective function on a batch of points.\n\n  If `batch_evaluate_objective` is True, returns\n  `objective function(arg_batch)` else it maps the `objective_function`\n  across the `arg_batch`.\n\n  Args:\n    objective_function: A Python callable that accepts a single `Tensor` of\n      rank 'R > 1' and any shape 's' and returns a scalar `Tensor` of real dtype\n      containing the value of the function at that point. If\n      `batch a `Tensor` of shape `[batch_size] + s ` where `batch_size` is the\n      size of the batch of args. In this case, the expected return value is a\n      `Tensor` of shape `[batch_size]`.\n    arg_batch: A `Tensor` of real dtype. The batch of arguments at which to\n      evaluate the `objective_function`. If `batch_evaluate_objective` is False,\n      `arg_batch` will be unpacked along the zeroth axis and the\n      `objective_function` will be applied to each element.\n    batch_evaluate_objective: `bool`. Whether the `objective_function` can\n      evaluate a batch of arguments at once.\n\n  Returns:\n    A tuple containing:\n      objective_values: A `Tensor` of real dtype and shape `[batch_size]`.\n        The value of the objective function evaluated at the supplied\n        `arg_batch`.\n      num_evaluations: An `int32` scalar `Tensor`containing the number of\n        points on which the objective function was evaluated (i.e `batch_size`).\n  \"\"\"\n  n_points = tf.shape(input=arg_batch)[0]\n  if batch_evaluate_objective:\n    return objective_function(arg_batch), n_points\n  return tf.map_fn(objective_function, arg_batch), n_points", "language": "python", "code": "def _evaluate_objective_multiple(objective_function, arg_batch,\n                                 batch_evaluate_objective):\n  \"\"\"Evaluates the objective function on a batch of points.\n\n  If `batch_evaluate_objective` is True, returns\n  `objective function(arg_batch)` else it maps the `objective_function`\n  across the `arg_batch`.\n\n  Args:\n    objective_function: A Python callable that accepts a single `Tensor` of\n      rank 'R > 1' and any shape 's' and returns a scalar `Tensor` of real dtype\n      containing the value of the function at that point. If\n      `batch a `Tensor` of shape `[batch_size] + s ` where `batch_size` is the\n      size of the batch of args. In this case, the expected return value is a\n      `Tensor` of shape `[batch_size]`.\n    arg_batch: A `Tensor` of real dtype. The batch of arguments at which to\n      evaluate the `objective_function`. If `batch_evaluate_objective` is False,\n      `arg_batch` will be unpacked along the zeroth axis and the\n      `objective_function` will be applied to each element.\n    batch_evaluate_objective: `bool`. Whether the `objective_function` can\n      evaluate a batch of arguments at once.\n\n  Returns:\n    A tuple containing:\n      objective_values: A `Tensor` of real dtype and shape `[batch_size]`.\n        The value of the objective function evaluated at the supplied\n        `arg_batch`.\n      num_evaluations: An `int32` scalar `Tensor`containing the number of\n        points on which the objective function was evaluated (i.e `batch_size`).\n  \"\"\"\n  n_points = tf.shape(input=arg_batch)[0]\n  if batch_evaluate_objective:\n    return objective_function(arg_batch), n_points\n  return tf.map_fn(objective_function, arg_batch), n_points", "code_tokens": ["def", "_evaluate_objective_multiple", "(", "objective_function", ",", "arg_batch", ",", "batch_evaluate_objective", ")", ":", "n_points", "=", "tf", ".", "shape", "(", "input", "=", "arg_batch", ")", "[", "0", "]", "if", "batch_evaluate_objective", ":", "return", "objective_function", "(", "arg_batch", ")", ",", "n_points", "return", "tf", ".", "map_fn", "(", "objective_function", ",", "arg_batch", ")", ",", "n_points"], "docstring": "Evaluates the objective function on a batch of points.\n\n  If `batch_evaluate_objective` is True, returns\n  `objective function(arg_batch)` else it maps the `objective_function`\n  across the `arg_batch`.\n\n  Args:\n    objective_function: A Python callable that accepts a single `Tensor` of\n      rank 'R > 1' and any shape 's' and returns a scalar `Tensor` of real dtype\n      containing the value of the function at that point. If\n      `batch a `Tensor` of shape `[batch_size] + s ` where `batch_size` is the\n      size of the batch of args. In this case, the expected return value is a\n      `Tensor` of shape `[batch_size]`.\n    arg_batch: A `Tensor` of real dtype. The batch of arguments at which to\n      evaluate the `objective_function`. If `batch_evaluate_objective` is False,\n      `arg_batch` will be unpacked along the zeroth axis and the\n      `objective_function` will be applied to each element.\n    batch_evaluate_objective: `bool`. Whether the `objective_function` can\n      evaluate a batch of arguments at once.\n\n  Returns:\n    A tuple containing:\n      objective_values: A `Tensor` of real dtype and shape `[batch_size]`.\n        The value of the objective function evaluated at the supplied\n        `arg_batch`.\n      num_evaluations: An `int32` scalar `Tensor`containing the number of\n        points on which the objective function was evaluated (i.e `batch_size`).", "docstring_tokens": ["Evaluates", "the", "objective", "function", "on", "a", "batch", "of", "points", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/nelder_mead.py#L847-L880", "partition": "test", "index": 869, "time": "2018-07-03 10:33:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/gaussian_process.py", "func_name": "GaussianProcess.get_marginal_distribution", "original_string": "def get_marginal_distribution(self, index_points=None):\n    \"\"\"Compute the marginal of this GP over function values at `index_points`.\n\n    Args:\n      index_points: `float` `Tensor` representing finite (batch of) vector(s) of\n        points in the index set over which the GP is defined. Shape has the form\n        `[b1, ..., bB, e, f1, ..., fF]` where `F` is the number of feature\n        dimensions and must equal `kernel.feature_ndims` and `e` is the number\n        (size) of index points in each batch. Ultimately this distribution\n        corresponds to a `e`-dimensional multivariate normal. The batch shape\n        must be broadcastable with `kernel.batch_shape` and any batch dims\n        yielded by `mean_fn`.\n\n    Returns:\n      marginal: a `Normal` or `MultivariateNormalLinearOperator` distribution,\n        according to whether `index_points` consists of one or many index\n        points, respectively.\n    \"\"\"\n    with self._name_scope('get_marginal_distribution'):\n      # TODO(cgs): consider caching the result here, keyed on `index_points`.\n      index_points = self._get_index_points(index_points)\n      covariance = self._compute_covariance(index_points)\n      loc = self._mean_fn(index_points)\n      # If we're sure the number of index points is 1, we can just construct a\n      # scalar Normal. This has computational benefits and supports things like\n      # CDF that aren't otherwise straightforward to provide.\n      if self._is_univariate_marginal(index_points):\n        scale = tf.sqrt(covariance)\n        # `loc` has a trailing 1 in the shape; squeeze it.\n        loc = tf.squeeze(loc, axis=-1)\n        return normal.Normal(\n            loc=loc,\n            scale=scale,\n            validate_args=self._validate_args,\n            allow_nan_stats=self._allow_nan_stats,\n            name='marginal_distribution')\n      else:\n        scale = tf.linalg.LinearOperatorLowerTriangular(\n            tf.linalg.cholesky(_add_diagonal_shift(covariance, self.jitter)),\n            is_non_singular=True,\n            name='GaussianProcessScaleLinearOperator')\n        return mvn_linear_operator.MultivariateNormalLinearOperator(\n            loc=loc,\n            scale=scale,\n            validate_args=self._validate_args,\n            allow_nan_stats=self._allow_nan_stats,\n            name='marginal_distribution')", "language": "python", "code": "def get_marginal_distribution(self, index_points=None):\n    \"\"\"Compute the marginal of this GP over function values at `index_points`.\n\n    Args:\n      index_points: `float` `Tensor` representing finite (batch of) vector(s) of\n        points in the index set over which the GP is defined. Shape has the form\n        `[b1, ..., bB, e, f1, ..., fF]` where `F` is the number of feature\n        dimensions and must equal `kernel.feature_ndims` and `e` is the number\n        (size) of index points in each batch. Ultimately this distribution\n        corresponds to a `e`-dimensional multivariate normal. The batch shape\n        must be broadcastable with `kernel.batch_shape` and any batch dims\n        yielded by `mean_fn`.\n\n    Returns:\n      marginal: a `Normal` or `MultivariateNormalLinearOperator` distribution,\n        according to whether `index_points` consists of one or many index\n        points, respectively.\n    \"\"\"\n    with self._name_scope('get_marginal_distribution'):\n      # TODO(cgs): consider caching the result here, keyed on `index_points`.\n      index_points = self._get_index_points(index_points)\n      covariance = self._compute_covariance(index_points)\n      loc = self._mean_fn(index_points)\n      # If we're sure the number of index points is 1, we can just construct a\n      # scalar Normal. This has computational benefits and supports things like\n      # CDF that aren't otherwise straightforward to provide.\n      if self._is_univariate_marginal(index_points):\n        scale = tf.sqrt(covariance)\n        # `loc` has a trailing 1 in the shape; squeeze it.\n        loc = tf.squeeze(loc, axis=-1)\n        return normal.Normal(\n            loc=loc,\n            scale=scale,\n            validate_args=self._validate_args,\n            allow_nan_stats=self._allow_nan_stats,\n            name='marginal_distribution')\n      else:\n        scale = tf.linalg.LinearOperatorLowerTriangular(\n            tf.linalg.cholesky(_add_diagonal_shift(covariance, self.jitter)),\n            is_non_singular=True,\n            name='GaussianProcessScaleLinearOperator')\n        return mvn_linear_operator.MultivariateNormalLinearOperator(\n            loc=loc,\n            scale=scale,\n            validate_args=self._validate_args,\n            allow_nan_stats=self._allow_nan_stats,\n            name='marginal_distribution')", "code_tokens": ["def", "get_marginal_distribution", "(", "self", ",", "index_points", "=", "None", ")", ":", "with", "self", ".", "_name_scope", "(", "'get_marginal_distribution'", ")", ":", "# TODO(cgs): consider caching the result here, keyed on `index_points`.", "index_points", "=", "self", ".", "_get_index_points", "(", "index_points", ")", "covariance", "=", "self", ".", "_compute_covariance", "(", "index_points", ")", "loc", "=", "self", ".", "_mean_fn", "(", "index_points", ")", "# If we're sure the number of index points is 1, we can just construct a", "# scalar Normal. This has computational benefits and supports things like", "# CDF that aren't otherwise straightforward to provide.", "if", "self", ".", "_is_univariate_marginal", "(", "index_points", ")", ":", "scale", "=", "tf", ".", "sqrt", "(", "covariance", ")", "# `loc` has a trailing 1 in the shape; squeeze it.", "loc", "=", "tf", ".", "squeeze", "(", "loc", ",", "axis", "=", "-", "1", ")", "return", "normal", ".", "Normal", "(", "loc", "=", "loc", ",", "scale", "=", "scale", ",", "validate_args", "=", "self", ".", "_validate_args", ",", "allow_nan_stats", "=", "self", ".", "_allow_nan_stats", ",", "name", "=", "'marginal_distribution'", ")", "else", ":", "scale", "=", "tf", ".", "linalg", ".", "LinearOperatorLowerTriangular", "(", "tf", ".", "linalg", ".", "cholesky", "(", "_add_diagonal_shift", "(", "covariance", ",", "self", ".", "jitter", ")", ")", ",", "is_non_singular", "=", "True", ",", "name", "=", "'GaussianProcessScaleLinearOperator'", ")", "return", "mvn_linear_operator", ".", "MultivariateNormalLinearOperator", "(", "loc", "=", "loc", ",", "scale", "=", "scale", ",", "validate_args", "=", "self", ".", "_validate_args", ",", "allow_nan_stats", "=", "self", ".", "_allow_nan_stats", ",", "name", "=", "'marginal_distribution'", ")"], "docstring": "Compute the marginal of this GP over function values at `index_points`.\n\n    Args:\n      index_points: `float` `Tensor` representing finite (batch of) vector(s) of\n        points in the index set over which the GP is defined. Shape has the form\n        `[b1, ..., bB, e, f1, ..., fF]` where `F` is the number of feature\n        dimensions and must equal `kernel.feature_ndims` and `e` is the number\n        (size) of index points in each batch. Ultimately this distribution\n        corresponds to a `e`-dimensional multivariate normal. The batch shape\n        must be broadcastable with `kernel.batch_shape` and any batch dims\n        yielded by `mean_fn`.\n\n    Returns:\n      marginal: a `Normal` or `MultivariateNormalLinearOperator` distribution,\n        according to whether `index_points` consists of one or many index\n        points, respectively.", "docstring_tokens": ["Compute", "the", "marginal", "of", "this", "GP", "over", "function", "values", "at", "index_points", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/gaussian_process.py#L320-L366", "partition": "test", "index": 751, "time": "2018-07-04 15:15:48"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/gaussian_process.py", "func_name": "GaussianProcess._is_univariate_marginal", "original_string": "def _is_univariate_marginal(self, index_points):\n    \"\"\"True if the given index_points would yield a univariate marginal.\n\n    Args:\n      index_points: the set of index set locations at which to compute the\n      marginal Gaussian distribution. If this set is of size 1, the marginal is\n      univariate.\n\n    Returns:\n      is_univariate: Boolean indicating whether the marginal is univariate or\n      multivariate. In the case of dynamic shape in the number of index points,\n      defaults to \"multivariate\" since that's the best we can do.\n    \"\"\"\n    num_index_points = tf.compat.dimension_value(\n        index_points.shape[-(self.kernel.feature_ndims + 1)])\n    if num_index_points is None:\n      warnings.warn(\n          'Unable to detect statically whether the number of index_points is '\n          '1. As a result, defaulting to treating the marginal GP at '\n          '`index_points` as a multivariate Gaussian. This makes some methods, '\n          'like `cdf` unavailable.')\n    return num_index_points == 1", "language": "python", "code": "def _is_univariate_marginal(self, index_points):\n    \"\"\"True if the given index_points would yield a univariate marginal.\n\n    Args:\n      index_points: the set of index set locations at which to compute the\n      marginal Gaussian distribution. If this set is of size 1, the marginal is\n      univariate.\n\n    Returns:\n      is_univariate: Boolean indicating whether the marginal is univariate or\n      multivariate. In the case of dynamic shape in the number of index points,\n      defaults to \"multivariate\" since that's the best we can do.\n    \"\"\"\n    num_index_points = tf.compat.dimension_value(\n        index_points.shape[-(self.kernel.feature_ndims + 1)])\n    if num_index_points is None:\n      warnings.warn(\n          'Unable to detect statically whether the number of index_points is '\n          '1. As a result, defaulting to treating the marginal GP at '\n          '`index_points` as a multivariate Gaussian. This makes some methods, '\n          'like `cdf` unavailable.')\n    return num_index_points == 1", "code_tokens": ["def", "_is_univariate_marginal", "(", "self", ",", "index_points", ")", ":", "num_index_points", "=", "tf", ".", "compat", ".", "dimension_value", "(", "index_points", ".", "shape", "[", "-", "(", "self", ".", "kernel", ".", "feature_ndims", "+", "1", ")", "]", ")", "if", "num_index_points", "is", "None", ":", "warnings", ".", "warn", "(", "'Unable to detect statically whether the number of index_points is '", "'1. As a result, defaulting to treating the marginal GP at '", "'`index_points` as a multivariate Gaussian. This makes some methods, '", "'like `cdf` unavailable.'", ")", "return", "num_index_points", "==", "1"], "docstring": "True if the given index_points would yield a univariate marginal.\n\n    Args:\n      index_points: the set of index set locations at which to compute the\n      marginal Gaussian distribution. If this set is of size 1, the marginal is\n      univariate.\n\n    Returns:\n      is_univariate: Boolean indicating whether the marginal is univariate or\n      multivariate. In the case of dynamic shape in the number of index points,\n      defaults to \"multivariate\" since that's the best we can do.", "docstring_tokens": ["True", "if", "the", "given", "index_points", "would", "yield", "a", "univariate", "marginal", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/gaussian_process.py#L286-L307", "partition": "test", "index": 750, "time": "2018-07-04 15:15:48"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/eight_schools_hmc.py", "func_name": "benchmark_eight_schools_hmc", "original_string": "def benchmark_eight_schools_hmc(\n    num_results=int(5e3),\n    num_burnin_steps=int(3e3),\n    num_leapfrog_steps=3,\n    step_size=0.4):\n  \"\"\"Runs HMC on the eight-schools unnormalized posterior.\"\"\"\n\n  num_schools = 8\n  treatment_effects = tf.constant(\n      [28, 8, -3, 7, -1, 1, 18, 12],\n      dtype=np.float32,\n      name='treatment_effects')\n  treatment_stddevs = tf.constant(\n      [15, 10, 16, 11, 9, 11, 10, 18],\n      dtype=np.float32,\n      name='treatment_stddevs')\n\n  def unnormalized_posterior_log_prob(\n      avg_effect, avg_stddev, school_effects_standard):\n    \"\"\"Eight-schools unnormalized log posterior.\"\"\"\n    return eight_schools_joint_log_prob(\n        treatment_effects, treatment_stddevs,\n        avg_effect, avg_stddev, school_effects_standard)\n\n  if tf.executing_eagerly():\n    sample_chain = tf.function(tfp.mcmc.sample_chain)\n  else:\n    sample_chain = tfp.mcmc.sample_chain\n\n  def computation():\n    \"\"\"The benchmark computation.\"\"\"\n    _, kernel_results = sample_chain(\n        num_results=num_results,\n        num_burnin_steps=num_burnin_steps,\n        current_state=(\n            tf.zeros([], name='init_avg_effect'),\n            tf.zeros([], name='init_avg_stddev'),\n            tf.ones([num_schools], name='init_school_effects_standard'),\n        ),\n        kernel=tfp.mcmc.HamiltonianMonteCarlo(\n            target_log_prob_fn=unnormalized_posterior_log_prob,\n            step_size=step_size,\n            num_leapfrog_steps=num_leapfrog_steps))\n\n    return kernel_results.is_accepted\n\n  # Let's force evaluation of graph to ensure build time is not part of our time\n  # trial.\n  is_accepted_tensor = computation()\n  if not tf.executing_eagerly():\n    session = tf.compat.v1.Session()\n    session.run(is_accepted_tensor)\n\n  start_time = time.time()\n  if tf.executing_eagerly():\n    is_accepted = computation()\n  else:\n    is_accepted = session.run(is_accepted_tensor)\n  wall_time = time.time() - start_time\n\n  num_accepted = np.sum(is_accepted)\n  acceptance_rate = np.float32(num_accepted) / np.float32(num_results)\n\n  return dict(\n      iters=(num_results + num_burnin_steps) * num_leapfrog_steps,\n      extras={'acceptance_rate': acceptance_rate},\n      wall_time=wall_time)", "language": "python", "code": "def benchmark_eight_schools_hmc(\n    num_results=int(5e3),\n    num_burnin_steps=int(3e3),\n    num_leapfrog_steps=3,\n    step_size=0.4):\n  \"\"\"Runs HMC on the eight-schools unnormalized posterior.\"\"\"\n\n  num_schools = 8\n  treatment_effects = tf.constant(\n      [28, 8, -3, 7, -1, 1, 18, 12],\n      dtype=np.float32,\n      name='treatment_effects')\n  treatment_stddevs = tf.constant(\n      [15, 10, 16, 11, 9, 11, 10, 18],\n      dtype=np.float32,\n      name='treatment_stddevs')\n\n  def unnormalized_posterior_log_prob(\n      avg_effect, avg_stddev, school_effects_standard):\n    \"\"\"Eight-schools unnormalized log posterior.\"\"\"\n    return eight_schools_joint_log_prob(\n        treatment_effects, treatment_stddevs,\n        avg_effect, avg_stddev, school_effects_standard)\n\n  if tf.executing_eagerly():\n    sample_chain = tf.function(tfp.mcmc.sample_chain)\n  else:\n    sample_chain = tfp.mcmc.sample_chain\n\n  def computation():\n    \"\"\"The benchmark computation.\"\"\"\n    _, kernel_results = sample_chain(\n        num_results=num_results,\n        num_burnin_steps=num_burnin_steps,\n        current_state=(\n            tf.zeros([], name='init_avg_effect'),\n            tf.zeros([], name='init_avg_stddev'),\n            tf.ones([num_schools], name='init_school_effects_standard'),\n        ),\n        kernel=tfp.mcmc.HamiltonianMonteCarlo(\n            target_log_prob_fn=unnormalized_posterior_log_prob,\n            step_size=step_size,\n            num_leapfrog_steps=num_leapfrog_steps))\n\n    return kernel_results.is_accepted\n\n  # Let's force evaluation of graph to ensure build time is not part of our time\n  # trial.\n  is_accepted_tensor = computation()\n  if not tf.executing_eagerly():\n    session = tf.compat.v1.Session()\n    session.run(is_accepted_tensor)\n\n  start_time = time.time()\n  if tf.executing_eagerly():\n    is_accepted = computation()\n  else:\n    is_accepted = session.run(is_accepted_tensor)\n  wall_time = time.time() - start_time\n\n  num_accepted = np.sum(is_accepted)\n  acceptance_rate = np.float32(num_accepted) / np.float32(num_results)\n\n  return dict(\n      iters=(num_results + num_burnin_steps) * num_leapfrog_steps,\n      extras={'acceptance_rate': acceptance_rate},\n      wall_time=wall_time)", "code_tokens": ["def", "benchmark_eight_schools_hmc", "(", "num_results", "=", "int", "(", "5e3", ")", ",", "num_burnin_steps", "=", "int", "(", "3e3", ")", ",", "num_leapfrog_steps", "=", "3", ",", "step_size", "=", "0.4", ")", ":", "num_schools", "=", "8", "treatment_effects", "=", "tf", ".", "constant", "(", "[", "28", ",", "8", ",", "-", "3", ",", "7", ",", "-", "1", ",", "1", ",", "18", ",", "12", "]", ",", "dtype", "=", "np", ".", "float32", ",", "name", "=", "'treatment_effects'", ")", "treatment_stddevs", "=", "tf", ".", "constant", "(", "[", "15", ",", "10", ",", "16", ",", "11", ",", "9", ",", "11", ",", "10", ",", "18", "]", ",", "dtype", "=", "np", ".", "float32", ",", "name", "=", "'treatment_stddevs'", ")", "def", "unnormalized_posterior_log_prob", "(", "avg_effect", ",", "avg_stddev", ",", "school_effects_standard", ")", ":", "\"\"\"Eight-schools unnormalized log posterior.\"\"\"", "return", "eight_schools_joint_log_prob", "(", "treatment_effects", ",", "treatment_stddevs", ",", "avg_effect", ",", "avg_stddev", ",", "school_effects_standard", ")", "if", "tf", ".", "executing_eagerly", "(", ")", ":", "sample_chain", "=", "tf", ".", "function", "(", "tfp", ".", "mcmc", ".", "sample_chain", ")", "else", ":", "sample_chain", "=", "tfp", ".", "mcmc", ".", "sample_chain", "def", "computation", "(", ")", ":", "\"\"\"The benchmark computation.\"\"\"", "_", ",", "kernel_results", "=", "sample_chain", "(", "num_results", "=", "num_results", ",", "num_burnin_steps", "=", "num_burnin_steps", ",", "current_state", "=", "(", "tf", ".", "zeros", "(", "[", "]", ",", "name", "=", "'init_avg_effect'", ")", ",", "tf", ".", "zeros", "(", "[", "]", ",", "name", "=", "'init_avg_stddev'", ")", ",", "tf", ".", "ones", "(", "[", "num_schools", "]", ",", "name", "=", "'init_school_effects_standard'", ")", ",", ")", ",", "kernel", "=", "tfp", ".", "mcmc", ".", "HamiltonianMonteCarlo", "(", "target_log_prob_fn", "=", "unnormalized_posterior_log_prob", ",", "step_size", "=", "step_size", ",", "num_leapfrog_steps", "=", "num_leapfrog_steps", ")", ")", "return", "kernel_results", ".", "is_accepted", "# Let's force evaluation of graph to ensure build time is not part of our time", "# trial.", "is_accepted_tensor", "=", "computation", "(", ")", "if", "not", "tf", ".", "executing_eagerly", "(", ")", ":", "session", "=", "tf", ".", "compat", ".", "v1", ".", "Session", "(", ")", "session", ".", "run", "(", "is_accepted_tensor", ")", "start_time", "=", "time", ".", "time", "(", ")", "if", "tf", ".", "executing_eagerly", "(", ")", ":", "is_accepted", "=", "computation", "(", ")", "else", ":", "is_accepted", "=", "session", ".", "run", "(", "is_accepted_tensor", ")", "wall_time", "=", "time", ".", "time", "(", ")", "-", "start_time", "num_accepted", "=", "np", ".", "sum", "(", "is_accepted", ")", "acceptance_rate", "=", "np", ".", "float32", "(", "num_accepted", ")", "/", "np", ".", "float32", "(", "num_results", ")", "return", "dict", "(", "iters", "=", "(", "num_results", "+", "num_burnin_steps", ")", "*", "num_leapfrog_steps", ",", "extras", "=", "{", "'acceptance_rate'", ":", "acceptance_rate", "}", ",", "wall_time", "=", "wall_time", ")"], "docstring": "Runs HMC on the eight-schools unnormalized posterior.", "docstring_tokens": ["Runs", "HMC", "on", "the", "eight", "-", "schools", "unnormalized", "posterior", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/eight_schools_hmc.py#L63-L129", "partition": "test", "index": 613, "time": "2018-07-05 14:29:33"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/eight_schools_hmc.py", "func_name": "mvn", "original_string": "def mvn(*args, **kwargs):\n  \"\"\"Convenience function to efficiently construct a MultivariateNormalDiag.\"\"\"\n  # Faster than using `tfd.MultivariateNormalDiag`.\n  return tfd.Independent(tfd.Normal(*args, **kwargs),\n                         reinterpreted_batch_ndims=1)", "language": "python", "code": "def mvn(*args, **kwargs):\n  \"\"\"Convenience function to efficiently construct a MultivariateNormalDiag.\"\"\"\n  # Faster than using `tfd.MultivariateNormalDiag`.\n  return tfd.Independent(tfd.Normal(*args, **kwargs),\n                         reinterpreted_batch_ndims=1)", "code_tokens": ["def", "mvn", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "# Faster than using `tfd.MultivariateNormalDiag`.", "return", "tfd", ".", "Independent", "(", "tfd", ".", "Normal", "(", "*", "args", ",", "*", "*", "kwargs", ")", ",", "reinterpreted_batch_ndims", "=", "1", ")"], "docstring": "Convenience function to efficiently construct a MultivariateNormalDiag.", "docstring_tokens": ["Convenience", "function", "to", "efficiently", "construct", "a", "MultivariateNormalDiag", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/eight_schools_hmc.py#L37-L41", "partition": "test", "index": 611, "time": "2018-07-05 14:29:33"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/eight_schools_hmc.py", "func_name": "eight_schools_joint_log_prob", "original_string": "def eight_schools_joint_log_prob(\n    treatment_effects, treatment_stddevs,\n    avg_effect, avg_stddev, school_effects_standard):\n  \"\"\"Eight-schools joint log-prob.\"\"\"\n  rv_avg_effect = tfd.Normal(loc=0., scale=10.)\n  rv_avg_stddev = tfd.Normal(loc=5., scale=1.)\n  rv_school_effects_standard = mvn(\n      loc=tf.zeros_like(school_effects_standard),\n      scale=tf.ones_like(school_effects_standard))\n  rv_treatment_effects = mvn(\n      loc=(avg_effect + tf.exp(avg_stddev) * school_effects_standard),\n      scale=treatment_stddevs)\n  return (\n      rv_avg_effect.log_prob(avg_effect) +\n      rv_avg_stddev.log_prob(avg_stddev) +\n      rv_school_effects_standard.log_prob(school_effects_standard) +\n      rv_treatment_effects.log_prob(treatment_effects))", "language": "python", "code": "def eight_schools_joint_log_prob(\n    treatment_effects, treatment_stddevs,\n    avg_effect, avg_stddev, school_effects_standard):\n  \"\"\"Eight-schools joint log-prob.\"\"\"\n  rv_avg_effect = tfd.Normal(loc=0., scale=10.)\n  rv_avg_stddev = tfd.Normal(loc=5., scale=1.)\n  rv_school_effects_standard = mvn(\n      loc=tf.zeros_like(school_effects_standard),\n      scale=tf.ones_like(school_effects_standard))\n  rv_treatment_effects = mvn(\n      loc=(avg_effect + tf.exp(avg_stddev) * school_effects_standard),\n      scale=treatment_stddevs)\n  return (\n      rv_avg_effect.log_prob(avg_effect) +\n      rv_avg_stddev.log_prob(avg_stddev) +\n      rv_school_effects_standard.log_prob(school_effects_standard) +\n      rv_treatment_effects.log_prob(treatment_effects))", "code_tokens": ["def", "eight_schools_joint_log_prob", "(", "treatment_effects", ",", "treatment_stddevs", ",", "avg_effect", ",", "avg_stddev", ",", "school_effects_standard", ")", ":", "rv_avg_effect", "=", "tfd", ".", "Normal", "(", "loc", "=", "0.", ",", "scale", "=", "10.", ")", "rv_avg_stddev", "=", "tfd", ".", "Normal", "(", "loc", "=", "5.", ",", "scale", "=", "1.", ")", "rv_school_effects_standard", "=", "mvn", "(", "loc", "=", "tf", ".", "zeros_like", "(", "school_effects_standard", ")", ",", "scale", "=", "tf", ".", "ones_like", "(", "school_effects_standard", ")", ")", "rv_treatment_effects", "=", "mvn", "(", "loc", "=", "(", "avg_effect", "+", "tf", ".", "exp", "(", "avg_stddev", ")", "*", "school_effects_standard", ")", ",", "scale", "=", "treatment_stddevs", ")", "return", "(", "rv_avg_effect", ".", "log_prob", "(", "avg_effect", ")", "+", "rv_avg_stddev", ".", "log_prob", "(", "avg_stddev", ")", "+", "rv_school_effects_standard", ".", "log_prob", "(", "school_effects_standard", ")", "+", "rv_treatment_effects", ".", "log_prob", "(", "treatment_effects", ")", ")"], "docstring": "Eight-schools joint log-prob.", "docstring_tokens": ["Eight", "-", "schools", "joint", "log", "-", "prob", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/eight_schools_hmc.py#L44-L60", "partition": "test", "index": 612, "time": "2018-07-05 14:29:33"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/von_mises_fisher.py", "func_name": "VonMisesFisher._mode", "original_string": "def _mode(self):\n    \"\"\"The mode of the von Mises-Fisher distribution is the mean direction.\"\"\"\n    return (self.mean_direction +\n            tf.zeros_like(self.concentration)[..., tf.newaxis])", "language": "python", "code": "def _mode(self):\n    \"\"\"The mode of the von Mises-Fisher distribution is the mean direction.\"\"\"\n    return (self.mean_direction +\n            tf.zeros_like(self.concentration)[..., tf.newaxis])", "code_tokens": ["def", "_mode", "(", "self", ")", ":", "return", "(", "self", ".", "mean_direction", "+", "tf", ".", "zeros_like", "(", "self", ".", "concentration", ")", "[", "...", ",", "tf", ".", "newaxis", "]", ")"], "docstring": "The mode of the von Mises-Fisher distribution is the mean direction.", "docstring_tokens": ["The", "mode", "of", "the", "von", "Mises", "-", "Fisher", "distribution", "is", "the", "mean", "direction", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/von_mises_fisher.py#L302-L305", "partition": "test", "index": 957, "time": "2018-07-24 14:23:26"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/von_mises_fisher.py", "func_name": "VonMisesFisher._log_normalization", "original_string": "def _log_normalization(self):\n    \"\"\"Computes the log-normalizer of the distribution.\"\"\"\n    event_dim = tf.compat.dimension_value(self.event_shape[0])\n    if event_dim is None:\n      raise ValueError('vMF _log_normalizer currently only supports '\n                       'statically known event shape')\n    safe_conc = tf.where(self.concentration > 0,\n                         self.concentration,\n                         tf.ones_like(self.concentration))\n    safe_lognorm = ((event_dim / 2 - 1) * tf.math.log(safe_conc) -\n                    (event_dim / 2) * np.log(2 * np.pi) -\n                    tf.math.log(_bessel_ive(event_dim / 2 - 1, safe_conc)) -\n                    tf.abs(safe_conc))\n    log_nsphere_surface_area = (\n        np.log(2.) + (event_dim / 2) * np.log(np.pi) -\n        tf.math.lgamma(tf.cast(event_dim / 2, self.dtype)))\n    return tf.where(self.concentration > 0,\n                    -safe_lognorm,\n                    log_nsphere_surface_area * tf.ones_like(safe_lognorm))", "language": "python", "code": "def _log_normalization(self):\n    \"\"\"Computes the log-normalizer of the distribution.\"\"\"\n    event_dim = tf.compat.dimension_value(self.event_shape[0])\n    if event_dim is None:\n      raise ValueError('vMF _log_normalizer currently only supports '\n                       'statically known event shape')\n    safe_conc = tf.where(self.concentration > 0,\n                         self.concentration,\n                         tf.ones_like(self.concentration))\n    safe_lognorm = ((event_dim / 2 - 1) * tf.math.log(safe_conc) -\n                    (event_dim / 2) * np.log(2 * np.pi) -\n                    tf.math.log(_bessel_ive(event_dim / 2 - 1, safe_conc)) -\n                    tf.abs(safe_conc))\n    log_nsphere_surface_area = (\n        np.log(2.) + (event_dim / 2) * np.log(np.pi) -\n        tf.math.lgamma(tf.cast(event_dim / 2, self.dtype)))\n    return tf.where(self.concentration > 0,\n                    -safe_lognorm,\n                    log_nsphere_surface_area * tf.ones_like(safe_lognorm))", "code_tokens": ["def", "_log_normalization", "(", "self", ")", ":", "event_dim", "=", "tf", ".", "compat", ".", "dimension_value", "(", "self", ".", "event_shape", "[", "0", "]", ")", "if", "event_dim", "is", "None", ":", "raise", "ValueError", "(", "'vMF _log_normalizer currently only supports '", "'statically known event shape'", ")", "safe_conc", "=", "tf", ".", "where", "(", "self", ".", "concentration", ">", "0", ",", "self", ".", "concentration", ",", "tf", ".", "ones_like", "(", "self", ".", "concentration", ")", ")", "safe_lognorm", "=", "(", "(", "event_dim", "/", "2", "-", "1", ")", "*", "tf", ".", "math", ".", "log", "(", "safe_conc", ")", "-", "(", "event_dim", "/", "2", ")", "*", "np", ".", "log", "(", "2", "*", "np", ".", "pi", ")", "-", "tf", ".", "math", ".", "log", "(", "_bessel_ive", "(", "event_dim", "/", "2", "-", "1", ",", "safe_conc", ")", ")", "-", "tf", ".", "abs", "(", "safe_conc", ")", ")", "log_nsphere_surface_area", "=", "(", "np", ".", "log", "(", "2.", ")", "+", "(", "event_dim", "/", "2", ")", "*", "np", ".", "log", "(", "np", ".", "pi", ")", "-", "tf", ".", "math", ".", "lgamma", "(", "tf", ".", "cast", "(", "event_dim", "/", "2", ",", "self", ".", "dtype", ")", ")", ")", "return", "tf", ".", "where", "(", "self", ".", "concentration", ">", "0", ",", "-", "safe_lognorm", ",", "log_nsphere_surface_area", "*", "tf", ".", "ones_like", "(", "safe_lognorm", ")", ")"], "docstring": "Computes the log-normalizer of the distribution.", "docstring_tokens": ["Computes", "the", "log", "-", "normalizer", "of", "the", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/von_mises_fisher.py#L262-L280", "partition": "test", "index": 956, "time": "2018-07-24 14:23:26"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/von_mises_fisher.py", "func_name": "VonMisesFisher._sample_3d", "original_string": "def _sample_3d(self, n, seed=None):\n    \"\"\"Specialized inversion sampler for 3D.\"\"\"\n    seed = seed_stream.SeedStream(seed, salt='von_mises_fisher_3d')\n    u_shape = tf.concat([[n], self._batch_shape_tensor()], axis=0)\n    z = tf.random.uniform(u_shape, seed=seed(), dtype=self.dtype)\n    # TODO(bjp): Higher-order odd dim analytic CDFs are available in [1], could\n    # be bisected for bounded sampling runtime (i.e. not rejection sampling).\n    # [1]: Inversion sampler via: https://ieeexplore.ieee.org/document/7347705/\n    # The inversion is: u = 1 + log(z + (1-z)*exp(-2*kappa)) / kappa\n    # We must protect against both kappa and z being zero.\n    safe_conc = tf.where(self.concentration > 0,\n                         self.concentration,\n                         tf.ones_like(self.concentration))\n    safe_z = tf.where(z > 0, z, tf.ones_like(z))\n    safe_u = 1 + tf.reduce_logsumexp(\n        input_tensor=[\n            tf.math.log(safe_z),\n            tf.math.log1p(-safe_z) - 2 * safe_conc\n        ],\n        axis=0) / safe_conc\n    # Limit of the above expression as kappa->0 is 2*z-1\n    u = tf.where(self.concentration > tf.zeros_like(safe_u), safe_u,\n                 2 * z - 1)\n    # Limit of the expression as z->0 is -1.\n    u = tf.where(tf.equal(z, 0), -tf.ones_like(u), u)\n    if not self._allow_nan_stats:\n      u = tf.debugging.check_numerics(u, 'u in _sample_3d')\n    return u[..., tf.newaxis]", "language": "python", "code": "def _sample_3d(self, n, seed=None):\n    \"\"\"Specialized inversion sampler for 3D.\"\"\"\n    seed = seed_stream.SeedStream(seed, salt='von_mises_fisher_3d')\n    u_shape = tf.concat([[n], self._batch_shape_tensor()], axis=0)\n    z = tf.random.uniform(u_shape, seed=seed(), dtype=self.dtype)\n    # TODO(bjp): Higher-order odd dim analytic CDFs are available in [1], could\n    # be bisected for bounded sampling runtime (i.e. not rejection sampling).\n    # [1]: Inversion sampler via: https://ieeexplore.ieee.org/document/7347705/\n    # The inversion is: u = 1 + log(z + (1-z)*exp(-2*kappa)) / kappa\n    # We must protect against both kappa and z being zero.\n    safe_conc = tf.where(self.concentration > 0,\n                         self.concentration,\n                         tf.ones_like(self.concentration))\n    safe_z = tf.where(z > 0, z, tf.ones_like(z))\n    safe_u = 1 + tf.reduce_logsumexp(\n        input_tensor=[\n            tf.math.log(safe_z),\n            tf.math.log1p(-safe_z) - 2 * safe_conc\n        ],\n        axis=0) / safe_conc\n    # Limit of the above expression as kappa->0 is 2*z-1\n    u = tf.where(self.concentration > tf.zeros_like(safe_u), safe_u,\n                 2 * z - 1)\n    # Limit of the expression as z->0 is -1.\n    u = tf.where(tf.equal(z, 0), -tf.ones_like(u), u)\n    if not self._allow_nan_stats:\n      u = tf.debugging.check_numerics(u, 'u in _sample_3d')\n    return u[..., tf.newaxis]", "code_tokens": ["def", "_sample_3d", "(", "self", ",", "n", ",", "seed", "=", "None", ")", ":", "seed", "=", "seed_stream", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'von_mises_fisher_3d'", ")", "u_shape", "=", "tf", ".", "concat", "(", "[", "[", "n", "]", ",", "self", ".", "_batch_shape_tensor", "(", ")", "]", ",", "axis", "=", "0", ")", "z", "=", "tf", ".", "random", ".", "uniform", "(", "u_shape", ",", "seed", "=", "seed", "(", ")", ",", "dtype", "=", "self", ".", "dtype", ")", "# TODO(bjp): Higher-order odd dim analytic CDFs are available in [1], could", "# be bisected for bounded sampling runtime (i.e. not rejection sampling).", "# [1]: Inversion sampler via: https://ieeexplore.ieee.org/document/7347705/", "# The inversion is: u = 1 + log(z + (1-z)*exp(-2*kappa)) / kappa", "# We must protect against both kappa and z being zero.", "safe_conc", "=", "tf", ".", "where", "(", "self", ".", "concentration", ">", "0", ",", "self", ".", "concentration", ",", "tf", ".", "ones_like", "(", "self", ".", "concentration", ")", ")", "safe_z", "=", "tf", ".", "where", "(", "z", ">", "0", ",", "z", ",", "tf", ".", "ones_like", "(", "z", ")", ")", "safe_u", "=", "1", "+", "tf", ".", "reduce_logsumexp", "(", "input_tensor", "=", "[", "tf", ".", "math", ".", "log", "(", "safe_z", ")", ",", "tf", ".", "math", ".", "log1p", "(", "-", "safe_z", ")", "-", "2", "*", "safe_conc", "]", ",", "axis", "=", "0", ")", "/", "safe_conc", "# Limit of the above expression as kappa->0 is 2*z-1", "u", "=", "tf", ".", "where", "(", "self", ".", "concentration", ">", "tf", ".", "zeros_like", "(", "safe_u", ")", ",", "safe_u", ",", "2", "*", "z", "-", "1", ")", "# Limit of the expression as z->0 is -1.", "u", "=", "tf", ".", "where", "(", "tf", ".", "equal", "(", "z", ",", "0", ")", ",", "-", "tf", ".", "ones_like", "(", "u", ")", ",", "u", ")", "if", "not", "self", ".", "_allow_nan_stats", ":", "u", "=", "tf", ".", "debugging", ".", "check_numerics", "(", "u", ",", "'u in _sample_3d'", ")", "return", "u", "[", "...", ",", "tf", ".", "newaxis", "]"], "docstring": "Specialized inversion sampler for 3D.", "docstring_tokens": ["Specialized", "inversion", "sampler", "for", "3D", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/von_mises_fisher.py#L358-L385", "partition": "test", "index": 959, "time": "2018-07-24 14:23:26"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/von_mises_fisher.py", "func_name": "VonMisesFisher._rotate", "original_string": "def _rotate(self, samples):\n    \"\"\"Applies a Householder rotation to `samples`.\"\"\"\n    event_dim = (\n        tf.compat.dimension_value(self.event_shape[0]) or\n        self._event_shape_tensor()[0])\n    basis = tf.concat([[1.], tf.zeros([event_dim - 1], dtype=self.dtype)],\n                      axis=0),\n    u = tf.nn.l2_normalize(basis - self.mean_direction, axis=-1)\n    return samples - 2 * tf.reduce_sum(\n        input_tensor=samples * u, axis=-1, keepdims=True) * u", "language": "python", "code": "def _rotate(self, samples):\n    \"\"\"Applies a Householder rotation to `samples`.\"\"\"\n    event_dim = (\n        tf.compat.dimension_value(self.event_shape[0]) or\n        self._event_shape_tensor()[0])\n    basis = tf.concat([[1.], tf.zeros([event_dim - 1], dtype=self.dtype)],\n                      axis=0),\n    u = tf.nn.l2_normalize(basis - self.mean_direction, axis=-1)\n    return samples - 2 * tf.reduce_sum(\n        input_tensor=samples * u, axis=-1, keepdims=True) * u", "code_tokens": ["def", "_rotate", "(", "self", ",", "samples", ")", ":", "event_dim", "=", "(", "tf", ".", "compat", ".", "dimension_value", "(", "self", ".", "event_shape", "[", "0", "]", ")", "or", "self", ".", "_event_shape_tensor", "(", ")", "[", "0", "]", ")", "basis", "=", "tf", ".", "concat", "(", "[", "[", "1.", "]", ",", "tf", ".", "zeros", "(", "[", "event_dim", "-", "1", "]", ",", "dtype", "=", "self", ".", "dtype", ")", "]", ",", "axis", "=", "0", ")", ",", "u", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "basis", "-", "self", ".", "mean_direction", ",", "axis", "=", "-", "1", ")", "return", "samples", "-", "2", "*", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "samples", "*", "u", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "*", "u"], "docstring": "Applies a Householder rotation to `samples`.", "docstring_tokens": ["Applies", "a", "Householder", "rotation", "to", "samples", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/von_mises_fisher.py#L347-L356", "partition": "test", "index": 958, "time": "2018-07-24 14:23:26"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/hmc.py", "func_name": "make_simple_step_size_update_policy", "original_string": "def make_simple_step_size_update_policy(num_adaptation_steps,\n                                        target_rate=0.75,\n                                        decrement_multiplier=0.01,\n                                        increment_multiplier=0.01,\n                                        step_counter=None):\n  \"\"\"Create a function implementing a step-size update policy.\n\n  The simple policy increases or decreases the `step_size_var` based on the\n  average of `exp(minimum(0., log_accept_ratio))`. It is based on\n  [Section 4.2 of Andrieu and Thoms (2008)](\n  https://people.eecs.berkeley.edu/~jordan/sail/readings/andrieu-thoms.pdf).\n\n  The `num_adaptation_steps` argument is set independently of any burnin\n  for the overall chain. In general, adaptation prevents the chain from\n  reaching a stationary distribution, so obtaining consistent samples requires\n  `num_adaptation_steps` be set to a value [somewhat smaller](\n  http://andrewgelman.com/2017/12/15/burn-vs-warm-iterative-simulation-algorithms/#comment-627745)\n  than the number of burnin steps. However, it may sometimes be helpful to set\n  `num_adaptation_steps` to a larger value during development in order to\n  inspect the behavior of the chain during adaptation.\n\n  Args:\n    num_adaptation_steps: Scalar `int` `Tensor` number of initial steps to\n      during which to adjust the step size. This may be greater, less than, or\n      equal to the number of burnin steps. If `None`, the step size is adapted\n      on every step (note this breaks stationarity of the chain!).\n    target_rate: Scalar `Tensor` representing desired `accept_ratio`.\n      Default value: `0.75` (i.e., [center of asymptotically optimal\n      rate](https://arxiv.org/abs/1411.6669)).\n    decrement_multiplier: `Tensor` representing amount to downscale current\n      `step_size`.\n      Default value: `0.01`.\n    increment_multiplier: `Tensor` representing amount to upscale current\n      `step_size`.\n      Default value: `0.01`.\n    step_counter: Scalar `int` `Variable` specifying the current step. The step\n      size is adapted iff `step_counter < num_adaptation_steps`.\n      Default value: if `None`, an internal variable\n        `step_size_adaptation_step_counter` is created and initialized to `-1`.\n\n  Returns:\n    step_size_simple_update_fn: Callable that takes args\n      `step_size_var, kernel_results` and returns updated step size(s).\n  \"\"\"\n  if step_counter is None and num_adaptation_steps is not None:\n    step_counter = tf.compat.v1.get_variable(\n        name='step_size_adaptation_step_counter',\n        initializer=np.array(-1, dtype=np.int32),\n        # Specify the dtype for variable sharing to work correctly\n        # (b/120599991).\n        dtype=tf.int32,\n        trainable=False,\n        use_resource=True)\n\n  def step_size_simple_update_fn(step_size_var, kernel_results):\n    \"\"\"Updates (list of) `step_size` using a standard adaptive MCMC procedure.\n\n    Args:\n      step_size_var: (List of) `tf.Variable`s representing the per `state_part`\n        HMC `step_size`.\n      kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from most recent call to `one_step`.\n\n    Returns:\n      step_size_assign: (List of) `Tensor`(s) representing updated\n        `step_size_var`(s).\n    \"\"\"\n\n    if kernel_results is None:\n      if mcmc_util.is_list_like(step_size_var):\n        return [tf.identity(ss) for ss in step_size_var]\n      return tf.identity(step_size_var)\n    log_n = tf.math.log(\n        tf.cast(\n            tf.size(input=kernel_results.log_accept_ratio),\n            kernel_results.log_accept_ratio.dtype))\n    log_mean_accept_ratio = tf.reduce_logsumexp(\n        input_tensor=tf.minimum(kernel_results.log_accept_ratio, 0.)) - log_n\n    adjustment = tf.where(\n        log_mean_accept_ratio < tf.cast(\n            tf.math.log(target_rate), log_mean_accept_ratio.dtype),\n        -decrement_multiplier / (1. + decrement_multiplier),\n        increment_multiplier)\n\n    def build_assign_op():\n      if mcmc_util.is_list_like(step_size_var):\n        return [\n            ss.assign_add(ss * tf.cast(adjustment, ss.dtype))\n            for ss in step_size_var\n        ]\n      return step_size_var.assign_add(\n          step_size_var * tf.cast(adjustment, step_size_var.dtype))\n\n    if num_adaptation_steps is None:\n      return build_assign_op()\n    else:\n      with tf.control_dependencies([step_counter.assign_add(1)]):\n        return tf.cond(\n            pred=step_counter < num_adaptation_steps,\n            true_fn=build_assign_op,\n            false_fn=lambda: step_size_var)\n\n  return step_size_simple_update_fn", "language": "python", "code": "def make_simple_step_size_update_policy(num_adaptation_steps,\n                                        target_rate=0.75,\n                                        decrement_multiplier=0.01,\n                                        increment_multiplier=0.01,\n                                        step_counter=None):\n  \"\"\"Create a function implementing a step-size update policy.\n\n  The simple policy increases or decreases the `step_size_var` based on the\n  average of `exp(minimum(0., log_accept_ratio))`. It is based on\n  [Section 4.2 of Andrieu and Thoms (2008)](\n  https://people.eecs.berkeley.edu/~jordan/sail/readings/andrieu-thoms.pdf).\n\n  The `num_adaptation_steps` argument is set independently of any burnin\n  for the overall chain. In general, adaptation prevents the chain from\n  reaching a stationary distribution, so obtaining consistent samples requires\n  `num_adaptation_steps` be set to a value [somewhat smaller](\n  http://andrewgelman.com/2017/12/15/burn-vs-warm-iterative-simulation-algorithms/#comment-627745)\n  than the number of burnin steps. However, it may sometimes be helpful to set\n  `num_adaptation_steps` to a larger value during development in order to\n  inspect the behavior of the chain during adaptation.\n\n  Args:\n    num_adaptation_steps: Scalar `int` `Tensor` number of initial steps to\n      during which to adjust the step size. This may be greater, less than, or\n      equal to the number of burnin steps. If `None`, the step size is adapted\n      on every step (note this breaks stationarity of the chain!).\n    target_rate: Scalar `Tensor` representing desired `accept_ratio`.\n      Default value: `0.75` (i.e., [center of asymptotically optimal\n      rate](https://arxiv.org/abs/1411.6669)).\n    decrement_multiplier: `Tensor` representing amount to downscale current\n      `step_size`.\n      Default value: `0.01`.\n    increment_multiplier: `Tensor` representing amount to upscale current\n      `step_size`.\n      Default value: `0.01`.\n    step_counter: Scalar `int` `Variable` specifying the current step. The step\n      size is adapted iff `step_counter < num_adaptation_steps`.\n      Default value: if `None`, an internal variable\n        `step_size_adaptation_step_counter` is created and initialized to `-1`.\n\n  Returns:\n    step_size_simple_update_fn: Callable that takes args\n      `step_size_var, kernel_results` and returns updated step size(s).\n  \"\"\"\n  if step_counter is None and num_adaptation_steps is not None:\n    step_counter = tf.compat.v1.get_variable(\n        name='step_size_adaptation_step_counter',\n        initializer=np.array(-1, dtype=np.int32),\n        # Specify the dtype for variable sharing to work correctly\n        # (b/120599991).\n        dtype=tf.int32,\n        trainable=False,\n        use_resource=True)\n\n  def step_size_simple_update_fn(step_size_var, kernel_results):\n    \"\"\"Updates (list of) `step_size` using a standard adaptive MCMC procedure.\n\n    Args:\n      step_size_var: (List of) `tf.Variable`s representing the per `state_part`\n        HMC `step_size`.\n      kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from most recent call to `one_step`.\n\n    Returns:\n      step_size_assign: (List of) `Tensor`(s) representing updated\n        `step_size_var`(s).\n    \"\"\"\n\n    if kernel_results is None:\n      if mcmc_util.is_list_like(step_size_var):\n        return [tf.identity(ss) for ss in step_size_var]\n      return tf.identity(step_size_var)\n    log_n = tf.math.log(\n        tf.cast(\n            tf.size(input=kernel_results.log_accept_ratio),\n            kernel_results.log_accept_ratio.dtype))\n    log_mean_accept_ratio = tf.reduce_logsumexp(\n        input_tensor=tf.minimum(kernel_results.log_accept_ratio, 0.)) - log_n\n    adjustment = tf.where(\n        log_mean_accept_ratio < tf.cast(\n            tf.math.log(target_rate), log_mean_accept_ratio.dtype),\n        -decrement_multiplier / (1. + decrement_multiplier),\n        increment_multiplier)\n\n    def build_assign_op():\n      if mcmc_util.is_list_like(step_size_var):\n        return [\n            ss.assign_add(ss * tf.cast(adjustment, ss.dtype))\n            for ss in step_size_var\n        ]\n      return step_size_var.assign_add(\n          step_size_var * tf.cast(adjustment, step_size_var.dtype))\n\n    if num_adaptation_steps is None:\n      return build_assign_op()\n    else:\n      with tf.control_dependencies([step_counter.assign_add(1)]):\n        return tf.cond(\n            pred=step_counter < num_adaptation_steps,\n            true_fn=build_assign_op,\n            false_fn=lambda: step_size_var)\n\n  return step_size_simple_update_fn", "code_tokens": ["def", "make_simple_step_size_update_policy", "(", "num_adaptation_steps", ",", "target_rate", "=", "0.75", ",", "decrement_multiplier", "=", "0.01", ",", "increment_multiplier", "=", "0.01", ",", "step_counter", "=", "None", ")", ":", "if", "step_counter", "is", "None", "and", "num_adaptation_steps", "is", "not", "None", ":", "step_counter", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "name", "=", "'step_size_adaptation_step_counter'", ",", "initializer", "=", "np", ".", "array", "(", "-", "1", ",", "dtype", "=", "np", ".", "int32", ")", ",", "# Specify the dtype for variable sharing to work correctly", "# (b/120599991).", "dtype", "=", "tf", ".", "int32", ",", "trainable", "=", "False", ",", "use_resource", "=", "True", ")", "def", "step_size_simple_update_fn", "(", "step_size_var", ",", "kernel_results", ")", ":", "\"\"\"Updates (list of) `step_size` using a standard adaptive MCMC procedure.\n\n    Args:\n      step_size_var: (List of) `tf.Variable`s representing the per `state_part`\n        HMC `step_size`.\n      kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from most recent call to `one_step`.\n\n    Returns:\n      step_size_assign: (List of) `Tensor`(s) representing updated\n        `step_size_var`(s).\n    \"\"\"", "if", "kernel_results", "is", "None", ":", "if", "mcmc_util", ".", "is_list_like", "(", "step_size_var", ")", ":", "return", "[", "tf", ".", "identity", "(", "ss", ")", "for", "ss", "in", "step_size_var", "]", "return", "tf", ".", "identity", "(", "step_size_var", ")", "log_n", "=", "tf", ".", "math", ".", "log", "(", "tf", ".", "cast", "(", "tf", ".", "size", "(", "input", "=", "kernel_results", ".", "log_accept_ratio", ")", ",", "kernel_results", ".", "log_accept_ratio", ".", "dtype", ")", ")", "log_mean_accept_ratio", "=", "tf", ".", "reduce_logsumexp", "(", "input_tensor", "=", "tf", ".", "minimum", "(", "kernel_results", ".", "log_accept_ratio", ",", "0.", ")", ")", "-", "log_n", "adjustment", "=", "tf", ".", "where", "(", "log_mean_accept_ratio", "<", "tf", ".", "cast", "(", "tf", ".", "math", ".", "log", "(", "target_rate", ")", ",", "log_mean_accept_ratio", ".", "dtype", ")", ",", "-", "decrement_multiplier", "/", "(", "1.", "+", "decrement_multiplier", ")", ",", "increment_multiplier", ")", "def", "build_assign_op", "(", ")", ":", "if", "mcmc_util", ".", "is_list_like", "(", "step_size_var", ")", ":", "return", "[", "ss", ".", "assign_add", "(", "ss", "*", "tf", ".", "cast", "(", "adjustment", ",", "ss", ".", "dtype", ")", ")", "for", "ss", "in", "step_size_var", "]", "return", "step_size_var", ".", "assign_add", "(", "step_size_var", "*", "tf", ".", "cast", "(", "adjustment", ",", "step_size_var", ".", "dtype", ")", ")", "if", "num_adaptation_steps", "is", "None", ":", "return", "build_assign_op", "(", ")", "else", ":", "with", "tf", ".", "control_dependencies", "(", "[", "step_counter", ".", "assign_add", "(", "1", ")", "]", ")", ":", "return", "tf", ".", "cond", "(", "pred", "=", "step_counter", "<", "num_adaptation_steps", ",", "true_fn", "=", "build_assign_op", ",", "false_fn", "=", "lambda", ":", "step_size_var", ")", "return", "step_size_simple_update_fn"], "docstring": "Create a function implementing a step-size update policy.\n\n  The simple policy increases or decreases the `step_size_var` based on the\n  average of `exp(minimum(0., log_accept_ratio))`. It is based on\n  [Section 4.2 of Andrieu and Thoms (2008)](\n  https://people.eecs.berkeley.edu/~jordan/sail/readings/andrieu-thoms.pdf).\n\n  The `num_adaptation_steps` argument is set independently of any burnin\n  for the overall chain. In general, adaptation prevents the chain from\n  reaching a stationary distribution, so obtaining consistent samples requires\n  `num_adaptation_steps` be set to a value [somewhat smaller](\n  http://andrewgelman.com/2017/12/15/burn-vs-warm-iterative-simulation-algorithms/#comment-627745)\n  than the number of burnin steps. However, it may sometimes be helpful to set\n  `num_adaptation_steps` to a larger value during development in order to\n  inspect the behavior of the chain during adaptation.\n\n  Args:\n    num_adaptation_steps: Scalar `int` `Tensor` number of initial steps to\n      during which to adjust the step size. This may be greater, less than, or\n      equal to the number of burnin steps. If `None`, the step size is adapted\n      on every step (note this breaks stationarity of the chain!).\n    target_rate: Scalar `Tensor` representing desired `accept_ratio`.\n      Default value: `0.75` (i.e., [center of asymptotically optimal\n      rate](https://arxiv.org/abs/1411.6669)).\n    decrement_multiplier: `Tensor` representing amount to downscale current\n      `step_size`.\n      Default value: `0.01`.\n    increment_multiplier: `Tensor` representing amount to upscale current\n      `step_size`.\n      Default value: `0.01`.\n    step_counter: Scalar `int` `Variable` specifying the current step. The step\n      size is adapted iff `step_counter < num_adaptation_steps`.\n      Default value: if `None`, an internal variable\n        `step_size_adaptation_step_counter` is created and initialized to `-1`.\n\n  Returns:\n    step_size_simple_update_fn: Callable that takes args\n      `step_size_var, kernel_results` and returns updated step size(s).", "docstring_tokens": ["Create", "a", "function", "implementing", "a", "step", "-", "size", "update", "policy", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/hmc.py#L60-L162", "partition": "test", "index": 1005, "time": "2018-07-25 13:43:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/replica_exchange_mc.py", "func_name": "_get_field", "original_string": "def _get_field(kernel_results, field_name):\n  \"\"\"field_name from kernel_results or kernel_results.accepted_results.\"\"\"\n  if hasattr(kernel_results, field_name):\n    return getattr(kernel_results, field_name)\n  if hasattr(kernel_results, 'accepted_results'):\n    return getattr(kernel_results.accepted_results, field_name)\n  raise TypeError('Cannot extract %s from %s' % (field_name, kernel_results))", "language": "python", "code": "def _get_field(kernel_results, field_name):\n  \"\"\"field_name from kernel_results or kernel_results.accepted_results.\"\"\"\n  if hasattr(kernel_results, field_name):\n    return getattr(kernel_results, field_name)\n  if hasattr(kernel_results, 'accepted_results'):\n    return getattr(kernel_results.accepted_results, field_name)\n  raise TypeError('Cannot extract %s from %s' % (field_name, kernel_results))", "code_tokens": ["def", "_get_field", "(", "kernel_results", ",", "field_name", ")", ":", "if", "hasattr", "(", "kernel_results", ",", "field_name", ")", ":", "return", "getattr", "(", "kernel_results", ",", "field_name", ")", "if", "hasattr", "(", "kernel_results", ",", "'accepted_results'", ")", ":", "return", "getattr", "(", "kernel_results", ".", "accepted_results", ",", "field_name", ")", "raise", "TypeError", "(", "'Cannot extract %s from %s'", "%", "(", "field_name", ",", "kernel_results", ")", ")"], "docstring": "field_name from kernel_results or kernel_results.accepted_results.", "docstring_tokens": ["field_name", "from", "kernel_results", "or", "kernel_results", ".", "accepted_results", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/replica_exchange_mc.py#L569-L575", "partition": "test", "index": 987, "time": "2018-07-25 18:01:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/replica_exchange_mc.py", "func_name": "ReplicaExchangeMC._get_exchanged_states", "original_string": "def _get_exchanged_states(self, old_states, exchange_proposed,\n                            exchange_proposed_n, sampled_replica_states,\n                            sampled_replica_results):\n    \"\"\"Get list of TensorArrays holding exchanged states, and zeros.\"\"\"\n    with tf.compat.v1.name_scope('get_exchanged_states'):\n\n      target_log_probs = []\n      for replica in range(self.num_replica):\n        replica_log_prob = _get_field(sampled_replica_results[replica],\n                                      'target_log_prob')\n        inverse_temp = self.inverse_temperatures[replica]\n        target_log_probs.append(replica_log_prob / inverse_temp)\n      target_log_probs = tf.stack(target_log_probs, axis=0)\n\n      dtype = target_log_probs.dtype\n      num_state_parts = len(sampled_replica_states[0])\n      # exchanged_states[k][i] is Tensor of (new) state part k, for replica i.\n      # The `k` will be known statically, and `i` is a Tensor.\n      # We will insert values into indices `i` for every replica with a proposed\n      # exchange.\n      exchanged_states = [\n          tf.TensorArray(\n              dtype,\n              size=self.num_replica,\n              dynamic_size=False,\n              tensor_array_name='exchanged_states',\n              # State part k has same shape, regardless of replica.  So use 0.\n              element_shape=sampled_replica_states[0][k].shape)\n          for k in range(num_state_parts)\n      ]\n\n      # Draw random variables here, to avoid sampling in the loop (and losing\n      # reproducibility).  This may mean we sample too many, but we will always\n      # have enough.\n      sample_shape = tf.concat(\n          ([self.num_replica // 2], tf.shape(input=target_log_probs)[1:]),\n          axis=0)\n      log_uniforms = tf.math.log(\n          tf.random.uniform(\n              shape=sample_shape, dtype=dtype, seed=self._seed_stream()))\n\n      def _swap(is_exchange_accepted, x, y):\n        \"\"\"Swap batches of x, y where accepted.\"\"\"\n        with tf.compat.v1.name_scope('swap_where_exchange_accepted'):\n          new_x = mcmc_util.choose(is_exchange_accepted, y, x)\n          new_y = mcmc_util.choose(is_exchange_accepted, x, y)\n        return new_x, new_y\n\n      def cond(i, unused_exchanged_states):\n        return i < exchange_proposed_n\n\n      def body(i, exchanged_states):\n        \"\"\"Body of while loop for exchanging states.\"\"\"\n        # Propose exchange between replicas indexed by m and n.\n        m, n = tf.unstack(exchange_proposed[i])\n\n        # Construct log_accept_ratio:  -temp_diff * target_log_prob_diff.\n        # Note target_log_prob_diff = -EnergyDiff (common definition is in terms\n        # of energy).\n        temp_diff = self.inverse_temperatures[m] - self.inverse_temperatures[n]\n        # Difference of target log probs may be +- Inf or NaN.  We want the\n        # product of this with the temperature difference to have \"alt value\" of\n        # -Inf.\n        log_accept_ratio = mcmc_util.safe_sum(\n            [-temp_diff * target_log_probs[m], temp_diff * target_log_probs[n]])\n\n        is_exchange_accepted = log_uniforms[i] < log_accept_ratio\n\n        for k in range(num_state_parts):\n          new_m, new_n = _swap(is_exchange_accepted, old_states[k].read(m),\n                               old_states[k].read(n))\n          exchanged_states[k] = exchanged_states[k].write(m, new_m)\n          exchanged_states[k] = exchanged_states[k].write(n, new_n)\n\n        return i + 1, exchanged_states\n\n      # At this point, exchanged_states[k] is a length num_replicas TensorArray.\n      return tf.while_loop(\n          cond=cond, body=body, loop_vars=[tf.constant(0),\n                                           exchanged_states])[1]", "language": "python", "code": "def _get_exchanged_states(self, old_states, exchange_proposed,\n                            exchange_proposed_n, sampled_replica_states,\n                            sampled_replica_results):\n    \"\"\"Get list of TensorArrays holding exchanged states, and zeros.\"\"\"\n    with tf.compat.v1.name_scope('get_exchanged_states'):\n\n      target_log_probs = []\n      for replica in range(self.num_replica):\n        replica_log_prob = _get_field(sampled_replica_results[replica],\n                                      'target_log_prob')\n        inverse_temp = self.inverse_temperatures[replica]\n        target_log_probs.append(replica_log_prob / inverse_temp)\n      target_log_probs = tf.stack(target_log_probs, axis=0)\n\n      dtype = target_log_probs.dtype\n      num_state_parts = len(sampled_replica_states[0])\n      # exchanged_states[k][i] is Tensor of (new) state part k, for replica i.\n      # The `k` will be known statically, and `i` is a Tensor.\n      # We will insert values into indices `i` for every replica with a proposed\n      # exchange.\n      exchanged_states = [\n          tf.TensorArray(\n              dtype,\n              size=self.num_replica,\n              dynamic_size=False,\n              tensor_array_name='exchanged_states',\n              # State part k has same shape, regardless of replica.  So use 0.\n              element_shape=sampled_replica_states[0][k].shape)\n          for k in range(num_state_parts)\n      ]\n\n      # Draw random variables here, to avoid sampling in the loop (and losing\n      # reproducibility).  This may mean we sample too many, but we will always\n      # have enough.\n      sample_shape = tf.concat(\n          ([self.num_replica // 2], tf.shape(input=target_log_probs)[1:]),\n          axis=0)\n      log_uniforms = tf.math.log(\n          tf.random.uniform(\n              shape=sample_shape, dtype=dtype, seed=self._seed_stream()))\n\n      def _swap(is_exchange_accepted, x, y):\n        \"\"\"Swap batches of x, y where accepted.\"\"\"\n        with tf.compat.v1.name_scope('swap_where_exchange_accepted'):\n          new_x = mcmc_util.choose(is_exchange_accepted, y, x)\n          new_y = mcmc_util.choose(is_exchange_accepted, x, y)\n        return new_x, new_y\n\n      def cond(i, unused_exchanged_states):\n        return i < exchange_proposed_n\n\n      def body(i, exchanged_states):\n        \"\"\"Body of while loop for exchanging states.\"\"\"\n        # Propose exchange between replicas indexed by m and n.\n        m, n = tf.unstack(exchange_proposed[i])\n\n        # Construct log_accept_ratio:  -temp_diff * target_log_prob_diff.\n        # Note target_log_prob_diff = -EnergyDiff (common definition is in terms\n        # of energy).\n        temp_diff = self.inverse_temperatures[m] - self.inverse_temperatures[n]\n        # Difference of target log probs may be +- Inf or NaN.  We want the\n        # product of this with the temperature difference to have \"alt value\" of\n        # -Inf.\n        log_accept_ratio = mcmc_util.safe_sum(\n            [-temp_diff * target_log_probs[m], temp_diff * target_log_probs[n]])\n\n        is_exchange_accepted = log_uniforms[i] < log_accept_ratio\n\n        for k in range(num_state_parts):\n          new_m, new_n = _swap(is_exchange_accepted, old_states[k].read(m),\n                               old_states[k].read(n))\n          exchanged_states[k] = exchanged_states[k].write(m, new_m)\n          exchanged_states[k] = exchanged_states[k].write(n, new_n)\n\n        return i + 1, exchanged_states\n\n      # At this point, exchanged_states[k] is a length num_replicas TensorArray.\n      return tf.while_loop(\n          cond=cond, body=body, loop_vars=[tf.constant(0),\n                                           exchanged_states])[1]", "code_tokens": ["def", "_get_exchanged_states", "(", "self", ",", "old_states", ",", "exchange_proposed", ",", "exchange_proposed_n", ",", "sampled_replica_states", ",", "sampled_replica_results", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'get_exchanged_states'", ")", ":", "target_log_probs", "=", "[", "]", "for", "replica", "in", "range", "(", "self", ".", "num_replica", ")", ":", "replica_log_prob", "=", "_get_field", "(", "sampled_replica_results", "[", "replica", "]", ",", "'target_log_prob'", ")", "inverse_temp", "=", "self", ".", "inverse_temperatures", "[", "replica", "]", "target_log_probs", ".", "append", "(", "replica_log_prob", "/", "inverse_temp", ")", "target_log_probs", "=", "tf", ".", "stack", "(", "target_log_probs", ",", "axis", "=", "0", ")", "dtype", "=", "target_log_probs", ".", "dtype", "num_state_parts", "=", "len", "(", "sampled_replica_states", "[", "0", "]", ")", "# exchanged_states[k][i] is Tensor of (new) state part k, for replica i.", "# The `k` will be known statically, and `i` is a Tensor.", "# We will insert values into indices `i` for every replica with a proposed", "# exchange.", "exchanged_states", "=", "[", "tf", ".", "TensorArray", "(", "dtype", ",", "size", "=", "self", ".", "num_replica", ",", "dynamic_size", "=", "False", ",", "tensor_array_name", "=", "'exchanged_states'", ",", "# State part k has same shape, regardless of replica.  So use 0.", "element_shape", "=", "sampled_replica_states", "[", "0", "]", "[", "k", "]", ".", "shape", ")", "for", "k", "in", "range", "(", "num_state_parts", ")", "]", "# Draw random variables here, to avoid sampling in the loop (and losing", "# reproducibility).  This may mean we sample too many, but we will always", "# have enough.", "sample_shape", "=", "tf", ".", "concat", "(", "(", "[", "self", ".", "num_replica", "//", "2", "]", ",", "tf", ".", "shape", "(", "input", "=", "target_log_probs", ")", "[", "1", ":", "]", ")", ",", "axis", "=", "0", ")", "log_uniforms", "=", "tf", ".", "math", ".", "log", "(", "tf", ".", "random", ".", "uniform", "(", "shape", "=", "sample_shape", ",", "dtype", "=", "dtype", ",", "seed", "=", "self", ".", "_seed_stream", "(", ")", ")", ")", "def", "_swap", "(", "is_exchange_accepted", ",", "x", ",", "y", ")", ":", "\"\"\"Swap batches of x, y where accepted.\"\"\"", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'swap_where_exchange_accepted'", ")", ":", "new_x", "=", "mcmc_util", ".", "choose", "(", "is_exchange_accepted", ",", "y", ",", "x", ")", "new_y", "=", "mcmc_util", ".", "choose", "(", "is_exchange_accepted", ",", "x", ",", "y", ")", "return", "new_x", ",", "new_y", "def", "cond", "(", "i", ",", "unused_exchanged_states", ")", ":", "return", "i", "<", "exchange_proposed_n", "def", "body", "(", "i", ",", "exchanged_states", ")", ":", "\"\"\"Body of while loop for exchanging states.\"\"\"", "# Propose exchange between replicas indexed by m and n.", "m", ",", "n", "=", "tf", ".", "unstack", "(", "exchange_proposed", "[", "i", "]", ")", "# Construct log_accept_ratio:  -temp_diff * target_log_prob_diff.", "# Note target_log_prob_diff = -EnergyDiff (common definition is in terms", "# of energy).", "temp_diff", "=", "self", ".", "inverse_temperatures", "[", "m", "]", "-", "self", ".", "inverse_temperatures", "[", "n", "]", "# Difference of target log probs may be +- Inf or NaN.  We want the", "# product of this with the temperature difference to have \"alt value\" of", "# -Inf.", "log_accept_ratio", "=", "mcmc_util", ".", "safe_sum", "(", "[", "-", "temp_diff", "*", "target_log_probs", "[", "m", "]", ",", "temp_diff", "*", "target_log_probs", "[", "n", "]", "]", ")", "is_exchange_accepted", "=", "log_uniforms", "[", "i", "]", "<", "log_accept_ratio", "for", "k", "in", "range", "(", "num_state_parts", ")", ":", "new_m", ",", "new_n", "=", "_swap", "(", "is_exchange_accepted", ",", "old_states", "[", "k", "]", ".", "read", "(", "m", ")", ",", "old_states", "[", "k", "]", ".", "read", "(", "n", ")", ")", "exchanged_states", "[", "k", "]", "=", "exchanged_states", "[", "k", "]", ".", "write", "(", "m", ",", "new_m", ")", "exchanged_states", "[", "k", "]", "=", "exchanged_states", "[", "k", "]", ".", "write", "(", "n", ",", "new_n", ")", "return", "i", "+", "1", ",", "exchanged_states", "# At this point, exchanged_states[k] is a length num_replicas TensorArray.", "return", "tf", ".", "while_loop", "(", "cond", "=", "cond", ",", "body", "=", "body", ",", "loop_vars", "=", "[", "tf", ".", "constant", "(", "0", ")", ",", "exchanged_states", "]", ")", "[", "1", "]"], "docstring": "Get list of TensorArrays holding exchanged states, and zeros.", "docstring_tokens": ["Get", "list", "of", "TensorArrays", "holding", "exchanged", "states", "and", "zeros", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/replica_exchange_mc.py#L419-L498", "partition": "test", "index": 988, "time": "2018-07-25 18:01:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/lkj.py", "func_name": "LKJ._log_normalization", "original_string": "def _log_normalization(self, name='log_normalization'):\n    \"\"\"Returns the log normalization of an LKJ distribution.\n\n    Args:\n      name: Python `str` name prefixed to Ops created by this function.\n\n    Returns:\n      log_z: A Tensor of the same shape and dtype as `concentration`, containing\n        the corresponding log normalizers.\n    \"\"\"\n    # The formula is from D. Lewandowski et al [1], p. 1999, from the\n    # proof that eqs 16 and 17 are equivalent.\n    with tf.name_scope(name or 'log_normalization_lkj'):\n      logpi = np.log(np.pi)\n      ans = tf.zeros_like(self.concentration)\n      for k in range(1, self.dimension):\n        ans += logpi * (k / 2.)\n        ans += tf.math.lgamma(self.concentration +\n                              (self.dimension - 1 - k) / 2.)\n        ans -= tf.math.lgamma(self.concentration + (self.dimension - 1) / 2.)\n      return ans", "language": "python", "code": "def _log_normalization(self, name='log_normalization'):\n    \"\"\"Returns the log normalization of an LKJ distribution.\n\n    Args:\n      name: Python `str` name prefixed to Ops created by this function.\n\n    Returns:\n      log_z: A Tensor of the same shape and dtype as `concentration`, containing\n        the corresponding log normalizers.\n    \"\"\"\n    # The formula is from D. Lewandowski et al [1], p. 1999, from the\n    # proof that eqs 16 and 17 are equivalent.\n    with tf.name_scope(name or 'log_normalization_lkj'):\n      logpi = np.log(np.pi)\n      ans = tf.zeros_like(self.concentration)\n      for k in range(1, self.dimension):\n        ans += logpi * (k / 2.)\n        ans += tf.math.lgamma(self.concentration +\n                              (self.dimension - 1 - k) / 2.)\n        ans -= tf.math.lgamma(self.concentration + (self.dimension - 1) / 2.)\n      return ans", "code_tokens": ["def", "_log_normalization", "(", "self", ",", "name", "=", "'log_normalization'", ")", ":", "# The formula is from D. Lewandowski et al [1], p. 1999, from the", "# proof that eqs 16 and 17 are equivalent.", "with", "tf", ".", "name_scope", "(", "name", "or", "'log_normalization_lkj'", ")", ":", "logpi", "=", "np", ".", "log", "(", "np", ".", "pi", ")", "ans", "=", "tf", ".", "zeros_like", "(", "self", ".", "concentration", ")", "for", "k", "in", "range", "(", "1", ",", "self", ".", "dimension", ")", ":", "ans", "+=", "logpi", "*", "(", "k", "/", "2.", ")", "ans", "+=", "tf", ".", "math", ".", "lgamma", "(", "self", ".", "concentration", "+", "(", "self", ".", "dimension", "-", "1", "-", "k", ")", "/", "2.", ")", "ans", "-=", "tf", ".", "math", ".", "lgamma", "(", "self", ".", "concentration", "+", "(", "self", ".", "dimension", "-", "1", ")", "/", "2.", ")", "return", "ans"], "docstring": "Returns the log normalization of an LKJ distribution.\n\n    Args:\n      name: Python `str` name prefixed to Ops created by this function.\n\n    Returns:\n      log_z: A Tensor of the same shape and dtype as `concentration`, containing\n        the corresponding log normalizers.", "docstring_tokens": ["Returns", "the", "log", "normalization", "of", "an", "LKJ", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/lkj.py#L412-L432", "partition": "test", "index": 776, "time": "2018-08-04 20:23:14"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/lkj.py", "func_name": "LKJ._log_unnorm_prob", "original_string": "def _log_unnorm_prob(self, x, name=None):\n    \"\"\"Returns the unnormalized log density of an LKJ distribution.\n\n    Args:\n      x: `float` or `double` `Tensor` of correlation matrices.  The shape of `x`\n        must be `B + [D, D]`, where `B` broadcasts with the shape of\n        `concentration`.\n      name: Python `str` name prefixed to Ops created by this function.\n\n    Returns:\n      log_p: A Tensor of the unnormalized log density of each matrix element of\n        `x`, with respect to an LKJ distribution with parameter the\n        corresponding element of `concentration`.\n    \"\"\"\n    with tf.name_scope(name or 'log_unnorm_prob_lkj'):\n      x = tf.convert_to_tensor(value=x, name='x')\n      # The density is det(matrix) ** (concentration - 1).\n      # Computing the determinant with `logdet` is usually fine, since\n      # correlation matrices are Hermitian and PSD. But in some cases, for a\n      # PSD matrix whose eigenvalues are close to zero, `logdet` raises an error\n      # complaining that it is not PSD. The root cause is the computation of the\n      # cholesky decomposition in `logdet`. Hence, we use the less efficient but\n      # more robust `slogdet` which does not use `cholesky`.\n      #\n      # An alternative would have been to check allow_nan_stats and use\n      #   eigenvalues = tf.linalg.self_adjoint_eigvals(x)\n      #   psd_mask = tf.cast(\n      #     tf.reduce_min(eigenvalues, axis=-1) >= 0, dtype=x.dtype)\n      #   tf.where(psd_mask, answer, float('-inf'))\n      # to emit probability 0 for inputs that are not PSD, without ever raising\n      # an error. More care must be taken, as due to numerical stability issues,\n      # self_adjoint_eigvals can return slightly negative eigenvalues even for\n      # a PSD matrix.\n      if self.input_output_cholesky:\n        logdet = 2.0 * tf.reduce_sum(\n            input_tensor=tf.math.log(tf.linalg.diag_part(x)), axis=[-1])\n      else:\n        _, logdet = tf.linalg.slogdet(x)\n      answer = (self.concentration - 1.) * logdet\n      return answer", "language": "python", "code": "def _log_unnorm_prob(self, x, name=None):\n    \"\"\"Returns the unnormalized log density of an LKJ distribution.\n\n    Args:\n      x: `float` or `double` `Tensor` of correlation matrices.  The shape of `x`\n        must be `B + [D, D]`, where `B` broadcasts with the shape of\n        `concentration`.\n      name: Python `str` name prefixed to Ops created by this function.\n\n    Returns:\n      log_p: A Tensor of the unnormalized log density of each matrix element of\n        `x`, with respect to an LKJ distribution with parameter the\n        corresponding element of `concentration`.\n    \"\"\"\n    with tf.name_scope(name or 'log_unnorm_prob_lkj'):\n      x = tf.convert_to_tensor(value=x, name='x')\n      # The density is det(matrix) ** (concentration - 1).\n      # Computing the determinant with `logdet` is usually fine, since\n      # correlation matrices are Hermitian and PSD. But in some cases, for a\n      # PSD matrix whose eigenvalues are close to zero, `logdet` raises an error\n      # complaining that it is not PSD. The root cause is the computation of the\n      # cholesky decomposition in `logdet`. Hence, we use the less efficient but\n      # more robust `slogdet` which does not use `cholesky`.\n      #\n      # An alternative would have been to check allow_nan_stats and use\n      #   eigenvalues = tf.linalg.self_adjoint_eigvals(x)\n      #   psd_mask = tf.cast(\n      #     tf.reduce_min(eigenvalues, axis=-1) >= 0, dtype=x.dtype)\n      #   tf.where(psd_mask, answer, float('-inf'))\n      # to emit probability 0 for inputs that are not PSD, without ever raising\n      # an error. More care must be taken, as due to numerical stability issues,\n      # self_adjoint_eigvals can return slightly negative eigenvalues even for\n      # a PSD matrix.\n      if self.input_output_cholesky:\n        logdet = 2.0 * tf.reduce_sum(\n            input_tensor=tf.math.log(tf.linalg.diag_part(x)), axis=[-1])\n      else:\n        _, logdet = tf.linalg.slogdet(x)\n      answer = (self.concentration - 1.) * logdet\n      return answer", "code_tokens": ["def", "_log_unnorm_prob", "(", "self", ",", "x", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "'log_unnorm_prob_lkj'", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ")", "# The density is det(matrix) ** (concentration - 1).", "# Computing the determinant with `logdet` is usually fine, since", "# correlation matrices are Hermitian and PSD. But in some cases, for a", "# PSD matrix whose eigenvalues are close to zero, `logdet` raises an error", "# complaining that it is not PSD. The root cause is the computation of the", "# cholesky decomposition in `logdet`. Hence, we use the less efficient but", "# more robust `slogdet` which does not use `cholesky`.", "#", "# An alternative would have been to check allow_nan_stats and use", "#   eigenvalues = tf.linalg.self_adjoint_eigvals(x)", "#   psd_mask = tf.cast(", "#     tf.reduce_min(eigenvalues, axis=-1) >= 0, dtype=x.dtype)", "#   tf.where(psd_mask, answer, float('-inf'))", "# to emit probability 0 for inputs that are not PSD, without ever raising", "# an error. More care must be taken, as due to numerical stability issues,", "# self_adjoint_eigvals can return slightly negative eigenvalues even for", "# a PSD matrix.", "if", "self", ".", "input_output_cholesky", ":", "logdet", "=", "2.0", "*", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "math", ".", "log", "(", "tf", ".", "linalg", ".", "diag_part", "(", "x", ")", ")", ",", "axis", "=", "[", "-", "1", "]", ")", "else", ":", "_", ",", "logdet", "=", "tf", ".", "linalg", ".", "slogdet", "(", "x", ")", "answer", "=", "(", "self", ".", "concentration", "-", "1.", ")", "*", "logdet", "return", "answer"], "docstring": "Returns the unnormalized log density of an LKJ distribution.\n\n    Args:\n      x: `float` or `double` `Tensor` of correlation matrices.  The shape of `x`\n        must be `B + [D, D]`, where `B` broadcasts with the shape of\n        `concentration`.\n      name: Python `str` name prefixed to Ops created by this function.\n\n    Returns:\n      log_p: A Tensor of the unnormalized log density of each matrix element of\n        `x`, with respect to an LKJ distribution with parameter the\n        corresponding element of `concentration`.", "docstring_tokens": ["Returns", "the", "unnormalized", "log", "density", "of", "an", "LKJ", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/lkj.py#L371-L410", "partition": "test", "index": 775, "time": "2018-08-04 20:23:14"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/lkj.py", "func_name": "_uniform_unit_norm", "original_string": "def _uniform_unit_norm(dimension, shape, dtype, seed):\n  \"\"\"Returns a batch of points chosen uniformly from the unit hypersphere.\"\"\"\n  # This works because the Gaussian distribution is spherically symmetric.\n  # raw shape: shape + [dimension]\n  raw = normal.Normal(\n      loc=dtype_util.as_numpy_dtype(dtype)(0),\n      scale=dtype_util.as_numpy_dtype(dtype)(1)).sample(\n          tf.concat([shape, [dimension]], axis=0), seed=seed())\n  unit_norm = raw / tf.norm(tensor=raw, ord=2, axis=-1)[..., tf.newaxis]\n  return unit_norm", "language": "python", "code": "def _uniform_unit_norm(dimension, shape, dtype, seed):\n  \"\"\"Returns a batch of points chosen uniformly from the unit hypersphere.\"\"\"\n  # This works because the Gaussian distribution is spherically symmetric.\n  # raw shape: shape + [dimension]\n  raw = normal.Normal(\n      loc=dtype_util.as_numpy_dtype(dtype)(0),\n      scale=dtype_util.as_numpy_dtype(dtype)(1)).sample(\n          tf.concat([shape, [dimension]], axis=0), seed=seed())\n  unit_norm = raw / tf.norm(tensor=raw, ord=2, axis=-1)[..., tf.newaxis]\n  return unit_norm", "code_tokens": ["def", "_uniform_unit_norm", "(", "dimension", ",", "shape", ",", "dtype", ",", "seed", ")", ":", "# This works because the Gaussian distribution is spherically symmetric.", "# raw shape: shape + [dimension]", "raw", "=", "normal", ".", "Normal", "(", "loc", "=", "dtype_util", ".", "as_numpy_dtype", "(", "dtype", ")", "(", "0", ")", ",", "scale", "=", "dtype_util", ".", "as_numpy_dtype", "(", "dtype", ")", "(", "1", ")", ")", ".", "sample", "(", "tf", ".", "concat", "(", "[", "shape", ",", "[", "dimension", "]", "]", ",", "axis", "=", "0", ")", ",", "seed", "=", "seed", "(", ")", ")", "unit_norm", "=", "raw", "/", "tf", ".", "norm", "(", "tensor", "=", "raw", ",", "ord", "=", "2", ",", "axis", "=", "-", "1", ")", "[", "...", ",", "tf", ".", "newaxis", "]", "return", "unit_norm"], "docstring": "Returns a batch of points chosen uniformly from the unit hypersphere.", "docstring_tokens": ["Returns", "a", "batch", "of", "points", "chosen", "uniformly", "from", "the", "unit", "hypersphere", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/lkj.py#L47-L56", "partition": "test", "index": 774, "time": "2018-08-04 20:23:14"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/glm/proximal_hessian.py", "func_name": "_grad_neg_log_likelihood_and_fim", "original_string": "def _grad_neg_log_likelihood_and_fim(model_matrix, linear_response, response,\n                                     model):\n  \"\"\"Computes the neg-log-likelihood gradient and Fisher information for a GLM.\n\n  Note that Fisher information is related to the Hessian of the log-likelihood\n  by the equation\n\n  ```none\n  FisherInfo = E[Hessian with respect to model_coefficients of -LogLikelihood(\n      Y | model_matrix, model_coefficients)]\n  ```\n\n  where `LogLikelihood` is the log-likelihood of a generalized linear model\n  parameterized by `model_matrix` and `model_coefficients`, and the expectation\n  is taken over Y, distributed according to the same GLM with the same parameter\n  values.\n\n  Args:\n    model_matrix: (Batch of) matrix-shaped, `float` `Tensor` or `SparseTensor`\n      where each row represents a sample's features.  Has shape `[N, n]` where\n      `N` is the number of data samples and `n` is the number of features per\n      sample.\n    linear_response: (Batch of) vector-shaped `Tensor` with the same dtype as\n      `model_matrix`, equal to `model_matix @ model_coefficients` where\n      `model_coefficients` are the coefficients of the linear component of the\n      GLM.\n    response: (Batch of) vector-shaped `Tensor` with the same dtype as\n      `model_matrix` where each element represents a sample's observed response\n      (to the corresponding row of features).\n    model: `tfp.glm.ExponentialFamily`-like instance, which specifies the link\n      function and distribution of the GLM, and thus characterizes the negative\n      log-likelihood. Must have sufficient statistic equal to the response, that\n      is, `T(y) = y`.\n\n  Returns:\n    grad_neg_log_likelihood: (Batch of) vector-shaped `Tensor` with the same\n      shape and dtype as a single row of `model_matrix`, representing the\n      gradient of the negative log likelihood of `response` given linear\n      response `linear_response`.\n    fim_middle: (Batch of) vector-shaped `Tensor` with the same shape and dtype\n      as a single column of `model_matrix`, satisfying the equation\n      `Fisher information =\n      Transpose(model_matrix)\n      @ diag(fim_middle)\n      @ model_matrix`.\n  \"\"\"\n  # TODO(b/111926503): Determine whether there are some practical cases where it\n  # is computationally favorable to compute the full FIM.\n  mean, variance, grad_mean = model(linear_response)\n\n  is_valid = (\n      tf.math.is_finite(grad_mean) & tf.not_equal(grad_mean, 0.)\n      & tf.math.is_finite(variance) & (variance > 0.))\n\n  def _mask_if_invalid(x, mask):\n    mask = tf.fill(\n        tf.shape(input=x), value=np.array(mask, x.dtype.as_numpy_dtype))\n    return tf.where(is_valid, x, mask)\n\n  # TODO(b/111923449): Link to derivation once it's available.\n  v = (response - mean) * _mask_if_invalid(grad_mean, 1) / _mask_if_invalid(\n      variance, np.inf)\n  grad_log_likelihood = sparse_or_dense_matvecmul(\n      model_matrix, v, adjoint_a=True)\n  fim_middle = _mask_if_invalid(grad_mean, 0.)**2 / _mask_if_invalid(\n      variance, np.inf)\n  return -grad_log_likelihood, fim_middle", "language": "python", "code": "def _grad_neg_log_likelihood_and_fim(model_matrix, linear_response, response,\n                                     model):\n  \"\"\"Computes the neg-log-likelihood gradient and Fisher information for a GLM.\n\n  Note that Fisher information is related to the Hessian of the log-likelihood\n  by the equation\n\n  ```none\n  FisherInfo = E[Hessian with respect to model_coefficients of -LogLikelihood(\n      Y | model_matrix, model_coefficients)]\n  ```\n\n  where `LogLikelihood` is the log-likelihood of a generalized linear model\n  parameterized by `model_matrix` and `model_coefficients`, and the expectation\n  is taken over Y, distributed according to the same GLM with the same parameter\n  values.\n\n  Args:\n    model_matrix: (Batch of) matrix-shaped, `float` `Tensor` or `SparseTensor`\n      where each row represents a sample's features.  Has shape `[N, n]` where\n      `N` is the number of data samples and `n` is the number of features per\n      sample.\n    linear_response: (Batch of) vector-shaped `Tensor` with the same dtype as\n      `model_matrix`, equal to `model_matix @ model_coefficients` where\n      `model_coefficients` are the coefficients of the linear component of the\n      GLM.\n    response: (Batch of) vector-shaped `Tensor` with the same dtype as\n      `model_matrix` where each element represents a sample's observed response\n      (to the corresponding row of features).\n    model: `tfp.glm.ExponentialFamily`-like instance, which specifies the link\n      function and distribution of the GLM, and thus characterizes the negative\n      log-likelihood. Must have sufficient statistic equal to the response, that\n      is, `T(y) = y`.\n\n  Returns:\n    grad_neg_log_likelihood: (Batch of) vector-shaped `Tensor` with the same\n      shape and dtype as a single row of `model_matrix`, representing the\n      gradient of the negative log likelihood of `response` given linear\n      response `linear_response`.\n    fim_middle: (Batch of) vector-shaped `Tensor` with the same shape and dtype\n      as a single column of `model_matrix`, satisfying the equation\n      `Fisher information =\n      Transpose(model_matrix)\n      @ diag(fim_middle)\n      @ model_matrix`.\n  \"\"\"\n  # TODO(b/111926503): Determine whether there are some practical cases where it\n  # is computationally favorable to compute the full FIM.\n  mean, variance, grad_mean = model(linear_response)\n\n  is_valid = (\n      tf.math.is_finite(grad_mean) & tf.not_equal(grad_mean, 0.)\n      & tf.math.is_finite(variance) & (variance > 0.))\n\n  def _mask_if_invalid(x, mask):\n    mask = tf.fill(\n        tf.shape(input=x), value=np.array(mask, x.dtype.as_numpy_dtype))\n    return tf.where(is_valid, x, mask)\n\n  # TODO(b/111923449): Link to derivation once it's available.\n  v = (response - mean) * _mask_if_invalid(grad_mean, 1) / _mask_if_invalid(\n      variance, np.inf)\n  grad_log_likelihood = sparse_or_dense_matvecmul(\n      model_matrix, v, adjoint_a=True)\n  fim_middle = _mask_if_invalid(grad_mean, 0.)**2 / _mask_if_invalid(\n      variance, np.inf)\n  return -grad_log_likelihood, fim_middle", "code_tokens": ["def", "_grad_neg_log_likelihood_and_fim", "(", "model_matrix", ",", "linear_response", ",", "response", ",", "model", ")", ":", "# TODO(b/111926503): Determine whether there are some practical cases where it", "# is computationally favorable to compute the full FIM.", "mean", ",", "variance", ",", "grad_mean", "=", "model", "(", "linear_response", ")", "is_valid", "=", "(", "tf", ".", "math", ".", "is_finite", "(", "grad_mean", ")", "&", "tf", ".", "not_equal", "(", "grad_mean", ",", "0.", ")", "&", "tf", ".", "math", ".", "is_finite", "(", "variance", ")", "&", "(", "variance", ">", "0.", ")", ")", "def", "_mask_if_invalid", "(", "x", ",", "mask", ")", ":", "mask", "=", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "value", "=", "np", ".", "array", "(", "mask", ",", "x", ".", "dtype", ".", "as_numpy_dtype", ")", ")", "return", "tf", ".", "where", "(", "is_valid", ",", "x", ",", "mask", ")", "# TODO(b/111923449): Link to derivation once it's available.", "v", "=", "(", "response", "-", "mean", ")", "*", "_mask_if_invalid", "(", "grad_mean", ",", "1", ")", "/", "_mask_if_invalid", "(", "variance", ",", "np", ".", "inf", ")", "grad_log_likelihood", "=", "sparse_or_dense_matvecmul", "(", "model_matrix", ",", "v", ",", "adjoint_a", "=", "True", ")", "fim_middle", "=", "_mask_if_invalid", "(", "grad_mean", ",", "0.", ")", "**", "2", "/", "_mask_if_invalid", "(", "variance", ",", "np", ".", "inf", ")", "return", "-", "grad_log_likelihood", ",", "fim_middle"], "docstring": "Computes the neg-log-likelihood gradient and Fisher information for a GLM.\n\n  Note that Fisher information is related to the Hessian of the log-likelihood\n  by the equation\n\n  ```none\n  FisherInfo = E[Hessian with respect to model_coefficients of -LogLikelihood(\n      Y | model_matrix, model_coefficients)]\n  ```\n\n  where `LogLikelihood` is the log-likelihood of a generalized linear model\n  parameterized by `model_matrix` and `model_coefficients`, and the expectation\n  is taken over Y, distributed according to the same GLM with the same parameter\n  values.\n\n  Args:\n    model_matrix: (Batch of) matrix-shaped, `float` `Tensor` or `SparseTensor`\n      where each row represents a sample's features.  Has shape `[N, n]` where\n      `N` is the number of data samples and `n` is the number of features per\n      sample.\n    linear_response: (Batch of) vector-shaped `Tensor` with the same dtype as\n      `model_matrix`, equal to `model_matix @ model_coefficients` where\n      `model_coefficients` are the coefficients of the linear component of the\n      GLM.\n    response: (Batch of) vector-shaped `Tensor` with the same dtype as\n      `model_matrix` where each element represents a sample's observed response\n      (to the corresponding row of features).\n    model: `tfp.glm.ExponentialFamily`-like instance, which specifies the link\n      function and distribution of the GLM, and thus characterizes the negative\n      log-likelihood. Must have sufficient statistic equal to the response, that\n      is, `T(y) = y`.\n\n  Returns:\n    grad_neg_log_likelihood: (Batch of) vector-shaped `Tensor` with the same\n      shape and dtype as a single row of `model_matrix`, representing the\n      gradient of the negative log likelihood of `response` given linear\n      response `linear_response`.\n    fim_middle: (Batch of) vector-shaped `Tensor` with the same shape and dtype\n      as a single column of `model_matrix`, satisfying the equation\n      `Fisher information =\n      Transpose(model_matrix)\n      @ diag(fim_middle)\n      @ model_matrix`.", "docstring_tokens": ["Computes", "the", "neg", "-", "log", "-", "likelihood", "gradient", "and", "Fisher", "information", "for", "a", "GLM", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/glm/proximal_hessian.py#L41-L107", "partition": "test", "index": 1113, "time": "2018-08-10 12:36:03"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/glm/proximal_hessian.py", "func_name": "fit_sparse", "original_string": "def fit_sparse(model_matrix,\n               response,\n               model,\n               model_coefficients_start,\n               tolerance,\n               l1_regularizer,\n               l2_regularizer=None,\n               maximum_iterations=None,\n               maximum_full_sweeps_per_iteration=1,\n               learning_rate=None,\n               name=None):\n  r\"\"\"Fits a GLM using coordinate-wise FIM-informed proximal gradient descent.\n\n  This function uses a L1- and L2-regularized, second-order quasi-Newton method\n  to find maximum-likelihood parameters for the given model and observed data.\n  The second-order approximations use negative Fisher information in place of\n  the Hessian, that is,\n\n  ```none\n  FisherInfo = E_Y[Hessian with respect to model_coefficients of -LogLikelihood(\n      Y | model_matrix, current value of model_coefficients)]\n  ```\n\n  For large, sparse data sets, `model_matrix` should be supplied as a\n  `SparseTensor`.\n\n  Args:\n    model_matrix: (Batch of) matrix-shaped, `float` `Tensor` or `SparseTensor`\n      where each row represents a sample's features.  Has shape `[N, n]` where\n      `N` is the number of data samples and `n` is the number of features per\n      sample.\n    response: (Batch of) vector-shaped `Tensor` with the same dtype as\n      `model_matrix` where each element represents a sample's observed response\n      (to the corresponding row of features).\n    model: `tfp.glm.ExponentialFamily`-like instance, which specifies the link\n      function and distribution of the GLM, and thus characterizes the negative\n      log-likelihood which will be minimized. Must have sufficient statistic\n      equal to the response, that is, `T(y) = y`.\n    model_coefficients_start: (Batch of) vector-shaped, `float` `Tensor` with\n      the same dtype as `model_matrix`, representing the initial values of the\n      coefficients for the GLM regression.  Has shape `[n]` where `model_matrix`\n      has shape `[N, n]`.\n    tolerance: scalar, `float` `Tensor` representing the tolerance for each\n      optiization step; see the `tolerance` argument of `fit_sparse_one_step`.\n    l1_regularizer: scalar, `float` `Tensor` representing the weight of the L1\n      regularization term.\n    l2_regularizer: scalar, `float` `Tensor` representing the weight of the L2\n      regularization term.\n      Default value: `None` (i.e., no L2 regularization).\n    maximum_iterations: Python integer specifying maximum number of iterations\n      of the outer loop of the optimizer (i.e., maximum number of calls to\n      `fit_sparse_one_step`).  After this many iterations of the outer loop, the\n      algorithm will terminate even if the return value `model_coefficients` has\n      not converged.\n      Default value: `1`.\n    maximum_full_sweeps_per_iteration: Python integer specifying the maximum\n      number of coordinate descent sweeps allowed in each iteration.\n      Default value: `1`.\n    learning_rate: scalar, `float` `Tensor` representing a multiplicative factor\n      used to dampen the proximal gradient descent steps.\n      Default value: `None` (i.e., factor is conceptually `1`).\n    name: Python string representing the name of the TensorFlow operation.\n      The default name is `\"fit_sparse\"`.\n\n  Returns:\n    model_coefficients: (Batch of) `Tensor` of the same shape and dtype as\n      `model_coefficients_start`, representing the computed model coefficients\n      which minimize the regularized negative log-likelihood.\n    is_converged: scalar, `bool` `Tensor` indicating whether the minimization\n      procedure converged across all batches within the specified number of\n      iterations.  Here convergence means that an iteration of the inner loop\n      (`fit_sparse_one_step`) returns `True` for its `is_converged` output\n      value.\n    iter: scalar, `int` `Tensor` indicating the actual number of iterations of\n      the outer loop of the optimizer completed (i.e., number of calls to\n      `fit_sparse_one_step` before achieving convergence).\n\n  #### Example\n\n  ```python\n  from __future__ import print_function\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  def make_dataset(n, d, link, scale=1., dtype=np.float32):\n    model_coefficients = tfd.Uniform(\n        low=np.array(-1, dtype), high=np.array(1, dtype)).sample(\n            d, seed=42)\n    radius = np.sqrt(2.)\n    model_coefficients *= radius / tf.linalg.norm(model_coefficients)\n    mask = tf.random_shuffle(tf.range(d)) < tf.to_int32(0.5 * tf.to_float(d))\n    model_coefficients = tf.where(mask, model_coefficients,\n                                  tf.zeros_like(model_coefficients))\n    model_matrix = tfd.Normal(\n        loc=np.array(0, dtype), scale=np.array(1, dtype)).sample(\n            [n, d], seed=43)\n    scale = tf.convert_to_tensor(scale, dtype)\n    linear_response = tf.matmul(model_matrix,\n                                model_coefficients[..., tf.newaxis])[..., 0]\n    if link == 'linear':\n      response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)\n    elif link == 'probit':\n      response = tf.cast(\n          tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,\n                     dtype)\n    elif link == 'logit':\n      response = tfd.Bernoulli(logits=linear_response).sample(seed=44)\n    else:\n      raise ValueError('unrecognized true link: {}'.format(link))\n    return model_matrix, response, model_coefficients, mask\n\n  with tf.Session() as sess:\n    x_, y_, model_coefficients_true_, _ = sess.run(make_dataset(\n        n=int(1e5), d=100, link='probit'))\n\n    model = tfp.glm.Bernoulli()\n    model_coefficients_start = tf.zeros(x_.shape[-1], np.float32)\n\n    model_coefficients, is_converged, num_iter = tfp.glm.fit_sparse(\n        model_matrix=tf.convert_to_tensor(x_),\n        response=tf.convert_to_tensor(y_),\n        model=model,\n        model_coefficients_start=model_coefficients_start,\n        l1_regularizer=800.,\n        l2_regularizer=None,\n        maximum_iterations=10,\n        maximum_full_sweeps_per_iteration=10,\n        tolerance=1e-6,\n        learning_rate=None)\n\n    model_coefficients_, is_converged_, num_iter_ = sess.run([\n        model_coefficients, is_converged, num_iter])\n\n    print(\"is_converged:\", is_converged_)\n    print(\"    num_iter:\", num_iter_)\n    print(\"\\nLearned / True\")\n    print(np.concatenate(\n        [[model_coefficients_], [model_coefficients_true_]], axis=0).T)\n\n  # ==>\n  # is_converged: True\n  #     num_iter: 1\n  #\n  # Learned / True\n  # [[ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.11195257  0.12484948]\n  #  [ 0.          0.        ]\n  #  [ 0.05191106  0.06394956]\n  #  [-0.15090358 -0.15325639]\n  #  [-0.18187316 -0.18825999]\n  #  [-0.06140942 -0.07994166]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.14474444  0.15810856]\n  #  [ 0.          0.        ]\n  #  [-0.25249591 -0.24260855]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.03888761 -0.06755984]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.0192222  -0.04169233]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.01434913  0.03568212]\n  #  [-0.11336883 -0.12873614]\n  #  [ 0.          0.        ]\n  #  [-0.24496339 -0.24048163]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.04088281  0.06565224]\n  #  [-0.12784363 -0.13359821]\n  #  [ 0.05618424  0.07396613]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.         -0.01719233]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.00076072 -0.03607186]\n  #  [ 0.21801499  0.21146794]\n  #  [-0.02161094 -0.04031265]\n  #  [ 0.0918689   0.10487888]\n  #  [ 0.0106154   0.03233612]\n  #  [-0.07817317 -0.09725142]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.23725343 -0.24194022]\n  #  [ 0.          0.        ]\n  #  [-0.08725718 -0.1048776 ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.02114314 -0.04145789]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.02710908 -0.04590397]\n  #  [ 0.15293184  0.15415154]\n  #  [ 0.2114463   0.2088728 ]\n  #  [-0.10969634 -0.12368613]\n  #  [ 0.         -0.01505797]\n  #  [-0.01140458 -0.03234904]\n  #  [ 0.16051085  0.1680062 ]\n  #  [ 0.09816848  0.11094204]\n  ```\n\n  #### References\n\n  [1]: Jerome Friedman, Trevor Hastie and Rob Tibshirani. Regularization Paths\n       for Generalized Linear Models via Coordinate Descent. _Journal of\n       Statistical Software_, 33(1), 2010.\n       https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf\n\n  [2]: Guo-Xun Yuan, Chia-Hua Ho and Chih-Jen Lin. An Improved GLMNET for\n       L1-regularized Logistic Regression. _Journal of Machine Learning\n       Research_, 13, 2012.\n       http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf\n  \"\"\"\n  graph_deps = [\n      model_matrix,\n      response,\n      model_coefficients_start,\n      l1_regularizer,\n      l2_regularizer,\n      maximum_iterations,\n      maximum_full_sweeps_per_iteration,\n      # TODO(b/111925792): Replace `tolerance` arg with something like\n      # `convergence_criteria_fn`.\n      tolerance,\n      learning_rate,\n  ]\n  with tf.compat.v1.name_scope(name, 'fit_sparse', graph_deps):\n    # TODO(b/111922388): Include dispersion and offset parameters.\n    def _grad_neg_log_likelihood_and_fim_fn(x):\n      predicted_linear_response = sparse_or_dense_matvecmul(model_matrix, x)\n      g, h_middle = _grad_neg_log_likelihood_and_fim(\n          model_matrix, predicted_linear_response, response, model)\n      return g, model_matrix, h_middle\n\n    return tfp.optimizer.proximal_hessian_sparse_minimize(\n        _grad_neg_log_likelihood_and_fim_fn,\n        x_start=model_coefficients_start,\n        l1_regularizer=l1_regularizer,\n        l2_regularizer=l2_regularizer,\n        maximum_iterations=maximum_iterations,\n        maximum_full_sweeps_per_iteration=maximum_full_sweeps_per_iteration,\n        learning_rate=learning_rate,\n        tolerance=tolerance,\n        name=name)", "language": "python", "code": "def fit_sparse(model_matrix,\n               response,\n               model,\n               model_coefficients_start,\n               tolerance,\n               l1_regularizer,\n               l2_regularizer=None,\n               maximum_iterations=None,\n               maximum_full_sweeps_per_iteration=1,\n               learning_rate=None,\n               name=None):\n  r\"\"\"Fits a GLM using coordinate-wise FIM-informed proximal gradient descent.\n\n  This function uses a L1- and L2-regularized, second-order quasi-Newton method\n  to find maximum-likelihood parameters for the given model and observed data.\n  The second-order approximations use negative Fisher information in place of\n  the Hessian, that is,\n\n  ```none\n  FisherInfo = E_Y[Hessian with respect to model_coefficients of -LogLikelihood(\n      Y | model_matrix, current value of model_coefficients)]\n  ```\n\n  For large, sparse data sets, `model_matrix` should be supplied as a\n  `SparseTensor`.\n\n  Args:\n    model_matrix: (Batch of) matrix-shaped, `float` `Tensor` or `SparseTensor`\n      where each row represents a sample's features.  Has shape `[N, n]` where\n      `N` is the number of data samples and `n` is the number of features per\n      sample.\n    response: (Batch of) vector-shaped `Tensor` with the same dtype as\n      `model_matrix` where each element represents a sample's observed response\n      (to the corresponding row of features).\n    model: `tfp.glm.ExponentialFamily`-like instance, which specifies the link\n      function and distribution of the GLM, and thus characterizes the negative\n      log-likelihood which will be minimized. Must have sufficient statistic\n      equal to the response, that is, `T(y) = y`.\n    model_coefficients_start: (Batch of) vector-shaped, `float` `Tensor` with\n      the same dtype as `model_matrix`, representing the initial values of the\n      coefficients for the GLM regression.  Has shape `[n]` where `model_matrix`\n      has shape `[N, n]`.\n    tolerance: scalar, `float` `Tensor` representing the tolerance for each\n      optiization step; see the `tolerance` argument of `fit_sparse_one_step`.\n    l1_regularizer: scalar, `float` `Tensor` representing the weight of the L1\n      regularization term.\n    l2_regularizer: scalar, `float` `Tensor` representing the weight of the L2\n      regularization term.\n      Default value: `None` (i.e., no L2 regularization).\n    maximum_iterations: Python integer specifying maximum number of iterations\n      of the outer loop of the optimizer (i.e., maximum number of calls to\n      `fit_sparse_one_step`).  After this many iterations of the outer loop, the\n      algorithm will terminate even if the return value `model_coefficients` has\n      not converged.\n      Default value: `1`.\n    maximum_full_sweeps_per_iteration: Python integer specifying the maximum\n      number of coordinate descent sweeps allowed in each iteration.\n      Default value: `1`.\n    learning_rate: scalar, `float` `Tensor` representing a multiplicative factor\n      used to dampen the proximal gradient descent steps.\n      Default value: `None` (i.e., factor is conceptually `1`).\n    name: Python string representing the name of the TensorFlow operation.\n      The default name is `\"fit_sparse\"`.\n\n  Returns:\n    model_coefficients: (Batch of) `Tensor` of the same shape and dtype as\n      `model_coefficients_start`, representing the computed model coefficients\n      which minimize the regularized negative log-likelihood.\n    is_converged: scalar, `bool` `Tensor` indicating whether the minimization\n      procedure converged across all batches within the specified number of\n      iterations.  Here convergence means that an iteration of the inner loop\n      (`fit_sparse_one_step`) returns `True` for its `is_converged` output\n      value.\n    iter: scalar, `int` `Tensor` indicating the actual number of iterations of\n      the outer loop of the optimizer completed (i.e., number of calls to\n      `fit_sparse_one_step` before achieving convergence).\n\n  #### Example\n\n  ```python\n  from __future__ import print_function\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  def make_dataset(n, d, link, scale=1., dtype=np.float32):\n    model_coefficients = tfd.Uniform(\n        low=np.array(-1, dtype), high=np.array(1, dtype)).sample(\n            d, seed=42)\n    radius = np.sqrt(2.)\n    model_coefficients *= radius / tf.linalg.norm(model_coefficients)\n    mask = tf.random_shuffle(tf.range(d)) < tf.to_int32(0.5 * tf.to_float(d))\n    model_coefficients = tf.where(mask, model_coefficients,\n                                  tf.zeros_like(model_coefficients))\n    model_matrix = tfd.Normal(\n        loc=np.array(0, dtype), scale=np.array(1, dtype)).sample(\n            [n, d], seed=43)\n    scale = tf.convert_to_tensor(scale, dtype)\n    linear_response = tf.matmul(model_matrix,\n                                model_coefficients[..., tf.newaxis])[..., 0]\n    if link == 'linear':\n      response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)\n    elif link == 'probit':\n      response = tf.cast(\n          tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,\n                     dtype)\n    elif link == 'logit':\n      response = tfd.Bernoulli(logits=linear_response).sample(seed=44)\n    else:\n      raise ValueError('unrecognized true link: {}'.format(link))\n    return model_matrix, response, model_coefficients, mask\n\n  with tf.Session() as sess:\n    x_, y_, model_coefficients_true_, _ = sess.run(make_dataset(\n        n=int(1e5), d=100, link='probit'))\n\n    model = tfp.glm.Bernoulli()\n    model_coefficients_start = tf.zeros(x_.shape[-1], np.float32)\n\n    model_coefficients, is_converged, num_iter = tfp.glm.fit_sparse(\n        model_matrix=tf.convert_to_tensor(x_),\n        response=tf.convert_to_tensor(y_),\n        model=model,\n        model_coefficients_start=model_coefficients_start,\n        l1_regularizer=800.,\n        l2_regularizer=None,\n        maximum_iterations=10,\n        maximum_full_sweeps_per_iteration=10,\n        tolerance=1e-6,\n        learning_rate=None)\n\n    model_coefficients_, is_converged_, num_iter_ = sess.run([\n        model_coefficients, is_converged, num_iter])\n\n    print(\"is_converged:\", is_converged_)\n    print(\"    num_iter:\", num_iter_)\n    print(\"\\nLearned / True\")\n    print(np.concatenate(\n        [[model_coefficients_], [model_coefficients_true_]], axis=0).T)\n\n  # ==>\n  # is_converged: True\n  #     num_iter: 1\n  #\n  # Learned / True\n  # [[ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.11195257  0.12484948]\n  #  [ 0.          0.        ]\n  #  [ 0.05191106  0.06394956]\n  #  [-0.15090358 -0.15325639]\n  #  [-0.18187316 -0.18825999]\n  #  [-0.06140942 -0.07994166]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.14474444  0.15810856]\n  #  [ 0.          0.        ]\n  #  [-0.25249591 -0.24260855]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.03888761 -0.06755984]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.0192222  -0.04169233]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.01434913  0.03568212]\n  #  [-0.11336883 -0.12873614]\n  #  [ 0.          0.        ]\n  #  [-0.24496339 -0.24048163]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.04088281  0.06565224]\n  #  [-0.12784363 -0.13359821]\n  #  [ 0.05618424  0.07396613]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.         -0.01719233]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.00076072 -0.03607186]\n  #  [ 0.21801499  0.21146794]\n  #  [-0.02161094 -0.04031265]\n  #  [ 0.0918689   0.10487888]\n  #  [ 0.0106154   0.03233612]\n  #  [-0.07817317 -0.09725142]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.23725343 -0.24194022]\n  #  [ 0.          0.        ]\n  #  [-0.08725718 -0.1048776 ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.02114314 -0.04145789]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.02710908 -0.04590397]\n  #  [ 0.15293184  0.15415154]\n  #  [ 0.2114463   0.2088728 ]\n  #  [-0.10969634 -0.12368613]\n  #  [ 0.         -0.01505797]\n  #  [-0.01140458 -0.03234904]\n  #  [ 0.16051085  0.1680062 ]\n  #  [ 0.09816848  0.11094204]\n  ```\n\n  #### References\n\n  [1]: Jerome Friedman, Trevor Hastie and Rob Tibshirani. Regularization Paths\n       for Generalized Linear Models via Coordinate Descent. _Journal of\n       Statistical Software_, 33(1), 2010.\n       https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf\n\n  [2]: Guo-Xun Yuan, Chia-Hua Ho and Chih-Jen Lin. An Improved GLMNET for\n       L1-regularized Logistic Regression. _Journal of Machine Learning\n       Research_, 13, 2012.\n       http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf\n  \"\"\"\n  graph_deps = [\n      model_matrix,\n      response,\n      model_coefficients_start,\n      l1_regularizer,\n      l2_regularizer,\n      maximum_iterations,\n      maximum_full_sweeps_per_iteration,\n      # TODO(b/111925792): Replace `tolerance` arg with something like\n      # `convergence_criteria_fn`.\n      tolerance,\n      learning_rate,\n  ]\n  with tf.compat.v1.name_scope(name, 'fit_sparse', graph_deps):\n    # TODO(b/111922388): Include dispersion and offset parameters.\n    def _grad_neg_log_likelihood_and_fim_fn(x):\n      predicted_linear_response = sparse_or_dense_matvecmul(model_matrix, x)\n      g, h_middle = _grad_neg_log_likelihood_and_fim(\n          model_matrix, predicted_linear_response, response, model)\n      return g, model_matrix, h_middle\n\n    return tfp.optimizer.proximal_hessian_sparse_minimize(\n        _grad_neg_log_likelihood_and_fim_fn,\n        x_start=model_coefficients_start,\n        l1_regularizer=l1_regularizer,\n        l2_regularizer=l2_regularizer,\n        maximum_iterations=maximum_iterations,\n        maximum_full_sweeps_per_iteration=maximum_full_sweeps_per_iteration,\n        learning_rate=learning_rate,\n        tolerance=tolerance,\n        name=name)", "code_tokens": ["def", "fit_sparse", "(", "model_matrix", ",", "response", ",", "model", ",", "model_coefficients_start", ",", "tolerance", ",", "l1_regularizer", ",", "l2_regularizer", "=", "None", ",", "maximum_iterations", "=", "None", ",", "maximum_full_sweeps_per_iteration", "=", "1", ",", "learning_rate", "=", "None", ",", "name", "=", "None", ")", ":", "graph_deps", "=", "[", "model_matrix", ",", "response", ",", "model_coefficients_start", ",", "l1_regularizer", ",", "l2_regularizer", ",", "maximum_iterations", ",", "maximum_full_sweeps_per_iteration", ",", "# TODO(b/111925792): Replace `tolerance` arg with something like", "# `convergence_criteria_fn`.", "tolerance", ",", "learning_rate", ",", "]", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'fit_sparse'", ",", "graph_deps", ")", ":", "# TODO(b/111922388): Include dispersion and offset parameters.", "def", "_grad_neg_log_likelihood_and_fim_fn", "(", "x", ")", ":", "predicted_linear_response", "=", "sparse_or_dense_matvecmul", "(", "model_matrix", ",", "x", ")", "g", ",", "h_middle", "=", "_grad_neg_log_likelihood_and_fim", "(", "model_matrix", ",", "predicted_linear_response", ",", "response", ",", "model", ")", "return", "g", ",", "model_matrix", ",", "h_middle", "return", "tfp", ".", "optimizer", ".", "proximal_hessian_sparse_minimize", "(", "_grad_neg_log_likelihood_and_fim_fn", ",", "x_start", "=", "model_coefficients_start", ",", "l1_regularizer", "=", "l1_regularizer", ",", "l2_regularizer", "=", "l2_regularizer", ",", "maximum_iterations", "=", "maximum_iterations", ",", "maximum_full_sweeps_per_iteration", "=", "maximum_full_sweeps_per_iteration", ",", "learning_rate", "=", "learning_rate", ",", "tolerance", "=", "tolerance", ",", "name", "=", "name", ")"], "docstring": "r\"\"\"Fits a GLM using coordinate-wise FIM-informed proximal gradient descent.\n\n  This function uses a L1- and L2-regularized, second-order quasi-Newton method\n  to find maximum-likelihood parameters for the given model and observed data.\n  The second-order approximations use negative Fisher information in place of\n  the Hessian, that is,\n\n  ```none\n  FisherInfo = E_Y[Hessian with respect to model_coefficients of -LogLikelihood(\n      Y | model_matrix, current value of model_coefficients)]\n  ```\n\n  For large, sparse data sets, `model_matrix` should be supplied as a\n  `SparseTensor`.\n\n  Args:\n    model_matrix: (Batch of) matrix-shaped, `float` `Tensor` or `SparseTensor`\n      where each row represents a sample's features.  Has shape `[N, n]` where\n      `N` is the number of data samples and `n` is the number of features per\n      sample.\n    response: (Batch of) vector-shaped `Tensor` with the same dtype as\n      `model_matrix` where each element represents a sample's observed response\n      (to the corresponding row of features).\n    model: `tfp.glm.ExponentialFamily`-like instance, which specifies the link\n      function and distribution of the GLM, and thus characterizes the negative\n      log-likelihood which will be minimized. Must have sufficient statistic\n      equal to the response, that is, `T(y) = y`.\n    model_coefficients_start: (Batch of) vector-shaped, `float` `Tensor` with\n      the same dtype as `model_matrix`, representing the initial values of the\n      coefficients for the GLM regression.  Has shape `[n]` where `model_matrix`\n      has shape `[N, n]`.\n    tolerance: scalar, `float` `Tensor` representing the tolerance for each\n      optiization step; see the `tolerance` argument of `fit_sparse_one_step`.\n    l1_regularizer: scalar, `float` `Tensor` representing the weight of the L1\n      regularization term.\n    l2_regularizer: scalar, `float` `Tensor` representing the weight of the L2\n      regularization term.\n      Default value: `None` (i.e., no L2 regularization).\n    maximum_iterations: Python integer specifying maximum number of iterations\n      of the outer loop of the optimizer (i.e., maximum number of calls to\n      `fit_sparse_one_step`).  After this many iterations of the outer loop, the\n      algorithm will terminate even if the return value `model_coefficients` has\n      not converged.\n      Default value: `1`.\n    maximum_full_sweeps_per_iteration: Python integer specifying the maximum\n      number of coordinate descent sweeps allowed in each iteration.\n      Default value: `1`.\n    learning_rate: scalar, `float` `Tensor` representing a multiplicative factor\n      used to dampen the proximal gradient descent steps.\n      Default value: `None` (i.e., factor is conceptually `1`).\n    name: Python string representing the name of the TensorFlow operation.\n      The default name is `\"fit_sparse\"`.\n\n  Returns:\n    model_coefficients: (Batch of) `Tensor` of the same shape and dtype as\n      `model_coefficients_start`, representing the computed model coefficients\n      which minimize the regularized negative log-likelihood.\n    is_converged: scalar, `bool` `Tensor` indicating whether the minimization\n      procedure converged across all batches within the specified number of\n      iterations.  Here convergence means that an iteration of the inner loop\n      (`fit_sparse_one_step`) returns `True` for its `is_converged` output\n      value.\n    iter: scalar, `int` `Tensor` indicating the actual number of iterations of\n      the outer loop of the optimizer completed (i.e., number of calls to\n      `fit_sparse_one_step` before achieving convergence).\n\n  #### Example\n\n  ```python\n  from __future__ import print_function\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  def make_dataset(n, d, link, scale=1., dtype=np.float32):\n    model_coefficients = tfd.Uniform(\n        low=np.array(-1, dtype), high=np.array(1, dtype)).sample(\n            d, seed=42)\n    radius = np.sqrt(2.)\n    model_coefficients *= radius / tf.linalg.norm(model_coefficients)\n    mask = tf.random_shuffle(tf.range(d)) < tf.to_int32(0.5 * tf.to_float(d))\n    model_coefficients = tf.where(mask, model_coefficients,\n                                  tf.zeros_like(model_coefficients))\n    model_matrix = tfd.Normal(\n        loc=np.array(0, dtype), scale=np.array(1, dtype)).sample(\n            [n, d], seed=43)\n    scale = tf.convert_to_tensor(scale, dtype)\n    linear_response = tf.matmul(model_matrix,\n                                model_coefficients[..., tf.newaxis])[..., 0]\n    if link == 'linear':\n      response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)\n    elif link == 'probit':\n      response = tf.cast(\n          tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,\n                     dtype)\n    elif link == 'logit':\n      response = tfd.Bernoulli(logits=linear_response).sample(seed=44)\n    else:\n      raise ValueError('unrecognized true link: {}'.format(link))\n    return model_matrix, response, model_coefficients, mask\n\n  with tf.Session() as sess:\n    x_, y_, model_coefficients_true_, _ = sess.run(make_dataset(\n        n=int(1e5), d=100, link='probit'))\n\n    model = tfp.glm.Bernoulli()\n    model_coefficients_start = tf.zeros(x_.shape[-1], np.float32)\n\n    model_coefficients, is_converged, num_iter = tfp.glm.fit_sparse(\n        model_matrix=tf.convert_to_tensor(x_),\n        response=tf.convert_to_tensor(y_),\n        model=model,\n        model_coefficients_start=model_coefficients_start,\n        l1_regularizer=800.,\n        l2_regularizer=None,\n        maximum_iterations=10,\n        maximum_full_sweeps_per_iteration=10,\n        tolerance=1e-6,\n        learning_rate=None)\n\n    model_coefficients_, is_converged_, num_iter_ = sess.run([\n        model_coefficients, is_converged, num_iter])\n\n    print(\"is_converged:\", is_converged_)\n    print(\"    num_iter:\", num_iter_)\n    print(\"\\nLearned / True\")\n    print(np.concatenate(\n        [[model_coefficients_], [model_coefficients_true_]], axis=0).T)\n\n  # ==>\n  # is_converged: True\n  #     num_iter: 1\n  #\n  # Learned / True\n  # [[ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.11195257  0.12484948]\n  #  [ 0.          0.        ]\n  #  [ 0.05191106  0.06394956]\n  #  [-0.15090358 -0.15325639]\n  #  [-0.18187316 -0.18825999]\n  #  [-0.06140942 -0.07994166]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.14474444  0.15810856]\n  #  [ 0.          0.        ]\n  #  [-0.25249591 -0.24260855]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.03888761 -0.06755984]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.0192222  -0.04169233]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.01434913  0.03568212]\n  #  [-0.11336883 -0.12873614]\n  #  [ 0.          0.        ]\n  #  [-0.24496339 -0.24048163]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.04088281  0.06565224]\n  #  [-0.12784363 -0.13359821]\n  #  [ 0.05618424  0.07396613]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [ 0.         -0.01719233]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.00076072 -0.03607186]\n  #  [ 0.21801499  0.21146794]\n  #  [-0.02161094 -0.04031265]\n  #  [ 0.0918689   0.10487888]\n  #  [ 0.0106154   0.03233612]\n  #  [-0.07817317 -0.09725142]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.23725343 -0.24194022]\n  #  [ 0.          0.        ]\n  #  [-0.08725718 -0.1048776 ]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.02114314 -0.04145789]\n  #  [ 0.          0.        ]\n  #  [ 0.          0.        ]\n  #  [-0.02710908 -0.04590397]\n  #  [ 0.15293184  0.15415154]\n  #  [ 0.2114463   0.2088728 ]\n  #  [-0.10969634 -0.12368613]\n  #  [ 0.         -0.01505797]\n  #  [-0.01140458 -0.03234904]\n  #  [ 0.16051085  0.1680062 ]\n  #  [ 0.09816848  0.11094204]\n  ```\n\n  #### References\n\n  [1]: Jerome Friedman, Trevor Hastie and Rob Tibshirani. Regularization Paths\n       for Generalized Linear Models via Coordinate Descent. _Journal of\n       Statistical Software_, 33(1), 2010.\n       https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf\n\n  [2]: Guo-Xun Yuan, Chia-Hua Ho and Chih-Jen Lin. An Improved GLMNET for\n       L1-regularized Logistic Regression. _Journal of Machine Learning\n       Research_, 13, 2012.\n       http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf", "docstring_tokens": ["r", "Fits", "a", "GLM", "using", "coordinate", "-", "wise", "FIM", "-", "informed", "proximal", "gradient", "descent", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/glm/proximal_hessian.py#L235-L488", "partition": "test", "index": 1114, "time": "2018-08-10 12:36:03"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/interceptor.py", "func_name": "tape", "original_string": "def tape():\n  \"\"\"Context manager for recording interceptable executions onto a tape.\n\n  Similar to `tf.GradientTape`, operations are recorded if they are executed\n  within this context manager. In addition, the operation must be registered\n  (wrapped) as `ed.interceptable`.\n\n  Yields:\n    tape: OrderedDict where operations are recorded in sequence. Keys are\n      the `name` keyword argument to the operation (typically, a random\n      variable's `name`) and values are the corresponding output of the\n      operation. If the operation has no name, it is not recorded.\n\n  #### Examples\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def probabilistic_matrix_factorization():\n    users = ed.Normal(0., 1., sample_shape=[5000, 128], name=\"users\")\n    items = ed.Normal(0., 1., sample_shape=[7500, 128], name=\"items\")\n    ratings = ed.Normal(loc=tf.matmul(users, items, transpose_b=True),\n                        scale=0.1,\n                        name=\"ratings\")\n    return ratings\n\n  with ed.tape() as model_tape:\n    ratings = probabilistic_matrix_factorization()\n\n  assert model_tape[\"users\"].shape == (5000, 128)\n  assert model_tape[\"items\"].shape == (7500, 128)\n  assert model_tape[\"ratings\"] == ratings\n  ```\n\n  \"\"\"\n  tape_data = collections.OrderedDict({})\n\n  def record(f, *args, **kwargs):\n    \"\"\"Records execution to a tape.\"\"\"\n    name = kwargs.get(\"name\")\n    output = interceptable(f)(*args, **kwargs)\n    if name:\n      tape_data[name] = output\n    return output\n\n  with interception(record):\n    yield tape_data", "language": "python", "code": "def tape():\n  \"\"\"Context manager for recording interceptable executions onto a tape.\n\n  Similar to `tf.GradientTape`, operations are recorded if they are executed\n  within this context manager. In addition, the operation must be registered\n  (wrapped) as `ed.interceptable`.\n\n  Yields:\n    tape: OrderedDict where operations are recorded in sequence. Keys are\n      the `name` keyword argument to the operation (typically, a random\n      variable's `name`) and values are the corresponding output of the\n      operation. If the operation has no name, it is not recorded.\n\n  #### Examples\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def probabilistic_matrix_factorization():\n    users = ed.Normal(0., 1., sample_shape=[5000, 128], name=\"users\")\n    items = ed.Normal(0., 1., sample_shape=[7500, 128], name=\"items\")\n    ratings = ed.Normal(loc=tf.matmul(users, items, transpose_b=True),\n                        scale=0.1,\n                        name=\"ratings\")\n    return ratings\n\n  with ed.tape() as model_tape:\n    ratings = probabilistic_matrix_factorization()\n\n  assert model_tape[\"users\"].shape == (5000, 128)\n  assert model_tape[\"items\"].shape == (7500, 128)\n  assert model_tape[\"ratings\"] == ratings\n  ```\n\n  \"\"\"\n  tape_data = collections.OrderedDict({})\n\n  def record(f, *args, **kwargs):\n    \"\"\"Records execution to a tape.\"\"\"\n    name = kwargs.get(\"name\")\n    output = interceptable(f)(*args, **kwargs)\n    if name:\n      tape_data[name] = output\n    return output\n\n  with interception(record):\n    yield tape_data", "code_tokens": ["def", "tape", "(", ")", ":", "tape_data", "=", "collections", ".", "OrderedDict", "(", "{", "}", ")", "def", "record", "(", "f", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "\"\"\"Records execution to a tape.\"\"\"", "name", "=", "kwargs", ".", "get", "(", "\"name\"", ")", "output", "=", "interceptable", "(", "f", ")", "(", "*", "args", ",", "*", "*", "kwargs", ")", "if", "name", ":", "tape_data", "[", "name", "]", "=", "output", "return", "output", "with", "interception", "(", "record", ")", ":", "yield", "tape_data"], "docstring": "Context manager for recording interceptable executions onto a tape.\n\n  Similar to `tf.GradientTape`, operations are recorded if they are executed\n  within this context manager. In addition, the operation must be registered\n  (wrapped) as `ed.interceptable`.\n\n  Yields:\n    tape: OrderedDict where operations are recorded in sequence. Keys are\n      the `name` keyword argument to the operation (typically, a random\n      variable's `name`) and values are the corresponding output of the\n      operation. If the operation has no name, it is not recorded.\n\n  #### Examples\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def probabilistic_matrix_factorization():\n    users = ed.Normal(0., 1., sample_shape=[5000, 128], name=\"users\")\n    items = ed.Normal(0., 1., sample_shape=[7500, 128], name=\"items\")\n    ratings = ed.Normal(loc=tf.matmul(users, items, transpose_b=True),\n                        scale=0.1,\n                        name=\"ratings\")\n    return ratings\n\n  with ed.tape() as model_tape:\n    ratings = probabilistic_matrix_factorization()\n\n  assert model_tape[\"users\"].shape == (5000, 128)\n  assert model_tape[\"items\"].shape == (7500, 128)\n  assert model_tape[\"ratings\"] == ratings\n  ```", "docstring_tokens": ["Context", "manager", "for", "recording", "interceptable", "executions", "onto", "a", "tape", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/interceptor.py#L199-L245", "partition": "test", "index": 630, "time": "2018-08-16 10:31:02"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/von_mises.py", "func_name": "_von_mises_cdf_series", "original_string": "def _von_mises_cdf_series(x, concentration, num_terms, dtype):\n  \"\"\"Computes the von Mises CDF and its derivative via series expansion.\"\"\"\n  # Keep the number of terms as a float. It should be a small integer, so\n  # exactly representable as a float.\n  num_terms = tf.cast(num_terms, dtype=dtype)\n\n  def loop_body(n, rn, drn_dconcentration, vn, dvn_dconcentration):\n    \"\"\"One iteration of the series loop.\"\"\"\n\n    denominator = 2. * n / concentration + rn\n    ddenominator_dk = -2. * n / concentration ** 2 + drn_dconcentration\n    rn = 1. / denominator\n    drn_dconcentration = -ddenominator_dk / denominator ** 2\n\n    multiplier = tf.sin(n * x) / n + vn\n    vn = rn * multiplier\n    dvn_dconcentration = (drn_dconcentration * multiplier +\n                          rn * dvn_dconcentration)\n    n -= 1.\n\n    return n, rn, drn_dconcentration, vn, dvn_dconcentration\n\n  (_, _, _, vn, dvn_dconcentration) = tf.while_loop(\n      cond=lambda n, *_: n > 0.,\n      body=loop_body,\n      loop_vars=(\n          num_terms,  # n\n          tf.zeros_like(x, name=\"rn\"),\n          tf.zeros_like(x, name=\"drn_dconcentration\"),\n          tf.zeros_like(x, name=\"vn\"),\n          tf.zeros_like(x, name=\"dvn_dconcentration\"),\n      ),\n  )\n\n  cdf = .5 + x / (2. * np.pi) + vn / np.pi\n  dcdf_dconcentration = dvn_dconcentration / np.pi\n\n  # Clip the result to [0, 1].\n  cdf_clipped = tf.clip_by_value(cdf, 0., 1.)\n  # The clipped values do not depend on concentration anymore, so set their\n  # derivative to zero.\n  dcdf_dconcentration *= tf.cast((cdf >= 0.) & (cdf <= 1.), dtype)\n\n  return cdf_clipped, dcdf_dconcentration", "language": "python", "code": "def _von_mises_cdf_series(x, concentration, num_terms, dtype):\n  \"\"\"Computes the von Mises CDF and its derivative via series expansion.\"\"\"\n  # Keep the number of terms as a float. It should be a small integer, so\n  # exactly representable as a float.\n  num_terms = tf.cast(num_terms, dtype=dtype)\n\n  def loop_body(n, rn, drn_dconcentration, vn, dvn_dconcentration):\n    \"\"\"One iteration of the series loop.\"\"\"\n\n    denominator = 2. * n / concentration + rn\n    ddenominator_dk = -2. * n / concentration ** 2 + drn_dconcentration\n    rn = 1. / denominator\n    drn_dconcentration = -ddenominator_dk / denominator ** 2\n\n    multiplier = tf.sin(n * x) / n + vn\n    vn = rn * multiplier\n    dvn_dconcentration = (drn_dconcentration * multiplier +\n                          rn * dvn_dconcentration)\n    n -= 1.\n\n    return n, rn, drn_dconcentration, vn, dvn_dconcentration\n\n  (_, _, _, vn, dvn_dconcentration) = tf.while_loop(\n      cond=lambda n, *_: n > 0.,\n      body=loop_body,\n      loop_vars=(\n          num_terms,  # n\n          tf.zeros_like(x, name=\"rn\"),\n          tf.zeros_like(x, name=\"drn_dconcentration\"),\n          tf.zeros_like(x, name=\"vn\"),\n          tf.zeros_like(x, name=\"dvn_dconcentration\"),\n      ),\n  )\n\n  cdf = .5 + x / (2. * np.pi) + vn / np.pi\n  dcdf_dconcentration = dvn_dconcentration / np.pi\n\n  # Clip the result to [0, 1].\n  cdf_clipped = tf.clip_by_value(cdf, 0., 1.)\n  # The clipped values do not depend on concentration anymore, so set their\n  # derivative to zero.\n  dcdf_dconcentration *= tf.cast((cdf >= 0.) & (cdf <= 1.), dtype)\n\n  return cdf_clipped, dcdf_dconcentration", "code_tokens": ["def", "_von_mises_cdf_series", "(", "x", ",", "concentration", ",", "num_terms", ",", "dtype", ")", ":", "# Keep the number of terms as a float. It should be a small integer, so", "# exactly representable as a float.", "num_terms", "=", "tf", ".", "cast", "(", "num_terms", ",", "dtype", "=", "dtype", ")", "def", "loop_body", "(", "n", ",", "rn", ",", "drn_dconcentration", ",", "vn", ",", "dvn_dconcentration", ")", ":", "\"\"\"One iteration of the series loop.\"\"\"", "denominator", "=", "2.", "*", "n", "/", "concentration", "+", "rn", "ddenominator_dk", "=", "-", "2.", "*", "n", "/", "concentration", "**", "2", "+", "drn_dconcentration", "rn", "=", "1.", "/", "denominator", "drn_dconcentration", "=", "-", "ddenominator_dk", "/", "denominator", "**", "2", "multiplier", "=", "tf", ".", "sin", "(", "n", "*", "x", ")", "/", "n", "+", "vn", "vn", "=", "rn", "*", "multiplier", "dvn_dconcentration", "=", "(", "drn_dconcentration", "*", "multiplier", "+", "rn", "*", "dvn_dconcentration", ")", "n", "-=", "1.", "return", "n", ",", "rn", ",", "drn_dconcentration", ",", "vn", ",", "dvn_dconcentration", "(", "_", ",", "_", ",", "_", ",", "vn", ",", "dvn_dconcentration", ")", "=", "tf", ".", "while_loop", "(", "cond", "=", "lambda", "n", ",", "*", "_", ":", "n", ">", "0.", ",", "body", "=", "loop_body", ",", "loop_vars", "=", "(", "num_terms", ",", "# n", "tf", ".", "zeros_like", "(", "x", ",", "name", "=", "\"rn\"", ")", ",", "tf", ".", "zeros_like", "(", "x", ",", "name", "=", "\"drn_dconcentration\"", ")", ",", "tf", ".", "zeros_like", "(", "x", ",", "name", "=", "\"vn\"", ")", ",", "tf", ".", "zeros_like", "(", "x", ",", "name", "=", "\"dvn_dconcentration\"", ")", ",", ")", ",", ")", "cdf", "=", ".5", "+", "x", "/", "(", "2.", "*", "np", ".", "pi", ")", "+", "vn", "/", "np", ".", "pi", "dcdf_dconcentration", "=", "dvn_dconcentration", "/", "np", ".", "pi", "# Clip the result to [0, 1].", "cdf_clipped", "=", "tf", ".", "clip_by_value", "(", "cdf", ",", "0.", ",", "1.", ")", "# The clipped values do not depend on concentration anymore, so set their", "# derivative to zero.", "dcdf_dconcentration", "*=", "tf", ".", "cast", "(", "(", "cdf", ">=", "0.", ")", "&", "(", "cdf", "<=", "1.", ")", ",", "dtype", ")", "return", "cdf_clipped", ",", "dcdf_dconcentration"], "docstring": "Computes the von Mises CDF and its derivative via series expansion.", "docstring_tokens": ["Computes", "the", "von", "Mises", "CDF", "and", "its", "derivative", "via", "series", "expansion", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/von_mises.py#L377-L420", "partition": "test", "index": 1083, "time": "2018-08-21 11:08:01"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/von_mises.py", "func_name": "_von_mises_cdf_normal", "original_string": "def _von_mises_cdf_normal(x, concentration, dtype):\n  \"\"\"Computes the von Mises CDF and its derivative via Normal approximation.\"\"\"\n\n  def cdf_func(concentration):\n    \"\"\"A helper function that is passed to value_and_gradient.\"\"\"\n    # z is an \"almost Normally distributed\" random variable.\n    z = ((np.sqrt(2. / np.pi) / tf.math.bessel_i0e(concentration)) *\n         tf.sin(.5 * x))\n\n    # This is the correction described in [1] which reduces the error\n    # of the Normal approximation.\n    z2 = z ** 2\n    z3 = z2 * z\n    z4 = z2 ** 2\n    c = 24. * concentration\n    c1 = 56.\n\n    xi = z - z3 / ((c - 2. * z2 - 16.) / 3. -\n                   (z4 + (7. / 4.) * z2 + 167. / 2.) / (c - c1 - z2 + 3.)) ** 2\n\n    distrib = normal.Normal(tf.cast(0., dtype), tf.cast(1., dtype))\n\n    return distrib.cdf(xi)\n\n  return value_and_gradient(cdf_func, concentration)", "language": "python", "code": "def _von_mises_cdf_normal(x, concentration, dtype):\n  \"\"\"Computes the von Mises CDF and its derivative via Normal approximation.\"\"\"\n\n  def cdf_func(concentration):\n    \"\"\"A helper function that is passed to value_and_gradient.\"\"\"\n    # z is an \"almost Normally distributed\" random variable.\n    z = ((np.sqrt(2. / np.pi) / tf.math.bessel_i0e(concentration)) *\n         tf.sin(.5 * x))\n\n    # This is the correction described in [1] which reduces the error\n    # of the Normal approximation.\n    z2 = z ** 2\n    z3 = z2 * z\n    z4 = z2 ** 2\n    c = 24. * concentration\n    c1 = 56.\n\n    xi = z - z3 / ((c - 2. * z2 - 16.) / 3. -\n                   (z4 + (7. / 4.) * z2 + 167. / 2.) / (c - c1 - z2 + 3.)) ** 2\n\n    distrib = normal.Normal(tf.cast(0., dtype), tf.cast(1., dtype))\n\n    return distrib.cdf(xi)\n\n  return value_and_gradient(cdf_func, concentration)", "code_tokens": ["def", "_von_mises_cdf_normal", "(", "x", ",", "concentration", ",", "dtype", ")", ":", "def", "cdf_func", "(", "concentration", ")", ":", "\"\"\"A helper function that is passed to value_and_gradient.\"\"\"", "# z is an \"almost Normally distributed\" random variable.", "z", "=", "(", "(", "np", ".", "sqrt", "(", "2.", "/", "np", ".", "pi", ")", "/", "tf", ".", "math", ".", "bessel_i0e", "(", "concentration", ")", ")", "*", "tf", ".", "sin", "(", ".5", "*", "x", ")", ")", "# This is the correction described in [1] which reduces the error", "# of the Normal approximation.", "z2", "=", "z", "**", "2", "z3", "=", "z2", "*", "z", "z4", "=", "z2", "**", "2", "c", "=", "24.", "*", "concentration", "c1", "=", "56.", "xi", "=", "z", "-", "z3", "/", "(", "(", "c", "-", "2.", "*", "z2", "-", "16.", ")", "/", "3.", "-", "(", "z4", "+", "(", "7.", "/", "4.", ")", "*", "z2", "+", "167.", "/", "2.", ")", "/", "(", "c", "-", "c1", "-", "z2", "+", "3.", ")", ")", "**", "2", "distrib", "=", "normal", ".", "Normal", "(", "tf", ".", "cast", "(", "0.", ",", "dtype", ")", ",", "tf", ".", "cast", "(", "1.", ",", "dtype", ")", ")", "return", "distrib", ".", "cdf", "(", "xi", ")", "return", "value_and_gradient", "(", "cdf_func", ",", "concentration", ")"], "docstring": "Computes the von Mises CDF and its derivative via Normal approximation.", "docstring_tokens": ["Computes", "the", "von", "Mises", "CDF", "and", "its", "derivative", "via", "Normal", "approximation", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/von_mises.py#L423-L447", "partition": "test", "index": 1084, "time": "2018-08-21 11:08:01"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/structural_time_series.py", "func_name": "StructuralTimeSeries.prior_sample", "original_string": "def prior_sample(self,\n                   num_timesteps,\n                   initial_step=0,\n                   params_sample_shape=(),\n                   trajectories_sample_shape=(),\n                   seed=None):\n    \"\"\"Sample from the joint prior over model parameters and trajectories.\n\n    Args:\n      num_timesteps: Scalar `int` `Tensor` number of timesteps to model.\n      initial_step: Optional scalar `int` `Tensor` specifying the starting\n        timestep.\n          Default value: 0.\n      params_sample_shape: Number of possible worlds to sample iid from the\n        parameter prior, or more generally, `Tensor` `int` shape to fill with\n        iid samples.\n          Default value: [] (i.e., draw a single sample and don't expand the\n          shape).\n      trajectories_sample_shape: For each sampled set of parameters, number\n        of trajectories to sample, or more generally, `Tensor` `int` shape to\n        fill with iid samples.\n        Default value: [] (i.e., draw a single sample and don't expand the\n          shape).\n      seed: Python `int` random seed.\n\n    Returns:\n      trajectories: `float` `Tensor` of shape\n        `trajectories_sample_shape + params_sample_shape + [num_timesteps, 1]`\n        containing all sampled trajectories.\n      param_samples: list of sampled parameter value `Tensor`s, in order\n        corresponding to `self.parameters`, each of shape\n        `params_sample_shape + prior.batch_shape + prior.event_shape`.\n    \"\"\"\n\n    seed = distributions.SeedStream(\n        seed, salt='StructuralTimeSeries_prior_sample')\n\n    with tf.compat.v1.name_scope(\n        'prior_sample',\n        values=[num_timesteps, params_sample_shape, trajectories_sample_shape]):\n      param_samples = [\n          p.prior.sample(params_sample_shape, seed=seed(), name=p.name)\n          for p in self.parameters\n      ]\n      model = self.make_state_space_model(\n          num_timesteps=num_timesteps,\n          initial_step=initial_step,\n          param_vals=param_samples)\n      return model.sample(trajectories_sample_shape, seed=seed()), param_samples", "language": "python", "code": "def prior_sample(self,\n                   num_timesteps,\n                   initial_step=0,\n                   params_sample_shape=(),\n                   trajectories_sample_shape=(),\n                   seed=None):\n    \"\"\"Sample from the joint prior over model parameters and trajectories.\n\n    Args:\n      num_timesteps: Scalar `int` `Tensor` number of timesteps to model.\n      initial_step: Optional scalar `int` `Tensor` specifying the starting\n        timestep.\n          Default value: 0.\n      params_sample_shape: Number of possible worlds to sample iid from the\n        parameter prior, or more generally, `Tensor` `int` shape to fill with\n        iid samples.\n          Default value: [] (i.e., draw a single sample and don't expand the\n          shape).\n      trajectories_sample_shape: For each sampled set of parameters, number\n        of trajectories to sample, or more generally, `Tensor` `int` shape to\n        fill with iid samples.\n        Default value: [] (i.e., draw a single sample and don't expand the\n          shape).\n      seed: Python `int` random seed.\n\n    Returns:\n      trajectories: `float` `Tensor` of shape\n        `trajectories_sample_shape + params_sample_shape + [num_timesteps, 1]`\n        containing all sampled trajectories.\n      param_samples: list of sampled parameter value `Tensor`s, in order\n        corresponding to `self.parameters`, each of shape\n        `params_sample_shape + prior.batch_shape + prior.event_shape`.\n    \"\"\"\n\n    seed = distributions.SeedStream(\n        seed, salt='StructuralTimeSeries_prior_sample')\n\n    with tf.compat.v1.name_scope(\n        'prior_sample',\n        values=[num_timesteps, params_sample_shape, trajectories_sample_shape]):\n      param_samples = [\n          p.prior.sample(params_sample_shape, seed=seed(), name=p.name)\n          for p in self.parameters\n      ]\n      model = self.make_state_space_model(\n          num_timesteps=num_timesteps,\n          initial_step=initial_step,\n          param_vals=param_samples)\n      return model.sample(trajectories_sample_shape, seed=seed()), param_samples", "code_tokens": ["def", "prior_sample", "(", "self", ",", "num_timesteps", ",", "initial_step", "=", "0", ",", "params_sample_shape", "=", "(", ")", ",", "trajectories_sample_shape", "=", "(", ")", ",", "seed", "=", "None", ")", ":", "seed", "=", "distributions", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'StructuralTimeSeries_prior_sample'", ")", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'prior_sample'", ",", "values", "=", "[", "num_timesteps", ",", "params_sample_shape", ",", "trajectories_sample_shape", "]", ")", ":", "param_samples", "=", "[", "p", ".", "prior", ".", "sample", "(", "params_sample_shape", ",", "seed", "=", "seed", "(", ")", ",", "name", "=", "p", ".", "name", ")", "for", "p", "in", "self", ".", "parameters", "]", "model", "=", "self", ".", "make_state_space_model", "(", "num_timesteps", "=", "num_timesteps", ",", "initial_step", "=", "initial_step", ",", "param_vals", "=", "param_samples", ")", "return", "model", ".", "sample", "(", "trajectories_sample_shape", ",", "seed", "=", "seed", "(", ")", ")", ",", "param_samples"], "docstring": "Sample from the joint prior over model parameters and trajectories.\n\n    Args:\n      num_timesteps: Scalar `int` `Tensor` number of timesteps to model.\n      initial_step: Optional scalar `int` `Tensor` specifying the starting\n        timestep.\n          Default value: 0.\n      params_sample_shape: Number of possible worlds to sample iid from the\n        parameter prior, or more generally, `Tensor` `int` shape to fill with\n        iid samples.\n          Default value: [] (i.e., draw a single sample and don't expand the\n          shape).\n      trajectories_sample_shape: For each sampled set of parameters, number\n        of trajectories to sample, or more generally, `Tensor` `int` shape to\n        fill with iid samples.\n        Default value: [] (i.e., draw a single sample and don't expand the\n          shape).\n      seed: Python `int` random seed.\n\n    Returns:\n      trajectories: `float` `Tensor` of shape\n        `trajectories_sample_shape + params_sample_shape + [num_timesteps, 1]`\n        containing all sampled trajectories.\n      param_samples: list of sampled parameter value `Tensor`s, in order\n        corresponding to `self.parameters`, each of shape\n        `params_sample_shape + prior.batch_shape + prior.event_shape`.", "docstring_tokens": ["Sample", "from", "the", "joint", "prior", "over", "model", "parameters", "and", "trajectories", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/structural_time_series.py#L161-L209", "partition": "test", "index": 736, "time": "2018-08-23 11:42:59"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/internal/util.py", "func_name": "empirical_statistics", "original_string": "def empirical_statistics(observed_time_series):\n  \"\"\"Compute statistics of a provided time series, as heuristic initialization.\n\n  Args:\n    observed_time_series: `Tensor` representing a time series, or batch of time\n       series, of shape either `batch_shape + [num_timesteps, 1]` or\n       `batch_shape + [num_timesteps]` (allowed if `num_timesteps > 1`).\n\n  Returns:\n    observed_mean: `Tensor` of shape `batch_shape`, giving the empirical\n      mean of each time series in the batch.\n    observed_stddev: `Tensor` of shape `batch_shape`, giving the empirical\n      standard deviation of each time series in the batch.\n    observed_initial_centered: `Tensor of shape `batch_shape`, giving the\n      initial value of each time series in the batch after centering\n      (subtracting the mean).\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      'empirical_statistics', values=[observed_time_series]):\n\n    [\n        observed_time_series,\n        mask\n    ] = canonicalize_observed_time_series_with_mask(observed_time_series)\n\n    squeezed_series = observed_time_series[..., 0]\n    if mask is None:\n      observed_mean, observed_variance = tf.nn.moments(\n          x=squeezed_series, axes=-1)\n      observed_initial = squeezed_series[..., 0]\n    else:\n      broadcast_mask = tf.broadcast_to(tf.cast(mask, tf.bool),\n                                       tf.shape(input=squeezed_series))\n      observed_mean, observed_variance = (\n          missing_values_util.moments_of_masked_time_series(\n              squeezed_series, broadcast_mask=broadcast_mask))\n      try:\n        observed_initial = (\n            missing_values_util.initial_value_of_masked_time_series(\n                squeezed_series, broadcast_mask=broadcast_mask))\n      except NotImplementedError:\n        tf.compat.v1.logging.warn(\n            'Cannot compute initial values for a masked time series'\n            'with dynamic shape; using the mean instead. This will'\n            'affect heuristic priors and may change the results of'\n            'inference.')\n        observed_initial = observed_mean\n\n    observed_stddev = tf.sqrt(observed_variance)\n    observed_initial_centered = observed_initial - observed_mean\n    return observed_mean, observed_stddev, observed_initial_centered", "language": "python", "code": "def empirical_statistics(observed_time_series):\n  \"\"\"Compute statistics of a provided time series, as heuristic initialization.\n\n  Args:\n    observed_time_series: `Tensor` representing a time series, or batch of time\n       series, of shape either `batch_shape + [num_timesteps, 1]` or\n       `batch_shape + [num_timesteps]` (allowed if `num_timesteps > 1`).\n\n  Returns:\n    observed_mean: `Tensor` of shape `batch_shape`, giving the empirical\n      mean of each time series in the batch.\n    observed_stddev: `Tensor` of shape `batch_shape`, giving the empirical\n      standard deviation of each time series in the batch.\n    observed_initial_centered: `Tensor of shape `batch_shape`, giving the\n      initial value of each time series in the batch after centering\n      (subtracting the mean).\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      'empirical_statistics', values=[observed_time_series]):\n\n    [\n        observed_time_series,\n        mask\n    ] = canonicalize_observed_time_series_with_mask(observed_time_series)\n\n    squeezed_series = observed_time_series[..., 0]\n    if mask is None:\n      observed_mean, observed_variance = tf.nn.moments(\n          x=squeezed_series, axes=-1)\n      observed_initial = squeezed_series[..., 0]\n    else:\n      broadcast_mask = tf.broadcast_to(tf.cast(mask, tf.bool),\n                                       tf.shape(input=squeezed_series))\n      observed_mean, observed_variance = (\n          missing_values_util.moments_of_masked_time_series(\n              squeezed_series, broadcast_mask=broadcast_mask))\n      try:\n        observed_initial = (\n            missing_values_util.initial_value_of_masked_time_series(\n                squeezed_series, broadcast_mask=broadcast_mask))\n      except NotImplementedError:\n        tf.compat.v1.logging.warn(\n            'Cannot compute initial values for a masked time series'\n            'with dynamic shape; using the mean instead. This will'\n            'affect heuristic priors and may change the results of'\n            'inference.')\n        observed_initial = observed_mean\n\n    observed_stddev = tf.sqrt(observed_variance)\n    observed_initial_centered = observed_initial - observed_mean\n    return observed_mean, observed_stddev, observed_initial_centered", "code_tokens": ["def", "empirical_statistics", "(", "observed_time_series", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'empirical_statistics'", ",", "values", "=", "[", "observed_time_series", "]", ")", ":", "[", "observed_time_series", ",", "mask", "]", "=", "canonicalize_observed_time_series_with_mask", "(", "observed_time_series", ")", "squeezed_series", "=", "observed_time_series", "[", "...", ",", "0", "]", "if", "mask", "is", "None", ":", "observed_mean", ",", "observed_variance", "=", "tf", ".", "nn", ".", "moments", "(", "x", "=", "squeezed_series", ",", "axes", "=", "-", "1", ")", "observed_initial", "=", "squeezed_series", "[", "...", ",", "0", "]", "else", ":", "broadcast_mask", "=", "tf", ".", "broadcast_to", "(", "tf", ".", "cast", "(", "mask", ",", "tf", ".", "bool", ")", ",", "tf", ".", "shape", "(", "input", "=", "squeezed_series", ")", ")", "observed_mean", ",", "observed_variance", "=", "(", "missing_values_util", ".", "moments_of_masked_time_series", "(", "squeezed_series", ",", "broadcast_mask", "=", "broadcast_mask", ")", ")", "try", ":", "observed_initial", "=", "(", "missing_values_util", ".", "initial_value_of_masked_time_series", "(", "squeezed_series", ",", "broadcast_mask", "=", "broadcast_mask", ")", ")", "except", "NotImplementedError", ":", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "warn", "(", "'Cannot compute initial values for a masked time series'", "'with dynamic shape; using the mean instead. This will'", "'affect heuristic priors and may change the results of'", "'inference.'", ")", "observed_initial", "=", "observed_mean", "observed_stddev", "=", "tf", ".", "sqrt", "(", "observed_variance", ")", "observed_initial_centered", "=", "observed_initial", "-", "observed_mean", "return", "observed_mean", ",", "observed_stddev", ",", "observed_initial_centered"], "docstring": "Compute statistics of a provided time series, as heuristic initialization.\n\n  Args:\n    observed_time_series: `Tensor` representing a time series, or batch of time\n       series, of shape either `batch_shape + [num_timesteps, 1]` or\n       `batch_shape + [num_timesteps]` (allowed if `num_timesteps > 1`).\n\n  Returns:\n    observed_mean: `Tensor` of shape `batch_shape`, giving the empirical\n      mean of each time series in the batch.\n    observed_stddev: `Tensor` of shape `batch_shape`, giving the empirical\n      standard deviation of each time series in the batch.\n    observed_initial_centered: `Tensor of shape `batch_shape`, giving the\n      initial value of each time series in the batch after centering\n      (subtracting the mean).", "docstring_tokens": ["Compute", "statistics", "of", "a", "provided", "time", "series", "as", "heuristic", "initialization", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/internal/util.py#L201-L252", "partition": "test", "index": 706, "time": "2018-08-23 11:42:59"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/structural_time_series.py", "func_name": "StructuralTimeSeries.make_state_space_model", "original_string": "def make_state_space_model(self,\n                             num_timesteps,\n                             param_vals=None,\n                             initial_state_prior=None,\n                             initial_step=0):\n    \"\"\"Instantiate this model as a Distribution over specified `num_timesteps`.\n\n    Args:\n      num_timesteps: Python `int` number of timesteps to model.\n      param_vals: a list of `Tensor` parameter values in order corresponding to\n        `self.parameters`, or a dict mapping from parameter names to values.\n      initial_state_prior: an optional `Distribution` instance overriding the\n        default prior on the model's initial state. This is used in forecasting\n        (\"today's prior is yesterday's posterior\").\n      initial_step: optional `int` specifying the initial timestep to model.\n        This is relevant when the model contains time-varying components,\n        e.g., holidays or seasonality.\n\n    Returns:\n      dist: a `LinearGaussianStateSpaceModel` Distribution object.\n    \"\"\"\n    return self._make_state_space_model(\n        num_timesteps=num_timesteps,\n        param_map=self._canonicalize_param_vals_as_map(param_vals),\n        initial_state_prior=initial_state_prior,\n        initial_step=initial_step)", "language": "python", "code": "def make_state_space_model(self,\n                             num_timesteps,\n                             param_vals=None,\n                             initial_state_prior=None,\n                             initial_step=0):\n    \"\"\"Instantiate this model as a Distribution over specified `num_timesteps`.\n\n    Args:\n      num_timesteps: Python `int` number of timesteps to model.\n      param_vals: a list of `Tensor` parameter values in order corresponding to\n        `self.parameters`, or a dict mapping from parameter names to values.\n      initial_state_prior: an optional `Distribution` instance overriding the\n        default prior on the model's initial state. This is used in forecasting\n        (\"today's prior is yesterday's posterior\").\n      initial_step: optional `int` specifying the initial timestep to model.\n        This is relevant when the model contains time-varying components,\n        e.g., holidays or seasonality.\n\n    Returns:\n      dist: a `LinearGaussianStateSpaceModel` Distribution object.\n    \"\"\"\n    return self._make_state_space_model(\n        num_timesteps=num_timesteps,\n        param_map=self._canonicalize_param_vals_as_map(param_vals),\n        initial_state_prior=initial_state_prior,\n        initial_step=initial_step)", "code_tokens": ["def", "make_state_space_model", "(", "self", ",", "num_timesteps", ",", "param_vals", "=", "None", ",", "initial_state_prior", "=", "None", ",", "initial_step", "=", "0", ")", ":", "return", "self", ".", "_make_state_space_model", "(", "num_timesteps", "=", "num_timesteps", ",", "param_map", "=", "self", ".", "_canonicalize_param_vals_as_map", "(", "param_vals", ")", ",", "initial_state_prior", "=", "initial_state_prior", ",", "initial_step", "=", "initial_step", ")"], "docstring": "Instantiate this model as a Distribution over specified `num_timesteps`.\n\n    Args:\n      num_timesteps: Python `int` number of timesteps to model.\n      param_vals: a list of `Tensor` parameter values in order corresponding to\n        `self.parameters`, or a dict mapping from parameter names to values.\n      initial_state_prior: an optional `Distribution` instance overriding the\n        default prior on the model's initial state. This is used in forecasting\n        (\"today's prior is yesterday's posterior\").\n      initial_step: optional `int` specifying the initial timestep to model.\n        This is relevant when the model contains time-varying components,\n        e.g., holidays or seasonality.\n\n    Returns:\n      dist: a `LinearGaussianStateSpaceModel` Distribution object.", "docstring_tokens": ["Instantiate", "this", "model", "as", "a", "Distribution", "over", "specified", "num_timesteps", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/structural_time_series.py#L134-L159", "partition": "test", "index": 735, "time": "2018-08-23 11:42:59"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/internal/util.py", "func_name": "_maybe_expand_trailing_dim", "original_string": "def _maybe_expand_trailing_dim(observed_time_series_tensor):\n  \"\"\"Ensures `observed_time_series_tensor` has a trailing dimension of size 1.\n\n  The `tfd.LinearGaussianStateSpaceModel` Distribution has event shape of\n  `[num_timesteps, observation_size]`, but canonical BSTS models\n  are univariate, so their observation_size is always `1`. The extra trailing\n  dimension gets annoying, so this method allows arguments with or without the\n  extra dimension. There is no ambiguity except in the trivial special case\n  where  `num_timesteps = 1`; this can be avoided by specifying any unit-length\n  series in the explicit `[num_timesteps, 1]` style.\n\n  Most users should not call this method directly, and instead call\n  `canonicalize_observed_time_series_with_mask`, which handles converting\n  to `Tensor` and specifying an optional missingness mask.\n\n  Args:\n    observed_time_series_tensor: `Tensor` of shape\n      `batch_shape + [num_timesteps, 1]` or `batch_shape + [num_timesteps]`,\n      where `num_timesteps > 1`.\n\n  Returns:\n    expanded_time_series: `Tensor` of shape `batch_shape + [num_timesteps, 1]`.\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      'maybe_expand_trailing_dim', values=[observed_time_series_tensor]):\n    if (observed_time_series_tensor.shape.ndims is not None and\n        tf.compat.dimension_value(\n            observed_time_series_tensor.shape[-1]) is not None):\n      expanded_time_series = (\n          observed_time_series_tensor\n          if observed_time_series_tensor.shape[-1] == 1\n          else observed_time_series_tensor[..., tf.newaxis])\n    else:\n      expanded_time_series = tf.cond(\n          pred=tf.equal(tf.shape(input=observed_time_series_tensor)[-1], 1),\n          true_fn=lambda: observed_time_series_tensor,\n          false_fn=lambda: observed_time_series_tensor[..., tf.newaxis])\n    return expanded_time_series", "language": "python", "code": "def _maybe_expand_trailing_dim(observed_time_series_tensor):\n  \"\"\"Ensures `observed_time_series_tensor` has a trailing dimension of size 1.\n\n  The `tfd.LinearGaussianStateSpaceModel` Distribution has event shape of\n  `[num_timesteps, observation_size]`, but canonical BSTS models\n  are univariate, so their observation_size is always `1`. The extra trailing\n  dimension gets annoying, so this method allows arguments with or without the\n  extra dimension. There is no ambiguity except in the trivial special case\n  where  `num_timesteps = 1`; this can be avoided by specifying any unit-length\n  series in the explicit `[num_timesteps, 1]` style.\n\n  Most users should not call this method directly, and instead call\n  `canonicalize_observed_time_series_with_mask`, which handles converting\n  to `Tensor` and specifying an optional missingness mask.\n\n  Args:\n    observed_time_series_tensor: `Tensor` of shape\n      `batch_shape + [num_timesteps, 1]` or `batch_shape + [num_timesteps]`,\n      where `num_timesteps > 1`.\n\n  Returns:\n    expanded_time_series: `Tensor` of shape `batch_shape + [num_timesteps, 1]`.\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      'maybe_expand_trailing_dim', values=[observed_time_series_tensor]):\n    if (observed_time_series_tensor.shape.ndims is not None and\n        tf.compat.dimension_value(\n            observed_time_series_tensor.shape[-1]) is not None):\n      expanded_time_series = (\n          observed_time_series_tensor\n          if observed_time_series_tensor.shape[-1] == 1\n          else observed_time_series_tensor[..., tf.newaxis])\n    else:\n      expanded_time_series = tf.cond(\n          pred=tf.equal(tf.shape(input=observed_time_series_tensor)[-1], 1),\n          true_fn=lambda: observed_time_series_tensor,\n          false_fn=lambda: observed_time_series_tensor[..., tf.newaxis])\n    return expanded_time_series", "code_tokens": ["def", "_maybe_expand_trailing_dim", "(", "observed_time_series_tensor", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'maybe_expand_trailing_dim'", ",", "values", "=", "[", "observed_time_series_tensor", "]", ")", ":", "if", "(", "observed_time_series_tensor", ".", "shape", ".", "ndims", "is", "not", "None", "and", "tf", ".", "compat", ".", "dimension_value", "(", "observed_time_series_tensor", ".", "shape", "[", "-", "1", "]", ")", "is", "not", "None", ")", ":", "expanded_time_series", "=", "(", "observed_time_series_tensor", "if", "observed_time_series_tensor", ".", "shape", "[", "-", "1", "]", "==", "1", "else", "observed_time_series_tensor", "[", "...", ",", "tf", ".", "newaxis", "]", ")", "else", ":", "expanded_time_series", "=", "tf", ".", "cond", "(", "pred", "=", "tf", ".", "equal", "(", "tf", ".", "shape", "(", "input", "=", "observed_time_series_tensor", ")", "[", "-", "1", "]", ",", "1", ")", ",", "true_fn", "=", "lambda", ":", "observed_time_series_tensor", ",", "false_fn", "=", "lambda", ":", "observed_time_series_tensor", "[", "...", ",", "tf", ".", "newaxis", "]", ")", "return", "expanded_time_series"], "docstring": "Ensures `observed_time_series_tensor` has a trailing dimension of size 1.\n\n  The `tfd.LinearGaussianStateSpaceModel` Distribution has event shape of\n  `[num_timesteps, observation_size]`, but canonical BSTS models\n  are univariate, so their observation_size is always `1`. The extra trailing\n  dimension gets annoying, so this method allows arguments with or without the\n  extra dimension. There is no ambiguity except in the trivial special case\n  where  `num_timesteps = 1`; this can be avoided by specifying any unit-length\n  series in the explicit `[num_timesteps, 1]` style.\n\n  Most users should not call this method directly, and instead call\n  `canonicalize_observed_time_series_with_mask`, which handles converting\n  to `Tensor` and specifying an optional missingness mask.\n\n  Args:\n    observed_time_series_tensor: `Tensor` of shape\n      `batch_shape + [num_timesteps, 1]` or `batch_shape + [num_timesteps]`,\n      where `num_timesteps > 1`.\n\n  Returns:\n    expanded_time_series: `Tensor` of shape `batch_shape + [num_timesteps, 1]`.", "docstring_tokens": ["Ensures", "observed_time_series_tensor", "has", "a", "trailing", "dimension", "of", "size", "1", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/internal/util.py#L255-L293", "partition": "test", "index": 707, "time": "2018-08-23 11:42:59"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/latent_dirichlet_allocation_edward2.py", "func_name": "latent_dirichlet_allocation", "original_string": "def latent_dirichlet_allocation(concentration, topics_words):\n  \"\"\"Latent Dirichlet Allocation in terms of its generative process.\n\n  The model posits a distribution over bags of words and is parameterized by\n  a concentration and the topic-word probabilities. It collapses per-word\n  topic assignments.\n\n  Args:\n    concentration: A Tensor of shape [1, num_topics], which parameterizes the\n      Dirichlet prior over topics.\n    topics_words: A Tensor of shape [num_topics, num_words], where each row\n      (topic) denotes the probability of each word being in that topic.\n\n  Returns:\n    bag_of_words: A random variable capturing a sample from the model, of shape\n      [1, num_words]. It represents one generated document as a bag of words.\n  \"\"\"\n  topics = ed.Dirichlet(concentration=concentration, name=\"topics\")\n  word_probs = tf.matmul(topics, topics_words)\n  # The observations are bags of words and therefore not one-hot. However,\n  # log_prob of OneHotCategorical computes the probability correctly in\n  # this case.\n  bag_of_words = ed.OneHotCategorical(probs=word_probs, name=\"bag_of_words\")\n  return bag_of_words", "language": "python", "code": "def latent_dirichlet_allocation(concentration, topics_words):\n  \"\"\"Latent Dirichlet Allocation in terms of its generative process.\n\n  The model posits a distribution over bags of words and is parameterized by\n  a concentration and the topic-word probabilities. It collapses per-word\n  topic assignments.\n\n  Args:\n    concentration: A Tensor of shape [1, num_topics], which parameterizes the\n      Dirichlet prior over topics.\n    topics_words: A Tensor of shape [num_topics, num_words], where each row\n      (topic) denotes the probability of each word being in that topic.\n\n  Returns:\n    bag_of_words: A random variable capturing a sample from the model, of shape\n      [1, num_words]. It represents one generated document as a bag of words.\n  \"\"\"\n  topics = ed.Dirichlet(concentration=concentration, name=\"topics\")\n  word_probs = tf.matmul(topics, topics_words)\n  # The observations are bags of words and therefore not one-hot. However,\n  # log_prob of OneHotCategorical computes the probability correctly in\n  # this case.\n  bag_of_words = ed.OneHotCategorical(probs=word_probs, name=\"bag_of_words\")\n  return bag_of_words", "code_tokens": ["def", "latent_dirichlet_allocation", "(", "concentration", ",", "topics_words", ")", ":", "topics", "=", "ed", ".", "Dirichlet", "(", "concentration", "=", "concentration", ",", "name", "=", "\"topics\"", ")", "word_probs", "=", "tf", ".", "matmul", "(", "topics", ",", "topics_words", ")", "# The observations are bags of words and therefore not one-hot. However,", "# log_prob of OneHotCategorical computes the probability correctly in", "# this case.", "bag_of_words", "=", "ed", ".", "OneHotCategorical", "(", "probs", "=", "word_probs", ",", "name", "=", "\"bag_of_words\"", ")", "return", "bag_of_words"], "docstring": "Latent Dirichlet Allocation in terms of its generative process.\n\n  The model posits a distribution over bags of words and is parameterized by\n  a concentration and the topic-word probabilities. It collapses per-word\n  topic assignments.\n\n  Args:\n    concentration: A Tensor of shape [1, num_topics], which parameterizes the\n      Dirichlet prior over topics.\n    topics_words: A Tensor of shape [num_topics, num_words], where each row\n      (topic) denotes the probability of each word being in that topic.\n\n  Returns:\n    bag_of_words: A random variable capturing a sample from the model, of shape\n      [1, num_words]. It represents one generated document as a bag of words.", "docstring_tokens": ["Latent", "Dirichlet", "Allocation", "in", "terms", "of", "its", "generative", "process", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/latent_dirichlet_allocation_edward2.py#L168-L191", "partition": "test", "index": 838, "time": "2018-08-23 19:28:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/deep_exponential_family.py", "func_name": "deep_exponential_family", "original_string": "def deep_exponential_family(data_size, feature_size, units, shape):\n  \"\"\"A multi-layered topic model over a documents-by-terms matrix.\"\"\"\n  w2 = ed.Gamma(0.1, 0.3, sample_shape=[units[2], units[1]], name=\"w2\")\n  w1 = ed.Gamma(0.1, 0.3, sample_shape=[units[1], units[0]], name=\"w1\")\n  w0 = ed.Gamma(0.1, 0.3, sample_shape=[units[0], feature_size], name=\"w0\")\n\n  z2 = ed.Gamma(0.1, 0.1, sample_shape=[data_size, units[2]], name=\"z2\")\n  z1 = ed.Gamma(shape, shape / tf.matmul(z2, w2), name=\"z1\")\n  z0 = ed.Gamma(shape, shape / tf.matmul(z1, w1), name=\"z0\")\n  x = ed.Poisson(tf.matmul(z0, w0), name=\"x\")\n  return x", "language": "python", "code": "def deep_exponential_family(data_size, feature_size, units, shape):\n  \"\"\"A multi-layered topic model over a documents-by-terms matrix.\"\"\"\n  w2 = ed.Gamma(0.1, 0.3, sample_shape=[units[2], units[1]], name=\"w2\")\n  w1 = ed.Gamma(0.1, 0.3, sample_shape=[units[1], units[0]], name=\"w1\")\n  w0 = ed.Gamma(0.1, 0.3, sample_shape=[units[0], feature_size], name=\"w0\")\n\n  z2 = ed.Gamma(0.1, 0.1, sample_shape=[data_size, units[2]], name=\"z2\")\n  z1 = ed.Gamma(shape, shape / tf.matmul(z2, w2), name=\"z1\")\n  z0 = ed.Gamma(shape, shape / tf.matmul(z1, w1), name=\"z0\")\n  x = ed.Poisson(tf.matmul(z0, w0), name=\"x\")\n  return x", "code_tokens": ["def", "deep_exponential_family", "(", "data_size", ",", "feature_size", ",", "units", ",", "shape", ")", ":", "w2", "=", "ed", ".", "Gamma", "(", "0.1", ",", "0.3", ",", "sample_shape", "=", "[", "units", "[", "2", "]", ",", "units", "[", "1", "]", "]", ",", "name", "=", "\"w2\"", ")", "w1", "=", "ed", ".", "Gamma", "(", "0.1", ",", "0.3", ",", "sample_shape", "=", "[", "units", "[", "1", "]", ",", "units", "[", "0", "]", "]", ",", "name", "=", "\"w1\"", ")", "w0", "=", "ed", ".", "Gamma", "(", "0.1", ",", "0.3", ",", "sample_shape", "=", "[", "units", "[", "0", "]", ",", "feature_size", "]", ",", "name", "=", "\"w0\"", ")", "z2", "=", "ed", ".", "Gamma", "(", "0.1", ",", "0.1", ",", "sample_shape", "=", "[", "data_size", ",", "units", "[", "2", "]", "]", ",", "name", "=", "\"z2\"", ")", "z1", "=", "ed", ".", "Gamma", "(", "shape", ",", "shape", "/", "tf", ".", "matmul", "(", "z2", ",", "w2", ")", ",", "name", "=", "\"z1\"", ")", "z0", "=", "ed", ".", "Gamma", "(", "shape", ",", "shape", "/", "tf", ".", "matmul", "(", "z1", ",", "w1", ")", ",", "name", "=", "\"z0\"", ")", "x", "=", "ed", ".", "Poisson", "(", "tf", ".", "matmul", "(", "z0", ",", "w0", ")", ",", "name", "=", "\"x\"", ")", "return", "x"], "docstring": "A multi-layered topic model over a documents-by-terms matrix.", "docstring_tokens": ["A", "multi", "-", "layered", "topic", "model", "over", "a", "documents", "-", "by", "-", "terms", "matrix", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/deep_exponential_family.py#L105-L115", "partition": "test", "index": 1016, "time": "2018-08-24 16:35:21"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/deep_exponential_family.py", "func_name": "trainable_positive_deterministic", "original_string": "def trainable_positive_deterministic(shape, min_loc=1e-3, name=None):\n  \"\"\"Learnable Deterministic distribution over positive reals.\"\"\"\n  with tf.compat.v1.variable_scope(\n      None, default_name=\"trainable_positive_deterministic\"):\n    unconstrained_loc = tf.compat.v1.get_variable(\"unconstrained_loc\", shape)\n    loc = tf.maximum(tf.nn.softplus(unconstrained_loc), min_loc)\n    rv = ed.Deterministic(loc=loc, name=name)\n    return rv", "language": "python", "code": "def trainable_positive_deterministic(shape, min_loc=1e-3, name=None):\n  \"\"\"Learnable Deterministic distribution over positive reals.\"\"\"\n  with tf.compat.v1.variable_scope(\n      None, default_name=\"trainable_positive_deterministic\"):\n    unconstrained_loc = tf.compat.v1.get_variable(\"unconstrained_loc\", shape)\n    loc = tf.maximum(tf.nn.softplus(unconstrained_loc), min_loc)\n    rv = ed.Deterministic(loc=loc, name=name)\n    return rv", "code_tokens": ["def", "trainable_positive_deterministic", "(", "shape", ",", "min_loc", "=", "1e-3", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "None", ",", "default_name", "=", "\"trainable_positive_deterministic\"", ")", ":", "unconstrained_loc", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"unconstrained_loc\"", ",", "shape", ")", "loc", "=", "tf", ".", "maximum", "(", "tf", ".", "nn", ".", "softplus", "(", "unconstrained_loc", ")", ",", "min_loc", ")", "rv", "=", "ed", ".", "Deterministic", "(", "loc", "=", "loc", ",", "name", "=", "name", ")", "return", "rv"], "docstring": "Learnable Deterministic distribution over positive reals.", "docstring_tokens": ["Learnable", "Deterministic", "distribution", "over", "positive", "reals", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/deep_exponential_family.py#L118-L125", "partition": "test", "index": 1017, "time": "2018-08-24 16:35:21"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/deep_exponential_family.py", "func_name": "trainable_gamma", "original_string": "def trainable_gamma(shape, min_concentration=1e-3, min_scale=1e-5, name=None):\n  \"\"\"Learnable Gamma via concentration and scale parameterization.\"\"\"\n  with tf.compat.v1.variable_scope(None, default_name=\"trainable_gamma\"):\n    unconstrained_concentration = tf.compat.v1.get_variable(\n        \"unconstrained_concentration\",\n        shape,\n        initializer=tf.compat.v1.initializers.random_normal(\n            mean=0.5, stddev=0.1))\n    unconstrained_scale = tf.compat.v1.get_variable(\n        \"unconstrained_scale\",\n        shape,\n        initializer=tf.compat.v1.initializers.random_normal(stddev=0.1))\n    concentration = tf.maximum(tf.nn.softplus(unconstrained_concentration),\n                               min_concentration)\n    rate = tf.maximum(1. / tf.nn.softplus(unconstrained_scale), 1. / min_scale)\n    rv = ed.Gamma(concentration=concentration, rate=rate, name=name)\n    return rv", "language": "python", "code": "def trainable_gamma(shape, min_concentration=1e-3, min_scale=1e-5, name=None):\n  \"\"\"Learnable Gamma via concentration and scale parameterization.\"\"\"\n  with tf.compat.v1.variable_scope(None, default_name=\"trainable_gamma\"):\n    unconstrained_concentration = tf.compat.v1.get_variable(\n        \"unconstrained_concentration\",\n        shape,\n        initializer=tf.compat.v1.initializers.random_normal(\n            mean=0.5, stddev=0.1))\n    unconstrained_scale = tf.compat.v1.get_variable(\n        \"unconstrained_scale\",\n        shape,\n        initializer=tf.compat.v1.initializers.random_normal(stddev=0.1))\n    concentration = tf.maximum(tf.nn.softplus(unconstrained_concentration),\n                               min_concentration)\n    rate = tf.maximum(1. / tf.nn.softplus(unconstrained_scale), 1. / min_scale)\n    rv = ed.Gamma(concentration=concentration, rate=rate, name=name)\n    return rv", "code_tokens": ["def", "trainable_gamma", "(", "shape", ",", "min_concentration", "=", "1e-3", ",", "min_scale", "=", "1e-5", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "None", ",", "default_name", "=", "\"trainable_gamma\"", ")", ":", "unconstrained_concentration", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"unconstrained_concentration\"", ",", "shape", ",", "initializer", "=", "tf", ".", "compat", ".", "v1", ".", "initializers", ".", "random_normal", "(", "mean", "=", "0.5", ",", "stddev", "=", "0.1", ")", ")", "unconstrained_scale", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"unconstrained_scale\"", ",", "shape", ",", "initializer", "=", "tf", ".", "compat", ".", "v1", ".", "initializers", ".", "random_normal", "(", "stddev", "=", "0.1", ")", ")", "concentration", "=", "tf", ".", "maximum", "(", "tf", ".", "nn", ".", "softplus", "(", "unconstrained_concentration", ")", ",", "min_concentration", ")", "rate", "=", "tf", ".", "maximum", "(", "1.", "/", "tf", ".", "nn", ".", "softplus", "(", "unconstrained_scale", ")", ",", "1.", "/", "min_scale", ")", "rv", "=", "ed", ".", "Gamma", "(", "concentration", "=", "concentration", ",", "rate", "=", "rate", ",", "name", "=", "name", ")", "return", "rv"], "docstring": "Learnable Gamma via concentration and scale parameterization.", "docstring_tokens": ["Learnable", "Gamma", "via", "concentration", "and", "scale", "parameterization", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/deep_exponential_family.py#L128-L144", "partition": "test", "index": 1018, "time": "2018-08-24 16:35:21"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/deep_exponential_family.py", "func_name": "load_nips2011_papers", "original_string": "def load_nips2011_papers(path):\n  \"\"\"Loads NIPS 2011 conference papers.\n\n  The NIPS 1987-2015 data set is in the form of a 11,463 x 5,812 matrix of\n  per-paper word counts, containing 11,463 words and 5,811 NIPS conference\n  papers (Perrone et al., 2016). We subset to papers in 2011 and words appearing\n  in at least two documents and having a total word count of at least 10.\n\n  Built from the Observations Python package.\n\n  Args:\n    path: str.\n      Path to directory which either stores file or otherwise file will\n      be downloaded and extracted there. Filename is `NIPS_1987-2015.csv`.\n\n  Returns:\n    bag_of_words: np.ndarray of shape [num_documents, num_words]. Each element\n      denotes the number of occurrences of a specific word in a specific\n      document.\n    words: List of strings, denoting the words for `bag_of_words`'s columns.\n  \"\"\"\n  path = os.path.expanduser(path)\n  filename = \"NIPS_1987-2015.csv\"\n  filepath = os.path.join(path, filename)\n  if not os.path.exists(filepath):\n    url = (\"https://archive.ics.uci.edu/ml/machine-learning-databases/\"\n           \"00371/NIPS_1987-2015.csv\")\n    if not tf.io.gfile.exists(path):\n      tf.io.gfile.makedirs(path)\n    print(\"Downloading %s to %s\" % (url, filepath))\n    urllib.request.urlretrieve(url, filepath)\n\n  with open(filepath) as f:\n    iterator = csv.reader(f)\n    documents = next(iterator)[1:]\n    words = []\n    x_train = []\n    for row in iterator:\n      words.append(row[0])\n      x_train.append(row[1:])\n\n  x_train = np.array(x_train, dtype=np.int)\n\n  # Subset to documents in 2011 and words appearing in at least two documents\n  # and have a total word count of at least 10.\n  doc_idx = [i for i, document in enumerate(documents)\n             if document.startswith(\"2011\")]\n  documents = [documents[doc] for doc in doc_idx]\n  x_train = x_train[:, doc_idx]\n  word_idx = np.logical_and(np.sum(x_train != 0, 1) >= 2,\n                            np.sum(x_train, 1) >= 10)\n  words = [word for word, idx in zip(words, word_idx) if idx]\n  bag_of_words = x_train[word_idx, :].T\n  return bag_of_words, words", "language": "python", "code": "def load_nips2011_papers(path):\n  \"\"\"Loads NIPS 2011 conference papers.\n\n  The NIPS 1987-2015 data set is in the form of a 11,463 x 5,812 matrix of\n  per-paper word counts, containing 11,463 words and 5,811 NIPS conference\n  papers (Perrone et al., 2016). We subset to papers in 2011 and words appearing\n  in at least two documents and having a total word count of at least 10.\n\n  Built from the Observations Python package.\n\n  Args:\n    path: str.\n      Path to directory which either stores file or otherwise file will\n      be downloaded and extracted there. Filename is `NIPS_1987-2015.csv`.\n\n  Returns:\n    bag_of_words: np.ndarray of shape [num_documents, num_words]. Each element\n      denotes the number of occurrences of a specific word in a specific\n      document.\n    words: List of strings, denoting the words for `bag_of_words`'s columns.\n  \"\"\"\n  path = os.path.expanduser(path)\n  filename = \"NIPS_1987-2015.csv\"\n  filepath = os.path.join(path, filename)\n  if not os.path.exists(filepath):\n    url = (\"https://archive.ics.uci.edu/ml/machine-learning-databases/\"\n           \"00371/NIPS_1987-2015.csv\")\n    if not tf.io.gfile.exists(path):\n      tf.io.gfile.makedirs(path)\n    print(\"Downloading %s to %s\" % (url, filepath))\n    urllib.request.urlretrieve(url, filepath)\n\n  with open(filepath) as f:\n    iterator = csv.reader(f)\n    documents = next(iterator)[1:]\n    words = []\n    x_train = []\n    for row in iterator:\n      words.append(row[0])\n      x_train.append(row[1:])\n\n  x_train = np.array(x_train, dtype=np.int)\n\n  # Subset to documents in 2011 and words appearing in at least two documents\n  # and have a total word count of at least 10.\n  doc_idx = [i for i, document in enumerate(documents)\n             if document.startswith(\"2011\")]\n  documents = [documents[doc] for doc in doc_idx]\n  x_train = x_train[:, doc_idx]\n  word_idx = np.logical_and(np.sum(x_train != 0, 1) >= 2,\n                            np.sum(x_train, 1) >= 10)\n  words = [word for word, idx in zip(words, word_idx) if idx]\n  bag_of_words = x_train[word_idx, :].T\n  return bag_of_words, words", "code_tokens": ["def", "load_nips2011_papers", "(", "path", ")", ":", "path", "=", "os", ".", "path", ".", "expanduser", "(", "path", ")", "filename", "=", "\"NIPS_1987-2015.csv\"", "filepath", "=", "os", ".", "path", ".", "join", "(", "path", ",", "filename", ")", "if", "not", "os", ".", "path", ".", "exists", "(", "filepath", ")", ":", "url", "=", "(", "\"https://archive.ics.uci.edu/ml/machine-learning-databases/\"", "\"00371/NIPS_1987-2015.csv\"", ")", "if", "not", "tf", ".", "io", ".", "gfile", ".", "exists", "(", "path", ")", ":", "tf", ".", "io", ".", "gfile", ".", "makedirs", "(", "path", ")", "print", "(", "\"Downloading %s to %s\"", "%", "(", "url", ",", "filepath", ")", ")", "urllib", ".", "request", ".", "urlretrieve", "(", "url", ",", "filepath", ")", "with", "open", "(", "filepath", ")", "as", "f", ":", "iterator", "=", "csv", ".", "reader", "(", "f", ")", "documents", "=", "next", "(", "iterator", ")", "[", "1", ":", "]", "words", "=", "[", "]", "x_train", "=", "[", "]", "for", "row", "in", "iterator", ":", "words", ".", "append", "(", "row", "[", "0", "]", ")", "x_train", ".", "append", "(", "row", "[", "1", ":", "]", ")", "x_train", "=", "np", ".", "array", "(", "x_train", ",", "dtype", "=", "np", ".", "int", ")", "# Subset to documents in 2011 and words appearing in at least two documents", "# and have a total word count of at least 10.", "doc_idx", "=", "[", "i", "for", "i", ",", "document", "in", "enumerate", "(", "documents", ")", "if", "document", ".", "startswith", "(", "\"2011\"", ")", "]", "documents", "=", "[", "documents", "[", "doc", "]", "for", "doc", "in", "doc_idx", "]", "x_train", "=", "x_train", "[", ":", ",", "doc_idx", "]", "word_idx", "=", "np", ".", "logical_and", "(", "np", ".", "sum", "(", "x_train", "!=", "0", ",", "1", ")", ">=", "2", ",", "np", ".", "sum", "(", "x_train", ",", "1", ")", ">=", "10", ")", "words", "=", "[", "word", "for", "word", ",", "idx", "in", "zip", "(", "words", ",", "word_idx", ")", "if", "idx", "]", "bag_of_words", "=", "x_train", "[", "word_idx", ",", ":", "]", ".", "T", "return", "bag_of_words", ",", "words"], "docstring": "Loads NIPS 2011 conference papers.\n\n  The NIPS 1987-2015 data set is in the form of a 11,463 x 5,812 matrix of\n  per-paper word counts, containing 11,463 words and 5,811 NIPS conference\n  papers (Perrone et al., 2016). We subset to papers in 2011 and words appearing\n  in at least two documents and having a total word count of at least 10.\n\n  Built from the Observations Python package.\n\n  Args:\n    path: str.\n      Path to directory which either stores file or otherwise file will\n      be downloaded and extracted there. Filename is `NIPS_1987-2015.csv`.\n\n  Returns:\n    bag_of_words: np.ndarray of shape [num_documents, num_words]. Each element\n      denotes the number of occurrences of a specific word in a specific\n      document.\n    words: List of strings, denoting the words for `bag_of_words`'s columns.", "docstring_tokens": ["Loads", "NIPS", "2011", "conference", "papers", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/deep_exponential_family.py#L178-L231", "partition": "test", "index": 1019, "time": "2018-08-24 16:35:21"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/vq_vae.py", "func_name": "load_bernoulli_mnist_dataset", "original_string": "def load_bernoulli_mnist_dataset(directory, split_name):\n  \"\"\"Returns Hugo Larochelle's binary static MNIST tf.data.Dataset.\"\"\"\n  amat_file = download(directory, FILE_TEMPLATE.format(split=split_name))\n  dataset = tf.data.TextLineDataset(amat_file)\n  str_to_arr = lambda string: np.array([c == b\"1\" for c in string.split()])\n\n  def _parser(s):\n    booltensor = tf.compat.v1.py_func(str_to_arr, [s], tf.bool)\n    reshaped = tf.reshape(booltensor, [28, 28, 1])\n    return tf.cast(reshaped, dtype=tf.float32), tf.constant(0, tf.int32)\n\n  return dataset.map(_parser)", "language": "python", "code": "def load_bernoulli_mnist_dataset(directory, split_name):\n  \"\"\"Returns Hugo Larochelle's binary static MNIST tf.data.Dataset.\"\"\"\n  amat_file = download(directory, FILE_TEMPLATE.format(split=split_name))\n  dataset = tf.data.TextLineDataset(amat_file)\n  str_to_arr = lambda string: np.array([c == b\"1\" for c in string.split()])\n\n  def _parser(s):\n    booltensor = tf.compat.v1.py_func(str_to_arr, [s], tf.bool)\n    reshaped = tf.reshape(booltensor, [28, 28, 1])\n    return tf.cast(reshaped, dtype=tf.float32), tf.constant(0, tf.int32)\n\n  return dataset.map(_parser)", "code_tokens": ["def", "load_bernoulli_mnist_dataset", "(", "directory", ",", "split_name", ")", ":", "amat_file", "=", "download", "(", "directory", ",", "FILE_TEMPLATE", ".", "format", "(", "split", "=", "split_name", ")", ")", "dataset", "=", "tf", ".", "data", ".", "TextLineDataset", "(", "amat_file", ")", "str_to_arr", "=", "lambda", "string", ":", "np", ".", "array", "(", "[", "c", "==", "b\"1\"", "for", "c", "in", "string", ".", "split", "(", ")", "]", ")", "def", "_parser", "(", "s", ")", ":", "booltensor", "=", "tf", ".", "compat", ".", "v1", ".", "py_func", "(", "str_to_arr", ",", "[", "s", "]", ",", "tf", ".", "bool", ")", "reshaped", "=", "tf", ".", "reshape", "(", "booltensor", ",", "[", "28", ",", "28", ",", "1", "]", ")", "return", "tf", ".", "cast", "(", "reshaped", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "tf", ".", "constant", "(", "0", ",", "tf", ".", "int32", ")", "return", "dataset", ".", "map", "(", "_parser", ")"], "docstring": "Returns Hugo Larochelle's binary static MNIST tf.data.Dataset.", "docstring_tokens": ["Returns", "Hugo", "Larochelle", "s", "binary", "static", "MNIST", "tf", ".", "data", ".", "Dataset", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/vq_vae.py#L389-L400", "partition": "test", "index": 848, "time": "2018-08-24 16:51:46"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/grammar_vae.py", "func_name": "SmilesGrammar.convert_to_string", "original_string": "def convert_to_string(self, productions):\n    \"\"\"Converts a sequence of productions into a string of terminal symbols.\n\n    Args:\n      productions: Tensor of shape [1, num_productions, num_production_rules].\n        Slices along the `num_productions` dimension represent one-hot vectors.\n\n    Returns:\n      str that concatenates all terminal symbols from `productions`.\n\n    Raises:\n      ValueError: If the first production rule does not begin with\n        `self.start_symbol`.\n    \"\"\"\n    symbols = []\n    for production in tf.unstack(productions, axis=1):\n      lhs, rhs = self.production_rules[tf.argmax(input=production, axis=-1)]\n      if not symbols:  # first iteration\n        if lhs != self.start_symbol:\n          raise ValueError(\"`productions` must begin with `self.start_symbol`.\")\n        symbols = rhs\n      else:\n        # Greedily unroll the nonterminal symbols based on the first occurrence\n        # in a linear sequence.\n        index = symbols.index(lhs)\n        symbols = symbols[:index] + rhs + symbols[index + 1:]\n    string = \"\".join(symbols)\n    return string", "language": "python", "code": "def convert_to_string(self, productions):\n    \"\"\"Converts a sequence of productions into a string of terminal symbols.\n\n    Args:\n      productions: Tensor of shape [1, num_productions, num_production_rules].\n        Slices along the `num_productions` dimension represent one-hot vectors.\n\n    Returns:\n      str that concatenates all terminal symbols from `productions`.\n\n    Raises:\n      ValueError: If the first production rule does not begin with\n        `self.start_symbol`.\n    \"\"\"\n    symbols = []\n    for production in tf.unstack(productions, axis=1):\n      lhs, rhs = self.production_rules[tf.argmax(input=production, axis=-1)]\n      if not symbols:  # first iteration\n        if lhs != self.start_symbol:\n          raise ValueError(\"`productions` must begin with `self.start_symbol`.\")\n        symbols = rhs\n      else:\n        # Greedily unroll the nonterminal symbols based on the first occurrence\n        # in a linear sequence.\n        index = symbols.index(lhs)\n        symbols = symbols[:index] + rhs + symbols[index + 1:]\n    string = \"\".join(symbols)\n    return string", "code_tokens": ["def", "convert_to_string", "(", "self", ",", "productions", ")", ":", "symbols", "=", "[", "]", "for", "production", "in", "tf", ".", "unstack", "(", "productions", ",", "axis", "=", "1", ")", ":", "lhs", ",", "rhs", "=", "self", ".", "production_rules", "[", "tf", ".", "argmax", "(", "input", "=", "production", ",", "axis", "=", "-", "1", ")", "]", "if", "not", "symbols", ":", "# first iteration", "if", "lhs", "!=", "self", ".", "start_symbol", ":", "raise", "ValueError", "(", "\"`productions` must begin with `self.start_symbol`.\"", ")", "symbols", "=", "rhs", "else", ":", "# Greedily unroll the nonterminal symbols based on the first occurrence", "# in a linear sequence.", "index", "=", "symbols", ".", "index", "(", "lhs", ")", "symbols", "=", "symbols", "[", ":", "index", "]", "+", "rhs", "+", "symbols", "[", "index", "+", "1", ":", "]", "string", "=", "\"\"", ".", "join", "(", "symbols", ")", "return", "string"], "docstring": "Converts a sequence of productions into a string of terminal symbols.\n\n    Args:\n      productions: Tensor of shape [1, num_productions, num_production_rules].\n        Slices along the `num_productions` dimension represent one-hot vectors.\n\n    Returns:\n      str that concatenates all terminal symbols from `productions`.\n\n    Raises:\n      ValueError: If the first production rule does not begin with\n        `self.start_symbol`.", "docstring_tokens": ["Converts", "a", "sequence", "of", "productions", "into", "a", "string", "of", "terminal", "symbols", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/grammar_vae.py#L137-L164", "partition": "test", "index": 1100, "time": "2018-08-27 21:04:03"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/grammar_vae.py", "func_name": "ProbabilisticGrammar.call", "original_string": "def call(self, inputs):\n    \"\"\"Runs the model forward to generate a sequence of productions.\n\n    Args:\n      inputs: Unused.\n\n    Returns:\n      productions: Tensor of shape [1, num_productions, num_production_rules].\n        Slices along the `num_productions` dimension represent one-hot vectors.\n    \"\"\"\n    del inputs  # unused\n    latent_code = ed.MultivariateNormalDiag(loc=tf.zeros(self.latent_size),\n                                            sample_shape=1,\n                                            name=\"latent_code\")\n    state = self.lstm.zero_state(1, dtype=tf.float32)\n    t = 0\n    productions = []\n    stack = [self.grammar.start_symbol]\n    while stack:\n      symbol = stack.pop()\n      net, state = self.lstm(latent_code, state)\n      logits = (self.output_layer(net) +\n                self.grammar.mask(symbol, on_value=0., off_value=-1e9))\n      production = ed.OneHotCategorical(logits=logits,\n                                        name=\"production_\" + str(t))\n      _, rhs = self.grammar.production_rules[tf.argmax(\n          input=production, axis=-1)]\n      for symbol in rhs:\n        if symbol in self.grammar.nonterminal_symbols:\n          stack.append(symbol)\n      productions.append(production)\n      t += 1\n    return tf.stack(productions, axis=1)", "language": "python", "code": "def call(self, inputs):\n    \"\"\"Runs the model forward to generate a sequence of productions.\n\n    Args:\n      inputs: Unused.\n\n    Returns:\n      productions: Tensor of shape [1, num_productions, num_production_rules].\n        Slices along the `num_productions` dimension represent one-hot vectors.\n    \"\"\"\n    del inputs  # unused\n    latent_code = ed.MultivariateNormalDiag(loc=tf.zeros(self.latent_size),\n                                            sample_shape=1,\n                                            name=\"latent_code\")\n    state = self.lstm.zero_state(1, dtype=tf.float32)\n    t = 0\n    productions = []\n    stack = [self.grammar.start_symbol]\n    while stack:\n      symbol = stack.pop()\n      net, state = self.lstm(latent_code, state)\n      logits = (self.output_layer(net) +\n                self.grammar.mask(symbol, on_value=0., off_value=-1e9))\n      production = ed.OneHotCategorical(logits=logits,\n                                        name=\"production_\" + str(t))\n      _, rhs = self.grammar.production_rules[tf.argmax(\n          input=production, axis=-1)]\n      for symbol in rhs:\n        if symbol in self.grammar.nonterminal_symbols:\n          stack.append(symbol)\n      productions.append(production)\n      t += 1\n    return tf.stack(productions, axis=1)", "code_tokens": ["def", "call", "(", "self", ",", "inputs", ")", ":", "del", "inputs", "# unused", "latent_code", "=", "ed", ".", "MultivariateNormalDiag", "(", "loc", "=", "tf", ".", "zeros", "(", "self", ".", "latent_size", ")", ",", "sample_shape", "=", "1", ",", "name", "=", "\"latent_code\"", ")", "state", "=", "self", ".", "lstm", ".", "zero_state", "(", "1", ",", "dtype", "=", "tf", ".", "float32", ")", "t", "=", "0", "productions", "=", "[", "]", "stack", "=", "[", "self", ".", "grammar", ".", "start_symbol", "]", "while", "stack", ":", "symbol", "=", "stack", ".", "pop", "(", ")", "net", ",", "state", "=", "self", ".", "lstm", "(", "latent_code", ",", "state", ")", "logits", "=", "(", "self", ".", "output_layer", "(", "net", ")", "+", "self", ".", "grammar", ".", "mask", "(", "symbol", ",", "on_value", "=", "0.", ",", "off_value", "=", "-", "1e9", ")", ")", "production", "=", "ed", ".", "OneHotCategorical", "(", "logits", "=", "logits", ",", "name", "=", "\"production_\"", "+", "str", "(", "t", ")", ")", "_", ",", "rhs", "=", "self", ".", "grammar", ".", "production_rules", "[", "tf", ".", "argmax", "(", "input", "=", "production", ",", "axis", "=", "-", "1", ")", "]", "for", "symbol", "in", "rhs", ":", "if", "symbol", "in", "self", ".", "grammar", ".", "nonterminal_symbols", ":", "stack", ".", "append", "(", "symbol", ")", "productions", ".", "append", "(", "production", ")", "t", "+=", "1", "return", "tf", ".", "stack", "(", "productions", ",", "axis", "=", "1", ")"], "docstring": "Runs the model forward to generate a sequence of productions.\n\n    Args:\n      inputs: Unused.\n\n    Returns:\n      productions: Tensor of shape [1, num_productions, num_production_rules].\n        Slices along the `num_productions` dimension represent one-hot vectors.", "docstring_tokens": ["Runs", "the", "model", "forward", "to", "generate", "a", "sequence", "of", "productions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/grammar_vae.py#L209-L241", "partition": "test", "index": 1101, "time": "2018-08-27 21:04:03"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/grammar_vae.py", "func_name": "ProbabilisticGrammarVariational.call", "original_string": "def call(self, inputs):\n    \"\"\"Runs the model forward to return a stochastic encoding.\n\n    Args:\n      inputs: Tensor of shape [1, num_productions, num_production_rules]. It is\n        a sequence of productions of length `num_productions`. Each production\n        is a one-hot vector of length `num_production_rules`: it determines\n        which production rule the production corresponds to.\n\n    Returns:\n      latent_code_posterior: A random variable capturing a sample from the\n        variational distribution, of shape [1, self.latent_size].\n    \"\"\"\n    net = self.encoder_net(tf.cast(inputs, tf.float32))\n    return ed.MultivariateNormalDiag(\n        loc=net[..., :self.latent_size],\n        scale_diag=tf.nn.softplus(net[..., self.latent_size:]),\n        name=\"latent_code_posterior\")", "language": "python", "code": "def call(self, inputs):\n    \"\"\"Runs the model forward to return a stochastic encoding.\n\n    Args:\n      inputs: Tensor of shape [1, num_productions, num_production_rules]. It is\n        a sequence of productions of length `num_productions`. Each production\n        is a one-hot vector of length `num_production_rules`: it determines\n        which production rule the production corresponds to.\n\n    Returns:\n      latent_code_posterior: A random variable capturing a sample from the\n        variational distribution, of shape [1, self.latent_size].\n    \"\"\"\n    net = self.encoder_net(tf.cast(inputs, tf.float32))\n    return ed.MultivariateNormalDiag(\n        loc=net[..., :self.latent_size],\n        scale_diag=tf.nn.softplus(net[..., self.latent_size:]),\n        name=\"latent_code_posterior\")", "code_tokens": ["def", "call", "(", "self", ",", "inputs", ")", ":", "net", "=", "self", ".", "encoder_net", "(", "tf", ".", "cast", "(", "inputs", ",", "tf", ".", "float32", ")", ")", "return", "ed", ".", "MultivariateNormalDiag", "(", "loc", "=", "net", "[", "...", ",", ":", "self", ".", "latent_size", "]", ",", "scale_diag", "=", "tf", ".", "nn", ".", "softplus", "(", "net", "[", "...", ",", "self", ".", "latent_size", ":", "]", ")", ",", "name", "=", "\"latent_code_posterior\"", ")"], "docstring": "Runs the model forward to return a stochastic encoding.\n\n    Args:\n      inputs: Tensor of shape [1, num_productions, num_production_rules]. It is\n        a sequence of productions of length `num_productions`. Each production\n        is a one-hot vector of length `num_production_rules`: it determines\n        which production rule the production corresponds to.\n\n    Returns:\n      latent_code_posterior: A random variable capturing a sample from the\n        variational distribution, of shape [1, self.latent_size].", "docstring_tokens": ["Runs", "the", "model", "forward", "to", "return", "a", "stochastic", "encoding", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/grammar_vae.py#L267-L284", "partition": "test", "index": 1102, "time": "2018-08-27 21:04:03"}
{"repo": "tensorflow/probability", "path": "experimental/no_u_turn_sampler/nuts.py", "func_name": "_has_no_u_turn", "original_string": "def _has_no_u_turn(state_one, state_two, momentum):\n  \"\"\"If two given states and momentum do not exhibit a U-turn pattern.\"\"\"\n  dot_product = sum([\n      tf.reduce_sum(input_tensor=(s1 - s2) * m)\n      for s1, s2, m in zip(state_one, state_two, momentum)\n  ])\n  return dot_product > 0", "language": "python", "code": "def _has_no_u_turn(state_one, state_two, momentum):\n  \"\"\"If two given states and momentum do not exhibit a U-turn pattern.\"\"\"\n  dot_product = sum([\n      tf.reduce_sum(input_tensor=(s1 - s2) * m)\n      for s1, s2, m in zip(state_one, state_two, momentum)\n  ])\n  return dot_product > 0", "code_tokens": ["def", "_has_no_u_turn", "(", "state_one", ",", "state_two", ",", "momentum", ")", ":", "dot_product", "=", "sum", "(", "[", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "(", "s1", "-", "s2", ")", "*", "m", ")", "for", "s1", ",", "s2", ",", "m", "in", "zip", "(", "state_one", ",", "state_two", ",", "momentum", ")", "]", ")", "return", "dot_product", ">", "0"], "docstring": "If two given states and momentum do not exhibit a U-turn pattern.", "docstring_tokens": ["If", "two", "given", "states", "and", "momentum", "do", "not", "exhibit", "a", "U", "-", "turn", "pattern", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/no_u_turn_sampler/nuts.py#L469-L475", "partition": "test", "index": 803, "time": "2018-08-28 12:10:55"}
{"repo": "tensorflow/probability", "path": "experimental/no_u_turn_sampler/nuts.py", "func_name": "_leapfrog", "original_string": "def _leapfrog(value_and_gradients_fn,\n              current_state,\n              current_grads_target_log_prob,\n              current_momentum,\n              step_size):\n  \"\"\"Runs one step of leapfrog integration.\"\"\"\n  mid_momentum = [\n      m + 0.5 * step * g for m, step, g in\n      zip(current_momentum, step_size, current_grads_target_log_prob)]\n  next_state = [\n      s + step * m for s, step, m in\n      zip(current_state, step_size, mid_momentum)]\n  next_target_log_prob, next_grads_target_log_prob = value_and_gradients_fn(\n      *next_state)\n  next_momentum = [\n      m + 0.5 * step * g for m, step, g in\n      zip(mid_momentum, step_size, next_grads_target_log_prob)]\n  return [\n      next_state,\n      next_target_log_prob,\n      next_grads_target_log_prob,\n      next_momentum,\n  ]", "language": "python", "code": "def _leapfrog(value_and_gradients_fn,\n              current_state,\n              current_grads_target_log_prob,\n              current_momentum,\n              step_size):\n  \"\"\"Runs one step of leapfrog integration.\"\"\"\n  mid_momentum = [\n      m + 0.5 * step * g for m, step, g in\n      zip(current_momentum, step_size, current_grads_target_log_prob)]\n  next_state = [\n      s + step * m for s, step, m in\n      zip(current_state, step_size, mid_momentum)]\n  next_target_log_prob, next_grads_target_log_prob = value_and_gradients_fn(\n      *next_state)\n  next_momentum = [\n      m + 0.5 * step * g for m, step, g in\n      zip(mid_momentum, step_size, next_grads_target_log_prob)]\n  return [\n      next_state,\n      next_target_log_prob,\n      next_grads_target_log_prob,\n      next_momentum,\n  ]", "code_tokens": ["def", "_leapfrog", "(", "value_and_gradients_fn", ",", "current_state", ",", "current_grads_target_log_prob", ",", "current_momentum", ",", "step_size", ")", ":", "mid_momentum", "=", "[", "m", "+", "0.5", "*", "step", "*", "g", "for", "m", ",", "step", ",", "g", "in", "zip", "(", "current_momentum", ",", "step_size", ",", "current_grads_target_log_prob", ")", "]", "next_state", "=", "[", "s", "+", "step", "*", "m", "for", "s", ",", "step", ",", "m", "in", "zip", "(", "current_state", ",", "step_size", ",", "mid_momentum", ")", "]", "next_target_log_prob", ",", "next_grads_target_log_prob", "=", "value_and_gradients_fn", "(", "*", "next_state", ")", "next_momentum", "=", "[", "m", "+", "0.5", "*", "step", "*", "g", "for", "m", ",", "step", ",", "g", "in", "zip", "(", "mid_momentum", ",", "step_size", ",", "next_grads_target_log_prob", ")", "]", "return", "[", "next_state", ",", "next_target_log_prob", ",", "next_grads_target_log_prob", ",", "next_momentum", ",", "]"], "docstring": "Runs one step of leapfrog integration.", "docstring_tokens": ["Runs", "one", "step", "of", "leapfrog", "integration", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/no_u_turn_sampler/nuts.py#L478-L500", "partition": "test", "index": 804, "time": "2018-08-28 12:10:55"}
{"repo": "tensorflow/probability", "path": "experimental/no_u_turn_sampler/nuts.py", "func_name": "_log_joint", "original_string": "def _log_joint(current_target_log_prob, current_momentum):\n  \"\"\"Log-joint probability given a state's log-probability and momentum.\"\"\"\n  momentum_log_prob = -sum(\n      [tf.reduce_sum(input_tensor=0.5 * (m**2.)) for m in current_momentum])\n  return current_target_log_prob + momentum_log_prob", "language": "python", "code": "def _log_joint(current_target_log_prob, current_momentum):\n  \"\"\"Log-joint probability given a state's log-probability and momentum.\"\"\"\n  momentum_log_prob = -sum(\n      [tf.reduce_sum(input_tensor=0.5 * (m**2.)) for m in current_momentum])\n  return current_target_log_prob + momentum_log_prob", "code_tokens": ["def", "_log_joint", "(", "current_target_log_prob", ",", "current_momentum", ")", ":", "momentum_log_prob", "=", "-", "sum", "(", "[", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "0.5", "*", "(", "m", "**", "2.", ")", ")", "for", "m", "in", "current_momentum", "]", ")", "return", "current_target_log_prob", "+", "momentum_log_prob"], "docstring": "Log-joint probability given a state's log-probability and momentum.", "docstring_tokens": ["Log", "-", "joint", "probability", "given", "a", "state", "s", "log", "-", "probability", "and", "momentum", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/no_u_turn_sampler/nuts.py#L503-L507", "partition": "test", "index": 805, "time": "2018-08-28 12:10:55"}
{"repo": "tensorflow/probability", "path": "experimental/no_u_turn_sampler/nuts.py", "func_name": "_random_bernoulli", "original_string": "def _random_bernoulli(shape, probs, dtype=tf.int32, seed=None, name=None):\n  \"\"\"Returns samples from a Bernoulli distribution.\"\"\"\n  with tf.compat.v1.name_scope(name, \"random_bernoulli\", [shape, probs]):\n    probs = tf.convert_to_tensor(value=probs)\n    random_uniform = tf.random.uniform(shape, dtype=probs.dtype, seed=seed)\n    return tf.cast(tf.less(random_uniform, probs), dtype)", "language": "python", "code": "def _random_bernoulli(shape, probs, dtype=tf.int32, seed=None, name=None):\n  \"\"\"Returns samples from a Bernoulli distribution.\"\"\"\n  with tf.compat.v1.name_scope(name, \"random_bernoulli\", [shape, probs]):\n    probs = tf.convert_to_tensor(value=probs)\n    random_uniform = tf.random.uniform(shape, dtype=probs.dtype, seed=seed)\n    return tf.cast(tf.less(random_uniform, probs), dtype)", "code_tokens": ["def", "_random_bernoulli", "(", "shape", ",", "probs", ",", "dtype", "=", "tf", ".", "int32", ",", "seed", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "\"random_bernoulli\"", ",", "[", "shape", ",", "probs", "]", ")", ":", "probs", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "probs", ")", "random_uniform", "=", "tf", ".", "random", ".", "uniform", "(", "shape", ",", "dtype", "=", "probs", ".", "dtype", ",", "seed", "=", "seed", ")", "return", "tf", ".", "cast", "(", "tf", ".", "less", "(", "random_uniform", ",", "probs", ")", ",", "dtype", ")"], "docstring": "Returns samples from a Bernoulli distribution.", "docstring_tokens": ["Returns", "samples", "from", "a", "Bernoulli", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/no_u_turn_sampler/nuts.py#L510-L515", "partition": "test", "index": 806, "time": "2018-08-28 12:10:55"}
{"repo": "tensorflow/probability", "path": "experimental/no_u_turn_sampler/logistic_regression.py", "func_name": "covertype", "original_string": "def covertype():\n  \"\"\"Builds the Covertype data set.\"\"\"\n  import sklearn.datasets  # pylint: disable=g-import-not-at-top\n  data = sklearn.datasets.covtype.fetch_covtype()\n  features = data.data\n  labels = data.target\n\n  # Normalize features and append a column of ones for the intercept.\n  features -= features.mean(0)\n  features /= features.std(0)\n  features = np.hstack([features, np.ones([features.shape[0], 1])])\n  features = tf.cast(features, dtype=tf.float32)\n\n  # Binarize outcomes on whether it is a specific category.\n  _, counts = np.unique(labels, return_counts=True)\n  specific_category = np.argmax(counts)\n  labels = (labels == specific_category)\n  labels = tf.cast(labels, dtype=tf.int32)\n  return features, labels", "language": "python", "code": "def covertype():\n  \"\"\"Builds the Covertype data set.\"\"\"\n  import sklearn.datasets  # pylint: disable=g-import-not-at-top\n  data = sklearn.datasets.covtype.fetch_covtype()\n  features = data.data\n  labels = data.target\n\n  # Normalize features and append a column of ones for the intercept.\n  features -= features.mean(0)\n  features /= features.std(0)\n  features = np.hstack([features, np.ones([features.shape[0], 1])])\n  features = tf.cast(features, dtype=tf.float32)\n\n  # Binarize outcomes on whether it is a specific category.\n  _, counts = np.unique(labels, return_counts=True)\n  specific_category = np.argmax(counts)\n  labels = (labels == specific_category)\n  labels = tf.cast(labels, dtype=tf.int32)\n  return features, labels", "code_tokens": ["def", "covertype", "(", ")", ":", "import", "sklearn", ".", "datasets", "# pylint: disable=g-import-not-at-top", "data", "=", "sklearn", ".", "datasets", ".", "covtype", ".", "fetch_covtype", "(", ")", "features", "=", "data", ".", "data", "labels", "=", "data", ".", "target", "# Normalize features and append a column of ones for the intercept.", "features", "-=", "features", ".", "mean", "(", "0", ")", "features", "/=", "features", ".", "std", "(", "0", ")", "features", "=", "np", ".", "hstack", "(", "[", "features", ",", "np", ".", "ones", "(", "[", "features", ".", "shape", "[", "0", "]", ",", "1", "]", ")", "]", ")", "features", "=", "tf", ".", "cast", "(", "features", ",", "dtype", "=", "tf", ".", "float32", ")", "# Binarize outcomes on whether it is a specific category.", "_", ",", "counts", "=", "np", ".", "unique", "(", "labels", ",", "return_counts", "=", "True", ")", "specific_category", "=", "np", ".", "argmax", "(", "counts", ")", "labels", "=", "(", "labels", "==", "specific_category", ")", "labels", "=", "tf", ".", "cast", "(", "labels", ",", "dtype", "=", "tf", ".", "int32", ")", "return", "features", ",", "labels"], "docstring": "Builds the Covertype data set.", "docstring_tokens": ["Builds", "the", "Covertype", "data", "set", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/no_u_turn_sampler/logistic_regression.py#L67-L85", "partition": "test", "index": 655, "time": "2018-08-28 12:10:55"}
{"repo": "tensorflow/probability", "path": "experimental/no_u_turn_sampler/nuts.py", "func_name": "_embed_no_none_gradient_check", "original_string": "def _embed_no_none_gradient_check(value_and_gradients_fn):\n  \"\"\"Wraps value and gradients function to assist with None gradients.\"\"\"\n  @functools.wraps(value_and_gradients_fn)\n  def func_wrapped(*args, **kwargs):\n    \"\"\"Wrapped function which checks for None gradients.\"\"\"\n    value, grads = value_and_gradients_fn(*args, **kwargs)\n    if any(grad is None for grad in grads):\n      raise ValueError(\"Gradient is None for a state.\")\n    return value, grads\n  return func_wrapped", "language": "python", "code": "def _embed_no_none_gradient_check(value_and_gradients_fn):\n  \"\"\"Wraps value and gradients function to assist with None gradients.\"\"\"\n  @functools.wraps(value_and_gradients_fn)\n  def func_wrapped(*args, **kwargs):\n    \"\"\"Wrapped function which checks for None gradients.\"\"\"\n    value, grads = value_and_gradients_fn(*args, **kwargs)\n    if any(grad is None for grad in grads):\n      raise ValueError(\"Gradient is None for a state.\")\n    return value, grads\n  return func_wrapped", "code_tokens": ["def", "_embed_no_none_gradient_check", "(", "value_and_gradients_fn", ")", ":", "@", "functools", ".", "wraps", "(", "value_and_gradients_fn", ")", "def", "func_wrapped", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "\"\"\"Wrapped function which checks for None gradients.\"\"\"", "value", ",", "grads", "=", "value_and_gradients_fn", "(", "*", "args", ",", "*", "*", "kwargs", ")", "if", "any", "(", "grad", "is", "None", "for", "grad", "in", "grads", ")", ":", "raise", "ValueError", "(", "\"Gradient is None for a state.\"", ")", "return", "value", ",", "grads", "return", "func_wrapped"], "docstring": "Wraps value and gradients function to assist with None gradients.", "docstring_tokens": ["Wraps", "value", "and", "gradients", "function", "to", "assist", "with", "None", "gradients", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/no_u_turn_sampler/nuts.py#L457-L466", "partition": "test", "index": 802, "time": "2018-08-28 12:10:55"}
{"repo": "tensorflow/probability", "path": "experimental/no_u_turn_sampler/logistic_regression.py", "func_name": "logistic_regression", "original_string": "def logistic_regression(features):\n  \"\"\"Bayesian logistic regression, which returns labels given features.\"\"\"\n  coeffs = ed.MultivariateNormalDiag(\n      loc=tf.zeros(features.shape[1]), name=\"coeffs\")\n  labels = ed.Bernoulli(\n      logits=tf.tensordot(features, coeffs, [[1], [0]]), name=\"labels\")\n  return labels", "language": "python", "code": "def logistic_regression(features):\n  \"\"\"Bayesian logistic regression, which returns labels given features.\"\"\"\n  coeffs = ed.MultivariateNormalDiag(\n      loc=tf.zeros(features.shape[1]), name=\"coeffs\")\n  labels = ed.Bernoulli(\n      logits=tf.tensordot(features, coeffs, [[1], [0]]), name=\"labels\")\n  return labels", "code_tokens": ["def", "logistic_regression", "(", "features", ")", ":", "coeffs", "=", "ed", ".", "MultivariateNormalDiag", "(", "loc", "=", "tf", ".", "zeros", "(", "features", ".", "shape", "[", "1", "]", ")", ",", "name", "=", "\"coeffs\"", ")", "labels", "=", "ed", ".", "Bernoulli", "(", "logits", "=", "tf", ".", "tensordot", "(", "features", ",", "coeffs", ",", "[", "[", "1", "]", ",", "[", "0", "]", "]", ")", ",", "name", "=", "\"labels\"", ")", "return", "labels"], "docstring": "Bayesian logistic regression, which returns labels given features.", "docstring_tokens": ["Bayesian", "logistic", "regression", "which", "returns", "labels", "given", "features", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/no_u_turn_sampler/logistic_regression.py#L58-L64", "partition": "test", "index": 654, "time": "2018-08-28 12:10:55"}
{"repo": "tensorflow/probability", "path": "experimental/no_u_turn_sampler/nuts.py", "func_name": "_build_tree", "original_string": "def _build_tree(value_and_gradients_fn,\n                current_state,\n                current_target_log_prob,\n                current_grads_target_log_prob,\n                current_momentum,\n                direction,\n                depth,\n                step_size,\n                log_slice_sample,\n                max_simulation_error=1000.,\n                seed=None):\n  \"\"\"Builds a tree at a given tree depth and at a given state.\n\n  The `current` state is immediately adjacent to, but outside of,\n  the subtrajectory spanned by the returned `forward` and `reverse` states.\n\n  Args:\n    value_and_gradients_fn: Python callable which takes an argument like\n      `*current_state` and returns a tuple of its (possibly unnormalized)\n      log-density under the target distribution and its gradient with respect to\n      each state.\n    current_state: List of `Tensor`s representing the current states of the\n      NUTS trajectory.\n    current_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at the `current_state`.\n    current_grads_target_log_prob: List of `Tensor`s representing gradient of\n      `current_target_log_prob` with respect to `current_state`. Must have same\n      shape as `current_state`.\n    current_momentum: List of `Tensor`s representing the momentums of\n      `current_state`. Must have same shape as `current_state`.\n    direction: int that is either -1 or 1. It determines whether to perform\n      leapfrog integration backwards (reverse) or forward in time respectively.\n    depth: non-negative int that indicates how deep of a tree to build.\n      Each call to `_build_tree` takes `2**depth` leapfrog steps.\n    step_size: List of `Tensor`s representing the step sizes for the leapfrog\n      integrator. Must have same shape as `current_state`.\n    log_slice_sample: The log of an auxiliary slice variable. It is used\n      together with `max_simulation_error` to avoid simulating trajectories with\n      too much numerical error.\n    max_simulation_error: Maximum simulation error to tolerate before\n      terminating the trajectory. Simulation error is the\n      `log_slice_sample` minus the log-joint probability at the simulated state.\n    seed: Integer to seed the random number generator.\n\n  Returns:\n    reverse_state: List of `Tensor`s representing the \"reverse\" states of the\n      NUTS trajectory. Has same shape as `current_state`.\n    reverse_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at the `reverse_state`.\n    reverse_grads_target_log_prob: List of `Tensor`s representing gradient of\n      `reverse_target_log_prob` with respect to `reverse_state`. Has same shape\n      as `reverse_state`.\n    reverse_momentum: List of `Tensor`s representing the momentums of\n      `reverse_state`. Has same shape as `reverse_state`.\n    forward_state: List of `Tensor`s representing the \"forward\" states of the\n      NUTS trajectory. Has same shape as `current_state`.\n    forward_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at the `forward_state`.\n    forward_grads_target_log_prob: List of `Tensor`s representing gradient of\n      `forward_target_log_prob` with respect to `forward_state`. Has same shape\n      as `forward_state`.\n    forward_momentum: List of `Tensor`s representing the momentums of\n      `forward_state`. Has same shape as `forward_state`.\n    next_state: List of `Tensor`s representing the next states of the NUTS\n      trajectory. Has same shape as `current_state`.\n    next_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at `next_state`.\n    next_grads_target_log_prob: List of `Tensor`s representing the gradient of\n      `next_target_log_prob` with respect to `next_state`.\n    num_states: Number of acceptable candidate states in the subtree. A state is\n      acceptable if it is \"in the slice\", that is, if its log-joint probability\n      with its momentum is greater than `log_slice_sample`.\n    continue_trajectory: bool determining whether to continue the simulation\n      trajectory. The trajectory is continued if no U-turns are encountered\n      within the built subtree, and if the log-probability accumulation due to\n      integration error does not exceed `max_simulation_error`.\n  \"\"\"\n  if depth == 0:  # base case\n    # Take a leapfrog step. Terminate the tree-building if the simulation\n    # error from the leapfrog integrator is too large. States discovered by\n    # continuing the simulation are likely to have very low probability.\n    [\n        next_state,\n        next_target_log_prob,\n        next_grads_target_log_prob,\n        next_momentum,\n    ] = _leapfrog(\n        value_and_gradients_fn=value_and_gradients_fn,\n        current_state=current_state,\n        current_grads_target_log_prob=current_grads_target_log_prob,\n        current_momentum=current_momentum,\n        step_size=direction * step_size)\n    next_log_joint = _log_joint(next_target_log_prob, next_momentum)\n    num_states = tf.cast(next_log_joint > log_slice_sample, dtype=tf.int32)\n    continue_trajectory = (next_log_joint >\n                           log_slice_sample - max_simulation_error)\n    return [\n        next_state,\n        next_target_log_prob,\n        next_grads_target_log_prob,\n        next_momentum,\n        next_state,\n        next_target_log_prob,\n        next_grads_target_log_prob,\n        next_momentum,\n        next_state,\n        next_target_log_prob,\n        next_grads_target_log_prob,\n        num_states,\n        continue_trajectory,\n    ]\n\n  # Build a tree at the current state.\n  seed_stream = tfd.SeedStream(seed, \"build_tree\")\n  [\n      reverse_state,\n      reverse_target_log_prob,\n      reverse_grads_target_log_prob,\n      reverse_momentum,\n      forward_state,\n      forward_target_log_prob,\n      forward_grads_target_log_prob,\n      forward_momentum,\n      next_state,\n      next_target_log_prob,\n      next_grads_target_log_prob,\n      num_states,\n      continue_trajectory,\n  ] = _build_tree(value_and_gradients_fn=value_and_gradients_fn,\n                  current_state=current_state,\n                  current_target_log_prob=current_target_log_prob,\n                  current_grads_target_log_prob=current_grads_target_log_prob,\n                  current_momentum=current_momentum,\n                  direction=direction,\n                  depth=depth - 1,\n                  step_size=step_size,\n                  log_slice_sample=log_slice_sample,\n                  seed=seed_stream())\n  if continue_trajectory:\n    # If the just-built subtree did not terminate, build a second subtree at\n    # the forward or reverse state, as appropriate.\n    if direction < 0:\n      [\n          reverse_state,\n          reverse_target_log_prob,\n          reverse_grads_target_log_prob,\n          reverse_momentum,\n          _,\n          _,\n          _,\n          _,\n          far_state,\n          far_target_log_prob,\n          far_grads_target_log_prob,\n          far_num_states,\n          far_continue_trajectory,\n      ] = _build_tree(\n          value_and_gradients_fn=value_and_gradients_fn,\n          current_state=reverse_state,\n          current_target_log_prob=reverse_target_log_prob,\n          current_grads_target_log_prob=reverse_grads_target_log_prob,\n          current_momentum=reverse_momentum,\n          direction=direction,\n          depth=depth - 1,\n          step_size=step_size,\n          log_slice_sample=log_slice_sample,\n          seed=seed_stream())\n    else:\n      [\n          _,\n          _,\n          _,\n          _,\n          forward_state,\n          forward_target_log_prob,\n          forward_grads_target_log_prob,\n          forward_momentum,\n          far_state,\n          far_target_log_prob,\n          far_grads_target_log_prob,\n          far_num_states,\n          far_continue_trajectory,\n      ] = _build_tree(\n          value_and_gradients_fn=value_and_gradients_fn,\n          current_state=forward_state,\n          current_target_log_prob=forward_target_log_prob,\n          current_grads_target_log_prob=forward_grads_target_log_prob,\n          current_momentum=forward_momentum,\n          direction=direction,\n          depth=depth - 1,\n          step_size=step_size,\n          log_slice_sample=log_slice_sample,\n          seed=seed_stream())\n\n    # Propose either `next_state` (which came from the first subtree and so is\n    # nearby) or the new forward/reverse state (which came from the second\n    # subtree and so is far away).\n    num_states += far_num_states\n    accept_far_state = _random_bernoulli(\n        [],\n        probs=far_num_states / num_states,\n        dtype=tf.bool,\n        seed=seed_stream())\n    if accept_far_state:\n      next_state = far_state\n      next_target_log_prob = far_target_log_prob\n      next_grads_target_log_prob = far_grads_target_log_prob\n\n    # Continue the NUTS trajectory if the far subtree did not terminate either,\n    # and if the reverse-most and forward-most states do not exhibit a U-turn.\n    has_no_u_turn = tf.logical_and(\n        _has_no_u_turn(forward_state, reverse_state, forward_momentum),\n        _has_no_u_turn(forward_state, reverse_state, reverse_momentum))\n    continue_trajectory = far_continue_trajectory and has_no_u_turn\n\n  return [\n      reverse_state,\n      reverse_target_log_prob,\n      reverse_grads_target_log_prob,\n      reverse_momentum,\n      forward_state,\n      forward_target_log_prob,\n      forward_grads_target_log_prob,\n      forward_momentum,\n      next_state,\n      next_target_log_prob,\n      next_grads_target_log_prob,\n      num_states,\n      continue_trajectory,\n  ]", "language": "python", "code": "def _build_tree(value_and_gradients_fn,\n                current_state,\n                current_target_log_prob,\n                current_grads_target_log_prob,\n                current_momentum,\n                direction,\n                depth,\n                step_size,\n                log_slice_sample,\n                max_simulation_error=1000.,\n                seed=None):\n  \"\"\"Builds a tree at a given tree depth and at a given state.\n\n  The `current` state is immediately adjacent to, but outside of,\n  the subtrajectory spanned by the returned `forward` and `reverse` states.\n\n  Args:\n    value_and_gradients_fn: Python callable which takes an argument like\n      `*current_state` and returns a tuple of its (possibly unnormalized)\n      log-density under the target distribution and its gradient with respect to\n      each state.\n    current_state: List of `Tensor`s representing the current states of the\n      NUTS trajectory.\n    current_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at the `current_state`.\n    current_grads_target_log_prob: List of `Tensor`s representing gradient of\n      `current_target_log_prob` with respect to `current_state`. Must have same\n      shape as `current_state`.\n    current_momentum: List of `Tensor`s representing the momentums of\n      `current_state`. Must have same shape as `current_state`.\n    direction: int that is either -1 or 1. It determines whether to perform\n      leapfrog integration backwards (reverse) or forward in time respectively.\n    depth: non-negative int that indicates how deep of a tree to build.\n      Each call to `_build_tree` takes `2**depth` leapfrog steps.\n    step_size: List of `Tensor`s representing the step sizes for the leapfrog\n      integrator. Must have same shape as `current_state`.\n    log_slice_sample: The log of an auxiliary slice variable. It is used\n      together with `max_simulation_error` to avoid simulating trajectories with\n      too much numerical error.\n    max_simulation_error: Maximum simulation error to tolerate before\n      terminating the trajectory. Simulation error is the\n      `log_slice_sample` minus the log-joint probability at the simulated state.\n    seed: Integer to seed the random number generator.\n\n  Returns:\n    reverse_state: List of `Tensor`s representing the \"reverse\" states of the\n      NUTS trajectory. Has same shape as `current_state`.\n    reverse_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at the `reverse_state`.\n    reverse_grads_target_log_prob: List of `Tensor`s representing gradient of\n      `reverse_target_log_prob` with respect to `reverse_state`. Has same shape\n      as `reverse_state`.\n    reverse_momentum: List of `Tensor`s representing the momentums of\n      `reverse_state`. Has same shape as `reverse_state`.\n    forward_state: List of `Tensor`s representing the \"forward\" states of the\n      NUTS trajectory. Has same shape as `current_state`.\n    forward_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at the `forward_state`.\n    forward_grads_target_log_prob: List of `Tensor`s representing gradient of\n      `forward_target_log_prob` with respect to `forward_state`. Has same shape\n      as `forward_state`.\n    forward_momentum: List of `Tensor`s representing the momentums of\n      `forward_state`. Has same shape as `forward_state`.\n    next_state: List of `Tensor`s representing the next states of the NUTS\n      trajectory. Has same shape as `current_state`.\n    next_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at `next_state`.\n    next_grads_target_log_prob: List of `Tensor`s representing the gradient of\n      `next_target_log_prob` with respect to `next_state`.\n    num_states: Number of acceptable candidate states in the subtree. A state is\n      acceptable if it is \"in the slice\", that is, if its log-joint probability\n      with its momentum is greater than `log_slice_sample`.\n    continue_trajectory: bool determining whether to continue the simulation\n      trajectory. The trajectory is continued if no U-turns are encountered\n      within the built subtree, and if the log-probability accumulation due to\n      integration error does not exceed `max_simulation_error`.\n  \"\"\"\n  if depth == 0:  # base case\n    # Take a leapfrog step. Terminate the tree-building if the simulation\n    # error from the leapfrog integrator is too large. States discovered by\n    # continuing the simulation are likely to have very low probability.\n    [\n        next_state,\n        next_target_log_prob,\n        next_grads_target_log_prob,\n        next_momentum,\n    ] = _leapfrog(\n        value_and_gradients_fn=value_and_gradients_fn,\n        current_state=current_state,\n        current_grads_target_log_prob=current_grads_target_log_prob,\n        current_momentum=current_momentum,\n        step_size=direction * step_size)\n    next_log_joint = _log_joint(next_target_log_prob, next_momentum)\n    num_states = tf.cast(next_log_joint > log_slice_sample, dtype=tf.int32)\n    continue_trajectory = (next_log_joint >\n                           log_slice_sample - max_simulation_error)\n    return [\n        next_state,\n        next_target_log_prob,\n        next_grads_target_log_prob,\n        next_momentum,\n        next_state,\n        next_target_log_prob,\n        next_grads_target_log_prob,\n        next_momentum,\n        next_state,\n        next_target_log_prob,\n        next_grads_target_log_prob,\n        num_states,\n        continue_trajectory,\n    ]\n\n  # Build a tree at the current state.\n  seed_stream = tfd.SeedStream(seed, \"build_tree\")\n  [\n      reverse_state,\n      reverse_target_log_prob,\n      reverse_grads_target_log_prob,\n      reverse_momentum,\n      forward_state,\n      forward_target_log_prob,\n      forward_grads_target_log_prob,\n      forward_momentum,\n      next_state,\n      next_target_log_prob,\n      next_grads_target_log_prob,\n      num_states,\n      continue_trajectory,\n  ] = _build_tree(value_and_gradients_fn=value_and_gradients_fn,\n                  current_state=current_state,\n                  current_target_log_prob=current_target_log_prob,\n                  current_grads_target_log_prob=current_grads_target_log_prob,\n                  current_momentum=current_momentum,\n                  direction=direction,\n                  depth=depth - 1,\n                  step_size=step_size,\n                  log_slice_sample=log_slice_sample,\n                  seed=seed_stream())\n  if continue_trajectory:\n    # If the just-built subtree did not terminate, build a second subtree at\n    # the forward or reverse state, as appropriate.\n    if direction < 0:\n      [\n          reverse_state,\n          reverse_target_log_prob,\n          reverse_grads_target_log_prob,\n          reverse_momentum,\n          _,\n          _,\n          _,\n          _,\n          far_state,\n          far_target_log_prob,\n          far_grads_target_log_prob,\n          far_num_states,\n          far_continue_trajectory,\n      ] = _build_tree(\n          value_and_gradients_fn=value_and_gradients_fn,\n          current_state=reverse_state,\n          current_target_log_prob=reverse_target_log_prob,\n          current_grads_target_log_prob=reverse_grads_target_log_prob,\n          current_momentum=reverse_momentum,\n          direction=direction,\n          depth=depth - 1,\n          step_size=step_size,\n          log_slice_sample=log_slice_sample,\n          seed=seed_stream())\n    else:\n      [\n          _,\n          _,\n          _,\n          _,\n          forward_state,\n          forward_target_log_prob,\n          forward_grads_target_log_prob,\n          forward_momentum,\n          far_state,\n          far_target_log_prob,\n          far_grads_target_log_prob,\n          far_num_states,\n          far_continue_trajectory,\n      ] = _build_tree(\n          value_and_gradients_fn=value_and_gradients_fn,\n          current_state=forward_state,\n          current_target_log_prob=forward_target_log_prob,\n          current_grads_target_log_prob=forward_grads_target_log_prob,\n          current_momentum=forward_momentum,\n          direction=direction,\n          depth=depth - 1,\n          step_size=step_size,\n          log_slice_sample=log_slice_sample,\n          seed=seed_stream())\n\n    # Propose either `next_state` (which came from the first subtree and so is\n    # nearby) or the new forward/reverse state (which came from the second\n    # subtree and so is far away).\n    num_states += far_num_states\n    accept_far_state = _random_bernoulli(\n        [],\n        probs=far_num_states / num_states,\n        dtype=tf.bool,\n        seed=seed_stream())\n    if accept_far_state:\n      next_state = far_state\n      next_target_log_prob = far_target_log_prob\n      next_grads_target_log_prob = far_grads_target_log_prob\n\n    # Continue the NUTS trajectory if the far subtree did not terminate either,\n    # and if the reverse-most and forward-most states do not exhibit a U-turn.\n    has_no_u_turn = tf.logical_and(\n        _has_no_u_turn(forward_state, reverse_state, forward_momentum),\n        _has_no_u_turn(forward_state, reverse_state, reverse_momentum))\n    continue_trajectory = far_continue_trajectory and has_no_u_turn\n\n  return [\n      reverse_state,\n      reverse_target_log_prob,\n      reverse_grads_target_log_prob,\n      reverse_momentum,\n      forward_state,\n      forward_target_log_prob,\n      forward_grads_target_log_prob,\n      forward_momentum,\n      next_state,\n      next_target_log_prob,\n      next_grads_target_log_prob,\n      num_states,\n      continue_trajectory,\n  ]", "code_tokens": ["def", "_build_tree", "(", "value_and_gradients_fn", ",", "current_state", ",", "current_target_log_prob", ",", "current_grads_target_log_prob", ",", "current_momentum", ",", "direction", ",", "depth", ",", "step_size", ",", "log_slice_sample", ",", "max_simulation_error", "=", "1000.", ",", "seed", "=", "None", ")", ":", "if", "depth", "==", "0", ":", "# base case", "# Take a leapfrog step. Terminate the tree-building if the simulation", "# error from the leapfrog integrator is too large. States discovered by", "# continuing the simulation are likely to have very low probability.", "[", "next_state", ",", "next_target_log_prob", ",", "next_grads_target_log_prob", ",", "next_momentum", ",", "]", "=", "_leapfrog", "(", "value_and_gradients_fn", "=", "value_and_gradients_fn", ",", "current_state", "=", "current_state", ",", "current_grads_target_log_prob", "=", "current_grads_target_log_prob", ",", "current_momentum", "=", "current_momentum", ",", "step_size", "=", "direction", "*", "step_size", ")", "next_log_joint", "=", "_log_joint", "(", "next_target_log_prob", ",", "next_momentum", ")", "num_states", "=", "tf", ".", "cast", "(", "next_log_joint", ">", "log_slice_sample", ",", "dtype", "=", "tf", ".", "int32", ")", "continue_trajectory", "=", "(", "next_log_joint", ">", "log_slice_sample", "-", "max_simulation_error", ")", "return", "[", "next_state", ",", "next_target_log_prob", ",", "next_grads_target_log_prob", ",", "next_momentum", ",", "next_state", ",", "next_target_log_prob", ",", "next_grads_target_log_prob", ",", "next_momentum", ",", "next_state", ",", "next_target_log_prob", ",", "next_grads_target_log_prob", ",", "num_states", ",", "continue_trajectory", ",", "]", "# Build a tree at the current state.", "seed_stream", "=", "tfd", ".", "SeedStream", "(", "seed", ",", "\"build_tree\"", ")", "[", "reverse_state", ",", "reverse_target_log_prob", ",", "reverse_grads_target_log_prob", ",", "reverse_momentum", ",", "forward_state", ",", "forward_target_log_prob", ",", "forward_grads_target_log_prob", ",", "forward_momentum", ",", "next_state", ",", "next_target_log_prob", ",", "next_grads_target_log_prob", ",", "num_states", ",", "continue_trajectory", ",", "]", "=", "_build_tree", "(", "value_and_gradients_fn", "=", "value_and_gradients_fn", ",", "current_state", "=", "current_state", ",", "current_target_log_prob", "=", "current_target_log_prob", ",", "current_grads_target_log_prob", "=", "current_grads_target_log_prob", ",", "current_momentum", "=", "current_momentum", ",", "direction", "=", "direction", ",", "depth", "=", "depth", "-", "1", ",", "step_size", "=", "step_size", ",", "log_slice_sample", "=", "log_slice_sample", ",", "seed", "=", "seed_stream", "(", ")", ")", "if", "continue_trajectory", ":", "# If the just-built subtree did not terminate, build a second subtree at", "# the forward or reverse state, as appropriate.", "if", "direction", "<", "0", ":", "[", "reverse_state", ",", "reverse_target_log_prob", ",", "reverse_grads_target_log_prob", ",", "reverse_momentum", ",", "_", ",", "_", ",", "_", ",", "_", ",", "far_state", ",", "far_target_log_prob", ",", "far_grads_target_log_prob", ",", "far_num_states", ",", "far_continue_trajectory", ",", "]", "=", "_build_tree", "(", "value_and_gradients_fn", "=", "value_and_gradients_fn", ",", "current_state", "=", "reverse_state", ",", "current_target_log_prob", "=", "reverse_target_log_prob", ",", "current_grads_target_log_prob", "=", "reverse_grads_target_log_prob", ",", "current_momentum", "=", "reverse_momentum", ",", "direction", "=", "direction", ",", "depth", "=", "depth", "-", "1", ",", "step_size", "=", "step_size", ",", "log_slice_sample", "=", "log_slice_sample", ",", "seed", "=", "seed_stream", "(", ")", ")", "else", ":", "[", "_", ",", "_", ",", "_", ",", "_", ",", "forward_state", ",", "forward_target_log_prob", ",", "forward_grads_target_log_prob", ",", "forward_momentum", ",", "far_state", ",", "far_target_log_prob", ",", "far_grads_target_log_prob", ",", "far_num_states", ",", "far_continue_trajectory", ",", "]", "=", "_build_tree", "(", "value_and_gradients_fn", "=", "value_and_gradients_fn", ",", "current_state", "=", "forward_state", ",", "current_target_log_prob", "=", "forward_target_log_prob", ",", "current_grads_target_log_prob", "=", "forward_grads_target_log_prob", ",", "current_momentum", "=", "forward_momentum", ",", "direction", "=", "direction", ",", "depth", "=", "depth", "-", "1", ",", "step_size", "=", "step_size", ",", "log_slice_sample", "=", "log_slice_sample", ",", "seed", "=", "seed_stream", "(", ")", ")", "# Propose either `next_state` (which came from the first subtree and so is", "# nearby) or the new forward/reverse state (which came from the second", "# subtree and so is far away).", "num_states", "+=", "far_num_states", "accept_far_state", "=", "_random_bernoulli", "(", "[", "]", ",", "probs", "=", "far_num_states", "/", "num_states", ",", "dtype", "=", "tf", ".", "bool", ",", "seed", "=", "seed_stream", "(", ")", ")", "if", "accept_far_state", ":", "next_state", "=", "far_state", "next_target_log_prob", "=", "far_target_log_prob", "next_grads_target_log_prob", "=", "far_grads_target_log_prob", "# Continue the NUTS trajectory if the far subtree did not terminate either,", "# and if the reverse-most and forward-most states do not exhibit a U-turn.", "has_no_u_turn", "=", "tf", ".", "logical_and", "(", "_has_no_u_turn", "(", "forward_state", ",", "reverse_state", ",", "forward_momentum", ")", ",", "_has_no_u_turn", "(", "forward_state", ",", "reverse_state", ",", "reverse_momentum", ")", ")", "continue_trajectory", "=", "far_continue_trajectory", "and", "has_no_u_turn", "return", "[", "reverse_state", ",", "reverse_target_log_prob", ",", "reverse_grads_target_log_prob", ",", "reverse_momentum", ",", "forward_state", ",", "forward_target_log_prob", ",", "forward_grads_target_log_prob", ",", "forward_momentum", ",", "next_state", ",", "next_target_log_prob", ",", "next_grads_target_log_prob", ",", "num_states", ",", "continue_trajectory", ",", "]"], "docstring": "Builds a tree at a given tree depth and at a given state.\n\n  The `current` state is immediately adjacent to, but outside of,\n  the subtrajectory spanned by the returned `forward` and `reverse` states.\n\n  Args:\n    value_and_gradients_fn: Python callable which takes an argument like\n      `*current_state` and returns a tuple of its (possibly unnormalized)\n      log-density under the target distribution and its gradient with respect to\n      each state.\n    current_state: List of `Tensor`s representing the current states of the\n      NUTS trajectory.\n    current_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at the `current_state`.\n    current_grads_target_log_prob: List of `Tensor`s representing gradient of\n      `current_target_log_prob` with respect to `current_state`. Must have same\n      shape as `current_state`.\n    current_momentum: List of `Tensor`s representing the momentums of\n      `current_state`. Must have same shape as `current_state`.\n    direction: int that is either -1 or 1. It determines whether to perform\n      leapfrog integration backwards (reverse) or forward in time respectively.\n    depth: non-negative int that indicates how deep of a tree to build.\n      Each call to `_build_tree` takes `2**depth` leapfrog steps.\n    step_size: List of `Tensor`s representing the step sizes for the leapfrog\n      integrator. Must have same shape as `current_state`.\n    log_slice_sample: The log of an auxiliary slice variable. It is used\n      together with `max_simulation_error` to avoid simulating trajectories with\n      too much numerical error.\n    max_simulation_error: Maximum simulation error to tolerate before\n      terminating the trajectory. Simulation error is the\n      `log_slice_sample` minus the log-joint probability at the simulated state.\n    seed: Integer to seed the random number generator.\n\n  Returns:\n    reverse_state: List of `Tensor`s representing the \"reverse\" states of the\n      NUTS trajectory. Has same shape as `current_state`.\n    reverse_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at the `reverse_state`.\n    reverse_grads_target_log_prob: List of `Tensor`s representing gradient of\n      `reverse_target_log_prob` with respect to `reverse_state`. Has same shape\n      as `reverse_state`.\n    reverse_momentum: List of `Tensor`s representing the momentums of\n      `reverse_state`. Has same shape as `reverse_state`.\n    forward_state: List of `Tensor`s representing the \"forward\" states of the\n      NUTS trajectory. Has same shape as `current_state`.\n    forward_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at the `forward_state`.\n    forward_grads_target_log_prob: List of `Tensor`s representing gradient of\n      `forward_target_log_prob` with respect to `forward_state`. Has same shape\n      as `forward_state`.\n    forward_momentum: List of `Tensor`s representing the momentums of\n      `forward_state`. Has same shape as `forward_state`.\n    next_state: List of `Tensor`s representing the next states of the NUTS\n      trajectory. Has same shape as `current_state`.\n    next_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at `next_state`.\n    next_grads_target_log_prob: List of `Tensor`s representing the gradient of\n      `next_target_log_prob` with respect to `next_state`.\n    num_states: Number of acceptable candidate states in the subtree. A state is\n      acceptable if it is \"in the slice\", that is, if its log-joint probability\n      with its momentum is greater than `log_slice_sample`.\n    continue_trajectory: bool determining whether to continue the simulation\n      trajectory. The trajectory is continued if no U-turns are encountered\n      within the built subtree, and if the log-probability accumulation due to\n      integration error does not exceed `max_simulation_error`.", "docstring_tokens": ["Builds", "a", "tree", "at", "a", "given", "tree", "depth", "and", "at", "a", "given", "state", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/no_u_turn_sampler/nuts.py#L225-L454", "partition": "test", "index": 801, "time": "2018-08-28 12:10:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/positive_semidefinite_kernels/internal/util.py", "func_name": "sqrt_with_finite_grads", "original_string": "def sqrt_with_finite_grads(x, name=None):\n  \"\"\"A sqrt function whose gradient at zero is very large but finite.\n\n  Args:\n    x: a `Tensor` whose sqrt is to be computed.\n    name: a Python `str` prefixed to all ops created by this function.\n      Default `None` (i.e., \"sqrt_with_finite_grads\").\n\n  Returns:\n    sqrt: the square root of `x`, with an overridden gradient at zero\n    grad: a gradient function, which is the same as sqrt's gradient everywhere\n      except at zero, where it is given a large finite value, instead of `inf`.\n\n  Raises:\n    TypeError: if `tf.convert_to_tensor(x)` is not a `float` type.\n\n  Often in kernel functions, we need to compute the L2 norm of the difference\n  between two vectors, `x` and `y`: `sqrt(sum_i((x_i - y_i) ** 2))`. In the\n  case where `x` and `y` are identical, e.g., on the diagonal of a kernel\n  matrix, we get `NaN`s when we take gradients with respect to the inputs. To\n  see, this consider the forward pass:\n\n    ```\n    [x_1 ... x_N]  -->  [x_1 ** 2 ... x_N ** 2]  -->\n        (x_1 ** 2 + ... + x_N ** 2)  -->  sqrt((x_1 ** 2 + ... + x_N ** 2))\n    ```\n\n  When we backprop through this forward pass, the `sqrt` yields an `inf` because\n  `grad_z(sqrt(z)) = 1 / (2 * sqrt(z))`. Continuing the backprop to the left, at\n  the `x ** 2` term, we pick up a `2 * x`, and when `x` is zero, we get\n  `0 * inf`, which is `NaN`.\n\n  We'd like to avoid these `NaN`s, since they infect the rest of the connected\n  computation graph. Practically, when two inputs to a kernel function are\n  equal, we are in one of two scenarios:\n    1. We are actually computing k(x, x), in which case norm(x - x) is\n       identically zero, independent of x. In this case, we'd like the\n       gradient to reflect this independence: it should be zero.\n    2. We are computing k(x, y), and x just *happens* to have the same value\n       as y. The gradient at such inputs is in fact ill-defined (there is a\n       cusp in the sqrt((x - y) ** 2) surface along the line x = y). There are,\n       however, an infinite number of sub-gradients, all of which are valid at\n       all such inputs. By symmetry, there is exactly one which is \"special\":\n       zero, and we elect to use that value here. In practice, having two\n       identical inputs to a kernel matrix is probably a pathological\n       situation to be avoided, but that is better resolved at a higher level\n       than this.\n\n  To avoid the infinite gradient at zero, we use tf.custom_gradient to redefine\n  the gradient at zero. We assign it to be a very large value, specifically\n  the sqrt of the max value of the floating point dtype of the input. We use\n  the sqrt (as opposed to just using the max floating point value) to avoid\n  potential overflow when combining this value with others downstream.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'sqrt_with_finite_grads', [x]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    if not x.dtype.is_floating:\n      raise TypeError('Input `x` must be floating type.')\n    def grad(grad_ys):\n      large_float_like_x = np.sqrt(np.finfo(x.dtype.as_numpy_dtype()).max)\n      safe_grads = tf.where(\n          tf.equal(x, 0), tf.fill(tf.shape(input=x), large_float_like_x),\n          0.5 * tf.math.rsqrt(x))\n      return grad_ys * safe_grads\n    return tf.sqrt(x), grad", "language": "python", "code": "def sqrt_with_finite_grads(x, name=None):\n  \"\"\"A sqrt function whose gradient at zero is very large but finite.\n\n  Args:\n    x: a `Tensor` whose sqrt is to be computed.\n    name: a Python `str` prefixed to all ops created by this function.\n      Default `None` (i.e., \"sqrt_with_finite_grads\").\n\n  Returns:\n    sqrt: the square root of `x`, with an overridden gradient at zero\n    grad: a gradient function, which is the same as sqrt's gradient everywhere\n      except at zero, where it is given a large finite value, instead of `inf`.\n\n  Raises:\n    TypeError: if `tf.convert_to_tensor(x)` is not a `float` type.\n\n  Often in kernel functions, we need to compute the L2 norm of the difference\n  between two vectors, `x` and `y`: `sqrt(sum_i((x_i - y_i) ** 2))`. In the\n  case where `x` and `y` are identical, e.g., on the diagonal of a kernel\n  matrix, we get `NaN`s when we take gradients with respect to the inputs. To\n  see, this consider the forward pass:\n\n    ```\n    [x_1 ... x_N]  -->  [x_1 ** 2 ... x_N ** 2]  -->\n        (x_1 ** 2 + ... + x_N ** 2)  -->  sqrt((x_1 ** 2 + ... + x_N ** 2))\n    ```\n\n  When we backprop through this forward pass, the `sqrt` yields an `inf` because\n  `grad_z(sqrt(z)) = 1 / (2 * sqrt(z))`. Continuing the backprop to the left, at\n  the `x ** 2` term, we pick up a `2 * x`, and when `x` is zero, we get\n  `0 * inf`, which is `NaN`.\n\n  We'd like to avoid these `NaN`s, since they infect the rest of the connected\n  computation graph. Practically, when two inputs to a kernel function are\n  equal, we are in one of two scenarios:\n    1. We are actually computing k(x, x), in which case norm(x - x) is\n       identically zero, independent of x. In this case, we'd like the\n       gradient to reflect this independence: it should be zero.\n    2. We are computing k(x, y), and x just *happens* to have the same value\n       as y. The gradient at such inputs is in fact ill-defined (there is a\n       cusp in the sqrt((x - y) ** 2) surface along the line x = y). There are,\n       however, an infinite number of sub-gradients, all of which are valid at\n       all such inputs. By symmetry, there is exactly one which is \"special\":\n       zero, and we elect to use that value here. In practice, having two\n       identical inputs to a kernel matrix is probably a pathological\n       situation to be avoided, but that is better resolved at a higher level\n       than this.\n\n  To avoid the infinite gradient at zero, we use tf.custom_gradient to redefine\n  the gradient at zero. We assign it to be a very large value, specifically\n  the sqrt of the max value of the floating point dtype of the input. We use\n  the sqrt (as opposed to just using the max floating point value) to avoid\n  potential overflow when combining this value with others downstream.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'sqrt_with_finite_grads', [x]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    if not x.dtype.is_floating:\n      raise TypeError('Input `x` must be floating type.')\n    def grad(grad_ys):\n      large_float_like_x = np.sqrt(np.finfo(x.dtype.as_numpy_dtype()).max)\n      safe_grads = tf.where(\n          tf.equal(x, 0), tf.fill(tf.shape(input=x), large_float_like_x),\n          0.5 * tf.math.rsqrt(x))\n      return grad_ys * safe_grads\n    return tf.sqrt(x), grad", "code_tokens": ["def", "sqrt_with_finite_grads", "(", "x", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'sqrt_with_finite_grads'", ",", "[", "x", "]", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ")", "if", "not", "x", ".", "dtype", ".", "is_floating", ":", "raise", "TypeError", "(", "'Input `x` must be floating type.'", ")", "def", "grad", "(", "grad_ys", ")", ":", "large_float_like_x", "=", "np", ".", "sqrt", "(", "np", ".", "finfo", "(", "x", ".", "dtype", ".", "as_numpy_dtype", "(", ")", ")", ".", "max", ")", "safe_grads", "=", "tf", ".", "where", "(", "tf", ".", "equal", "(", "x", ",", "0", ")", ",", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "large_float_like_x", ")", ",", "0.5", "*", "tf", ".", "math", ".", "rsqrt", "(", "x", ")", ")", "return", "grad_ys", "*", "safe_grads", "return", "tf", ".", "sqrt", "(", "x", ")", ",", "grad"], "docstring": "A sqrt function whose gradient at zero is very large but finite.\n\n  Args:\n    x: a `Tensor` whose sqrt is to be computed.\n    name: a Python `str` prefixed to all ops created by this function.\n      Default `None` (i.e., \"sqrt_with_finite_grads\").\n\n  Returns:\n    sqrt: the square root of `x`, with an overridden gradient at zero\n    grad: a gradient function, which is the same as sqrt's gradient everywhere\n      except at zero, where it is given a large finite value, instead of `inf`.\n\n  Raises:\n    TypeError: if `tf.convert_to_tensor(x)` is not a `float` type.\n\n  Often in kernel functions, we need to compute the L2 norm of the difference\n  between two vectors, `x` and `y`: `sqrt(sum_i((x_i - y_i) ** 2))`. In the\n  case where `x` and `y` are identical, e.g., on the diagonal of a kernel\n  matrix, we get `NaN`s when we take gradients with respect to the inputs. To\n  see, this consider the forward pass:\n\n    ```\n    [x_1 ... x_N]  -->  [x_1 ** 2 ... x_N ** 2]  -->\n        (x_1 ** 2 + ... + x_N ** 2)  -->  sqrt((x_1 ** 2 + ... + x_N ** 2))\n    ```\n\n  When we backprop through this forward pass, the `sqrt` yields an `inf` because\n  `grad_z(sqrt(z)) = 1 / (2 * sqrt(z))`. Continuing the backprop to the left, at\n  the `x ** 2` term, we pick up a `2 * x`, and when `x` is zero, we get\n  `0 * inf`, which is `NaN`.\n\n  We'd like to avoid these `NaN`s, since they infect the rest of the connected\n  computation graph. Practically, when two inputs to a kernel function are\n  equal, we are in one of two scenarios:\n    1. We are actually computing k(x, x), in which case norm(x - x) is\n       identically zero, independent of x. In this case, we'd like the\n       gradient to reflect this independence: it should be zero.\n    2. We are computing k(x, y), and x just *happens* to have the same value\n       as y. The gradient at such inputs is in fact ill-defined (there is a\n       cusp in the sqrt((x - y) ** 2) surface along the line x = y). There are,\n       however, an infinite number of sub-gradients, all of which are valid at\n       all such inputs. By symmetry, there is exactly one which is \"special\":\n       zero, and we elect to use that value here. In practice, having two\n       identical inputs to a kernel matrix is probably a pathological\n       situation to be avoided, but that is better resolved at a higher level\n       than this.\n\n  To avoid the infinite gradient at zero, we use tf.custom_gradient to redefine\n  the gradient at zero. We assign it to be a very large value, specifically\n  the sqrt of the max value of the floating point dtype of the input. We use\n  the sqrt (as opposed to just using the max floating point value) to avoid\n  potential overflow when combining this value with others downstream.", "docstring_tokens": ["A", "sqrt", "function", "whose", "gradient", "at", "zero", "is", "very", "large", "but", "finite", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/positive_semidefinite_kernels/internal/util.py#L90-L154", "partition": "test", "index": 1070, "time": "2018-08-31 11:29:09"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/models/bayesian_resnet.py", "func_name": "bayesian_resnet", "original_string": "def bayesian_resnet(input_shape,\n                    num_classes=10,\n                    kernel_posterior_scale_mean=-9.0,\n                    kernel_posterior_scale_stddev=0.1,\n                    kernel_posterior_scale_constraint=0.2):\n  \"\"\"Constructs a ResNet18 model.\n\n  Args:\n    input_shape: A `tuple` indicating the Tensor shape.\n    num_classes: `int` representing the number of class labels.\n    kernel_posterior_scale_mean: Python `int` number for the kernel\n      posterior's scale (log variance) mean. The smaller the mean the closer\n      is the initialization to a deterministic network.\n    kernel_posterior_scale_stddev: Python `float` number for the initial kernel\n      posterior's scale stddev.\n      ```\n      q(W|x) ~ N(mu, var),\n      log_var ~ N(kernel_posterior_scale_mean, kernel_posterior_scale_stddev)\n      ````\n    kernel_posterior_scale_constraint: Python `float` number for the log value\n      to constrain the log variance throughout training.\n      i.e. log_var <= log(kernel_posterior_scale_constraint).\n\n  Returns:\n    tf.keras.Model.\n  \"\"\"\n\n  filters = [64, 128, 256, 512]\n  kernels = [3, 3, 3, 3]\n  strides = [1, 2, 2, 2]\n\n  def _untransformed_scale_constraint(t):\n    return tf.clip_by_value(t, -1000,\n                            tf.math.log(kernel_posterior_scale_constraint))\n\n  kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn(\n      untransformed_scale_initializer=tf.compat.v1.initializers.random_normal(\n          mean=kernel_posterior_scale_mean,\n          stddev=kernel_posterior_scale_stddev),\n      untransformed_scale_constraint=_untransformed_scale_constraint)\n\n  image = tf.keras.layers.Input(shape=input_shape, dtype='float32')\n  x = tfp.layers.Convolution2DFlipout(\n      64,\n      3,\n      strides=1,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(image)\n\n  for i in range(len(kernels)):\n    x = _resnet_block(\n        x,\n        filters[i],\n        kernels[i],\n        strides[i],\n        kernel_posterior_fn)\n\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Activation('relu')(x)\n  x = tf.keras.layers.AveragePooling2D(4, 1)(x)\n  x = tf.keras.layers.Flatten()(x)\n\n  x = tfp.layers.DenseFlipout(\n      num_classes,\n      kernel_posterior_fn=kernel_posterior_fn)(x)\n\n  model = tf.keras.Model(inputs=image, outputs=x, name='resnet18')\n  return model", "language": "python", "code": "def bayesian_resnet(input_shape,\n                    num_classes=10,\n                    kernel_posterior_scale_mean=-9.0,\n                    kernel_posterior_scale_stddev=0.1,\n                    kernel_posterior_scale_constraint=0.2):\n  \"\"\"Constructs a ResNet18 model.\n\n  Args:\n    input_shape: A `tuple` indicating the Tensor shape.\n    num_classes: `int` representing the number of class labels.\n    kernel_posterior_scale_mean: Python `int` number for the kernel\n      posterior's scale (log variance) mean. The smaller the mean the closer\n      is the initialization to a deterministic network.\n    kernel_posterior_scale_stddev: Python `float` number for the initial kernel\n      posterior's scale stddev.\n      ```\n      q(W|x) ~ N(mu, var),\n      log_var ~ N(kernel_posterior_scale_mean, kernel_posterior_scale_stddev)\n      ````\n    kernel_posterior_scale_constraint: Python `float` number for the log value\n      to constrain the log variance throughout training.\n      i.e. log_var <= log(kernel_posterior_scale_constraint).\n\n  Returns:\n    tf.keras.Model.\n  \"\"\"\n\n  filters = [64, 128, 256, 512]\n  kernels = [3, 3, 3, 3]\n  strides = [1, 2, 2, 2]\n\n  def _untransformed_scale_constraint(t):\n    return tf.clip_by_value(t, -1000,\n                            tf.math.log(kernel_posterior_scale_constraint))\n\n  kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn(\n      untransformed_scale_initializer=tf.compat.v1.initializers.random_normal(\n          mean=kernel_posterior_scale_mean,\n          stddev=kernel_posterior_scale_stddev),\n      untransformed_scale_constraint=_untransformed_scale_constraint)\n\n  image = tf.keras.layers.Input(shape=input_shape, dtype='float32')\n  x = tfp.layers.Convolution2DFlipout(\n      64,\n      3,\n      strides=1,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(image)\n\n  for i in range(len(kernels)):\n    x = _resnet_block(\n        x,\n        filters[i],\n        kernels[i],\n        strides[i],\n        kernel_posterior_fn)\n\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Activation('relu')(x)\n  x = tf.keras.layers.AveragePooling2D(4, 1)(x)\n  x = tf.keras.layers.Flatten()(x)\n\n  x = tfp.layers.DenseFlipout(\n      num_classes,\n      kernel_posterior_fn=kernel_posterior_fn)(x)\n\n  model = tf.keras.Model(inputs=image, outputs=x, name='resnet18')\n  return model", "code_tokens": ["def", "bayesian_resnet", "(", "input_shape", ",", "num_classes", "=", "10", ",", "kernel_posterior_scale_mean", "=", "-", "9.0", ",", "kernel_posterior_scale_stddev", "=", "0.1", ",", "kernel_posterior_scale_constraint", "=", "0.2", ")", ":", "filters", "=", "[", "64", ",", "128", ",", "256", ",", "512", "]", "kernels", "=", "[", "3", ",", "3", ",", "3", ",", "3", "]", "strides", "=", "[", "1", ",", "2", ",", "2", ",", "2", "]", "def", "_untransformed_scale_constraint", "(", "t", ")", ":", "return", "tf", ".", "clip_by_value", "(", "t", ",", "-", "1000", ",", "tf", ".", "math", ".", "log", "(", "kernel_posterior_scale_constraint", ")", ")", "kernel_posterior_fn", "=", "tfp", ".", "layers", ".", "default_mean_field_normal_fn", "(", "untransformed_scale_initializer", "=", "tf", ".", "compat", ".", "v1", ".", "initializers", ".", "random_normal", "(", "mean", "=", "kernel_posterior_scale_mean", ",", "stddev", "=", "kernel_posterior_scale_stddev", ")", ",", "untransformed_scale_constraint", "=", "_untransformed_scale_constraint", ")", "image", "=", "tf", ".", "keras", ".", "layers", ".", "Input", "(", "shape", "=", "input_shape", ",", "dtype", "=", "'float32'", ")", "x", "=", "tfp", ".", "layers", ".", "Convolution2DFlipout", "(", "64", ",", "3", ",", "strides", "=", "1", ",", "padding", "=", "'same'", ",", "kernel_posterior_fn", "=", "kernel_posterior_fn", ")", "(", "image", ")", "for", "i", "in", "range", "(", "len", "(", "kernels", ")", ")", ":", "x", "=", "_resnet_block", "(", "x", ",", "filters", "[", "i", "]", ",", "kernels", "[", "i", "]", ",", "strides", "[", "i", "]", ",", "kernel_posterior_fn", ")", "x", "=", "tf", ".", "keras", ".", "layers", ".", "BatchNormalization", "(", ")", "(", "x", ")", "x", "=", "tf", ".", "keras", ".", "layers", ".", "Activation", "(", "'relu'", ")", "(", "x", ")", "x", "=", "tf", ".", "keras", ".", "layers", ".", "AveragePooling2D", "(", "4", ",", "1", ")", "(", "x", ")", "x", "=", "tf", ".", "keras", ".", "layers", ".", "Flatten", "(", ")", "(", "x", ")", "x", "=", "tfp", ".", "layers", ".", "DenseFlipout", "(", "num_classes", ",", "kernel_posterior_fn", "=", "kernel_posterior_fn", ")", "(", "x", ")", "model", "=", "tf", ".", "keras", ".", "Model", "(", "inputs", "=", "image", ",", "outputs", "=", "x", ",", "name", "=", "'resnet18'", ")", "return", "model"], "docstring": "Constructs a ResNet18 model.\n\n  Args:\n    input_shape: A `tuple` indicating the Tensor shape.\n    num_classes: `int` representing the number of class labels.\n    kernel_posterior_scale_mean: Python `int` number for the kernel\n      posterior's scale (log variance) mean. The smaller the mean the closer\n      is the initialization to a deterministic network.\n    kernel_posterior_scale_stddev: Python `float` number for the initial kernel\n      posterior's scale stddev.\n      ```\n      q(W|x) ~ N(mu, var),\n      log_var ~ N(kernel_posterior_scale_mean, kernel_posterior_scale_stddev)\n      ````\n    kernel_posterior_scale_constraint: Python `float` number for the log value\n      to constrain the log variance throughout training.\n      i.e. log_var <= log(kernel_posterior_scale_constraint).\n\n  Returns:\n    tf.keras.Model.", "docstring_tokens": ["Constructs", "a", "ResNet18", "model", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/models/bayesian_resnet.py#L25-L92", "partition": "test", "index": 1010, "time": "2018-09-06 10:42:58"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/models/bayesian_vgg.py", "func_name": "_vggconv_block", "original_string": "def _vggconv_block(x, filters, kernel, stride, kernel_posterior_fn):\n  \"\"\"Network block for VGG.\"\"\"\n  out = tfp.layers.Convolution2DFlipout(\n      filters,\n      kernel,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(x)\n  out = tf.keras.layers.BatchNormalization()(out)\n  out = tf.keras.layers.Activation('relu')(out)\n\n  out = tfp.layers.Convolution2DFlipout(\n      filters,\n      kernel,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(out)\n  out = tf.keras.layers.BatchNormalization()(out)\n  out = tf.keras.layers.Activation('relu')(out)\n\n  out = tf.keras.layers.MaxPooling2D(\n      pool_size=(2, 2), strides=stride)(out)\n  return out", "language": "python", "code": "def _vggconv_block(x, filters, kernel, stride, kernel_posterior_fn):\n  \"\"\"Network block for VGG.\"\"\"\n  out = tfp.layers.Convolution2DFlipout(\n      filters,\n      kernel,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(x)\n  out = tf.keras.layers.BatchNormalization()(out)\n  out = tf.keras.layers.Activation('relu')(out)\n\n  out = tfp.layers.Convolution2DFlipout(\n      filters,\n      kernel,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(out)\n  out = tf.keras.layers.BatchNormalization()(out)\n  out = tf.keras.layers.Activation('relu')(out)\n\n  out = tf.keras.layers.MaxPooling2D(\n      pool_size=(2, 2), strides=stride)(out)\n  return out", "code_tokens": ["def", "_vggconv_block", "(", "x", ",", "filters", ",", "kernel", ",", "stride", ",", "kernel_posterior_fn", ")", ":", "out", "=", "tfp", ".", "layers", ".", "Convolution2DFlipout", "(", "filters", ",", "kernel", ",", "padding", "=", "'same'", ",", "kernel_posterior_fn", "=", "kernel_posterior_fn", ")", "(", "x", ")", "out", "=", "tf", ".", "keras", ".", "layers", ".", "BatchNormalization", "(", ")", "(", "out", ")", "out", "=", "tf", ".", "keras", ".", "layers", ".", "Activation", "(", "'relu'", ")", "(", "out", ")", "out", "=", "tfp", ".", "layers", ".", "Convolution2DFlipout", "(", "filters", ",", "kernel", ",", "padding", "=", "'same'", ",", "kernel_posterior_fn", "=", "kernel_posterior_fn", ")", "(", "out", ")", "out", "=", "tf", ".", "keras", ".", "layers", ".", "BatchNormalization", "(", ")", "(", "out", ")", "out", "=", "tf", ".", "keras", ".", "layers", ".", "Activation", "(", "'relu'", ")", "(", "out", ")", "out", "=", "tf", ".", "keras", ".", "layers", ".", "MaxPooling2D", "(", "pool_size", "=", "(", "2", ",", "2", ")", ",", "strides", "=", "stride", ")", "(", "out", ")", "return", "out"], "docstring": "Network block for VGG.", "docstring_tokens": ["Network", "block", "for", "VGG", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/models/bayesian_vgg.py#L85-L105", "partition": "test", "index": 800, "time": "2018-09-06 10:42:58"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/models/bayesian_resnet.py", "func_name": "_resnet_block", "original_string": "def _resnet_block(x, filters, kernel, stride, kernel_posterior_fn):\n  \"\"\"Network block for ResNet.\"\"\"\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Activation('relu')(x)\n\n  if stride != 1 or filters != x.shape[1]:\n    shortcut = _projection_shortcut(x, filters, stride, kernel_posterior_fn)\n  else:\n    shortcut = x\n\n  x = tfp.layers.Convolution2DFlipout(\n      filters,\n      kernel,\n      strides=stride,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(x)\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Activation('relu')(x)\n\n  x = tfp.layers.Convolution2DFlipout(\n      filters,\n      kernel,\n      strides=1,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(x)\n  x = tf.keras.layers.add([x, shortcut])\n  return x", "language": "python", "code": "def _resnet_block(x, filters, kernel, stride, kernel_posterior_fn):\n  \"\"\"Network block for ResNet.\"\"\"\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Activation('relu')(x)\n\n  if stride != 1 or filters != x.shape[1]:\n    shortcut = _projection_shortcut(x, filters, stride, kernel_posterior_fn)\n  else:\n    shortcut = x\n\n  x = tfp.layers.Convolution2DFlipout(\n      filters,\n      kernel,\n      strides=stride,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(x)\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Activation('relu')(x)\n\n  x = tfp.layers.Convolution2DFlipout(\n      filters,\n      kernel,\n      strides=1,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(x)\n  x = tf.keras.layers.add([x, shortcut])\n  return x", "code_tokens": ["def", "_resnet_block", "(", "x", ",", "filters", ",", "kernel", ",", "stride", ",", "kernel_posterior_fn", ")", ":", "x", "=", "tf", ".", "keras", ".", "layers", ".", "BatchNormalization", "(", ")", "(", "x", ")", "x", "=", "tf", ".", "keras", ".", "layers", ".", "Activation", "(", "'relu'", ")", "(", "x", ")", "if", "stride", "!=", "1", "or", "filters", "!=", "x", ".", "shape", "[", "1", "]", ":", "shortcut", "=", "_projection_shortcut", "(", "x", ",", "filters", ",", "stride", ",", "kernel_posterior_fn", ")", "else", ":", "shortcut", "=", "x", "x", "=", "tfp", ".", "layers", ".", "Convolution2DFlipout", "(", "filters", ",", "kernel", ",", "strides", "=", "stride", ",", "padding", "=", "'same'", ",", "kernel_posterior_fn", "=", "kernel_posterior_fn", ")", "(", "x", ")", "x", "=", "tf", ".", "keras", ".", "layers", ".", "BatchNormalization", "(", ")", "(", "x", ")", "x", "=", "tf", ".", "keras", ".", "layers", ".", "Activation", "(", "'relu'", ")", "(", "x", ")", "x", "=", "tfp", ".", "layers", ".", "Convolution2DFlipout", "(", "filters", ",", "kernel", ",", "strides", "=", "1", ",", "padding", "=", "'same'", ",", "kernel_posterior_fn", "=", "kernel_posterior_fn", ")", "(", "x", ")", "x", "=", "tf", ".", "keras", ".", "layers", ".", "add", "(", "[", "x", ",", "shortcut", "]", ")", "return", "x"], "docstring": "Network block for ResNet.", "docstring_tokens": ["Network", "block", "for", "ResNet", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/models/bayesian_resnet.py#L95-L121", "partition": "test", "index": 1011, "time": "2018-09-06 10:42:58"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/cifar10_bnn.py", "func_name": "build_fake_data", "original_string": "def build_fake_data():\n  \"\"\"Build fake CIFAR10-style data for unit testing.\"\"\"\n  num_examples = 10\n  x_train = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n  y_train = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n  x_test = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n  y_test = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n  return (x_train, y_train), (x_test, y_test)", "language": "python", "code": "def build_fake_data():\n  \"\"\"Build fake CIFAR10-style data for unit testing.\"\"\"\n  num_examples = 10\n  x_train = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n  y_train = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n  x_test = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n  y_test = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n  return (x_train, y_train), (x_test, y_test)", "code_tokens": ["def", "build_fake_data", "(", ")", ":", "num_examples", "=", "10", "x_train", "=", "np", ".", "random", ".", "rand", "(", "num_examples", ",", "*", "IMAGE_SHAPE", ")", ".", "astype", "(", "np", ".", "float32", ")", "y_train", "=", "np", ".", "random", ".", "permutation", "(", "np", ".", "arange", "(", "num_examples", ")", ")", ".", "astype", "(", "np", ".", "int32", ")", "x_test", "=", "np", ".", "random", ".", "rand", "(", "num_examples", ",", "*", "IMAGE_SHAPE", ")", ".", "astype", "(", "np", ".", "float32", ")", "y_test", "=", "np", ".", "random", ".", "permutation", "(", "np", ".", "arange", "(", "num_examples", ")", ")", ".", "astype", "(", "np", ".", "int32", ")", "return", "(", "x_train", ",", "y_train", ")", ",", "(", "x_test", ",", "y_test", ")"], "docstring": "Build fake CIFAR10-style data for unit testing.", "docstring_tokens": ["Build", "fake", "CIFAR10", "-", "style", "data", "for", "unit", "testing", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/cifar10_bnn.py#L155-L162", "partition": "test", "index": 1031, "time": "2018-09-11 16:08:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/internal/util.py", "func_name": "factored_joint_mvn", "original_string": "def factored_joint_mvn(distributions):\n  \"\"\"Combine MultivariateNormals into a factored joint distribution.\n\n   Given a list of multivariate normal distributions\n   `dist[i] = Normal(loc[i], scale[i])`, construct the joint\n   distribution given by concatenating independent samples from these\n   distributions. This is multivariate normal with mean vector given by the\n   concatenation of the component mean vectors, and block-diagonal covariance\n   matrix in which the blocks are the component covariances.\n\n   Note that for computational efficiency, multivariate normals are represented\n   by a 'scale' (factored covariance) linear operator rather than the full\n   covariance matrix.\n\n  Args:\n    distributions: Python `iterable` of MultivariateNormal distribution\n      instances (e.g., `tfd.MultivariateNormalDiag`,\n      `tfd.MultivariateNormalTriL`, etc.). These must be broadcastable to a\n      consistent batch shape, but may have different event shapes\n      (i.e., defined over spaces of different dimension).\n\n  Returns:\n    joint_distribution: An instance of `tfd.MultivariateNormalLinearOperator`\n      representing the joint distribution constructed by concatenating\n      an independent sample from each input distributions.\n  \"\"\"\n\n  graph_parents = [tensor for distribution in distributions\n                   for tensor in distribution._graph_parents]  # pylint: disable=protected-access\n  with tf.compat.v1.name_scope('factored_joint_mvn', values=graph_parents):\n\n    # We explicitly broadcast the `locs` so that we can concatenate them.\n    # We don't have direct numerical access to the `scales`, which are arbitrary\n    # linear operators, but `LinearOperatorBlockDiag` appears to do the right\n    # thing without further intervention.\n    dtype = tf.debugging.assert_same_float_dtype(distributions)\n    broadcast_ones = tf.ones(broadcast_batch_shape(distributions),\n                             dtype=dtype)[..., tf.newaxis]\n    return MultivariateNormalLinearOperator(\n        loc=tf.concat([mvn.mean() * broadcast_ones for mvn in distributions],\n                      axis=-1),\n        scale=tfl.LinearOperatorBlockDiag([mvn.scale for mvn in distributions],\n                                          is_square=True))", "language": "python", "code": "def factored_joint_mvn(distributions):\n  \"\"\"Combine MultivariateNormals into a factored joint distribution.\n\n   Given a list of multivariate normal distributions\n   `dist[i] = Normal(loc[i], scale[i])`, construct the joint\n   distribution given by concatenating independent samples from these\n   distributions. This is multivariate normal with mean vector given by the\n   concatenation of the component mean vectors, and block-diagonal covariance\n   matrix in which the blocks are the component covariances.\n\n   Note that for computational efficiency, multivariate normals are represented\n   by a 'scale' (factored covariance) linear operator rather than the full\n   covariance matrix.\n\n  Args:\n    distributions: Python `iterable` of MultivariateNormal distribution\n      instances (e.g., `tfd.MultivariateNormalDiag`,\n      `tfd.MultivariateNormalTriL`, etc.). These must be broadcastable to a\n      consistent batch shape, but may have different event shapes\n      (i.e., defined over spaces of different dimension).\n\n  Returns:\n    joint_distribution: An instance of `tfd.MultivariateNormalLinearOperator`\n      representing the joint distribution constructed by concatenating\n      an independent sample from each input distributions.\n  \"\"\"\n\n  graph_parents = [tensor for distribution in distributions\n                   for tensor in distribution._graph_parents]  # pylint: disable=protected-access\n  with tf.compat.v1.name_scope('factored_joint_mvn', values=graph_parents):\n\n    # We explicitly broadcast the `locs` so that we can concatenate them.\n    # We don't have direct numerical access to the `scales`, which are arbitrary\n    # linear operators, but `LinearOperatorBlockDiag` appears to do the right\n    # thing without further intervention.\n    dtype = tf.debugging.assert_same_float_dtype(distributions)\n    broadcast_ones = tf.ones(broadcast_batch_shape(distributions),\n                             dtype=dtype)[..., tf.newaxis]\n    return MultivariateNormalLinearOperator(\n        loc=tf.concat([mvn.mean() * broadcast_ones for mvn in distributions],\n                      axis=-1),\n        scale=tfl.LinearOperatorBlockDiag([mvn.scale for mvn in distributions],\n                                          is_square=True))", "code_tokens": ["def", "factored_joint_mvn", "(", "distributions", ")", ":", "graph_parents", "=", "[", "tensor", "for", "distribution", "in", "distributions", "for", "tensor", "in", "distribution", ".", "_graph_parents", "]", "# pylint: disable=protected-access", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'factored_joint_mvn'", ",", "values", "=", "graph_parents", ")", ":", "# We explicitly broadcast the `locs` so that we can concatenate them.", "# We don't have direct numerical access to the `scales`, which are arbitrary", "# linear operators, but `LinearOperatorBlockDiag` appears to do the right", "# thing without further intervention.", "dtype", "=", "tf", ".", "debugging", ".", "assert_same_float_dtype", "(", "distributions", ")", "broadcast_ones", "=", "tf", ".", "ones", "(", "broadcast_batch_shape", "(", "distributions", ")", ",", "dtype", "=", "dtype", ")", "[", "...", ",", "tf", ".", "newaxis", "]", "return", "MultivariateNormalLinearOperator", "(", "loc", "=", "tf", ".", "concat", "(", "[", "mvn", ".", "mean", "(", ")", "*", "broadcast_ones", "for", "mvn", "in", "distributions", "]", ",", "axis", "=", "-", "1", ")", ",", "scale", "=", "tfl", ".", "LinearOperatorBlockDiag", "(", "[", "mvn", ".", "scale", "for", "mvn", "in", "distributions", "]", ",", "is_square", "=", "True", ")", ")"], "docstring": "Combine MultivariateNormals into a factored joint distribution.\n\n   Given a list of multivariate normal distributions\n   `dist[i] = Normal(loc[i], scale[i])`, construct the joint\n   distribution given by concatenating independent samples from these\n   distributions. This is multivariate normal with mean vector given by the\n   concatenation of the component mean vectors, and block-diagonal covariance\n   matrix in which the blocks are the component covariances.\n\n   Note that for computational efficiency, multivariate normals are represented\n   by a 'scale' (factored covariance) linear operator rather than the full\n   covariance matrix.\n\n  Args:\n    distributions: Python `iterable` of MultivariateNormal distribution\n      instances (e.g., `tfd.MultivariateNormalDiag`,\n      `tfd.MultivariateNormalTriL`, etc.). These must be broadcastable to a\n      consistent batch shape, but may have different event shapes\n      (i.e., defined over spaces of different dimension).\n\n  Returns:\n    joint_distribution: An instance of `tfd.MultivariateNormalLinearOperator`\n      representing the joint distribution constructed by concatenating\n      an independent sample from each input distributions.", "docstring_tokens": ["Combine", "MultivariateNormals", "into", "a", "factored", "joint", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/internal/util.py#L119-L161", "partition": "test", "index": 704, "time": "2018-09-14 13:15:56"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/internal/util.py", "func_name": "sum_mvns", "original_string": "def sum_mvns(distributions):\n  \"\"\"Attempt to sum MultivariateNormal distributions.\n\n  The sum of (multivariate) normal random variables is itself (multivariate)\n  normal, with mean given by the sum of means and (co)variance given by the\n  sum of (co)variances. This method exploits this fact to compute the\n  sum of a list of `tfd.MultivariateNormalDiag` objects.\n\n  It may in the future be extended to support summation of other forms of\n  (Multivariate)Normal distributions.\n\n  Args:\n    distributions: Python `iterable` of `tfd.MultivariateNormalDiag`\n      distribution instances. These must all have the same event\n      shape, and broadcast to a consistent batch shape.\n\n  Returns:\n    sum_distribution: A `tfd.MultivariateNormalDiag` instance with mean\n      equal to the sum of input means and covariance equal to the sum of\n      input covariances.\n  \"\"\"\n\n  graph_parents = [tensor for distribution in distributions\n                   for tensor in distribution._graph_parents]  # pylint: disable=protected-access\n  with tf.compat.v1.name_scope('sum_mvns', values=graph_parents):\n    if all([isinstance(mvn, tfd.MultivariateNormalDiag)\n            for mvn in distributions]):\n      return tfd.MultivariateNormalDiag(\n          loc=sum([mvn.mean() for mvn in distributions]),\n          scale_diag=tf.sqrt(sum([\n              mvn.scale.diag**2 for mvn in distributions])))\n    else:\n      raise NotImplementedError(\n          'Sums of distributions other than MultivariateNormalDiag are not '\n          'currently implemented. (given: {})'.format(distributions))", "language": "python", "code": "def sum_mvns(distributions):\n  \"\"\"Attempt to sum MultivariateNormal distributions.\n\n  The sum of (multivariate) normal random variables is itself (multivariate)\n  normal, with mean given by the sum of means and (co)variance given by the\n  sum of (co)variances. This method exploits this fact to compute the\n  sum of a list of `tfd.MultivariateNormalDiag` objects.\n\n  It may in the future be extended to support summation of other forms of\n  (Multivariate)Normal distributions.\n\n  Args:\n    distributions: Python `iterable` of `tfd.MultivariateNormalDiag`\n      distribution instances. These must all have the same event\n      shape, and broadcast to a consistent batch shape.\n\n  Returns:\n    sum_distribution: A `tfd.MultivariateNormalDiag` instance with mean\n      equal to the sum of input means and covariance equal to the sum of\n      input covariances.\n  \"\"\"\n\n  graph_parents = [tensor for distribution in distributions\n                   for tensor in distribution._graph_parents]  # pylint: disable=protected-access\n  with tf.compat.v1.name_scope('sum_mvns', values=graph_parents):\n    if all([isinstance(mvn, tfd.MultivariateNormalDiag)\n            for mvn in distributions]):\n      return tfd.MultivariateNormalDiag(\n          loc=sum([mvn.mean() for mvn in distributions]),\n          scale_diag=tf.sqrt(sum([\n              mvn.scale.diag**2 for mvn in distributions])))\n    else:\n      raise NotImplementedError(\n          'Sums of distributions other than MultivariateNormalDiag are not '\n          'currently implemented. (given: {})'.format(distributions))", "code_tokens": ["def", "sum_mvns", "(", "distributions", ")", ":", "graph_parents", "=", "[", "tensor", "for", "distribution", "in", "distributions", "for", "tensor", "in", "distribution", ".", "_graph_parents", "]", "# pylint: disable=protected-access", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'sum_mvns'", ",", "values", "=", "graph_parents", ")", ":", "if", "all", "(", "[", "isinstance", "(", "mvn", ",", "tfd", ".", "MultivariateNormalDiag", ")", "for", "mvn", "in", "distributions", "]", ")", ":", "return", "tfd", ".", "MultivariateNormalDiag", "(", "loc", "=", "sum", "(", "[", "mvn", ".", "mean", "(", ")", "for", "mvn", "in", "distributions", "]", ")", ",", "scale_diag", "=", "tf", ".", "sqrt", "(", "sum", "(", "[", "mvn", ".", "scale", ".", "diag", "**", "2", "for", "mvn", "in", "distributions", "]", ")", ")", ")", "else", ":", "raise", "NotImplementedError", "(", "'Sums of distributions other than MultivariateNormalDiag are not '", "'currently implemented. (given: {})'", ".", "format", "(", "distributions", ")", ")"], "docstring": "Attempt to sum MultivariateNormal distributions.\n\n  The sum of (multivariate) normal random variables is itself (multivariate)\n  normal, with mean given by the sum of means and (co)variance given by the\n  sum of (co)variances. This method exploits this fact to compute the\n  sum of a list of `tfd.MultivariateNormalDiag` objects.\n\n  It may in the future be extended to support summation of other forms of\n  (Multivariate)Normal distributions.\n\n  Args:\n    distributions: Python `iterable` of `tfd.MultivariateNormalDiag`\n      distribution instances. These must all have the same event\n      shape, and broadcast to a consistent batch shape.\n\n  Returns:\n    sum_distribution: A `tfd.MultivariateNormalDiag` instance with mean\n      equal to the sum of input means and covariance equal to the sum of\n      input covariances.", "docstring_tokens": ["Attempt", "to", "sum", "MultivariateNormal", "distributions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/internal/util.py#L164-L198", "partition": "test", "index": 705, "time": "2018-09-14 13:15:56"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/internal/util.py", "func_name": "broadcast_batch_shape", "original_string": "def broadcast_batch_shape(distributions):\n  \"\"\"Get broadcast batch shape from distributions, statically if possible.\"\"\"\n\n  # Static case\n  batch_shape = distributions[0].batch_shape\n  for distribution in distributions:\n    batch_shape = tf.broadcast_static_shape(batch_shape,\n                                            distribution.batch_shape)\n  if batch_shape.is_fully_defined():\n    return batch_shape.as_list()\n\n  # Fallback on dynamic.\n  batch_shape = distributions[0].batch_shape_tensor()\n  for distribution in distributions:\n    batch_shape = tf.broadcast_dynamic_shape(batch_shape,\n                                             distribution.batch_shape_tensor())\n\n  return tf.convert_to_tensor(value=batch_shape)", "language": "python", "code": "def broadcast_batch_shape(distributions):\n  \"\"\"Get broadcast batch shape from distributions, statically if possible.\"\"\"\n\n  # Static case\n  batch_shape = distributions[0].batch_shape\n  for distribution in distributions:\n    batch_shape = tf.broadcast_static_shape(batch_shape,\n                                            distribution.batch_shape)\n  if batch_shape.is_fully_defined():\n    return batch_shape.as_list()\n\n  # Fallback on dynamic.\n  batch_shape = distributions[0].batch_shape_tensor()\n  for distribution in distributions:\n    batch_shape = tf.broadcast_dynamic_shape(batch_shape,\n                                             distribution.batch_shape_tensor())\n\n  return tf.convert_to_tensor(value=batch_shape)", "code_tokens": ["def", "broadcast_batch_shape", "(", "distributions", ")", ":", "# Static case", "batch_shape", "=", "distributions", "[", "0", "]", ".", "batch_shape", "for", "distribution", "in", "distributions", ":", "batch_shape", "=", "tf", ".", "broadcast_static_shape", "(", "batch_shape", ",", "distribution", ".", "batch_shape", ")", "if", "batch_shape", ".", "is_fully_defined", "(", ")", ":", "return", "batch_shape", ".", "as_list", "(", ")", "# Fallback on dynamic.", "batch_shape", "=", "distributions", "[", "0", "]", ".", "batch_shape_tensor", "(", ")", "for", "distribution", "in", "distributions", ":", "batch_shape", "=", "tf", ".", "broadcast_dynamic_shape", "(", "batch_shape", ",", "distribution", ".", "batch_shape_tensor", "(", ")", ")", "return", "tf", ".", "convert_to_tensor", "(", "value", "=", "batch_shape", ")"], "docstring": "Get broadcast batch shape from distributions, statically if possible.", "docstring_tokens": ["Get", "broadcast", "batch", "shape", "from", "distributions", "statically", "if", "possible", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/internal/util.py#L32-L49", "partition": "test", "index": 703, "time": "2018-09-14 13:15:56"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/sprites_dataset.py", "func_name": "create_sprites_dataset", "original_string": "def create_sprites_dataset(characters, actions, directions, channels=3,\n                           length=8, shuffle=False, fake_data=False):\n  \"\"\"Creates a tf.data pipeline for the sprites dataset.\n\n  Args:\n    characters: A list of (skin, hair, top, pants) tuples containing\n      relative paths to the sprite png image for each attribute.\n    actions: A list of Actions.\n    directions: A list of Directions.\n    channels: Number of image channels to yield.\n    length: Desired length of the sequences.\n    shuffle: Whether or not to shuffle the characters and sequences\n      start frame.\n    fake_data: Boolean for whether or not to yield synthetic data.\n\n  Returns:\n    A tf.data.Dataset yielding (seq, skin label index, hair label index,\n    top label index, pants label index, action label index, skin label\n    name, hair label_name, top label name, pants label name, action\n    label name) tuples.\n  \"\"\"\n  if fake_data:\n    dummy_image = tf.random.normal([HEIGHT, WIDTH, CHANNELS])\n  else:\n    basedir = download_sprites()\n\n  action_names = [action.name for action in actions]\n  action_metadata = [(action.start_row, action.frames) for action in actions]\n\n  direction_rows = [direction.row_offset for direction in directions]\n\n  chars = tf.data.Dataset.from_tensor_slices(characters)\n  act_names = tf.data.Dataset.from_tensor_slices(action_names).repeat()\n  acts_metadata = tf.data.Dataset.from_tensor_slices(action_metadata).repeat()\n  dir_rows = tf.data.Dataset.from_tensor_slices(direction_rows).repeat()\n\n  if shuffle:\n    chars = chars.shuffle(len(characters))\n\n  dataset = tf.data.Dataset.zip((chars, act_names, acts_metadata, dir_rows))\n\n  skin_table = tf.contrib.lookup.index_table_from_tensor(sorted(SKIN_COLORS))\n  hair_table = tf.contrib.lookup.index_table_from_tensor(sorted(HAIRSTYLES))\n  top_table = tf.contrib.lookup.index_table_from_tensor(sorted(TOPS))\n  pants_table = tf.contrib.lookup.index_table_from_tensor(sorted(PANTS))\n  action_table = tf.contrib.lookup.index_table_from_tensor(sorted(action_names))\n\n  def process_example(attrs, act_name, act_metadata, dir_row_offset):\n    \"\"\"Processes a dataset row.\"\"\"\n    skin_name = attrs[0]\n    hair_name = attrs[1]\n    top_name = attrs[2]\n    pants_name = attrs[3]\n\n    if fake_data:\n      char = dummy_image\n    else:\n      skin = read_image(basedir + os.sep + skin_name)\n      hair = read_image(basedir + os.sep + hair_name)\n      top = read_image(basedir + os.sep + top_name)\n      pants = read_image(basedir + os.sep + pants_name)\n      char = create_character(skin, hair, top, pants)\n\n    if shuffle:\n      seq = create_random_seq(char, act_metadata, dir_row_offset, length)\n    else:\n      seq = create_seq(char, act_metadata, dir_row_offset, length)\n    seq = seq[..., :channels]  # limit output channels\n\n    skin_idx = skin_table.lookup(skin_name)\n    hair_idx = hair_table.lookup(hair_name)\n    top_idx = top_table.lookup(top_name)\n    pants_idx = pants_table.lookup(pants_name)\n    act_idx = action_table.lookup(act_name)\n\n    return (seq, skin_idx, hair_idx, top_idx, pants_idx, act_idx,\n            skin_name, hair_name, top_name, pants_name, act_name)\n\n  dataset = dataset.map(process_example)\n  return dataset", "language": "python", "code": "def create_sprites_dataset(characters, actions, directions, channels=3,\n                           length=8, shuffle=False, fake_data=False):\n  \"\"\"Creates a tf.data pipeline for the sprites dataset.\n\n  Args:\n    characters: A list of (skin, hair, top, pants) tuples containing\n      relative paths to the sprite png image for each attribute.\n    actions: A list of Actions.\n    directions: A list of Directions.\n    channels: Number of image channels to yield.\n    length: Desired length of the sequences.\n    shuffle: Whether or not to shuffle the characters and sequences\n      start frame.\n    fake_data: Boolean for whether or not to yield synthetic data.\n\n  Returns:\n    A tf.data.Dataset yielding (seq, skin label index, hair label index,\n    top label index, pants label index, action label index, skin label\n    name, hair label_name, top label name, pants label name, action\n    label name) tuples.\n  \"\"\"\n  if fake_data:\n    dummy_image = tf.random.normal([HEIGHT, WIDTH, CHANNELS])\n  else:\n    basedir = download_sprites()\n\n  action_names = [action.name for action in actions]\n  action_metadata = [(action.start_row, action.frames) for action in actions]\n\n  direction_rows = [direction.row_offset for direction in directions]\n\n  chars = tf.data.Dataset.from_tensor_slices(characters)\n  act_names = tf.data.Dataset.from_tensor_slices(action_names).repeat()\n  acts_metadata = tf.data.Dataset.from_tensor_slices(action_metadata).repeat()\n  dir_rows = tf.data.Dataset.from_tensor_slices(direction_rows).repeat()\n\n  if shuffle:\n    chars = chars.shuffle(len(characters))\n\n  dataset = tf.data.Dataset.zip((chars, act_names, acts_metadata, dir_rows))\n\n  skin_table = tf.contrib.lookup.index_table_from_tensor(sorted(SKIN_COLORS))\n  hair_table = tf.contrib.lookup.index_table_from_tensor(sorted(HAIRSTYLES))\n  top_table = tf.contrib.lookup.index_table_from_tensor(sorted(TOPS))\n  pants_table = tf.contrib.lookup.index_table_from_tensor(sorted(PANTS))\n  action_table = tf.contrib.lookup.index_table_from_tensor(sorted(action_names))\n\n  def process_example(attrs, act_name, act_metadata, dir_row_offset):\n    \"\"\"Processes a dataset row.\"\"\"\n    skin_name = attrs[0]\n    hair_name = attrs[1]\n    top_name = attrs[2]\n    pants_name = attrs[3]\n\n    if fake_data:\n      char = dummy_image\n    else:\n      skin = read_image(basedir + os.sep + skin_name)\n      hair = read_image(basedir + os.sep + hair_name)\n      top = read_image(basedir + os.sep + top_name)\n      pants = read_image(basedir + os.sep + pants_name)\n      char = create_character(skin, hair, top, pants)\n\n    if shuffle:\n      seq = create_random_seq(char, act_metadata, dir_row_offset, length)\n    else:\n      seq = create_seq(char, act_metadata, dir_row_offset, length)\n    seq = seq[..., :channels]  # limit output channels\n\n    skin_idx = skin_table.lookup(skin_name)\n    hair_idx = hair_table.lookup(hair_name)\n    top_idx = top_table.lookup(top_name)\n    pants_idx = pants_table.lookup(pants_name)\n    act_idx = action_table.lookup(act_name)\n\n    return (seq, skin_idx, hair_idx, top_idx, pants_idx, act_idx,\n            skin_name, hair_name, top_name, pants_name, act_name)\n\n  dataset = dataset.map(process_example)\n  return dataset", "code_tokens": ["def", "create_sprites_dataset", "(", "characters", ",", "actions", ",", "directions", ",", "channels", "=", "3", ",", "length", "=", "8", ",", "shuffle", "=", "False", ",", "fake_data", "=", "False", ")", ":", "if", "fake_data", ":", "dummy_image", "=", "tf", ".", "random", ".", "normal", "(", "[", "HEIGHT", ",", "WIDTH", ",", "CHANNELS", "]", ")", "else", ":", "basedir", "=", "download_sprites", "(", ")", "action_names", "=", "[", "action", ".", "name", "for", "action", "in", "actions", "]", "action_metadata", "=", "[", "(", "action", ".", "start_row", ",", "action", ".", "frames", ")", "for", "action", "in", "actions", "]", "direction_rows", "=", "[", "direction", ".", "row_offset", "for", "direction", "in", "directions", "]", "chars", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "characters", ")", "act_names", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "action_names", ")", ".", "repeat", "(", ")", "acts_metadata", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "action_metadata", ")", ".", "repeat", "(", ")", "dir_rows", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "direction_rows", ")", ".", "repeat", "(", ")", "if", "shuffle", ":", "chars", "=", "chars", ".", "shuffle", "(", "len", "(", "characters", ")", ")", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "zip", "(", "(", "chars", ",", "act_names", ",", "acts_metadata", ",", "dir_rows", ")", ")", "skin_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "sorted", "(", "SKIN_COLORS", ")", ")", "hair_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "sorted", "(", "HAIRSTYLES", ")", ")", "top_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "sorted", "(", "TOPS", ")", ")", "pants_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "sorted", "(", "PANTS", ")", ")", "action_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "sorted", "(", "action_names", ")", ")", "def", "process_example", "(", "attrs", ",", "act_name", ",", "act_metadata", ",", "dir_row_offset", ")", ":", "\"\"\"Processes a dataset row.\"\"\"", "skin_name", "=", "attrs", "[", "0", "]", "hair_name", "=", "attrs", "[", "1", "]", "top_name", "=", "attrs", "[", "2", "]", "pants_name", "=", "attrs", "[", "3", "]", "if", "fake_data", ":", "char", "=", "dummy_image", "else", ":", "skin", "=", "read_image", "(", "basedir", "+", "os", ".", "sep", "+", "skin_name", ")", "hair", "=", "read_image", "(", "basedir", "+", "os", ".", "sep", "+", "hair_name", ")", "top", "=", "read_image", "(", "basedir", "+", "os", ".", "sep", "+", "top_name", ")", "pants", "=", "read_image", "(", "basedir", "+", "os", ".", "sep", "+", "pants_name", ")", "char", "=", "create_character", "(", "skin", ",", "hair", ",", "top", ",", "pants", ")", "if", "shuffle", ":", "seq", "=", "create_random_seq", "(", "char", ",", "act_metadata", ",", "dir_row_offset", ",", "length", ")", "else", ":", "seq", "=", "create_seq", "(", "char", ",", "act_metadata", ",", "dir_row_offset", ",", "length", ")", "seq", "=", "seq", "[", "...", ",", ":", "channels", "]", "# limit output channels", "skin_idx", "=", "skin_table", ".", "lookup", "(", "skin_name", ")", "hair_idx", "=", "hair_table", ".", "lookup", "(", "hair_name", ")", "top_idx", "=", "top_table", ".", "lookup", "(", "top_name", ")", "pants_idx", "=", "pants_table", ".", "lookup", "(", "pants_name", ")", "act_idx", "=", "action_table", ".", "lookup", "(", "act_name", ")", "return", "(", "seq", ",", "skin_idx", ",", "hair_idx", ",", "top_idx", ",", "pants_idx", ",", "act_idx", ",", "skin_name", ",", "hair_name", ",", "top_name", ",", "pants_name", ",", "act_name", ")", "dataset", "=", "dataset", ".", "map", "(", "process_example", ")", "return", "dataset"], "docstring": "Creates a tf.data pipeline for the sprites dataset.\n\n  Args:\n    characters: A list of (skin, hair, top, pants) tuples containing\n      relative paths to the sprite png image for each attribute.\n    actions: A list of Actions.\n    directions: A list of Directions.\n    channels: Number of image channels to yield.\n    length: Desired length of the sequences.\n    shuffle: Whether or not to shuffle the characters and sequences\n      start frame.\n    fake_data: Boolean for whether or not to yield synthetic data.\n\n  Returns:\n    A tf.data.Dataset yielding (seq, skin label index, hair label index,\n    top label index, pants label index, action label index, skin label\n    name, hair label_name, top label name, pants label name, action\n    label name) tuples.", "docstring_tokens": ["Creates", "a", "tf", ".", "data", "pipeline", "for", "the", "sprites", "dataset", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/sprites_dataset.py#L194-L273", "partition": "test", "index": 1027, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/sprites_dataset.py", "func_name": "create_seq", "original_string": "def create_seq(character, action_metadata, direction, length=8, start=0):\n  \"\"\"Creates a sequence.\n\n  Args:\n    character: A character sprite tensor.\n    action_metadata: An action metadata tuple.\n    direction: An integer representing the direction, i.e., the row\n      offset within each action group corresponding to a particular\n      direction.\n    length: Desired length of the sequence. If this is longer than\n      the number of available frames, it will roll over to the\n      beginning.\n    start: Index of possible frames at which to start the sequence.\n\n  Returns:\n    A sequence tensor.\n  \"\"\"\n  sprite_start = (action_metadata[0]+direction) * FRAME_SIZE\n  sprite_end = (action_metadata[0]+direction+1) * FRAME_SIZE\n  sprite_line = character[sprite_start:sprite_end, ...]\n\n  # Extract 64x64 patches that are side-by-side in the sprite, and limit\n  # to the actual number of frames for the given action.\n  frames = tf.stack(tf.split(sprite_line, 13, axis=1))  # 13 is a hack\n  frames = frames[0:action_metadata[1]]\n\n  # Extract a slice of the desired length.\n  # NOTE: Length could be longer than the number of frames, so tile as needed.\n  frames = tf.roll(frames, shift=-start, axis=0)\n  frames = tf.tile(frames, [2, 1, 1, 1])  # 2 is a hack\n  frames = frames[:length]\n  frames = tf.cast(frames, dtype=tf.float32)\n  frames.set_shape([length, FRAME_SIZE, FRAME_SIZE, CHANNELS])\n  return frames", "language": "python", "code": "def create_seq(character, action_metadata, direction, length=8, start=0):\n  \"\"\"Creates a sequence.\n\n  Args:\n    character: A character sprite tensor.\n    action_metadata: An action metadata tuple.\n    direction: An integer representing the direction, i.e., the row\n      offset within each action group corresponding to a particular\n      direction.\n    length: Desired length of the sequence. If this is longer than\n      the number of available frames, it will roll over to the\n      beginning.\n    start: Index of possible frames at which to start the sequence.\n\n  Returns:\n    A sequence tensor.\n  \"\"\"\n  sprite_start = (action_metadata[0]+direction) * FRAME_SIZE\n  sprite_end = (action_metadata[0]+direction+1) * FRAME_SIZE\n  sprite_line = character[sprite_start:sprite_end, ...]\n\n  # Extract 64x64 patches that are side-by-side in the sprite, and limit\n  # to the actual number of frames for the given action.\n  frames = tf.stack(tf.split(sprite_line, 13, axis=1))  # 13 is a hack\n  frames = frames[0:action_metadata[1]]\n\n  # Extract a slice of the desired length.\n  # NOTE: Length could be longer than the number of frames, so tile as needed.\n  frames = tf.roll(frames, shift=-start, axis=0)\n  frames = tf.tile(frames, [2, 1, 1, 1])  # 2 is a hack\n  frames = frames[:length]\n  frames = tf.cast(frames, dtype=tf.float32)\n  frames.set_shape([length, FRAME_SIZE, FRAME_SIZE, CHANNELS])\n  return frames", "code_tokens": ["def", "create_seq", "(", "character", ",", "action_metadata", ",", "direction", ",", "length", "=", "8", ",", "start", "=", "0", ")", ":", "sprite_start", "=", "(", "action_metadata", "[", "0", "]", "+", "direction", ")", "*", "FRAME_SIZE", "sprite_end", "=", "(", "action_metadata", "[", "0", "]", "+", "direction", "+", "1", ")", "*", "FRAME_SIZE", "sprite_line", "=", "character", "[", "sprite_start", ":", "sprite_end", ",", "...", "]", "# Extract 64x64 patches that are side-by-side in the sprite, and limit", "# to the actual number of frames for the given action.", "frames", "=", "tf", ".", "stack", "(", "tf", ".", "split", "(", "sprite_line", ",", "13", ",", "axis", "=", "1", ")", ")", "# 13 is a hack", "frames", "=", "frames", "[", "0", ":", "action_metadata", "[", "1", "]", "]", "# Extract a slice of the desired length.", "# NOTE: Length could be longer than the number of frames, so tile as needed.", "frames", "=", "tf", ".", "roll", "(", "frames", ",", "shift", "=", "-", "start", ",", "axis", "=", "0", ")", "frames", "=", "tf", ".", "tile", "(", "frames", ",", "[", "2", ",", "1", ",", "1", ",", "1", "]", ")", "# 2 is a hack", "frames", "=", "frames", "[", ":", "length", "]", "frames", "=", "tf", ".", "cast", "(", "frames", ",", "dtype", "=", "tf", ".", "float32", ")", "frames", ".", "set_shape", "(", "[", "length", ",", "FRAME_SIZE", ",", "FRAME_SIZE", ",", "CHANNELS", "]", ")", "return", "frames"], "docstring": "Creates a sequence.\n\n  Args:\n    character: A character sprite tensor.\n    action_metadata: An action metadata tuple.\n    direction: An integer representing the direction, i.e., the row\n      offset within each action group corresponding to a particular\n      direction.\n    length: Desired length of the sequence. If this is longer than\n      the number of available frames, it will roll over to the\n      beginning.\n    start: Index of possible frames at which to start the sequence.\n\n  Returns:\n    A sequence tensor.", "docstring_tokens": ["Creates", "a", "sequence", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/sprites_dataset.py#L152-L185", "partition": "test", "index": 1025, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/sprites_dataset.py", "func_name": "create_character", "original_string": "def create_character(skin, hair, top, pants):\n  \"\"\"Creates a character sprite from a set of attribute sprites.\"\"\"\n  dtype = skin.dtype\n  hair_mask = tf.cast(hair[..., -1:] <= 0, dtype)\n  top_mask = tf.cast(top[..., -1:] <= 0, dtype)\n  pants_mask = tf.cast(pants[..., -1:] <= 0, dtype)\n  char = (skin * hair_mask) + hair\n  char = (char * top_mask) + top\n  char = (char * pants_mask) + pants\n  return char", "language": "python", "code": "def create_character(skin, hair, top, pants):\n  \"\"\"Creates a character sprite from a set of attribute sprites.\"\"\"\n  dtype = skin.dtype\n  hair_mask = tf.cast(hair[..., -1:] <= 0, dtype)\n  top_mask = tf.cast(top[..., -1:] <= 0, dtype)\n  pants_mask = tf.cast(pants[..., -1:] <= 0, dtype)\n  char = (skin * hair_mask) + hair\n  char = (char * top_mask) + top\n  char = (char * pants_mask) + pants\n  return char", "code_tokens": ["def", "create_character", "(", "skin", ",", "hair", ",", "top", ",", "pants", ")", ":", "dtype", "=", "skin", ".", "dtype", "hair_mask", "=", "tf", ".", "cast", "(", "hair", "[", "...", ",", "-", "1", ":", "]", "<=", "0", ",", "dtype", ")", "top_mask", "=", "tf", ".", "cast", "(", "top", "[", "...", ",", "-", "1", ":", "]", "<=", "0", ",", "dtype", ")", "pants_mask", "=", "tf", ".", "cast", "(", "pants", "[", "...", ",", "-", "1", ":", "]", "<=", "0", ",", "dtype", ")", "char", "=", "(", "skin", "*", "hair_mask", ")", "+", "hair", "char", "=", "(", "char", "*", "top_mask", ")", "+", "top", "char", "=", "(", "char", "*", "pants_mask", ")", "+", "pants", "return", "char"], "docstring": "Creates a character sprite from a set of attribute sprites.", "docstring_tokens": ["Creates", "a", "character", "sprite", "from", "a", "set", "of", "attribute", "sprites", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/sprites_dataset.py#L140-L149", "partition": "test", "index": 1024, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/sprites_dataset.py", "func_name": "download_sprites", "original_string": "def download_sprites():\n  \"\"\"Downloads the sprites data and returns the saved filepath.\"\"\"\n  filepath = os.path.join(FLAGS.data_dir, DATA_SPRITES_DIR)\n  if not tf.io.gfile.exists(filepath):\n    if not tf.io.gfile.exists(FLAGS.data_dir):\n      tf.io.gfile.makedirs(FLAGS.data_dir)\n    zip_name = \"{}.zip\".format(filepath)\n    urllib.request.urlretrieve(DATA_SPRITES_URL, zip_name)\n    with zipfile.ZipFile(zip_name, \"r\") as zip_file:\n      zip_file.extractall(FLAGS.data_dir)\n    tf.io.gfile.remove(zip_name)\n  return filepath", "language": "python", "code": "def download_sprites():\n  \"\"\"Downloads the sprites data and returns the saved filepath.\"\"\"\n  filepath = os.path.join(FLAGS.data_dir, DATA_SPRITES_DIR)\n  if not tf.io.gfile.exists(filepath):\n    if not tf.io.gfile.exists(FLAGS.data_dir):\n      tf.io.gfile.makedirs(FLAGS.data_dir)\n    zip_name = \"{}.zip\".format(filepath)\n    urllib.request.urlretrieve(DATA_SPRITES_URL, zip_name)\n    with zipfile.ZipFile(zip_name, \"r\") as zip_file:\n      zip_file.extractall(FLAGS.data_dir)\n    tf.io.gfile.remove(zip_name)\n  return filepath", "code_tokens": ["def", "download_sprites", "(", ")", ":", "filepath", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "data_dir", ",", "DATA_SPRITES_DIR", ")", "if", "not", "tf", ".", "io", ".", "gfile", ".", "exists", "(", "filepath", ")", ":", "if", "not", "tf", ".", "io", ".", "gfile", ".", "exists", "(", "FLAGS", ".", "data_dir", ")", ":", "tf", ".", "io", ".", "gfile", ".", "makedirs", "(", "FLAGS", ".", "data_dir", ")", "zip_name", "=", "\"{}.zip\"", ".", "format", "(", "filepath", ")", "urllib", ".", "request", ".", "urlretrieve", "(", "DATA_SPRITES_URL", ",", "zip_name", ")", "with", "zipfile", ".", "ZipFile", "(", "zip_name", ",", "\"r\"", ")", "as", "zip_file", ":", "zip_file", ".", "extractall", "(", "FLAGS", ".", "data_dir", ")", "tf", ".", "io", ".", "gfile", ".", "remove", "(", "zip_name", ")", "return", "filepath"], "docstring": "Downloads the sprites data and returns the saved filepath.", "docstring_tokens": ["Downloads", "the", "sprites", "data", "and", "returns", "the", "saved", "filepath", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/sprites_dataset.py#L126-L137", "partition": "test", "index": 1023, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/sprites_dataset.py", "func_name": "read_image", "original_string": "def read_image(filepath):\n  \"\"\"Returns an image tensor.\"\"\"\n  im_bytes = tf.io.read_file(filepath)\n  im = tf.image.decode_image(im_bytes, channels=CHANNELS)\n  im = tf.image.convert_image_dtype(im, tf.float32)\n  return im", "language": "python", "code": "def read_image(filepath):\n  \"\"\"Returns an image tensor.\"\"\"\n  im_bytes = tf.io.read_file(filepath)\n  im = tf.image.decode_image(im_bytes, channels=CHANNELS)\n  im = tf.image.convert_image_dtype(im, tf.float32)\n  return im", "code_tokens": ["def", "read_image", "(", "filepath", ")", ":", "im_bytes", "=", "tf", ".", "io", ".", "read_file", "(", "filepath", ")", "im", "=", "tf", ".", "image", ".", "decode_image", "(", "im_bytes", ",", "channels", "=", "CHANNELS", ")", "im", "=", "tf", ".", "image", ".", "convert_image_dtype", "(", "im", ",", "tf", ".", "float32", ")", "return", "im"], "docstring": "Returns an image tensor.", "docstring_tokens": ["Returns", "an", "image", "tensor", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/sprites_dataset.py#L113-L118", "partition": "test", "index": 1022, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/sprites_dataset.py", "func_name": "create_random_seq", "original_string": "def create_random_seq(character, action_metadata, direction, length=8):\n  \"\"\"Creates a random sequence.\"\"\"\n  start = tf.random.uniform([], maxval=action_metadata[1], dtype=tf.int32)\n  return create_seq(character, action_metadata, direction, length, start)", "language": "python", "code": "def create_random_seq(character, action_metadata, direction, length=8):\n  \"\"\"Creates a random sequence.\"\"\"\n  start = tf.random.uniform([], maxval=action_metadata[1], dtype=tf.int32)\n  return create_seq(character, action_metadata, direction, length, start)", "code_tokens": ["def", "create_random_seq", "(", "character", ",", "action_metadata", ",", "direction", ",", "length", "=", "8", ")", ":", "start", "=", "tf", ".", "random", ".", "uniform", "(", "[", "]", ",", "maxval", "=", "action_metadata", "[", "1", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "return", "create_seq", "(", "character", ",", "action_metadata", ",", "direction", ",", "length", ",", "start", ")"], "docstring": "Creates a random sequence.", "docstring_tokens": ["Creates", "a", "random", "sequence", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/sprites_dataset.py#L188-L191", "partition": "test", "index": 1026, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "visualize_qualitative_analysis", "original_string": "def visualize_qualitative_analysis(inputs, model, samples=1, batch_size=3,\n                                   length=8):\n  \"\"\"Visualizes a qualitative analysis of a given model.\n\n  Args:\n    inputs: A tensor of the original inputs, of shape [batch, timesteps,\n      h, w, c].\n    model: A DisentangledSequentialVAE model.\n    samples: Number of samples to draw from the latent distributions.\n    batch_size: Number of sequences to generate.\n    length: Number of timesteps to generate for each sequence.\n  \"\"\"\n  average = lambda dist: tf.reduce_mean(\n      input_tensor=dist.mean(), axis=0)  # avg over samples\n  with tf.compat.v1.name_scope(\"val_reconstruction\"):\n    reconstruct = functools.partial(model.reconstruct, inputs=inputs,\n                                    samples=samples)\n    visualize_reconstruction(inputs, average(reconstruct()))\n    visualize_reconstruction(inputs, average(reconstruct(sample_static=True)),\n                             name=\"static_prior\")\n    visualize_reconstruction(inputs, average(reconstruct(sample_dynamic=True)),\n                             name=\"dynamic_prior\")\n    visualize_reconstruction(inputs, average(reconstruct(swap_static=True)),\n                             name=\"swap_static\")\n    visualize_reconstruction(inputs, average(reconstruct(swap_dynamic=True)),\n                             name=\"swap_dynamic\")\n\n  with tf.compat.v1.name_scope(\"generation\"):\n    generate = functools.partial(model.generate, batch_size=batch_size,\n                                 length=length, samples=samples)\n    image_summary(average(generate(fix_static=True)), \"fix_static\")\n    image_summary(average(generate(fix_dynamic=True)), \"fix_dynamic\")", "language": "python", "code": "def visualize_qualitative_analysis(inputs, model, samples=1, batch_size=3,\n                                   length=8):\n  \"\"\"Visualizes a qualitative analysis of a given model.\n\n  Args:\n    inputs: A tensor of the original inputs, of shape [batch, timesteps,\n      h, w, c].\n    model: A DisentangledSequentialVAE model.\n    samples: Number of samples to draw from the latent distributions.\n    batch_size: Number of sequences to generate.\n    length: Number of timesteps to generate for each sequence.\n  \"\"\"\n  average = lambda dist: tf.reduce_mean(\n      input_tensor=dist.mean(), axis=0)  # avg over samples\n  with tf.compat.v1.name_scope(\"val_reconstruction\"):\n    reconstruct = functools.partial(model.reconstruct, inputs=inputs,\n                                    samples=samples)\n    visualize_reconstruction(inputs, average(reconstruct()))\n    visualize_reconstruction(inputs, average(reconstruct(sample_static=True)),\n                             name=\"static_prior\")\n    visualize_reconstruction(inputs, average(reconstruct(sample_dynamic=True)),\n                             name=\"dynamic_prior\")\n    visualize_reconstruction(inputs, average(reconstruct(swap_static=True)),\n                             name=\"swap_static\")\n    visualize_reconstruction(inputs, average(reconstruct(swap_dynamic=True)),\n                             name=\"swap_dynamic\")\n\n  with tf.compat.v1.name_scope(\"generation\"):\n    generate = functools.partial(model.generate, batch_size=batch_size,\n                                 length=length, samples=samples)\n    image_summary(average(generate(fix_static=True)), \"fix_static\")\n    image_summary(average(generate(fix_dynamic=True)), \"fix_dynamic\")", "code_tokens": ["def", "visualize_qualitative_analysis", "(", "inputs", ",", "model", ",", "samples", "=", "1", ",", "batch_size", "=", "3", ",", "length", "=", "8", ")", ":", "average", "=", "lambda", "dist", ":", "tf", ".", "reduce_mean", "(", "input_tensor", "=", "dist", ".", "mean", "(", ")", ",", "axis", "=", "0", ")", "# avg over samples", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "\"val_reconstruction\"", ")", ":", "reconstruct", "=", "functools", ".", "partial", "(", "model", ".", "reconstruct", ",", "inputs", "=", "inputs", ",", "samples", "=", "samples", ")", "visualize_reconstruction", "(", "inputs", ",", "average", "(", "reconstruct", "(", ")", ")", ")", "visualize_reconstruction", "(", "inputs", ",", "average", "(", "reconstruct", "(", "sample_static", "=", "True", ")", ")", ",", "name", "=", "\"static_prior\"", ")", "visualize_reconstruction", "(", "inputs", ",", "average", "(", "reconstruct", "(", "sample_dynamic", "=", "True", ")", ")", ",", "name", "=", "\"dynamic_prior\"", ")", "visualize_reconstruction", "(", "inputs", ",", "average", "(", "reconstruct", "(", "swap_static", "=", "True", ")", ")", ",", "name", "=", "\"swap_static\"", ")", "visualize_reconstruction", "(", "inputs", ",", "average", "(", "reconstruct", "(", "swap_dynamic", "=", "True", ")", ")", ",", "name", "=", "\"swap_dynamic\"", ")", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "\"generation\"", ")", ":", "generate", "=", "functools", ".", "partial", "(", "model", ".", "generate", ",", "batch_size", "=", "batch_size", ",", "length", "=", "length", ",", "samples", "=", "samples", ")", "image_summary", "(", "average", "(", "generate", "(", "fix_static", "=", "True", ")", ")", ",", "\"fix_static\"", ")", "image_summary", "(", "average", "(", "generate", "(", "fix_dynamic", "=", "True", ")", ")", ",", "\"fix_dynamic\"", ")"], "docstring": "Visualizes a qualitative analysis of a given model.\n\n  Args:\n    inputs: A tensor of the original inputs, of shape [batch, timesteps,\n      h, w, c].\n    model: A DisentangledSequentialVAE model.\n    samples: Number of samples to draw from the latent distributions.\n    batch_size: Number of sequences to generate.\n    length: Number of timesteps to generate for each sequence.", "docstring_tokens": ["Visualizes", "a", "qualitative", "analysis", "of", "a", "given", "model", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L1001-L1032", "partition": "test", "index": 722, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "visualize_reconstruction", "original_string": "def visualize_reconstruction(inputs, reconstruct, num=3, name=\"reconstruction\"):\n  \"\"\"Visualizes the reconstruction of inputs in TensorBoard.\n\n  Args:\n    inputs: A tensor of the original inputs, of shape [batch, timesteps,\n      h, w, c].\n    reconstruct: A tensor of a reconstruction of inputs, of shape\n      [batch, timesteps, h, w, c].\n    num: Integer for the number of examples to visualize.\n    name: String name of this summary.\n  \"\"\"\n  reconstruct = tf.clip_by_value(reconstruct, 0., 1.)\n  inputs_and_reconstruct = tf.concat((inputs[:num], reconstruct[:num]), axis=0)\n  image_summary(inputs_and_reconstruct, name)", "language": "python", "code": "def visualize_reconstruction(inputs, reconstruct, num=3, name=\"reconstruction\"):\n  \"\"\"Visualizes the reconstruction of inputs in TensorBoard.\n\n  Args:\n    inputs: A tensor of the original inputs, of shape [batch, timesteps,\n      h, w, c].\n    reconstruct: A tensor of a reconstruction of inputs, of shape\n      [batch, timesteps, h, w, c].\n    num: Integer for the number of examples to visualize.\n    name: String name of this summary.\n  \"\"\"\n  reconstruct = tf.clip_by_value(reconstruct, 0., 1.)\n  inputs_and_reconstruct = tf.concat((inputs[:num], reconstruct[:num]), axis=0)\n  image_summary(inputs_and_reconstruct, name)", "code_tokens": ["def", "visualize_reconstruction", "(", "inputs", ",", "reconstruct", ",", "num", "=", "3", ",", "name", "=", "\"reconstruction\"", ")", ":", "reconstruct", "=", "tf", ".", "clip_by_value", "(", "reconstruct", ",", "0.", ",", "1.", ")", "inputs_and_reconstruct", "=", "tf", ".", "concat", "(", "(", "inputs", "[", ":", "num", "]", ",", "reconstruct", "[", ":", "num", "]", ")", ",", "axis", "=", "0", ")", "image_summary", "(", "inputs_and_reconstruct", ",", "name", ")"], "docstring": "Visualizes the reconstruction of inputs in TensorBoard.\n\n  Args:\n    inputs: A tensor of the original inputs, of shape [batch, timesteps,\n      h, w, c].\n    reconstruct: A tensor of a reconstruction of inputs, of shape\n      [batch, timesteps, h, w, c].\n    num: Integer for the number of examples to visualize.\n    name: String name of this summary.", "docstring_tokens": ["Visualizes", "the", "reconstruction", "of", "inputs", "in", "TensorBoard", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L985-L998", "partition": "test", "index": 721, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "LearnableMultivariateNormalDiagCell.call", "original_string": "def call(self, inputs, state):\n    \"\"\"Runs the model to generate a distribution for a single timestep.\n\n    This generates a batched MultivariateNormalDiag distribution using\n    the output of the recurrent model at the current timestep to\n    parameterize the distribution.\n\n    Args:\n      inputs: The sampled value of `z` at the previous timestep, i.e.,\n        `z_{t-1}`, of shape [..., dimensions].\n        `z_0` should be set to the empty matrix.\n      state: A tuple containing the (hidden, cell) state.\n\n    Returns:\n      A tuple of a MultivariateNormalDiag distribution, and the state of\n      the recurrent function at the end of the current timestep. The\n      distribution will have event shape [dimensions], batch shape\n      [...], and sample shape [sample_shape, ..., dimensions].\n    \"\"\"\n    # In order to allow the user to pass in a single example without a batch\n    # dimension, we always expand the input to at least two dimensions, then\n    # fix the output shape to remove the batch dimension if necessary.\n    original_shape = inputs.shape\n    if len(original_shape) < 2:\n      inputs = tf.reshape(inputs, [1, -1])\n    out, state = self.lstm_cell(inputs, state)\n    out = self.output_layer(out)\n    correct_shape = tf.concat((original_shape[:-1], tf.shape(input=out)[-1:]),\n                              0)\n    out = tf.reshape(out, correct_shape)\n    loc = out[..., :self.dimensions]\n    scale_diag = tf.nn.softplus(out[..., self.dimensions:]) + 1e-5  # keep > 0\n    return tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag), state", "language": "python", "code": "def call(self, inputs, state):\n    \"\"\"Runs the model to generate a distribution for a single timestep.\n\n    This generates a batched MultivariateNormalDiag distribution using\n    the output of the recurrent model at the current timestep to\n    parameterize the distribution.\n\n    Args:\n      inputs: The sampled value of `z` at the previous timestep, i.e.,\n        `z_{t-1}`, of shape [..., dimensions].\n        `z_0` should be set to the empty matrix.\n      state: A tuple containing the (hidden, cell) state.\n\n    Returns:\n      A tuple of a MultivariateNormalDiag distribution, and the state of\n      the recurrent function at the end of the current timestep. The\n      distribution will have event shape [dimensions], batch shape\n      [...], and sample shape [sample_shape, ..., dimensions].\n    \"\"\"\n    # In order to allow the user to pass in a single example without a batch\n    # dimension, we always expand the input to at least two dimensions, then\n    # fix the output shape to remove the batch dimension if necessary.\n    original_shape = inputs.shape\n    if len(original_shape) < 2:\n      inputs = tf.reshape(inputs, [1, -1])\n    out, state = self.lstm_cell(inputs, state)\n    out = self.output_layer(out)\n    correct_shape = tf.concat((original_shape[:-1], tf.shape(input=out)[-1:]),\n                              0)\n    out = tf.reshape(out, correct_shape)\n    loc = out[..., :self.dimensions]\n    scale_diag = tf.nn.softplus(out[..., self.dimensions:]) + 1e-5  # keep > 0\n    return tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag), state", "code_tokens": ["def", "call", "(", "self", ",", "inputs", ",", "state", ")", ":", "# In order to allow the user to pass in a single example without a batch", "# dimension, we always expand the input to at least two dimensions, then", "# fix the output shape to remove the batch dimension if necessary.", "original_shape", "=", "inputs", ".", "shape", "if", "len", "(", "original_shape", ")", "<", "2", ":", "inputs", "=", "tf", ".", "reshape", "(", "inputs", ",", "[", "1", ",", "-", "1", "]", ")", "out", ",", "state", "=", "self", ".", "lstm_cell", "(", "inputs", ",", "state", ")", "out", "=", "self", ".", "output_layer", "(", "out", ")", "correct_shape", "=", "tf", ".", "concat", "(", "(", "original_shape", "[", ":", "-", "1", "]", ",", "tf", ".", "shape", "(", "input", "=", "out", ")", "[", "-", "1", ":", "]", ")", ",", "0", ")", "out", "=", "tf", ".", "reshape", "(", "out", ",", "correct_shape", ")", "loc", "=", "out", "[", "...", ",", ":", "self", ".", "dimensions", "]", "scale_diag", "=", "tf", ".", "nn", ".", "softplus", "(", "out", "[", "...", ",", "self", ".", "dimensions", ":", "]", ")", "+", "1e-5", "# keep > 0", "return", "tfd", ".", "MultivariateNormalDiag", "(", "loc", "=", "loc", ",", "scale_diag", "=", "scale_diag", ")", ",", "state"], "docstring": "Runs the model to generate a distribution for a single timestep.\n\n    This generates a batched MultivariateNormalDiag distribution using\n    the output of the recurrent model at the current timestep to\n    parameterize the distribution.\n\n    Args:\n      inputs: The sampled value of `z` at the previous timestep, i.e.,\n        `z_{t-1}`, of shape [..., dimensions].\n        `z_0` should be set to the empty matrix.\n      state: A tuple containing the (hidden, cell) state.\n\n    Returns:\n      A tuple of a MultivariateNormalDiag distribution, and the state of\n      the recurrent function at the end of the current timestep. The\n      distribution will have event shape [dimensions], batch shape\n      [...], and sample shape [sample_shape, ..., dimensions].", "docstring_tokens": ["Runs", "the", "model", "to", "generate", "a", "distribution", "for", "a", "single", "timestep", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L283-L315", "partition": "test", "index": 727, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "LearnableMultivariateNormalDiagCell.zero_state", "original_string": "def zero_state(self, sample_batch_shape=()):\n    \"\"\"Returns an initial state for the LSTM cell.\n\n    Args:\n      sample_batch_shape: A 0D or 1D tensor of the combined sample and\n        batch shape.\n\n    Returns:\n      A tuple of the initial previous output at timestep 0 of shape\n      [sample_batch_shape, dimensions], and the cell state.\n    \"\"\"\n    h0 = tf.zeros([1, self.hidden_size])\n    c0 = tf.zeros([1, self.hidden_size])\n    combined_shape = tf.concat((tf.convert_to_tensor(\n        value=sample_batch_shape, dtype=tf.int32), [self.dimensions]),\n                               axis=-1)\n    previous_output = tf.zeros(combined_shape)\n    return previous_output, (h0, c0)", "language": "python", "code": "def zero_state(self, sample_batch_shape=()):\n    \"\"\"Returns an initial state for the LSTM cell.\n\n    Args:\n      sample_batch_shape: A 0D or 1D tensor of the combined sample and\n        batch shape.\n\n    Returns:\n      A tuple of the initial previous output at timestep 0 of shape\n      [sample_batch_shape, dimensions], and the cell state.\n    \"\"\"\n    h0 = tf.zeros([1, self.hidden_size])\n    c0 = tf.zeros([1, self.hidden_size])\n    combined_shape = tf.concat((tf.convert_to_tensor(\n        value=sample_batch_shape, dtype=tf.int32), [self.dimensions]),\n                               axis=-1)\n    previous_output = tf.zeros(combined_shape)\n    return previous_output, (h0, c0)", "code_tokens": ["def", "zero_state", "(", "self", ",", "sample_batch_shape", "=", "(", ")", ")", ":", "h0", "=", "tf", ".", "zeros", "(", "[", "1", ",", "self", ".", "hidden_size", "]", ")", "c0", "=", "tf", ".", "zeros", "(", "[", "1", ",", "self", ".", "hidden_size", "]", ")", "combined_shape", "=", "tf", ".", "concat", "(", "(", "tf", ".", "convert_to_tensor", "(", "value", "=", "sample_batch_shape", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "[", "self", ".", "dimensions", "]", ")", ",", "axis", "=", "-", "1", ")", "previous_output", "=", "tf", ".", "zeros", "(", "combined_shape", ")", "return", "previous_output", ",", "(", "h0", ",", "c0", ")"], "docstring": "Returns an initial state for the LSTM cell.\n\n    Args:\n      sample_batch_shape: A 0D or 1D tensor of the combined sample and\n        batch shape.\n\n    Returns:\n      A tuple of the initial previous output at timestep 0 of shape\n      [sample_batch_shape, dimensions], and the cell state.", "docstring_tokens": ["Returns", "an", "initial", "state", "for", "the", "LSTM", "cell", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L264-L281", "partition": "test", "index": 726, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "LearnableMultivariateNormalDiag.call", "original_string": "def call(self, inputs):\n    \"\"\"Runs the model to generate multivariate normal distribution.\n\n    Args:\n      inputs: Unused.\n\n    Returns:\n      A MultivariateNormalDiag distribution with event shape\n      [dimensions], batch shape [], and sample shape [sample_shape,\n      dimensions].\n    \"\"\"\n    del inputs  # unused\n    with tf.compat.v1.name_scope(self._name):\n      return tfd.MultivariateNormalDiag(self.loc, self.scale_diag)", "language": "python", "code": "def call(self, inputs):\n    \"\"\"Runs the model to generate multivariate normal distribution.\n\n    Args:\n      inputs: Unused.\n\n    Returns:\n      A MultivariateNormalDiag distribution with event shape\n      [dimensions], batch shape [], and sample shape [sample_shape,\n      dimensions].\n    \"\"\"\n    del inputs  # unused\n    with tf.compat.v1.name_scope(self._name):\n      return tfd.MultivariateNormalDiag(self.loc, self.scale_diag)", "code_tokens": ["def", "call", "(", "self", ",", "inputs", ")", ":", "del", "inputs", "# unused", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "self", ".", "_name", ")", ":", "return", "tfd", ".", "MultivariateNormalDiag", "(", "self", ".", "loc", ",", "self", ".", "scale_diag", ")"], "docstring": "Runs the model to generate multivariate normal distribution.\n\n    Args:\n      inputs: Unused.\n\n    Returns:\n      A MultivariateNormalDiag distribution with event shape\n      [dimensions], batch shape [], and sample shape [sample_shape,\n      dimensions].", "docstring_tokens": ["Runs", "the", "model", "to", "generate", "multivariate", "normal", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L216-L229", "partition": "test", "index": 725, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "DisentangledSequentialVAE.sample_dynamic_prior", "original_string": "def sample_dynamic_prior(self, samples, batch_size, length, fixed=False):\n    \"\"\"Sample the dynamic latent prior.\n\n    Args:\n      samples: Number of samples to draw from the latent distribution.\n      batch_size: Number of sequences to sample.\n      length: Number of timesteps to sample for each sequence.\n      fixed: Boolean for whether or not to share the same random\n        sample across all sequences.\n\n    Returns:\n      A tuple of a sample tensor of shape [samples, batch_size, length\n      latent_size], and a MultivariateNormalDiag distribution from which\n      the tensor was sampled, with event shape [latent_size], and batch\n      shape [samples, 1, length] if fixed or [samples, batch_size,\n      length] otherwise.\n    \"\"\"\n    if fixed:\n      sample_batch_size = 1\n    else:\n      sample_batch_size = batch_size\n\n    sample, state = self.dynamic_prior.zero_state([samples, sample_batch_size])\n    locs = []\n    scale_diags = []\n    sample_list = []\n    for _ in range(length):\n      dist, state = self.dynamic_prior(sample, state)\n      sample = dist.sample()\n      locs.append(dist.parameters[\"loc\"])\n      scale_diags.append(dist.parameters[\"scale_diag\"])\n      sample_list.append(sample)\n\n    sample = tf.stack(sample_list, axis=2)\n    loc = tf.stack(locs, axis=2)\n    scale_diag = tf.stack(scale_diags, axis=2)\n\n    if fixed:  # tile along the batch axis\n      sample = sample + tf.zeros([batch_size, 1, 1])\n\n    return sample, tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)", "language": "python", "code": "def sample_dynamic_prior(self, samples, batch_size, length, fixed=False):\n    \"\"\"Sample the dynamic latent prior.\n\n    Args:\n      samples: Number of samples to draw from the latent distribution.\n      batch_size: Number of sequences to sample.\n      length: Number of timesteps to sample for each sequence.\n      fixed: Boolean for whether or not to share the same random\n        sample across all sequences.\n\n    Returns:\n      A tuple of a sample tensor of shape [samples, batch_size, length\n      latent_size], and a MultivariateNormalDiag distribution from which\n      the tensor was sampled, with event shape [latent_size], and batch\n      shape [samples, 1, length] if fixed or [samples, batch_size,\n      length] otherwise.\n    \"\"\"\n    if fixed:\n      sample_batch_size = 1\n    else:\n      sample_batch_size = batch_size\n\n    sample, state = self.dynamic_prior.zero_state([samples, sample_batch_size])\n    locs = []\n    scale_diags = []\n    sample_list = []\n    for _ in range(length):\n      dist, state = self.dynamic_prior(sample, state)\n      sample = dist.sample()\n      locs.append(dist.parameters[\"loc\"])\n      scale_diags.append(dist.parameters[\"scale_diag\"])\n      sample_list.append(sample)\n\n    sample = tf.stack(sample_list, axis=2)\n    loc = tf.stack(locs, axis=2)\n    scale_diag = tf.stack(scale_diags, axis=2)\n\n    if fixed:  # tile along the batch axis\n      sample = sample + tf.zeros([batch_size, 1, 1])\n\n    return sample, tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)", "code_tokens": ["def", "sample_dynamic_prior", "(", "self", ",", "samples", ",", "batch_size", ",", "length", ",", "fixed", "=", "False", ")", ":", "if", "fixed", ":", "sample_batch_size", "=", "1", "else", ":", "sample_batch_size", "=", "batch_size", "sample", ",", "state", "=", "self", ".", "dynamic_prior", ".", "zero_state", "(", "[", "samples", ",", "sample_batch_size", "]", ")", "locs", "=", "[", "]", "scale_diags", "=", "[", "]", "sample_list", "=", "[", "]", "for", "_", "in", "range", "(", "length", ")", ":", "dist", ",", "state", "=", "self", ".", "dynamic_prior", "(", "sample", ",", "state", ")", "sample", "=", "dist", ".", "sample", "(", ")", "locs", ".", "append", "(", "dist", ".", "parameters", "[", "\"loc\"", "]", ")", "scale_diags", ".", "append", "(", "dist", ".", "parameters", "[", "\"scale_diag\"", "]", ")", "sample_list", ".", "append", "(", "sample", ")", "sample", "=", "tf", ".", "stack", "(", "sample_list", ",", "axis", "=", "2", ")", "loc", "=", "tf", ".", "stack", "(", "locs", ",", "axis", "=", "2", ")", "scale_diag", "=", "tf", ".", "stack", "(", "scale_diags", ",", "axis", "=", "2", ")", "if", "fixed", ":", "# tile along the batch axis", "sample", "=", "sample", "+", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", ",", "1", "]", ")", "return", "sample", ",", "tfd", ".", "MultivariateNormalDiag", "(", "loc", "=", "loc", ",", "scale_diag", "=", "scale_diag", ")"], "docstring": "Sample the dynamic latent prior.\n\n    Args:\n      samples: Number of samples to draw from the latent distribution.\n      batch_size: Number of sequences to sample.\n      length: Number of timesteps to sample for each sequence.\n      fixed: Boolean for whether or not to share the same random\n        sample across all sequences.\n\n    Returns:\n      A tuple of a sample tensor of shape [samples, batch_size, length\n      latent_size], and a MultivariateNormalDiag distribution from which\n      the tensor was sampled, with event shape [latent_size], and batch\n      shape [samples, 1, length] if fixed or [samples, batch_size,\n      length] otherwise.", "docstring_tokens": ["Sample", "the", "dynamic", "latent", "prior", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L887-L927", "partition": "test", "index": 732, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "DisentangledSequentialVAE.sample_static_prior", "original_string": "def sample_static_prior(self, samples, batch_size, fixed=False):\n    \"\"\"Sample the static latent prior.\n\n    Args:\n      samples: Number of samples to draw from the latent distribution.\n      batch_size: Number of sequences to sample.\n      fixed: Boolean for whether or not to share the same random\n        sample across all sequences.\n\n    Returns:\n      A tuple of a sample tensor of shape [samples, batch_size,\n      latent_size], and a MultivariateNormalDiag distribution from which\n      the tensor was sampled, with event shape [latent_size], and batch\n      shape [].\n    \"\"\"\n    dist = self.static_prior()\n    if fixed:  # in either case, shape is (samples, batch, latent)\n      sample = dist.sample((samples, 1)) + tf.zeros([batch_size, 1])\n    else:\n      sample = dist.sample((samples, batch_size))\n    return sample, dist", "language": "python", "code": "def sample_static_prior(self, samples, batch_size, fixed=False):\n    \"\"\"Sample the static latent prior.\n\n    Args:\n      samples: Number of samples to draw from the latent distribution.\n      batch_size: Number of sequences to sample.\n      fixed: Boolean for whether or not to share the same random\n        sample across all sequences.\n\n    Returns:\n      A tuple of a sample tensor of shape [samples, batch_size,\n      latent_size], and a MultivariateNormalDiag distribution from which\n      the tensor was sampled, with event shape [latent_size], and batch\n      shape [].\n    \"\"\"\n    dist = self.static_prior()\n    if fixed:  # in either case, shape is (samples, batch, latent)\n      sample = dist.sample((samples, 1)) + tf.zeros([batch_size, 1])\n    else:\n      sample = dist.sample((samples, batch_size))\n    return sample, dist", "code_tokens": ["def", "sample_static_prior", "(", "self", ",", "samples", ",", "batch_size", ",", "fixed", "=", "False", ")", ":", "dist", "=", "self", ".", "static_prior", "(", ")", "if", "fixed", ":", "# in either case, shape is (samples, batch, latent)", "sample", "=", "dist", ".", "sample", "(", "(", "samples", ",", "1", ")", ")", "+", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", "]", ")", "else", ":", "sample", "=", "dist", ".", "sample", "(", "(", "samples", ",", "batch_size", ")", ")", "return", "sample", ",", "dist"], "docstring": "Sample the static latent prior.\n\n    Args:\n      samples: Number of samples to draw from the latent distribution.\n      batch_size: Number of sequences to sample.\n      fixed: Boolean for whether or not to share the same random\n        sample across all sequences.\n\n    Returns:\n      A tuple of a sample tensor of shape [samples, batch_size,\n      latent_size], and a MultivariateNormalDiag distribution from which\n      the tensor was sampled, with event shape [latent_size], and batch\n      shape [].", "docstring_tokens": ["Sample", "the", "static", "latent", "prior", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L846-L866", "partition": "test", "index": 731, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "image_summary", "original_string": "def image_summary(seqs, name, num=None):\n  \"\"\"Visualizes sequences as TensorBoard summaries.\n\n  Args:\n    seqs: A tensor of shape [n, t, h, w, c].\n    name: String name of this summary.\n    num: Integer for the number of examples to visualize. Defaults to\n      all examples.\n  \"\"\"\n  seqs = tf.clip_by_value(seqs, 0., 1.)\n  seqs = tf.unstack(seqs[:num])\n  joined_seqs = [tf.concat(tf.unstack(seq), 1) for seq in seqs]\n  joined_seqs = tf.expand_dims(tf.concat(joined_seqs, 0), 0)\n  tf.compat.v2.summary.image(\n      name,\n      joined_seqs,\n      max_outputs=1,\n      step=tf.compat.v1.train.get_or_create_global_step())", "language": "python", "code": "def image_summary(seqs, name, num=None):\n  \"\"\"Visualizes sequences as TensorBoard summaries.\n\n  Args:\n    seqs: A tensor of shape [n, t, h, w, c].\n    name: String name of this summary.\n    num: Integer for the number of examples to visualize. Defaults to\n      all examples.\n  \"\"\"\n  seqs = tf.clip_by_value(seqs, 0., 1.)\n  seqs = tf.unstack(seqs[:num])\n  joined_seqs = [tf.concat(tf.unstack(seq), 1) for seq in seqs]\n  joined_seqs = tf.expand_dims(tf.concat(joined_seqs, 0), 0)\n  tf.compat.v2.summary.image(\n      name,\n      joined_seqs,\n      max_outputs=1,\n      step=tf.compat.v1.train.get_or_create_global_step())", "code_tokens": ["def", "image_summary", "(", "seqs", ",", "name", ",", "num", "=", "None", ")", ":", "seqs", "=", "tf", ".", "clip_by_value", "(", "seqs", ",", "0.", ",", "1.", ")", "seqs", "=", "tf", ".", "unstack", "(", "seqs", "[", ":", "num", "]", ")", "joined_seqs", "=", "[", "tf", ".", "concat", "(", "tf", ".", "unstack", "(", "seq", ")", ",", "1", ")", "for", "seq", "in", "seqs", "]", "joined_seqs", "=", "tf", ".", "expand_dims", "(", "tf", ".", "concat", "(", "joined_seqs", ",", "0", ")", ",", "0", ")", "tf", ".", "compat", ".", "v2", ".", "summary", ".", "image", "(", "name", ",", "joined_seqs", ",", "max_outputs", "=", "1", ",", "step", "=", "tf", ".", "compat", ".", "v1", ".", "train", ".", "get_or_create_global_step", "(", ")", ")"], "docstring": "Visualizes sequences as TensorBoard summaries.\n\n  Args:\n    seqs: A tensor of shape [n, t, h, w, c].\n    name: String name of this summary.\n    num: Integer for the number of examples to visualize. Defaults to\n      all examples.", "docstring_tokens": ["Visualizes", "sequences", "as", "TensorBoard", "summaries", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L965-L982", "partition": "test", "index": 720, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "Compressor.call", "original_string": "def call(self, inputs):\n    \"\"\"Runs the model to generate an intermediate representation of x_t.\n\n    Args:\n      inputs: A batch of image sequences `x_{1:T}` of shape\n        `[sample_shape, batch_size, timesteps, height, width,\n        channels]`.\n\n    Returns:\n      A batch of intermediate representations of shape [sample_shape,\n      batch_size, timesteps, hidden_size].\n    \"\"\"\n    image_shape = tf.shape(input=inputs)[-3:]\n    collapsed_shape = tf.concat(([-1], image_shape), axis=0)\n    out = tf.reshape(inputs, collapsed_shape)  # (sample*batch*T, h, w, c)\n    out = self.conv1(out)\n    out = self.conv2(out)\n    out = self.conv3(out)\n    out = self.conv4(out)\n    expanded_shape = tf.concat((tf.shape(input=inputs)[:-3], [-1]), axis=0)\n    return tf.reshape(out, expanded_shape)", "language": "python", "code": "def call(self, inputs):\n    \"\"\"Runs the model to generate an intermediate representation of x_t.\n\n    Args:\n      inputs: A batch of image sequences `x_{1:T}` of shape\n        `[sample_shape, batch_size, timesteps, height, width,\n        channels]`.\n\n    Returns:\n      A batch of intermediate representations of shape [sample_shape,\n      batch_size, timesteps, hidden_size].\n    \"\"\"\n    image_shape = tf.shape(input=inputs)[-3:]\n    collapsed_shape = tf.concat(([-1], image_shape), axis=0)\n    out = tf.reshape(inputs, collapsed_shape)  # (sample*batch*T, h, w, c)\n    out = self.conv1(out)\n    out = self.conv2(out)\n    out = self.conv3(out)\n    out = self.conv4(out)\n    expanded_shape = tf.concat((tf.shape(input=inputs)[:-3], [-1]), axis=0)\n    return tf.reshape(out, expanded_shape)", "code_tokens": ["def", "call", "(", "self", ",", "inputs", ")", ":", "image_shape", "=", "tf", ".", "shape", "(", "input", "=", "inputs", ")", "[", "-", "3", ":", "]", "collapsed_shape", "=", "tf", ".", "concat", "(", "(", "[", "-", "1", "]", ",", "image_shape", ")", ",", "axis", "=", "0", ")", "out", "=", "tf", ".", "reshape", "(", "inputs", ",", "collapsed_shape", ")", "# (sample*batch*T, h, w, c)", "out", "=", "self", ".", "conv1", "(", "out", ")", "out", "=", "self", ".", "conv2", "(", "out", ")", "out", "=", "self", ".", "conv3", "(", "out", ")", "out", "=", "self", ".", "conv4", "(", "out", ")", "expanded_shape", "=", "tf", ".", "concat", "(", "(", "tf", ".", "shape", "(", "input", "=", "inputs", ")", "[", ":", "-", "3", "]", ",", "[", "-", "1", "]", ")", ",", "axis", "=", "0", ")", "return", "tf", ".", "reshape", "(", "out", ",", "expanded_shape", ")"], "docstring": "Runs the model to generate an intermediate representation of x_t.\n\n    Args:\n      inputs: A batch of image sequences `x_{1:T}` of shape\n        `[sample_shape, batch_size, timesteps, height, width,\n        channels]`.\n\n    Returns:\n      A batch of intermediate representations of shape [sample_shape,\n      batch_size, timesteps, hidden_size].", "docstring_tokens": ["Runs", "the", "model", "to", "generate", "an", "intermediate", "representation", "of", "x_t", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L421-L441", "partition": "test", "index": 728, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "DisentangledSequentialVAE.generate", "original_string": "def generate(self, batch_size, length, samples=1, fix_static=False,\n               fix_dynamic=False):\n    \"\"\"Generate new sequences.\n\n    Args:\n      batch_size: Number of sequences to generate.\n      length: Number of timesteps to generate for each sequence.\n      samples: Number of samples to draw from the latent distributions.\n      fix_static: Boolean for whether or not to share the same random\n        sample of the static latent variable `f` from its prior across\n        all examples.\n      fix_dynamic: Boolean for whether or not to share the same random\n        sample of the dynamic latent variable `z_{1:T}` from its prior\n        across all examples.\n\n    Returns:\n      A batched Independent distribution wrapping a set of Normal\n      distributions over the pixels of the generated sequences, where\n      the Independent distribution has event shape [height, width,\n      channels], batch shape [samples, batch_size, timesteps], and\n      sample shape [sample_shape, samples, batch_size, timesteps,\n      height, width, channels].\n    \"\"\"\n    static_sample, _ = self.sample_static_prior(samples, batch_size, fix_static)\n    dynamic_sample, _ = self.sample_dynamic_prior(samples, batch_size, length,\n                                                  fix_dynamic)\n    likelihood = self.decoder((dynamic_sample, static_sample))\n    return likelihood", "language": "python", "code": "def generate(self, batch_size, length, samples=1, fix_static=False,\n               fix_dynamic=False):\n    \"\"\"Generate new sequences.\n\n    Args:\n      batch_size: Number of sequences to generate.\n      length: Number of timesteps to generate for each sequence.\n      samples: Number of samples to draw from the latent distributions.\n      fix_static: Boolean for whether or not to share the same random\n        sample of the static latent variable `f` from its prior across\n        all examples.\n      fix_dynamic: Boolean for whether or not to share the same random\n        sample of the dynamic latent variable `z_{1:T}` from its prior\n        across all examples.\n\n    Returns:\n      A batched Independent distribution wrapping a set of Normal\n      distributions over the pixels of the generated sequences, where\n      the Independent distribution has event shape [height, width,\n      channels], batch shape [samples, batch_size, timesteps], and\n      sample shape [sample_shape, samples, batch_size, timesteps,\n      height, width, channels].\n    \"\"\"\n    static_sample, _ = self.sample_static_prior(samples, batch_size, fix_static)\n    dynamic_sample, _ = self.sample_dynamic_prior(samples, batch_size, length,\n                                                  fix_dynamic)\n    likelihood = self.decoder((dynamic_sample, static_sample))\n    return likelihood", "code_tokens": ["def", "generate", "(", "self", ",", "batch_size", ",", "length", ",", "samples", "=", "1", ",", "fix_static", "=", "False", ",", "fix_dynamic", "=", "False", ")", ":", "static_sample", ",", "_", "=", "self", ".", "sample_static_prior", "(", "samples", ",", "batch_size", ",", "fix_static", ")", "dynamic_sample", ",", "_", "=", "self", ".", "sample_dynamic_prior", "(", "samples", ",", "batch_size", ",", "length", ",", "fix_dynamic", ")", "likelihood", "=", "self", ".", "decoder", "(", "(", "dynamic_sample", ",", "static_sample", ")", ")", "return", "likelihood"], "docstring": "Generate new sequences.\n\n    Args:\n      batch_size: Number of sequences to generate.\n      length: Number of timesteps to generate for each sequence.\n      samples: Number of samples to draw from the latent distributions.\n      fix_static: Boolean for whether or not to share the same random\n        sample of the static latent variable `f` from its prior across\n        all examples.\n      fix_dynamic: Boolean for whether or not to share the same random\n        sample of the dynamic latent variable `z_{1:T}` from its prior\n        across all examples.\n\n    Returns:\n      A batched Independent distribution wrapping a set of Normal\n      distributions over the pixels of the generated sequences, where\n      the Independent distribution has event shape [height, width,\n      channels], batch shape [samples, batch_size, timesteps], and\n      sample shape [sample_shape, samples, batch_size, timesteps,\n      height, width, channels].", "docstring_tokens": ["Generate", "new", "sequences", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L758-L785", "partition": "test", "index": 729, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "DisentangledSequentialVAE.reconstruct", "original_string": "def reconstruct(self, inputs, samples=1, sample_static=False,\n                  sample_dynamic=False, swap_static=False, swap_dynamic=False,\n                  fix_static=False, fix_dynamic=False):\n    \"\"\"Reconstruct the given input sequences.\n\n    Args:\n      inputs: A batch of image sequences `x_{1:T}` of shape\n        `[batch_size, timesteps, height, width, channels]`.\n      samples: Number of samples to draw from the latent distributions.\n      sample_static: Boolean for whether or not to randomly sample the\n        static latent variable `f` from its prior distribution.\n      sample_dynamic: Boolean for whether or not to randomly sample the\n        dynamic latent variable `z_{1:T}` from its prior distribution.\n      swap_static: Boolean for whether or not to swap the encodings for\n        the static latent variable `f` between the examples.\n      swap_dynamic: Boolean for whether or not to swap the encodings for\n        the dynamic latent variable `z_{1:T}` between the examples.\n      fix_static: Boolean for whether or not to share the same random\n        sample of the static latent variable `f` from its prior across\n        all examples.\n      fix_dynamic: Boolean for whether or not to share the same random\n        sample of the dynamic latent variable `z_{1:T}` from its prior\n        across all examples.\n\n    Returns:\n      A batched Independent distribution wrapping a set of Normal\n      distributions over the pixels of the reconstruction of the input,\n      where the Independent distribution has event shape [height, width,\n      channels], batch shape [samples, batch_size, timesteps], and\n      sample shape [sample_shape, samples, batch_size, timesteps,\n      height, width, channels].\n    \"\"\"\n    batch_size = tf.shape(input=inputs)[-5]\n    length = len(tf.unstack(inputs, axis=-4))  # hack for graph mode\n\n    features = self.compressor(inputs)  # (..., batch, timesteps, hidden)\n\n    if sample_static:\n      static_sample, _ = self.sample_static_prior(\n          samples, batch_size, fix_static)\n    else:\n      static_sample, _ = self.sample_static_posterior(features, samples)\n\n    if swap_static:\n      static_sample = tf.reverse(static_sample, axis=[1])\n\n    if sample_dynamic:\n      dynamic_sample, _ = self.sample_dynamic_prior(\n          samples, batch_size, length, fix_dynamic)\n    else:\n      dynamic_sample, _ = self.sample_dynamic_posterior(\n          features, samples, static_sample)\n\n    if swap_dynamic:\n      dynamic_sample = tf.reverse(dynamic_sample, axis=[1])\n\n    likelihood = self.decoder((dynamic_sample, static_sample))\n    return likelihood", "language": "python", "code": "def reconstruct(self, inputs, samples=1, sample_static=False,\n                  sample_dynamic=False, swap_static=False, swap_dynamic=False,\n                  fix_static=False, fix_dynamic=False):\n    \"\"\"Reconstruct the given input sequences.\n\n    Args:\n      inputs: A batch of image sequences `x_{1:T}` of shape\n        `[batch_size, timesteps, height, width, channels]`.\n      samples: Number of samples to draw from the latent distributions.\n      sample_static: Boolean for whether or not to randomly sample the\n        static latent variable `f` from its prior distribution.\n      sample_dynamic: Boolean for whether or not to randomly sample the\n        dynamic latent variable `z_{1:T}` from its prior distribution.\n      swap_static: Boolean for whether or not to swap the encodings for\n        the static latent variable `f` between the examples.\n      swap_dynamic: Boolean for whether or not to swap the encodings for\n        the dynamic latent variable `z_{1:T}` between the examples.\n      fix_static: Boolean for whether or not to share the same random\n        sample of the static latent variable `f` from its prior across\n        all examples.\n      fix_dynamic: Boolean for whether or not to share the same random\n        sample of the dynamic latent variable `z_{1:T}` from its prior\n        across all examples.\n\n    Returns:\n      A batched Independent distribution wrapping a set of Normal\n      distributions over the pixels of the reconstruction of the input,\n      where the Independent distribution has event shape [height, width,\n      channels], batch shape [samples, batch_size, timesteps], and\n      sample shape [sample_shape, samples, batch_size, timesteps,\n      height, width, channels].\n    \"\"\"\n    batch_size = tf.shape(input=inputs)[-5]\n    length = len(tf.unstack(inputs, axis=-4))  # hack for graph mode\n\n    features = self.compressor(inputs)  # (..., batch, timesteps, hidden)\n\n    if sample_static:\n      static_sample, _ = self.sample_static_prior(\n          samples, batch_size, fix_static)\n    else:\n      static_sample, _ = self.sample_static_posterior(features, samples)\n\n    if swap_static:\n      static_sample = tf.reverse(static_sample, axis=[1])\n\n    if sample_dynamic:\n      dynamic_sample, _ = self.sample_dynamic_prior(\n          samples, batch_size, length, fix_dynamic)\n    else:\n      dynamic_sample, _ = self.sample_dynamic_posterior(\n          features, samples, static_sample)\n\n    if swap_dynamic:\n      dynamic_sample = tf.reverse(dynamic_sample, axis=[1])\n\n    likelihood = self.decoder((dynamic_sample, static_sample))\n    return likelihood", "code_tokens": ["def", "reconstruct", "(", "self", ",", "inputs", ",", "samples", "=", "1", ",", "sample_static", "=", "False", ",", "sample_dynamic", "=", "False", ",", "swap_static", "=", "False", ",", "swap_dynamic", "=", "False", ",", "fix_static", "=", "False", ",", "fix_dynamic", "=", "False", ")", ":", "batch_size", "=", "tf", ".", "shape", "(", "input", "=", "inputs", ")", "[", "-", "5", "]", "length", "=", "len", "(", "tf", ".", "unstack", "(", "inputs", ",", "axis", "=", "-", "4", ")", ")", "# hack for graph mode", "features", "=", "self", ".", "compressor", "(", "inputs", ")", "# (..., batch, timesteps, hidden)", "if", "sample_static", ":", "static_sample", ",", "_", "=", "self", ".", "sample_static_prior", "(", "samples", ",", "batch_size", ",", "fix_static", ")", "else", ":", "static_sample", ",", "_", "=", "self", ".", "sample_static_posterior", "(", "features", ",", "samples", ")", "if", "swap_static", ":", "static_sample", "=", "tf", ".", "reverse", "(", "static_sample", ",", "axis", "=", "[", "1", "]", ")", "if", "sample_dynamic", ":", "dynamic_sample", ",", "_", "=", "self", ".", "sample_dynamic_prior", "(", "samples", ",", "batch_size", ",", "length", ",", "fix_dynamic", ")", "else", ":", "dynamic_sample", ",", "_", "=", "self", ".", "sample_dynamic_posterior", "(", "features", ",", "samples", ",", "static_sample", ")", "if", "swap_dynamic", ":", "dynamic_sample", "=", "tf", ".", "reverse", "(", "dynamic_sample", ",", "axis", "=", "[", "1", "]", ")", "likelihood", "=", "self", ".", "decoder", "(", "(", "dynamic_sample", ",", "static_sample", ")", ")", "return", "likelihood"], "docstring": "Reconstruct the given input sequences.\n\n    Args:\n      inputs: A batch of image sequences `x_{1:T}` of shape\n        `[batch_size, timesteps, height, width, channels]`.\n      samples: Number of samples to draw from the latent distributions.\n      sample_static: Boolean for whether or not to randomly sample the\n        static latent variable `f` from its prior distribution.\n      sample_dynamic: Boolean for whether or not to randomly sample the\n        dynamic latent variable `z_{1:T}` from its prior distribution.\n      swap_static: Boolean for whether or not to swap the encodings for\n        the static latent variable `f` between the examples.\n      swap_dynamic: Boolean for whether or not to swap the encodings for\n        the dynamic latent variable `z_{1:T}` between the examples.\n      fix_static: Boolean for whether or not to share the same random\n        sample of the static latent variable `f` from its prior across\n        all examples.\n      fix_dynamic: Boolean for whether or not to share the same random\n        sample of the dynamic latent variable `z_{1:T}` from its prior\n        across all examples.\n\n    Returns:\n      A batched Independent distribution wrapping a set of Normal\n      distributions over the pixels of the reconstruction of the input,\n      where the Independent distribution has event shape [height, width,\n      channels], batch shape [samples, batch_size, timesteps], and\n      sample shape [sample_shape, samples, batch_size, timesteps,\n      height, width, channels].", "docstring_tokens": ["Reconstruct", "the", "given", "input", "sequences", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L787-L844", "partition": "test", "index": 730, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "summarize_dist_params", "original_string": "def summarize_dist_params(dist, name, name_scope=\"dist_params\"):\n  \"\"\"Summarize the parameters of a distribution.\n\n  Args:\n    dist: A Distribution object with mean and standard deviation\n      parameters.\n    name: The name of the distribution.\n    name_scope: The name scope of this summary.\n  \"\"\"\n  with tf.compat.v1.name_scope(name_scope):\n    tf.compat.v2.summary.histogram(\n        name=\"{}/{}\".format(name, \"mean\"),\n        data=dist.mean(),\n        step=tf.compat.v1.train.get_or_create_global_step())\n    tf.compat.v2.summary.histogram(\n        name=\"{}/{}\".format(name, \"stddev\"),\n        data=dist.stddev(),\n        step=tf.compat.v1.train.get_or_create_global_step())", "language": "python", "code": "def summarize_dist_params(dist, name, name_scope=\"dist_params\"):\n  \"\"\"Summarize the parameters of a distribution.\n\n  Args:\n    dist: A Distribution object with mean and standard deviation\n      parameters.\n    name: The name of the distribution.\n    name_scope: The name scope of this summary.\n  \"\"\"\n  with tf.compat.v1.name_scope(name_scope):\n    tf.compat.v2.summary.histogram(\n        name=\"{}/{}\".format(name, \"mean\"),\n        data=dist.mean(),\n        step=tf.compat.v1.train.get_or_create_global_step())\n    tf.compat.v2.summary.histogram(\n        name=\"{}/{}\".format(name, \"stddev\"),\n        data=dist.stddev(),\n        step=tf.compat.v1.train.get_or_create_global_step())", "code_tokens": ["def", "summarize_dist_params", "(", "dist", ",", "name", ",", "name_scope", "=", "\"dist_params\"", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name_scope", ")", ":", "tf", ".", "compat", ".", "v2", ".", "summary", ".", "histogram", "(", "name", "=", "\"{}/{}\"", ".", "format", "(", "name", ",", "\"mean\"", ")", ",", "data", "=", "dist", ".", "mean", "(", ")", ",", "step", "=", "tf", ".", "compat", ".", "v1", ".", "train", ".", "get_or_create_global_step", "(", ")", ")", "tf", ".", "compat", ".", "v2", ".", "summary", ".", "histogram", "(", "name", "=", "\"{}/{}\"", ".", "format", "(", "name", ",", "\"stddev\"", ")", ",", "data", "=", "dist", ".", "stddev", "(", ")", ",", "step", "=", "tf", ".", "compat", ".", "v1", ".", "train", ".", "get_or_create_global_step", "(", ")", ")"], "docstring": "Summarize the parameters of a distribution.\n\n  Args:\n    dist: A Distribution object with mean and standard deviation\n      parameters.\n    name: The name of the distribution.\n    name_scope: The name scope of this summary.", "docstring_tokens": ["Summarize", "the", "parameters", "of", "a", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L1035-L1052", "partition": "test", "index": 723, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "summarize_mean_in_nats_and_bits", "original_string": "def summarize_mean_in_nats_and_bits(inputs, units, name,\n                                    nats_name_scope=\"nats\",\n                                    bits_name_scope=\"bits_per_dim\"):\n  \"\"\"Summarize the mean of a tensor in nats and bits per unit.\n\n  Args:\n    inputs: A tensor of values measured in nats.\n    units: The units of the tensor with which to compute the mean bits\n      per unit.\n    name: The name of the tensor.\n    nats_name_scope: The name scope of the nats summary.\n    bits_name_scope: The name scope of the bits summary.\n  \"\"\"\n  mean = tf.reduce_mean(input_tensor=inputs)\n  with tf.compat.v1.name_scope(nats_name_scope):\n    tf.compat.v2.summary.scalar(\n        name,\n        mean,\n        step=tf.compat.v1.train.get_or_create_global_step())\n  with tf.compat.v1.name_scope(bits_name_scope):\n    tf.compat.v2.summary.scalar(\n        name,\n        mean / units / tf.math.log(2.),\n        step=tf.compat.v1.train.get_or_create_global_step())", "language": "python", "code": "def summarize_mean_in_nats_and_bits(inputs, units, name,\n                                    nats_name_scope=\"nats\",\n                                    bits_name_scope=\"bits_per_dim\"):\n  \"\"\"Summarize the mean of a tensor in nats and bits per unit.\n\n  Args:\n    inputs: A tensor of values measured in nats.\n    units: The units of the tensor with which to compute the mean bits\n      per unit.\n    name: The name of the tensor.\n    nats_name_scope: The name scope of the nats summary.\n    bits_name_scope: The name scope of the bits summary.\n  \"\"\"\n  mean = tf.reduce_mean(input_tensor=inputs)\n  with tf.compat.v1.name_scope(nats_name_scope):\n    tf.compat.v2.summary.scalar(\n        name,\n        mean,\n        step=tf.compat.v1.train.get_or_create_global_step())\n  with tf.compat.v1.name_scope(bits_name_scope):\n    tf.compat.v2.summary.scalar(\n        name,\n        mean / units / tf.math.log(2.),\n        step=tf.compat.v1.train.get_or_create_global_step())", "code_tokens": ["def", "summarize_mean_in_nats_and_bits", "(", "inputs", ",", "units", ",", "name", ",", "nats_name_scope", "=", "\"nats\"", ",", "bits_name_scope", "=", "\"bits_per_dim\"", ")", ":", "mean", "=", "tf", ".", "reduce_mean", "(", "input_tensor", "=", "inputs", ")", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "nats_name_scope", ")", ":", "tf", ".", "compat", ".", "v2", ".", "summary", ".", "scalar", "(", "name", ",", "mean", ",", "step", "=", "tf", ".", "compat", ".", "v1", ".", "train", ".", "get_or_create_global_step", "(", ")", ")", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "bits_name_scope", ")", ":", "tf", ".", "compat", ".", "v2", ".", "summary", ".", "scalar", "(", "name", ",", "mean", "/", "units", "/", "tf", ".", "math", ".", "log", "(", "2.", ")", ",", "step", "=", "tf", ".", "compat", ".", "v1", ".", "train", ".", "get_or_create_global_step", "(", ")", ")"], "docstring": "Summarize the mean of a tensor in nats and bits per unit.\n\n  Args:\n    inputs: A tensor of values measured in nats.\n    units: The units of the tensor with which to compute the mean bits\n      per unit.\n    name: The name of the tensor.\n    nats_name_scope: The name scope of the nats summary.\n    bits_name_scope: The name scope of the bits summary.", "docstring_tokens": ["Summarize", "the", "mean", "of", "a", "tensor", "in", "nats", "and", "bits", "per", "unit", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L1055-L1078", "partition": "test", "index": 724, "time": "2018-09-14 16:01:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/half_cauchy.py", "func_name": "check_arg_in_support", "original_string": "def check_arg_in_support(f):\n  \"\"\"Decorator function for argument bounds checking.\n\n  This decorator is meant to be used with methods that require the first\n  argument to be in the support of the distribution. If `validate_args` is\n  `True`, the method is wrapped with an assertion that the first argument is\n  greater than or equal to `loc`, since the support of the half-Cauchy\n  distribution is given by `[loc, infinity)`.\n\n\n  Args:\n    f: method to be decorated.\n\n  Returns:\n    Returns a decorated method that, when `validate_args` attribute of the class\n    is `True`, will assert that all elements in the first argument are within\n    the support of the distribution before executing the original method.\n  \"\"\"\n  @functools.wraps(f)\n  def _check_arg_and_apply_f(*args, **kwargs):\n    dist = args[0]\n    x = args[1]\n    with tf.control_dependencies([\n        assert_util.assert_greater_equal(\n            x, dist.loc, message=\"x is not in the support of the distribution\")\n    ] if dist.validate_args else []):\n      return f(*args, **kwargs)\n  return _check_arg_and_apply_f", "language": "python", "code": "def check_arg_in_support(f):\n  \"\"\"Decorator function for argument bounds checking.\n\n  This decorator is meant to be used with methods that require the first\n  argument to be in the support of the distribution. If `validate_args` is\n  `True`, the method is wrapped with an assertion that the first argument is\n  greater than or equal to `loc`, since the support of the half-Cauchy\n  distribution is given by `[loc, infinity)`.\n\n\n  Args:\n    f: method to be decorated.\n\n  Returns:\n    Returns a decorated method that, when `validate_args` attribute of the class\n    is `True`, will assert that all elements in the first argument are within\n    the support of the distribution before executing the original method.\n  \"\"\"\n  @functools.wraps(f)\n  def _check_arg_and_apply_f(*args, **kwargs):\n    dist = args[0]\n    x = args[1]\n    with tf.control_dependencies([\n        assert_util.assert_greater_equal(\n            x, dist.loc, message=\"x is not in the support of the distribution\")\n    ] if dist.validate_args else []):\n      return f(*args, **kwargs)\n  return _check_arg_and_apply_f", "code_tokens": ["def", "check_arg_in_support", "(", "f", ")", ":", "@", "functools", ".", "wraps", "(", "f", ")", "def", "_check_arg_and_apply_f", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "dist", "=", "args", "[", "0", "]", "x", "=", "args", "[", "1", "]", "with", "tf", ".", "control_dependencies", "(", "[", "assert_util", ".", "assert_greater_equal", "(", "x", ",", "dist", ".", "loc", ",", "message", "=", "\"x is not in the support of the distribution\"", ")", "]", "if", "dist", ".", "validate_args", "else", "[", "]", ")", ":", "return", "f", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "_check_arg_and_apply_f"], "docstring": "Decorator function for argument bounds checking.\n\n  This decorator is meant to be used with methods that require the first\n  argument to be in the support of the distribution. If `validate_args` is\n  `True`, the method is wrapped with an assertion that the first argument is\n  greater than or equal to `loc`, since the support of the half-Cauchy\n  distribution is given by `[loc, infinity)`.\n\n\n  Args:\n    f: method to be decorated.\n\n  Returns:\n    Returns a decorated method that, when `validate_args` attribute of the class\n    is `True`, will assert that all elements in the first argument are within\n    the support of the distribution before executing the original method.", "docstring_tokens": ["Decorator", "function", "for", "argument", "bounds", "checking", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/half_cauchy.py#L36-L63", "partition": "test", "index": 719, "time": "2018-09-18 17:10:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/bijector.py", "func_name": "_Mapping._merge", "original_string": "def _merge(self, old, new, use_equals=False):\n    \"\"\"Helper to merge which handles merging one value.\"\"\"\n    if old is None:\n      return new\n    if new is None:\n      return old\n    if (old == new) if use_equals else (old is new):\n      return old\n    raise ValueError(\"Incompatible values: %s != %s\" % (old, new))", "language": "python", "code": "def _merge(self, old, new, use_equals=False):\n    \"\"\"Helper to merge which handles merging one value.\"\"\"\n    if old is None:\n      return new\n    if new is None:\n      return old\n    if (old == new) if use_equals else (old is new):\n      return old\n    raise ValueError(\"Incompatible values: %s != %s\" % (old, new))", "code_tokens": ["def", "_merge", "(", "self", ",", "old", ",", "new", ",", "use_equals", "=", "False", ")", ":", "if", "old", "is", "None", ":", "return", "new", "if", "new", "is", "None", ":", "return", "old", "if", "(", "old", "==", "new", ")", "if", "use_equals", "else", "(", "old", "is", "new", ")", ":", "return", "old", "raise", "ValueError", "(", "\"Incompatible values: %s != %s\"", "%", "(", "old", ",", "new", ")", ")"], "docstring": "Helper to merge which handles merging one value.", "docstring_tokens": ["Helper", "to", "merge", "which", "handles", "merging", "one", "value", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/bijector.py#L112-L120", "partition": "test", "index": 790, "time": "2018-09-18 18:21:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/bijector.py", "func_name": "_Mapping.merge", "original_string": "def merge(self, x=None, y=None, ildj=None, kwargs=None, mapping=None):\n    \"\"\"Returns new _Mapping with args merged with self.\n\n    Args:\n      x: `Tensor` or None. Input to forward; output of inverse.\n      y: `Tensor` or None. Input to inverse; output of forward.\n      ildj: `Tensor`. This is the (un-reduce_sum'ed) inverse log det jacobian.\n      kwargs: Python dictionary. Extra args supplied to forward/inverse/etc\n        functions.\n      mapping: Instance of _Mapping to merge. Can only be specified if no other\n        arg is specified.\n\n    Returns:\n      mapping: New instance of `_Mapping` which has inputs merged with self.\n\n    Raises:\n      ValueError: if mapping and any other arg is not `None`.\n    \"\"\"\n    if mapping is None:\n      mapping = _Mapping(x=x, y=y, ildj=ildj, kwargs=kwargs)\n    elif any(arg is not None for arg in [x, y, ildj, kwargs]):\n      raise ValueError(\"Cannot simultaneously specify mapping and individual \"\n                       \"arguments.\")\n\n    return _Mapping(\n        x=self._merge(self.x, mapping.x),\n        y=self._merge(self.y, mapping.y),\n        ildj=self._merge(self.ildj, mapping.ildj),\n        kwargs=self._merge(self.kwargs, mapping.kwargs, use_equals=True))", "language": "python", "code": "def merge(self, x=None, y=None, ildj=None, kwargs=None, mapping=None):\n    \"\"\"Returns new _Mapping with args merged with self.\n\n    Args:\n      x: `Tensor` or None. Input to forward; output of inverse.\n      y: `Tensor` or None. Input to inverse; output of forward.\n      ildj: `Tensor`. This is the (un-reduce_sum'ed) inverse log det jacobian.\n      kwargs: Python dictionary. Extra args supplied to forward/inverse/etc\n        functions.\n      mapping: Instance of _Mapping to merge. Can only be specified if no other\n        arg is specified.\n\n    Returns:\n      mapping: New instance of `_Mapping` which has inputs merged with self.\n\n    Raises:\n      ValueError: if mapping and any other arg is not `None`.\n    \"\"\"\n    if mapping is None:\n      mapping = _Mapping(x=x, y=y, ildj=ildj, kwargs=kwargs)\n    elif any(arg is not None for arg in [x, y, ildj, kwargs]):\n      raise ValueError(\"Cannot simultaneously specify mapping and individual \"\n                       \"arguments.\")\n\n    return _Mapping(\n        x=self._merge(self.x, mapping.x),\n        y=self._merge(self.y, mapping.y),\n        ildj=self._merge(self.ildj, mapping.ildj),\n        kwargs=self._merge(self.kwargs, mapping.kwargs, use_equals=True))", "code_tokens": ["def", "merge", "(", "self", ",", "x", "=", "None", ",", "y", "=", "None", ",", "ildj", "=", "None", ",", "kwargs", "=", "None", ",", "mapping", "=", "None", ")", ":", "if", "mapping", "is", "None", ":", "mapping", "=", "_Mapping", "(", "x", "=", "x", ",", "y", "=", "y", ",", "ildj", "=", "ildj", ",", "kwargs", "=", "kwargs", ")", "elif", "any", "(", "arg", "is", "not", "None", "for", "arg", "in", "[", "x", ",", "y", ",", "ildj", ",", "kwargs", "]", ")", ":", "raise", "ValueError", "(", "\"Cannot simultaneously specify mapping and individual \"", "\"arguments.\"", ")", "return", "_Mapping", "(", "x", "=", "self", ".", "_merge", "(", "self", ".", "x", ",", "mapping", ".", "x", ")", ",", "y", "=", "self", ".", "_merge", "(", "self", ".", "y", ",", "mapping", ".", "y", ")", ",", "ildj", "=", "self", ".", "_merge", "(", "self", ".", "ildj", ",", "mapping", ".", "ildj", ")", ",", "kwargs", "=", "self", ".", "_merge", "(", "self", ".", "kwargs", ",", "mapping", ".", "kwargs", ",", "use_equals", "=", "True", ")", ")"], "docstring": "Returns new _Mapping with args merged with self.\n\n    Args:\n      x: `Tensor` or None. Input to forward; output of inverse.\n      y: `Tensor` or None. Input to inverse; output of forward.\n      ildj: `Tensor`. This is the (un-reduce_sum'ed) inverse log det jacobian.\n      kwargs: Python dictionary. Extra args supplied to forward/inverse/etc\n        functions.\n      mapping: Instance of _Mapping to merge. Can only be specified if no other\n        arg is specified.\n\n    Returns:\n      mapping: New instance of `_Mapping` which has inputs merged with self.\n\n    Raises:\n      ValueError: if mapping and any other arg is not `None`.", "docstring_tokens": ["Returns", "new", "_Mapping", "with", "args", "merged", "with", "self", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/bijector.py#L74-L102", "partition": "test", "index": 788, "time": "2018-09-18 18:21:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/positive_semidefinite_kernels/matern.py", "func_name": "_AmplitudeLengthScaleMixin._init_params", "original_string": "def _init_params(self, amplitude, length_scale, validate_args):\n    \"\"\"Shared init logic for `amplitude` and `length_scale` params.\n\n    Args:\n      amplitude: `Tensor` (or convertible) or `None` to convert, validate.\n      length_scale: `Tensor` (or convertible) or `None` to convert, validate.\n      validate_args: If `True`, parameters are checked for validity despite\n        possibly degrading runtime performance\n\n    Returns:\n      dtype: The common `DType` of the parameters.\n    \"\"\"\n    dtype = util.maybe_get_common_dtype(\n        [amplitude, length_scale])\n    if amplitude is not None:\n      amplitude = tf.convert_to_tensor(\n          value=amplitude, name='amplitude', dtype=dtype)\n    self._amplitude = _validate_arg_if_not_none(\n        amplitude, tf.compat.v1.assert_positive, validate_args)\n    if length_scale is not None:\n      length_scale = tf.convert_to_tensor(\n          value=length_scale, name='length_scale', dtype=dtype)\n    self._length_scale = _validate_arg_if_not_none(\n        length_scale, tf.compat.v1.assert_positive, validate_args)\n    return dtype", "language": "python", "code": "def _init_params(self, amplitude, length_scale, validate_args):\n    \"\"\"Shared init logic for `amplitude` and `length_scale` params.\n\n    Args:\n      amplitude: `Tensor` (or convertible) or `None` to convert, validate.\n      length_scale: `Tensor` (or convertible) or `None` to convert, validate.\n      validate_args: If `True`, parameters are checked for validity despite\n        possibly degrading runtime performance\n\n    Returns:\n      dtype: The common `DType` of the parameters.\n    \"\"\"\n    dtype = util.maybe_get_common_dtype(\n        [amplitude, length_scale])\n    if amplitude is not None:\n      amplitude = tf.convert_to_tensor(\n          value=amplitude, name='amplitude', dtype=dtype)\n    self._amplitude = _validate_arg_if_not_none(\n        amplitude, tf.compat.v1.assert_positive, validate_args)\n    if length_scale is not None:\n      length_scale = tf.convert_to_tensor(\n          value=length_scale, name='length_scale', dtype=dtype)\n    self._length_scale = _validate_arg_if_not_none(\n        length_scale, tf.compat.v1.assert_positive, validate_args)\n    return dtype", "code_tokens": ["def", "_init_params", "(", "self", ",", "amplitude", ",", "length_scale", ",", "validate_args", ")", ":", "dtype", "=", "util", ".", "maybe_get_common_dtype", "(", "[", "amplitude", ",", "length_scale", "]", ")", "if", "amplitude", "is", "not", "None", ":", "amplitude", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "amplitude", ",", "name", "=", "'amplitude'", ",", "dtype", "=", "dtype", ")", "self", ".", "_amplitude", "=", "_validate_arg_if_not_none", "(", "amplitude", ",", "tf", ".", "compat", ".", "v1", ".", "assert_positive", ",", "validate_args", ")", "if", "length_scale", "is", "not", "None", ":", "length_scale", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "length_scale", ",", "name", "=", "'length_scale'", ",", "dtype", "=", "dtype", ")", "self", ".", "_length_scale", "=", "_validate_arg_if_not_none", "(", "length_scale", ",", "tf", ".", "compat", ".", "v1", ".", "assert_positive", ",", "validate_args", ")", "return", "dtype"], "docstring": "Shared init logic for `amplitude` and `length_scale` params.\n\n    Args:\n      amplitude: `Tensor` (or convertible) or `None` to convert, validate.\n      length_scale: `Tensor` (or convertible) or `None` to convert, validate.\n      validate_args: If `True`, parameters are checked for validity despite\n        possibly degrading runtime performance\n\n    Returns:\n      dtype: The common `DType` of the parameters.", "docstring_tokens": ["Shared", "init", "logic", "for", "amplitude", "and", "length_scale", "params", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/positive_semidefinite_kernels/matern.py#L44-L68", "partition": "test", "index": 1020, "time": "2018-09-24 10:51:40"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/structural_time_series.py", "func_name": "StructuralTimeSeries.batch_shape_tensor", "original_string": "def batch_shape_tensor(self):\n    \"\"\"Runtime batch shape of models represented by this component.\n\n    Returns:\n      batch_shape: `int` `Tensor` giving the broadcast batch shape of\n        all model parameters. This should match the batch shape of\n        derived state space models, i.e.,\n        `self.make_state_space_model(...).batch_shape_tensor()`.\n    \"\"\"\n    batch_shape = tf.constant([], dtype=tf.int32)\n    for param in self.parameters:\n      batch_shape = tf.broadcast_dynamic_shape(\n          batch_shape, param.prior.batch_shape_tensor())\n    return batch_shape", "language": "python", "code": "def batch_shape_tensor(self):\n    \"\"\"Runtime batch shape of models represented by this component.\n\n    Returns:\n      batch_shape: `int` `Tensor` giving the broadcast batch shape of\n        all model parameters. This should match the batch shape of\n        derived state space models, i.e.,\n        `self.make_state_space_model(...).batch_shape_tensor()`.\n    \"\"\"\n    batch_shape = tf.constant([], dtype=tf.int32)\n    for param in self.parameters:\n      batch_shape = tf.broadcast_dynamic_shape(\n          batch_shape, param.prior.batch_shape_tensor())\n    return batch_shape", "code_tokens": ["def", "batch_shape_tensor", "(", "self", ")", ":", "batch_shape", "=", "tf", ".", "constant", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "for", "param", "in", "self", ".", "parameters", ":", "batch_shape", "=", "tf", ".", "broadcast_dynamic_shape", "(", "batch_shape", ",", "param", ".", "prior", ".", "batch_shape_tensor", "(", ")", ")", "return", "batch_shape"], "docstring": "Runtime batch shape of models represented by this component.\n\n    Returns:\n      batch_shape: `int` `Tensor` giving the broadcast batch shape of\n        all model parameters. This should match the batch shape of\n        derived state space models, i.e.,\n        `self.make_state_space_model(...).batch_shape_tensor()`.", "docstring_tokens": ["Runtime", "batch", "shape", "of", "models", "represented", "by", "this", "component", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/structural_time_series.py#L96-L109", "partition": "test", "index": 734, "time": "2018-09-24 12:15:45"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/structural_time_series.py", "func_name": "StructuralTimeSeries.batch_shape", "original_string": "def batch_shape(self):\n    \"\"\"Static batch shape of models represented by this component.\n\n    Returns:\n      batch_shape: A `tf.TensorShape` giving the broadcast batch shape of\n        all model parameters. This should match the batch shape of\n        derived state space models, i.e.,\n        `self.make_state_space_model(...).batch_shape`. It may be partially\n        defined or unknown.\n    \"\"\"\n    batch_shape = tf.TensorShape([])\n    for param in self.parameters:\n      batch_shape = tf.broadcast_static_shape(\n          batch_shape, param.prior.batch_shape)\n    return batch_shape", "language": "python", "code": "def batch_shape(self):\n    \"\"\"Static batch shape of models represented by this component.\n\n    Returns:\n      batch_shape: A `tf.TensorShape` giving the broadcast batch shape of\n        all model parameters. This should match the batch shape of\n        derived state space models, i.e.,\n        `self.make_state_space_model(...).batch_shape`. It may be partially\n        defined or unknown.\n    \"\"\"\n    batch_shape = tf.TensorShape([])\n    for param in self.parameters:\n      batch_shape = tf.broadcast_static_shape(\n          batch_shape, param.prior.batch_shape)\n    return batch_shape", "code_tokens": ["def", "batch_shape", "(", "self", ")", ":", "batch_shape", "=", "tf", ".", "TensorShape", "(", "[", "]", ")", "for", "param", "in", "self", ".", "parameters", ":", "batch_shape", "=", "tf", ".", "broadcast_static_shape", "(", "batch_shape", ",", "param", ".", "prior", ".", "batch_shape", ")", "return", "batch_shape"], "docstring": "Static batch shape of models represented by this component.\n\n    Returns:\n      batch_shape: A `tf.TensorShape` giving the broadcast batch shape of\n        all model parameters. This should match the batch shape of\n        derived state space models, i.e.,\n        `self.make_state_space_model(...).batch_shape`. It may be partially\n        defined or unknown.", "docstring_tokens": ["Static", "batch", "shape", "of", "models", "represented", "by", "this", "component", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/structural_time_series.py#L80-L94", "partition": "test", "index": 733, "time": "2018-09-24 12:15:45"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "pick_scalar_condition", "original_string": "def pick_scalar_condition(pred, true_value, false_value, name=None):\n  \"\"\"Convenience function that chooses one of two values based on the predicate.\n\n  This utility is equivalent to a version of `tf.where` that accepts only a\n  scalar predicate and computes its result statically when possible. It may also\n  be used in place of `tf.cond` when both branches yield a `Tensor` of the same\n  shape; the operational difference is that `tf.cond` uses control flow to\n  evaluate only the branch that's needed, while `tf.where` (and thus\n  this method) may evaluate both branches before the predicate's truth is known.\n  This means that `tf.cond` is preferred when one of the branches is expensive\n  to evaluate (like performing a large matmul), while this method is preferred\n  when both branches are cheap, e.g., constants. In the latter case, we expect\n  this method to be substantially faster than `tf.cond` on GPU and to give\n  similar performance on CPU.\n\n  Args:\n    pred: Scalar `bool` `Tensor` predicate.\n    true_value: `Tensor` to return if `pred` is `True`.\n    false_value: `Tensor` to return if `pred` is `False`. Must have the same\n      shape as `true_value`.\n    name: Python `str` name given to ops managed by this object.\n\n  Returns:\n    result: a `Tensor` (or `Tensor`-convertible Python value) equal to\n      `true_value` if `pred` evaluates to `True` and `false_value` otherwise.\n      If the condition can be evaluated statically, the result returned is one\n      of the input Python values, with no graph side effects.\n  \"\"\"\n  with tf.name_scope(name or \"pick_scalar_condition\"):\n    pred = tf.convert_to_tensor(\n        value=pred, dtype_hint=tf.bool, name=\"pred\")\n    true_value = tf.convert_to_tensor(value=true_value, name=\"true_value\")\n    false_value = tf.convert_to_tensor(value=false_value, name=\"false_value\")\n    pred_ = tf.get_static_value(pred)\n    if pred_ is None:\n      return tf.where(pred, true_value, false_value)\n    return true_value if pred_ else false_value", "language": "python", "code": "def pick_scalar_condition(pred, true_value, false_value, name=None):\n  \"\"\"Convenience function that chooses one of two values based on the predicate.\n\n  This utility is equivalent to a version of `tf.where` that accepts only a\n  scalar predicate and computes its result statically when possible. It may also\n  be used in place of `tf.cond` when both branches yield a `Tensor` of the same\n  shape; the operational difference is that `tf.cond` uses control flow to\n  evaluate only the branch that's needed, while `tf.where` (and thus\n  this method) may evaluate both branches before the predicate's truth is known.\n  This means that `tf.cond` is preferred when one of the branches is expensive\n  to evaluate (like performing a large matmul), while this method is preferred\n  when both branches are cheap, e.g., constants. In the latter case, we expect\n  this method to be substantially faster than `tf.cond` on GPU and to give\n  similar performance on CPU.\n\n  Args:\n    pred: Scalar `bool` `Tensor` predicate.\n    true_value: `Tensor` to return if `pred` is `True`.\n    false_value: `Tensor` to return if `pred` is `False`. Must have the same\n      shape as `true_value`.\n    name: Python `str` name given to ops managed by this object.\n\n  Returns:\n    result: a `Tensor` (or `Tensor`-convertible Python value) equal to\n      `true_value` if `pred` evaluates to `True` and `false_value` otherwise.\n      If the condition can be evaluated statically, the result returned is one\n      of the input Python values, with no graph side effects.\n  \"\"\"\n  with tf.name_scope(name or \"pick_scalar_condition\"):\n    pred = tf.convert_to_tensor(\n        value=pred, dtype_hint=tf.bool, name=\"pred\")\n    true_value = tf.convert_to_tensor(value=true_value, name=\"true_value\")\n    false_value = tf.convert_to_tensor(value=false_value, name=\"false_value\")\n    pred_ = tf.get_static_value(pred)\n    if pred_ is None:\n      return tf.where(pred, true_value, false_value)\n    return true_value if pred_ else false_value", "code_tokens": ["def", "pick_scalar_condition", "(", "pred", ",", "true_value", ",", "false_value", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "\"pick_scalar_condition\"", ")", ":", "pred", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "pred", ",", "dtype_hint", "=", "tf", ".", "bool", ",", "name", "=", "\"pred\"", ")", "true_value", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "true_value", ",", "name", "=", "\"true_value\"", ")", "false_value", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "false_value", ",", "name", "=", "\"false_value\"", ")", "pred_", "=", "tf", ".", "get_static_value", "(", "pred", ")", "if", "pred_", "is", "None", ":", "return", "tf", ".", "where", "(", "pred", ",", "true_value", ",", "false_value", ")", "return", "true_value", "if", "pred_", "else", "false_value"], "docstring": "Convenience function that chooses one of two values based on the predicate.\n\n  This utility is equivalent to a version of `tf.where` that accepts only a\n  scalar predicate and computes its result statically when possible. It may also\n  be used in place of `tf.cond` when both branches yield a `Tensor` of the same\n  shape; the operational difference is that `tf.cond` uses control flow to\n  evaluate only the branch that's needed, while `tf.where` (and thus\n  this method) may evaluate both branches before the predicate's truth is known.\n  This means that `tf.cond` is preferred when one of the branches is expensive\n  to evaluate (like performing a large matmul), while this method is preferred\n  when both branches are cheap, e.g., constants. In the latter case, we expect\n  this method to be substantially faster than `tf.cond` on GPU and to give\n  similar performance on CPU.\n\n  Args:\n    pred: Scalar `bool` `Tensor` predicate.\n    true_value: `Tensor` to return if `pred` is `True`.\n    false_value: `Tensor` to return if `pred` is `False`. Must have the same\n      shape as `true_value`.\n    name: Python `str` name given to ops managed by this object.\n\n  Returns:\n    result: a `Tensor` (or `Tensor`-convertible Python value) equal to\n      `true_value` if `pred` evaluates to `True` and `false_value` otherwise.\n      If the condition can be evaluated statically, the result returned is one\n      of the input Python values, with no graph side effects.", "docstring_tokens": ["Convenience", "function", "that", "chooses", "one", "of", "two", "values", "based", "on", "the", "predicate", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L506-L542", "partition": "test", "index": 909, "time": "2018-09-24 16:51:30"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py", "func_name": "_psd_mask", "original_string": "def _psd_mask(x):\n  \"\"\"Computes whether each square matrix in the input is positive semi-definite.\n\n  Args:\n    x: A floating-point `Tensor` of shape `[B1, ..., Bn, M, M]`.\n\n  Returns:\n    mask: A floating-point `Tensor` of shape `[B1, ... Bn]`.  Each\n      scalar is 1 if the corresponding matrix was PSD, otherwise 0.\n  \"\"\"\n  # Allegedly\n  # https://scicomp.stackexchange.com/questions/12979/testing-if-a-matrix-is-positive-semi-definite\n  # it is more efficient to test for positive semi-definiteness by\n  # trying to compute the Cholesky decomposition -- the matrix is PSD\n  # if you succeed and not PSD if you fail.  However, TensorFlow's\n  # Cholesky raises an exception if _any_ of the input matrices are\n  # not PSD, from which I don't know how to extract _which ones_, so I\n  # proceed by explicitly computing all the eigenvalues and checking\n  # whether they are all positive or not.\n  #\n  # Also, as was discussed in the answer, it is somewhat dangerous to\n  # treat SPD-ness as binary in floating-point arithmetic. Cholesky\n  # factorization can complete and 'look' like everything is fine\n  # (e.g., O(1) entries and a diagonal of all ones) but the matrix can\n  # have an exponential condition number.\n  eigenvalues, _ = tf.linalg.eigh(x)\n  return tf.cast(\n      tf.reduce_min(input_tensor=eigenvalues, axis=-1) >= 0, dtype=x.dtype)", "language": "python", "code": "def _psd_mask(x):\n  \"\"\"Computes whether each square matrix in the input is positive semi-definite.\n\n  Args:\n    x: A floating-point `Tensor` of shape `[B1, ..., Bn, M, M]`.\n\n  Returns:\n    mask: A floating-point `Tensor` of shape `[B1, ... Bn]`.  Each\n      scalar is 1 if the corresponding matrix was PSD, otherwise 0.\n  \"\"\"\n  # Allegedly\n  # https://scicomp.stackexchange.com/questions/12979/testing-if-a-matrix-is-positive-semi-definite\n  # it is more efficient to test for positive semi-definiteness by\n  # trying to compute the Cholesky decomposition -- the matrix is PSD\n  # if you succeed and not PSD if you fail.  However, TensorFlow's\n  # Cholesky raises an exception if _any_ of the input matrices are\n  # not PSD, from which I don't know how to extract _which ones_, so I\n  # proceed by explicitly computing all the eigenvalues and checking\n  # whether they are all positive or not.\n  #\n  # Also, as was discussed in the answer, it is somewhat dangerous to\n  # treat SPD-ness as binary in floating-point arithmetic. Cholesky\n  # factorization can complete and 'look' like everything is fine\n  # (e.g., O(1) entries and a diagonal of all ones) but the matrix can\n  # have an exponential condition number.\n  eigenvalues, _ = tf.linalg.eigh(x)\n  return tf.cast(\n      tf.reduce_min(input_tensor=eigenvalues, axis=-1) >= 0, dtype=x.dtype)", "code_tokens": ["def", "_psd_mask", "(", "x", ")", ":", "# Allegedly", "# https://scicomp.stackexchange.com/questions/12979/testing-if-a-matrix-is-positive-semi-definite", "# it is more efficient to test for positive semi-definiteness by", "# trying to compute the Cholesky decomposition -- the matrix is PSD", "# if you succeed and not PSD if you fail.  However, TensorFlow's", "# Cholesky raises an exception if _any_ of the input matrices are", "# not PSD, from which I don't know how to extract _which ones_, so I", "# proceed by explicitly computing all the eigenvalues and checking", "# whether they are all positive or not.", "#", "# Also, as was discussed in the answer, it is somewhat dangerous to", "# treat SPD-ness as binary in floating-point arithmetic. Cholesky", "# factorization can complete and 'look' like everything is fine", "# (e.g., O(1) entries and a diagonal of all ones) but the matrix can", "# have an exponential condition number.", "eigenvalues", ",", "_", "=", "tf", ".", "linalg", ".", "eigh", "(", "x", ")", "return", "tf", ".", "cast", "(", "tf", ".", "reduce_min", "(", "input_tensor", "=", "eigenvalues", ",", "axis", "=", "-", "1", ")", ">=", "0", ",", "dtype", "=", "x", ".", "dtype", ")"], "docstring": "Computes whether each square matrix in the input is positive semi-definite.\n\n  Args:\n    x: A floating-point `Tensor` of shape `[B1, ..., Bn, M, M]`.\n\n  Returns:\n    mask: A floating-point `Tensor` of shape `[B1, ... Bn]`.  Each\n      scalar is 1 if the corresponding matrix was PSD, otherwise 0.", "docstring_tokens": ["Computes", "whether", "each", "square", "matrix", "in", "the", "input", "is", "positive", "semi", "-", "definite", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py#L77-L104", "partition": "test", "index": 1077, "time": "2018-09-25 15:04:25"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py", "func_name": "_det_large_enough_mask", "original_string": "def _det_large_enough_mask(x, det_bounds):\n  \"\"\"Returns whether the input matches the given determinant limit.\n\n  Args:\n    x: A floating-point `Tensor` of shape `[B1, ..., Bn, M, M]`.\n    det_bounds: A floating-point `Tensor` that must broadcast to shape\n      `[B1, ..., Bn]`, giving the desired lower bound on the\n      determinants in `x`.\n\n  Returns:\n    mask: A floating-point `Tensor` of shape [B1, ..., Bn].  Each\n      scalar is 1 if the corresponding matrix had determinant above\n      the corresponding bound, otherwise 0.\n  \"\"\"\n  # For the curious: I wonder whether it is possible and desirable to\n  # use a Cholesky decomposition-based algorithm for this, since the\n  # only matrices whose determinant this code cares about will be PSD.\n  # Didn't figure out how to code that in TensorFlow.\n  #\n  # Expert opinion is that it would be about twice as fast since\n  # Cholesky is roughly half the cost of Gaussian Elimination with\n  # Partial Pivoting. But this is less of an impact than the switch in\n  # _psd_mask.\n  return tf.cast(tf.linalg.det(x) > det_bounds, dtype=x.dtype)", "language": "python", "code": "def _det_large_enough_mask(x, det_bounds):\n  \"\"\"Returns whether the input matches the given determinant limit.\n\n  Args:\n    x: A floating-point `Tensor` of shape `[B1, ..., Bn, M, M]`.\n    det_bounds: A floating-point `Tensor` that must broadcast to shape\n      `[B1, ..., Bn]`, giving the desired lower bound on the\n      determinants in `x`.\n\n  Returns:\n    mask: A floating-point `Tensor` of shape [B1, ..., Bn].  Each\n      scalar is 1 if the corresponding matrix had determinant above\n      the corresponding bound, otherwise 0.\n  \"\"\"\n  # For the curious: I wonder whether it is possible and desirable to\n  # use a Cholesky decomposition-based algorithm for this, since the\n  # only matrices whose determinant this code cares about will be PSD.\n  # Didn't figure out how to code that in TensorFlow.\n  #\n  # Expert opinion is that it would be about twice as fast since\n  # Cholesky is roughly half the cost of Gaussian Elimination with\n  # Partial Pivoting. But this is less of an impact than the switch in\n  # _psd_mask.\n  return tf.cast(tf.linalg.det(x) > det_bounds, dtype=x.dtype)", "code_tokens": ["def", "_det_large_enough_mask", "(", "x", ",", "det_bounds", ")", ":", "# For the curious: I wonder whether it is possible and desirable to", "# use a Cholesky decomposition-based algorithm for this, since the", "# only matrices whose determinant this code cares about will be PSD.", "# Didn't figure out how to code that in TensorFlow.", "#", "# Expert opinion is that it would be about twice as fast since", "# Cholesky is roughly half the cost of Gaussian Elimination with", "# Partial Pivoting. But this is less of an impact than the switch in", "# _psd_mask.", "return", "tf", ".", "cast", "(", "tf", ".", "linalg", ".", "det", "(", "x", ")", ">", "det_bounds", ",", "dtype", "=", "x", ".", "dtype", ")"], "docstring": "Returns whether the input matches the given determinant limit.\n\n  Args:\n    x: A floating-point `Tensor` of shape `[B1, ..., Bn, M, M]`.\n    det_bounds: A floating-point `Tensor` that must broadcast to shape\n      `[B1, ..., Bn]`, giving the desired lower bound on the\n      determinants in `x`.\n\n  Returns:\n    mask: A floating-point `Tensor` of shape [B1, ..., Bn].  Each\n      scalar is 1 if the corresponding matrix had determinant above\n      the corresponding bound, otherwise 0.", "docstring_tokens": ["Returns", "whether", "the", "input", "matches", "the", "given", "determinant", "limit", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py#L107-L130", "partition": "test", "index": 1078, "time": "2018-09-25 15:04:25"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py", "func_name": "_uniform_correlation_like_matrix", "original_string": "def _uniform_correlation_like_matrix(num_rows, batch_shape, dtype, seed):\n  \"\"\"Returns a uniformly random `Tensor` of \"correlation-like\" matrices.\n\n  A \"correlation-like\" matrix is a symmetric square matrix with all entries\n  between -1 and 1 (inclusive) and 1s on the main diagonal.  Of these,\n  the ones that are positive semi-definite are exactly the correlation\n  matrices.\n\n  Args:\n    num_rows: Python `int` dimension of the correlation-like matrices.\n    batch_shape: `Tensor` or Python `tuple` of `int` shape of the\n      batch to return.\n    dtype: `dtype` of the `Tensor` to return.\n    seed: Random seed.\n\n  Returns:\n    matrices: A `Tensor` of shape `batch_shape + [num_rows, num_rows]`\n      and dtype `dtype`.  Each entry is in [-1, 1], and each matrix\n      along the bottom two dimensions is symmetric and has 1s on the\n      main diagonal.\n  \"\"\"\n  num_entries = num_rows * (num_rows + 1) / 2\n  ones = tf.ones(shape=[num_entries], dtype=dtype)\n  # It seems wasteful to generate random values for the diagonal since\n  # I am going to throw them away, but `fill_triangular` fills the\n  # diagonal, so I probably need them.\n  # It's not impossible that it would be more efficient to just fill\n  # the whole matrix with random values instead of messing with\n  # `fill_triangular`.  Then would need to filter almost half out with\n  # `matrix_band_part`.\n  unifs = uniform.Uniform(-ones, ones).sample(batch_shape, seed=seed)\n  tril = util.fill_triangular(unifs)\n  symmetric = tril + tf.linalg.matrix_transpose(tril)\n  diagonal_ones = tf.ones(\n      shape=util.pad(batch_shape, axis=0, back=True, value=num_rows),\n      dtype=dtype)\n  return tf.linalg.set_diag(symmetric, diagonal_ones)", "language": "python", "code": "def _uniform_correlation_like_matrix(num_rows, batch_shape, dtype, seed):\n  \"\"\"Returns a uniformly random `Tensor` of \"correlation-like\" matrices.\n\n  A \"correlation-like\" matrix is a symmetric square matrix with all entries\n  between -1 and 1 (inclusive) and 1s on the main diagonal.  Of these,\n  the ones that are positive semi-definite are exactly the correlation\n  matrices.\n\n  Args:\n    num_rows: Python `int` dimension of the correlation-like matrices.\n    batch_shape: `Tensor` or Python `tuple` of `int` shape of the\n      batch to return.\n    dtype: `dtype` of the `Tensor` to return.\n    seed: Random seed.\n\n  Returns:\n    matrices: A `Tensor` of shape `batch_shape + [num_rows, num_rows]`\n      and dtype `dtype`.  Each entry is in [-1, 1], and each matrix\n      along the bottom two dimensions is symmetric and has 1s on the\n      main diagonal.\n  \"\"\"\n  num_entries = num_rows * (num_rows + 1) / 2\n  ones = tf.ones(shape=[num_entries], dtype=dtype)\n  # It seems wasteful to generate random values for the diagonal since\n  # I am going to throw them away, but `fill_triangular` fills the\n  # diagonal, so I probably need them.\n  # It's not impossible that it would be more efficient to just fill\n  # the whole matrix with random values instead of messing with\n  # `fill_triangular`.  Then would need to filter almost half out with\n  # `matrix_band_part`.\n  unifs = uniform.Uniform(-ones, ones).sample(batch_shape, seed=seed)\n  tril = util.fill_triangular(unifs)\n  symmetric = tril + tf.linalg.matrix_transpose(tril)\n  diagonal_ones = tf.ones(\n      shape=util.pad(batch_shape, axis=0, back=True, value=num_rows),\n      dtype=dtype)\n  return tf.linalg.set_diag(symmetric, diagonal_ones)", "code_tokens": ["def", "_uniform_correlation_like_matrix", "(", "num_rows", ",", "batch_shape", ",", "dtype", ",", "seed", ")", ":", "num_entries", "=", "num_rows", "*", "(", "num_rows", "+", "1", ")", "/", "2", "ones", "=", "tf", ".", "ones", "(", "shape", "=", "[", "num_entries", "]", ",", "dtype", "=", "dtype", ")", "# It seems wasteful to generate random values for the diagonal since", "# I am going to throw them away, but `fill_triangular` fills the", "# diagonal, so I probably need them.", "# It's not impossible that it would be more efficient to just fill", "# the whole matrix with random values instead of messing with", "# `fill_triangular`.  Then would need to filter almost half out with", "# `matrix_band_part`.", "unifs", "=", "uniform", ".", "Uniform", "(", "-", "ones", ",", "ones", ")", ".", "sample", "(", "batch_shape", ",", "seed", "=", "seed", ")", "tril", "=", "util", ".", "fill_triangular", "(", "unifs", ")", "symmetric", "=", "tril", "+", "tf", ".", "linalg", ".", "matrix_transpose", "(", "tril", ")", "diagonal_ones", "=", "tf", ".", "ones", "(", "shape", "=", "util", ".", "pad", "(", "batch_shape", ",", "axis", "=", "0", ",", "back", "=", "True", ",", "value", "=", "num_rows", ")", ",", "dtype", "=", "dtype", ")", "return", "tf", ".", "linalg", ".", "set_diag", "(", "symmetric", ",", "diagonal_ones", ")"], "docstring": "Returns a uniformly random `Tensor` of \"correlation-like\" matrices.\n\n  A \"correlation-like\" matrix is a symmetric square matrix with all entries\n  between -1 and 1 (inclusive) and 1s on the main diagonal.  Of these,\n  the ones that are positive semi-definite are exactly the correlation\n  matrices.\n\n  Args:\n    num_rows: Python `int` dimension of the correlation-like matrices.\n    batch_shape: `Tensor` or Python `tuple` of `int` shape of the\n      batch to return.\n    dtype: `dtype` of the `Tensor` to return.\n    seed: Random seed.\n\n  Returns:\n    matrices: A `Tensor` of shape `batch_shape + [num_rows, num_rows]`\n      and dtype `dtype`.  Each entry is in [-1, 1], and each matrix\n      along the bottom two dimensions is symmetric and has 1s on the\n      main diagonal.", "docstring_tokens": ["Returns", "a", "uniformly", "random", "Tensor", "of", "correlation", "-", "like", "matrices", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py#L133-L169", "partition": "test", "index": 1079, "time": "2018-09-25 15:04:25"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py", "func_name": "correlation_matrix_volume_rejection_samples", "original_string": "def correlation_matrix_volume_rejection_samples(\n    det_bounds, dim, sample_shape, dtype, seed):\n  \"\"\"Returns rejection samples from trying to get good correlation matrices.\n\n  The proposal being rejected from is the uniform distribution on\n  \"correlation-like\" matrices.  We say a matrix is \"correlation-like\"\n  if it is a symmetric square matrix with all entries between -1 and 1\n  (inclusive) and 1s on the main diagonal.  Of these, the ones that\n  are positive semi-definite are exactly the correlation matrices.\n\n  The rejection algorithm, then, is to sample a `Tensor` of\n  `sample_shape` correlation-like matrices of dimensions `dim` by\n  `dim`, and check each one for (i) being a correlation matrix (i.e.,\n  PSD), and (ii) having determinant at least the corresponding entry\n  of `det_bounds`.\n\n  Args:\n    det_bounds: A `Tensor` of lower bounds on the determinants of\n      acceptable matrices.  The shape must broadcast with `sample_shape`.\n    dim: A Python `int` dimension of correlation matrices to sample.\n    sample_shape: Python `tuple` of `int` shape of the samples to\n      compute, excluding the two matrix dimensions.\n    dtype: The `dtype` in which to do the computation.\n    seed: Random seed.\n\n  Returns:\n    weights: A `Tensor` of shape `sample_shape`.  Each entry is 0 if the\n      corresponding matrix was not a correlation matrix, or had too\n      small of a determinant.  Otherwise, the entry is the\n      multiplicative inverse of the density of proposing that matrix\n      uniformly, i.e., the volume of the set of `dim` by `dim`\n      correlation-like matrices.\n    volume: The volume of the set of `dim` by `dim` correlation-like\n      matrices.\n  \"\"\"\n  with tf.compat.v1.name_scope(\"rejection_sampler\"):\n    rej_proposals = _uniform_correlation_like_matrix(\n        dim, sample_shape, dtype, seed=seed)\n    rej_proposal_volume = 2. ** (dim * (dim - 1) / 2.)\n    # The density of proposing any given point is 1 / rej_proposal_volume;\n    # The weight of that point should be scaled by\n    # 1 / density = rej_proposal_volume.\n    rej_weights = rej_proposal_volume * _psd_mask(\n        rej_proposals) * _det_large_enough_mask(rej_proposals, det_bounds)\n    return rej_weights, rej_proposal_volume", "language": "python", "code": "def correlation_matrix_volume_rejection_samples(\n    det_bounds, dim, sample_shape, dtype, seed):\n  \"\"\"Returns rejection samples from trying to get good correlation matrices.\n\n  The proposal being rejected from is the uniform distribution on\n  \"correlation-like\" matrices.  We say a matrix is \"correlation-like\"\n  if it is a symmetric square matrix with all entries between -1 and 1\n  (inclusive) and 1s on the main diagonal.  Of these, the ones that\n  are positive semi-definite are exactly the correlation matrices.\n\n  The rejection algorithm, then, is to sample a `Tensor` of\n  `sample_shape` correlation-like matrices of dimensions `dim` by\n  `dim`, and check each one for (i) being a correlation matrix (i.e.,\n  PSD), and (ii) having determinant at least the corresponding entry\n  of `det_bounds`.\n\n  Args:\n    det_bounds: A `Tensor` of lower bounds on the determinants of\n      acceptable matrices.  The shape must broadcast with `sample_shape`.\n    dim: A Python `int` dimension of correlation matrices to sample.\n    sample_shape: Python `tuple` of `int` shape of the samples to\n      compute, excluding the two matrix dimensions.\n    dtype: The `dtype` in which to do the computation.\n    seed: Random seed.\n\n  Returns:\n    weights: A `Tensor` of shape `sample_shape`.  Each entry is 0 if the\n      corresponding matrix was not a correlation matrix, or had too\n      small of a determinant.  Otherwise, the entry is the\n      multiplicative inverse of the density of proposing that matrix\n      uniformly, i.e., the volume of the set of `dim` by `dim`\n      correlation-like matrices.\n    volume: The volume of the set of `dim` by `dim` correlation-like\n      matrices.\n  \"\"\"\n  with tf.compat.v1.name_scope(\"rejection_sampler\"):\n    rej_proposals = _uniform_correlation_like_matrix(\n        dim, sample_shape, dtype, seed=seed)\n    rej_proposal_volume = 2. ** (dim * (dim - 1) / 2.)\n    # The density of proposing any given point is 1 / rej_proposal_volume;\n    # The weight of that point should be scaled by\n    # 1 / density = rej_proposal_volume.\n    rej_weights = rej_proposal_volume * _psd_mask(\n        rej_proposals) * _det_large_enough_mask(rej_proposals, det_bounds)\n    return rej_weights, rej_proposal_volume", "code_tokens": ["def", "correlation_matrix_volume_rejection_samples", "(", "det_bounds", ",", "dim", ",", "sample_shape", ",", "dtype", ",", "seed", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "\"rejection_sampler\"", ")", ":", "rej_proposals", "=", "_uniform_correlation_like_matrix", "(", "dim", ",", "sample_shape", ",", "dtype", ",", "seed", "=", "seed", ")", "rej_proposal_volume", "=", "2.", "**", "(", "dim", "*", "(", "dim", "-", "1", ")", "/", "2.", ")", "# The density of proposing any given point is 1 / rej_proposal_volume;", "# The weight of that point should be scaled by", "# 1 / density = rej_proposal_volume.", "rej_weights", "=", "rej_proposal_volume", "*", "_psd_mask", "(", "rej_proposals", ")", "*", "_det_large_enough_mask", "(", "rej_proposals", ",", "det_bounds", ")", "return", "rej_weights", ",", "rej_proposal_volume"], "docstring": "Returns rejection samples from trying to get good correlation matrices.\n\n  The proposal being rejected from is the uniform distribution on\n  \"correlation-like\" matrices.  We say a matrix is \"correlation-like\"\n  if it is a symmetric square matrix with all entries between -1 and 1\n  (inclusive) and 1s on the main diagonal.  Of these, the ones that\n  are positive semi-definite are exactly the correlation matrices.\n\n  The rejection algorithm, then, is to sample a `Tensor` of\n  `sample_shape` correlation-like matrices of dimensions `dim` by\n  `dim`, and check each one for (i) being a correlation matrix (i.e.,\n  PSD), and (ii) having determinant at least the corresponding entry\n  of `det_bounds`.\n\n  Args:\n    det_bounds: A `Tensor` of lower bounds on the determinants of\n      acceptable matrices.  The shape must broadcast with `sample_shape`.\n    dim: A Python `int` dimension of correlation matrices to sample.\n    sample_shape: Python `tuple` of `int` shape of the samples to\n      compute, excluding the two matrix dimensions.\n    dtype: The `dtype` in which to do the computation.\n    seed: Random seed.\n\n  Returns:\n    weights: A `Tensor` of shape `sample_shape`.  Each entry is 0 if the\n      corresponding matrix was not a correlation matrix, or had too\n      small of a determinant.  Otherwise, the entry is the\n      multiplicative inverse of the density of proposing that matrix\n      uniformly, i.e., the volume of the set of `dim` by `dim`\n      correlation-like matrices.\n    volume: The volume of the set of `dim` by `dim` correlation-like\n      matrices.", "docstring_tokens": ["Returns", "rejection", "samples", "from", "trying", "to", "get", "good", "correlation", "matrices", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py#L172-L216", "partition": "test", "index": 1080, "time": "2018-09-25 15:04:25"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py", "func_name": "_clopper_pearson_confidence_interval", "original_string": "def _clopper_pearson_confidence_interval(samples, error_rate):\n  \"\"\"Computes a confidence interval for the mean of the given 1-D distribution.\n\n  Assumes (and checks) that the given distribution is Bernoulli, i.e.,\n  takes only two values.  This licenses using the CDF of the binomial\n  distribution for the confidence, which is tighter (for extreme\n  probabilities) than the DKWM inequality.  The method is known as the\n  [Clopper-Pearson method]\n  (https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval).\n\n  Assumes:\n\n  - The given samples were drawn iid from the distribution of interest.\n\n  - The given distribution is a Bernoulli, i.e., supported only on\n    low and high.\n\n  Guarantees:\n\n  - The probability (over the randomness of drawing the given sample)\n    that the true mean is outside the returned interval is no more\n    than the given error_rate.\n\n  Args:\n    samples: `np.ndarray` of samples drawn iid from the distribution\n      of interest.\n    error_rate: Python `float` admissible rate of mistakes.\n\n  Returns:\n    low: Lower bound of confidence interval.\n    high: Upper bound of confidence interval.\n\n  Raises:\n    ValueError: If `samples` has rank other than 1 (batch semantics\n      are not implemented), or if `samples` contains values other than\n      `low` or `high` (as that makes the distribution not Bernoulli).\n  \"\"\"\n  # TODO(b/78025336) Migrate this confidence interval function\n  # to statistical_testing.py.  In order to do that\n  # - Get the binomial CDF from the Binomial distribution\n  # - Implement scalar root finding in TF.  Batch bisection search\n  #   shouldn't be too hard, and is definitely good enough for this\n  #   problem.  Batching the Brent algorithm (from scipy) that is used\n  #   here may be more involved, but may also not be necessary---it's\n  #   only used here because scipy made it convenient.  In particular,\n  #   robustness is more important than speed here, which may make\n  #   bisection search actively better.\n  # - The rest is just a matter of rewriting in the appropriate style.\n  if optimize is None or stats is None:\n    raise ValueError(\n        \"Scipy is required for computing Clopper-Pearson confidence intervals\")\n  if len(samples.shape) != 1:\n    raise ValueError(\"Batch semantics not implemented\")\n  n = len(samples)\n  low = np.amin(samples)\n  high = np.amax(samples)\n  successes = np.count_nonzero(samples - low)\n  failures = np.count_nonzero(samples - high)\n  if successes + failures != n:\n    uniques = np.unique(samples)\n    msg = (\"Purportedly Bernoulli distribution had distinct samples\"\n           \" {}, {}, and {}\".format(uniques[0], uniques[1], uniques[2]))\n    raise ValueError(msg)\n  def p_small_enough(p):\n    prob = stats.binom.logcdf(successes, n, p)\n    return prob - np.log(error_rate / 2.)\n  def p_big_enough(p):\n    prob = stats.binom.logsf(successes, n, p)\n    return prob - np.log(error_rate / 2.)\n  high_p = optimize.brentq(\n      p_small_enough, float(successes) / n, 1., rtol=1e-9)\n  low_p = optimize.brentq(\n      p_big_enough, 0., float(successes) / n, rtol=1e-9)\n  low_interval = low + (high - low) * low_p\n  high_interval = low + (high - low) * high_p\n  return (low_interval, high_interval)", "language": "python", "code": "def _clopper_pearson_confidence_interval(samples, error_rate):\n  \"\"\"Computes a confidence interval for the mean of the given 1-D distribution.\n\n  Assumes (and checks) that the given distribution is Bernoulli, i.e.,\n  takes only two values.  This licenses using the CDF of the binomial\n  distribution for the confidence, which is tighter (for extreme\n  probabilities) than the DKWM inequality.  The method is known as the\n  [Clopper-Pearson method]\n  (https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval).\n\n  Assumes:\n\n  - The given samples were drawn iid from the distribution of interest.\n\n  - The given distribution is a Bernoulli, i.e., supported only on\n    low and high.\n\n  Guarantees:\n\n  - The probability (over the randomness of drawing the given sample)\n    that the true mean is outside the returned interval is no more\n    than the given error_rate.\n\n  Args:\n    samples: `np.ndarray` of samples drawn iid from the distribution\n      of interest.\n    error_rate: Python `float` admissible rate of mistakes.\n\n  Returns:\n    low: Lower bound of confidence interval.\n    high: Upper bound of confidence interval.\n\n  Raises:\n    ValueError: If `samples` has rank other than 1 (batch semantics\n      are not implemented), or if `samples` contains values other than\n      `low` or `high` (as that makes the distribution not Bernoulli).\n  \"\"\"\n  # TODO(b/78025336) Migrate this confidence interval function\n  # to statistical_testing.py.  In order to do that\n  # - Get the binomial CDF from the Binomial distribution\n  # - Implement scalar root finding in TF.  Batch bisection search\n  #   shouldn't be too hard, and is definitely good enough for this\n  #   problem.  Batching the Brent algorithm (from scipy) that is used\n  #   here may be more involved, but may also not be necessary---it's\n  #   only used here because scipy made it convenient.  In particular,\n  #   robustness is more important than speed here, which may make\n  #   bisection search actively better.\n  # - The rest is just a matter of rewriting in the appropriate style.\n  if optimize is None or stats is None:\n    raise ValueError(\n        \"Scipy is required for computing Clopper-Pearson confidence intervals\")\n  if len(samples.shape) != 1:\n    raise ValueError(\"Batch semantics not implemented\")\n  n = len(samples)\n  low = np.amin(samples)\n  high = np.amax(samples)\n  successes = np.count_nonzero(samples - low)\n  failures = np.count_nonzero(samples - high)\n  if successes + failures != n:\n    uniques = np.unique(samples)\n    msg = (\"Purportedly Bernoulli distribution had distinct samples\"\n           \" {}, {}, and {}\".format(uniques[0], uniques[1], uniques[2]))\n    raise ValueError(msg)\n  def p_small_enough(p):\n    prob = stats.binom.logcdf(successes, n, p)\n    return prob - np.log(error_rate / 2.)\n  def p_big_enough(p):\n    prob = stats.binom.logsf(successes, n, p)\n    return prob - np.log(error_rate / 2.)\n  high_p = optimize.brentq(\n      p_small_enough, float(successes) / n, 1., rtol=1e-9)\n  low_p = optimize.brentq(\n      p_big_enough, 0., float(successes) / n, rtol=1e-9)\n  low_interval = low + (high - low) * low_p\n  high_interval = low + (high - low) * high_p\n  return (low_interval, high_interval)", "code_tokens": ["def", "_clopper_pearson_confidence_interval", "(", "samples", ",", "error_rate", ")", ":", "# TODO(b/78025336) Migrate this confidence interval function", "# to statistical_testing.py.  In order to do that", "# - Get the binomial CDF from the Binomial distribution", "# - Implement scalar root finding in TF.  Batch bisection search", "#   shouldn't be too hard, and is definitely good enough for this", "#   problem.  Batching the Brent algorithm (from scipy) that is used", "#   here may be more involved, but may also not be necessary---it's", "#   only used here because scipy made it convenient.  In particular,", "#   robustness is more important than speed here, which may make", "#   bisection search actively better.", "# - The rest is just a matter of rewriting in the appropriate style.", "if", "optimize", "is", "None", "or", "stats", "is", "None", ":", "raise", "ValueError", "(", "\"Scipy is required for computing Clopper-Pearson confidence intervals\"", ")", "if", "len", "(", "samples", ".", "shape", ")", "!=", "1", ":", "raise", "ValueError", "(", "\"Batch semantics not implemented\"", ")", "n", "=", "len", "(", "samples", ")", "low", "=", "np", ".", "amin", "(", "samples", ")", "high", "=", "np", ".", "amax", "(", "samples", ")", "successes", "=", "np", ".", "count_nonzero", "(", "samples", "-", "low", ")", "failures", "=", "np", ".", "count_nonzero", "(", "samples", "-", "high", ")", "if", "successes", "+", "failures", "!=", "n", ":", "uniques", "=", "np", ".", "unique", "(", "samples", ")", "msg", "=", "(", "\"Purportedly Bernoulli distribution had distinct samples\"", "\" {}, {}, and {}\"", ".", "format", "(", "uniques", "[", "0", "]", ",", "uniques", "[", "1", "]", ",", "uniques", "[", "2", "]", ")", ")", "raise", "ValueError", "(", "msg", ")", "def", "p_small_enough", "(", "p", ")", ":", "prob", "=", "stats", ".", "binom", ".", "logcdf", "(", "successes", ",", "n", ",", "p", ")", "return", "prob", "-", "np", ".", "log", "(", "error_rate", "/", "2.", ")", "def", "p_big_enough", "(", "p", ")", ":", "prob", "=", "stats", ".", "binom", ".", "logsf", "(", "successes", ",", "n", ",", "p", ")", "return", "prob", "-", "np", ".", "log", "(", "error_rate", "/", "2.", ")", "high_p", "=", "optimize", ".", "brentq", "(", "p_small_enough", ",", "float", "(", "successes", ")", "/", "n", ",", "1.", ",", "rtol", "=", "1e-9", ")", "low_p", "=", "optimize", ".", "brentq", "(", "p_big_enough", ",", "0.", ",", "float", "(", "successes", ")", "/", "n", ",", "rtol", "=", "1e-9", ")", "low_interval", "=", "low", "+", "(", "high", "-", "low", ")", "*", "low_p", "high_interval", "=", "low", "+", "(", "high", "-", "low", ")", "*", "high_p", "return", "(", "low_interval", ",", "high_interval", ")"], "docstring": "Computes a confidence interval for the mean of the given 1-D distribution.\n\n  Assumes (and checks) that the given distribution is Bernoulli, i.e.,\n  takes only two values.  This licenses using the CDF of the binomial\n  distribution for the confidence, which is tighter (for extreme\n  probabilities) than the DKWM inequality.  The method is known as the\n  [Clopper-Pearson method]\n  (https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval).\n\n  Assumes:\n\n  - The given samples were drawn iid from the distribution of interest.\n\n  - The given distribution is a Bernoulli, i.e., supported only on\n    low and high.\n\n  Guarantees:\n\n  - The probability (over the randomness of drawing the given sample)\n    that the true mean is outside the returned interval is no more\n    than the given error_rate.\n\n  Args:\n    samples: `np.ndarray` of samples drawn iid from the distribution\n      of interest.\n    error_rate: Python `float` admissible rate of mistakes.\n\n  Returns:\n    low: Lower bound of confidence interval.\n    high: Upper bound of confidence interval.\n\n  Raises:\n    ValueError: If `samples` has rank other than 1 (batch semantics\n      are not implemented), or if `samples` contains values other than\n      `low` or `high` (as that makes the distribution not Bernoulli).", "docstring_tokens": ["Computes", "a", "confidence", "interval", "for", "the", "mean", "of", "the", "given", "1", "-", "D", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py#L219-L294", "partition": "test", "index": 1081, "time": "2018-09-25 15:04:25"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py", "func_name": "compute_true_volumes", "original_string": "def compute_true_volumes(\n    det_bounds, dim, num_samples, error_rate=1e-6, seed=42):\n  \"\"\"Returns confidence intervals for the desired correlation matrix volumes.\n\n  The confidence intervals are computed by the [Clopper-Pearson method]\n  (https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval).\n\n  Args:\n    det_bounds: A rank-1 numpy array of lower bounds on the\n      determinants of acceptable matrices.  Entries must be unique.\n    dim: A Python `int` dimension of correlation matrices to sample.\n    num_samples: The number of samples to draw.\n    error_rate: The statistical significance of the returned\n      confidence intervals.  The significance is broadcast: Each\n      returned interval separately may be incorrect with probability\n      (under the sample of correlation-like matrices drawn internally)\n      at most `error_rate`.\n    seed: Random seed.\n\n  Returns:\n    bounds: A Python `dict` mapping each determinant bound to the low, high\n      tuple giving the confidence interval.\n  \"\"\"\n  bounds = {}\n  with tf.compat.v1.Session() as sess:\n    rej_weights, _ = correlation_matrix_volume_rejection_samples(\n        det_bounds, dim, [num_samples, len(det_bounds)], np.float32, seed=seed)\n    rej_weights = sess.run(rej_weights)\n    for rw, det in zip(np.rollaxis(rej_weights, 1), det_bounds):\n      template = (\"Estimating volume of {}x{} correlation \"\n                  \"matrices with determinant >= {}.\")\n      print(template.format(dim, dim, det))\n      sys.stdout.flush()\n      bounds[det] = _clopper_pearson_confidence_interval(\n          rw, error_rate=error_rate)\n    return bounds", "language": "python", "code": "def compute_true_volumes(\n    det_bounds, dim, num_samples, error_rate=1e-6, seed=42):\n  \"\"\"Returns confidence intervals for the desired correlation matrix volumes.\n\n  The confidence intervals are computed by the [Clopper-Pearson method]\n  (https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval).\n\n  Args:\n    det_bounds: A rank-1 numpy array of lower bounds on the\n      determinants of acceptable matrices.  Entries must be unique.\n    dim: A Python `int` dimension of correlation matrices to sample.\n    num_samples: The number of samples to draw.\n    error_rate: The statistical significance of the returned\n      confidence intervals.  The significance is broadcast: Each\n      returned interval separately may be incorrect with probability\n      (under the sample of correlation-like matrices drawn internally)\n      at most `error_rate`.\n    seed: Random seed.\n\n  Returns:\n    bounds: A Python `dict` mapping each determinant bound to the low, high\n      tuple giving the confidence interval.\n  \"\"\"\n  bounds = {}\n  with tf.compat.v1.Session() as sess:\n    rej_weights, _ = correlation_matrix_volume_rejection_samples(\n        det_bounds, dim, [num_samples, len(det_bounds)], np.float32, seed=seed)\n    rej_weights = sess.run(rej_weights)\n    for rw, det in zip(np.rollaxis(rej_weights, 1), det_bounds):\n      template = (\"Estimating volume of {}x{} correlation \"\n                  \"matrices with determinant >= {}.\")\n      print(template.format(dim, dim, det))\n      sys.stdout.flush()\n      bounds[det] = _clopper_pearson_confidence_interval(\n          rw, error_rate=error_rate)\n    return bounds", "code_tokens": ["def", "compute_true_volumes", "(", "det_bounds", ",", "dim", ",", "num_samples", ",", "error_rate", "=", "1e-6", ",", "seed", "=", "42", ")", ":", "bounds", "=", "{", "}", "with", "tf", ".", "compat", ".", "v1", ".", "Session", "(", ")", "as", "sess", ":", "rej_weights", ",", "_", "=", "correlation_matrix_volume_rejection_samples", "(", "det_bounds", ",", "dim", ",", "[", "num_samples", ",", "len", "(", "det_bounds", ")", "]", ",", "np", ".", "float32", ",", "seed", "=", "seed", ")", "rej_weights", "=", "sess", ".", "run", "(", "rej_weights", ")", "for", "rw", ",", "det", "in", "zip", "(", "np", ".", "rollaxis", "(", "rej_weights", ",", "1", ")", ",", "det_bounds", ")", ":", "template", "=", "(", "\"Estimating volume of {}x{} correlation \"", "\"matrices with determinant >= {}.\"", ")", "print", "(", "template", ".", "format", "(", "dim", ",", "dim", ",", "det", ")", ")", "sys", ".", "stdout", ".", "flush", "(", ")", "bounds", "[", "det", "]", "=", "_clopper_pearson_confidence_interval", "(", "rw", ",", "error_rate", "=", "error_rate", ")", "return", "bounds"], "docstring": "Returns confidence intervals for the desired correlation matrix volumes.\n\n  The confidence intervals are computed by the [Clopper-Pearson method]\n  (https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval).\n\n  Args:\n    det_bounds: A rank-1 numpy array of lower bounds on the\n      determinants of acceptable matrices.  Entries must be unique.\n    dim: A Python `int` dimension of correlation matrices to sample.\n    num_samples: The number of samples to draw.\n    error_rate: The statistical significance of the returned\n      confidence intervals.  The significance is broadcast: Each\n      returned interval separately may be incorrect with probability\n      (under the sample of correlation-like matrices drawn internally)\n      at most `error_rate`.\n    seed: Random seed.\n\n  Returns:\n    bounds: A Python `dict` mapping each determinant bound to the low, high\n      tuple giving the confidence interval.", "docstring_tokens": ["Returns", "confidence", "intervals", "for", "the", "desired", "correlation", "matrix", "volumes", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py#L297-L332", "partition": "test", "index": 1082, "time": "2018-09-25 15:04:25"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs.py", "func_name": "_inv_hessian_control_inputs", "original_string": "def _inv_hessian_control_inputs(inv_hessian):\n  \"\"\"Computes control inputs to validate a provided inverse Hessian.\n\n  These ensure that the provided inverse Hessian is positive definite and\n  symmetric.\n\n  Args:\n    inv_hessian: The starting estimate for the inverse of the Hessian at the\n      initial point.\n\n  Returns:\n    A list of tf.Assert ops suitable for use with tf.control_dependencies.\n  \"\"\"\n  # The easiest way to validate if the inverse Hessian is positive definite is\n  # to compute its Cholesky decomposition.\n  is_positive_definite = tf.reduce_all(\n      input_tensor=tf.math.is_finite(tf.linalg.cholesky(inv_hessian)),\n      axis=[-1, -2])\n\n  # Then check that the supplied inverse Hessian is symmetric.\n  is_symmetric = tf.equal(bfgs_utils.norm(\n      inv_hessian - _batch_transpose(inv_hessian), dims=2), 0)\n\n  # Simply adding a control dependencies on these results is not enough to\n  # trigger them, we need to add asserts on the results.\n  return [tf.Assert(is_positive_definite,\n                    ['Initial inverse Hessian is not positive definite.',\n                     inv_hessian]),\n          tf.Assert(is_symmetric,\n                    ['Initial inverse Hessian is not symmetric',\n                     inv_hessian])]", "language": "python", "code": "def _inv_hessian_control_inputs(inv_hessian):\n  \"\"\"Computes control inputs to validate a provided inverse Hessian.\n\n  These ensure that the provided inverse Hessian is positive definite and\n  symmetric.\n\n  Args:\n    inv_hessian: The starting estimate for the inverse of the Hessian at the\n      initial point.\n\n  Returns:\n    A list of tf.Assert ops suitable for use with tf.control_dependencies.\n  \"\"\"\n  # The easiest way to validate if the inverse Hessian is positive definite is\n  # to compute its Cholesky decomposition.\n  is_positive_definite = tf.reduce_all(\n      input_tensor=tf.math.is_finite(tf.linalg.cholesky(inv_hessian)),\n      axis=[-1, -2])\n\n  # Then check that the supplied inverse Hessian is symmetric.\n  is_symmetric = tf.equal(bfgs_utils.norm(\n      inv_hessian - _batch_transpose(inv_hessian), dims=2), 0)\n\n  # Simply adding a control dependencies on these results is not enough to\n  # trigger them, we need to add asserts on the results.\n  return [tf.Assert(is_positive_definite,\n                    ['Initial inverse Hessian is not positive definite.',\n                     inv_hessian]),\n          tf.Assert(is_symmetric,\n                    ['Initial inverse Hessian is not symmetric',\n                     inv_hessian])]", "code_tokens": ["def", "_inv_hessian_control_inputs", "(", "inv_hessian", ")", ":", "# The easiest way to validate if the inverse Hessian is positive definite is", "# to compute its Cholesky decomposition.", "is_positive_definite", "=", "tf", ".", "reduce_all", "(", "input_tensor", "=", "tf", ".", "math", ".", "is_finite", "(", "tf", ".", "linalg", ".", "cholesky", "(", "inv_hessian", ")", ")", ",", "axis", "=", "[", "-", "1", ",", "-", "2", "]", ")", "# Then check that the supplied inverse Hessian is symmetric.", "is_symmetric", "=", "tf", ".", "equal", "(", "bfgs_utils", ".", "norm", "(", "inv_hessian", "-", "_batch_transpose", "(", "inv_hessian", ")", ",", "dims", "=", "2", ")", ",", "0", ")", "# Simply adding a control dependencies on these results is not enough to", "# trigger them, we need to add asserts on the results.", "return", "[", "tf", ".", "Assert", "(", "is_positive_definite", ",", "[", "'Initial inverse Hessian is not positive definite.'", ",", "inv_hessian", "]", ")", ",", "tf", ".", "Assert", "(", "is_symmetric", ",", "[", "'Initial inverse Hessian is not symmetric'", ",", "inv_hessian", "]", ")", "]"], "docstring": "Computes control inputs to validate a provided inverse Hessian.\n\n  These ensure that the provided inverse Hessian is positive definite and\n  symmetric.\n\n  Args:\n    inv_hessian: The starting estimate for the inverse of the Hessian at the\n      initial point.\n\n  Returns:\n    A list of tf.Assert ops suitable for use with tf.control_dependencies.", "docstring_tokens": ["Computes", "control", "inputs", "to", "validate", "a", "provided", "inverse", "Hessian", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs.py#L289-L319", "partition": "test", "index": 1062, "time": "2018-09-27 03:52:46"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", "func_name": "_print", "original_string": "def _print(pass_through_tensor, values):\n  \"\"\"Wrapper for tf.Print which supports lists and namedtuples for printing.\"\"\"\n  flat_values = []\n  for value in values:\n    # Checks if it is a namedtuple.\n    if hasattr(value, '_fields'):\n      for field in value._fields:\n        flat_values.extend([field, _to_str(getattr(value, field))])\n      continue\n    if isinstance(value, (list, tuple)):\n      for v in value:\n        flat_values.append(_to_str(v))\n      continue\n    flat_values.append(_to_str(value))\n  return tf.compat.v1.Print(pass_through_tensor, flat_values)", "language": "python", "code": "def _print(pass_through_tensor, values):\n  \"\"\"Wrapper for tf.Print which supports lists and namedtuples for printing.\"\"\"\n  flat_values = []\n  for value in values:\n    # Checks if it is a namedtuple.\n    if hasattr(value, '_fields'):\n      for field in value._fields:\n        flat_values.extend([field, _to_str(getattr(value, field))])\n      continue\n    if isinstance(value, (list, tuple)):\n      for v in value:\n        flat_values.append(_to_str(v))\n      continue\n    flat_values.append(_to_str(value))\n  return tf.compat.v1.Print(pass_through_tensor, flat_values)", "code_tokens": ["def", "_print", "(", "pass_through_tensor", ",", "values", ")", ":", "flat_values", "=", "[", "]", "for", "value", "in", "values", ":", "# Checks if it is a namedtuple.", "if", "hasattr", "(", "value", ",", "'_fields'", ")", ":", "for", "field", "in", "value", ".", "_fields", ":", "flat_values", ".", "extend", "(", "[", "field", ",", "_to_str", "(", "getattr", "(", "value", ",", "field", ")", ")", "]", ")", "continue", "if", "isinstance", "(", "value", ",", "(", "list", ",", "tuple", ")", ")", ":", "for", "v", "in", "value", ":", "flat_values", ".", "append", "(", "_to_str", "(", "v", ")", ")", "continue", "flat_values", ".", "append", "(", "_to_str", "(", "value", ")", ")", "return", "tf", ".", "compat", ".", "v1", ".", "Print", "(", "pass_through_tensor", ",", "flat_values", ")"], "docstring": "Wrapper for tf.Print which supports lists and namedtuples for printing.", "docstring_tokens": ["Wrapper", "for", "tf", ".", "Print", "which", "supports", "lists", "and", "namedtuples", "for", "printing", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py#L666-L680", "partition": "test", "index": 677, "time": "2018-09-27 03:52:46"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", "func_name": "_line_search_inner_bisection", "original_string": "def _line_search_inner_bisection(\n    value_and_gradients_function,\n    search_interval,\n    active,\n    f_lim):\n  \"\"\"Performs bisection and updates the interval.\"\"\"\n  midpoint = (search_interval.left.x + search_interval.right.x) / 2\n  val_mid = value_and_gradients_function(midpoint)\n  is_valid_mid = hzl.is_finite(val_mid)\n\n  still_active = active & is_valid_mid\n  new_failed = active & ~is_valid_mid\n  next_inteval = search_interval._replace(\n      failed=search_interval.failed | new_failed,\n      func_evals=search_interval.func_evals + 1)\n\n  def _apply_update():\n    update_result = hzl.update(\n        value_and_gradients_function, next_inteval.left, next_inteval.right,\n        val_mid, f_lim, active=still_active)\n    return HagerZhangLineSearchResult(\n        converged=next_inteval.converged,\n        failed=next_inteval.failed | update_result.failed,\n        iterations=next_inteval.iterations + update_result.iteration,\n        func_evals=next_inteval.func_evals + update_result.num_evals,\n        left=update_result.left,\n        right=update_result.right)\n\n  return prefer_static.cond(\n      tf.reduce_any(input_tensor=still_active),\n      _apply_update,\n      lambda: next_inteval)", "language": "python", "code": "def _line_search_inner_bisection(\n    value_and_gradients_function,\n    search_interval,\n    active,\n    f_lim):\n  \"\"\"Performs bisection and updates the interval.\"\"\"\n  midpoint = (search_interval.left.x + search_interval.right.x) / 2\n  val_mid = value_and_gradients_function(midpoint)\n  is_valid_mid = hzl.is_finite(val_mid)\n\n  still_active = active & is_valid_mid\n  new_failed = active & ~is_valid_mid\n  next_inteval = search_interval._replace(\n      failed=search_interval.failed | new_failed,\n      func_evals=search_interval.func_evals + 1)\n\n  def _apply_update():\n    update_result = hzl.update(\n        value_and_gradients_function, next_inteval.left, next_inteval.right,\n        val_mid, f_lim, active=still_active)\n    return HagerZhangLineSearchResult(\n        converged=next_inteval.converged,\n        failed=next_inteval.failed | update_result.failed,\n        iterations=next_inteval.iterations + update_result.iteration,\n        func_evals=next_inteval.func_evals + update_result.num_evals,\n        left=update_result.left,\n        right=update_result.right)\n\n  return prefer_static.cond(\n      tf.reduce_any(input_tensor=still_active),\n      _apply_update,\n      lambda: next_inteval)", "code_tokens": ["def", "_line_search_inner_bisection", "(", "value_and_gradients_function", ",", "search_interval", ",", "active", ",", "f_lim", ")", ":", "midpoint", "=", "(", "search_interval", ".", "left", ".", "x", "+", "search_interval", ".", "right", ".", "x", ")", "/", "2", "val_mid", "=", "value_and_gradients_function", "(", "midpoint", ")", "is_valid_mid", "=", "hzl", ".", "is_finite", "(", "val_mid", ")", "still_active", "=", "active", "&", "is_valid_mid", "new_failed", "=", "active", "&", "~", "is_valid_mid", "next_inteval", "=", "search_interval", ".", "_replace", "(", "failed", "=", "search_interval", ".", "failed", "|", "new_failed", ",", "func_evals", "=", "search_interval", ".", "func_evals", "+", "1", ")", "def", "_apply_update", "(", ")", ":", "update_result", "=", "hzl", ".", "update", "(", "value_and_gradients_function", ",", "next_inteval", ".", "left", ",", "next_inteval", ".", "right", ",", "val_mid", ",", "f_lim", ",", "active", "=", "still_active", ")", "return", "HagerZhangLineSearchResult", "(", "converged", "=", "next_inteval", ".", "converged", ",", "failed", "=", "next_inteval", ".", "failed", "|", "update_result", ".", "failed", ",", "iterations", "=", "next_inteval", ".", "iterations", "+", "update_result", ".", "iteration", ",", "func_evals", "=", "next_inteval", ".", "func_evals", "+", "update_result", ".", "num_evals", ",", "left", "=", "update_result", ".", "left", ",", "right", "=", "update_result", ".", "right", ")", "return", "prefer_static", ".", "cond", "(", "tf", ".", "reduce_any", "(", "input_tensor", "=", "still_active", ")", ",", "_apply_update", ",", "lambda", ":", "next_inteval", ")"], "docstring": "Performs bisection and updates the interval.", "docstring_tokens": ["Performs", "bisection", "and", "updates", "the", "interval", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py#L545-L576", "partition": "test", "index": 675, "time": "2018-09-27 03:52:46"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", "func_name": "_fix_step_size", "original_string": "def _fix_step_size(value_and_gradients_function,\n                   val_c_input,\n                   active,\n                   step_size_shrink_param):\n  \"\"\"Shrinks the input step size until the value and grad become finite.\"\"\"\n  # The maximum iterations permitted are determined as the number of halvings\n  # it takes to reduce 1 to 0 in the given dtype.\n  iter_max = np.ceil(-np.log2(_machine_eps(val_c_input.x.dtype)))\n\n  def _cond(i, val_c, to_fix):\n    del val_c  # Unused.\n    return (i < iter_max) & tf.reduce_any(input_tensor=to_fix)\n\n  def _body(i, val_c, to_fix):\n    next_c = tf.where(to_fix, val_c.x * step_size_shrink_param, val_c.x)\n    next_val_c = value_and_gradients_function(next_c)\n    still_to_fix = to_fix & ~hzl.is_finite(next_val_c)\n    return (i + 1, next_val_c, still_to_fix)\n\n  to_fix = active & ~hzl.is_finite(val_c_input)\n  return tf.while_loop(\n      cond=_cond, body=_body, loop_vars=(0, val_c_input, to_fix))", "language": "python", "code": "def _fix_step_size(value_and_gradients_function,\n                   val_c_input,\n                   active,\n                   step_size_shrink_param):\n  \"\"\"Shrinks the input step size until the value and grad become finite.\"\"\"\n  # The maximum iterations permitted are determined as the number of halvings\n  # it takes to reduce 1 to 0 in the given dtype.\n  iter_max = np.ceil(-np.log2(_machine_eps(val_c_input.x.dtype)))\n\n  def _cond(i, val_c, to_fix):\n    del val_c  # Unused.\n    return (i < iter_max) & tf.reduce_any(input_tensor=to_fix)\n\n  def _body(i, val_c, to_fix):\n    next_c = tf.where(to_fix, val_c.x * step_size_shrink_param, val_c.x)\n    next_val_c = value_and_gradients_function(next_c)\n    still_to_fix = to_fix & ~hzl.is_finite(next_val_c)\n    return (i + 1, next_val_c, still_to_fix)\n\n  to_fix = active & ~hzl.is_finite(val_c_input)\n  return tf.while_loop(\n      cond=_cond, body=_body, loop_vars=(0, val_c_input, to_fix))", "code_tokens": ["def", "_fix_step_size", "(", "value_and_gradients_function", ",", "val_c_input", ",", "active", ",", "step_size_shrink_param", ")", ":", "# The maximum iterations permitted are determined as the number of halvings", "# it takes to reduce 1 to 0 in the given dtype.", "iter_max", "=", "np", ".", "ceil", "(", "-", "np", ".", "log2", "(", "_machine_eps", "(", "val_c_input", ".", "x", ".", "dtype", ")", ")", ")", "def", "_cond", "(", "i", ",", "val_c", ",", "to_fix", ")", ":", "del", "val_c", "# Unused.", "return", "(", "i", "<", "iter_max", ")", "&", "tf", ".", "reduce_any", "(", "input_tensor", "=", "to_fix", ")", "def", "_body", "(", "i", ",", "val_c", ",", "to_fix", ")", ":", "next_c", "=", "tf", ".", "where", "(", "to_fix", ",", "val_c", ".", "x", "*", "step_size_shrink_param", ",", "val_c", ".", "x", ")", "next_val_c", "=", "value_and_gradients_function", "(", "next_c", ")", "still_to_fix", "=", "to_fix", "&", "~", "hzl", ".", "is_finite", "(", "next_val_c", ")", "return", "(", "i", "+", "1", ",", "next_val_c", ",", "still_to_fix", ")", "to_fix", "=", "active", "&", "~", "hzl", ".", "is_finite", "(", "val_c_input", ")", "return", "tf", ".", "while_loop", "(", "cond", "=", "_cond", ",", "body", "=", "_body", ",", "loop_vars", "=", "(", "0", ",", "val_c_input", ",", "to_fix", ")", ")"], "docstring": "Shrinks the input step size until the value and grad become finite.", "docstring_tokens": ["Shrinks", "the", "input", "step", "size", "until", "the", "value", "and", "grad", "become", "finite", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py#L299-L320", "partition": "test", "index": 672, "time": "2018-09-27 03:52:46"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", "func_name": "_machine_eps", "original_string": "def _machine_eps(dtype):\n  \"\"\"Returns the machine epsilon for the supplied dtype.\"\"\"\n  if isinstance(dtype, tf.DType):\n    dtype = dtype.as_numpy_dtype()\n  return np.finfo(dtype).eps", "language": "python", "code": "def _machine_eps(dtype):\n  \"\"\"Returns the machine epsilon for the supplied dtype.\"\"\"\n  if isinstance(dtype, tf.DType):\n    dtype = dtype.as_numpy_dtype()\n  return np.finfo(dtype).eps", "code_tokens": ["def", "_machine_eps", "(", "dtype", ")", ":", "if", "isinstance", "(", "dtype", ",", "tf", ".", "DType", ")", ":", "dtype", "=", "dtype", ".", "as_numpy_dtype", "(", ")", "return", "np", ".", "finfo", "(", "dtype", ")", ".", "eps"], "docstring": "Returns the machine epsilon for the supplied dtype.", "docstring_tokens": ["Returns", "the", "machine", "epsilon", "for", "the", "supplied", "dtype", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py#L45-L49", "partition": "test", "index": 670, "time": "2018-09-27 03:52:46"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/empirical.py", "func_name": "_broadcast_event_and_samples", "original_string": "def _broadcast_event_and_samples(event, samples, event_ndims):\n  \"\"\"Broadcasts the event or samples.\"\"\"\n  # This is the shape of self.samples, without the samples axis, i.e. the shape\n  # of the result of a call to dist.sample(). This way we can broadcast it with\n  # event to get a properly-sized event, then add the singleton dim back at\n  # -event_ndims - 1.\n  samples_shape = tf.concat(\n      [tf.shape(input=samples)[:-event_ndims - 1],\n       tf.shape(input=samples)[tf.rank(samples) - event_ndims:]],\n      axis=0)\n  event *= tf.ones(samples_shape, dtype=event.dtype)\n  event = tf.expand_dims(event, axis=-event_ndims - 1)\n  samples *= tf.ones_like(event, dtype=samples.dtype)\n\n  return event, samples", "language": "python", "code": "def _broadcast_event_and_samples(event, samples, event_ndims):\n  \"\"\"Broadcasts the event or samples.\"\"\"\n  # This is the shape of self.samples, without the samples axis, i.e. the shape\n  # of the result of a call to dist.sample(). This way we can broadcast it with\n  # event to get a properly-sized event, then add the singleton dim back at\n  # -event_ndims - 1.\n  samples_shape = tf.concat(\n      [tf.shape(input=samples)[:-event_ndims - 1],\n       tf.shape(input=samples)[tf.rank(samples) - event_ndims:]],\n      axis=0)\n  event *= tf.ones(samples_shape, dtype=event.dtype)\n  event = tf.expand_dims(event, axis=-event_ndims - 1)\n  samples *= tf.ones_like(event, dtype=samples.dtype)\n\n  return event, samples", "code_tokens": ["def", "_broadcast_event_and_samples", "(", "event", ",", "samples", ",", "event_ndims", ")", ":", "# This is the shape of self.samples, without the samples axis, i.e. the shape", "# of the result of a call to dist.sample(). This way we can broadcast it with", "# event to get a properly-sized event, then add the singleton dim back at", "# -event_ndims - 1.", "samples_shape", "=", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "input", "=", "samples", ")", "[", ":", "-", "event_ndims", "-", "1", "]", ",", "tf", ".", "shape", "(", "input", "=", "samples", ")", "[", "tf", ".", "rank", "(", "samples", ")", "-", "event_ndims", ":", "]", "]", ",", "axis", "=", "0", ")", "event", "*=", "tf", ".", "ones", "(", "samples_shape", ",", "dtype", "=", "event", ".", "dtype", ")", "event", "=", "tf", ".", "expand_dims", "(", "event", ",", "axis", "=", "-", "event_ndims", "-", "1", ")", "samples", "*=", "tf", ".", "ones_like", "(", "event", ",", "dtype", "=", "samples", ".", "dtype", ")", "return", "event", ",", "samples"], "docstring": "Broadcasts the event or samples.", "docstring_tokens": ["Broadcasts", "the", "event", "or", "samples", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/empirical.py#L35-L49", "partition": "test", "index": 1060, "time": "2018-09-29 09:36:52"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/util.py", "func_name": "smart_for_loop", "original_string": "def smart_for_loop(loop_num_iter, body_fn, initial_loop_vars,\n                   parallel_iterations=10, name=None):\n  \"\"\"Construct a for loop, preferring a python loop if `n` is staticaly known.\n\n  Given `loop_num_iter` and `body_fn`, return an op corresponding to executing\n  `body_fn` `loop_num_iter` times, feeding previous outputs of `body_fn` into\n  the next iteration.\n\n  If `loop_num_iter` is statically known, the op is constructed via python for\n  loop, and otherwise a `tf.while_loop` is used.\n\n  Args:\n    loop_num_iter: `Integer` `Tensor` representing the number of loop\n      iterations.\n    body_fn: Callable to be executed `loop_num_iter` times.\n    initial_loop_vars: Listlike object of `Tensors` to be passed in to\n      `body_fn`'s first execution.\n    parallel_iterations: The number of iterations allowed to run in parallel.\n      It must be a positive integer. See `tf.while_loop` for more details.\n      Default value: `10`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"smart_for_loop\").\n  Returns:\n    result: `Tensor` representing applying `body_fn` iteratively `n` times.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'smart_for_loop',\n                               [loop_num_iter, initial_loop_vars]):\n    loop_num_iter_ = tf.get_static_value(loop_num_iter)\n    if (loop_num_iter_ is None or tf.executing_eagerly() or\n        control_flow_util.GraphOrParentsInXlaContext(\n            tf.compat.v1.get_default_graph())):\n      # Cast to int32 to run the comparison against i in host memory,\n      # where while/LoopCond needs it.\n      loop_num_iter = tf.cast(loop_num_iter, dtype=tf.int32)\n      return tf.while_loop(\n          cond=lambda i, *args: i < loop_num_iter,\n          body=lambda i, *args: [i + 1] + list(body_fn(*args)),\n          loop_vars=[np.int32(0)] + initial_loop_vars,\n          parallel_iterations=parallel_iterations\n      )[1:]\n    result = initial_loop_vars\n    for _ in range(loop_num_iter_):\n      result = body_fn(*result)\n    return result", "language": "python", "code": "def smart_for_loop(loop_num_iter, body_fn, initial_loop_vars,\n                   parallel_iterations=10, name=None):\n  \"\"\"Construct a for loop, preferring a python loop if `n` is staticaly known.\n\n  Given `loop_num_iter` and `body_fn`, return an op corresponding to executing\n  `body_fn` `loop_num_iter` times, feeding previous outputs of `body_fn` into\n  the next iteration.\n\n  If `loop_num_iter` is statically known, the op is constructed via python for\n  loop, and otherwise a `tf.while_loop` is used.\n\n  Args:\n    loop_num_iter: `Integer` `Tensor` representing the number of loop\n      iterations.\n    body_fn: Callable to be executed `loop_num_iter` times.\n    initial_loop_vars: Listlike object of `Tensors` to be passed in to\n      `body_fn`'s first execution.\n    parallel_iterations: The number of iterations allowed to run in parallel.\n      It must be a positive integer. See `tf.while_loop` for more details.\n      Default value: `10`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"smart_for_loop\").\n  Returns:\n    result: `Tensor` representing applying `body_fn` iteratively `n` times.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'smart_for_loop',\n                               [loop_num_iter, initial_loop_vars]):\n    loop_num_iter_ = tf.get_static_value(loop_num_iter)\n    if (loop_num_iter_ is None or tf.executing_eagerly() or\n        control_flow_util.GraphOrParentsInXlaContext(\n            tf.compat.v1.get_default_graph())):\n      # Cast to int32 to run the comparison against i in host memory,\n      # where while/LoopCond needs it.\n      loop_num_iter = tf.cast(loop_num_iter, dtype=tf.int32)\n      return tf.while_loop(\n          cond=lambda i, *args: i < loop_num_iter,\n          body=lambda i, *args: [i + 1] + list(body_fn(*args)),\n          loop_vars=[np.int32(0)] + initial_loop_vars,\n          parallel_iterations=parallel_iterations\n      )[1:]\n    result = initial_loop_vars\n    for _ in range(loop_num_iter_):\n      result = body_fn(*result)\n    return result", "code_tokens": ["def", "smart_for_loop", "(", "loop_num_iter", ",", "body_fn", ",", "initial_loop_vars", ",", "parallel_iterations", "=", "10", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'smart_for_loop'", ",", "[", "loop_num_iter", ",", "initial_loop_vars", "]", ")", ":", "loop_num_iter_", "=", "tf", ".", "get_static_value", "(", "loop_num_iter", ")", "if", "(", "loop_num_iter_", "is", "None", "or", "tf", ".", "executing_eagerly", "(", ")", "or", "control_flow_util", ".", "GraphOrParentsInXlaContext", "(", "tf", ".", "compat", ".", "v1", ".", "get_default_graph", "(", ")", ")", ")", ":", "# Cast to int32 to run the comparison against i in host memory,", "# where while/LoopCond needs it.", "loop_num_iter", "=", "tf", ".", "cast", "(", "loop_num_iter", ",", "dtype", "=", "tf", ".", "int32", ")", "return", "tf", ".", "while_loop", "(", "cond", "=", "lambda", "i", ",", "*", "args", ":", "i", "<", "loop_num_iter", ",", "body", "=", "lambda", "i", ",", "*", "args", ":", "[", "i", "+", "1", "]", "+", "list", "(", "body_fn", "(", "*", "args", ")", ")", ",", "loop_vars", "=", "[", "np", ".", "int32", "(", "0", ")", "]", "+", "initial_loop_vars", ",", "parallel_iterations", "=", "parallel_iterations", ")", "[", "1", ":", "]", "result", "=", "initial_loop_vars", "for", "_", "in", "range", "(", "loop_num_iter_", ")", ":", "result", "=", "body_fn", "(", "*", "result", ")", "return", "result"], "docstring": "Construct a for loop, preferring a python loop if `n` is staticaly known.\n\n  Given `loop_num_iter` and `body_fn`, return an op corresponding to executing\n  `body_fn` `loop_num_iter` times, feeding previous outputs of `body_fn` into\n  the next iteration.\n\n  If `loop_num_iter` is statically known, the op is constructed via python for\n  loop, and otherwise a `tf.while_loop` is used.\n\n  Args:\n    loop_num_iter: `Integer` `Tensor` representing the number of loop\n      iterations.\n    body_fn: Callable to be executed `loop_num_iter` times.\n    initial_loop_vars: Listlike object of `Tensors` to be passed in to\n      `body_fn`'s first execution.\n    parallel_iterations: The number of iterations allowed to run in parallel.\n      It must be a positive integer. See `tf.while_loop` for more details.\n      Default value: `10`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"smart_for_loop\").\n  Returns:\n    result: `Tensor` representing applying `body_fn` iteratively `n` times.", "docstring_tokens": ["Construct", "a", "for", "loop", "preferring", "a", "python", "loop", "if", "n", "is", "staticaly", "known", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/util.py#L247-L290", "partition": "test", "index": 970, "time": "2018-09-30 14:22:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "_is_known_unsigned_by_dtype", "original_string": "def _is_known_unsigned_by_dtype(dt):\n  \"\"\"Helper returning True if dtype is known to be unsigned.\"\"\"\n  return {\n      tf.bool: True,\n      tf.uint8: True,\n      tf.uint16: True,\n  }.get(dt.base_dtype, False)", "language": "python", "code": "def _is_known_unsigned_by_dtype(dt):\n  \"\"\"Helper returning True if dtype is known to be unsigned.\"\"\"\n  return {\n      tf.bool: True,\n      tf.uint8: True,\n      tf.uint16: True,\n  }.get(dt.base_dtype, False)", "code_tokens": ["def", "_is_known_unsigned_by_dtype", "(", "dt", ")", ":", "return", "{", "tf", ".", "bool", ":", "True", ",", "tf", ".", "uint8", ":", "True", ",", "tf", ".", "uint16", ":", "True", ",", "}", ".", "get", "(", "dt", ".", "base_dtype", ",", "False", ")"], "docstring": "Helper returning True if dtype is known to be unsigned.", "docstring_tokens": ["Helper", "returning", "True", "if", "dtype", "is", "known", "to", "be", "unsigned", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L848-L854", "partition": "test", "index": 914, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "maybe_get_static_value", "original_string": "def maybe_get_static_value(x, dtype=None):\n  \"\"\"Helper which tries to return a static value.\n\n  Given `x`, extract it's value statically, optionally casting to a specific\n  dtype. If this is not possible, None is returned.\n\n  Args:\n    x: `Tensor` for which to extract a value statically.\n    dtype: Optional dtype to cast to.\n\n  Returns:\n    Statically inferred value if possible, otherwise None.\n  \"\"\"\n  if x is None:\n    return x\n  try:\n    # This returns an np.ndarray.\n    x_ = tf.get_static_value(x)\n  except TypeError:\n    x_ = x\n  if x_ is None or dtype is None:\n    return x_\n  return np.array(x_, dtype)", "language": "python", "code": "def maybe_get_static_value(x, dtype=None):\n  \"\"\"Helper which tries to return a static value.\n\n  Given `x`, extract it's value statically, optionally casting to a specific\n  dtype. If this is not possible, None is returned.\n\n  Args:\n    x: `Tensor` for which to extract a value statically.\n    dtype: Optional dtype to cast to.\n\n  Returns:\n    Statically inferred value if possible, otherwise None.\n  \"\"\"\n  if x is None:\n    return x\n  try:\n    # This returns an np.ndarray.\n    x_ = tf.get_static_value(x)\n  except TypeError:\n    x_ = x\n  if x_ is None or dtype is None:\n    return x_\n  return np.array(x_, dtype)", "code_tokens": ["def", "maybe_get_static_value", "(", "x", ",", "dtype", "=", "None", ")", ":", "if", "x", "is", "None", ":", "return", "x", "try", ":", "# This returns an np.ndarray.", "x_", "=", "tf", ".", "get_static_value", "(", "x", ")", "except", "TypeError", ":", "x_", "=", "x", "if", "x_", "is", "None", "or", "dtype", "is", "None", ":", "return", "x_", "return", "np", ".", "array", "(", "x_", ",", "dtype", ")"], "docstring": "Helper which tries to return a static value.\n\n  Given `x`, extract it's value statically, optionally casting to a specific\n  dtype. If this is not possible, None is returned.\n\n  Args:\n    x: `Tensor` for which to extract a value statically.\n    dtype: Optional dtype to cast to.\n\n  Returns:\n    Statically inferred value if possible, otherwise None.", "docstring_tokens": ["Helper", "which", "tries", "to", "return", "a", "static", "value", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L740-L762", "partition": "test", "index": 913, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "_is_known_signed_by_dtype", "original_string": "def _is_known_signed_by_dtype(dt):\n  \"\"\"Helper returning True if dtype is known to be signed.\"\"\"\n  return {\n      tf.float16: True,\n      tf.float32: True,\n      tf.float64: True,\n      tf.int8: True,\n      tf.int16: True,\n      tf.int32: True,\n      tf.int64: True,\n  }.get(dt.base_dtype, False)", "language": "python", "code": "def _is_known_signed_by_dtype(dt):\n  \"\"\"Helper returning True if dtype is known to be signed.\"\"\"\n  return {\n      tf.float16: True,\n      tf.float32: True,\n      tf.float64: True,\n      tf.int8: True,\n      tf.int16: True,\n      tf.int32: True,\n      tf.int64: True,\n  }.get(dt.base_dtype, False)", "code_tokens": ["def", "_is_known_signed_by_dtype", "(", "dt", ")", ":", "return", "{", "tf", ".", "float16", ":", "True", ",", "tf", ".", "float32", ":", "True", ",", "tf", ".", "float64", ":", "True", ",", "tf", ".", "int8", ":", "True", ",", "tf", ".", "int16", ":", "True", ",", "tf", ".", "int32", ":", "True", ",", "tf", ".", "int64", ":", "True", ",", "}", ".", "get", "(", "dt", ".", "base_dtype", ",", "False", ")"], "docstring": "Helper returning True if dtype is known to be signed.", "docstring_tokens": ["Helper", "returning", "True", "if", "dtype", "is", "known", "to", "be", "signed", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L857-L867", "partition": "test", "index": 915, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "_largest_integer_by_dtype", "original_string": "def _largest_integer_by_dtype(dt):\n  \"\"\"Helper returning the largest integer exactly representable by dtype.\"\"\"\n  if not _is_known_dtype(dt):\n    raise TypeError(\"Unrecognized dtype: {}\".format(dt.name))\n  if dt.is_floating:\n    return int(2**(np.finfo(dt.as_numpy_dtype).nmant + 1))\n  if dt.is_integer:\n    return np.iinfo(dt.as_numpy_dtype).max\n  if dt.base_dtype == tf.bool:\n    return int(1)\n  # We actually can't land here but keep the case for completeness.\n  raise TypeError(\"Unrecognized dtype: {}\".format(dt.name))", "language": "python", "code": "def _largest_integer_by_dtype(dt):\n  \"\"\"Helper returning the largest integer exactly representable by dtype.\"\"\"\n  if not _is_known_dtype(dt):\n    raise TypeError(\"Unrecognized dtype: {}\".format(dt.name))\n  if dt.is_floating:\n    return int(2**(np.finfo(dt.as_numpy_dtype).nmant + 1))\n  if dt.is_integer:\n    return np.iinfo(dt.as_numpy_dtype).max\n  if dt.base_dtype == tf.bool:\n    return int(1)\n  # We actually can't land here but keep the case for completeness.\n  raise TypeError(\"Unrecognized dtype: {}\".format(dt.name))", "code_tokens": ["def", "_largest_integer_by_dtype", "(", "dt", ")", ":", "if", "not", "_is_known_dtype", "(", "dt", ")", ":", "raise", "TypeError", "(", "\"Unrecognized dtype: {}\"", ".", "format", "(", "dt", ".", "name", ")", ")", "if", "dt", ".", "is_floating", ":", "return", "int", "(", "2", "**", "(", "np", ".", "finfo", "(", "dt", ".", "as_numpy_dtype", ")", ".", "nmant", "+", "1", ")", ")", "if", "dt", ".", "is_integer", ":", "return", "np", ".", "iinfo", "(", "dt", ".", "as_numpy_dtype", ")", ".", "max", "if", "dt", ".", "base_dtype", "==", "tf", ".", "bool", ":", "return", "int", "(", "1", ")", "# We actually can't land here but keep the case for completeness.", "raise", "TypeError", "(", "\"Unrecognized dtype: {}\"", ".", "format", "(", "dt", ".", "name", ")", ")"], "docstring": "Helper returning the largest integer exactly representable by dtype.", "docstring_tokens": ["Helper", "returning", "the", "largest", "integer", "exactly", "representable", "by", "dtype", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L875-L886", "partition": "test", "index": 916, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "rotate_transpose", "original_string": "def rotate_transpose(x, shift, name=\"rotate_transpose\"):\n  \"\"\"Circularly moves dims left or right.\n\n  Effectively identical to:\n\n  ```python\n  numpy.transpose(x, numpy.roll(numpy.arange(len(x.shape)), shift))\n  ```\n\n  When `validate_args=False` additional graph-runtime checks are\n  performed. These checks entail moving data from to GPU to CPU.\n\n  Example:\n\n  ```python\n  x = tf.random_normal([1, 2, 3, 4])  # Tensor of shape [1, 2, 3, 4].\n  rotate_transpose(x, -1).shape == [2, 3, 4, 1]\n  rotate_transpose(x, -2).shape == [3, 4, 1, 2]\n  rotate_transpose(x,  1).shape == [4, 1, 2, 3]\n  rotate_transpose(x,  2).shape == [3, 4, 1, 2]\n  rotate_transpose(x,  7).shape == rotate_transpose(x, 3).shape  # [2, 3, 4, 1]\n  rotate_transpose(x, -7).shape == rotate_transpose(x, -3).shape  # [4, 1, 2, 3]\n  ```\n\n  Args:\n    x: `Tensor`.\n    shift: `Tensor`. Number of dimensions to transpose left (shift<0) or\n      transpose right (shift>0).\n    name: Python `str`. The name to give this op.\n\n  Returns:\n    rotated_x: Input `Tensor` with dimensions circularly rotated by shift.\n\n  Raises:\n    TypeError: if shift is not integer type.\n  \"\"\"\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    shift = tf.convert_to_tensor(value=shift, name=\"shift\")\n    # We do not assign back to preserve constant-ness.\n    assert_util.assert_integer(shift)\n    shift_value_static = tf.get_static_value(shift)\n    ndims = tensorshape_util.rank(x.shape)\n    if ndims is not None and shift_value_static is not None:\n      if ndims < 2:\n        return x\n      shift_value_static = np.sign(shift_value_static) * (\n          abs(shift_value_static) % ndims)\n      if shift_value_static == 0:\n        return x\n      perm = np.roll(np.arange(ndims), shift_value_static)\n      return tf.transpose(a=x, perm=perm)\n    else:\n      # Consider if we always had a positive shift, and some specified\n      # direction.\n      # When shifting left we want the new array:\n      #   last(x, n-shift) + first(x, shift)\n      # and if shifting right then we want:\n      #   last(x, shift) + first(x, n-shift)\n      # Observe that last(a) == slice(a, n) and first(a) == slice(0, a).\n      # Also, we can encode direction and shift as one: direction * shift.\n      # Combining these facts, we have:\n      #   a = cond(shift<0, -shift, n-shift)\n      #   last(x, n-a) + first(x, a) == x[a:n] + x[0:a]\n      # Finally, we transform shift by modulo length so it can be specified\n      # independently from the array upon which it operates (like python).\n      ndims = tf.rank(x)\n      shift = tf.where(\n          tf.less(shift, 0), -shift % ndims,\n          ndims - shift % ndims)\n      first = tf.range(0, shift)\n      last = tf.range(shift, ndims)\n      perm = tf.concat([last, first], 0)\n      return tf.transpose(a=x, perm=perm)", "language": "python", "code": "def rotate_transpose(x, shift, name=\"rotate_transpose\"):\n  \"\"\"Circularly moves dims left or right.\n\n  Effectively identical to:\n\n  ```python\n  numpy.transpose(x, numpy.roll(numpy.arange(len(x.shape)), shift))\n  ```\n\n  When `validate_args=False` additional graph-runtime checks are\n  performed. These checks entail moving data from to GPU to CPU.\n\n  Example:\n\n  ```python\n  x = tf.random_normal([1, 2, 3, 4])  # Tensor of shape [1, 2, 3, 4].\n  rotate_transpose(x, -1).shape == [2, 3, 4, 1]\n  rotate_transpose(x, -2).shape == [3, 4, 1, 2]\n  rotate_transpose(x,  1).shape == [4, 1, 2, 3]\n  rotate_transpose(x,  2).shape == [3, 4, 1, 2]\n  rotate_transpose(x,  7).shape == rotate_transpose(x, 3).shape  # [2, 3, 4, 1]\n  rotate_transpose(x, -7).shape == rotate_transpose(x, -3).shape  # [4, 1, 2, 3]\n  ```\n\n  Args:\n    x: `Tensor`.\n    shift: `Tensor`. Number of dimensions to transpose left (shift<0) or\n      transpose right (shift>0).\n    name: Python `str`. The name to give this op.\n\n  Returns:\n    rotated_x: Input `Tensor` with dimensions circularly rotated by shift.\n\n  Raises:\n    TypeError: if shift is not integer type.\n  \"\"\"\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    shift = tf.convert_to_tensor(value=shift, name=\"shift\")\n    # We do not assign back to preserve constant-ness.\n    assert_util.assert_integer(shift)\n    shift_value_static = tf.get_static_value(shift)\n    ndims = tensorshape_util.rank(x.shape)\n    if ndims is not None and shift_value_static is not None:\n      if ndims < 2:\n        return x\n      shift_value_static = np.sign(shift_value_static) * (\n          abs(shift_value_static) % ndims)\n      if shift_value_static == 0:\n        return x\n      perm = np.roll(np.arange(ndims), shift_value_static)\n      return tf.transpose(a=x, perm=perm)\n    else:\n      # Consider if we always had a positive shift, and some specified\n      # direction.\n      # When shifting left we want the new array:\n      #   last(x, n-shift) + first(x, shift)\n      # and if shifting right then we want:\n      #   last(x, shift) + first(x, n-shift)\n      # Observe that last(a) == slice(a, n) and first(a) == slice(0, a).\n      # Also, we can encode direction and shift as one: direction * shift.\n      # Combining these facts, we have:\n      #   a = cond(shift<0, -shift, n-shift)\n      #   last(x, n-a) + first(x, a) == x[a:n] + x[0:a]\n      # Finally, we transform shift by modulo length so it can be specified\n      # independently from the array upon which it operates (like python).\n      ndims = tf.rank(x)\n      shift = tf.where(\n          tf.less(shift, 0), -shift % ndims,\n          ndims - shift % ndims)\n      first = tf.range(0, shift)\n      last = tf.range(shift, ndims)\n      perm = tf.concat([last, first], 0)\n      return tf.transpose(a=x, perm=perm)", "code_tokens": ["def", "rotate_transpose", "(", "x", ",", "shift", ",", "name", "=", "\"rotate_transpose\"", ")", ":", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "\"x\"", ")", "shift", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "shift", ",", "name", "=", "\"shift\"", ")", "# We do not assign back to preserve constant-ness.", "assert_util", ".", "assert_integer", "(", "shift", ")", "shift_value_static", "=", "tf", ".", "get_static_value", "(", "shift", ")", "ndims", "=", "tensorshape_util", ".", "rank", "(", "x", ".", "shape", ")", "if", "ndims", "is", "not", "None", "and", "shift_value_static", "is", "not", "None", ":", "if", "ndims", "<", "2", ":", "return", "x", "shift_value_static", "=", "np", ".", "sign", "(", "shift_value_static", ")", "*", "(", "abs", "(", "shift_value_static", ")", "%", "ndims", ")", "if", "shift_value_static", "==", "0", ":", "return", "x", "perm", "=", "np", ".", "roll", "(", "np", ".", "arange", "(", "ndims", ")", ",", "shift_value_static", ")", "return", "tf", ".", "transpose", "(", "a", "=", "x", ",", "perm", "=", "perm", ")", "else", ":", "# Consider if we always had a positive shift, and some specified", "# direction.", "# When shifting left we want the new array:", "#   last(x, n-shift) + first(x, shift)", "# and if shifting right then we want:", "#   last(x, shift) + first(x, n-shift)", "# Observe that last(a) == slice(a, n) and first(a) == slice(0, a).", "# Also, we can encode direction and shift as one: direction * shift.", "# Combining these facts, we have:", "#   a = cond(shift<0, -shift, n-shift)", "#   last(x, n-a) + first(x, a) == x[a:n] + x[0:a]", "# Finally, we transform shift by modulo length so it can be specified", "# independently from the array upon which it operates (like python).", "ndims", "=", "tf", ".", "rank", "(", "x", ")", "shift", "=", "tf", ".", "where", "(", "tf", ".", "less", "(", "shift", ",", "0", ")", ",", "-", "shift", "%", "ndims", ",", "ndims", "-", "shift", "%", "ndims", ")", "first", "=", "tf", ".", "range", "(", "0", ",", "shift", ")", "last", "=", "tf", ".", "range", "(", "shift", ",", "ndims", ")", "perm", "=", "tf", ".", "concat", "(", "[", "last", ",", "first", "]", ",", "0", ")", "return", "tf", ".", "transpose", "(", "a", "=", "x", ",", "perm", "=", "perm", ")"], "docstring": "Circularly moves dims left or right.\n\n  Effectively identical to:\n\n  ```python\n  numpy.transpose(x, numpy.roll(numpy.arange(len(x.shape)), shift))\n  ```\n\n  When `validate_args=False` additional graph-runtime checks are\n  performed. These checks entail moving data from to GPU to CPU.\n\n  Example:\n\n  ```python\n  x = tf.random_normal([1, 2, 3, 4])  # Tensor of shape [1, 2, 3, 4].\n  rotate_transpose(x, -1).shape == [2, 3, 4, 1]\n  rotate_transpose(x, -2).shape == [3, 4, 1, 2]\n  rotate_transpose(x,  1).shape == [4, 1, 2, 3]\n  rotate_transpose(x,  2).shape == [3, 4, 1, 2]\n  rotate_transpose(x,  7).shape == rotate_transpose(x, 3).shape  # [2, 3, 4, 1]\n  rotate_transpose(x, -7).shape == rotate_transpose(x, -3).shape  # [4, 1, 2, 3]\n  ```\n\n  Args:\n    x: `Tensor`.\n    shift: `Tensor`. Number of dimensions to transpose left (shift<0) or\n      transpose right (shift>0).\n    name: Python `str`. The name to give this op.\n\n  Returns:\n    rotated_x: Input `Tensor` with dimensions circularly rotated by shift.\n\n  Raises:\n    TypeError: if shift is not integer type.", "docstring_tokens": ["Circularly", "moves", "dims", "left", "or", "right", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L1198-L1271", "partition": "test", "index": 921, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "_is_integer_like_by_dtype", "original_string": "def _is_integer_like_by_dtype(dt):\n  \"\"\"Helper returning True if dtype.is_integer or is `bool`.\"\"\"\n  if not _is_known_dtype(dt):\n    raise TypeError(\"Unrecognized dtype: {}\".format(dt.name))\n  return dt.is_integer or dt.base_dtype == tf.bool", "language": "python", "code": "def _is_integer_like_by_dtype(dt):\n  \"\"\"Helper returning True if dtype.is_integer or is `bool`.\"\"\"\n  if not _is_known_dtype(dt):\n    raise TypeError(\"Unrecognized dtype: {}\".format(dt.name))\n  return dt.is_integer or dt.base_dtype == tf.bool", "code_tokens": ["def", "_is_integer_like_by_dtype", "(", "dt", ")", ":", "if", "not", "_is_known_dtype", "(", "dt", ")", ":", "raise", "TypeError", "(", "\"Unrecognized dtype: {}\"", ".", "format", "(", "dt", ".", "name", ")", ")", "return", "dt", ".", "is_integer", "or", "dt", ".", "base_dtype", "==", "tf", ".", "bool"], "docstring": "Helper returning True if dtype.is_integer or is `bool`.", "docstring_tokens": ["Helper", "returning", "True", "if", "dtype", ".", "is_integer", "or", "is", "bool", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L898-L902", "partition": "test", "index": 918, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "embed_check_categorical_event_shape", "original_string": "def embed_check_categorical_event_shape(\n    categorical_param, name=\"embed_check_categorical_event_shape\"):\n  \"\"\"Embeds checks that categorical distributions don't have too many classes.\n\n  A categorical-type distribution is one which, e.g., returns the class label\n  rather than a one-hot encoding.  E.g., `Categorical(probs)`.\n\n  Since distributions output samples in the same dtype as the parameters, we\n  must ensure that casting doesn't lose precision. That is, the\n  `parameter.dtype` implies a maximum number of classes. However, since shape is\n  `int32` and categorical variables are presumed to be indexes into a `Tensor`,\n  we must also ensure that the number of classes is no larger than the largest\n  possible `int32` index, i.e., `2**31-1`.\n\n  In other words the number of classes, `K`, must satisfy the following\n  condition:\n\n  ```python\n  K <= min(\n      int(2**31 - 1),  # Largest float as an index.\n      {\n          tf.float16: int(2**11),   # Largest int as a float16.\n          tf.float32: int(2**24),\n          tf.float64: int(2**53),\n      }.get(dtype_util.base_dtype(categorical_param.dtype), 0))\n  ```\n\n  Args:\n    categorical_param: Floating-point `Tensor` representing parameters of\n      distribution over categories. The rightmost shape is presumed to be the\n      number of categories.\n    name: A name for this operation (optional).\n\n  Returns:\n    categorical_param: Input `Tensor` with appropriate assertions embedded.\n\n  Raises:\n    TypeError: if `categorical_param` has an unknown `dtype`.\n    ValueError: if we can statically identify `categorical_param` as being too\n      large (for being closed under int32/float casting).\n  \"\"\"\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=categorical_param, name=\"categorical_param\")\n    # The size must not exceed both of:\n    # - The largest possible int32 (since categorical values are presumed to be\n    #   indexes into a Tensor).\n    # - The largest possible integer exactly representable under the given\n    #   floating-point dtype (since we need to cast to/from).\n    #\n    # The chosen floating-point thresholds are 2**(1 + mantissa_bits).\n    # For more details, see:\n    # https://en.wikipedia.org/wiki/Floating-point_arithmetic#Internal_representation\n    x_dtype = dtype_util.base_dtype(x.dtype)\n    max_event_size = (\n        _largest_integer_by_dtype(x_dtype)\n        if dtype_util.is_floating(x_dtype) else 0)\n    if max_event_size is 0:\n      raise TypeError(\"Unable to validate size of unrecognized dtype \"\n                      \"({}).\".format(dtype_util.name(x_dtype)))\n    try:\n      x_shape_static = tensorshape_util.with_rank_at_least(x.shape, 1)\n    except ValueError:\n      raise ValueError(\"A categorical-distribution parameter must have \"\n                       \"at least 1 dimension.\")\n    event_size = tf.compat.dimension_value(x_shape_static[-1])\n    if event_size is not None:\n      if event_size < 2:\n        raise ValueError(\"A categorical-distribution parameter must have at \"\n                         \"least 2 events.\")\n      if event_size > max_event_size:\n        raise ValueError(\"Number of classes exceeds `dtype` precision, i.e., \"\n                         \"{} implies shape ({}) cannot exceed {}.\".format(\n                             dtype_util.name(x_dtype), event_size,\n                             max_event_size))\n      return x\n    else:\n      event_size = tf.shape(input=x, out_type=tf.int64, name=\"x_shape\")[-1]\n      return with_dependencies([\n          assert_util.assert_rank_at_least(\n              x,\n              1,\n              message=(\"A categorical-distribution parameter must have \"\n                       \"at least 1 dimension.\")),\n          assert_util.assert_greater_equal(\n              tf.shape(input=x)[-1],\n              2,\n              message=(\"A categorical-distribution parameter must have at \"\n                       \"least 2 events.\")),\n          assert_util.assert_less_equal(\n              event_size,\n              tf.convert_to_tensor(max_event_size, dtype=tf.int64),\n              message=\"Number of classes exceeds `dtype` precision, \"\n              \"i.e., {} dtype cannot exceed {} shape.\".format(\n                  dtype_util.name(x_dtype), max_event_size)),\n      ], x)", "language": "python", "code": "def embed_check_categorical_event_shape(\n    categorical_param, name=\"embed_check_categorical_event_shape\"):\n  \"\"\"Embeds checks that categorical distributions don't have too many classes.\n\n  A categorical-type distribution is one which, e.g., returns the class label\n  rather than a one-hot encoding.  E.g., `Categorical(probs)`.\n\n  Since distributions output samples in the same dtype as the parameters, we\n  must ensure that casting doesn't lose precision. That is, the\n  `parameter.dtype` implies a maximum number of classes. However, since shape is\n  `int32` and categorical variables are presumed to be indexes into a `Tensor`,\n  we must also ensure that the number of classes is no larger than the largest\n  possible `int32` index, i.e., `2**31-1`.\n\n  In other words the number of classes, `K`, must satisfy the following\n  condition:\n\n  ```python\n  K <= min(\n      int(2**31 - 1),  # Largest float as an index.\n      {\n          tf.float16: int(2**11),   # Largest int as a float16.\n          tf.float32: int(2**24),\n          tf.float64: int(2**53),\n      }.get(dtype_util.base_dtype(categorical_param.dtype), 0))\n  ```\n\n  Args:\n    categorical_param: Floating-point `Tensor` representing parameters of\n      distribution over categories. The rightmost shape is presumed to be the\n      number of categories.\n    name: A name for this operation (optional).\n\n  Returns:\n    categorical_param: Input `Tensor` with appropriate assertions embedded.\n\n  Raises:\n    TypeError: if `categorical_param` has an unknown `dtype`.\n    ValueError: if we can statically identify `categorical_param` as being too\n      large (for being closed under int32/float casting).\n  \"\"\"\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=categorical_param, name=\"categorical_param\")\n    # The size must not exceed both of:\n    # - The largest possible int32 (since categorical values are presumed to be\n    #   indexes into a Tensor).\n    # - The largest possible integer exactly representable under the given\n    #   floating-point dtype (since we need to cast to/from).\n    #\n    # The chosen floating-point thresholds are 2**(1 + mantissa_bits).\n    # For more details, see:\n    # https://en.wikipedia.org/wiki/Floating-point_arithmetic#Internal_representation\n    x_dtype = dtype_util.base_dtype(x.dtype)\n    max_event_size = (\n        _largest_integer_by_dtype(x_dtype)\n        if dtype_util.is_floating(x_dtype) else 0)\n    if max_event_size is 0:\n      raise TypeError(\"Unable to validate size of unrecognized dtype \"\n                      \"({}).\".format(dtype_util.name(x_dtype)))\n    try:\n      x_shape_static = tensorshape_util.with_rank_at_least(x.shape, 1)\n    except ValueError:\n      raise ValueError(\"A categorical-distribution parameter must have \"\n                       \"at least 1 dimension.\")\n    event_size = tf.compat.dimension_value(x_shape_static[-1])\n    if event_size is not None:\n      if event_size < 2:\n        raise ValueError(\"A categorical-distribution parameter must have at \"\n                         \"least 2 events.\")\n      if event_size > max_event_size:\n        raise ValueError(\"Number of classes exceeds `dtype` precision, i.e., \"\n                         \"{} implies shape ({}) cannot exceed {}.\".format(\n                             dtype_util.name(x_dtype), event_size,\n                             max_event_size))\n      return x\n    else:\n      event_size = tf.shape(input=x, out_type=tf.int64, name=\"x_shape\")[-1]\n      return with_dependencies([\n          assert_util.assert_rank_at_least(\n              x,\n              1,\n              message=(\"A categorical-distribution parameter must have \"\n                       \"at least 1 dimension.\")),\n          assert_util.assert_greater_equal(\n              tf.shape(input=x)[-1],\n              2,\n              message=(\"A categorical-distribution parameter must have at \"\n                       \"least 2 events.\")),\n          assert_util.assert_less_equal(\n              event_size,\n              tf.convert_to_tensor(max_event_size, dtype=tf.int64),\n              message=\"Number of classes exceeds `dtype` precision, \"\n              \"i.e., {} dtype cannot exceed {} shape.\".format(\n                  dtype_util.name(x_dtype), max_event_size)),\n      ], x)", "code_tokens": ["def", "embed_check_categorical_event_shape", "(", "categorical_param", ",", "name", "=", "\"embed_check_categorical_event_shape\"", ")", ":", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "categorical_param", ",", "name", "=", "\"categorical_param\"", ")", "# The size must not exceed both of:", "# - The largest possible int32 (since categorical values are presumed to be", "#   indexes into a Tensor).", "# - The largest possible integer exactly representable under the given", "#   floating-point dtype (since we need to cast to/from).", "#", "# The chosen floating-point thresholds are 2**(1 + mantissa_bits).", "# For more details, see:", "# https://en.wikipedia.org/wiki/Floating-point_arithmetic#Internal_representation", "x_dtype", "=", "dtype_util", ".", "base_dtype", "(", "x", ".", "dtype", ")", "max_event_size", "=", "(", "_largest_integer_by_dtype", "(", "x_dtype", ")", "if", "dtype_util", ".", "is_floating", "(", "x_dtype", ")", "else", "0", ")", "if", "max_event_size", "is", "0", ":", "raise", "TypeError", "(", "\"Unable to validate size of unrecognized dtype \"", "\"({}).\"", ".", "format", "(", "dtype_util", ".", "name", "(", "x_dtype", ")", ")", ")", "try", ":", "x_shape_static", "=", "tensorshape_util", ".", "with_rank_at_least", "(", "x", ".", "shape", ",", "1", ")", "except", "ValueError", ":", "raise", "ValueError", "(", "\"A categorical-distribution parameter must have \"", "\"at least 1 dimension.\"", ")", "event_size", "=", "tf", ".", "compat", ".", "dimension_value", "(", "x_shape_static", "[", "-", "1", "]", ")", "if", "event_size", "is", "not", "None", ":", "if", "event_size", "<", "2", ":", "raise", "ValueError", "(", "\"A categorical-distribution parameter must have at \"", "\"least 2 events.\"", ")", "if", "event_size", ">", "max_event_size", ":", "raise", "ValueError", "(", "\"Number of classes exceeds `dtype` precision, i.e., \"", "\"{} implies shape ({}) cannot exceed {}.\"", ".", "format", "(", "dtype_util", ".", "name", "(", "x_dtype", ")", ",", "event_size", ",", "max_event_size", ")", ")", "return", "x", "else", ":", "event_size", "=", "tf", ".", "shape", "(", "input", "=", "x", ",", "out_type", "=", "tf", ".", "int64", ",", "name", "=", "\"x_shape\"", ")", "[", "-", "1", "]", "return", "with_dependencies", "(", "[", "assert_util", ".", "assert_rank_at_least", "(", "x", ",", "1", ",", "message", "=", "(", "\"A categorical-distribution parameter must have \"", "\"at least 1 dimension.\"", ")", ")", ",", "assert_util", ".", "assert_greater_equal", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", "[", "-", "1", "]", ",", "2", ",", "message", "=", "(", "\"A categorical-distribution parameter must have at \"", "\"least 2 events.\"", ")", ")", ",", "assert_util", ".", "assert_less_equal", "(", "event_size", ",", "tf", ".", "convert_to_tensor", "(", "max_event_size", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "message", "=", "\"Number of classes exceeds `dtype` precision, \"", "\"i.e., {} dtype cannot exceed {} shape.\"", ".", "format", "(", "dtype_util", ".", "name", "(", "x_dtype", ")", ",", "max_event_size", ")", ")", ",", "]", ",", "x", ")"], "docstring": "Embeds checks that categorical distributions don't have too many classes.\n\n  A categorical-type distribution is one which, e.g., returns the class label\n  rather than a one-hot encoding.  E.g., `Categorical(probs)`.\n\n  Since distributions output samples in the same dtype as the parameters, we\n  must ensure that casting doesn't lose precision. That is, the\n  `parameter.dtype` implies a maximum number of classes. However, since shape is\n  `int32` and categorical variables are presumed to be indexes into a `Tensor`,\n  we must also ensure that the number of classes is no larger than the largest\n  possible `int32` index, i.e., `2**31-1`.\n\n  In other words the number of classes, `K`, must satisfy the following\n  condition:\n\n  ```python\n  K <= min(\n      int(2**31 - 1),  # Largest float as an index.\n      {\n          tf.float16: int(2**11),   # Largest int as a float16.\n          tf.float32: int(2**24),\n          tf.float64: int(2**53),\n      }.get(dtype_util.base_dtype(categorical_param.dtype), 0))\n  ```\n\n  Args:\n    categorical_param: Floating-point `Tensor` representing parameters of\n      distribution over categories. The rightmost shape is presumed to be the\n      number of categories.\n    name: A name for this operation (optional).\n\n  Returns:\n    categorical_param: Input `Tensor` with appropriate assertions embedded.\n\n  Raises:\n    TypeError: if `categorical_param` has an unknown `dtype`.\n    ValueError: if we can statically identify `categorical_param` as being too\n      large (for being closed under int32/float casting).", "docstring_tokens": ["Embeds", "checks", "that", "categorical", "distributions", "don", "t", "have", "too", "many", "classes", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L905-L999", "partition": "test", "index": 919, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "log_combinations", "original_string": "def log_combinations(n, counts, name=\"log_combinations\"):\n  \"\"\"Multinomial coefficient.\n\n  Given `n` and `counts`, where `counts` has last dimension `k`, we compute\n  the multinomial coefficient as:\n\n  ```n! / sum_i n_i!```\n\n  where `i` runs over all `k` classes.\n\n  Args:\n    n: Floating-point `Tensor` broadcastable with `counts`. This represents `n`\n      outcomes.\n    counts: Floating-point `Tensor` broadcastable with `n`. This represents\n      counts in `k` classes, where `k` is the last dimension of the tensor.\n    name: A name for this operation (optional).\n\n  Returns:\n    `Tensor` representing the multinomial coefficient between `n` and `counts`.\n  \"\"\"\n  # First a bit about the number of ways counts could have come in:\n  # E.g. if counts = [1, 2], then this is 3 choose 2.\n  # In general, this is (sum counts)! / sum(counts!)\n  # The sum should be along the last dimension of counts. This is the\n  # \"distribution\" dimension. Here n a priori represents the sum of counts.\n  with tf.name_scope(name):\n    n = tf.convert_to_tensor(value=n, name=\"n\")\n    counts = tf.convert_to_tensor(value=counts, name=\"counts\")\n    total_permutations = tf.math.lgamma(n + 1)\n    counts_factorial = tf.math.lgamma(counts + 1)\n    redundant_permutations = tf.reduce_sum(\n        input_tensor=counts_factorial, axis=[-1])\n    return total_permutations - redundant_permutations", "language": "python", "code": "def log_combinations(n, counts, name=\"log_combinations\"):\n  \"\"\"Multinomial coefficient.\n\n  Given `n` and `counts`, where `counts` has last dimension `k`, we compute\n  the multinomial coefficient as:\n\n  ```n! / sum_i n_i!```\n\n  where `i` runs over all `k` classes.\n\n  Args:\n    n: Floating-point `Tensor` broadcastable with `counts`. This represents `n`\n      outcomes.\n    counts: Floating-point `Tensor` broadcastable with `n`. This represents\n      counts in `k` classes, where `k` is the last dimension of the tensor.\n    name: A name for this operation (optional).\n\n  Returns:\n    `Tensor` representing the multinomial coefficient between `n` and `counts`.\n  \"\"\"\n  # First a bit about the number of ways counts could have come in:\n  # E.g. if counts = [1, 2], then this is 3 choose 2.\n  # In general, this is (sum counts)! / sum(counts!)\n  # The sum should be along the last dimension of counts. This is the\n  # \"distribution\" dimension. Here n a priori represents the sum of counts.\n  with tf.name_scope(name):\n    n = tf.convert_to_tensor(value=n, name=\"n\")\n    counts = tf.convert_to_tensor(value=counts, name=\"counts\")\n    total_permutations = tf.math.lgamma(n + 1)\n    counts_factorial = tf.math.lgamma(counts + 1)\n    redundant_permutations = tf.reduce_sum(\n        input_tensor=counts_factorial, axis=[-1])\n    return total_permutations - redundant_permutations", "code_tokens": ["def", "log_combinations", "(", "n", ",", "counts", ",", "name", "=", "\"log_combinations\"", ")", ":", "# First a bit about the number of ways counts could have come in:", "# E.g. if counts = [1, 2], then this is 3 choose 2.", "# In general, this is (sum counts)! / sum(counts!)", "# The sum should be along the last dimension of counts. This is the", "# \"distribution\" dimension. Here n a priori represents the sum of counts.", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "n", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "n", ",", "name", "=", "\"n\"", ")", "counts", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "counts", ",", "name", "=", "\"counts\"", ")", "total_permutations", "=", "tf", ".", "math", ".", "lgamma", "(", "n", "+", "1", ")", "counts_factorial", "=", "tf", ".", "math", ".", "lgamma", "(", "counts", "+", "1", ")", "redundant_permutations", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "counts_factorial", ",", "axis", "=", "[", "-", "1", "]", ")", "return", "total_permutations", "-", "redundant_permutations"], "docstring": "Multinomial coefficient.\n\n  Given `n` and `counts`, where `counts` has last dimension `k`, we compute\n  the multinomial coefficient as:\n\n  ```n! / sum_i n_i!```\n\n  where `i` runs over all `k` classes.\n\n  Args:\n    n: Floating-point `Tensor` broadcastable with `counts`. This represents `n`\n      outcomes.\n    counts: Floating-point `Tensor` broadcastable with `n`. This represents\n      counts in `k` classes, where `k` is the last dimension of the tensor.\n    name: A name for this operation (optional).\n\n  Returns:\n    `Tensor` representing the multinomial coefficient between `n` and `counts`.", "docstring_tokens": ["Multinomial", "coefficient", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L1101-L1133", "partition": "test", "index": 920, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "pick_vector", "original_string": "def pick_vector(cond, true_vector, false_vector, name=\"pick_vector\"):\n  \"\"\"Picks possibly different length row `Tensor`s based on condition.\n\n  Value `Tensor`s should have exactly one dimension.\n\n  If `cond` is a python Boolean or `tf.constant` then either `true_vector` or\n  `false_vector` is immediately returned. I.e., no graph nodes are created and\n  no validation happens.\n\n  Args:\n    cond: `Tensor`. Must have `dtype=tf.bool` and be scalar.\n    true_vector: `Tensor` of one dimension. Returned when cond is `True`.\n    false_vector: `Tensor` of one dimension. Returned when cond is `False`.\n    name: Python `str`. The name to give this op.\n  Example:  ```python pick_vector(tf.less(0, 5), tf.range(10, 12), tf.range(15,\n    18))  # [10, 11] pick_vector(tf.less(5, 0), tf.range(10, 12), tf.range(15,\n    18))  # [15, 16, 17] ```\n\n  Returns:\n    true_or_false_vector: `Tensor`.\n\n  Raises:\n    TypeError: if `cond.dtype != tf.bool`\n    TypeError: if `cond` is not a constant and\n      `true_vector.dtype != false_vector.dtype`\n  \"\"\"\n  with tf.name_scope(name):\n    cond = tf.convert_to_tensor(\n        value=cond, dtype_hint=tf.bool, name=\"cond\")\n    if cond.dtype != tf.bool:\n      raise TypeError(\n          \"{}.dtype={} which is not {}\".format(cond, cond.dtype, tf.bool))\n\n    true_vector = tf.convert_to_tensor(value=true_vector, name=\"true_vector\")\n    false_vector = tf.convert_to_tensor(value=false_vector, name=\"false_vector\")\n    if true_vector.dtype != false_vector.dtype:\n      raise TypeError(\n          \"{}.dtype={} does not match {}.dtype={}\".format(\n              true_vector, true_vector.dtype, false_vector, false_vector.dtype))\n\n    cond_value_static = tf.get_static_value(cond)\n    if cond_value_static is not None:\n      return true_vector if cond_value_static else false_vector\n    n = tf.shape(input=true_vector)[0]\n    return tf.slice(\n        tf.concat([true_vector, false_vector], 0), [tf.where(cond, 0, n)],\n        [tf.where(cond, n, -1)])", "language": "python", "code": "def pick_vector(cond, true_vector, false_vector, name=\"pick_vector\"):\n  \"\"\"Picks possibly different length row `Tensor`s based on condition.\n\n  Value `Tensor`s should have exactly one dimension.\n\n  If `cond` is a python Boolean or `tf.constant` then either `true_vector` or\n  `false_vector` is immediately returned. I.e., no graph nodes are created and\n  no validation happens.\n\n  Args:\n    cond: `Tensor`. Must have `dtype=tf.bool` and be scalar.\n    true_vector: `Tensor` of one dimension. Returned when cond is `True`.\n    false_vector: `Tensor` of one dimension. Returned when cond is `False`.\n    name: Python `str`. The name to give this op.\n  Example:  ```python pick_vector(tf.less(0, 5), tf.range(10, 12), tf.range(15,\n    18))  # [10, 11] pick_vector(tf.less(5, 0), tf.range(10, 12), tf.range(15,\n    18))  # [15, 16, 17] ```\n\n  Returns:\n    true_or_false_vector: `Tensor`.\n\n  Raises:\n    TypeError: if `cond.dtype != tf.bool`\n    TypeError: if `cond` is not a constant and\n      `true_vector.dtype != false_vector.dtype`\n  \"\"\"\n  with tf.name_scope(name):\n    cond = tf.convert_to_tensor(\n        value=cond, dtype_hint=tf.bool, name=\"cond\")\n    if cond.dtype != tf.bool:\n      raise TypeError(\n          \"{}.dtype={} which is not {}\".format(cond, cond.dtype, tf.bool))\n\n    true_vector = tf.convert_to_tensor(value=true_vector, name=\"true_vector\")\n    false_vector = tf.convert_to_tensor(value=false_vector, name=\"false_vector\")\n    if true_vector.dtype != false_vector.dtype:\n      raise TypeError(\n          \"{}.dtype={} does not match {}.dtype={}\".format(\n              true_vector, true_vector.dtype, false_vector, false_vector.dtype))\n\n    cond_value_static = tf.get_static_value(cond)\n    if cond_value_static is not None:\n      return true_vector if cond_value_static else false_vector\n    n = tf.shape(input=true_vector)[0]\n    return tf.slice(\n        tf.concat([true_vector, false_vector], 0), [tf.where(cond, 0, n)],\n        [tf.where(cond, n, -1)])", "code_tokens": ["def", "pick_vector", "(", "cond", ",", "true_vector", ",", "false_vector", ",", "name", "=", "\"pick_vector\"", ")", ":", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "cond", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "cond", ",", "dtype_hint", "=", "tf", ".", "bool", ",", "name", "=", "\"cond\"", ")", "if", "cond", ".", "dtype", "!=", "tf", ".", "bool", ":", "raise", "TypeError", "(", "\"{}.dtype={} which is not {}\"", ".", "format", "(", "cond", ",", "cond", ".", "dtype", ",", "tf", ".", "bool", ")", ")", "true_vector", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "true_vector", ",", "name", "=", "\"true_vector\"", ")", "false_vector", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "false_vector", ",", "name", "=", "\"false_vector\"", ")", "if", "true_vector", ".", "dtype", "!=", "false_vector", ".", "dtype", ":", "raise", "TypeError", "(", "\"{}.dtype={} does not match {}.dtype={}\"", ".", "format", "(", "true_vector", ",", "true_vector", ".", "dtype", ",", "false_vector", ",", "false_vector", ".", "dtype", ")", ")", "cond_value_static", "=", "tf", ".", "get_static_value", "(", "cond", ")", "if", "cond_value_static", "is", "not", "None", ":", "return", "true_vector", "if", "cond_value_static", "else", "false_vector", "n", "=", "tf", ".", "shape", "(", "input", "=", "true_vector", ")", "[", "0", "]", "return", "tf", ".", "slice", "(", "tf", ".", "concat", "(", "[", "true_vector", ",", "false_vector", "]", ",", "0", ")", ",", "[", "tf", ".", "where", "(", "cond", ",", "0", ",", "n", ")", "]", ",", "[", "tf", ".", "where", "(", "cond", ",", "n", ",", "-", "1", ")", "]", ")"], "docstring": "Picks possibly different length row `Tensor`s based on condition.\n\n  Value `Tensor`s should have exactly one dimension.\n\n  If `cond` is a python Boolean or `tf.constant` then either `true_vector` or\n  `false_vector` is immediately returned. I.e., no graph nodes are created and\n  no validation happens.\n\n  Args:\n    cond: `Tensor`. Must have `dtype=tf.bool` and be scalar.\n    true_vector: `Tensor` of one dimension. Returned when cond is `True`.\n    false_vector: `Tensor` of one dimension. Returned when cond is `False`.\n    name: Python `str`. The name to give this op.\n  Example:  ```python pick_vector(tf.less(0, 5), tf.range(10, 12), tf.range(15,\n    18))  # [10, 11] pick_vector(tf.less(5, 0), tf.range(10, 12), tf.range(15,\n    18))  # [15, 16, 17] ```\n\n  Returns:\n    true_or_false_vector: `Tensor`.\n\n  Raises:\n    TypeError: if `cond.dtype != tf.bool`\n    TypeError: if `cond` is not a constant and\n      `true_vector.dtype != false_vector.dtype`", "docstring_tokens": ["Picks", "possibly", "different", "length", "row", "Tensor", "s", "based", "on", "condition", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L1274-L1320", "partition": "test", "index": 922, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "prefer_static_broadcast_shape", "original_string": "def prefer_static_broadcast_shape(shape1,\n                                  shape2,\n                                  name=\"prefer_static_broadcast_shape\"):\n  \"\"\"Convenience function which statically broadcasts shape when possible.\n\n  Args:\n    shape1:  `1-D` integer `Tensor`.  Already converted to tensor!\n    shape2:  `1-D` integer `Tensor`.  Already converted to tensor!\n    name:  A string name to prepend to created ops.\n\n  Returns:\n    The broadcast shape, either as `TensorShape` (if broadcast can be done\n      statically), or as a `Tensor`.\n  \"\"\"\n  with tf.name_scope(name):\n\n    def make_shape_tensor(x):\n      return tf.convert_to_tensor(value=x, name=\"shape\", dtype=tf.int32)\n\n    def get_tensor_shape(s):\n      if isinstance(s, tf.TensorShape):\n        return s\n      s_ = tf.get_static_value(make_shape_tensor(s))\n      if s_ is not None:\n        return tf.TensorShape(s_)\n      return None\n\n    def get_shape_tensor(s):\n      if not isinstance(s, tf.TensorShape):\n        return make_shape_tensor(s)\n      if tensorshape_util.is_fully_defined(s):\n        return make_shape_tensor(tensorshape_util.as_list(s))\n      raise ValueError(\"Cannot broadcast from partially \"\n                       \"defined `TensorShape`.\")\n\n    shape1_ = get_tensor_shape(shape1)\n    shape2_ = get_tensor_shape(shape2)\n    if shape1_ is not None and shape2_ is not None:\n      return tf.broadcast_static_shape(shape1_, shape2_)\n\n    shape1_ = get_shape_tensor(shape1)\n    shape2_ = get_shape_tensor(shape2)\n    return tf.broadcast_dynamic_shape(shape1_, shape2_)", "language": "python", "code": "def prefer_static_broadcast_shape(shape1,\n                                  shape2,\n                                  name=\"prefer_static_broadcast_shape\"):\n  \"\"\"Convenience function which statically broadcasts shape when possible.\n\n  Args:\n    shape1:  `1-D` integer `Tensor`.  Already converted to tensor!\n    shape2:  `1-D` integer `Tensor`.  Already converted to tensor!\n    name:  A string name to prepend to created ops.\n\n  Returns:\n    The broadcast shape, either as `TensorShape` (if broadcast can be done\n      statically), or as a `Tensor`.\n  \"\"\"\n  with tf.name_scope(name):\n\n    def make_shape_tensor(x):\n      return tf.convert_to_tensor(value=x, name=\"shape\", dtype=tf.int32)\n\n    def get_tensor_shape(s):\n      if isinstance(s, tf.TensorShape):\n        return s\n      s_ = tf.get_static_value(make_shape_tensor(s))\n      if s_ is not None:\n        return tf.TensorShape(s_)\n      return None\n\n    def get_shape_tensor(s):\n      if not isinstance(s, tf.TensorShape):\n        return make_shape_tensor(s)\n      if tensorshape_util.is_fully_defined(s):\n        return make_shape_tensor(tensorshape_util.as_list(s))\n      raise ValueError(\"Cannot broadcast from partially \"\n                       \"defined `TensorShape`.\")\n\n    shape1_ = get_tensor_shape(shape1)\n    shape2_ = get_tensor_shape(shape2)\n    if shape1_ is not None and shape2_ is not None:\n      return tf.broadcast_static_shape(shape1_, shape2_)\n\n    shape1_ = get_shape_tensor(shape1)\n    shape2_ = get_shape_tensor(shape2)\n    return tf.broadcast_dynamic_shape(shape1_, shape2_)", "code_tokens": ["def", "prefer_static_broadcast_shape", "(", "shape1", ",", "shape2", ",", "name", "=", "\"prefer_static_broadcast_shape\"", ")", ":", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "def", "make_shape_tensor", "(", "x", ")", ":", "return", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "\"shape\"", ",", "dtype", "=", "tf", ".", "int32", ")", "def", "get_tensor_shape", "(", "s", ")", ":", "if", "isinstance", "(", "s", ",", "tf", ".", "TensorShape", ")", ":", "return", "s", "s_", "=", "tf", ".", "get_static_value", "(", "make_shape_tensor", "(", "s", ")", ")", "if", "s_", "is", "not", "None", ":", "return", "tf", ".", "TensorShape", "(", "s_", ")", "return", "None", "def", "get_shape_tensor", "(", "s", ")", ":", "if", "not", "isinstance", "(", "s", ",", "tf", ".", "TensorShape", ")", ":", "return", "make_shape_tensor", "(", "s", ")", "if", "tensorshape_util", ".", "is_fully_defined", "(", "s", ")", ":", "return", "make_shape_tensor", "(", "tensorshape_util", ".", "as_list", "(", "s", ")", ")", "raise", "ValueError", "(", "\"Cannot broadcast from partially \"", "\"defined `TensorShape`.\"", ")", "shape1_", "=", "get_tensor_shape", "(", "shape1", ")", "shape2_", "=", "get_tensor_shape", "(", "shape2", ")", "if", "shape1_", "is", "not", "None", "and", "shape2_", "is", "not", "None", ":", "return", "tf", ".", "broadcast_static_shape", "(", "shape1_", ",", "shape2_", ")", "shape1_", "=", "get_shape_tensor", "(", "shape1", ")", "shape2_", "=", "get_shape_tensor", "(", "shape2", ")", "return", "tf", ".", "broadcast_dynamic_shape", "(", "shape1_", ",", "shape2_", ")"], "docstring": "Convenience function which statically broadcasts shape when possible.\n\n  Args:\n    shape1:  `1-D` integer `Tensor`.  Already converted to tensor!\n    shape2:  `1-D` integer `Tensor`.  Already converted to tensor!\n    name:  A string name to prepend to created ops.\n\n  Returns:\n    The broadcast shape, either as `TensorShape` (if broadcast can be done\n      statically), or as a `Tensor`.", "docstring_tokens": ["Convenience", "function", "which", "statically", "broadcasts", "shape", "when", "possible", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L1323-L1365", "partition": "test", "index": 923, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "gen_new_seed", "original_string": "def gen_new_seed(seed, salt):\n  \"\"\"Generate a new seed, from the given seed and salt.\"\"\"\n  if seed is None:\n    return None\n  string = (str(seed) + salt).encode(\"utf-8\")\n  return int(hashlib.md5(string).hexdigest()[:8], 16) & 0x7FFFFFFF", "language": "python", "code": "def gen_new_seed(seed, salt):\n  \"\"\"Generate a new seed, from the given seed and salt.\"\"\"\n  if seed is None:\n    return None\n  string = (str(seed) + salt).encode(\"utf-8\")\n  return int(hashlib.md5(string).hexdigest()[:8], 16) & 0x7FFFFFFF", "code_tokens": ["def", "gen_new_seed", "(", "seed", ",", "salt", ")", ":", "if", "seed", "is", "None", ":", "return", "None", "string", "=", "(", "str", "(", "seed", ")", "+", "salt", ")", ".", "encode", "(", "\"utf-8\"", ")", "return", "int", "(", "hashlib", ".", "md5", "(", "string", ")", ".", "hexdigest", "(", ")", "[", ":", "8", "]", ",", "16", ")", "&", "0x7FFFFFFF"], "docstring": "Generate a new seed, from the given seed and salt.", "docstring_tokens": ["Generate", "a", "new", "seed", "from", "the", "given", "seed", "and", "salt", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L1407-L1412", "partition": "test", "index": 924, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "dimension_size", "original_string": "def dimension_size(x, axis):\n  \"\"\"Returns the size of a specific dimension.\"\"\"\n  # Since tf.gather isn't \"constant-in, constant-out\", we must first check the\n  # static shape or fallback to dynamic shape.\n  s = tf.compat.dimension_value(\n      tensorshape_util.with_rank_at_least(x.shape, np.abs(axis))[axis])\n  if s is not None:\n    return s\n  return tf.shape(input=x)[axis]", "language": "python", "code": "def dimension_size(x, axis):\n  \"\"\"Returns the size of a specific dimension.\"\"\"\n  # Since tf.gather isn't \"constant-in, constant-out\", we must first check the\n  # static shape or fallback to dynamic shape.\n  s = tf.compat.dimension_value(\n      tensorshape_util.with_rank_at_least(x.shape, np.abs(axis))[axis])\n  if s is not None:\n    return s\n  return tf.shape(input=x)[axis]", "code_tokens": ["def", "dimension_size", "(", "x", ",", "axis", ")", ":", "# Since tf.gather isn't \"constant-in, constant-out\", we must first check the", "# static shape or fallback to dynamic shape.", "s", "=", "tf", ".", "compat", ".", "dimension_value", "(", "tensorshape_util", ".", "with_rank_at_least", "(", "x", ".", "shape", ",", "np", ".", "abs", "(", "axis", ")", ")", "[", "axis", "]", ")", "if", "s", "is", "not", "None", ":", "return", "s", "return", "tf", ".", "shape", "(", "input", "=", "x", ")", "[", "axis", "]"], "docstring": "Returns the size of a specific dimension.", "docstring_tokens": ["Returns", "the", "size", "of", "a", "specific", "dimension", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L1853-L1861", "partition": "test", "index": 926, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "process_quadrature_grid_and_probs", "original_string": "def process_quadrature_grid_and_probs(quadrature_grid_and_probs,\n                                      dtype,\n                                      validate_args,\n                                      name=None):\n  \"\"\"Validates quadrature grid, probs or computes them as necessary.\n\n  Args:\n    quadrature_grid_and_probs: Python pair of `float`-like `Tensor`s\n      representing the sample points and the corresponding (possibly\n      normalized) weight.  When `None`, defaults to:\n        `np.polynomial.hermite.hermgauss(deg=8)`.\n    dtype: The expected `dtype` of `grid` and `probs`.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n     quadrature_grid_and_probs: Python pair of `float`-like `Tensor`s\n      representing the sample points and the corresponding (possibly\n      normalized) weight.\n\n  Raises:\n    ValueError: if `quadrature_grid_and_probs is not None` and\n      `len(quadrature_grid_and_probs[0]) != len(quadrature_grid_and_probs[1])`\n  \"\"\"\n  with tf.name_scope(name or \"process_quadrature_grid_and_probs\"):\n    if quadrature_grid_and_probs is None:\n      grid, probs = np.polynomial.hermite.hermgauss(deg=8)\n      grid = grid.astype(dtype_util.as_numpy_dtype(dtype))\n      probs = probs.astype(dtype_util.as_numpy_dtype(dtype))\n      probs /= np.linalg.norm(probs, ord=1, keepdims=True)\n      grid = tf.convert_to_tensor(value=grid, name=\"grid\", dtype=dtype)\n      probs = tf.convert_to_tensor(value=probs, name=\"probs\", dtype=dtype)\n      return grid, probs\n\n    grid, probs = tuple(quadrature_grid_and_probs)\n    grid = tf.convert_to_tensor(value=grid, name=\"grid\", dtype=dtype)\n    probs = tf.convert_to_tensor(\n        value=probs, name=\"unnormalized_probs\", dtype=dtype)\n    probs /= tf.norm(tensor=probs, ord=1, axis=-1, keepdims=True, name=\"probs\")\n\n    def _static_event_size(x):\n      \"\"\"Returns the static size of a specific dimension or `None`.\"\"\"\n      return tf.compat.dimension_value(\n          tensorshape_util.with_rank_at_least(x.shape, 1)[-1])\n\n    m, n = _static_event_size(probs), _static_event_size(grid)\n    if m is not None and n is not None:\n      if m != n:\n        raise ValueError(\"`quadrature_grid_and_probs` must be a `tuple` of \"\n                         \"same-length zero-th-dimension `Tensor`s \"\n                         \"(saw lengths {}, {})\".format(m, n))\n    elif validate_args:\n      assertions = [\n          assert_util.assert_equal(\n              dimension_size(probs, axis=-1),\n              dimension_size(grid, axis=-1),\n              message=(\"`quadrature_grid_and_probs` must be a `tuple` of \"\n                       \"same-length zero-th-dimension `Tensor`s\")),\n      ]\n      with tf.control_dependencies(assertions):\n        grid = tf.identity(grid)\n        probs = tf.identity(probs)\n    return grid, probs", "language": "python", "code": "def process_quadrature_grid_and_probs(quadrature_grid_and_probs,\n                                      dtype,\n                                      validate_args,\n                                      name=None):\n  \"\"\"Validates quadrature grid, probs or computes them as necessary.\n\n  Args:\n    quadrature_grid_and_probs: Python pair of `float`-like `Tensor`s\n      representing the sample points and the corresponding (possibly\n      normalized) weight.  When `None`, defaults to:\n        `np.polynomial.hermite.hermgauss(deg=8)`.\n    dtype: The expected `dtype` of `grid` and `probs`.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n     quadrature_grid_and_probs: Python pair of `float`-like `Tensor`s\n      representing the sample points and the corresponding (possibly\n      normalized) weight.\n\n  Raises:\n    ValueError: if `quadrature_grid_and_probs is not None` and\n      `len(quadrature_grid_and_probs[0]) != len(quadrature_grid_and_probs[1])`\n  \"\"\"\n  with tf.name_scope(name or \"process_quadrature_grid_and_probs\"):\n    if quadrature_grid_and_probs is None:\n      grid, probs = np.polynomial.hermite.hermgauss(deg=8)\n      grid = grid.astype(dtype_util.as_numpy_dtype(dtype))\n      probs = probs.astype(dtype_util.as_numpy_dtype(dtype))\n      probs /= np.linalg.norm(probs, ord=1, keepdims=True)\n      grid = tf.convert_to_tensor(value=grid, name=\"grid\", dtype=dtype)\n      probs = tf.convert_to_tensor(value=probs, name=\"probs\", dtype=dtype)\n      return grid, probs\n\n    grid, probs = tuple(quadrature_grid_and_probs)\n    grid = tf.convert_to_tensor(value=grid, name=\"grid\", dtype=dtype)\n    probs = tf.convert_to_tensor(\n        value=probs, name=\"unnormalized_probs\", dtype=dtype)\n    probs /= tf.norm(tensor=probs, ord=1, axis=-1, keepdims=True, name=\"probs\")\n\n    def _static_event_size(x):\n      \"\"\"Returns the static size of a specific dimension or `None`.\"\"\"\n      return tf.compat.dimension_value(\n          tensorshape_util.with_rank_at_least(x.shape, 1)[-1])\n\n    m, n = _static_event_size(probs), _static_event_size(grid)\n    if m is not None and n is not None:\n      if m != n:\n        raise ValueError(\"`quadrature_grid_and_probs` must be a `tuple` of \"\n                         \"same-length zero-th-dimension `Tensor`s \"\n                         \"(saw lengths {}, {})\".format(m, n))\n    elif validate_args:\n      assertions = [\n          assert_util.assert_equal(\n              dimension_size(probs, axis=-1),\n              dimension_size(grid, axis=-1),\n              message=(\"`quadrature_grid_and_probs` must be a `tuple` of \"\n                       \"same-length zero-th-dimension `Tensor`s\")),\n      ]\n      with tf.control_dependencies(assertions):\n        grid = tf.identity(grid)\n        probs = tf.identity(probs)\n    return grid, probs", "code_tokens": ["def", "process_quadrature_grid_and_probs", "(", "quadrature_grid_and_probs", ",", "dtype", ",", "validate_args", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "\"process_quadrature_grid_and_probs\"", ")", ":", "if", "quadrature_grid_and_probs", "is", "None", ":", "grid", ",", "probs", "=", "np", ".", "polynomial", ".", "hermite", ".", "hermgauss", "(", "deg", "=", "8", ")", "grid", "=", "grid", ".", "astype", "(", "dtype_util", ".", "as_numpy_dtype", "(", "dtype", ")", ")", "probs", "=", "probs", ".", "astype", "(", "dtype_util", ".", "as_numpy_dtype", "(", "dtype", ")", ")", "probs", "/=", "np", ".", "linalg", ".", "norm", "(", "probs", ",", "ord", "=", "1", ",", "keepdims", "=", "True", ")", "grid", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "grid", ",", "name", "=", "\"grid\"", ",", "dtype", "=", "dtype", ")", "probs", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "probs", ",", "name", "=", "\"probs\"", ",", "dtype", "=", "dtype", ")", "return", "grid", ",", "probs", "grid", ",", "probs", "=", "tuple", "(", "quadrature_grid_and_probs", ")", "grid", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "grid", ",", "name", "=", "\"grid\"", ",", "dtype", "=", "dtype", ")", "probs", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "probs", ",", "name", "=", "\"unnormalized_probs\"", ",", "dtype", "=", "dtype", ")", "probs", "/=", "tf", ".", "norm", "(", "tensor", "=", "probs", ",", "ord", "=", "1", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ",", "name", "=", "\"probs\"", ")", "def", "_static_event_size", "(", "x", ")", ":", "\"\"\"Returns the static size of a specific dimension or `None`.\"\"\"", "return", "tf", ".", "compat", ".", "dimension_value", "(", "tensorshape_util", ".", "with_rank_at_least", "(", "x", ".", "shape", ",", "1", ")", "[", "-", "1", "]", ")", "m", ",", "n", "=", "_static_event_size", "(", "probs", ")", ",", "_static_event_size", "(", "grid", ")", "if", "m", "is", "not", "None", "and", "n", "is", "not", "None", ":", "if", "m", "!=", "n", ":", "raise", "ValueError", "(", "\"`quadrature_grid_and_probs` must be a `tuple` of \"", "\"same-length zero-th-dimension `Tensor`s \"", "\"(saw lengths {}, {})\"", ".", "format", "(", "m", ",", "n", ")", ")", "elif", "validate_args", ":", "assertions", "=", "[", "assert_util", ".", "assert_equal", "(", "dimension_size", "(", "probs", ",", "axis", "=", "-", "1", ")", ",", "dimension_size", "(", "grid", ",", "axis", "=", "-", "1", ")", ",", "message", "=", "(", "\"`quadrature_grid_and_probs` must be a `tuple` of \"", "\"same-length zero-th-dimension `Tensor`s\"", ")", ")", ",", "]", "with", "tf", ".", "control_dependencies", "(", "assertions", ")", ":", "grid", "=", "tf", ".", "identity", "(", "grid", ")", "probs", "=", "tf", ".", "identity", "(", "probs", ")", "return", "grid", ",", "probs"], "docstring": "Validates quadrature grid, probs or computes them as necessary.\n\n  Args:\n    quadrature_grid_and_probs: Python pair of `float`-like `Tensor`s\n      representing the sample points and the corresponding (possibly\n      normalized) weight.  When `None`, defaults to:\n        `np.polynomial.hermite.hermgauss(deg=8)`.\n    dtype: The expected `dtype` of `grid` and `probs`.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n     quadrature_grid_and_probs: Python pair of `float`-like `Tensor`s\n      representing the sample points and the corresponding (possibly\n      normalized) weight.\n\n  Raises:\n    ValueError: if `quadrature_grid_and_probs is not None` and\n      `len(quadrature_grid_and_probs[0]) != len(quadrature_grid_and_probs[1])`", "docstring_tokens": ["Validates", "quadrature", "grid", "probs", "or", "computes", "them", "as", "necessary", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L1864-L1929", "partition": "test", "index": 927, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "parent_frame_arguments", "original_string": "def parent_frame_arguments():\n  \"\"\"Returns parent frame arguments.\n\n  When called inside a function, returns a dictionary with the caller's function\n  arguments. These are positional arguments and keyword arguments (**kwargs),\n  while variable arguments (*varargs) are excluded.\n\n  When called at global scope, this will return an empty dictionary, since there\n  are no arguments.\n\n  WARNING: If caller function argument names are overloaded before invoking\n  this method, then values will reflect the overloaded value. For this reason,\n  we recommend calling `parent_frame_arguments` at the beginning of the\n  function.\n  \"\"\"\n  # All arguments and the names used for *varargs, and **kwargs\n  arg_names, variable_arg_name, keyword_arg_name, local_vars = (\n      tf_inspect._inspect.getargvalues(  # pylint: disable=protected-access\n          # Get the first frame of the caller of this method.\n          tf_inspect._inspect.stack()[1][0]))  # pylint: disable=protected-access\n\n  # Remove the *varargs, and flatten the **kwargs. Both are\n  # nested lists.\n  local_vars.pop(variable_arg_name, {})\n  keyword_args = local_vars.pop(keyword_arg_name, {})\n\n  final_args = {}\n  # Copy over arguments and their values. In general, local_vars\n  # may contain more than just the arguments, since this method\n  # can be called anywhere in a function.\n  for arg_name in arg_names:\n    final_args[arg_name] = local_vars.pop(arg_name)\n  final_args.update(keyword_args)\n\n  return final_args", "language": "python", "code": "def parent_frame_arguments():\n  \"\"\"Returns parent frame arguments.\n\n  When called inside a function, returns a dictionary with the caller's function\n  arguments. These are positional arguments and keyword arguments (**kwargs),\n  while variable arguments (*varargs) are excluded.\n\n  When called at global scope, this will return an empty dictionary, since there\n  are no arguments.\n\n  WARNING: If caller function argument names are overloaded before invoking\n  this method, then values will reflect the overloaded value. For this reason,\n  we recommend calling `parent_frame_arguments` at the beginning of the\n  function.\n  \"\"\"\n  # All arguments and the names used for *varargs, and **kwargs\n  arg_names, variable_arg_name, keyword_arg_name, local_vars = (\n      tf_inspect._inspect.getargvalues(  # pylint: disable=protected-access\n          # Get the first frame of the caller of this method.\n          tf_inspect._inspect.stack()[1][0]))  # pylint: disable=protected-access\n\n  # Remove the *varargs, and flatten the **kwargs. Both are\n  # nested lists.\n  local_vars.pop(variable_arg_name, {})\n  keyword_args = local_vars.pop(keyword_arg_name, {})\n\n  final_args = {}\n  # Copy over arguments and their values. In general, local_vars\n  # may contain more than just the arguments, since this method\n  # can be called anywhere in a function.\n  for arg_name in arg_names:\n    final_args[arg_name] = local_vars.pop(arg_name)\n  final_args.update(keyword_args)\n\n  return final_args", "code_tokens": ["def", "parent_frame_arguments", "(", ")", ":", "# All arguments and the names used for *varargs, and **kwargs", "arg_names", ",", "variable_arg_name", ",", "keyword_arg_name", ",", "local_vars", "=", "(", "tf_inspect", ".", "_inspect", ".", "getargvalues", "(", "# pylint: disable=protected-access", "# Get the first frame of the caller of this method.", "tf_inspect", ".", "_inspect", ".", "stack", "(", ")", "[", "1", "]", "[", "0", "]", ")", ")", "# pylint: disable=protected-access", "# Remove the *varargs, and flatten the **kwargs. Both are", "# nested lists.", "local_vars", ".", "pop", "(", "variable_arg_name", ",", "{", "}", ")", "keyword_args", "=", "local_vars", ".", "pop", "(", "keyword_arg_name", ",", "{", "}", ")", "final_args", "=", "{", "}", "# Copy over arguments and their values. In general, local_vars", "# may contain more than just the arguments, since this method", "# can be called anywhere in a function.", "for", "arg_name", "in", "arg_names", ":", "final_args", "[", "arg_name", "]", "=", "local_vars", ".", "pop", "(", "arg_name", ")", "final_args", ".", "update", "(", "keyword_args", ")", "return", "final_args"], "docstring": "Returns parent frame arguments.\n\n  When called inside a function, returns a dictionary with the caller's function\n  arguments. These are positional arguments and keyword arguments (**kwargs),\n  while variable arguments (*varargs) are excluded.\n\n  When called at global scope, this will return an empty dictionary, since there\n  are no arguments.\n\n  WARNING: If caller function argument names are overloaded before invoking\n  this method, then values will reflect the overloaded value. For this reason,\n  we recommend calling `parent_frame_arguments` at the beginning of the\n  function.", "docstring_tokens": ["Returns", "parent", "frame", "arguments", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L2005-L2039", "partition": "test", "index": 928, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/dirichlet_multinomial.py", "func_name": "DirichletMultinomial._variance_scale_term", "original_string": "def _variance_scale_term(self):\n    \"\"\"Helper to `_covariance` and `_variance` which computes a shared scale.\"\"\"\n    # Expand back the last dim so the shape of _variance_scale_term matches the\n    # shape of self.concentration.\n    c0 = self.total_concentration[..., tf.newaxis]\n    return tf.sqrt((1. + c0 / self.total_count[..., tf.newaxis]) / (1. + c0))", "language": "python", "code": "def _variance_scale_term(self):\n    \"\"\"Helper to `_covariance` and `_variance` which computes a shared scale.\"\"\"\n    # Expand back the last dim so the shape of _variance_scale_term matches the\n    # shape of self.concentration.\n    c0 = self.total_concentration[..., tf.newaxis]\n    return tf.sqrt((1. + c0 / self.total_count[..., tf.newaxis]) / (1. + c0))", "code_tokens": ["def", "_variance_scale_term", "(", "self", ")", ":", "# Expand back the last dim so the shape of _variance_scale_term matches the", "# shape of self.concentration.", "c0", "=", "self", ".", "total_concentration", "[", "...", ",", "tf", ".", "newaxis", "]", "return", "tf", ".", "sqrt", "(", "(", "1.", "+", "c0", "/", "self", ".", "total_count", "[", "...", ",", "tf", ".", "newaxis", "]", ")", "/", "(", "1.", "+", "c0", ")", ")"], "docstring": "Helper to `_covariance` and `_variance` which computes a shared scale.", "docstring_tokens": ["Helper", "to", "_covariance", "and", "_variance", "which", "computes", "a", "shared", "scale", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/dirichlet_multinomial.py#L322-L327", "partition": "test", "index": 989, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/distribution.py", "func_name": "_copy_fn", "original_string": "def _copy_fn(fn):\n  \"\"\"Create a deep copy of fn.\n\n  Args:\n    fn: a callable\n\n  Returns:\n    A `FunctionType`: a deep copy of fn.\n\n  Raises:\n    TypeError: if `fn` is not a callable.\n  \"\"\"\n  if not callable(fn):\n    raise TypeError(\"fn is not callable: {}\".format(fn))\n  # The blessed way to copy a function. copy.deepcopy fails to create a\n  # non-reference copy. Since:\n  #   types.FunctionType == type(lambda: None),\n  # and the docstring for the function type states:\n  #\n  #   function(code, globals[, name[, argdefs[, closure]]])\n  #\n  #   Create a function object from a code object and a dictionary.\n  #   ...\n  #\n  # Here we can use this to create a new function with the old function's\n  # code, globals, closure, etc.\n  return types.FunctionType(\n      code=fn.__code__, globals=fn.__globals__,\n      name=fn.__name__, argdefs=fn.__defaults__,\n      closure=fn.__closure__)", "language": "python", "code": "def _copy_fn(fn):\n  \"\"\"Create a deep copy of fn.\n\n  Args:\n    fn: a callable\n\n  Returns:\n    A `FunctionType`: a deep copy of fn.\n\n  Raises:\n    TypeError: if `fn` is not a callable.\n  \"\"\"\n  if not callable(fn):\n    raise TypeError(\"fn is not callable: {}\".format(fn))\n  # The blessed way to copy a function. copy.deepcopy fails to create a\n  # non-reference copy. Since:\n  #   types.FunctionType == type(lambda: None),\n  # and the docstring for the function type states:\n  #\n  #   function(code, globals[, name[, argdefs[, closure]]])\n  #\n  #   Create a function object from a code object and a dictionary.\n  #   ...\n  #\n  # Here we can use this to create a new function with the old function's\n  # code, globals, closure, etc.\n  return types.FunctionType(\n      code=fn.__code__, globals=fn.__globals__,\n      name=fn.__name__, argdefs=fn.__defaults__,\n      closure=fn.__closure__)", "code_tokens": ["def", "_copy_fn", "(", "fn", ")", ":", "if", "not", "callable", "(", "fn", ")", ":", "raise", "TypeError", "(", "\"fn is not callable: {}\"", ".", "format", "(", "fn", ")", ")", "# The blessed way to copy a function. copy.deepcopy fails to create a", "# non-reference copy. Since:", "#   types.FunctionType == type(lambda: None),", "# and the docstring for the function type states:", "#", "#   function(code, globals[, name[, argdefs[, closure]]])", "#", "#   Create a function object from a code object and a dictionary.", "#   ...", "#", "# Here we can use this to create a new function with the old function's", "# code, globals, closure, etc.", "return", "types", ".", "FunctionType", "(", "code", "=", "fn", ".", "__code__", ",", "globals", "=", "fn", ".", "__globals__", ",", "name", "=", "fn", ".", "__name__", ",", "argdefs", "=", "fn", ".", "__defaults__", ",", "closure", "=", "fn", ".", "__closure__", ")"], "docstring": "Create a deep copy of fn.\n\n  Args:\n    fn: a callable\n\n  Returns:\n    A `FunctionType`: a deep copy of fn.\n\n  Raises:\n    TypeError: if `fn` is not a callable.", "docstring_tokens": ["Create", "a", "deep", "copy", "of", "fn", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/distribution.py#L75-L104", "partition": "test", "index": 960, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/normal.py", "func_name": "Normal._inv_z", "original_string": "def _inv_z(self, z):\n    \"\"\"Reconstruct input `x` from a its normalized version.\"\"\"\n    with tf.name_scope(\"reconstruct\"):\n      return z * self.scale + self.loc", "language": "python", "code": "def _inv_z(self, z):\n    \"\"\"Reconstruct input `x` from a its normalized version.\"\"\"\n    with tf.name_scope(\"reconstruct\"):\n      return z * self.scale + self.loc", "code_tokens": ["def", "_inv_z", "(", "self", ",", "z", ")", ":", "with", "tf", ".", "name_scope", "(", "\"reconstruct\"", ")", ":", "return", "z", "*", "self", ".", "scale", "+", "self", ".", "loc"], "docstring": "Reconstruct input `x` from a its normalized version.", "docstring_tokens": ["Reconstruct", "input", "x", "from", "a", "its", "normalized", "version", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/normal.py#L234-L237", "partition": "test", "index": 662, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/normal.py", "func_name": "Normal._z", "original_string": "def _z(self, x):\n    \"\"\"Standardize input `x` to a unit normal.\"\"\"\n    with tf.name_scope(\"standardize\"):\n      return (x - self.loc) / self.scale", "language": "python", "code": "def _z(self, x):\n    \"\"\"Standardize input `x` to a unit normal.\"\"\"\n    with tf.name_scope(\"standardize\"):\n      return (x - self.loc) / self.scale", "code_tokens": ["def", "_z", "(", "self", ",", "x", ")", ":", "with", "tf", ".", "name_scope", "(", "\"standardize\"", ")", ":", "return", "(", "x", "-", "self", ".", "loc", ")", "/", "self", ".", "scale"], "docstring": "Standardize input `x` to a unit normal.", "docstring_tokens": ["Standardize", "input", "x", "to", "a", "unit", "normal", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/normal.py#L229-L232", "partition": "test", "index": 661, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "_smallest_integer_by_dtype", "original_string": "def _smallest_integer_by_dtype(dt):\n  \"\"\"Helper returning the smallest integer exactly representable by dtype.\"\"\"\n  if not _is_known_dtype(dt):\n    raise TypeError(\"Unrecognized dtype: {}\".format(dt.name))\n  if _is_known_unsigned_by_dtype(dt):\n    return 0\n  return -1 * _largest_integer_by_dtype(dt)", "language": "python", "code": "def _smallest_integer_by_dtype(dt):\n  \"\"\"Helper returning the smallest integer exactly representable by dtype.\"\"\"\n  if not _is_known_dtype(dt):\n    raise TypeError(\"Unrecognized dtype: {}\".format(dt.name))\n  if _is_known_unsigned_by_dtype(dt):\n    return 0\n  return -1 * _largest_integer_by_dtype(dt)", "code_tokens": ["def", "_smallest_integer_by_dtype", "(", "dt", ")", ":", "if", "not", "_is_known_dtype", "(", "dt", ")", ":", "raise", "TypeError", "(", "\"Unrecognized dtype: {}\"", ".", "format", "(", "dt", ".", "name", ")", ")", "if", "_is_known_unsigned_by_dtype", "(", "dt", ")", ":", "return", "0", "return", "-", "1", "*", "_largest_integer_by_dtype", "(", "dt", ")"], "docstring": "Helper returning the smallest integer exactly representable by dtype.", "docstring_tokens": ["Helper", "returning", "the", "smallest", "integer", "exactly", "representable", "by", "dtype", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L889-L895", "partition": "test", "index": 917, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "same_dynamic_shape", "original_string": "def same_dynamic_shape(a, b):\n  \"\"\"Returns whether a and b have the same dynamic shape.\n\n  Args:\n    a: `Tensor`\n    b: `Tensor`\n\n  Returns:\n    `bool` `Tensor` representing if both tensors have the same shape.\n  \"\"\"\n  a = tf.convert_to_tensor(value=a, name=\"a\")\n  b = tf.convert_to_tensor(value=b, name=\"b\")\n\n  # Here we can't just do tf.equal(a.shape, b.shape), since\n  # static shape inference may break the equality comparison between\n  # shape(a) and shape(b) in tf.equal.\n  def all_shapes_equal():\n    return tf.reduce_all(\n        input_tensor=tf.equal(\n            tf.concat([tf.shape(input=a), tf.shape(input=b)], 0),\n            tf.concat([tf.shape(input=b), tf.shape(input=a)], 0)))\n\n  # One of the shapes isn't fully defined, so we need to use the dynamic\n  # shape.\n  return tf.cond(\n      pred=tf.equal(tf.rank(a), tf.rank(b)),\n      true_fn=all_shapes_equal,\n      false_fn=lambda: tf.constant(False))", "language": "python", "code": "def same_dynamic_shape(a, b):\n  \"\"\"Returns whether a and b have the same dynamic shape.\n\n  Args:\n    a: `Tensor`\n    b: `Tensor`\n\n  Returns:\n    `bool` `Tensor` representing if both tensors have the same shape.\n  \"\"\"\n  a = tf.convert_to_tensor(value=a, name=\"a\")\n  b = tf.convert_to_tensor(value=b, name=\"b\")\n\n  # Here we can't just do tf.equal(a.shape, b.shape), since\n  # static shape inference may break the equality comparison between\n  # shape(a) and shape(b) in tf.equal.\n  def all_shapes_equal():\n    return tf.reduce_all(\n        input_tensor=tf.equal(\n            tf.concat([tf.shape(input=a), tf.shape(input=b)], 0),\n            tf.concat([tf.shape(input=b), tf.shape(input=a)], 0)))\n\n  # One of the shapes isn't fully defined, so we need to use the dynamic\n  # shape.\n  return tf.cond(\n      pred=tf.equal(tf.rank(a), tf.rank(b)),\n      true_fn=all_shapes_equal,\n      false_fn=lambda: tf.constant(False))", "code_tokens": ["def", "same_dynamic_shape", "(", "a", ",", "b", ")", ":", "a", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "a", ",", "name", "=", "\"a\"", ")", "b", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "b", ",", "name", "=", "\"b\"", ")", "# Here we can't just do tf.equal(a.shape, b.shape), since", "# static shape inference may break the equality comparison between", "# shape(a) and shape(b) in tf.equal.", "def", "all_shapes_equal", "(", ")", ":", "return", "tf", ".", "reduce_all", "(", "input_tensor", "=", "tf", ".", "equal", "(", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "input", "=", "a", ")", ",", "tf", ".", "shape", "(", "input", "=", "b", ")", "]", ",", "0", ")", ",", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "input", "=", "b", ")", ",", "tf", ".", "shape", "(", "input", "=", "a", ")", "]", ",", "0", ")", ")", ")", "# One of the shapes isn't fully defined, so we need to use the dynamic", "# shape.", "return", "tf", ".", "cond", "(", "pred", "=", "tf", ".", "equal", "(", "tf", ".", "rank", "(", "a", ")", ",", "tf", ".", "rank", "(", "b", ")", ")", ",", "true_fn", "=", "all_shapes_equal", ",", "false_fn", "=", "lambda", ":", "tf", ".", "constant", "(", "False", ")", ")"], "docstring": "Returns whether a and b have the same dynamic shape.\n\n  Args:\n    a: `Tensor`\n    b: `Tensor`\n\n  Returns:\n    `bool` `Tensor` representing if both tensors have the same shape.", "docstring_tokens": ["Returns", "whether", "a", "and", "b", "have", "the", "same", "dynamic", "shape", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L710-L737", "partition": "test", "index": 912, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "tridiag", "original_string": "def tridiag(below=None, diag=None, above=None, name=None):\n  \"\"\"Creates a matrix with values set above, below, and on the diagonal.\n\n  Example:\n\n  ```python\n  tridiag(below=[1., 2., 3.],\n          diag=[4., 5., 6., 7.],\n          above=[8., 9., 10.])\n  # ==> array([[  4.,   8.,   0.,   0.],\n  #            [  1.,   5.,   9.,   0.],\n  #            [  0.,   2.,   6.,  10.],\n  #            [  0.,   0.,   3.,   7.]], dtype=float32)\n  ```\n\n  Warning: This Op is intended for convenience, not efficiency.\n\n  Args:\n    below: `Tensor` of shape `[B1, ..., Bb, d-1]` corresponding to the below\n      diagonal part. `None` is logically equivalent to `below = 0`.\n    diag: `Tensor` of shape `[B1, ..., Bb, d]` corresponding to the diagonal\n      part.  `None` is logically equivalent to `diag = 0`.\n    above: `Tensor` of shape `[B1, ..., Bb, d-1]` corresponding to the above\n      diagonal part.  `None` is logically equivalent to `above = 0`.\n    name: Python `str`. The name to give this op.\n\n  Returns:\n    tridiag: `Tensor` with values set above, below and on the diagonal.\n\n  Raises:\n    ValueError: if all inputs are `None`.\n  \"\"\"\n\n  def _pad(x):\n    \"\"\"Prepends and appends a zero to every vector in a batch of vectors.\"\"\"\n    shape = tf.concat([tf.shape(input=x)[:-1], [1]], axis=0)\n    z = tf.zeros(shape, dtype=x.dtype)\n    return tf.concat([z, x, z], axis=-1)\n\n  def _add(*x):\n    \"\"\"Adds list of Tensors, ignoring `None`.\"\"\"\n    s = None\n    for y in x:\n      if y is None:\n        continue\n      elif s is None:\n        s = y\n      else:\n        s += y\n    if s is None:\n      raise ValueError(\"Must specify at least one of `below`, `diag`, `above`.\")\n    return s\n\n  with tf.name_scope(name or \"tridiag\"):\n    if below is not None:\n      below = tf.convert_to_tensor(value=below, name=\"below\")\n      below = tf.linalg.diag(_pad(below))[..., :-1, 1:]\n    if diag is not None:\n      diag = tf.convert_to_tensor(value=diag, name=\"diag\")\n      diag = tf.linalg.diag(diag)\n    if above is not None:\n      above = tf.convert_to_tensor(value=above, name=\"above\")\n      above = tf.linalg.diag(_pad(above))[..., 1:, :-1]\n    # TODO(jvdillon): Consider using scatter_nd instead of creating three full\n    # matrices.\n    return _add(below, diag, above)", "language": "python", "code": "def tridiag(below=None, diag=None, above=None, name=None):\n  \"\"\"Creates a matrix with values set above, below, and on the diagonal.\n\n  Example:\n\n  ```python\n  tridiag(below=[1., 2., 3.],\n          diag=[4., 5., 6., 7.],\n          above=[8., 9., 10.])\n  # ==> array([[  4.,   8.,   0.,   0.],\n  #            [  1.,   5.,   9.,   0.],\n  #            [  0.,   2.,   6.,  10.],\n  #            [  0.,   0.,   3.,   7.]], dtype=float32)\n  ```\n\n  Warning: This Op is intended for convenience, not efficiency.\n\n  Args:\n    below: `Tensor` of shape `[B1, ..., Bb, d-1]` corresponding to the below\n      diagonal part. `None` is logically equivalent to `below = 0`.\n    diag: `Tensor` of shape `[B1, ..., Bb, d]` corresponding to the diagonal\n      part.  `None` is logically equivalent to `diag = 0`.\n    above: `Tensor` of shape `[B1, ..., Bb, d-1]` corresponding to the above\n      diagonal part.  `None` is logically equivalent to `above = 0`.\n    name: Python `str`. The name to give this op.\n\n  Returns:\n    tridiag: `Tensor` with values set above, below and on the diagonal.\n\n  Raises:\n    ValueError: if all inputs are `None`.\n  \"\"\"\n\n  def _pad(x):\n    \"\"\"Prepends and appends a zero to every vector in a batch of vectors.\"\"\"\n    shape = tf.concat([tf.shape(input=x)[:-1], [1]], axis=0)\n    z = tf.zeros(shape, dtype=x.dtype)\n    return tf.concat([z, x, z], axis=-1)\n\n  def _add(*x):\n    \"\"\"Adds list of Tensors, ignoring `None`.\"\"\"\n    s = None\n    for y in x:\n      if y is None:\n        continue\n      elif s is None:\n        s = y\n      else:\n        s += y\n    if s is None:\n      raise ValueError(\"Must specify at least one of `below`, `diag`, `above`.\")\n    return s\n\n  with tf.name_scope(name or \"tridiag\"):\n    if below is not None:\n      below = tf.convert_to_tensor(value=below, name=\"below\")\n      below = tf.linalg.diag(_pad(below))[..., :-1, 1:]\n    if diag is not None:\n      diag = tf.convert_to_tensor(value=diag, name=\"diag\")\n      diag = tf.linalg.diag(diag)\n    if above is not None:\n      above = tf.convert_to_tensor(value=above, name=\"above\")\n      above = tf.linalg.diag(_pad(above))[..., 1:, :-1]\n    # TODO(jvdillon): Consider using scatter_nd instead of creating three full\n    # matrices.\n    return _add(below, diag, above)", "code_tokens": ["def", "tridiag", "(", "below", "=", "None", ",", "diag", "=", "None", ",", "above", "=", "None", ",", "name", "=", "None", ")", ":", "def", "_pad", "(", "x", ")", ":", "\"\"\"Prepends and appends a zero to every vector in a batch of vectors.\"\"\"", "shape", "=", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "input", "=", "x", ")", "[", ":", "-", "1", "]", ",", "[", "1", "]", "]", ",", "axis", "=", "0", ")", "z", "=", "tf", ".", "zeros", "(", "shape", ",", "dtype", "=", "x", ".", "dtype", ")", "return", "tf", ".", "concat", "(", "[", "z", ",", "x", ",", "z", "]", ",", "axis", "=", "-", "1", ")", "def", "_add", "(", "*", "x", ")", ":", "\"\"\"Adds list of Tensors, ignoring `None`.\"\"\"", "s", "=", "None", "for", "y", "in", "x", ":", "if", "y", "is", "None", ":", "continue", "elif", "s", "is", "None", ":", "s", "=", "y", "else", ":", "s", "+=", "y", "if", "s", "is", "None", ":", "raise", "ValueError", "(", "\"Must specify at least one of `below`, `diag`, `above`.\"", ")", "return", "s", "with", "tf", ".", "name_scope", "(", "name", "or", "\"tridiag\"", ")", ":", "if", "below", "is", "not", "None", ":", "below", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "below", ",", "name", "=", "\"below\"", ")", "below", "=", "tf", ".", "linalg", ".", "diag", "(", "_pad", "(", "below", ")", ")", "[", "...", ",", ":", "-", "1", ",", "1", ":", "]", "if", "diag", "is", "not", "None", ":", "diag", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "diag", ",", "name", "=", "\"diag\"", ")", "diag", "=", "tf", ".", "linalg", ".", "diag", "(", "diag", ")", "if", "above", "is", "not", "None", ":", "above", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "above", ",", "name", "=", "\"above\"", ")", "above", "=", "tf", ".", "linalg", ".", "diag", "(", "_pad", "(", "above", ")", ")", "[", "...", ",", "1", ":", ",", ":", "-", "1", "]", "# TODO(jvdillon): Consider using scatter_nd instead of creating three full", "# matrices.", "return", "_add", "(", "below", ",", "diag", ",", "above", ")"], "docstring": "Creates a matrix with values set above, below, and on the diagonal.\n\n  Example:\n\n  ```python\n  tridiag(below=[1., 2., 3.],\n          diag=[4., 5., 6., 7.],\n          above=[8., 9., 10.])\n  # ==> array([[  4.,   8.,   0.,   0.],\n  #            [  1.,   5.,   9.,   0.],\n  #            [  0.,   2.,   6.,  10.],\n  #            [  0.,   0.,   3.,   7.]], dtype=float32)\n  ```\n\n  Warning: This Op is intended for convenience, not efficiency.\n\n  Args:\n    below: `Tensor` of shape `[B1, ..., Bb, d-1]` corresponding to the below\n      diagonal part. `None` is logically equivalent to `below = 0`.\n    diag: `Tensor` of shape `[B1, ..., Bb, d]` corresponding to the diagonal\n      part.  `None` is logically equivalent to `diag = 0`.\n    above: `Tensor` of shape `[B1, ..., Bb, d-1]` corresponding to the above\n      diagonal part.  `None` is logically equivalent to `above = 0`.\n    name: Python `str`. The name to give this op.\n\n  Returns:\n    tridiag: `Tensor` with values set above, below and on the diagonal.\n\n  Raises:\n    ValueError: if all inputs are `None`.", "docstring_tokens": ["Creates", "a", "matrix", "with", "values", "set", "above", "below", "and", "on", "the", "diagonal", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L1633-L1698", "partition": "test", "index": 925, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/categorical.py", "func_name": "_broadcast_cat_event_and_params", "original_string": "def _broadcast_cat_event_and_params(event, params, base_dtype):\n  \"\"\"Broadcasts the event or distribution parameters.\"\"\"\n  if dtype_util.is_integer(event.dtype):\n    pass\n  elif dtype_util.is_floating(event.dtype):\n    # When `validate_args=True` we've already ensured int/float casting\n    # is closed.\n    event = tf.cast(event, dtype=tf.int32)\n  else:\n    raise TypeError(\"`value` should have integer `dtype` or \"\n                    \"`self.dtype` ({})\".format(base_dtype))\n  shape_known_statically = (\n      tensorshape_util.rank(params.shape) is not None and\n      tensorshape_util.is_fully_defined(params.shape[:-1]) and\n      tensorshape_util.is_fully_defined(event.shape))\n  if not shape_known_statically or params.shape[:-1] != event.shape:\n    params *= tf.ones_like(event[..., tf.newaxis],\n                           dtype=params.dtype)\n    params_shape = tf.shape(input=params)[:-1]\n    event *= tf.ones(params_shape, dtype=event.dtype)\n    if tensorshape_util.rank(params.shape) is not None:\n      tensorshape_util.set_shape(event, params.shape[:-1])\n\n  return event, params", "language": "python", "code": "def _broadcast_cat_event_and_params(event, params, base_dtype):\n  \"\"\"Broadcasts the event or distribution parameters.\"\"\"\n  if dtype_util.is_integer(event.dtype):\n    pass\n  elif dtype_util.is_floating(event.dtype):\n    # When `validate_args=True` we've already ensured int/float casting\n    # is closed.\n    event = tf.cast(event, dtype=tf.int32)\n  else:\n    raise TypeError(\"`value` should have integer `dtype` or \"\n                    \"`self.dtype` ({})\".format(base_dtype))\n  shape_known_statically = (\n      tensorshape_util.rank(params.shape) is not None and\n      tensorshape_util.is_fully_defined(params.shape[:-1]) and\n      tensorshape_util.is_fully_defined(event.shape))\n  if not shape_known_statically or params.shape[:-1] != event.shape:\n    params *= tf.ones_like(event[..., tf.newaxis],\n                           dtype=params.dtype)\n    params_shape = tf.shape(input=params)[:-1]\n    event *= tf.ones(params_shape, dtype=event.dtype)\n    if tensorshape_util.rank(params.shape) is not None:\n      tensorshape_util.set_shape(event, params.shape[:-1])\n\n  return event, params", "code_tokens": ["def", "_broadcast_cat_event_and_params", "(", "event", ",", "params", ",", "base_dtype", ")", ":", "if", "dtype_util", ".", "is_integer", "(", "event", ".", "dtype", ")", ":", "pass", "elif", "dtype_util", ".", "is_floating", "(", "event", ".", "dtype", ")", ":", "# When `validate_args=True` we've already ensured int/float casting", "# is closed.", "event", "=", "tf", ".", "cast", "(", "event", ",", "dtype", "=", "tf", ".", "int32", ")", "else", ":", "raise", "TypeError", "(", "\"`value` should have integer `dtype` or \"", "\"`self.dtype` ({})\"", ".", "format", "(", "base_dtype", ")", ")", "shape_known_statically", "=", "(", "tensorshape_util", ".", "rank", "(", "params", ".", "shape", ")", "is", "not", "None", "and", "tensorshape_util", ".", "is_fully_defined", "(", "params", ".", "shape", "[", ":", "-", "1", "]", ")", "and", "tensorshape_util", ".", "is_fully_defined", "(", "event", ".", "shape", ")", ")", "if", "not", "shape_known_statically", "or", "params", ".", "shape", "[", ":", "-", "1", "]", "!=", "event", ".", "shape", ":", "params", "*=", "tf", ".", "ones_like", "(", "event", "[", "...", ",", "tf", ".", "newaxis", "]", ",", "dtype", "=", "params", ".", "dtype", ")", "params_shape", "=", "tf", ".", "shape", "(", "input", "=", "params", ")", "[", ":", "-", "1", "]", "event", "*=", "tf", ".", "ones", "(", "params_shape", ",", "dtype", "=", "event", ".", "dtype", ")", "if", "tensorshape_util", ".", "rank", "(", "params", ".", "shape", ")", "is", "not", "None", ":", "tensorshape_util", ".", "set_shape", "(", "event", ",", "params", ".", "shape", "[", ":", "-", "1", "]", ")", "return", "event", ",", "params"], "docstring": "Broadcasts the event or distribution parameters.", "docstring_tokens": ["Broadcasts", "the", "event", "or", "distribution", "parameters", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/categorical.py#L33-L56", "partition": "test", "index": 1058, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/uniform.py", "func_name": "Uniform.range", "original_string": "def range(self, name=\"range\"):\n    \"\"\"`high - low`.\"\"\"\n    with self._name_scope(name):\n      return self.high - self.low", "language": "python", "code": "def range(self, name=\"range\"):\n    \"\"\"`high - low`.\"\"\"\n    with self._name_scope(name):\n      return self.high - self.low", "code_tokens": ["def", "range", "(", "self", ",", "name", "=", "\"range\"", ")", ":", "with", "self", ".", "_name_scope", "(", "name", ")", ":", "return", "self", ".", "high", "-", "self", ".", "low"], "docstring": "`high - low`.", "docstring_tokens": ["high", "-", "low", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/uniform.py#L144-L147", "partition": "test", "index": 710, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/transformed_distribution.py", "func_name": "_pick_scalar_condition", "original_string": "def _pick_scalar_condition(pred, cond_true, cond_false):\n  \"\"\"Convenience function which chooses the condition based on the predicate.\"\"\"\n  # Note: This function is only valid if all of pred, cond_true, and cond_false\n  # are scalars. This means its semantics are arguably more like tf.cond than\n  # tf.where even though we use tf.where to implement it.\n  pred_ = tf.get_static_value(tf.convert_to_tensor(value=pred))\n  if pred_ is None:\n    return tf.where(pred, cond_true, cond_false)\n  return cond_true if pred_ else cond_false", "language": "python", "code": "def _pick_scalar_condition(pred, cond_true, cond_false):\n  \"\"\"Convenience function which chooses the condition based on the predicate.\"\"\"\n  # Note: This function is only valid if all of pred, cond_true, and cond_false\n  # are scalars. This means its semantics are arguably more like tf.cond than\n  # tf.where even though we use tf.where to implement it.\n  pred_ = tf.get_static_value(tf.convert_to_tensor(value=pred))\n  if pred_ is None:\n    return tf.where(pred, cond_true, cond_false)\n  return cond_true if pred_ else cond_false", "code_tokens": ["def", "_pick_scalar_condition", "(", "pred", ",", "cond_true", ",", "cond_false", ")", ":", "# Note: This function is only valid if all of pred, cond_true, and cond_false", "# are scalars. This means its semantics are arguably more like tf.cond than", "# tf.where even though we use tf.where to implement it.", "pred_", "=", "tf", ".", "get_static_value", "(", "tf", ".", "convert_to_tensor", "(", "value", "=", "pred", ")", ")", "if", "pred_", "is", "None", ":", "return", "tf", ".", "where", "(", "pred", ",", "cond_true", ",", "cond_false", ")", "return", "cond_true", "if", "pred_", "else", "cond_false"], "docstring": "Convenience function which chooses the condition based on the predicate.", "docstring_tokens": ["Convenience", "function", "which", "chooses", "the", "condition", "based", "on", "the", "predicate", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/transformed_distribution.py#L41-L49", "partition": "test", "index": 881, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/transformed_distribution.py", "func_name": "TransformedDistribution._finish_prob_for_one_fiber", "original_string": "def _finish_prob_for_one_fiber(self, y, x, ildj, event_ndims,\n                                 **distribution_kwargs):\n    \"\"\"Finish computation of prob on one element of the inverse image.\"\"\"\n    x = self._maybe_rotate_dims(x, rotate_right=True)\n    prob = self.distribution.prob(x, **distribution_kwargs)\n    if self._is_maybe_event_override:\n      prob = tf.reduce_prod(input_tensor=prob, axis=self._reduce_event_indices)\n    prob *= tf.exp(tf.cast(ildj, prob.dtype))\n    if self._is_maybe_event_override and isinstance(event_ndims, int):\n      tensorshape_util.set_shape(\n          prob,\n          tf.broadcast_static_shape(\n              tensorshape_util.with_rank_at_least(y.shape, 1)[:-event_ndims],\n              self.batch_shape))\n    return prob", "language": "python", "code": "def _finish_prob_for_one_fiber(self, y, x, ildj, event_ndims,\n                                 **distribution_kwargs):\n    \"\"\"Finish computation of prob on one element of the inverse image.\"\"\"\n    x = self._maybe_rotate_dims(x, rotate_right=True)\n    prob = self.distribution.prob(x, **distribution_kwargs)\n    if self._is_maybe_event_override:\n      prob = tf.reduce_prod(input_tensor=prob, axis=self._reduce_event_indices)\n    prob *= tf.exp(tf.cast(ildj, prob.dtype))\n    if self._is_maybe_event_override and isinstance(event_ndims, int):\n      tensorshape_util.set_shape(\n          prob,\n          tf.broadcast_static_shape(\n              tensorshape_util.with_rank_at_least(y.shape, 1)[:-event_ndims],\n              self.batch_shape))\n    return prob", "code_tokens": ["def", "_finish_prob_for_one_fiber", "(", "self", ",", "y", ",", "x", ",", "ildj", ",", "event_ndims", ",", "*", "*", "distribution_kwargs", ")", ":", "x", "=", "self", ".", "_maybe_rotate_dims", "(", "x", ",", "rotate_right", "=", "True", ")", "prob", "=", "self", ".", "distribution", ".", "prob", "(", "x", ",", "*", "*", "distribution_kwargs", ")", "if", "self", ".", "_is_maybe_event_override", ":", "prob", "=", "tf", ".", "reduce_prod", "(", "input_tensor", "=", "prob", ",", "axis", "=", "self", ".", "_reduce_event_indices", ")", "prob", "*=", "tf", ".", "exp", "(", "tf", ".", "cast", "(", "ildj", ",", "prob", ".", "dtype", ")", ")", "if", "self", ".", "_is_maybe_event_override", "and", "isinstance", "(", "event_ndims", ",", "int", ")", ":", "tensorshape_util", ".", "set_shape", "(", "prob", ",", "tf", ".", "broadcast_static_shape", "(", "tensorshape_util", ".", "with_rank_at_least", "(", "y", ".", "shape", ",", "1", ")", "[", ":", "-", "event_ndims", "]", ",", "self", ".", "batch_shape", ")", ")", "return", "prob"], "docstring": "Finish computation of prob on one element of the inverse image.", "docstring_tokens": ["Finish", "computation", "of", "prob", "on", "one", "element", "of", "the", "inverse", "image", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/transformed_distribution.py#L451-L465", "partition": "test", "index": 883, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/transformed_distribution.py", "func_name": "TransformedDistribution._maybe_rotate_dims", "original_string": "def _maybe_rotate_dims(self, x, rotate_right=False):\n    \"\"\"Helper which rolls left event_dims left or right event_dims right.\"\"\"\n    needs_rotation_const = tf.get_static_value(self._needs_rotation)\n    if needs_rotation_const is not None and not needs_rotation_const:\n      return x\n    ndims = prefer_static.rank(x)\n    n = (ndims - self._rotate_ndims) if rotate_right else self._rotate_ndims\n    perm = prefer_static.concat([\n        prefer_static.range(n, ndims), prefer_static.range(0, n)], axis=0)\n    return tf.transpose(a=x, perm=perm)", "language": "python", "code": "def _maybe_rotate_dims(self, x, rotate_right=False):\n    \"\"\"Helper which rolls left event_dims left or right event_dims right.\"\"\"\n    needs_rotation_const = tf.get_static_value(self._needs_rotation)\n    if needs_rotation_const is not None and not needs_rotation_const:\n      return x\n    ndims = prefer_static.rank(x)\n    n = (ndims - self._rotate_ndims) if rotate_right else self._rotate_ndims\n    perm = prefer_static.concat([\n        prefer_static.range(n, ndims), prefer_static.range(0, n)], axis=0)\n    return tf.transpose(a=x, perm=perm)", "code_tokens": ["def", "_maybe_rotate_dims", "(", "self", ",", "x", ",", "rotate_right", "=", "False", ")", ":", "needs_rotation_const", "=", "tf", ".", "get_static_value", "(", "self", ".", "_needs_rotation", ")", "if", "needs_rotation_const", "is", "not", "None", "and", "not", "needs_rotation_const", ":", "return", "x", "ndims", "=", "prefer_static", ".", "rank", "(", "x", ")", "n", "=", "(", "ndims", "-", "self", ".", "_rotate_ndims", ")", "if", "rotate_right", "else", "self", ".", "_rotate_ndims", "perm", "=", "prefer_static", ".", "concat", "(", "[", "prefer_static", ".", "range", "(", "n", ",", "ndims", ")", ",", "prefer_static", ".", "range", "(", "0", ",", "n", ")", "]", ",", "axis", "=", "0", ")", "return", "tf", ".", "transpose", "(", "a", "=", "x", ",", "perm", "=", "perm", ")"], "docstring": "Helper which rolls left event_dims left or right event_dims right.", "docstring_tokens": ["Helper", "which", "rolls", "left", "event_dims", "left", "or", "right", "event_dims", "right", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/transformed_distribution.py#L657-L666", "partition": "test", "index": 884, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "embed_check_nonnegative_integer_form", "original_string": "def embed_check_nonnegative_integer_form(\n    x, name=\"embed_check_nonnegative_integer_form\"):\n  \"\"\"Assert x is a non-negative tensor, and optionally of integers.\"\"\"\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    assertions = [\n        assert_util.assert_non_negative(\n            x, message=\"'{}' must be non-negative.\".format(x)),\n    ]\n    if not dtype_util.is_integer(x.dtype):\n      assertions += [\n          assert_integer_form(\n              x,\n              message=\"'{}' cannot contain fractional components.\".format(x)),\n      ]\n    return with_dependencies(assertions, x)", "language": "python", "code": "def embed_check_nonnegative_integer_form(\n    x, name=\"embed_check_nonnegative_integer_form\"):\n  \"\"\"Assert x is a non-negative tensor, and optionally of integers.\"\"\"\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    assertions = [\n        assert_util.assert_non_negative(\n            x, message=\"'{}' must be non-negative.\".format(x)),\n    ]\n    if not dtype_util.is_integer(x.dtype):\n      assertions += [\n          assert_integer_form(\n              x,\n              message=\"'{}' cannot contain fractional components.\".format(x)),\n      ]\n    return with_dependencies(assertions, x)", "code_tokens": ["def", "embed_check_nonnegative_integer_form", "(", "x", ",", "name", "=", "\"embed_check_nonnegative_integer_form\"", ")", ":", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "\"x\"", ")", "assertions", "=", "[", "assert_util", ".", "assert_non_negative", "(", "x", ",", "message", "=", "\"'{}' must be non-negative.\"", ".", "format", "(", "x", ")", ")", ",", "]", "if", "not", "dtype_util", ".", "is_integer", "(", "x", ".", "dtype", ")", ":", "assertions", "+=", "[", "assert_integer_form", "(", "x", ",", "message", "=", "\"'{}' cannot contain fractional components.\"", ".", "format", "(", "x", ")", ")", ",", "]", "return", "with_dependencies", "(", "assertions", ",", "x", ")"], "docstring": "Assert x is a non-negative tensor, and optionally of integers.", "docstring_tokens": ["Assert", "x", "is", "a", "non", "-", "negative", "tensor", "and", "optionally", "of", "integers", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L692-L707", "partition": "test", "index": 911, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/transformed_distribution.py", "func_name": "TransformedDistribution._finish_log_prob_for_one_fiber", "original_string": "def _finish_log_prob_for_one_fiber(self, y, x, ildj, event_ndims,\n                                     **distribution_kwargs):\n    \"\"\"Finish computation of log_prob on one element of the inverse image.\"\"\"\n    x = self._maybe_rotate_dims(x, rotate_right=True)\n    log_prob = self.distribution.log_prob(x, **distribution_kwargs)\n    if self._is_maybe_event_override:\n      log_prob = tf.reduce_sum(\n          input_tensor=log_prob, axis=self._reduce_event_indices)\n    log_prob += tf.cast(ildj, log_prob.dtype)\n    if self._is_maybe_event_override and isinstance(event_ndims, int):\n      tensorshape_util.set_shape(\n          log_prob,\n          tf.broadcast_static_shape(\n              tensorshape_util.with_rank_at_least(y.shape, 1)[:-event_ndims],\n              self.batch_shape))\n    return log_prob", "language": "python", "code": "def _finish_log_prob_for_one_fiber(self, y, x, ildj, event_ndims,\n                                     **distribution_kwargs):\n    \"\"\"Finish computation of log_prob on one element of the inverse image.\"\"\"\n    x = self._maybe_rotate_dims(x, rotate_right=True)\n    log_prob = self.distribution.log_prob(x, **distribution_kwargs)\n    if self._is_maybe_event_override:\n      log_prob = tf.reduce_sum(\n          input_tensor=log_prob, axis=self._reduce_event_indices)\n    log_prob += tf.cast(ildj, log_prob.dtype)\n    if self._is_maybe_event_override and isinstance(event_ndims, int):\n      tensorshape_util.set_shape(\n          log_prob,\n          tf.broadcast_static_shape(\n              tensorshape_util.with_rank_at_least(y.shape, 1)[:-event_ndims],\n              self.batch_shape))\n    return log_prob", "code_tokens": ["def", "_finish_log_prob_for_one_fiber", "(", "self", ",", "y", ",", "x", ",", "ildj", ",", "event_ndims", ",", "*", "*", "distribution_kwargs", ")", ":", "x", "=", "self", ".", "_maybe_rotate_dims", "(", "x", ",", "rotate_right", "=", "True", ")", "log_prob", "=", "self", ".", "distribution", ".", "log_prob", "(", "x", ",", "*", "*", "distribution_kwargs", ")", "if", "self", ".", "_is_maybe_event_override", ":", "log_prob", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "log_prob", ",", "axis", "=", "self", ".", "_reduce_event_indices", ")", "log_prob", "+=", "tf", ".", "cast", "(", "ildj", ",", "log_prob", ".", "dtype", ")", "if", "self", ".", "_is_maybe_event_override", "and", "isinstance", "(", "event_ndims", ",", "int", ")", ":", "tensorshape_util", ".", "set_shape", "(", "log_prob", ",", "tf", ".", "broadcast_static_shape", "(", "tensorshape_util", ".", "with_rank_at_least", "(", "y", ".", "shape", ",", "1", ")", "[", ":", "-", "event_ndims", "]", ",", "self", ".", "batch_shape", ")", ")", "return", "log_prob"], "docstring": "Finish computation of log_prob on one element of the inverse image.", "docstring_tokens": ["Finish", "computation", "of", "log_prob", "on", "one", "element", "of", "the", "inverse", "image", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/transformed_distribution.py#L415-L430", "partition": "test", "index": 882, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/kullback_leibler.py", "func_name": "_registered_kl", "original_string": "def _registered_kl(type_a, type_b):\n  \"\"\"Get the KL function registered for classes a and b.\"\"\"\n  hierarchy_a = tf_inspect.getmro(type_a)\n  hierarchy_b = tf_inspect.getmro(type_b)\n  dist_to_children = None\n  kl_fn = None\n  for mro_to_a, parent_a in enumerate(hierarchy_a):\n    for mro_to_b, parent_b in enumerate(hierarchy_b):\n      candidate_dist = mro_to_a + mro_to_b\n      candidate_kl_fn = _DIVERGENCES.get((parent_a, parent_b), None)\n      if not kl_fn or (candidate_kl_fn and candidate_dist < dist_to_children):\n        dist_to_children = candidate_dist\n        kl_fn = candidate_kl_fn\n  return kl_fn", "language": "python", "code": "def _registered_kl(type_a, type_b):\n  \"\"\"Get the KL function registered for classes a and b.\"\"\"\n  hierarchy_a = tf_inspect.getmro(type_a)\n  hierarchy_b = tf_inspect.getmro(type_b)\n  dist_to_children = None\n  kl_fn = None\n  for mro_to_a, parent_a in enumerate(hierarchy_a):\n    for mro_to_b, parent_b in enumerate(hierarchy_b):\n      candidate_dist = mro_to_a + mro_to_b\n      candidate_kl_fn = _DIVERGENCES.get((parent_a, parent_b), None)\n      if not kl_fn or (candidate_kl_fn and candidate_dist < dist_to_children):\n        dist_to_children = candidate_dist\n        kl_fn = candidate_kl_fn\n  return kl_fn", "code_tokens": ["def", "_registered_kl", "(", "type_a", ",", "type_b", ")", ":", "hierarchy_a", "=", "tf_inspect", ".", "getmro", "(", "type_a", ")", "hierarchy_b", "=", "tf_inspect", ".", "getmro", "(", "type_b", ")", "dist_to_children", "=", "None", "kl_fn", "=", "None", "for", "mro_to_a", ",", "parent_a", "in", "enumerate", "(", "hierarchy_a", ")", ":", "for", "mro_to_b", ",", "parent_b", "in", "enumerate", "(", "hierarchy_b", ")", ":", "candidate_dist", "=", "mro_to_a", "+", "mro_to_b", "candidate_kl_fn", "=", "_DIVERGENCES", ".", "get", "(", "(", "parent_a", ",", "parent_b", ")", ",", "None", ")", "if", "not", "kl_fn", "or", "(", "candidate_kl_fn", "and", "candidate_dist", "<", "dist_to_children", ")", ":", "dist_to_children", "=", "candidate_dist", "kl_fn", "=", "candidate_kl_fn", "return", "kl_fn"], "docstring": "Get the KL function registered for classes a and b.", "docstring_tokens": ["Get", "the", "KL", "function", "registered", "for", "classes", "a", "and", "b", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/kullback_leibler.py#L34-L47", "partition": "test", "index": 1021, "time": "2018-10-02 12:47:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/special_math.py", "func_name": "log_cdf_laplace", "original_string": "def log_cdf_laplace(x, name=\"log_cdf_laplace\"):\n  \"\"\"Log Laplace distribution function.\n\n  This function calculates `Log[L(x)]`, where `L(x)` is the cumulative\n  distribution function of the Laplace distribution, i.e.\n\n  ```L(x) := 0.5 * int_{-infty}^x e^{-|t|} dt```\n\n  For numerical accuracy, `L(x)` is computed in different ways depending on `x`,\n\n  ```\n  x <= 0:\n    Log[L(x)] = Log[0.5] + x, which is exact\n\n  0 < x:\n    Log[L(x)] = Log[1 - 0.5 * e^{-x}], which is exact\n  ```\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"log_ndtr\").\n\n  Returns:\n    `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x.dtype` is not handled.\n  \"\"\"\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n\n    # For x < 0, L(x) = 0.5 * exp{x} exactly, so Log[L(x)] = log(0.5) + x.\n    lower_solution = -np.log(2.) + x\n\n    # safe_exp_neg_x = exp{-x} for x > 0, but is\n    # bounded above by 1, which avoids\n    #   log[1 - 1] = -inf for x = log(1/2), AND\n    #   exp{-x} --> inf, for x << -1\n    safe_exp_neg_x = tf.exp(-tf.abs(x))\n\n    # log1p(z) = log(1 + z) approx z for |z| << 1. This approxmation is used\n    # internally by log1p, rather than being done explicitly here.\n    upper_solution = tf.math.log1p(-0.5 * safe_exp_neg_x)\n\n    return tf.where(x < 0., lower_solution, upper_solution)", "language": "python", "code": "def log_cdf_laplace(x, name=\"log_cdf_laplace\"):\n  \"\"\"Log Laplace distribution function.\n\n  This function calculates `Log[L(x)]`, where `L(x)` is the cumulative\n  distribution function of the Laplace distribution, i.e.\n\n  ```L(x) := 0.5 * int_{-infty}^x e^{-|t|} dt```\n\n  For numerical accuracy, `L(x)` is computed in different ways depending on `x`,\n\n  ```\n  x <= 0:\n    Log[L(x)] = Log[0.5] + x, which is exact\n\n  0 < x:\n    Log[L(x)] = Log[1 - 0.5 * e^{-x}], which is exact\n  ```\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"log_ndtr\").\n\n  Returns:\n    `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x.dtype` is not handled.\n  \"\"\"\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n\n    # For x < 0, L(x) = 0.5 * exp{x} exactly, so Log[L(x)] = log(0.5) + x.\n    lower_solution = -np.log(2.) + x\n\n    # safe_exp_neg_x = exp{-x} for x > 0, but is\n    # bounded above by 1, which avoids\n    #   log[1 - 1] = -inf for x = log(1/2), AND\n    #   exp{-x} --> inf, for x << -1\n    safe_exp_neg_x = tf.exp(-tf.abs(x))\n\n    # log1p(z) = log(1 + z) approx z for |z| << 1. This approxmation is used\n    # internally by log1p, rather than being done explicitly here.\n    upper_solution = tf.math.log1p(-0.5 * safe_exp_neg_x)\n\n    return tf.where(x < 0., lower_solution, upper_solution)", "code_tokens": ["def", "log_cdf_laplace", "(", "x", ",", "name", "=", "\"log_cdf_laplace\"", ")", ":", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "\"x\"", ")", "# For x < 0, L(x) = 0.5 * exp{x} exactly, so Log[L(x)] = log(0.5) + x.", "lower_solution", "=", "-", "np", ".", "log", "(", "2.", ")", "+", "x", "# safe_exp_neg_x = exp{-x} for x > 0, but is", "# bounded above by 1, which avoids", "#   log[1 - 1] = -inf for x = log(1/2), AND", "#   exp{-x} --> inf, for x << -1", "safe_exp_neg_x", "=", "tf", ".", "exp", "(", "-", "tf", ".", "abs", "(", "x", ")", ")", "# log1p(z) = log(1 + z) approx z for |z| << 1. This approxmation is used", "# internally by log1p, rather than being done explicitly here.", "upper_solution", "=", "tf", ".", "math", ".", "log1p", "(", "-", "0.5", "*", "safe_exp_neg_x", ")", "return", "tf", ".", "where", "(", "x", "<", "0.", ",", "lower_solution", ",", "upper_solution", ")"], "docstring": "Log Laplace distribution function.\n\n  This function calculates `Log[L(x)]`, where `L(x)` is the cumulative\n  distribution function of the Laplace distribution, i.e.\n\n  ```L(x) := 0.5 * int_{-infty}^x e^{-|t|} dt```\n\n  For numerical accuracy, `L(x)` is computed in different ways depending on `x`,\n\n  ```\n  x <= 0:\n    Log[L(x)] = Log[0.5] + x, which is exact\n\n  0 < x:\n    Log[L(x)] = Log[1 - 0.5 * e^{-x}], which is exact\n  ```\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"log_ndtr\").\n\n  Returns:\n    `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x.dtype` is not handled.", "docstring_tokens": ["Log", "Laplace", "distribution", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/special_math.py#L437-L482", "partition": "test", "index": 747, "time": "2018-10-15 10:00:17"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/special_math.py", "func_name": "erfinv", "original_string": "def erfinv(x, name=\"erfinv\"):\n  \"\"\"The inverse function for erf, the error function.\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"erfinv\").\n\n  Returns:\n    x: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x` is not floating-type.\n  \"\"\"\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:\n      raise TypeError(\"x.dtype={} is not handled, see docstring for supported \"\n                      \"types.\".format(dtype_util.name(x.dtype)))\n    return ndtri((x + 1.) / 2.) / np.sqrt(2.)", "language": "python", "code": "def erfinv(x, name=\"erfinv\"):\n  \"\"\"The inverse function for erf, the error function.\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"erfinv\").\n\n  Returns:\n    x: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x` is not floating-type.\n  \"\"\"\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:\n      raise TypeError(\"x.dtype={} is not handled, see docstring for supported \"\n                      \"types.\".format(dtype_util.name(x.dtype)))\n    return ndtri((x + 1.) / 2.) / np.sqrt(2.)", "code_tokens": ["def", "erfinv", "(", "x", ",", "name", "=", "\"erfinv\"", ")", ":", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "\"x\"", ")", "if", "dtype_util", ".", "as_numpy_dtype", "(", "x", ".", "dtype", ")", "not", "in", "[", "np", ".", "float32", ",", "np", ".", "float64", "]", ":", "raise", "TypeError", "(", "\"x.dtype={} is not handled, see docstring for supported \"", "\"types.\"", ".", "format", "(", "dtype_util", ".", "name", "(", "x", ".", "dtype", ")", ")", ")", "return", "ndtri", "(", "(", "x", "+", "1.", ")", "/", "2.", ")", "/", "np", ".", "sqrt", "(", "2.", ")"], "docstring": "The inverse function for erf, the error function.\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"erfinv\").\n\n  Returns:\n    x: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x` is not floating-type.", "docstring_tokens": ["The", "inverse", "function", "for", "erf", "the", "error", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/special_math.py#L410-L429", "partition": "test", "index": 746, "time": "2018-10-15 10:00:17"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/special_math.py", "func_name": "_ndtr", "original_string": "def _ndtr(x):\n  \"\"\"Implements ndtr core logic.\"\"\"\n  half_sqrt_2 = tf.constant(\n      0.5 * np.sqrt(2.), dtype=x.dtype, name=\"half_sqrt_2\")\n  w = x * half_sqrt_2\n  z = tf.abs(w)\n  y = tf.where(\n      tf.less(z, half_sqrt_2), 1. + tf.math.erf(w),\n      tf.where(tf.greater(w, 0.), 2. - tf.math.erfc(z), tf.math.erfc(z)))\n  return 0.5 * y", "language": "python", "code": "def _ndtr(x):\n  \"\"\"Implements ndtr core logic.\"\"\"\n  half_sqrt_2 = tf.constant(\n      0.5 * np.sqrt(2.), dtype=x.dtype, name=\"half_sqrt_2\")\n  w = x * half_sqrt_2\n  z = tf.abs(w)\n  y = tf.where(\n      tf.less(z, half_sqrt_2), 1. + tf.math.erf(w),\n      tf.where(tf.greater(w, 0.), 2. - tf.math.erfc(z), tf.math.erfc(z)))\n  return 0.5 * y", "code_tokens": ["def", "_ndtr", "(", "x", ")", ":", "half_sqrt_2", "=", "tf", ".", "constant", "(", "0.5", "*", "np", ".", "sqrt", "(", "2.", ")", ",", "dtype", "=", "x", ".", "dtype", ",", "name", "=", "\"half_sqrt_2\"", ")", "w", "=", "x", "*", "half_sqrt_2", "z", "=", "tf", ".", "abs", "(", "w", ")", "y", "=", "tf", ".", "where", "(", "tf", ".", "less", "(", "z", ",", "half_sqrt_2", ")", ",", "1.", "+", "tf", ".", "math", ".", "erf", "(", "w", ")", ",", "tf", ".", "where", "(", "tf", ".", "greater", "(", "w", ",", "0.", ")", ",", "2.", "-", "tf", ".", "math", ".", "erfc", "(", "z", ")", ",", "tf", ".", "math", ".", "erfc", "(", "z", ")", ")", ")", "return", "0.5", "*", "y"], "docstring": "Implements ndtr core logic.", "docstring_tokens": ["Implements", "ndtr", "core", "logic", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/special_math.py#L145-L154", "partition": "test", "index": 742, "time": "2018-10-15 10:00:17"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/special_math.py", "func_name": "ndtr", "original_string": "def ndtr(x, name=\"ndtr\"):\n  \"\"\"Normal distribution function.\n\n  Returns the area under the Gaussian probability density function, integrated\n  from minus infinity to x:\n\n  ```\n                    1       / x\n     ndtr(x)  = ----------  |    exp(-0.5 t**2) dt\n                sqrt(2 pi)  /-inf\n\n              = 0.5 (1 + erf(x / sqrt(2)))\n              = 0.5 erfc(x / sqrt(2))\n  ```\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"ndtr\").\n\n  Returns:\n    ndtr: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x` is not floating-type.\n  \"\"\"\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:\n      raise TypeError(\n          \"x.dtype=%s is not handled, see docstring for supported types.\"\n          % x.dtype)\n    return _ndtr(x)", "language": "python", "code": "def ndtr(x, name=\"ndtr\"):\n  \"\"\"Normal distribution function.\n\n  Returns the area under the Gaussian probability density function, integrated\n  from minus infinity to x:\n\n  ```\n                    1       / x\n     ndtr(x)  = ----------  |    exp(-0.5 t**2) dt\n                sqrt(2 pi)  /-inf\n\n              = 0.5 (1 + erf(x / sqrt(2)))\n              = 0.5 erfc(x / sqrt(2))\n  ```\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"ndtr\").\n\n  Returns:\n    ndtr: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x` is not floating-type.\n  \"\"\"\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:\n      raise TypeError(\n          \"x.dtype=%s is not handled, see docstring for supported types.\"\n          % x.dtype)\n    return _ndtr(x)", "code_tokens": ["def", "ndtr", "(", "x", ",", "name", "=", "\"ndtr\"", ")", ":", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "\"x\"", ")", "if", "dtype_util", ".", "as_numpy_dtype", "(", "x", ".", "dtype", ")", "not", "in", "[", "np", ".", "float32", ",", "np", ".", "float64", "]", ":", "raise", "TypeError", "(", "\"x.dtype=%s is not handled, see docstring for supported types.\"", "%", "x", ".", "dtype", ")", "return", "_ndtr", "(", "x", ")"], "docstring": "Normal distribution function.\n\n  Returns the area under the Gaussian probability density function, integrated\n  from minus infinity to x:\n\n  ```\n                    1       / x\n     ndtr(x)  = ----------  |    exp(-0.5 t**2) dt\n                sqrt(2 pi)  /-inf\n\n              = 0.5 (1 + erf(x / sqrt(2)))\n              = 0.5 erfc(x / sqrt(2))\n  ```\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"ndtr\").\n\n  Returns:\n    ndtr: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x` is not floating-type.", "docstring_tokens": ["Normal", "distribution", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/special_math.py#L110-L142", "partition": "test", "index": 741, "time": "2018-10-15 10:00:17"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/special_math.py", "func_name": "ndtri", "original_string": "def ndtri(p, name=\"ndtri\"):\n  \"\"\"The inverse of the CDF of the Normal distribution function.\n\n  Returns x such that the area under the pdf from minus infinity to x is equal\n  to p.\n\n  A piece-wise rational approximation is done for the function.\n  This is a port of the implementation in netlib.\n\n  Args:\n    p: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"ndtri\").\n\n  Returns:\n    x: `Tensor` with `dtype=p.dtype`.\n\n  Raises:\n    TypeError: if `p` is not floating-type.\n  \"\"\"\n\n  with tf.name_scope(name):\n    p = tf.convert_to_tensor(value=p, name=\"p\")\n    if dtype_util.as_numpy_dtype(p.dtype) not in [np.float32, np.float64]:\n      raise TypeError(\n          \"p.dtype=%s is not handled, see docstring for supported types.\"\n          % p.dtype)\n    return _ndtri(p)", "language": "python", "code": "def ndtri(p, name=\"ndtri\"):\n  \"\"\"The inverse of the CDF of the Normal distribution function.\n\n  Returns x such that the area under the pdf from minus infinity to x is equal\n  to p.\n\n  A piece-wise rational approximation is done for the function.\n  This is a port of the implementation in netlib.\n\n  Args:\n    p: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"ndtri\").\n\n  Returns:\n    x: `Tensor` with `dtype=p.dtype`.\n\n  Raises:\n    TypeError: if `p` is not floating-type.\n  \"\"\"\n\n  with tf.name_scope(name):\n    p = tf.convert_to_tensor(value=p, name=\"p\")\n    if dtype_util.as_numpy_dtype(p.dtype) not in [np.float32, np.float64]:\n      raise TypeError(\n          \"p.dtype=%s is not handled, see docstring for supported types.\"\n          % p.dtype)\n    return _ndtri(p)", "code_tokens": ["def", "ndtri", "(", "p", ",", "name", "=", "\"ndtri\"", ")", ":", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "p", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "p", ",", "name", "=", "\"p\"", ")", "if", "dtype_util", ".", "as_numpy_dtype", "(", "p", ".", "dtype", ")", "not", "in", "[", "np", ".", "float32", ",", "np", ".", "float64", "]", ":", "raise", "TypeError", "(", "\"p.dtype=%s is not handled, see docstring for supported types.\"", "%", "p", ".", "dtype", ")", "return", "_ndtri", "(", "p", ")"], "docstring": "The inverse of the CDF of the Normal distribution function.\n\n  Returns x such that the area under the pdf from minus infinity to x is equal\n  to p.\n\n  A piece-wise rational approximation is done for the function.\n  This is a port of the implementation in netlib.\n\n  Args:\n    p: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"ndtri\").\n\n  Returns:\n    x: `Tensor` with `dtype=p.dtype`.\n\n  Raises:\n    TypeError: if `p` is not floating-type.", "docstring_tokens": ["The", "inverse", "of", "the", "CDF", "of", "the", "Normal", "distribution", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/special_math.py#L157-L183", "partition": "test", "index": 743, "time": "2018-10-15 10:00:17"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/special_math.py", "func_name": "log_ndtr", "original_string": "def log_ndtr(x, series_order=3, name=\"log_ndtr\"):\n  \"\"\"Log Normal distribution function.\n\n  For details of the Normal distribution function see `ndtr`.\n\n  This function calculates `(log o ndtr)(x)` by either calling `log(ndtr(x))` or\n  using an asymptotic series. Specifically:\n  - For `x > upper_segment`, use the approximation `-ndtr(-x)` based on\n    `log(1-x) ~= -x, x << 1`.\n  - For `lower_segment < x <= upper_segment`, use the existing `ndtr` technique\n    and take a log.\n  - For `x <= lower_segment`, we use the series approximation of erf to compute\n    the log CDF directly.\n\n  The `lower_segment` is set based on the precision of the input:\n\n  ```\n  lower_segment = { -20,  x.dtype=float64\n                  { -10,  x.dtype=float32\n  upper_segment = {   8,  x.dtype=float64\n                  {   5,  x.dtype=float32\n  ```\n\n  When `x < lower_segment`, the `ndtr` asymptotic series approximation is:\n\n  ```\n     ndtr(x) = scale * (1 + sum) + R_N\n     scale   = exp(-0.5 x**2) / (-x sqrt(2 pi))\n     sum     = Sum{(-1)^n (2n-1)!! / (x**2)^n, n=1:N}\n     R_N     = O(exp(-0.5 x**2) (2N+1)!! / |x|^{2N+3})\n  ```\n\n  where `(2n-1)!! = (2n-1) (2n-3) (2n-5) ...  (3) (1)` is a\n  [double-factorial](https://en.wikipedia.org/wiki/Double_factorial).\n\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    series_order: Positive Python `integer`. Maximum depth to\n      evaluate the asymptotic expansion. This is the `N` above.\n    name: Python string. A name for the operation (default=\"log_ndtr\").\n\n  Returns:\n    log_ndtr: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x.dtype` is not handled.\n    TypeError: if `series_order` is a not Python `integer.`\n    ValueError:  if `series_order` is not in `[0, 30]`.\n  \"\"\"\n  if not isinstance(series_order, int):\n    raise TypeError(\"series_order must be a Python integer.\")\n  if series_order < 0:\n    raise ValueError(\"series_order must be non-negative.\")\n  if series_order > 30:\n    raise ValueError(\"series_order must be <= 30.\")\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n\n    if dtype_util.base_equal(x.dtype, tf.float64):\n      lower_segment = LOGNDTR_FLOAT64_LOWER\n      upper_segment = LOGNDTR_FLOAT64_UPPER\n    elif dtype_util.base_equal(x.dtype, tf.float32):\n      lower_segment = LOGNDTR_FLOAT32_LOWER\n      upper_segment = LOGNDTR_FLOAT32_UPPER\n    else:\n      raise TypeError(\"x.dtype=%s is not supported.\" % x.dtype)\n\n    # The basic idea here was ported from:\n    #   https://root.cern.ch/doc/v608/SpecFuncCephesInv_8cxx_source.html\n    # We copy the main idea, with a few changes\n    # * For x >> 1, and X ~ Normal(0, 1),\n    #     Log[P[X < x]] = Log[1 - P[X < -x]] approx -P[X < -x],\n    #     which extends the range of validity of this function.\n    # * We use one fixed series_order for all of 'x', rather than adaptive.\n    # * Our docstring properly reflects that this is an asymptotic series, not a\n    #   Taylor series. We also provided a correct bound on the remainder.\n    # * We need to use the max/min in the _log_ndtr_lower arg to avoid nan when\n    #   x=0. This happens even though the branch is unchosen because when x=0\n    #   the gradient of a select involves the calculation 1*dy+0*(-inf)=nan\n    #   regardless of whether dy is finite. Note that the minimum is a NOP if\n    #   the branch is chosen.\n    return tf.where(\n        tf.greater(x, upper_segment),\n        -_ndtr(-x),  # log(1-x) ~= -x, x << 1\n        tf.where(\n            tf.greater(x, lower_segment),\n            tf.math.log(_ndtr(tf.maximum(x, lower_segment))),\n            _log_ndtr_lower(tf.minimum(x, lower_segment), series_order)))", "language": "python", "code": "def log_ndtr(x, series_order=3, name=\"log_ndtr\"):\n  \"\"\"Log Normal distribution function.\n\n  For details of the Normal distribution function see `ndtr`.\n\n  This function calculates `(log o ndtr)(x)` by either calling `log(ndtr(x))` or\n  using an asymptotic series. Specifically:\n  - For `x > upper_segment`, use the approximation `-ndtr(-x)` based on\n    `log(1-x) ~= -x, x << 1`.\n  - For `lower_segment < x <= upper_segment`, use the existing `ndtr` technique\n    and take a log.\n  - For `x <= lower_segment`, we use the series approximation of erf to compute\n    the log CDF directly.\n\n  The `lower_segment` is set based on the precision of the input:\n\n  ```\n  lower_segment = { -20,  x.dtype=float64\n                  { -10,  x.dtype=float32\n  upper_segment = {   8,  x.dtype=float64\n                  {   5,  x.dtype=float32\n  ```\n\n  When `x < lower_segment`, the `ndtr` asymptotic series approximation is:\n\n  ```\n     ndtr(x) = scale * (1 + sum) + R_N\n     scale   = exp(-0.5 x**2) / (-x sqrt(2 pi))\n     sum     = Sum{(-1)^n (2n-1)!! / (x**2)^n, n=1:N}\n     R_N     = O(exp(-0.5 x**2) (2N+1)!! / |x|^{2N+3})\n  ```\n\n  where `(2n-1)!! = (2n-1) (2n-3) (2n-5) ...  (3) (1)` is a\n  [double-factorial](https://en.wikipedia.org/wiki/Double_factorial).\n\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    series_order: Positive Python `integer`. Maximum depth to\n      evaluate the asymptotic expansion. This is the `N` above.\n    name: Python string. A name for the operation (default=\"log_ndtr\").\n\n  Returns:\n    log_ndtr: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x.dtype` is not handled.\n    TypeError: if `series_order` is a not Python `integer.`\n    ValueError:  if `series_order` is not in `[0, 30]`.\n  \"\"\"\n  if not isinstance(series_order, int):\n    raise TypeError(\"series_order must be a Python integer.\")\n  if series_order < 0:\n    raise ValueError(\"series_order must be non-negative.\")\n  if series_order > 30:\n    raise ValueError(\"series_order must be <= 30.\")\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n\n    if dtype_util.base_equal(x.dtype, tf.float64):\n      lower_segment = LOGNDTR_FLOAT64_LOWER\n      upper_segment = LOGNDTR_FLOAT64_UPPER\n    elif dtype_util.base_equal(x.dtype, tf.float32):\n      lower_segment = LOGNDTR_FLOAT32_LOWER\n      upper_segment = LOGNDTR_FLOAT32_UPPER\n    else:\n      raise TypeError(\"x.dtype=%s is not supported.\" % x.dtype)\n\n    # The basic idea here was ported from:\n    #   https://root.cern.ch/doc/v608/SpecFuncCephesInv_8cxx_source.html\n    # We copy the main idea, with a few changes\n    # * For x >> 1, and X ~ Normal(0, 1),\n    #     Log[P[X < x]] = Log[1 - P[X < -x]] approx -P[X < -x],\n    #     which extends the range of validity of this function.\n    # * We use one fixed series_order for all of 'x', rather than adaptive.\n    # * Our docstring properly reflects that this is an asymptotic series, not a\n    #   Taylor series. We also provided a correct bound on the remainder.\n    # * We need to use the max/min in the _log_ndtr_lower arg to avoid nan when\n    #   x=0. This happens even though the branch is unchosen because when x=0\n    #   the gradient of a select involves the calculation 1*dy+0*(-inf)=nan\n    #   regardless of whether dy is finite. Note that the minimum is a NOP if\n    #   the branch is chosen.\n    return tf.where(\n        tf.greater(x, upper_segment),\n        -_ndtr(-x),  # log(1-x) ~= -x, x << 1\n        tf.where(\n            tf.greater(x, lower_segment),\n            tf.math.log(_ndtr(tf.maximum(x, lower_segment))),\n            _log_ndtr_lower(tf.minimum(x, lower_segment), series_order)))", "code_tokens": ["def", "log_ndtr", "(", "x", ",", "series_order", "=", "3", ",", "name", "=", "\"log_ndtr\"", ")", ":", "if", "not", "isinstance", "(", "series_order", ",", "int", ")", ":", "raise", "TypeError", "(", "\"series_order must be a Python integer.\"", ")", "if", "series_order", "<", "0", ":", "raise", "ValueError", "(", "\"series_order must be non-negative.\"", ")", "if", "series_order", ">", "30", ":", "raise", "ValueError", "(", "\"series_order must be <= 30.\"", ")", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "\"x\"", ")", "if", "dtype_util", ".", "base_equal", "(", "x", ".", "dtype", ",", "tf", ".", "float64", ")", ":", "lower_segment", "=", "LOGNDTR_FLOAT64_LOWER", "upper_segment", "=", "LOGNDTR_FLOAT64_UPPER", "elif", "dtype_util", ".", "base_equal", "(", "x", ".", "dtype", ",", "tf", ".", "float32", ")", ":", "lower_segment", "=", "LOGNDTR_FLOAT32_LOWER", "upper_segment", "=", "LOGNDTR_FLOAT32_UPPER", "else", ":", "raise", "TypeError", "(", "\"x.dtype=%s is not supported.\"", "%", "x", ".", "dtype", ")", "# The basic idea here was ported from:", "#   https://root.cern.ch/doc/v608/SpecFuncCephesInv_8cxx_source.html", "# We copy the main idea, with a few changes", "# * For x >> 1, and X ~ Normal(0, 1),", "#     Log[P[X < x]] = Log[1 - P[X < -x]] approx -P[X < -x],", "#     which extends the range of validity of this function.", "# * We use one fixed series_order for all of 'x', rather than adaptive.", "# * Our docstring properly reflects that this is an asymptotic series, not a", "#   Taylor series. We also provided a correct bound on the remainder.", "# * We need to use the max/min in the _log_ndtr_lower arg to avoid nan when", "#   x=0. This happens even though the branch is unchosen because when x=0", "#   the gradient of a select involves the calculation 1*dy+0*(-inf)=nan", "#   regardless of whether dy is finite. Note that the minimum is a NOP if", "#   the branch is chosen.", "return", "tf", ".", "where", "(", "tf", ".", "greater", "(", "x", ",", "upper_segment", ")", ",", "-", "_ndtr", "(", "-", "x", ")", ",", "# log(1-x) ~= -x, x << 1", "tf", ".", "where", "(", "tf", ".", "greater", "(", "x", ",", "lower_segment", ")", ",", "tf", ".", "math", ".", "log", "(", "_ndtr", "(", "tf", ".", "maximum", "(", "x", ",", "lower_segment", ")", ")", ")", ",", "_log_ndtr_lower", "(", "tf", ".", "minimum", "(", "x", ",", "lower_segment", ")", ",", "series_order", ")", ")", ")"], "docstring": "Log Normal distribution function.\n\n  For details of the Normal distribution function see `ndtr`.\n\n  This function calculates `(log o ndtr)(x)` by either calling `log(ndtr(x))` or\n  using an asymptotic series. Specifically:\n  - For `x > upper_segment`, use the approximation `-ndtr(-x)` based on\n    `log(1-x) ~= -x, x << 1`.\n  - For `lower_segment < x <= upper_segment`, use the existing `ndtr` technique\n    and take a log.\n  - For `x <= lower_segment`, we use the series approximation of erf to compute\n    the log CDF directly.\n\n  The `lower_segment` is set based on the precision of the input:\n\n  ```\n  lower_segment = { -20,  x.dtype=float64\n                  { -10,  x.dtype=float32\n  upper_segment = {   8,  x.dtype=float64\n                  {   5,  x.dtype=float32\n  ```\n\n  When `x < lower_segment`, the `ndtr` asymptotic series approximation is:\n\n  ```\n     ndtr(x) = scale * (1 + sum) + R_N\n     scale   = exp(-0.5 x**2) / (-x sqrt(2 pi))\n     sum     = Sum{(-1)^n (2n-1)!! / (x**2)^n, n=1:N}\n     R_N     = O(exp(-0.5 x**2) (2N+1)!! / |x|^{2N+3})\n  ```\n\n  where `(2n-1)!! = (2n-1) (2n-3) (2n-5) ...  (3) (1)` is a\n  [double-factorial](https://en.wikipedia.org/wiki/Double_factorial).\n\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    series_order: Positive Python `integer`. Maximum depth to\n      evaluate the asymptotic expansion. This is the `N` above.\n    name: Python string. A name for the operation (default=\"log_ndtr\").\n\n  Returns:\n    log_ndtr: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x.dtype` is not handled.\n    TypeError: if `series_order` is a not Python `integer.`\n    ValueError:  if `series_order` is not in `[0, 30]`.", "docstring_tokens": ["Log", "Normal", "distribution", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/special_math.py#L291-L380", "partition": "test", "index": 744, "time": "2018-10-15 10:00:17"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/special_math.py", "func_name": "_log_ndtr_asymptotic_series", "original_string": "def _log_ndtr_asymptotic_series(x, series_order):\n  \"\"\"Calculates the asymptotic series used in log_ndtr.\"\"\"\n  npdt = dtype_util.as_numpy_dtype(x.dtype)\n  if series_order <= 0:\n    return npdt(1)\n  x_2 = tf.square(x)\n  even_sum = tf.zeros_like(x)\n  odd_sum = tf.zeros_like(x)\n  x_2n = x_2  # Start with x^{2*1} = x^{2*n} with n = 1.\n  for n in range(1, series_order + 1):\n    y = npdt(_double_factorial(2 * n - 1)) / x_2n\n    if n % 2:\n      odd_sum += y\n    else:\n      even_sum += y\n    x_2n *= x_2\n  return 1. + even_sum - odd_sum", "language": "python", "code": "def _log_ndtr_asymptotic_series(x, series_order):\n  \"\"\"Calculates the asymptotic series used in log_ndtr.\"\"\"\n  npdt = dtype_util.as_numpy_dtype(x.dtype)\n  if series_order <= 0:\n    return npdt(1)\n  x_2 = tf.square(x)\n  even_sum = tf.zeros_like(x)\n  odd_sum = tf.zeros_like(x)\n  x_2n = x_2  # Start with x^{2*1} = x^{2*n} with n = 1.\n  for n in range(1, series_order + 1):\n    y = npdt(_double_factorial(2 * n - 1)) / x_2n\n    if n % 2:\n      odd_sum += y\n    else:\n      even_sum += y\n    x_2n *= x_2\n  return 1. + even_sum - odd_sum", "code_tokens": ["def", "_log_ndtr_asymptotic_series", "(", "x", ",", "series_order", ")", ":", "npdt", "=", "dtype_util", ".", "as_numpy_dtype", "(", "x", ".", "dtype", ")", "if", "series_order", "<=", "0", ":", "return", "npdt", "(", "1", ")", "x_2", "=", "tf", ".", "square", "(", "x", ")", "even_sum", "=", "tf", ".", "zeros_like", "(", "x", ")", "odd_sum", "=", "tf", ".", "zeros_like", "(", "x", ")", "x_2n", "=", "x_2", "# Start with x^{2*1} = x^{2*n} with n = 1.", "for", "n", "in", "range", "(", "1", ",", "series_order", "+", "1", ")", ":", "y", "=", "npdt", "(", "_double_factorial", "(", "2", "*", "n", "-", "1", ")", ")", "/", "x_2n", "if", "n", "%", "2", ":", "odd_sum", "+=", "y", "else", ":", "even_sum", "+=", "y", "x_2n", "*=", "x_2", "return", "1.", "+", "even_sum", "-", "odd_sum"], "docstring": "Calculates the asymptotic series used in log_ndtr.", "docstring_tokens": ["Calculates", "the", "asymptotic", "series", "used", "in", "log_ndtr", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/special_math.py#L391-L407", "partition": "test", "index": 745, "time": "2018-10-15 10:00:17"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/multivariate_student_t.py", "func_name": "MultivariateStudentTLinearOperator._std_var_helper", "original_string": "def _std_var_helper(self, statistic, statistic_name, statistic_ndims,\n                      df_factor_fn):\n    \"\"\"Helper to compute stddev, covariance and variance.\"\"\"\n    df = tf.reshape(\n        self.df,\n        tf.concat([\n            tf.shape(input=self.df),\n            tf.ones([statistic_ndims], dtype=tf.int32)\n        ], -1))\n    df = _broadcast_to_shape(df, tf.shape(input=statistic))\n    # We need to put the tf.where inside the outer tf.where to ensure we never\n    # hit a NaN in the gradient.\n    denom = tf.where(df > 2., df - 2., tf.ones_like(df))\n    statistic = statistic * df_factor_fn(df / denom)\n    # When 1 < df <= 2, stddev/variance are infinite.\n    inf = dtype_util.as_numpy_dtype(self.dtype)(np.inf)\n    result_where_defined = tf.where(\n        df > 2., statistic, tf.fill(tf.shape(input=statistic), inf, name=\"inf\"))\n\n    if self.allow_nan_stats:\n      nan = dtype_util.as_numpy_dtype(self.dtype)(np.nan)\n      return tf.where(df > 1., result_where_defined,\n                      tf.fill(tf.shape(input=statistic), nan, name=\"nan\"))\n    else:\n      with tf.control_dependencies([\n          assert_util.assert_less(\n              tf.cast(1., self.dtype),\n              df,\n              message=statistic_name +\n              \" not defined for components of df <= 1\"),\n      ]):\n        return tf.identity(result_where_defined)", "language": "python", "code": "def _std_var_helper(self, statistic, statistic_name, statistic_ndims,\n                      df_factor_fn):\n    \"\"\"Helper to compute stddev, covariance and variance.\"\"\"\n    df = tf.reshape(\n        self.df,\n        tf.concat([\n            tf.shape(input=self.df),\n            tf.ones([statistic_ndims], dtype=tf.int32)\n        ], -1))\n    df = _broadcast_to_shape(df, tf.shape(input=statistic))\n    # We need to put the tf.where inside the outer tf.where to ensure we never\n    # hit a NaN in the gradient.\n    denom = tf.where(df > 2., df - 2., tf.ones_like(df))\n    statistic = statistic * df_factor_fn(df / denom)\n    # When 1 < df <= 2, stddev/variance are infinite.\n    inf = dtype_util.as_numpy_dtype(self.dtype)(np.inf)\n    result_where_defined = tf.where(\n        df > 2., statistic, tf.fill(tf.shape(input=statistic), inf, name=\"inf\"))\n\n    if self.allow_nan_stats:\n      nan = dtype_util.as_numpy_dtype(self.dtype)(np.nan)\n      return tf.where(df > 1., result_where_defined,\n                      tf.fill(tf.shape(input=statistic), nan, name=\"nan\"))\n    else:\n      with tf.control_dependencies([\n          assert_util.assert_less(\n              tf.cast(1., self.dtype),\n              df,\n              message=statistic_name +\n              \" not defined for components of df <= 1\"),\n      ]):\n        return tf.identity(result_where_defined)", "code_tokens": ["def", "_std_var_helper", "(", "self", ",", "statistic", ",", "statistic_name", ",", "statistic_ndims", ",", "df_factor_fn", ")", ":", "df", "=", "tf", ".", "reshape", "(", "self", ".", "df", ",", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "input", "=", "self", ".", "df", ")", ",", "tf", ".", "ones", "(", "[", "statistic_ndims", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "]", ",", "-", "1", ")", ")", "df", "=", "_broadcast_to_shape", "(", "df", ",", "tf", ".", "shape", "(", "input", "=", "statistic", ")", ")", "# We need to put the tf.where inside the outer tf.where to ensure we never", "# hit a NaN in the gradient.", "denom", "=", "tf", ".", "where", "(", "df", ">", "2.", ",", "df", "-", "2.", ",", "tf", ".", "ones_like", "(", "df", ")", ")", "statistic", "=", "statistic", "*", "df_factor_fn", "(", "df", "/", "denom", ")", "# When 1 < df <= 2, stddev/variance are infinite.", "inf", "=", "dtype_util", ".", "as_numpy_dtype", "(", "self", ".", "dtype", ")", "(", "np", ".", "inf", ")", "result_where_defined", "=", "tf", ".", "where", "(", "df", ">", "2.", ",", "statistic", ",", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "input", "=", "statistic", ")", ",", "inf", ",", "name", "=", "\"inf\"", ")", ")", "if", "self", ".", "allow_nan_stats", ":", "nan", "=", "dtype_util", ".", "as_numpy_dtype", "(", "self", ".", "dtype", ")", "(", "np", ".", "nan", ")", "return", "tf", ".", "where", "(", "df", ">", "1.", ",", "result_where_defined", ",", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "input", "=", "statistic", ")", ",", "nan", ",", "name", "=", "\"nan\"", ")", ")", "else", ":", "with", "tf", ".", "control_dependencies", "(", "[", "assert_util", ".", "assert_less", "(", "tf", ".", "cast", "(", "1.", ",", "self", ".", "dtype", ")", ",", "df", ",", "message", "=", "statistic_name", "+", "\" not defined for components of df <= 1\"", ")", ",", "]", ")", ":", "return", "tf", ".", "identity", "(", "result_where_defined", ")"], "docstring": "Helper to compute stddev, covariance and variance.", "docstring_tokens": ["Helper", "to", "compute", "stddev", "covariance", "and", "variance", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/multivariate_student_t.py#L306-L337", "partition": "test", "index": 876, "time": "2018-10-18 17:25:33"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/zipf.py", "func_name": "Zipf._hat_integral_inverse", "original_string": "def _hat_integral_inverse(self, x):\n    \"\"\"Inverse function of _hat_integral.\"\"\"\n    x = tf.cast(x, self.power.dtype)\n    t = self.power - 1.\n    return tf.math.expm1(-(tf.math.log(t) + tf.math.log(x)) / t)", "language": "python", "code": "def _hat_integral_inverse(self, x):\n    \"\"\"Inverse function of _hat_integral.\"\"\"\n    x = tf.cast(x, self.power.dtype)\n    t = self.power - 1.\n    return tf.math.expm1(-(tf.math.log(t) + tf.math.log(x)) / t)", "code_tokens": ["def", "_hat_integral_inverse", "(", "self", ",", "x", ")", ":", "x", "=", "tf", ".", "cast", "(", "x", ",", "self", ".", "power", ".", "dtype", ")", "t", "=", "self", ".", "power", "-", "1.", "return", "tf", ".", "math", ".", "expm1", "(", "-", "(", "tf", ".", "math", ".", "log", "(", "t", ")", "+", "tf", ".", "math", ".", "log", "(", "x", ")", ")", "/", "t", ")"], "docstring": "Inverse function of _hat_integral.", "docstring_tokens": ["Inverse", "function", "of", "_hat_integral", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/zipf.py#L324-L328", "partition": "test", "index": 1104, "time": "2018-10-25 10:55:10"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/zipf.py", "func_name": "Zipf._hat_integral", "original_string": "def _hat_integral(self, x):\n    \"\"\"Integral of the `hat` function, used for sampling.\n\n    We choose a `hat` function, h(x) = x^(-power), which is a continuous\n    (unnormalized) density touching each positive integer at the (unnormalized)\n    pmf. This function implements `hat` integral: H(x) = int_x^inf h(t) dt;\n    which is needed for sampling purposes.\n\n    Arguments:\n      x: A Tensor of points x at which to evaluate H(x).\n\n    Returns:\n      A Tensor containing evaluation H(x) at x.\n    \"\"\"\n    x = tf.cast(x, self.power.dtype)\n    t = self.power - 1.\n    return tf.exp((-t) * tf.math.log1p(x) - tf.math.log(t))", "language": "python", "code": "def _hat_integral(self, x):\n    \"\"\"Integral of the `hat` function, used for sampling.\n\n    We choose a `hat` function, h(x) = x^(-power), which is a continuous\n    (unnormalized) density touching each positive integer at the (unnormalized)\n    pmf. This function implements `hat` integral: H(x) = int_x^inf h(t) dt;\n    which is needed for sampling purposes.\n\n    Arguments:\n      x: A Tensor of points x at which to evaluate H(x).\n\n    Returns:\n      A Tensor containing evaluation H(x) at x.\n    \"\"\"\n    x = tf.cast(x, self.power.dtype)\n    t = self.power - 1.\n    return tf.exp((-t) * tf.math.log1p(x) - tf.math.log(t))", "code_tokens": ["def", "_hat_integral", "(", "self", ",", "x", ")", ":", "x", "=", "tf", ".", "cast", "(", "x", ",", "self", ".", "power", ".", "dtype", ")", "t", "=", "self", ".", "power", "-", "1.", "return", "tf", ".", "exp", "(", "(", "-", "t", ")", "*", "tf", ".", "math", ".", "log1p", "(", "x", ")", "-", "tf", ".", "math", ".", "log", "(", "t", ")", ")"], "docstring": "Integral of the `hat` function, used for sampling.\n\n    We choose a `hat` function, h(x) = x^(-power), which is a continuous\n    (unnormalized) density touching each positive integer at the (unnormalized)\n    pmf. This function implements `hat` integral: H(x) = int_x^inf h(t) dt;\n    which is needed for sampling purposes.\n\n    Arguments:\n      x: A Tensor of points x at which to evaluate H(x).\n\n    Returns:\n      A Tensor containing evaluation H(x) at x.", "docstring_tokens": ["Integral", "of", "the", "hat", "function", "used", "for", "sampling", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/zipf.py#L306-L322", "partition": "test", "index": 1103, "time": "2018-10-25 10:55:10"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/triangular.py", "func_name": "Triangular._pdf_at_peak", "original_string": "def _pdf_at_peak(self):\n    \"\"\"Pdf evaluated at the peak.\"\"\"\n    return (self.peak - self.low) / (self.high - self.low)", "language": "python", "code": "def _pdf_at_peak(self):\n    \"\"\"Pdf evaluated at the peak.\"\"\"\n    return (self.peak - self.low) / (self.high - self.low)", "code_tokens": ["def", "_pdf_at_peak", "(", "self", ")", ":", "return", "(", "self", ".", "peak", "-", "self", ".", "low", ")", "/", "(", "self", ".", "high", "-", "self", ".", "low", ")"], "docstring": "Pdf evaluated at the peak.", "docstring_tokens": ["Pdf", "evaluated", "at", "the", "peak", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/triangular.py#L186-L188", "partition": "test", "index": 780, "time": "2018-10-25 17:44:04"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/triangular.py", "func_name": "_broadcast_to", "original_string": "def _broadcast_to(tensor_to_broadcast, target_tensors):\n  \"\"\"Helper to broadcast a tensor using a list of target tensors.\"\"\"\n  output = tensor_to_broadcast\n  for tensor in target_tensors:\n    output += tf.zeros_like(tensor)\n  return output", "language": "python", "code": "def _broadcast_to(tensor_to_broadcast, target_tensors):\n  \"\"\"Helper to broadcast a tensor using a list of target tensors.\"\"\"\n  output = tensor_to_broadcast\n  for tensor in target_tensors:\n    output += tf.zeros_like(tensor)\n  return output", "code_tokens": ["def", "_broadcast_to", "(", "tensor_to_broadcast", ",", "target_tensors", ")", ":", "output", "=", "tensor_to_broadcast", "for", "tensor", "in", "target_tensors", ":", "output", "+=", "tf", ".", "zeros_like", "(", "tensor", ")", "return", "output"], "docstring": "Helper to broadcast a tensor using a list of target tensors.", "docstring_tokens": ["Helper", "to", "broadcast", "a", "tensor", "using", "a", "list", "of", "target", "tensors", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/triangular.py#L33-L38", "partition": "test", "index": 779, "time": "2018-10-25 17:44:04"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/sample_stats.py", "func_name": "_squeeze", "original_string": "def _squeeze(x, axis):\n  \"\"\"A version of squeeze that works with dynamic axis.\"\"\"\n  x = tf.convert_to_tensor(value=x, name='x')\n  if axis is None:\n    return tf.squeeze(x, axis=None)\n  axis = tf.convert_to_tensor(value=axis, name='axis', dtype=tf.int32)\n  axis += tf.zeros([1], dtype=axis.dtype)  # Make axis at least 1d.\n  keep_axis, _ = tf.compat.v1.setdiff1d(tf.range(0, tf.rank(x)), axis)\n  return tf.reshape(x, tf.gather(tf.shape(input=x), keep_axis))", "language": "python", "code": "def _squeeze(x, axis):\n  \"\"\"A version of squeeze that works with dynamic axis.\"\"\"\n  x = tf.convert_to_tensor(value=x, name='x')\n  if axis is None:\n    return tf.squeeze(x, axis=None)\n  axis = tf.convert_to_tensor(value=axis, name='axis', dtype=tf.int32)\n  axis += tf.zeros([1], dtype=axis.dtype)  # Make axis at least 1d.\n  keep_axis, _ = tf.compat.v1.setdiff1d(tf.range(0, tf.rank(x)), axis)\n  return tf.reshape(x, tf.gather(tf.shape(input=x), keep_axis))", "code_tokens": ["def", "_squeeze", "(", "x", ",", "axis", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ")", "if", "axis", "is", "None", ":", "return", "tf", ".", "squeeze", "(", "x", ",", "axis", "=", "None", ")", "axis", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "axis", ",", "name", "=", "'axis'", ",", "dtype", "=", "tf", ".", "int32", ")", "axis", "+=", "tf", ".", "zeros", "(", "[", "1", "]", ",", "dtype", "=", "axis", ".", "dtype", ")", "# Make axis at least 1d.", "keep_axis", ",", "_", "=", "tf", ".", "compat", ".", "v1", ".", "setdiff1d", "(", "tf", ".", "range", "(", "0", ",", "tf", ".", "rank", "(", "x", ")", ")", ",", "axis", ")", "return", "tf", ".", "reshape", "(", "x", ",", "tf", ".", "gather", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "keep_axis", ")", ")"], "docstring": "A version of squeeze that works with dynamic axis.", "docstring_tokens": ["A", "version", "of", "squeeze", "that", "works", "with", "dynamic", "axis", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/sample_stats.py#L686-L694", "partition": "test", "index": 660, "time": "2018-10-28 12:27:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/sample_stats.py", "func_name": "_make_positive_axis", "original_string": "def _make_positive_axis(axis, ndims):\n  \"\"\"Rectify possibly negatively axis. Prefer return Python list.\"\"\"\n  axis = _make_list_or_1d_tensor(axis)\n\n  ndims = tf.convert_to_tensor(value=ndims, name='ndims', dtype=tf.int32)\n  ndims_ = tf.get_static_value(ndims)\n\n  if _is_list_like(axis) and ndims_ is not None:\n    # Static case\n    positive_axis = []\n    for a in axis:\n      if a < 0:\n        a = ndims_ + a\n      positive_axis.append(a)\n  else:\n    # Dynamic case\n    axis = tf.convert_to_tensor(value=axis, name='axis', dtype=tf.int32)\n    positive_axis = tf.where(axis >= 0, axis, axis + ndims)\n\n  return positive_axis", "language": "python", "code": "def _make_positive_axis(axis, ndims):\n  \"\"\"Rectify possibly negatively axis. Prefer return Python list.\"\"\"\n  axis = _make_list_or_1d_tensor(axis)\n\n  ndims = tf.convert_to_tensor(value=ndims, name='ndims', dtype=tf.int32)\n  ndims_ = tf.get_static_value(ndims)\n\n  if _is_list_like(axis) and ndims_ is not None:\n    # Static case\n    positive_axis = []\n    for a in axis:\n      if a < 0:\n        a = ndims_ + a\n      positive_axis.append(a)\n  else:\n    # Dynamic case\n    axis = tf.convert_to_tensor(value=axis, name='axis', dtype=tf.int32)\n    positive_axis = tf.where(axis >= 0, axis, axis + ndims)\n\n  return positive_axis", "code_tokens": ["def", "_make_positive_axis", "(", "axis", ",", "ndims", ")", ":", "axis", "=", "_make_list_or_1d_tensor", "(", "axis", ")", "ndims", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "ndims", ",", "name", "=", "'ndims'", ",", "dtype", "=", "tf", ".", "int32", ")", "ndims_", "=", "tf", ".", "get_static_value", "(", "ndims", ")", "if", "_is_list_like", "(", "axis", ")", "and", "ndims_", "is", "not", "None", ":", "# Static case", "positive_axis", "=", "[", "]", "for", "a", "in", "axis", ":", "if", "a", "<", "0", ":", "a", "=", "ndims_", "+", "a", "positive_axis", ".", "append", "(", "a", ")", "else", ":", "# Dynamic case", "axis", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "axis", ",", "name", "=", "'axis'", ",", "dtype", "=", "tf", ".", "int32", ")", "positive_axis", "=", "tf", ".", "where", "(", "axis", ">=", "0", ",", "axis", ",", "axis", "+", "ndims", ")", "return", "positive_axis"], "docstring": "Rectify possibly negatively axis. Prefer return Python list.", "docstring_tokens": ["Rectify", "possibly", "negatively", "axis", ".", "Prefer", "return", "Python", "list", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/sample_stats.py#L664-L683", "partition": "test", "index": 659, "time": "2018-10-28 12:27:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/sample_stats.py", "func_name": "variance", "original_string": "def variance(x, sample_axis=0, keepdims=False, name=None):\n  \"\"\"Estimate variance using samples.\n\n  Given `N` samples of scalar valued random variable `X`, variance may\n  be estimated as\n\n  ```none\n  Var[X] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(X_n - Xbar)}\n  Xbar := N^{-1} sum_{n=1}^N X_n\n  ```\n\n  ```python\n  x = tf.random_normal(shape=(100, 2, 3))\n\n  # var[i, j] is the sample variance of the (i, j) batch member of x.\n  var = tfp.stats.variance(x, sample_axis=0)\n  ```\n\n  Notice we divide by `N` (the numpy default), which does not create `NaN`\n  when `N = 1`, but is slightly biased.\n\n  Args:\n    x:  A numeric `Tensor` holding samples.\n    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or\n      `None` (meaning all axis hold samples).\n      Default value: `0` (leftmost dimension).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'variance'`).\n\n  Returns:\n    var: A `Tensor` of same `dtype` as the `x`, and rank equal to\n      `rank(x) - len(sample_axis)`\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'variance', values=[x, sample_axis]):\n    return covariance(\n        x, y=None, sample_axis=sample_axis, event_axis=None, keepdims=keepdims)", "language": "python", "code": "def variance(x, sample_axis=0, keepdims=False, name=None):\n  \"\"\"Estimate variance using samples.\n\n  Given `N` samples of scalar valued random variable `X`, variance may\n  be estimated as\n\n  ```none\n  Var[X] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(X_n - Xbar)}\n  Xbar := N^{-1} sum_{n=1}^N X_n\n  ```\n\n  ```python\n  x = tf.random_normal(shape=(100, 2, 3))\n\n  # var[i, j] is the sample variance of the (i, j) batch member of x.\n  var = tfp.stats.variance(x, sample_axis=0)\n  ```\n\n  Notice we divide by `N` (the numpy default), which does not create `NaN`\n  when `N = 1`, but is slightly biased.\n\n  Args:\n    x:  A numeric `Tensor` holding samples.\n    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or\n      `None` (meaning all axis hold samples).\n      Default value: `0` (leftmost dimension).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'variance'`).\n\n  Returns:\n    var: A `Tensor` of same `dtype` as the `x`, and rank equal to\n      `rank(x) - len(sample_axis)`\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'variance', values=[x, sample_axis]):\n    return covariance(\n        x, y=None, sample_axis=sample_axis, event_axis=None, keepdims=keepdims)", "code_tokens": ["def", "variance", "(", "x", ",", "sample_axis", "=", "0", ",", "keepdims", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'variance'", ",", "values", "=", "[", "x", ",", "sample_axis", "]", ")", ":", "return", "covariance", "(", "x", ",", "y", "=", "None", ",", "sample_axis", "=", "sample_axis", ",", "event_axis", "=", "None", ",", "keepdims", "=", "keepdims", ")"], "docstring": "Estimate variance using samples.\n\n  Given `N` samples of scalar valued random variable `X`, variance may\n  be estimated as\n\n  ```none\n  Var[X] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(X_n - Xbar)}\n  Xbar := N^{-1} sum_{n=1}^N X_n\n  ```\n\n  ```python\n  x = tf.random_normal(shape=(100, 2, 3))\n\n  # var[i, j] is the sample variance of the (i, j) batch member of x.\n  var = tfp.stats.variance(x, sample_axis=0)\n  ```\n\n  Notice we divide by `N` (the numpy default), which does not create `NaN`\n  when `N = 1`, but is slightly biased.\n\n  Args:\n    x:  A numeric `Tensor` holding samples.\n    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or\n      `None` (meaning all axis hold samples).\n      Default value: `0` (leftmost dimension).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'variance'`).\n\n  Returns:\n    var: A `Tensor` of same `dtype` as the `x`, and rank equal to\n      `rank(x) - len(sample_axis)`", "docstring_tokens": ["Estimate", "variance", "using", "samples", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/sample_stats.py#L602-L638", "partition": "test", "index": 658, "time": "2018-10-28 12:27:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/fitting.py", "func_name": "_build_trainable_posterior", "original_string": "def _build_trainable_posterior(param, initial_loc_fn):\n  \"\"\"Built a transformed-normal variational dist over a parameter's support.\"\"\"\n  loc = tf.compat.v1.get_variable(\n      param.name + '_loc',\n      initializer=lambda: initial_loc_fn(param),\n      dtype=param.prior.dtype,\n      use_resource=True)\n  scale = tf.nn.softplus(\n      tf.compat.v1.get_variable(\n          param.name + '_scale',\n          initializer=lambda: -4 * tf.ones_like(initial_loc_fn(param)),\n          dtype=param.prior.dtype,\n          use_resource=True))\n\n  q = tfd.Normal(loc=loc, scale=scale)\n\n  # Ensure the `event_shape` of the variational distribution matches the\n  # parameter.\n  if (param.prior.event_shape.ndims is None\n      or param.prior.event_shape.ndims > 0):\n    q = tfd.Independent(\n        q, reinterpreted_batch_ndims=param.prior.event_shape.ndims)\n\n  # Transform to constrained parameter space.\n  return tfd.TransformedDistribution(q, param.bijector)", "language": "python", "code": "def _build_trainable_posterior(param, initial_loc_fn):\n  \"\"\"Built a transformed-normal variational dist over a parameter's support.\"\"\"\n  loc = tf.compat.v1.get_variable(\n      param.name + '_loc',\n      initializer=lambda: initial_loc_fn(param),\n      dtype=param.prior.dtype,\n      use_resource=True)\n  scale = tf.nn.softplus(\n      tf.compat.v1.get_variable(\n          param.name + '_scale',\n          initializer=lambda: -4 * tf.ones_like(initial_loc_fn(param)),\n          dtype=param.prior.dtype,\n          use_resource=True))\n\n  q = tfd.Normal(loc=loc, scale=scale)\n\n  # Ensure the `event_shape` of the variational distribution matches the\n  # parameter.\n  if (param.prior.event_shape.ndims is None\n      or param.prior.event_shape.ndims > 0):\n    q = tfd.Independent(\n        q, reinterpreted_batch_ndims=param.prior.event_shape.ndims)\n\n  # Transform to constrained parameter space.\n  return tfd.TransformedDistribution(q, param.bijector)", "code_tokens": ["def", "_build_trainable_posterior", "(", "param", ",", "initial_loc_fn", ")", ":", "loc", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "param", ".", "name", "+", "'_loc'", ",", "initializer", "=", "lambda", ":", "initial_loc_fn", "(", "param", ")", ",", "dtype", "=", "param", ".", "prior", ".", "dtype", ",", "use_resource", "=", "True", ")", "scale", "=", "tf", ".", "nn", ".", "softplus", "(", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "param", ".", "name", "+", "'_scale'", ",", "initializer", "=", "lambda", ":", "-", "4", "*", "tf", ".", "ones_like", "(", "initial_loc_fn", "(", "param", ")", ")", ",", "dtype", "=", "param", ".", "prior", ".", "dtype", ",", "use_resource", "=", "True", ")", ")", "q", "=", "tfd", ".", "Normal", "(", "loc", "=", "loc", ",", "scale", "=", "scale", ")", "# Ensure the `event_shape` of the variational distribution matches the", "# parameter.", "if", "(", "param", ".", "prior", ".", "event_shape", ".", "ndims", "is", "None", "or", "param", ".", "prior", ".", "event_shape", ".", "ndims", ">", "0", ")", ":", "q", "=", "tfd", ".", "Independent", "(", "q", ",", "reinterpreted_batch_ndims", "=", "param", ".", "prior", ".", "event_shape", ".", "ndims", ")", "# Transform to constrained parameter space.", "return", "tfd", ".", "TransformedDistribution", "(", "q", ",", "param", ".", "bijector", ")"], "docstring": "Built a transformed-normal variational dist over a parameter's support.", "docstring_tokens": ["Built", "a", "transformed", "-", "normal", "variational", "dist", "over", "a", "parameter", "s", "support", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/fitting.py#L65-L89", "partition": "test", "index": 698, "time": "2018-10-29 12:40:53"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/fitting.py", "func_name": "build_factored_variational_loss", "original_string": "def build_factored_variational_loss(model,\n                                    observed_time_series,\n                                    init_batch_shape=(),\n                                    seed=None,\n                                    name=None):\n  \"\"\"Build a loss function for variational inference in STS models.\n\n  Variational inference searches for the distribution within some family of\n  approximate posteriors that minimizes a divergence between the approximate\n  posterior `q(z)` and true posterior `p(z|observed_time_series)`. By converting\n  inference to optimization, it's generally much faster than sampling-based\n  inference algorithms such as HMC. The tradeoff is that the approximating\n  family rarely contains the true posterior, so it may miss important aspects of\n  posterior structure (in particular, dependence between variables) and should\n  not be blindly trusted. Results may vary; it's generally wise to compare to\n  HMC to evaluate whether inference quality is sufficient for your task at hand.\n\n  This method constructs a loss function for variational inference using the\n  Kullback-Liebler divergence `KL[q(z) || p(z|observed_time_series)]`, with an\n  approximating family given by independent Normal distributions transformed to\n  the appropriate parameter space for each parameter. Minimizing this loss (the\n  negative ELBO) maximizes a lower bound on the log model evidence `-log\n  p(observed_time_series)`. This is equivalent to the 'mean-field' method\n  implemented in [1]. and is a standard approach. The resulting posterior\n  approximations are unimodal; they will tend to underestimate posterior\n  uncertainty when the true posterior contains multiple modes (the `KL[q||p]`\n  divergence encourages choosing a single mode) or dependence between variables.\n\n  Args:\n    model: An instance of `StructuralTimeSeries` representing a\n      time-series model. This represents a joint distribution over\n      time-series and their parameters with batch shape `[b1, ..., bN]`.\n    observed_time_series: `float` `Tensor` of shape\n      `concat([sample_shape, model.batch_shape, [num_timesteps, 1]]) where\n      `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`\n      dimension may (optionally) be omitted if `num_timesteps > 1`. May\n      optionally be an instance of `tfp.sts.MaskedTimeSeries`, which includes\n      a mask `Tensor` to specify timesteps with missing observations.\n    init_batch_shape: Batch shape (Python `tuple`, `list`, or `int`) of initial\n      states to optimize in parallel.\n      Default value: `()`. (i.e., just run a single optimization).\n    seed: Python integer to seed the random number generator.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., 'build_factored_variational_loss').\n\n  Returns:\n    variational_loss: `float` `Tensor` of shape\n      `concat([init_batch_shape, model.batch_shape])`, encoding a stochastic\n      estimate of an upper bound on the negative model evidence `-log p(y)`.\n      Minimizing this loss performs variational inference; the gap between the\n      variational bound and the true (generally unknown) model evidence\n      corresponds to the divergence `KL[q||p]` between the approximate and true\n      posterior.\n    variational_distributions: `collections.OrderedDict` giving\n      the approximate posterior for each model parameter. The keys are\n      Python `str` parameter names in order, corresponding to\n      `[param.name for param in model.parameters]`. The values are\n      `tfd.Distribution` instances with batch shape\n      `concat([init_batch_shape, model.batch_shape])`; these will typically be\n      of the form `tfd.TransformedDistribution(tfd.Normal(...),\n      bijector=param.bijector)`.\n\n  #### Examples\n\n  Assume we've built a structural time-series model:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n  ```\n\n  To run variational inference, we simply construct the loss and optimize\n  it:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series)\n\n    train_op = tf.train.AdamOptimizer(0.1).minimize(variational_loss)\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      for step in range(200):\n        _, loss_ = sess.run((train_op, variational_loss))\n\n        if step % 20 == 0:\n          print(\"step {} loss {}\".format(step, loss_))\n\n      posterior_samples_ = sess.run({\n        param_name: q.sample(50)\n        for param_name, q in variational_distributions.items()})\n  ```\n\n  As a more complex example, we might try to avoid local optima by optimizing\n  from multiple initializations in parallel, and selecting the result with the\n  lowest loss:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series,\n       init_batch_shape=[10])\n\n    train_op = tf.train.AdamOptimizer(0.1).minimize(variational_loss)\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      for step in range(200):\n        _, loss_ = sess.run((train_op, variational_loss))\n\n        if step % 20 == 0:\n          print(\"step {} losses {}\".format(step, loss_))\n\n      # Draw multiple samples to reduce Monte Carlo error in the optimized\n      # variational bounds.\n      avg_loss = np.mean(\n        [sess.run(variational_loss) for _ in range(25)], axis=0)\n      best_posterior_idx = np.argmin(avg_loss, axis=0).astype(np.int32)\n  ```\n\n  #### References\n\n  [1]: Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and\n       David M. Blei. Automatic Differentiation Variational Inference. In\n       _Journal of Machine Learning Research_, 2017.\n       https://arxiv.org/abs/1603.00788\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      name, 'build_factored_variational_loss',\n      values=[observed_time_series]) as name:\n    seed = tfd.SeedStream(\n        seed, salt='StructuralTimeSeries_build_factored_variational_loss')\n\n    variational_distributions = collections.OrderedDict()\n    variational_samples = []\n    for param in model.parameters:\n      def initial_loc_fn(param):\n        return sample_uniform_initial_state(\n            param, return_constrained=True,\n            init_sample_shape=init_batch_shape,\n            seed=seed())\n      q = _build_trainable_posterior(param, initial_loc_fn=initial_loc_fn)\n      variational_distributions[param.name] = q\n      variational_samples.append(q.sample(seed=seed()))\n\n    # Multiple initializations (similar to HMC chains) manifest as an extra\n    # param batch dimension, so we need to add corresponding batch dimension(s)\n    # to `observed_time_series`.\n    observed_time_series = sts_util.pad_batch_dimension_for_multiple_chains(\n        observed_time_series, model, chain_batch_shape=init_batch_shape)\n\n    # Construct the variational bound.\n    log_prob_fn = model.joint_log_prob(observed_time_series)\n    expected_log_joint = log_prob_fn(*variational_samples)\n    entropy = tf.reduce_sum(\n        input_tensor=[\n            -q.log_prob(sample) for (q, sample) in zip(\n                variational_distributions.values(), variational_samples)\n        ],\n        axis=0)\n    variational_loss = -(expected_log_joint + entropy)  # -ELBO\n\n  return variational_loss, variational_distributions", "language": "python", "code": "def build_factored_variational_loss(model,\n                                    observed_time_series,\n                                    init_batch_shape=(),\n                                    seed=None,\n                                    name=None):\n  \"\"\"Build a loss function for variational inference in STS models.\n\n  Variational inference searches for the distribution within some family of\n  approximate posteriors that minimizes a divergence between the approximate\n  posterior `q(z)` and true posterior `p(z|observed_time_series)`. By converting\n  inference to optimization, it's generally much faster than sampling-based\n  inference algorithms such as HMC. The tradeoff is that the approximating\n  family rarely contains the true posterior, so it may miss important aspects of\n  posterior structure (in particular, dependence between variables) and should\n  not be blindly trusted. Results may vary; it's generally wise to compare to\n  HMC to evaluate whether inference quality is sufficient for your task at hand.\n\n  This method constructs a loss function for variational inference using the\n  Kullback-Liebler divergence `KL[q(z) || p(z|observed_time_series)]`, with an\n  approximating family given by independent Normal distributions transformed to\n  the appropriate parameter space for each parameter. Minimizing this loss (the\n  negative ELBO) maximizes a lower bound on the log model evidence `-log\n  p(observed_time_series)`. This is equivalent to the 'mean-field' method\n  implemented in [1]. and is a standard approach. The resulting posterior\n  approximations are unimodal; they will tend to underestimate posterior\n  uncertainty when the true posterior contains multiple modes (the `KL[q||p]`\n  divergence encourages choosing a single mode) or dependence between variables.\n\n  Args:\n    model: An instance of `StructuralTimeSeries` representing a\n      time-series model. This represents a joint distribution over\n      time-series and their parameters with batch shape `[b1, ..., bN]`.\n    observed_time_series: `float` `Tensor` of shape\n      `concat([sample_shape, model.batch_shape, [num_timesteps, 1]]) where\n      `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`\n      dimension may (optionally) be omitted if `num_timesteps > 1`. May\n      optionally be an instance of `tfp.sts.MaskedTimeSeries`, which includes\n      a mask `Tensor` to specify timesteps with missing observations.\n    init_batch_shape: Batch shape (Python `tuple`, `list`, or `int`) of initial\n      states to optimize in parallel.\n      Default value: `()`. (i.e., just run a single optimization).\n    seed: Python integer to seed the random number generator.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., 'build_factored_variational_loss').\n\n  Returns:\n    variational_loss: `float` `Tensor` of shape\n      `concat([init_batch_shape, model.batch_shape])`, encoding a stochastic\n      estimate of an upper bound on the negative model evidence `-log p(y)`.\n      Minimizing this loss performs variational inference; the gap between the\n      variational bound and the true (generally unknown) model evidence\n      corresponds to the divergence `KL[q||p]` between the approximate and true\n      posterior.\n    variational_distributions: `collections.OrderedDict` giving\n      the approximate posterior for each model parameter. The keys are\n      Python `str` parameter names in order, corresponding to\n      `[param.name for param in model.parameters]`. The values are\n      `tfd.Distribution` instances with batch shape\n      `concat([init_batch_shape, model.batch_shape])`; these will typically be\n      of the form `tfd.TransformedDistribution(tfd.Normal(...),\n      bijector=param.bijector)`.\n\n  #### Examples\n\n  Assume we've built a structural time-series model:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n  ```\n\n  To run variational inference, we simply construct the loss and optimize\n  it:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series)\n\n    train_op = tf.train.AdamOptimizer(0.1).minimize(variational_loss)\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      for step in range(200):\n        _, loss_ = sess.run((train_op, variational_loss))\n\n        if step % 20 == 0:\n          print(\"step {} loss {}\".format(step, loss_))\n\n      posterior_samples_ = sess.run({\n        param_name: q.sample(50)\n        for param_name, q in variational_distributions.items()})\n  ```\n\n  As a more complex example, we might try to avoid local optima by optimizing\n  from multiple initializations in parallel, and selecting the result with the\n  lowest loss:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series,\n       init_batch_shape=[10])\n\n    train_op = tf.train.AdamOptimizer(0.1).minimize(variational_loss)\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      for step in range(200):\n        _, loss_ = sess.run((train_op, variational_loss))\n\n        if step % 20 == 0:\n          print(\"step {} losses {}\".format(step, loss_))\n\n      # Draw multiple samples to reduce Monte Carlo error in the optimized\n      # variational bounds.\n      avg_loss = np.mean(\n        [sess.run(variational_loss) for _ in range(25)], axis=0)\n      best_posterior_idx = np.argmin(avg_loss, axis=0).astype(np.int32)\n  ```\n\n  #### References\n\n  [1]: Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and\n       David M. Blei. Automatic Differentiation Variational Inference. In\n       _Journal of Machine Learning Research_, 2017.\n       https://arxiv.org/abs/1603.00788\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      name, 'build_factored_variational_loss',\n      values=[observed_time_series]) as name:\n    seed = tfd.SeedStream(\n        seed, salt='StructuralTimeSeries_build_factored_variational_loss')\n\n    variational_distributions = collections.OrderedDict()\n    variational_samples = []\n    for param in model.parameters:\n      def initial_loc_fn(param):\n        return sample_uniform_initial_state(\n            param, return_constrained=True,\n            init_sample_shape=init_batch_shape,\n            seed=seed())\n      q = _build_trainable_posterior(param, initial_loc_fn=initial_loc_fn)\n      variational_distributions[param.name] = q\n      variational_samples.append(q.sample(seed=seed()))\n\n    # Multiple initializations (similar to HMC chains) manifest as an extra\n    # param batch dimension, so we need to add corresponding batch dimension(s)\n    # to `observed_time_series`.\n    observed_time_series = sts_util.pad_batch_dimension_for_multiple_chains(\n        observed_time_series, model, chain_batch_shape=init_batch_shape)\n\n    # Construct the variational bound.\n    log_prob_fn = model.joint_log_prob(observed_time_series)\n    expected_log_joint = log_prob_fn(*variational_samples)\n    entropy = tf.reduce_sum(\n        input_tensor=[\n            -q.log_prob(sample) for (q, sample) in zip(\n                variational_distributions.values(), variational_samples)\n        ],\n        axis=0)\n    variational_loss = -(expected_log_joint + entropy)  # -ELBO\n\n  return variational_loss, variational_distributions", "code_tokens": ["def", "build_factored_variational_loss", "(", "model", ",", "observed_time_series", ",", "init_batch_shape", "=", "(", ")", ",", "seed", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'build_factored_variational_loss'", ",", "values", "=", "[", "observed_time_series", "]", ")", "as", "name", ":", "seed", "=", "tfd", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'StructuralTimeSeries_build_factored_variational_loss'", ")", "variational_distributions", "=", "collections", ".", "OrderedDict", "(", ")", "variational_samples", "=", "[", "]", "for", "param", "in", "model", ".", "parameters", ":", "def", "initial_loc_fn", "(", "param", ")", ":", "return", "sample_uniform_initial_state", "(", "param", ",", "return_constrained", "=", "True", ",", "init_sample_shape", "=", "init_batch_shape", ",", "seed", "=", "seed", "(", ")", ")", "q", "=", "_build_trainable_posterior", "(", "param", ",", "initial_loc_fn", "=", "initial_loc_fn", ")", "variational_distributions", "[", "param", ".", "name", "]", "=", "q", "variational_samples", ".", "append", "(", "q", ".", "sample", "(", "seed", "=", "seed", "(", ")", ")", ")", "# Multiple initializations (similar to HMC chains) manifest as an extra", "# param batch dimension, so we need to add corresponding batch dimension(s)", "# to `observed_time_series`.", "observed_time_series", "=", "sts_util", ".", "pad_batch_dimension_for_multiple_chains", "(", "observed_time_series", ",", "model", ",", "chain_batch_shape", "=", "init_batch_shape", ")", "# Construct the variational bound.", "log_prob_fn", "=", "model", ".", "joint_log_prob", "(", "observed_time_series", ")", "expected_log_joint", "=", "log_prob_fn", "(", "*", "variational_samples", ")", "entropy", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "[", "-", "q", ".", "log_prob", "(", "sample", ")", "for", "(", "q", ",", "sample", ")", "in", "zip", "(", "variational_distributions", ".", "values", "(", ")", ",", "variational_samples", ")", "]", ",", "axis", "=", "0", ")", "variational_loss", "=", "-", "(", "expected_log_joint", "+", "entropy", ")", "# -ELBO", "return", "variational_loss", ",", "variational_distributions"], "docstring": "Build a loss function for variational inference in STS models.\n\n  Variational inference searches for the distribution within some family of\n  approximate posteriors that minimizes a divergence between the approximate\n  posterior `q(z)` and true posterior `p(z|observed_time_series)`. By converting\n  inference to optimization, it's generally much faster than sampling-based\n  inference algorithms such as HMC. The tradeoff is that the approximating\n  family rarely contains the true posterior, so it may miss important aspects of\n  posterior structure (in particular, dependence between variables) and should\n  not be blindly trusted. Results may vary; it's generally wise to compare to\n  HMC to evaluate whether inference quality is sufficient for your task at hand.\n\n  This method constructs a loss function for variational inference using the\n  Kullback-Liebler divergence `KL[q(z) || p(z|observed_time_series)]`, with an\n  approximating family given by independent Normal distributions transformed to\n  the appropriate parameter space for each parameter. Minimizing this loss (the\n  negative ELBO) maximizes a lower bound on the log model evidence `-log\n  p(observed_time_series)`. This is equivalent to the 'mean-field' method\n  implemented in [1]. and is a standard approach. The resulting posterior\n  approximations are unimodal; they will tend to underestimate posterior\n  uncertainty when the true posterior contains multiple modes (the `KL[q||p]`\n  divergence encourages choosing a single mode) or dependence between variables.\n\n  Args:\n    model: An instance of `StructuralTimeSeries` representing a\n      time-series model. This represents a joint distribution over\n      time-series and their parameters with batch shape `[b1, ..., bN]`.\n    observed_time_series: `float` `Tensor` of shape\n      `concat([sample_shape, model.batch_shape, [num_timesteps, 1]]) where\n      `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`\n      dimension may (optionally) be omitted if `num_timesteps > 1`. May\n      optionally be an instance of `tfp.sts.MaskedTimeSeries`, which includes\n      a mask `Tensor` to specify timesteps with missing observations.\n    init_batch_shape: Batch shape (Python `tuple`, `list`, or `int`) of initial\n      states to optimize in parallel.\n      Default value: `()`. (i.e., just run a single optimization).\n    seed: Python integer to seed the random number generator.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., 'build_factored_variational_loss').\n\n  Returns:\n    variational_loss: `float` `Tensor` of shape\n      `concat([init_batch_shape, model.batch_shape])`, encoding a stochastic\n      estimate of an upper bound on the negative model evidence `-log p(y)`.\n      Minimizing this loss performs variational inference; the gap between the\n      variational bound and the true (generally unknown) model evidence\n      corresponds to the divergence `KL[q||p]` between the approximate and true\n      posterior.\n    variational_distributions: `collections.OrderedDict` giving\n      the approximate posterior for each model parameter. The keys are\n      Python `str` parameter names in order, corresponding to\n      `[param.name for param in model.parameters]`. The values are\n      `tfd.Distribution` instances with batch shape\n      `concat([init_batch_shape, model.batch_shape])`; these will typically be\n      of the form `tfd.TransformedDistribution(tfd.Normal(...),\n      bijector=param.bijector)`.\n\n  #### Examples\n\n  Assume we've built a structural time-series model:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n  ```\n\n  To run variational inference, we simply construct the loss and optimize\n  it:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series)\n\n    train_op = tf.train.AdamOptimizer(0.1).minimize(variational_loss)\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      for step in range(200):\n        _, loss_ = sess.run((train_op, variational_loss))\n\n        if step % 20 == 0:\n          print(\"step {} loss {}\".format(step, loss_))\n\n      posterior_samples_ = sess.run({\n        param_name: q.sample(50)\n        for param_name, q in variational_distributions.items()})\n  ```\n\n  As a more complex example, we might try to avoid local optima by optimizing\n  from multiple initializations in parallel, and selecting the result with the\n  lowest loss:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series,\n       init_batch_shape=[10])\n\n    train_op = tf.train.AdamOptimizer(0.1).minimize(variational_loss)\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      for step in range(200):\n        _, loss_ = sess.run((train_op, variational_loss))\n\n        if step % 20 == 0:\n          print(\"step {} losses {}\".format(step, loss_))\n\n      # Draw multiple samples to reduce Monte Carlo error in the optimized\n      # variational bounds.\n      avg_loss = np.mean(\n        [sess.run(variational_loss) for _ in range(25)], axis=0)\n      best_posterior_idx = np.argmin(avg_loss, axis=0).astype(np.int32)\n  ```\n\n  #### References\n\n  [1]: Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and\n       David M. Blei. Automatic Differentiation Variational Inference. In\n       _Journal of Machine Learning Research_, 2017.\n       https://arxiv.org/abs/1603.00788", "docstring_tokens": ["Build", "a", "loss", "function", "for", "variational", "inference", "in", "STS", "models", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/fitting.py#L92-L264", "partition": "test", "index": 699, "time": "2018-10-29 12:40:53"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/fitting.py", "func_name": "_minimize_in_graph", "original_string": "def _minimize_in_graph(build_loss_fn, num_steps=200, optimizer=None):\n  \"\"\"Run an optimizer within the graph to minimize a loss function.\"\"\"\n  optimizer = tf.compat.v1.train.AdamOptimizer(\n      0.1) if optimizer is None else optimizer\n\n  def train_loop_body(step):\n    train_op = optimizer.minimize(\n        build_loss_fn if tf.executing_eagerly() else build_loss_fn())\n    return tf.tuple(tensors=[tf.add(step, 1)], control_inputs=[train_op])\n\n  minimize_op = tf.compat.v1.while_loop(\n      cond=lambda step: step < num_steps,\n      body=train_loop_body,\n      loop_vars=[tf.constant(0)],\n      return_same_structure=True)[0]  # Always return a single op.\n  return minimize_op", "language": "python", "code": "def _minimize_in_graph(build_loss_fn, num_steps=200, optimizer=None):\n  \"\"\"Run an optimizer within the graph to minimize a loss function.\"\"\"\n  optimizer = tf.compat.v1.train.AdamOptimizer(\n      0.1) if optimizer is None else optimizer\n\n  def train_loop_body(step):\n    train_op = optimizer.minimize(\n        build_loss_fn if tf.executing_eagerly() else build_loss_fn())\n    return tf.tuple(tensors=[tf.add(step, 1)], control_inputs=[train_op])\n\n  minimize_op = tf.compat.v1.while_loop(\n      cond=lambda step: step < num_steps,\n      body=train_loop_body,\n      loop_vars=[tf.constant(0)],\n      return_same_structure=True)[0]  # Always return a single op.\n  return minimize_op", "code_tokens": ["def", "_minimize_in_graph", "(", "build_loss_fn", ",", "num_steps", "=", "200", ",", "optimizer", "=", "None", ")", ":", "optimizer", "=", "tf", ".", "compat", ".", "v1", ".", "train", ".", "AdamOptimizer", "(", "0.1", ")", "if", "optimizer", "is", "None", "else", "optimizer", "def", "train_loop_body", "(", "step", ")", ":", "train_op", "=", "optimizer", ".", "minimize", "(", "build_loss_fn", "if", "tf", ".", "executing_eagerly", "(", ")", "else", "build_loss_fn", "(", ")", ")", "return", "tf", ".", "tuple", "(", "tensors", "=", "[", "tf", ".", "add", "(", "step", ",", "1", ")", "]", ",", "control_inputs", "=", "[", "train_op", "]", ")", "minimize_op", "=", "tf", ".", "compat", ".", "v1", ".", "while_loop", "(", "cond", "=", "lambda", "step", ":", "step", "<", "num_steps", ",", "body", "=", "train_loop_body", ",", "loop_vars", "=", "[", "tf", ".", "constant", "(", "0", ")", "]", ",", "return_same_structure", "=", "True", ")", "[", "0", "]", "# Always return a single op.", "return", "minimize_op"], "docstring": "Run an optimizer within the graph to minimize a loss function.", "docstring_tokens": ["Run", "an", "optimizer", "within", "the", "graph", "to", "minimize", "a", "loss", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/fitting.py#L267-L282", "partition": "test", "index": 700, "time": "2018-10-29 12:40:53"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/sample_stats.py", "func_name": "stddev", "original_string": "def stddev(x, sample_axis=0, keepdims=False, name=None):\n  \"\"\"Estimate standard deviation using samples.\n\n  Given `N` samples of scalar valued random variable `X`, standard deviation may\n  be estimated as\n\n  ```none\n  Stddev[X] := Sqrt[Var[X]],\n  Var[X] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(X_n - Xbar)},\n  Xbar := N^{-1} sum_{n=1}^N X_n\n  ```\n\n  ```python\n  x = tf.random_normal(shape=(100, 2, 3))\n\n  # stddev[i, j] is the sample standard deviation of the (i, j) batch member.\n  stddev = tfp.stats.stddev(x, sample_axis=0)\n  ```\n\n  Scaling a unit normal by a standard deviation produces normal samples\n  with that standard deviation.\n\n  ```python\n  observed_data = read_data_samples(...)\n  stddev = tfp.stats.stddev(observed_data)\n\n  # Make fake_data with the same standard deviation as observed_data.\n  fake_data = stddev * tf.random_normal(shape=(100,))\n  ```\n\n  Notice we divide by `N` (the numpy default), which does not create `NaN`\n  when `N = 1`, but is slightly biased.\n\n  Args:\n    x:  A numeric `Tensor` holding samples.\n    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or\n      `None` (meaning all axis hold samples).\n      Default value: `0` (leftmost dimension).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'stddev'`).\n\n  Returns:\n    stddev: A `Tensor` of same `dtype` as the `x`, and rank equal to\n      `rank(x) - len(sample_axis)`\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'stddev', values=[x, sample_axis]):\n    return tf.sqrt(variance(x, sample_axis=sample_axis, keepdims=keepdims))", "language": "python", "code": "def stddev(x, sample_axis=0, keepdims=False, name=None):\n  \"\"\"Estimate standard deviation using samples.\n\n  Given `N` samples of scalar valued random variable `X`, standard deviation may\n  be estimated as\n\n  ```none\n  Stddev[X] := Sqrt[Var[X]],\n  Var[X] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(X_n - Xbar)},\n  Xbar := N^{-1} sum_{n=1}^N X_n\n  ```\n\n  ```python\n  x = tf.random_normal(shape=(100, 2, 3))\n\n  # stddev[i, j] is the sample standard deviation of the (i, j) batch member.\n  stddev = tfp.stats.stddev(x, sample_axis=0)\n  ```\n\n  Scaling a unit normal by a standard deviation produces normal samples\n  with that standard deviation.\n\n  ```python\n  observed_data = read_data_samples(...)\n  stddev = tfp.stats.stddev(observed_data)\n\n  # Make fake_data with the same standard deviation as observed_data.\n  fake_data = stddev * tf.random_normal(shape=(100,))\n  ```\n\n  Notice we divide by `N` (the numpy default), which does not create `NaN`\n  when `N = 1`, but is slightly biased.\n\n  Args:\n    x:  A numeric `Tensor` holding samples.\n    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or\n      `None` (meaning all axis hold samples).\n      Default value: `0` (leftmost dimension).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'stddev'`).\n\n  Returns:\n    stddev: A `Tensor` of same `dtype` as the `x`, and rank equal to\n      `rank(x) - len(sample_axis)`\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'stddev', values=[x, sample_axis]):\n    return tf.sqrt(variance(x, sample_axis=sample_axis, keepdims=keepdims))", "code_tokens": ["def", "stddev", "(", "x", ",", "sample_axis", "=", "0", ",", "keepdims", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'stddev'", ",", "values", "=", "[", "x", ",", "sample_axis", "]", ")", ":", "return", "tf", ".", "sqrt", "(", "variance", "(", "x", ",", "sample_axis", "=", "sample_axis", ",", "keepdims", "=", "keepdims", ")", ")"], "docstring": "Estimate standard deviation using samples.\n\n  Given `N` samples of scalar valued random variable `X`, standard deviation may\n  be estimated as\n\n  ```none\n  Stddev[X] := Sqrt[Var[X]],\n  Var[X] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(X_n - Xbar)},\n  Xbar := N^{-1} sum_{n=1}^N X_n\n  ```\n\n  ```python\n  x = tf.random_normal(shape=(100, 2, 3))\n\n  # stddev[i, j] is the sample standard deviation of the (i, j) batch member.\n  stddev = tfp.stats.stddev(x, sample_axis=0)\n  ```\n\n  Scaling a unit normal by a standard deviation produces normal samples\n  with that standard deviation.\n\n  ```python\n  observed_data = read_data_samples(...)\n  stddev = tfp.stats.stddev(observed_data)\n\n  # Make fake_data with the same standard deviation as observed_data.\n  fake_data = stddev * tf.random_normal(shape=(100,))\n  ```\n\n  Notice we divide by `N` (the numpy default), which does not create `NaN`\n  when `N = 1`, but is slightly biased.\n\n  Args:\n    x:  A numeric `Tensor` holding samples.\n    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or\n      `None` (meaning all axis hold samples).\n      Default value: `0` (leftmost dimension).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'stddev'`).\n\n  Returns:\n    stddev: A `Tensor` of same `dtype` as the `x`, and rank equal to\n      `rank(x) - len(sample_axis)`", "docstring_tokens": ["Estimate", "standard", "deviation", "using", "samples", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/sample_stats.py#L552-L599", "partition": "test", "index": 657, "time": "2018-10-30 15:32:40"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/sample_stats.py", "func_name": "cholesky_covariance", "original_string": "def cholesky_covariance(x, sample_axis=0, keepdims=False, name=None):\n  \"\"\"Cholesky factor of the covariance matrix of vector-variate random samples.\n\n  This function can be use to fit a multivariate normal to data.\n\n  ```python\n  tf.enable_eager_execution()\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Assume data.shape = (1000, 2).  1000 samples of a random variable in R^2.\n  observed_data = read_data_samples(...)\n\n  # The mean is easy\n  mu = tf.reduce_mean(observed_data, axis=0)\n\n  # Get the scale matrix\n  L = tfp.stats.cholesky_covariance(observed_data)\n\n  # Make the best fit multivariate normal (under maximum likelihood condition).\n  mvn = tfd.MultivariateNormalTriL(loc=mu, scale_tril=L)\n\n  # Plot contours of the pdf.\n  xs, ys = tf.meshgrid(\n      tf.linspace(-5., 5., 50), tf.linspace(-5., 5., 50), indexing='ij')\n  xy = tf.stack((tf.reshape(xs, [-1]), tf.reshape(ys, [-1])), axis=-1)\n  pdf = tf.reshape(mvn.prob(xy), (50, 50))\n  CS = plt.contour(xs, ys, pdf, 10)\n  plt.clabel(CS, inline=1, fontsize=10)\n  ```\n\n  Why does this work?\n  Given vector-variate random variables `X = (X1, ..., Xd)`, one may obtain the\n  sample covariance matrix in `R^{d x d}` (see `tfp.stats.covariance`).\n\n  The [Cholesky factor](https://en.wikipedia.org/wiki/Cholesky_decomposition)\n  of this matrix is analogous to standard deviation for scalar random variables:\n  Suppose `X` has covariance matrix `C`, with Cholesky factorization `C = L L^T`\n  Then multiplying a vector of iid random variables which have unit variance by\n  `L` produces a vector with covariance `L L^T`, which is the same as `X`.\n\n  ```python\n  observed_data = read_data_samples(...)\n  L = tfp.stats.cholesky_covariance(observed_data, sample_axis=0)\n\n  # Make fake_data with the same covariance as observed_data.\n  uncorrelated_normal = tf.random_normal(shape=(500, 10))\n  fake_data = tf.linalg.matvec(L, uncorrelated_normal)\n  ```\n\n  Args:\n    x:  Numeric `Tensor`.  The rightmost dimension of `x` indexes events. E.g.\n      dimensions of a random vector.\n    sample_axis: Scalar or vector `Tensor` designating axis holding samples.\n      Default value: `0` (leftmost dimension). Cannot be the rightmost dimension\n        (since this indexes events).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'covariance'`).\n\n  Returns:\n    chol:  `Tensor` of same `dtype` as `x`.  The last two dimensions hold\n      lower triangular matrices (the Cholesky factors).\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'cholesky_covariance', values=[x, sample_axis]):\n    sample_axis = tf.convert_to_tensor(value=sample_axis, dtype=tf.int32)\n    cov = covariance(\n        x, sample_axis=sample_axis, event_axis=-1, keepdims=keepdims)\n    return tf.linalg.cholesky(cov)", "language": "python", "code": "def cholesky_covariance(x, sample_axis=0, keepdims=False, name=None):\n  \"\"\"Cholesky factor of the covariance matrix of vector-variate random samples.\n\n  This function can be use to fit a multivariate normal to data.\n\n  ```python\n  tf.enable_eager_execution()\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Assume data.shape = (1000, 2).  1000 samples of a random variable in R^2.\n  observed_data = read_data_samples(...)\n\n  # The mean is easy\n  mu = tf.reduce_mean(observed_data, axis=0)\n\n  # Get the scale matrix\n  L = tfp.stats.cholesky_covariance(observed_data)\n\n  # Make the best fit multivariate normal (under maximum likelihood condition).\n  mvn = tfd.MultivariateNormalTriL(loc=mu, scale_tril=L)\n\n  # Plot contours of the pdf.\n  xs, ys = tf.meshgrid(\n      tf.linspace(-5., 5., 50), tf.linspace(-5., 5., 50), indexing='ij')\n  xy = tf.stack((tf.reshape(xs, [-1]), tf.reshape(ys, [-1])), axis=-1)\n  pdf = tf.reshape(mvn.prob(xy), (50, 50))\n  CS = plt.contour(xs, ys, pdf, 10)\n  plt.clabel(CS, inline=1, fontsize=10)\n  ```\n\n  Why does this work?\n  Given vector-variate random variables `X = (X1, ..., Xd)`, one may obtain the\n  sample covariance matrix in `R^{d x d}` (see `tfp.stats.covariance`).\n\n  The [Cholesky factor](https://en.wikipedia.org/wiki/Cholesky_decomposition)\n  of this matrix is analogous to standard deviation for scalar random variables:\n  Suppose `X` has covariance matrix `C`, with Cholesky factorization `C = L L^T`\n  Then multiplying a vector of iid random variables which have unit variance by\n  `L` produces a vector with covariance `L L^T`, which is the same as `X`.\n\n  ```python\n  observed_data = read_data_samples(...)\n  L = tfp.stats.cholesky_covariance(observed_data, sample_axis=0)\n\n  # Make fake_data with the same covariance as observed_data.\n  uncorrelated_normal = tf.random_normal(shape=(500, 10))\n  fake_data = tf.linalg.matvec(L, uncorrelated_normal)\n  ```\n\n  Args:\n    x:  Numeric `Tensor`.  The rightmost dimension of `x` indexes events. E.g.\n      dimensions of a random vector.\n    sample_axis: Scalar or vector `Tensor` designating axis holding samples.\n      Default value: `0` (leftmost dimension). Cannot be the rightmost dimension\n        (since this indexes events).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'covariance'`).\n\n  Returns:\n    chol:  `Tensor` of same `dtype` as `x`.  The last two dimensions hold\n      lower triangular matrices (the Cholesky factors).\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'cholesky_covariance', values=[x, sample_axis]):\n    sample_axis = tf.convert_to_tensor(value=sample_axis, dtype=tf.int32)\n    cov = covariance(\n        x, sample_axis=sample_axis, event_axis=-1, keepdims=keepdims)\n    return tf.linalg.cholesky(cov)", "code_tokens": ["def", "cholesky_covariance", "(", "x", ",", "sample_axis", "=", "0", ",", "keepdims", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'cholesky_covariance'", ",", "values", "=", "[", "x", ",", "sample_axis", "]", ")", ":", "sample_axis", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "sample_axis", ",", "dtype", "=", "tf", ".", "int32", ")", "cov", "=", "covariance", "(", "x", ",", "sample_axis", "=", "sample_axis", ",", "event_axis", "=", "-", "1", ",", "keepdims", "=", "keepdims", ")", "return", "tf", ".", "linalg", ".", "cholesky", "(", "cov", ")"], "docstring": "Cholesky factor of the covariance matrix of vector-variate random samples.\n\n  This function can be use to fit a multivariate normal to data.\n\n  ```python\n  tf.enable_eager_execution()\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Assume data.shape = (1000, 2).  1000 samples of a random variable in R^2.\n  observed_data = read_data_samples(...)\n\n  # The mean is easy\n  mu = tf.reduce_mean(observed_data, axis=0)\n\n  # Get the scale matrix\n  L = tfp.stats.cholesky_covariance(observed_data)\n\n  # Make the best fit multivariate normal (under maximum likelihood condition).\n  mvn = tfd.MultivariateNormalTriL(loc=mu, scale_tril=L)\n\n  # Plot contours of the pdf.\n  xs, ys = tf.meshgrid(\n      tf.linspace(-5., 5., 50), tf.linspace(-5., 5., 50), indexing='ij')\n  xy = tf.stack((tf.reshape(xs, [-1]), tf.reshape(ys, [-1])), axis=-1)\n  pdf = tf.reshape(mvn.prob(xy), (50, 50))\n  CS = plt.contour(xs, ys, pdf, 10)\n  plt.clabel(CS, inline=1, fontsize=10)\n  ```\n\n  Why does this work?\n  Given vector-variate random variables `X = (X1, ..., Xd)`, one may obtain the\n  sample covariance matrix in `R^{d x d}` (see `tfp.stats.covariance`).\n\n  The [Cholesky factor](https://en.wikipedia.org/wiki/Cholesky_decomposition)\n  of this matrix is analogous to standard deviation for scalar random variables:\n  Suppose `X` has covariance matrix `C`, with Cholesky factorization `C = L L^T`\n  Then multiplying a vector of iid random variables which have unit variance by\n  `L` produces a vector with covariance `L L^T`, which is the same as `X`.\n\n  ```python\n  observed_data = read_data_samples(...)\n  L = tfp.stats.cholesky_covariance(observed_data, sample_axis=0)\n\n  # Make fake_data with the same covariance as observed_data.\n  uncorrelated_normal = tf.random_normal(shape=(500, 10))\n  fake_data = tf.linalg.matvec(L, uncorrelated_normal)\n  ```\n\n  Args:\n    x:  Numeric `Tensor`.  The rightmost dimension of `x` indexes events. E.g.\n      dimensions of a random vector.\n    sample_axis: Scalar or vector `Tensor` designating axis holding samples.\n      Default value: `0` (leftmost dimension). Cannot be the rightmost dimension\n        (since this indexes events).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'covariance'`).\n\n  Returns:\n    chol:  `Tensor` of same `dtype` as `x`.  The last two dimensions hold\n      lower triangular matrices (the Cholesky factors).", "docstring_tokens": ["Cholesky", "factor", "of", "the", "covariance", "matrix", "of", "vector", "-", "variate", "random", "samples", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/sample_stats.py#L212-L281", "partition": "test", "index": 656, "time": "2018-10-30 15:32:40"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/__init__.py", "func_name": "_ensure_tf_install", "original_string": "def _ensure_tf_install():  # pylint: disable=g-statement-before-imports\n  \"\"\"Attempt to import tensorflow, and ensure its version is sufficient.\n\n  Raises:\n    ImportError: if either tensorflow is not importable or its version is\n    inadequate.\n  \"\"\"\n  try:\n    import tensorflow as tf\n  except ImportError:\n    # Print more informative error message, then reraise.\n    print(\"\\n\\nFailed to import TensorFlow. Please note that TensorFlow is not \"\n          \"installed by default when you install TensorFlow Probability. This \"\n          \"is so that users can decide whether to install the GPU-enabled \"\n          \"TensorFlow package. To use TensorFlow Probability, please install \"\n          \"the most recent version of TensorFlow, by following instructions at \"\n          \"https://tensorflow.org/install.\\n\\n\")\n    raise\n\n  import distutils.version\n\n  #\n  # Update this whenever we need to depend on a newer TensorFlow release.\n  #\n  required_tensorflow_version = \"1.13\"\n\n  if (distutils.version.LooseVersion(tf.__version__) <\n      distutils.version.LooseVersion(required_tensorflow_version)):\n    raise ImportError(\n        \"This version of TensorFlow Probability requires TensorFlow \"\n        \"version >= {required}; Detected an installation of version {present}. \"\n        \"Please upgrade TensorFlow to proceed.\".format(\n            required=required_tensorflow_version,\n            present=tf.__version__))", "language": "python", "code": "def _ensure_tf_install():  # pylint: disable=g-statement-before-imports\n  \"\"\"Attempt to import tensorflow, and ensure its version is sufficient.\n\n  Raises:\n    ImportError: if either tensorflow is not importable or its version is\n    inadequate.\n  \"\"\"\n  try:\n    import tensorflow as tf\n  except ImportError:\n    # Print more informative error message, then reraise.\n    print(\"\\n\\nFailed to import TensorFlow. Please note that TensorFlow is not \"\n          \"installed by default when you install TensorFlow Probability. This \"\n          \"is so that users can decide whether to install the GPU-enabled \"\n          \"TensorFlow package. To use TensorFlow Probability, please install \"\n          \"the most recent version of TensorFlow, by following instructions at \"\n          \"https://tensorflow.org/install.\\n\\n\")\n    raise\n\n  import distutils.version\n\n  #\n  # Update this whenever we need to depend on a newer TensorFlow release.\n  #\n  required_tensorflow_version = \"1.13\"\n\n  if (distutils.version.LooseVersion(tf.__version__) <\n      distutils.version.LooseVersion(required_tensorflow_version)):\n    raise ImportError(\n        \"This version of TensorFlow Probability requires TensorFlow \"\n        \"version >= {required}; Detected an installation of version {present}. \"\n        \"Please upgrade TensorFlow to proceed.\".format(\n            required=required_tensorflow_version,\n            present=tf.__version__))", "code_tokens": ["def", "_ensure_tf_install", "(", ")", ":", "# pylint: disable=g-statement-before-imports", "try", ":", "import", "tensorflow", "as", "tf", "except", "ImportError", ":", "# Print more informative error message, then reraise.", "print", "(", "\"\\n\\nFailed to import TensorFlow. Please note that TensorFlow is not \"", "\"installed by default when you install TensorFlow Probability. This \"", "\"is so that users can decide whether to install the GPU-enabled \"", "\"TensorFlow package. To use TensorFlow Probability, please install \"", "\"the most recent version of TensorFlow, by following instructions at \"", "\"https://tensorflow.org/install.\\n\\n\"", ")", "raise", "import", "distutils", ".", "version", "#", "# Update this whenever we need to depend on a newer TensorFlow release.", "#", "required_tensorflow_version", "=", "\"1.13\"", "if", "(", "distutils", ".", "version", ".", "LooseVersion", "(", "tf", ".", "__version__", ")", "<", "distutils", ".", "version", ".", "LooseVersion", "(", "required_tensorflow_version", ")", ")", ":", "raise", "ImportError", "(", "\"This version of TensorFlow Probability requires TensorFlow \"", "\"version >= {required}; Detected an installation of version {present}. \"", "\"Please upgrade TensorFlow to proceed.\"", ".", "format", "(", "required", "=", "required_tensorflow_version", ",", "present", "=", "tf", ".", "__version__", ")", ")"], "docstring": "Attempt to import tensorflow, and ensure its version is sufficient.\n\n  Raises:\n    ImportError: if either tensorflow is not importable or its version is\n    inadequate.", "docstring_tokens": ["Attempt", "to", "import", "tensorflow", "and", "ensure", "its", "version", "is", "sufficient", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/__init__.py#L32-L65", "partition": "test", "index": 653, "time": "2018-10-31 12:11:52"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/hidden_markov_model.py", "func_name": "_log_vector_matrix", "original_string": "def _log_vector_matrix(vs, ms):\n  \"\"\"Multiply tensor of vectors by matrices assuming values stored are logs.\"\"\"\n\n  return tf.reduce_logsumexp(input_tensor=vs[..., tf.newaxis] + ms, axis=-2)", "language": "python", "code": "def _log_vector_matrix(vs, ms):\n  \"\"\"Multiply tensor of vectors by matrices assuming values stored are logs.\"\"\"\n\n  return tf.reduce_logsumexp(input_tensor=vs[..., tf.newaxis] + ms, axis=-2)", "code_tokens": ["def", "_log_vector_matrix", "(", "vs", ",", "ms", ")", ":", "return", "tf", ".", "reduce_logsumexp", "(", "input_tensor", "=", "vs", "[", "...", ",", "tf", ".", "newaxis", "]", "+", "ms", ",", "axis", "=", "-", "2", ")"], "docstring": "Multiply tensor of vectors by matrices assuming values stored are logs.", "docstring_tokens": ["Multiply", "tensor", "of", "vectors", "by", "matrices", "assuming", "values", "stored", "are", "logs", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/hidden_markov_model.py#L908-L911", "partition": "test", "index": 686, "time": "2018-11-06 09:38:43"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/hidden_markov_model.py", "func_name": "_extract_log_probs", "original_string": "def _extract_log_probs(num_states, dist):\n  \"\"\"Tabulate log probabilities from a batch of distributions.\"\"\"\n\n  states = tf.reshape(tf.range(num_states),\n                      tf.concat([[num_states],\n                                 tf.ones_like(dist.batch_shape_tensor())],\n                                axis=0))\n  return distribution_util.move_dimension(dist.log_prob(states), 0, -1)", "language": "python", "code": "def _extract_log_probs(num_states, dist):\n  \"\"\"Tabulate log probabilities from a batch of distributions.\"\"\"\n\n  states = tf.reshape(tf.range(num_states),\n                      tf.concat([[num_states],\n                                 tf.ones_like(dist.batch_shape_tensor())],\n                                axis=0))\n  return distribution_util.move_dimension(dist.log_prob(states), 0, -1)", "code_tokens": ["def", "_extract_log_probs", "(", "num_states", ",", "dist", ")", ":", "states", "=", "tf", ".", "reshape", "(", "tf", ".", "range", "(", "num_states", ")", ",", "tf", ".", "concat", "(", "[", "[", "num_states", "]", ",", "tf", ".", "ones_like", "(", "dist", ".", "batch_shape_tensor", "(", ")", ")", "]", ",", "axis", "=", "0", ")", ")", "return", "distribution_util", ".", "move_dimension", "(", "dist", ".", "log_prob", "(", "states", ")", ",", "0", ",", "-", "1", ")"], "docstring": "Tabulate log probabilities from a batch of distributions.", "docstring_tokens": ["Tabulate", "log", "probabilities", "from", "a", "batch", "of", "distributions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/hidden_markov_model.py#L926-L933", "partition": "test", "index": 689, "time": "2018-11-06 09:38:43"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/hidden_markov_model.py", "func_name": "_vector_matrix", "original_string": "def _vector_matrix(vs, ms):\n  \"\"\"Multiply tensor of vectors by matrices.\"\"\"\n\n  return tf.reduce_sum(input_tensor=vs[..., tf.newaxis] * ms, axis=-2)", "language": "python", "code": "def _vector_matrix(vs, ms):\n  \"\"\"Multiply tensor of vectors by matrices.\"\"\"\n\n  return tf.reduce_sum(input_tensor=vs[..., tf.newaxis] * ms, axis=-2)", "code_tokens": ["def", "_vector_matrix", "(", "vs", ",", "ms", ")", ":", "return", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "vs", "[", "...", ",", "tf", ".", "newaxis", "]", "*", "ms", ",", "axis", "=", "-", "2", ")"], "docstring": "Multiply tensor of vectors by matrices.", "docstring_tokens": ["Multiply", "tensor", "of", "vectors", "by", "matrices", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/hidden_markov_model.py#L920-L923", "partition": "test", "index": 688, "time": "2018-11-06 09:38:43"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/hidden_markov_model.py", "func_name": "HiddenMarkovModel._marginal_hidden_probs", "original_string": "def _marginal_hidden_probs(self):\n    \"\"\"Compute marginal pdf for each individual observable.\"\"\"\n\n    initial_log_probs = tf.broadcast_to(self._log_init,\n                                        tf.concat([self.batch_shape_tensor(),\n                                                   [self._num_states]],\n                                                  axis=0))\n    # initial_log_probs :: batch_shape num_states\n\n    if self._num_steps > 1:\n      transition_log_probs = self._log_trans\n\n      def forward_step(log_probs, _):\n        return _log_vector_matrix(log_probs, transition_log_probs)\n\n      dummy_index = tf.zeros(self._num_steps - 1, dtype=tf.float32)\n\n      forward_log_probs = tf.scan(forward_step, dummy_index,\n                                  initializer=initial_log_probs,\n                                  name=\"forward_log_probs\")\n\n      forward_log_probs = tf.concat([[initial_log_probs], forward_log_probs],\n                                    axis=0)\n    else:\n      forward_log_probs = initial_log_probs[tf.newaxis, ...]\n\n    # returns :: num_steps batch_shape num_states\n\n    return tf.exp(forward_log_probs)", "language": "python", "code": "def _marginal_hidden_probs(self):\n    \"\"\"Compute marginal pdf for each individual observable.\"\"\"\n\n    initial_log_probs = tf.broadcast_to(self._log_init,\n                                        tf.concat([self.batch_shape_tensor(),\n                                                   [self._num_states]],\n                                                  axis=0))\n    # initial_log_probs :: batch_shape num_states\n\n    if self._num_steps > 1:\n      transition_log_probs = self._log_trans\n\n      def forward_step(log_probs, _):\n        return _log_vector_matrix(log_probs, transition_log_probs)\n\n      dummy_index = tf.zeros(self._num_steps - 1, dtype=tf.float32)\n\n      forward_log_probs = tf.scan(forward_step, dummy_index,\n                                  initializer=initial_log_probs,\n                                  name=\"forward_log_probs\")\n\n      forward_log_probs = tf.concat([[initial_log_probs], forward_log_probs],\n                                    axis=0)\n    else:\n      forward_log_probs = initial_log_probs[tf.newaxis, ...]\n\n    # returns :: num_steps batch_shape num_states\n\n    return tf.exp(forward_log_probs)", "code_tokens": ["def", "_marginal_hidden_probs", "(", "self", ")", ":", "initial_log_probs", "=", "tf", ".", "broadcast_to", "(", "self", ".", "_log_init", ",", "tf", ".", "concat", "(", "[", "self", ".", "batch_shape_tensor", "(", ")", ",", "[", "self", ".", "_num_states", "]", "]", ",", "axis", "=", "0", ")", ")", "# initial_log_probs :: batch_shape num_states", "if", "self", ".", "_num_steps", ">", "1", ":", "transition_log_probs", "=", "self", ".", "_log_trans", "def", "forward_step", "(", "log_probs", ",", "_", ")", ":", "return", "_log_vector_matrix", "(", "log_probs", ",", "transition_log_probs", ")", "dummy_index", "=", "tf", ".", "zeros", "(", "self", ".", "_num_steps", "-", "1", ",", "dtype", "=", "tf", ".", "float32", ")", "forward_log_probs", "=", "tf", ".", "scan", "(", "forward_step", ",", "dummy_index", ",", "initializer", "=", "initial_log_probs", ",", "name", "=", "\"forward_log_probs\"", ")", "forward_log_probs", "=", "tf", ".", "concat", "(", "[", "[", "initial_log_probs", "]", ",", "forward_log_probs", "]", ",", "axis", "=", "0", ")", "else", ":", "forward_log_probs", "=", "initial_log_probs", "[", "tf", ".", "newaxis", ",", "...", "]", "# returns :: num_steps batch_shape num_states", "return", "tf", ".", "exp", "(", "forward_log_probs", ")"], "docstring": "Compute marginal pdf for each individual observable.", "docstring_tokens": ["Compute", "marginal", "pdf", "for", "each", "individual", "observable", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/hidden_markov_model.py#L489-L517", "partition": "test", "index": 690, "time": "2018-11-06 09:38:43"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs.py", "func_name": "_mul_right", "original_string": "def _mul_right(mat, vec):\n  \"\"\"Computes the product of a matrix with a vector on the right.\n\n  Note this supports dynamic shapes and batched computation.\n\n  Examples:\n\n    M = tf.reshape(tf.range(6), shape=(3, 2))\n    # => [[0, 1],\n    #     [2, 3],\n    #     [4, 5]]\n    v = tf.constant([1, 2])  # Shape: (2,)\n    _mul_right(M, v)\n    # => [ 2,  8, 14]  # Shape: (3,)\n\n    M = tf.reshape(tf.range(30), shape=(2, 3, 5))\n    # => [[[ 0,  1,  2,  3,  4],\n    #     [ 5,  6,  7,  8,  9],\n    #     [10, 11, 12, 13, 14]],\n    #\n    #    [[15, 16, 17, 18, 19],\n    #     [20, 21, 22, 23, 24],\n    #     [25, 26, 27, 28, 29]]]\n    v = tf.reshape(tf.range(10), shape=(2, 5))\n    # => [[0, 1, 2, 3, 4],\n    #     [5, 6, 7, 8, 9]]\n    _mul_right(M, v)\n    # => [[ 30,  80, 130],\n    #     [605, 780, 955]]  # Shape: (2, 3)\n\n  Args:\n    mat: A `tf.Tensor` of shape `[..., n, m]`.\n    vec: A `tf.Tensor` of shape `[..., m]`.\n\n  Returns:\n    A tensor of shape `[..., n]` with matching batch dimensions.\n  \"\"\"\n  return tf.squeeze(tf.matmul(mat, tf.expand_dims(vec, axis=-1)), axis=-1)", "language": "python", "code": "def _mul_right(mat, vec):\n  \"\"\"Computes the product of a matrix with a vector on the right.\n\n  Note this supports dynamic shapes and batched computation.\n\n  Examples:\n\n    M = tf.reshape(tf.range(6), shape=(3, 2))\n    # => [[0, 1],\n    #     [2, 3],\n    #     [4, 5]]\n    v = tf.constant([1, 2])  # Shape: (2,)\n    _mul_right(M, v)\n    # => [ 2,  8, 14]  # Shape: (3,)\n\n    M = tf.reshape(tf.range(30), shape=(2, 3, 5))\n    # => [[[ 0,  1,  2,  3,  4],\n    #     [ 5,  6,  7,  8,  9],\n    #     [10, 11, 12, 13, 14]],\n    #\n    #    [[15, 16, 17, 18, 19],\n    #     [20, 21, 22, 23, 24],\n    #     [25, 26, 27, 28, 29]]]\n    v = tf.reshape(tf.range(10), shape=(2, 5))\n    # => [[0, 1, 2, 3, 4],\n    #     [5, 6, 7, 8, 9]]\n    _mul_right(M, v)\n    # => [[ 30,  80, 130],\n    #     [605, 780, 955]]  # Shape: (2, 3)\n\n  Args:\n    mat: A `tf.Tensor` of shape `[..., n, m]`.\n    vec: A `tf.Tensor` of shape `[..., m]`.\n\n  Returns:\n    A tensor of shape `[..., n]` with matching batch dimensions.\n  \"\"\"\n  return tf.squeeze(tf.matmul(mat, tf.expand_dims(vec, axis=-1)), axis=-1)", "code_tokens": ["def", "_mul_right", "(", "mat", ",", "vec", ")", ":", "return", "tf", ".", "squeeze", "(", "tf", ".", "matmul", "(", "mat", ",", "tf", ".", "expand_dims", "(", "vec", ",", "axis", "=", "-", "1", ")", ")", ",", "axis", "=", "-", "1", ")"], "docstring": "Computes the product of a matrix with a vector on the right.\n\n  Note this supports dynamic shapes and batched computation.\n\n  Examples:\n\n    M = tf.reshape(tf.range(6), shape=(3, 2))\n    # => [[0, 1],\n    #     [2, 3],\n    #     [4, 5]]\n    v = tf.constant([1, 2])  # Shape: (2,)\n    _mul_right(M, v)\n    # => [ 2,  8, 14]  # Shape: (3,)\n\n    M = tf.reshape(tf.range(30), shape=(2, 3, 5))\n    # => [[[ 0,  1,  2,  3,  4],\n    #     [ 5,  6,  7,  8,  9],\n    #     [10, 11, 12, 13, 14]],\n    #\n    #    [[15, 16, 17, 18, 19],\n    #     [20, 21, 22, 23, 24],\n    #     [25, 26, 27, 28, 29]]]\n    v = tf.reshape(tf.range(10), shape=(2, 5))\n    # => [[0, 1, 2, 3, 4],\n    #     [5, 6, 7, 8, 9]]\n    _mul_right(M, v)\n    # => [[ 30,  80, 130],\n    #     [605, 780, 955]]  # Shape: (2, 3)\n\n  Args:\n    mat: A `tf.Tensor` of shape `[..., n, m]`.\n    vec: A `tf.Tensor` of shape `[..., m]`.\n\n  Returns:\n    A tensor of shape `[..., n]` with matching batch dimensions.", "docstring_tokens": ["Computes", "the", "product", "of", "a", "matrix", "with", "a", "vector", "on", "the", "right", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs.py#L436-L473", "partition": "test", "index": 1065, "time": "2018-11-14 01:41:57"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/interpolation.py", "func_name": "_assert_ndims_statically", "original_string": "def _assert_ndims_statically(x,\n                             expect_ndims=None,\n                             expect_ndims_at_least=None,\n                             expect_static=False):\n  \"\"\"Assert that Tensor x has expected number of dimensions.\"\"\"\n  ndims = x.shape.ndims\n  if ndims is None:\n    if expect_static:\n      raise ValueError('Expected static ndims. Found: {}'.format(x))\n    return\n  if expect_ndims is not None and ndims != expect_ndims:\n    raise ValueError('ndims must be {}.  Found: {}'.format(expect_ndims, ndims))\n  if expect_ndims_at_least is not None and ndims < expect_ndims_at_least:\n    raise ValueError('ndims must be at least {}. Found {}'.format(\n        expect_ndims_at_least, ndims))", "language": "python", "code": "def _assert_ndims_statically(x,\n                             expect_ndims=None,\n                             expect_ndims_at_least=None,\n                             expect_static=False):\n  \"\"\"Assert that Tensor x has expected number of dimensions.\"\"\"\n  ndims = x.shape.ndims\n  if ndims is None:\n    if expect_static:\n      raise ValueError('Expected static ndims. Found: {}'.format(x))\n    return\n  if expect_ndims is not None and ndims != expect_ndims:\n    raise ValueError('ndims must be {}.  Found: {}'.format(expect_ndims, ndims))\n  if expect_ndims_at_least is not None and ndims < expect_ndims_at_least:\n    raise ValueError('ndims must be at least {}. Found {}'.format(\n        expect_ndims_at_least, ndims))", "code_tokens": ["def", "_assert_ndims_statically", "(", "x", ",", "expect_ndims", "=", "None", ",", "expect_ndims_at_least", "=", "None", ",", "expect_static", "=", "False", ")", ":", "ndims", "=", "x", ".", "shape", ".", "ndims", "if", "ndims", "is", "None", ":", "if", "expect_static", ":", "raise", "ValueError", "(", "'Expected static ndims. Found: {}'", ".", "format", "(", "x", ")", ")", "return", "if", "expect_ndims", "is", "not", "None", "and", "ndims", "!=", "expect_ndims", ":", "raise", "ValueError", "(", "'ndims must be {}.  Found: {}'", ".", "format", "(", "expect_ndims", ",", "ndims", ")", ")", "if", "expect_ndims_at_least", "is", "not", "None", "and", "ndims", "<", "expect_ndims_at_least", ":", "raise", "ValueError", "(", "'ndims must be at least {}. Found {}'", ".", "format", "(", "expect_ndims_at_least", ",", "ndims", ")", ")"], "docstring": "Assert that Tensor x has expected number of dimensions.", "docstring_tokens": ["Assert", "that", "Tensor", "x", "has", "expected", "number", "of", "dimensions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/interpolation.py#L839-L853", "partition": "test", "index": 1056, "time": "2018-11-14 15:22:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs.py", "func_name": "_update_inv_hessian", "original_string": "def _update_inv_hessian(prev_state, next_state):\n  \"\"\"Update the BGFS state by computing the next inverse hessian estimate.\"\"\"\n  # Only update the inverse Hessian if not already failed or converged.\n  should_update = ~next_state.converged & ~next_state.failed\n\n  # Compute the normalization term (y^T . s), should not update if is singular.\n  gradient_delta = next_state.objective_gradient - prev_state.objective_gradient\n  position_delta = next_state.position - prev_state.position\n  normalization_factor = tf.reduce_sum(\n      input_tensor=gradient_delta * position_delta, axis=-1)\n  should_update = should_update & ~tf.equal(normalization_factor, 0)\n\n  def _do_update_inv_hessian():\n    next_inv_hessian = _bfgs_inv_hessian_update(\n        gradient_delta, position_delta, normalization_factor,\n        prev_state.inverse_hessian_estimate)\n    return bfgs_utils.update_fields(\n        next_state,\n        inverse_hessian_estimate=tf.where(should_update,\n                                          next_inv_hessian,\n                                          prev_state.inverse_hessian_estimate))\n\n  return prefer_static.cond(\n      tf.reduce_any(input_tensor=should_update),\n      _do_update_inv_hessian,\n      lambda: next_state)", "language": "python", "code": "def _update_inv_hessian(prev_state, next_state):\n  \"\"\"Update the BGFS state by computing the next inverse hessian estimate.\"\"\"\n  # Only update the inverse Hessian if not already failed or converged.\n  should_update = ~next_state.converged & ~next_state.failed\n\n  # Compute the normalization term (y^T . s), should not update if is singular.\n  gradient_delta = next_state.objective_gradient - prev_state.objective_gradient\n  position_delta = next_state.position - prev_state.position\n  normalization_factor = tf.reduce_sum(\n      input_tensor=gradient_delta * position_delta, axis=-1)\n  should_update = should_update & ~tf.equal(normalization_factor, 0)\n\n  def _do_update_inv_hessian():\n    next_inv_hessian = _bfgs_inv_hessian_update(\n        gradient_delta, position_delta, normalization_factor,\n        prev_state.inverse_hessian_estimate)\n    return bfgs_utils.update_fields(\n        next_state,\n        inverse_hessian_estimate=tf.where(should_update,\n                                          next_inv_hessian,\n                                          prev_state.inverse_hessian_estimate))\n\n  return prefer_static.cond(\n      tf.reduce_any(input_tensor=should_update),\n      _do_update_inv_hessian,\n      lambda: next_state)", "code_tokens": ["def", "_update_inv_hessian", "(", "prev_state", ",", "next_state", ")", ":", "# Only update the inverse Hessian if not already failed or converged.", "should_update", "=", "~", "next_state", ".", "converged", "&", "~", "next_state", ".", "failed", "# Compute the normalization term (y^T . s), should not update if is singular.", "gradient_delta", "=", "next_state", ".", "objective_gradient", "-", "prev_state", ".", "objective_gradient", "position_delta", "=", "next_state", ".", "position", "-", "prev_state", ".", "position", "normalization_factor", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "gradient_delta", "*", "position_delta", ",", "axis", "=", "-", "1", ")", "should_update", "=", "should_update", "&", "~", "tf", ".", "equal", "(", "normalization_factor", ",", "0", ")", "def", "_do_update_inv_hessian", "(", ")", ":", "next_inv_hessian", "=", "_bfgs_inv_hessian_update", "(", "gradient_delta", ",", "position_delta", ",", "normalization_factor", ",", "prev_state", ".", "inverse_hessian_estimate", ")", "return", "bfgs_utils", ".", "update_fields", "(", "next_state", ",", "inverse_hessian_estimate", "=", "tf", ".", "where", "(", "should_update", ",", "next_inv_hessian", ",", "prev_state", ".", "inverse_hessian_estimate", ")", ")", "return", "prefer_static", ".", "cond", "(", "tf", ".", "reduce_any", "(", "input_tensor", "=", "should_update", ")", ",", "_do_update_inv_hessian", ",", "lambda", ":", "next_state", ")"], "docstring": "Update the BGFS state by computing the next inverse hessian estimate.", "docstring_tokens": ["Update", "the", "BGFS", "state", "by", "computing", "the", "next", "inverse", "hessian", "estimate", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs.py#L327-L352", "partition": "test", "index": 1063, "time": "2018-11-15 00:12:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/quantiles.py", "func_name": "_get_static_ndims", "original_string": "def _get_static_ndims(x,\n                      expect_static=False,\n                      expect_ndims=None,\n                      expect_ndims_no_more_than=None,\n                      expect_ndims_at_least=None):\n  \"\"\"Get static number of dimensions and assert that some expectations are met.\n\n  This function returns the number of dimensions 'ndims' of x, as a Python int.\n\n  The optional expect arguments are used to check the ndims of x, but this is\n  only done if the static ndims of x is not None.\n\n  Args:\n    x:  A Tensor.\n    expect_static:  Expect `x` to have statically defined `ndims`.\n    expect_ndims:  Optional Python integer.  If provided, assert that x has\n      number of dimensions equal to this.\n    expect_ndims_no_more_than:  Optional Python integer.  If provided, assert\n      that x has no more than this many dimensions.\n    expect_ndims_at_least:  Optional Python integer.  If provided, assert that x\n      has at least this many dimensions.\n\n  Returns:\n    ndims:  A Python integer.\n\n  Raises:\n    ValueError:  If any of the expectations above are violated.\n  \"\"\"\n  ndims = x.shape.ndims\n  if ndims is None:\n    shape_const = tf.get_static_value(tf.shape(input=x))\n    if shape_const is not None:\n      ndims = shape_const.ndim\n\n  if ndims is None:\n    if expect_static:\n      raise ValueError(\n          'Expected argument `x` to have statically defined `ndims`.  Found: ' %\n          x)\n    return\n\n  if expect_ndims is not None:\n    ndims_message = ('Expected argument `x` to have ndims %s.  Found tensor %s'\n                     % (expect_ndims, x))\n    if ndims != expect_ndims:\n      raise ValueError(ndims_message)\n\n  if expect_ndims_at_least is not None:\n    ndims_at_least_message = (\n        'Expected argument `x` to have ndims >= %d.  Found tensor %s' %\n        (expect_ndims_at_least, x))\n    if ndims < expect_ndims_at_least:\n      raise ValueError(ndims_at_least_message)\n\n  if expect_ndims_no_more_than is not None:\n    ndims_no_more_than_message = (\n        'Expected argument `x` to have ndims <= %d.  Found tensor %s' %\n        (expect_ndims_no_more_than, x))\n    if ndims > expect_ndims_no_more_than:\n      raise ValueError(ndims_no_more_than_message)\n\n  return ndims", "language": "python", "code": "def _get_static_ndims(x,\n                      expect_static=False,\n                      expect_ndims=None,\n                      expect_ndims_no_more_than=None,\n                      expect_ndims_at_least=None):\n  \"\"\"Get static number of dimensions and assert that some expectations are met.\n\n  This function returns the number of dimensions 'ndims' of x, as a Python int.\n\n  The optional expect arguments are used to check the ndims of x, but this is\n  only done if the static ndims of x is not None.\n\n  Args:\n    x:  A Tensor.\n    expect_static:  Expect `x` to have statically defined `ndims`.\n    expect_ndims:  Optional Python integer.  If provided, assert that x has\n      number of dimensions equal to this.\n    expect_ndims_no_more_than:  Optional Python integer.  If provided, assert\n      that x has no more than this many dimensions.\n    expect_ndims_at_least:  Optional Python integer.  If provided, assert that x\n      has at least this many dimensions.\n\n  Returns:\n    ndims:  A Python integer.\n\n  Raises:\n    ValueError:  If any of the expectations above are violated.\n  \"\"\"\n  ndims = x.shape.ndims\n  if ndims is None:\n    shape_const = tf.get_static_value(tf.shape(input=x))\n    if shape_const is not None:\n      ndims = shape_const.ndim\n\n  if ndims is None:\n    if expect_static:\n      raise ValueError(\n          'Expected argument `x` to have statically defined `ndims`.  Found: ' %\n          x)\n    return\n\n  if expect_ndims is not None:\n    ndims_message = ('Expected argument `x` to have ndims %s.  Found tensor %s'\n                     % (expect_ndims, x))\n    if ndims != expect_ndims:\n      raise ValueError(ndims_message)\n\n  if expect_ndims_at_least is not None:\n    ndims_at_least_message = (\n        'Expected argument `x` to have ndims >= %d.  Found tensor %s' %\n        (expect_ndims_at_least, x))\n    if ndims < expect_ndims_at_least:\n      raise ValueError(ndims_at_least_message)\n\n  if expect_ndims_no_more_than is not None:\n    ndims_no_more_than_message = (\n        'Expected argument `x` to have ndims <= %d.  Found tensor %s' %\n        (expect_ndims_no_more_than, x))\n    if ndims > expect_ndims_no_more_than:\n      raise ValueError(ndims_no_more_than_message)\n\n  return ndims", "code_tokens": ["def", "_get_static_ndims", "(", "x", ",", "expect_static", "=", "False", ",", "expect_ndims", "=", "None", ",", "expect_ndims_no_more_than", "=", "None", ",", "expect_ndims_at_least", "=", "None", ")", ":", "ndims", "=", "x", ".", "shape", ".", "ndims", "if", "ndims", "is", "None", ":", "shape_const", "=", "tf", ".", "get_static_value", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ")", "if", "shape_const", "is", "not", "None", ":", "ndims", "=", "shape_const", ".", "ndim", "if", "ndims", "is", "None", ":", "if", "expect_static", ":", "raise", "ValueError", "(", "'Expected argument `x` to have statically defined `ndims`.  Found: '", "%", "x", ")", "return", "if", "expect_ndims", "is", "not", "None", ":", "ndims_message", "=", "(", "'Expected argument `x` to have ndims %s.  Found tensor %s'", "%", "(", "expect_ndims", ",", "x", ")", ")", "if", "ndims", "!=", "expect_ndims", ":", "raise", "ValueError", "(", "ndims_message", ")", "if", "expect_ndims_at_least", "is", "not", "None", ":", "ndims_at_least_message", "=", "(", "'Expected argument `x` to have ndims >= %d.  Found tensor %s'", "%", "(", "expect_ndims_at_least", ",", "x", ")", ")", "if", "ndims", "<", "expect_ndims_at_least", ":", "raise", "ValueError", "(", "ndims_at_least_message", ")", "if", "expect_ndims_no_more_than", "is", "not", "None", ":", "ndims_no_more_than_message", "=", "(", "'Expected argument `x` to have ndims <= %d.  Found tensor %s'", "%", "(", "expect_ndims_no_more_than", ",", "x", ")", ")", "if", "ndims", ">", "expect_ndims_no_more_than", ":", "raise", "ValueError", "(", "ndims_no_more_than_message", ")", "return", "ndims"], "docstring": "Get static number of dimensions and assert that some expectations are met.\n\n  This function returns the number of dimensions 'ndims' of x, as a Python int.\n\n  The optional expect arguments are used to check the ndims of x, but this is\n  only done if the static ndims of x is not None.\n\n  Args:\n    x:  A Tensor.\n    expect_static:  Expect `x` to have statically defined `ndims`.\n    expect_ndims:  Optional Python integer.  If provided, assert that x has\n      number of dimensions equal to this.\n    expect_ndims_no_more_than:  Optional Python integer.  If provided, assert\n      that x has no more than this many dimensions.\n    expect_ndims_at_least:  Optional Python integer.  If provided, assert that x\n      has at least this many dimensions.\n\n  Returns:\n    ndims:  A Python integer.\n\n  Raises:\n    ValueError:  If any of the expectations above are violated.", "docstring_tokens": ["Get", "static", "number", "of", "dimensions", "and", "assert", "that", "some", "expectations", "are", "met", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/quantiles.py#L726-L787", "partition": "test", "index": 1036, "time": "2018-11-26 09:28:10"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/quantiles.py", "func_name": "_insert_back_keep_dims", "original_string": "def _insert_back_keep_dims(x, axis):\n  \"\"\"Insert the dims in `axis` back as singletons after being removed.\n\n  Args:\n    x:  `Tensor`.\n    axis:  Python list of integers.\n\n  Returns:\n    `Tensor` with same values as `x`, but additional singleton dimensions.\n  \"\"\"\n  for i in sorted(axis):\n    x = tf.expand_dims(x, axis=i)\n  return x", "language": "python", "code": "def _insert_back_keep_dims(x, axis):\n  \"\"\"Insert the dims in `axis` back as singletons after being removed.\n\n  Args:\n    x:  `Tensor`.\n    axis:  Python list of integers.\n\n  Returns:\n    `Tensor` with same values as `x`, but additional singleton dimensions.\n  \"\"\"\n  for i in sorted(axis):\n    x = tf.expand_dims(x, axis=i)\n  return x", "code_tokens": ["def", "_insert_back_keep_dims", "(", "x", ",", "axis", ")", ":", "for", "i", "in", "sorted", "(", "axis", ")", ":", "x", "=", "tf", ".", "expand_dims", "(", "x", ",", "axis", "=", "i", ")", "return", "x"], "docstring": "Insert the dims in `axis` back as singletons after being removed.\n\n  Args:\n    x:  `Tensor`.\n    axis:  Python list of integers.\n\n  Returns:\n    `Tensor` with same values as `x`, but additional singleton dimensions.", "docstring_tokens": ["Insert", "the", "dims", "in", "axis", "back", "as", "singletons", "after", "being", "removed", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/quantiles.py#L805-L817", "partition": "test", "index": 1037, "time": "2018-11-26 09:28:10"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/quantiles.py", "func_name": "_make_static_axis_non_negative_list", "original_string": "def _make_static_axis_non_negative_list(axis, ndims):\n  \"\"\"Convert possibly negatively indexed axis to non-negative list of ints.\n\n  Args:\n    axis:  Integer Tensor.\n    ndims:  Number of dimensions into which axis indexes.\n\n  Returns:\n    A list of non-negative Python integers.\n\n  Raises:\n    ValueError: If `axis` is not statically defined.\n  \"\"\"\n  axis = distribution_util.make_non_negative_axis(axis, ndims)\n\n  axis_const = tf.get_static_value(axis)\n  if axis_const is None:\n    raise ValueError(\n        'Expected argument `axis` to be statically available.  Found: %s' %\n        axis)\n\n  # Make at least 1-D.\n  axis = axis_const + np.zeros([1], dtype=axis_const.dtype)\n\n  return list(int(dim) for dim in axis)", "language": "python", "code": "def _make_static_axis_non_negative_list(axis, ndims):\n  \"\"\"Convert possibly negatively indexed axis to non-negative list of ints.\n\n  Args:\n    axis:  Integer Tensor.\n    ndims:  Number of dimensions into which axis indexes.\n\n  Returns:\n    A list of non-negative Python integers.\n\n  Raises:\n    ValueError: If `axis` is not statically defined.\n  \"\"\"\n  axis = distribution_util.make_non_negative_axis(axis, ndims)\n\n  axis_const = tf.get_static_value(axis)\n  if axis_const is None:\n    raise ValueError(\n        'Expected argument `axis` to be statically available.  Found: %s' %\n        axis)\n\n  # Make at least 1-D.\n  axis = axis_const + np.zeros([1], dtype=axis_const.dtype)\n\n  return list(int(dim) for dim in axis)", "code_tokens": ["def", "_make_static_axis_non_negative_list", "(", "axis", ",", "ndims", ")", ":", "axis", "=", "distribution_util", ".", "make_non_negative_axis", "(", "axis", ",", "ndims", ")", "axis_const", "=", "tf", ".", "get_static_value", "(", "axis", ")", "if", "axis_const", "is", "None", ":", "raise", "ValueError", "(", "'Expected argument `axis` to be statically available.  Found: %s'", "%", "axis", ")", "# Make at least 1-D.", "axis", "=", "axis_const", "+", "np", ".", "zeros", "(", "[", "1", "]", ",", "dtype", "=", "axis_const", ".", "dtype", ")", "return", "list", "(", "int", "(", "dim", ")", "for", "dim", "in", "axis", ")"], "docstring": "Convert possibly negatively indexed axis to non-negative list of ints.\n\n  Args:\n    axis:  Integer Tensor.\n    ndims:  Number of dimensions into which axis indexes.\n\n  Returns:\n    A list of non-negative Python integers.\n\n  Raises:\n    ValueError: If `axis` is not statically defined.", "docstring_tokens": ["Convert", "possibly", "negatively", "indexed", "axis", "to", "non", "-", "negative", "list", "of", "ints", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/quantiles.py#L820-L844", "partition": "test", "index": 1038, "time": "2018-11-26 09:28:10"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/quantiles.py", "func_name": "_move_dims_to_flat_end", "original_string": "def _move_dims_to_flat_end(x, axis, x_ndims, right_end=True):\n  \"\"\"Move dims corresponding to `axis` in `x` to the end, then flatten.\n\n  Args:\n    x: `Tensor` with shape `[B0,B1,...,Bb]`.\n    axis:  Python list of indices into dimensions of `x`.\n    x_ndims:  Python integer holding number of dimensions in `x`.\n    right_end:  Python bool.  Whether to move dims to the right end (else left).\n\n  Returns:\n    `Tensor` with value from `x` and dims in `axis` moved to end into one single\n      dimension.\n  \"\"\"\n\n  if not axis:\n    return x\n\n  # Suppose x.shape = [a, b, c, d]\n  # Suppose axis = [1, 3]\n\n  # other_dims = [0, 2] in example above.\n  other_dims = sorted(set(range(x_ndims)).difference(axis))\n  # x_permed.shape = [a, c, b, d]\n  perm = other_dims + list(axis) if right_end else list(axis) + other_dims\n  x_permed = tf.transpose(a=x, perm=perm)\n\n  if x.shape.is_fully_defined():\n    x_shape = x.shape.as_list()\n    # other_shape = [a, c], end_shape = [b * d]\n    other_shape = [x_shape[i] for i in other_dims]\n    end_shape = [np.prod([x_shape[i] for i in axis])]\n    full_shape = (\n        other_shape + end_shape if right_end else end_shape + other_shape)\n  else:\n    other_shape = tf.gather(tf.shape(input=x), other_dims)\n    full_shape = tf.concat(\n        [other_shape, [-1]] if right_end else [[-1], other_shape], axis=0)\n  return tf.reshape(x_permed, shape=full_shape)", "language": "python", "code": "def _move_dims_to_flat_end(x, axis, x_ndims, right_end=True):\n  \"\"\"Move dims corresponding to `axis` in `x` to the end, then flatten.\n\n  Args:\n    x: `Tensor` with shape `[B0,B1,...,Bb]`.\n    axis:  Python list of indices into dimensions of `x`.\n    x_ndims:  Python integer holding number of dimensions in `x`.\n    right_end:  Python bool.  Whether to move dims to the right end (else left).\n\n  Returns:\n    `Tensor` with value from `x` and dims in `axis` moved to end into one single\n      dimension.\n  \"\"\"\n\n  if not axis:\n    return x\n\n  # Suppose x.shape = [a, b, c, d]\n  # Suppose axis = [1, 3]\n\n  # other_dims = [0, 2] in example above.\n  other_dims = sorted(set(range(x_ndims)).difference(axis))\n  # x_permed.shape = [a, c, b, d]\n  perm = other_dims + list(axis) if right_end else list(axis) + other_dims\n  x_permed = tf.transpose(a=x, perm=perm)\n\n  if x.shape.is_fully_defined():\n    x_shape = x.shape.as_list()\n    # other_shape = [a, c], end_shape = [b * d]\n    other_shape = [x_shape[i] for i in other_dims]\n    end_shape = [np.prod([x_shape[i] for i in axis])]\n    full_shape = (\n        other_shape + end_shape if right_end else end_shape + other_shape)\n  else:\n    other_shape = tf.gather(tf.shape(input=x), other_dims)\n    full_shape = tf.concat(\n        [other_shape, [-1]] if right_end else [[-1], other_shape], axis=0)\n  return tf.reshape(x_permed, shape=full_shape)", "code_tokens": ["def", "_move_dims_to_flat_end", "(", "x", ",", "axis", ",", "x_ndims", ",", "right_end", "=", "True", ")", ":", "if", "not", "axis", ":", "return", "x", "# Suppose x.shape = [a, b, c, d]", "# Suppose axis = [1, 3]", "# other_dims = [0, 2] in example above.", "other_dims", "=", "sorted", "(", "set", "(", "range", "(", "x_ndims", ")", ")", ".", "difference", "(", "axis", ")", ")", "# x_permed.shape = [a, c, b, d]", "perm", "=", "other_dims", "+", "list", "(", "axis", ")", "if", "right_end", "else", "list", "(", "axis", ")", "+", "other_dims", "x_permed", "=", "tf", ".", "transpose", "(", "a", "=", "x", ",", "perm", "=", "perm", ")", "if", "x", ".", "shape", ".", "is_fully_defined", "(", ")", ":", "x_shape", "=", "x", ".", "shape", ".", "as_list", "(", ")", "# other_shape = [a, c], end_shape = [b * d]", "other_shape", "=", "[", "x_shape", "[", "i", "]", "for", "i", "in", "other_dims", "]", "end_shape", "=", "[", "np", ".", "prod", "(", "[", "x_shape", "[", "i", "]", "for", "i", "in", "axis", "]", ")", "]", "full_shape", "=", "(", "other_shape", "+", "end_shape", "if", "right_end", "else", "end_shape", "+", "other_shape", ")", "else", ":", "other_shape", "=", "tf", ".", "gather", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "other_dims", ")", "full_shape", "=", "tf", ".", "concat", "(", "[", "other_shape", ",", "[", "-", "1", "]", "]", "if", "right_end", "else", "[", "[", "-", "1", "]", ",", "other_shape", "]", ",", "axis", "=", "0", ")", "return", "tf", ".", "reshape", "(", "x_permed", ",", "shape", "=", "full_shape", ")"], "docstring": "Move dims corresponding to `axis` in `x` to the end, then flatten.\n\n  Args:\n    x: `Tensor` with shape `[B0,B1,...,Bb]`.\n    axis:  Python list of indices into dimensions of `x`.\n    x_ndims:  Python integer holding number of dimensions in `x`.\n    right_end:  Python bool.  Whether to move dims to the right end (else left).\n\n  Returns:\n    `Tensor` with value from `x` and dims in `axis` moved to end into one single\n      dimension.", "docstring_tokens": ["Move", "dims", "corresponding", "to", "axis", "in", "x", "to", "the", "end", "then", "flatten", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/quantiles.py#L847-L884", "partition": "test", "index": 1039, "time": "2018-11-26 09:28:10"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/quantiles.py", "func_name": "_sort_tensor", "original_string": "def _sort_tensor(tensor):\n  \"\"\"Use `top_k` to sort a `Tensor` along the last dimension.\"\"\"\n  sorted_, _ = tf.nn.top_k(tensor, k=tf.shape(input=tensor)[-1])\n  sorted_.set_shape(tensor.shape)\n  return sorted_", "language": "python", "code": "def _sort_tensor(tensor):\n  \"\"\"Use `top_k` to sort a `Tensor` along the last dimension.\"\"\"\n  sorted_, _ = tf.nn.top_k(tensor, k=tf.shape(input=tensor)[-1])\n  sorted_.set_shape(tensor.shape)\n  return sorted_", "code_tokens": ["def", "_sort_tensor", "(", "tensor", ")", ":", "sorted_", ",", "_", "=", "tf", ".", "nn", ".", "top_k", "(", "tensor", ",", "k", "=", "tf", ".", "shape", "(", "input", "=", "tensor", ")", "[", "-", "1", "]", ")", "sorted_", ".", "set_shape", "(", "tensor", ".", "shape", ")", "return", "sorted_"], "docstring": "Use `top_k` to sort a `Tensor` along the last dimension.", "docstring_tokens": ["Use", "top_k", "to", "sort", "a", "Tensor", "along", "the", "last", "dimension", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/quantiles.py#L887-L891", "partition": "test", "index": 1040, "time": "2018-11-26 09:28:10"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs_utils.py", "func_name": "_restrict_along_direction", "original_string": "def _restrict_along_direction(value_and_gradients_function,\n                              position,\n                              direction):\n  \"\"\"Restricts a function in n-dimensions to a given direction.\n\n  Suppose f: R^n -> R. Then given a point x0 and a vector p0 in R^n, the\n  restriction of the function along that direction is defined by:\n\n  ```None\n  g(t) = f(x0 + t * p0)\n  ```\n\n  This function performs this restriction on the given function. In addition, it\n  also computes the gradient of the restricted function along the restriction\n  direction. This is equivalent to computing `dg/dt` in the definition above.\n\n  Args:\n    value_and_gradients_function: Callable accepting a single real `Tensor`\n      argument of shape `[..., n]` and returning a tuple of a real `Tensor` of\n      shape `[...]` and a real `Tensor` of shape `[..., n]`. The multivariate\n      function whose restriction is to be computed. The output values of the\n      callable are the function value and the gradients at the input argument.\n    position: `Tensor` of real dtype and shape consumable by\n      `value_and_gradients_function`. Corresponds to `x0` in the definition\n      above.\n    direction: `Tensor` of the same dtype and shape as `position`. The direction\n      along which to restrict the function. Note that the direction need not\n      be a unit vector.\n\n  Returns:\n    restricted_value_and_gradients_func: A callable accepting a tensor of shape\n      broadcastable to `[...]` and same dtype as `position` and returning a\n      namedtuple of `Tensors`. The input tensor is the parameter along the\n      direction labelled `t` above. The return value contains fields:\n        x: A real `Tensor` of shape `[...]`. The input value `t` where the line\n          function was evaluated, after any necessary broadcasting.\n        f: A real `Tensor` of shape `[...]` containing the value of the\n          function at the point `position + t * direction`.\n        df: A real `Tensor` of shape `[...]` containing the derivative at\n          `position + t * direction`.\n        full_gradient: A real `Tensor` of shape `[..., n]`, the full gradient\n          of the original `value_and_gradients_function`.\n  \"\"\"\n  def _restricted_func(t):\n    t = _broadcast(t, position)\n    pt = position + tf.expand_dims(t, axis=-1) * direction\n    objective_value, gradient = value_and_gradients_function(pt)\n    return ValueAndGradient(\n        x=t,\n        f=objective_value,\n        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),\n        full_gradient=gradient)\n\n  return _restricted_func", "language": "python", "code": "def _restrict_along_direction(value_and_gradients_function,\n                              position,\n                              direction):\n  \"\"\"Restricts a function in n-dimensions to a given direction.\n\n  Suppose f: R^n -> R. Then given a point x0 and a vector p0 in R^n, the\n  restriction of the function along that direction is defined by:\n\n  ```None\n  g(t) = f(x0 + t * p0)\n  ```\n\n  This function performs this restriction on the given function. In addition, it\n  also computes the gradient of the restricted function along the restriction\n  direction. This is equivalent to computing `dg/dt` in the definition above.\n\n  Args:\n    value_and_gradients_function: Callable accepting a single real `Tensor`\n      argument of shape `[..., n]` and returning a tuple of a real `Tensor` of\n      shape `[...]` and a real `Tensor` of shape `[..., n]`. The multivariate\n      function whose restriction is to be computed. The output values of the\n      callable are the function value and the gradients at the input argument.\n    position: `Tensor` of real dtype and shape consumable by\n      `value_and_gradients_function`. Corresponds to `x0` in the definition\n      above.\n    direction: `Tensor` of the same dtype and shape as `position`. The direction\n      along which to restrict the function. Note that the direction need not\n      be a unit vector.\n\n  Returns:\n    restricted_value_and_gradients_func: A callable accepting a tensor of shape\n      broadcastable to `[...]` and same dtype as `position` and returning a\n      namedtuple of `Tensors`. The input tensor is the parameter along the\n      direction labelled `t` above. The return value contains fields:\n        x: A real `Tensor` of shape `[...]`. The input value `t` where the line\n          function was evaluated, after any necessary broadcasting.\n        f: A real `Tensor` of shape `[...]` containing the value of the\n          function at the point `position + t * direction`.\n        df: A real `Tensor` of shape `[...]` containing the derivative at\n          `position + t * direction`.\n        full_gradient: A real `Tensor` of shape `[..., n]`, the full gradient\n          of the original `value_and_gradients_function`.\n  \"\"\"\n  def _restricted_func(t):\n    t = _broadcast(t, position)\n    pt = position + tf.expand_dims(t, axis=-1) * direction\n    objective_value, gradient = value_and_gradients_function(pt)\n    return ValueAndGradient(\n        x=t,\n        f=objective_value,\n        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),\n        full_gradient=gradient)\n\n  return _restricted_func", "code_tokens": ["def", "_restrict_along_direction", "(", "value_and_gradients_function", ",", "position", ",", "direction", ")", ":", "def", "_restricted_func", "(", "t", ")", ":", "t", "=", "_broadcast", "(", "t", ",", "position", ")", "pt", "=", "position", "+", "tf", ".", "expand_dims", "(", "t", ",", "axis", "=", "-", "1", ")", "*", "direction", "objective_value", ",", "gradient", "=", "value_and_gradients_function", "(", "pt", ")", "return", "ValueAndGradient", "(", "x", "=", "t", ",", "f", "=", "objective_value", ",", "df", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "gradient", "*", "direction", ",", "axis", "=", "-", "1", ")", ",", "full_gradient", "=", "gradient", ")", "return", "_restricted_func"], "docstring": "Restricts a function in n-dimensions to a given direction.\n\n  Suppose f: R^n -> R. Then given a point x0 and a vector p0 in R^n, the\n  restriction of the function along that direction is defined by:\n\n  ```None\n  g(t) = f(x0 + t * p0)\n  ```\n\n  This function performs this restriction on the given function. In addition, it\n  also computes the gradient of the restricted function along the restriction\n  direction. This is equivalent to computing `dg/dt` in the definition above.\n\n  Args:\n    value_and_gradients_function: Callable accepting a single real `Tensor`\n      argument of shape `[..., n]` and returning a tuple of a real `Tensor` of\n      shape `[...]` and a real `Tensor` of shape `[..., n]`. The multivariate\n      function whose restriction is to be computed. The output values of the\n      callable are the function value and the gradients at the input argument.\n    position: `Tensor` of real dtype and shape consumable by\n      `value_and_gradients_function`. Corresponds to `x0` in the definition\n      above.\n    direction: `Tensor` of the same dtype and shape as `position`. The direction\n      along which to restrict the function. Note that the direction need not\n      be a unit vector.\n\n  Returns:\n    restricted_value_and_gradients_func: A callable accepting a tensor of shape\n      broadcastable to `[...]` and same dtype as `position` and returning a\n      namedtuple of `Tensors`. The input tensor is the parameter along the\n      direction labelled `t` above. The return value contains fields:\n        x: A real `Tensor` of shape `[...]`. The input value `t` where the line\n          function was evaluated, after any necessary broadcasting.\n        f: A real `Tensor` of shape `[...]` containing the value of the\n          function at the point `position + t * direction`.\n        df: A real `Tensor` of shape `[...]` containing the derivative at\n          `position + t * direction`.\n        full_gradient: A real `Tensor` of shape `[..., n]`, the full gradient\n          of the original `value_and_gradients_function`.", "docstring_tokens": ["Restricts", "a", "function", "in", "n", "-", "dimensions", "to", "a", "given", "direction", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs_utils.py#L202-L255", "partition": "test", "index": 981, "time": "2018-11-28 05:27:14"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs_utils.py", "func_name": "_broadcast", "original_string": "def _broadcast(value, target):\n  \"\"\"Broadcast a value to match the batching dimensions of a target.\n\n  If necessary the value is converted into a tensor. Both value and target\n  should be of the same dtype.\n\n  Args:\n    value: A value to broadcast.\n    target: A `Tensor` of shape [b1, ..., bn, d].\n\n  Returns:\n    A `Tensor` of shape [b1, ..., bn] and same dtype as the target.\n  \"\"\"\n  return tf.broadcast_to(\n      tf.convert_to_tensor(value=value, dtype=target.dtype),\n      distribution_util.prefer_static_shape(target)[:-1])", "language": "python", "code": "def _broadcast(value, target):\n  \"\"\"Broadcast a value to match the batching dimensions of a target.\n\n  If necessary the value is converted into a tensor. Both value and target\n  should be of the same dtype.\n\n  Args:\n    value: A value to broadcast.\n    target: A `Tensor` of shape [b1, ..., bn, d].\n\n  Returns:\n    A `Tensor` of shape [b1, ..., bn] and same dtype as the target.\n  \"\"\"\n  return tf.broadcast_to(\n      tf.convert_to_tensor(value=value, dtype=target.dtype),\n      distribution_util.prefer_static_shape(target)[:-1])", "code_tokens": ["def", "_broadcast", "(", "value", ",", "target", ")", ":", "return", "tf", ".", "broadcast_to", "(", "tf", ".", "convert_to_tensor", "(", "value", "=", "value", ",", "dtype", "=", "target", ".", "dtype", ")", ",", "distribution_util", ".", "prefer_static_shape", "(", "target", ")", "[", ":", "-", "1", "]", ")"], "docstring": "Broadcast a value to match the batching dimensions of a target.\n\n  If necessary the value is converted into a tensor. Both value and target\n  should be of the same dtype.\n\n  Args:\n    value: A value to broadcast.\n    target: A `Tensor` of shape [b1, ..., bn, d].\n\n  Returns:\n    A `Tensor` of shape [b1, ..., bn] and same dtype as the target.", "docstring_tokens": ["Broadcast", "a", "value", "to", "match", "the", "batching", "dimensions", "of", "a", "target", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs_utils.py#L325-L340", "partition": "test", "index": 984, "time": "2018-11-28 05:27:14"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs_utils.py", "func_name": "get_initial_state_args", "original_string": "def get_initial_state_args(value_and_gradients_function,\n                           initial_position,\n                           grad_tolerance,\n                           control_inputs=None):\n  \"\"\"Returns a dictionary to populate the initial state of the search procedure.\n\n  Performs an initial convergence check and the first evaluation of the\n  objective function.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a tensor and\n      returns a tuple of two tensors: the objective function value and its\n      derivative.\n    initial_position: The starting point of the search procedure.\n    grad_tolerance: The gradient tolerance for the procedure.\n    control_inputs: Optional ops used to assert the validity of inputs, these\n      are added as control dependencies to execute before the objective\n      function is evaluated for the first time.\n\n  Returns:\n    An dictionary with values for the following keys:\n      converged: True if the convergence check finds that the initial position\n        is already an argmin of the objective function.\n      failed: Initialized to False.\n      num_objective_evaluations: Initialized to 1.\n      position: Initialized to the initial position.\n      objective_value: Initialized to the value of the objective function at\n        the initial position.\n      objective_gradient: Initialized to the gradient of the objective\n        function at the initial position.\n  \"\"\"\n  if control_inputs:\n    with tf.control_dependencies(control_inputs):\n      f0, df0 = value_and_gradients_function(initial_position)\n  else:\n    f0, df0 = value_and_gradients_function(initial_position)\n  converged = norm(df0, dims=1) < grad_tolerance\n  return dict(\n      converged=converged,\n      failed=tf.zeros_like(converged),  # i.e. False.\n      num_iterations=tf.convert_to_tensor(value=0),\n      num_objective_evaluations=tf.convert_to_tensor(value=1),\n      position=initial_position,\n      objective_value=f0,\n      objective_gradient=df0)", "language": "python", "code": "def get_initial_state_args(value_and_gradients_function,\n                           initial_position,\n                           grad_tolerance,\n                           control_inputs=None):\n  \"\"\"Returns a dictionary to populate the initial state of the search procedure.\n\n  Performs an initial convergence check and the first evaluation of the\n  objective function.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a tensor and\n      returns a tuple of two tensors: the objective function value and its\n      derivative.\n    initial_position: The starting point of the search procedure.\n    grad_tolerance: The gradient tolerance for the procedure.\n    control_inputs: Optional ops used to assert the validity of inputs, these\n      are added as control dependencies to execute before the objective\n      function is evaluated for the first time.\n\n  Returns:\n    An dictionary with values for the following keys:\n      converged: True if the convergence check finds that the initial position\n        is already an argmin of the objective function.\n      failed: Initialized to False.\n      num_objective_evaluations: Initialized to 1.\n      position: Initialized to the initial position.\n      objective_value: Initialized to the value of the objective function at\n        the initial position.\n      objective_gradient: Initialized to the gradient of the objective\n        function at the initial position.\n  \"\"\"\n  if control_inputs:\n    with tf.control_dependencies(control_inputs):\n      f0, df0 = value_and_gradients_function(initial_position)\n  else:\n    f0, df0 = value_and_gradients_function(initial_position)\n  converged = norm(df0, dims=1) < grad_tolerance\n  return dict(\n      converged=converged,\n      failed=tf.zeros_like(converged),  # i.e. False.\n      num_iterations=tf.convert_to_tensor(value=0),\n      num_objective_evaluations=tf.convert_to_tensor(value=1),\n      position=initial_position,\n      objective_value=f0,\n      objective_gradient=df0)", "code_tokens": ["def", "get_initial_state_args", "(", "value_and_gradients_function", ",", "initial_position", ",", "grad_tolerance", ",", "control_inputs", "=", "None", ")", ":", "if", "control_inputs", ":", "with", "tf", ".", "control_dependencies", "(", "control_inputs", ")", ":", "f0", ",", "df0", "=", "value_and_gradients_function", "(", "initial_position", ")", "else", ":", "f0", ",", "df0", "=", "value_and_gradients_function", "(", "initial_position", ")", "converged", "=", "norm", "(", "df0", ",", "dims", "=", "1", ")", "<", "grad_tolerance", "return", "dict", "(", "converged", "=", "converged", ",", "failed", "=", "tf", ".", "zeros_like", "(", "converged", ")", ",", "# i.e. False.", "num_iterations", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "0", ")", ",", "num_objective_evaluations", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "1", ")", ",", "position", "=", "initial_position", ",", "objective_value", "=", "f0", ",", "objective_gradient", "=", "df0", ")"], "docstring": "Returns a dictionary to populate the initial state of the search procedure.\n\n  Performs an initial convergence check and the first evaluation of the\n  objective function.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a tensor and\n      returns a tuple of two tensors: the objective function value and its\n      derivative.\n    initial_position: The starting point of the search procedure.\n    grad_tolerance: The gradient tolerance for the procedure.\n    control_inputs: Optional ops used to assert the validity of inputs, these\n      are added as control dependencies to execute before the objective\n      function is evaluated for the first time.\n\n  Returns:\n    An dictionary with values for the following keys:\n      converged: True if the convergence check finds that the initial position\n        is already an argmin of the objective function.\n      failed: Initialized to False.\n      num_objective_evaluations: Initialized to 1.\n      position: Initialized to the initial position.\n      objective_value: Initialized to the value of the objective function at\n        the initial position.\n      objective_gradient: Initialized to the gradient of the objective\n        function at the initial position.", "docstring_tokens": ["Returns", "a", "dictionary", "to", "populate", "the", "initial", "state", "of", "the", "search", "procedure", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs_utils.py#L47-L91", "partition": "test", "index": 979, "time": "2018-11-28 05:27:14"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs_utils.py", "func_name": "_check_convergence", "original_string": "def _check_convergence(current_position,\n                       next_position,\n                       current_objective,\n                       next_objective,\n                       next_gradient,\n                       grad_tolerance,\n                       f_relative_tolerance,\n                       x_tolerance):\n  \"\"\"Checks if the algorithm satisfies the convergence criteria.\"\"\"\n  grad_converged = norm(next_gradient, dims=1) <= grad_tolerance\n  x_converged = norm(next_position - current_position, dims=1) <= x_tolerance\n  f_converged = (norm(next_objective - current_objective, dims=0) <=\n                 f_relative_tolerance * current_objective)\n  return grad_converged | x_converged | f_converged", "language": "python", "code": "def _check_convergence(current_position,\n                       next_position,\n                       current_objective,\n                       next_objective,\n                       next_gradient,\n                       grad_tolerance,\n                       f_relative_tolerance,\n                       x_tolerance):\n  \"\"\"Checks if the algorithm satisfies the convergence criteria.\"\"\"\n  grad_converged = norm(next_gradient, dims=1) <= grad_tolerance\n  x_converged = norm(next_position - current_position, dims=1) <= x_tolerance\n  f_converged = (norm(next_objective - current_objective, dims=0) <=\n                 f_relative_tolerance * current_objective)\n  return grad_converged | x_converged | f_converged", "code_tokens": ["def", "_check_convergence", "(", "current_position", ",", "next_position", ",", "current_objective", ",", "next_objective", ",", "next_gradient", ",", "grad_tolerance", ",", "f_relative_tolerance", ",", "x_tolerance", ")", ":", "grad_converged", "=", "norm", "(", "next_gradient", ",", "dims", "=", "1", ")", "<=", "grad_tolerance", "x_converged", "=", "norm", "(", "next_position", "-", "current_position", ",", "dims", "=", "1", ")", "<=", "x_tolerance", "f_converged", "=", "(", "norm", "(", "next_objective", "-", "current_objective", ",", "dims", "=", "0", ")", "<=", "f_relative_tolerance", "*", "current_objective", ")", "return", "grad_converged", "|", "x_converged", "|", "f_converged"], "docstring": "Checks if the algorithm satisfies the convergence criteria.", "docstring_tokens": ["Checks", "if", "the", "algorithm", "satisfies", "the", "convergence", "criteria", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs_utils.py#L309-L322", "partition": "test", "index": 983, "time": "2018-11-28 05:27:14"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs_utils.py", "func_name": "line_search_step", "original_string": "def line_search_step(state, value_and_gradients_function, search_direction,\n                     grad_tolerance, f_relative_tolerance, x_tolerance,\n                     stopping_condition):\n  \"\"\"Performs the line search step of the BFGS search procedure.\n\n  Uses hager_zhang line search procedure to compute a suitable step size\n  to advance the current `state.position` along the given `search_direction`.\n  Also, if the line search is successful, updates the `state.position` by\n  taking the corresponding step.\n\n  Args:\n    state: A namedtuple instance holding values for the current state of the\n      search procedure. The state must include the fields: `position`,\n      `objective_value`, `objective_gradient`, `num_iterations`,\n      `num_objective_evaluations`, `converged` and `failed`.\n    value_and_gradients_function: A Python callable that accepts a point as a\n      real `Tensor` of shape `[..., n]` and returns a tuple of two tensors of\n      the same dtype: the objective function value, a real `Tensor` of shape\n      `[...]`, and its derivative, another real `Tensor` of shape `[..., n]`.\n    search_direction: A real `Tensor` of shape `[..., n]`. The direction along\n      which to perform line search.\n    grad_tolerance: Scalar `Tensor` of real dtype. Specifies the gradient\n      tolerance for the procedure.\n    f_relative_tolerance: Scalar `Tensor` of real dtype. Specifies the\n      tolerance for the relative change in the objective value.\n    x_tolerance: Scalar `Tensor` of real dtype. Specifies the tolerance for the\n      change in the position.\n    stopping_condition: A Python function that takes as input two Boolean\n      tensors of shape `[...]`, and returns a Boolean scalar tensor. The input\n      tensors are `converged` and `failed`, indicating the current status of\n      each respective batch member; the return value states whether the\n      algorithm should stop.\n  Returns:\n    A copy of the input state with the following fields updated:\n      converged: a Boolean `Tensor` of shape `[...]` indicating whether the\n        convergence criteria has been met.\n      failed: a Boolean `Tensor` of shape `[...]` indicating whether the line\n        search procedure failed to converge, or if either the updated gradient\n        or objective function are no longer finite.\n      num_iterations: Increased by 1.\n      num_objective_evaluations: Increased by the number of times that the\n        objective function got evaluated.\n      position, objective_value, objective_gradient: If line search succeeded,\n        updated by computing the new position and evaluating the objective\n        function at that position.\n  \"\"\"\n  line_search_value_grad_func = _restrict_along_direction(\n      value_and_gradients_function, state.position, search_direction)\n  derivative_at_start_pt = tf.reduce_sum(\n      input_tensor=state.objective_gradient * search_direction, axis=-1)\n  val_0 = ValueAndGradient(x=_broadcast(0, state.position),\n                           f=state.objective_value,\n                           df=derivative_at_start_pt,\n                           full_gradient=state.objective_gradient)\n  inactive = state.failed | state.converged\n  ls_result = linesearch.hager_zhang(\n      line_search_value_grad_func,\n      initial_step_size=_broadcast(1, state.position),\n      value_at_zero=val_0,\n      converged=inactive)  # No search needed for these.\n\n  state_after_ls = update_fields(\n      state,\n      failed=state.failed | ~ls_result.converged,\n      num_iterations=state.num_iterations + 1,\n      num_objective_evaluations=(\n          state.num_objective_evaluations + ls_result.func_evals))\n\n  def _do_update_position():\n    # For inactive batch members `left.x` is zero. However, their\n    # `search_direction` might also be undefined, so we can't rely on\n    # multiplication by zero to produce a `position_delta` of zero.\n    position_delta = tf.where(\n        inactive,\n        tf.zeros_like(search_direction),\n        search_direction * tf.expand_dims(ls_result.left.x, axis=-1))\n    return _update_position(\n        state_after_ls,\n        position_delta,\n        ls_result.left.f,\n        ls_result.left.full_gradient,\n        grad_tolerance, f_relative_tolerance, x_tolerance)\n\n  return prefer_static.cond(\n      stopping_condition(state.converged, state.failed),\n      true_fn=lambda: state_after_ls,\n      false_fn=_do_update_position)", "language": "python", "code": "def line_search_step(state, value_and_gradients_function, search_direction,\n                     grad_tolerance, f_relative_tolerance, x_tolerance,\n                     stopping_condition):\n  \"\"\"Performs the line search step of the BFGS search procedure.\n\n  Uses hager_zhang line search procedure to compute a suitable step size\n  to advance the current `state.position` along the given `search_direction`.\n  Also, if the line search is successful, updates the `state.position` by\n  taking the corresponding step.\n\n  Args:\n    state: A namedtuple instance holding values for the current state of the\n      search procedure. The state must include the fields: `position`,\n      `objective_value`, `objective_gradient`, `num_iterations`,\n      `num_objective_evaluations`, `converged` and `failed`.\n    value_and_gradients_function: A Python callable that accepts a point as a\n      real `Tensor` of shape `[..., n]` and returns a tuple of two tensors of\n      the same dtype: the objective function value, a real `Tensor` of shape\n      `[...]`, and its derivative, another real `Tensor` of shape `[..., n]`.\n    search_direction: A real `Tensor` of shape `[..., n]`. The direction along\n      which to perform line search.\n    grad_tolerance: Scalar `Tensor` of real dtype. Specifies the gradient\n      tolerance for the procedure.\n    f_relative_tolerance: Scalar `Tensor` of real dtype. Specifies the\n      tolerance for the relative change in the objective value.\n    x_tolerance: Scalar `Tensor` of real dtype. Specifies the tolerance for the\n      change in the position.\n    stopping_condition: A Python function that takes as input two Boolean\n      tensors of shape `[...]`, and returns a Boolean scalar tensor. The input\n      tensors are `converged` and `failed`, indicating the current status of\n      each respective batch member; the return value states whether the\n      algorithm should stop.\n  Returns:\n    A copy of the input state with the following fields updated:\n      converged: a Boolean `Tensor` of shape `[...]` indicating whether the\n        convergence criteria has been met.\n      failed: a Boolean `Tensor` of shape `[...]` indicating whether the line\n        search procedure failed to converge, or if either the updated gradient\n        or objective function are no longer finite.\n      num_iterations: Increased by 1.\n      num_objective_evaluations: Increased by the number of times that the\n        objective function got evaluated.\n      position, objective_value, objective_gradient: If line search succeeded,\n        updated by computing the new position and evaluating the objective\n        function at that position.\n  \"\"\"\n  line_search_value_grad_func = _restrict_along_direction(\n      value_and_gradients_function, state.position, search_direction)\n  derivative_at_start_pt = tf.reduce_sum(\n      input_tensor=state.objective_gradient * search_direction, axis=-1)\n  val_0 = ValueAndGradient(x=_broadcast(0, state.position),\n                           f=state.objective_value,\n                           df=derivative_at_start_pt,\n                           full_gradient=state.objective_gradient)\n  inactive = state.failed | state.converged\n  ls_result = linesearch.hager_zhang(\n      line_search_value_grad_func,\n      initial_step_size=_broadcast(1, state.position),\n      value_at_zero=val_0,\n      converged=inactive)  # No search needed for these.\n\n  state_after_ls = update_fields(\n      state,\n      failed=state.failed | ~ls_result.converged,\n      num_iterations=state.num_iterations + 1,\n      num_objective_evaluations=(\n          state.num_objective_evaluations + ls_result.func_evals))\n\n  def _do_update_position():\n    # For inactive batch members `left.x` is zero. However, their\n    # `search_direction` might also be undefined, so we can't rely on\n    # multiplication by zero to produce a `position_delta` of zero.\n    position_delta = tf.where(\n        inactive,\n        tf.zeros_like(search_direction),\n        search_direction * tf.expand_dims(ls_result.left.x, axis=-1))\n    return _update_position(\n        state_after_ls,\n        position_delta,\n        ls_result.left.f,\n        ls_result.left.full_gradient,\n        grad_tolerance, f_relative_tolerance, x_tolerance)\n\n  return prefer_static.cond(\n      stopping_condition(state.converged, state.failed),\n      true_fn=lambda: state_after_ls,\n      false_fn=_do_update_position)", "code_tokens": ["def", "line_search_step", "(", "state", ",", "value_and_gradients_function", ",", "search_direction", ",", "grad_tolerance", ",", "f_relative_tolerance", ",", "x_tolerance", ",", "stopping_condition", ")", ":", "line_search_value_grad_func", "=", "_restrict_along_direction", "(", "value_and_gradients_function", ",", "state", ".", "position", ",", "search_direction", ")", "derivative_at_start_pt", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "state", ".", "objective_gradient", "*", "search_direction", ",", "axis", "=", "-", "1", ")", "val_0", "=", "ValueAndGradient", "(", "x", "=", "_broadcast", "(", "0", ",", "state", ".", "position", ")", ",", "f", "=", "state", ".", "objective_value", ",", "df", "=", "derivative_at_start_pt", ",", "full_gradient", "=", "state", ".", "objective_gradient", ")", "inactive", "=", "state", ".", "failed", "|", "state", ".", "converged", "ls_result", "=", "linesearch", ".", "hager_zhang", "(", "line_search_value_grad_func", ",", "initial_step_size", "=", "_broadcast", "(", "1", ",", "state", ".", "position", ")", ",", "value_at_zero", "=", "val_0", ",", "converged", "=", "inactive", ")", "# No search needed for these.", "state_after_ls", "=", "update_fields", "(", "state", ",", "failed", "=", "state", ".", "failed", "|", "~", "ls_result", ".", "converged", ",", "num_iterations", "=", "state", ".", "num_iterations", "+", "1", ",", "num_objective_evaluations", "=", "(", "state", ".", "num_objective_evaluations", "+", "ls_result", ".", "func_evals", ")", ")", "def", "_do_update_position", "(", ")", ":", "# For inactive batch members `left.x` is zero. However, their", "# `search_direction` might also be undefined, so we can't rely on", "# multiplication by zero to produce a `position_delta` of zero.", "position_delta", "=", "tf", ".", "where", "(", "inactive", ",", "tf", ".", "zeros_like", "(", "search_direction", ")", ",", "search_direction", "*", "tf", ".", "expand_dims", "(", "ls_result", ".", "left", ".", "x", ",", "axis", "=", "-", "1", ")", ")", "return", "_update_position", "(", "state_after_ls", ",", "position_delta", ",", "ls_result", ".", "left", ".", "f", ",", "ls_result", ".", "left", ".", "full_gradient", ",", "grad_tolerance", ",", "f_relative_tolerance", ",", "x_tolerance", ")", "return", "prefer_static", ".", "cond", "(", "stopping_condition", "(", "state", ".", "converged", ",", "state", ".", "failed", ")", ",", "true_fn", "=", "lambda", ":", "state_after_ls", ",", "false_fn", "=", "_do_update_position", ")"], "docstring": "Performs the line search step of the BFGS search procedure.\n\n  Uses hager_zhang line search procedure to compute a suitable step size\n  to advance the current `state.position` along the given `search_direction`.\n  Also, if the line search is successful, updates the `state.position` by\n  taking the corresponding step.\n\n  Args:\n    state: A namedtuple instance holding values for the current state of the\n      search procedure. The state must include the fields: `position`,\n      `objective_value`, `objective_gradient`, `num_iterations`,\n      `num_objective_evaluations`, `converged` and `failed`.\n    value_and_gradients_function: A Python callable that accepts a point as a\n      real `Tensor` of shape `[..., n]` and returns a tuple of two tensors of\n      the same dtype: the objective function value, a real `Tensor` of shape\n      `[...]`, and its derivative, another real `Tensor` of shape `[..., n]`.\n    search_direction: A real `Tensor` of shape `[..., n]`. The direction along\n      which to perform line search.\n    grad_tolerance: Scalar `Tensor` of real dtype. Specifies the gradient\n      tolerance for the procedure.\n    f_relative_tolerance: Scalar `Tensor` of real dtype. Specifies the\n      tolerance for the relative change in the objective value.\n    x_tolerance: Scalar `Tensor` of real dtype. Specifies the tolerance for the\n      change in the position.\n    stopping_condition: A Python function that takes as input two Boolean\n      tensors of shape `[...]`, and returns a Boolean scalar tensor. The input\n      tensors are `converged` and `failed`, indicating the current status of\n      each respective batch member; the return value states whether the\n      algorithm should stop.\n  Returns:\n    A copy of the input state with the following fields updated:\n      converged: a Boolean `Tensor` of shape `[...]` indicating whether the\n        convergence criteria has been met.\n      failed: a Boolean `Tensor` of shape `[...]` indicating whether the line\n        search procedure failed to converge, or if either the updated gradient\n        or objective function are no longer finite.\n      num_iterations: Increased by 1.\n      num_objective_evaluations: Increased by the number of times that the\n        objective function got evaluated.\n      position, objective_value, objective_gradient: If line search succeeded,\n        updated by computing the new position and evaluating the objective\n        function at that position.", "docstring_tokens": ["Performs", "the", "line", "search", "step", "of", "the", "BFGS", "search", "procedure", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs_utils.py#L94-L180", "partition": "test", "index": 980, "time": "2018-11-28 05:27:14"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs_utils.py", "func_name": "_update_position", "original_string": "def _update_position(state,\n                     position_delta,\n                     next_objective,\n                     next_gradient,\n                     grad_tolerance,\n                     f_relative_tolerance,\n                     x_tolerance):\n  \"\"\"Updates the state advancing its position by a given position_delta.\"\"\"\n  failed = state.failed | ~tf.math.is_finite(next_objective) | ~tf.reduce_all(\n      input_tensor=tf.math.is_finite(next_gradient), axis=-1)\n\n  next_position = state.position + position_delta\n  converged = ~failed & _check_convergence(state.position,\n                                           next_position,\n                                           state.objective_value,\n                                           next_objective,\n                                           next_gradient,\n                                           grad_tolerance,\n                                           f_relative_tolerance,\n                                           x_tolerance)\n  return update_fields(\n      state,\n      converged=state.converged | converged,\n      failed=failed,\n      position=next_position,\n      objective_value=next_objective,\n      objective_gradient=next_gradient)", "language": "python", "code": "def _update_position(state,\n                     position_delta,\n                     next_objective,\n                     next_gradient,\n                     grad_tolerance,\n                     f_relative_tolerance,\n                     x_tolerance):\n  \"\"\"Updates the state advancing its position by a given position_delta.\"\"\"\n  failed = state.failed | ~tf.math.is_finite(next_objective) | ~tf.reduce_all(\n      input_tensor=tf.math.is_finite(next_gradient), axis=-1)\n\n  next_position = state.position + position_delta\n  converged = ~failed & _check_convergence(state.position,\n                                           next_position,\n                                           state.objective_value,\n                                           next_objective,\n                                           next_gradient,\n                                           grad_tolerance,\n                                           f_relative_tolerance,\n                                           x_tolerance)\n  return update_fields(\n      state,\n      converged=state.converged | converged,\n      failed=failed,\n      position=next_position,\n      objective_value=next_objective,\n      objective_gradient=next_gradient)", "code_tokens": ["def", "_update_position", "(", "state", ",", "position_delta", ",", "next_objective", ",", "next_gradient", ",", "grad_tolerance", ",", "f_relative_tolerance", ",", "x_tolerance", ")", ":", "failed", "=", "state", ".", "failed", "|", "~", "tf", ".", "math", ".", "is_finite", "(", "next_objective", ")", "|", "~", "tf", ".", "reduce_all", "(", "input_tensor", "=", "tf", ".", "math", ".", "is_finite", "(", "next_gradient", ")", ",", "axis", "=", "-", "1", ")", "next_position", "=", "state", ".", "position", "+", "position_delta", "converged", "=", "~", "failed", "&", "_check_convergence", "(", "state", ".", "position", ",", "next_position", ",", "state", ".", "objective_value", ",", "next_objective", ",", "next_gradient", ",", "grad_tolerance", ",", "f_relative_tolerance", ",", "x_tolerance", ")", "return", "update_fields", "(", "state", ",", "converged", "=", "state", ".", "converged", "|", "converged", ",", "failed", "=", "failed", ",", "position", "=", "next_position", ",", "objective_value", "=", "next_objective", ",", "objective_gradient", "=", "next_gradient", ")"], "docstring": "Updates the state advancing its position by a given position_delta.", "docstring_tokens": ["Updates", "the", "state", "advancing", "its", "position", "by", "a", "given", "position_delta", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs_utils.py#L258-L284", "partition": "test", "index": 982, "time": "2018-11-28 05:27:14"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/interpolation.py", "func_name": "_batch_gather_with_broadcast", "original_string": "def _batch_gather_with_broadcast(params, indices, axis):\n  \"\"\"Like batch_gather, but broadcasts to the left of axis.\"\"\"\n  # batch_gather assumes...\n  #   params.shape =  [A1,...,AN, B1,...,BM]\n  #   indices.shape = [A1,...,AN, C]\n  # which gives output of shape\n  #                   [A1,...,AN, C, B1,...,BM]\n  # Here we broadcast dims of each to the left of `axis` in params, and left of\n  # the rightmost dim in indices, e.g. we can\n  # have\n  #   params.shape =  [A1,...,AN, B1,...,BM]\n  #   indices.shape = [a1,...,aN, C],\n  # where ai broadcasts with Ai.\n\n  # leading_bcast_shape is the broadcast of [A1,...,AN] and [a1,...,aN].\n  leading_bcast_shape = tf.broadcast_dynamic_shape(\n      tf.shape(input=params)[:axis],\n      tf.shape(input=indices)[:-1])\n  params += tf.zeros(\n      tf.concat((leading_bcast_shape, tf.shape(input=params)[axis:]), axis=0),\n      dtype=params.dtype)\n  indices += tf.zeros(\n      tf.concat((leading_bcast_shape, tf.shape(input=indices)[-1:]), axis=0),\n      dtype=indices.dtype)\n  return tf.compat.v1.batch_gather(params, indices)", "language": "python", "code": "def _batch_gather_with_broadcast(params, indices, axis):\n  \"\"\"Like batch_gather, but broadcasts to the left of axis.\"\"\"\n  # batch_gather assumes...\n  #   params.shape =  [A1,...,AN, B1,...,BM]\n  #   indices.shape = [A1,...,AN, C]\n  # which gives output of shape\n  #                   [A1,...,AN, C, B1,...,BM]\n  # Here we broadcast dims of each to the left of `axis` in params, and left of\n  # the rightmost dim in indices, e.g. we can\n  # have\n  #   params.shape =  [A1,...,AN, B1,...,BM]\n  #   indices.shape = [a1,...,aN, C],\n  # where ai broadcasts with Ai.\n\n  # leading_bcast_shape is the broadcast of [A1,...,AN] and [a1,...,aN].\n  leading_bcast_shape = tf.broadcast_dynamic_shape(\n      tf.shape(input=params)[:axis],\n      tf.shape(input=indices)[:-1])\n  params += tf.zeros(\n      tf.concat((leading_bcast_shape, tf.shape(input=params)[axis:]), axis=0),\n      dtype=params.dtype)\n  indices += tf.zeros(\n      tf.concat((leading_bcast_shape, tf.shape(input=indices)[-1:]), axis=0),\n      dtype=indices.dtype)\n  return tf.compat.v1.batch_gather(params, indices)", "code_tokens": ["def", "_batch_gather_with_broadcast", "(", "params", ",", "indices", ",", "axis", ")", ":", "# batch_gather assumes...", "#   params.shape =  [A1,...,AN, B1,...,BM]", "#   indices.shape = [A1,...,AN, C]", "# which gives output of shape", "#                   [A1,...,AN, C, B1,...,BM]", "# Here we broadcast dims of each to the left of `axis` in params, and left of", "# the rightmost dim in indices, e.g. we can", "# have", "#   params.shape =  [A1,...,AN, B1,...,BM]", "#   indices.shape = [a1,...,aN, C],", "# where ai broadcasts with Ai.", "# leading_bcast_shape is the broadcast of [A1,...,AN] and [a1,...,aN].", "leading_bcast_shape", "=", "tf", ".", "broadcast_dynamic_shape", "(", "tf", ".", "shape", "(", "input", "=", "params", ")", "[", ":", "axis", "]", ",", "tf", ".", "shape", "(", "input", "=", "indices", ")", "[", ":", "-", "1", "]", ")", "params", "+=", "tf", ".", "zeros", "(", "tf", ".", "concat", "(", "(", "leading_bcast_shape", ",", "tf", ".", "shape", "(", "input", "=", "params", ")", "[", "axis", ":", "]", ")", ",", "axis", "=", "0", ")", ",", "dtype", "=", "params", ".", "dtype", ")", "indices", "+=", "tf", ".", "zeros", "(", "tf", ".", "concat", "(", "(", "leading_bcast_shape", ",", "tf", ".", "shape", "(", "input", "=", "indices", ")", "[", "-", "1", ":", "]", ")", ",", "axis", "=", "0", ")", ",", "dtype", "=", "indices", ".", "dtype", ")", "return", "tf", ".", "compat", ".", "v1", ".", "batch_gather", "(", "params", ",", "indices", ")"], "docstring": "Like batch_gather, but broadcasts to the left of axis.", "docstring_tokens": ["Like", "batch_gather", "but", "broadcasts", "to", "the", "left", "of", "axis", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/interpolation.py#L930-L954", "partition": "test", "index": 1057, "time": "2018-12-02 09:49:15"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/internal/distribution_tensor_coercible.py", "func_name": "_get_tensor_like_attributes", "original_string": "def _get_tensor_like_attributes():\n  \"\"\"Returns `Tensor` attributes related to shape and Python builtins.\"\"\"\n  # Enable \"Tensor semantics\" for distributions.\n  # See tensorflow/python/framework/ops.py `class Tensor` for details.\n  attrs = dict()\n  # Setup overloadable operators and white-listed members / properties.\n  attrs.update((attr, _wrap_method(tf.Tensor, attr))\n               for attr in tf.Tensor.OVERLOADABLE_OPERATORS.union({'__iter__'}))\n  # Copy some members straight-through.\n  attrs.update((attr, getattr(tf.Tensor, attr))\n               for attr in {'__nonzero__', '__bool__', '__array_priority__'})\n  return attrs", "language": "python", "code": "def _get_tensor_like_attributes():\n  \"\"\"Returns `Tensor` attributes related to shape and Python builtins.\"\"\"\n  # Enable \"Tensor semantics\" for distributions.\n  # See tensorflow/python/framework/ops.py `class Tensor` for details.\n  attrs = dict()\n  # Setup overloadable operators and white-listed members / properties.\n  attrs.update((attr, _wrap_method(tf.Tensor, attr))\n               for attr in tf.Tensor.OVERLOADABLE_OPERATORS.union({'__iter__'}))\n  # Copy some members straight-through.\n  attrs.update((attr, getattr(tf.Tensor, attr))\n               for attr in {'__nonzero__', '__bool__', '__array_priority__'})\n  return attrs", "code_tokens": ["def", "_get_tensor_like_attributes", "(", ")", ":", "# Enable \"Tensor semantics\" for distributions.", "# See tensorflow/python/framework/ops.py `class Tensor` for details.", "attrs", "=", "dict", "(", ")", "# Setup overloadable operators and white-listed members / properties.", "attrs", ".", "update", "(", "(", "attr", ",", "_wrap_method", "(", "tf", ".", "Tensor", ",", "attr", ")", ")", "for", "attr", "in", "tf", ".", "Tensor", ".", "OVERLOADABLE_OPERATORS", ".", "union", "(", "{", "'__iter__'", "}", ")", ")", "# Copy some members straight-through.", "attrs", ".", "update", "(", "(", "attr", ",", "getattr", "(", "tf", ".", "Tensor", ",", "attr", ")", ")", "for", "attr", "in", "{", "'__nonzero__'", ",", "'__bool__'", ",", "'__array_priority__'", "}", ")", "return", "attrs"], "docstring": "Returns `Tensor` attributes related to shape and Python builtins.", "docstring_tokens": ["Returns", "Tensor", "attributes", "related", "to", "shape", "and", "Python", "builtins", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/internal/distribution_tensor_coercible.py#L57-L68", "partition": "test", "index": 816, "time": "2018-12-03 16:40:01"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/distribution_layer.py", "func_name": "_eval_all_one_hot", "original_string": "def _eval_all_one_hot(fn, dist, name=None):\n  \"\"\"OneHotCategorical helper computing probs, cdf, etc over its support.\"\"\"\n  with tf.compat.v1.name_scope(name, 'eval_all_one_hot'):\n    event_size = dist.event_shape_tensor()[-1]\n    batch_ndims = tf.size(input=dist.batch_shape_tensor())\n    # Reshape `eye(d)` to: `[d] + [1]*batch_ndims + [d]`.\n    x = tf.reshape(\n        tf.eye(event_size, dtype=dist.dtype),\n        shape=tf.pad(\n            tensor=tf.ones(batch_ndims, tf.int32),\n            paddings=[[1, 1]],\n            constant_values=event_size))\n    # Compute `fn(x)` then cyclically left-transpose one dim.\n    perm = tf.pad(tensor=tf.range(1, batch_ndims + 1), paddings=[[0, 1]])\n    return tf.transpose(a=fn(dist, x), perm=perm)", "language": "python", "code": "def _eval_all_one_hot(fn, dist, name=None):\n  \"\"\"OneHotCategorical helper computing probs, cdf, etc over its support.\"\"\"\n  with tf.compat.v1.name_scope(name, 'eval_all_one_hot'):\n    event_size = dist.event_shape_tensor()[-1]\n    batch_ndims = tf.size(input=dist.batch_shape_tensor())\n    # Reshape `eye(d)` to: `[d] + [1]*batch_ndims + [d]`.\n    x = tf.reshape(\n        tf.eye(event_size, dtype=dist.dtype),\n        shape=tf.pad(\n            tensor=tf.ones(batch_ndims, tf.int32),\n            paddings=[[1, 1]],\n            constant_values=event_size))\n    # Compute `fn(x)` then cyclically left-transpose one dim.\n    perm = tf.pad(tensor=tf.range(1, batch_ndims + 1), paddings=[[0, 1]])\n    return tf.transpose(a=fn(dist, x), perm=perm)", "code_tokens": ["def", "_eval_all_one_hot", "(", "fn", ",", "dist", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'eval_all_one_hot'", ")", ":", "event_size", "=", "dist", ".", "event_shape_tensor", "(", ")", "[", "-", "1", "]", "batch_ndims", "=", "tf", ".", "size", "(", "input", "=", "dist", ".", "batch_shape_tensor", "(", ")", ")", "# Reshape `eye(d)` to: `[d] + [1]*batch_ndims + [d]`.", "x", "=", "tf", ".", "reshape", "(", "tf", ".", "eye", "(", "event_size", ",", "dtype", "=", "dist", ".", "dtype", ")", ",", "shape", "=", "tf", ".", "pad", "(", "tensor", "=", "tf", ".", "ones", "(", "batch_ndims", ",", "tf", ".", "int32", ")", ",", "paddings", "=", "[", "[", "1", ",", "1", "]", "]", ",", "constant_values", "=", "event_size", ")", ")", "# Compute `fn(x)` then cyclically left-transpose one dim.", "perm", "=", "tf", ".", "pad", "(", "tensor", "=", "tf", ".", "range", "(", "1", ",", "batch_ndims", "+", "1", ")", ",", "paddings", "=", "[", "[", "0", ",", "1", "]", "]", ")", "return", "tf", ".", "transpose", "(", "a", "=", "fn", "(", "dist", ",", "x", ")", ",", "perm", "=", "perm", ")"], "docstring": "OneHotCategorical helper computing probs, cdf, etc over its support.", "docstring_tokens": ["OneHotCategorical", "helper", "computing", "probs", "cdf", "etc", "over", "its", "support", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/distribution_layer.py#L754-L768", "partition": "test", "index": 625, "time": "2018-12-03 16:40:01"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/text_messages_hmc.py", "func_name": "benchmark_text_messages_hmc", "original_string": "def benchmark_text_messages_hmc(\n    num_results=int(3e3),\n    num_burnin_steps=int(3e3),\n    num_leapfrog_steps=3):\n  \"\"\"Runs HMC on the text-messages unnormalized posterior.\"\"\"\n\n  if not tf.executing_eagerly():\n    tf.compat.v1.reset_default_graph()\n\n  # Build a static, pretend dataset.\n  count_data = tf.cast(\n      tf.concat(\n          [tfd.Poisson(rate=15.).sample(43),\n           tfd.Poisson(rate=25.).sample(31)],\n          axis=0),\n      dtype=tf.float32)\n  if tf.executing_eagerly():\n    count_data = count_data.numpy()\n  else:\n    with tf.compat.v1.Session():\n      count_data = count_data.eval()\n\n  # Define a closure over our joint_log_prob.\n  def unnormalized_log_posterior(lambda1, lambda2, tau):\n    return text_messages_joint_log_prob(count_data, lambda1, lambda2, tau)\n\n  if tf.executing_eagerly():\n    sample_chain = tf.function(tfp.mcmc.sample_chain)\n  else:\n    sample_chain = tfp.mcmc.sample_chain\n\n  # Initialize the step_size. (It will be automatically adapted.)\n  step_size = tf.compat.v2.Variable(\n      name='step_size',\n      initial_value=tf.constant(0.05, dtype=tf.float32),\n      trainable=False)\n\n  def computation():\n    \"\"\"The benchmark computation.\"\"\"\n\n    initial_chain_state = [\n        tf.constant(count_data.mean(), name='init_lambda1'),\n        tf.constant(count_data.mean(), name='init_lambda2'),\n        tf.constant(0.5, name='init_tau'),\n    ]\n\n    unconstraining_bijectors = [\n        tfp.bijectors.Exp(),       # Maps a positive real to R.\n        tfp.bijectors.Exp(),       # Maps a positive real to R.\n        tfp.bijectors.Sigmoid(),   # Maps [0,1] to R.\n    ]\n\n    _, kernel_results = sample_chain(\n        num_results=num_results,\n        num_burnin_steps=num_burnin_steps,\n        current_state=initial_chain_state,\n        kernel=tfp.mcmc.TransformedTransitionKernel(\n            inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n                target_log_prob_fn=unnormalized_log_posterior,\n                num_leapfrog_steps=num_leapfrog_steps,\n                step_size=step_size,\n                step_size_update_fn=\n                tfp.mcmc.make_simple_step_size_update_policy(num_burnin_steps),\n                state_gradients_are_stopped=True),\n            bijector=unconstraining_bijectors))\n\n    return kernel_results.inner_results.is_accepted\n\n  # Let's force evaluation of graph to ensure build time is not part of our time\n  # trial.\n  is_accepted_tensor = computation()\n  if not tf.executing_eagerly():\n    session = tf.compat.v1.Session()\n    session.run(tf.compat.v1.global_variables_initializer())\n    session.run(is_accepted_tensor)\n\n  start_time = time.time()\n  if tf.executing_eagerly():\n    is_accepted = computation()\n  else:\n    is_accepted = session.run(is_accepted_tensor)\n  wall_time = time.time() - start_time\n\n  num_accepted = np.sum(is_accepted)\n  acceptance_rate = np.float32(num_accepted) / np.float32(num_results)\n\n  return dict(\n      iters=(num_results + num_burnin_steps) * num_leapfrog_steps,\n      extras={'acceptance_rate': acceptance_rate},\n      wall_time=wall_time)", "language": "python", "code": "def benchmark_text_messages_hmc(\n    num_results=int(3e3),\n    num_burnin_steps=int(3e3),\n    num_leapfrog_steps=3):\n  \"\"\"Runs HMC on the text-messages unnormalized posterior.\"\"\"\n\n  if not tf.executing_eagerly():\n    tf.compat.v1.reset_default_graph()\n\n  # Build a static, pretend dataset.\n  count_data = tf.cast(\n      tf.concat(\n          [tfd.Poisson(rate=15.).sample(43),\n           tfd.Poisson(rate=25.).sample(31)],\n          axis=0),\n      dtype=tf.float32)\n  if tf.executing_eagerly():\n    count_data = count_data.numpy()\n  else:\n    with tf.compat.v1.Session():\n      count_data = count_data.eval()\n\n  # Define a closure over our joint_log_prob.\n  def unnormalized_log_posterior(lambda1, lambda2, tau):\n    return text_messages_joint_log_prob(count_data, lambda1, lambda2, tau)\n\n  if tf.executing_eagerly():\n    sample_chain = tf.function(tfp.mcmc.sample_chain)\n  else:\n    sample_chain = tfp.mcmc.sample_chain\n\n  # Initialize the step_size. (It will be automatically adapted.)\n  step_size = tf.compat.v2.Variable(\n      name='step_size',\n      initial_value=tf.constant(0.05, dtype=tf.float32),\n      trainable=False)\n\n  def computation():\n    \"\"\"The benchmark computation.\"\"\"\n\n    initial_chain_state = [\n        tf.constant(count_data.mean(), name='init_lambda1'),\n        tf.constant(count_data.mean(), name='init_lambda2'),\n        tf.constant(0.5, name='init_tau'),\n    ]\n\n    unconstraining_bijectors = [\n        tfp.bijectors.Exp(),       # Maps a positive real to R.\n        tfp.bijectors.Exp(),       # Maps a positive real to R.\n        tfp.bijectors.Sigmoid(),   # Maps [0,1] to R.\n    ]\n\n    _, kernel_results = sample_chain(\n        num_results=num_results,\n        num_burnin_steps=num_burnin_steps,\n        current_state=initial_chain_state,\n        kernel=tfp.mcmc.TransformedTransitionKernel(\n            inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n                target_log_prob_fn=unnormalized_log_posterior,\n                num_leapfrog_steps=num_leapfrog_steps,\n                step_size=step_size,\n                step_size_update_fn=\n                tfp.mcmc.make_simple_step_size_update_policy(num_burnin_steps),\n                state_gradients_are_stopped=True),\n            bijector=unconstraining_bijectors))\n\n    return kernel_results.inner_results.is_accepted\n\n  # Let's force evaluation of graph to ensure build time is not part of our time\n  # trial.\n  is_accepted_tensor = computation()\n  if not tf.executing_eagerly():\n    session = tf.compat.v1.Session()\n    session.run(tf.compat.v1.global_variables_initializer())\n    session.run(is_accepted_tensor)\n\n  start_time = time.time()\n  if tf.executing_eagerly():\n    is_accepted = computation()\n  else:\n    is_accepted = session.run(is_accepted_tensor)\n  wall_time = time.time() - start_time\n\n  num_accepted = np.sum(is_accepted)\n  acceptance_rate = np.float32(num_accepted) / np.float32(num_results)\n\n  return dict(\n      iters=(num_results + num_burnin_steps) * num_leapfrog_steps,\n      extras={'acceptance_rate': acceptance_rate},\n      wall_time=wall_time)", "code_tokens": ["def", "benchmark_text_messages_hmc", "(", "num_results", "=", "int", "(", "3e3", ")", ",", "num_burnin_steps", "=", "int", "(", "3e3", ")", ",", "num_leapfrog_steps", "=", "3", ")", ":", "if", "not", "tf", ".", "executing_eagerly", "(", ")", ":", "tf", ".", "compat", ".", "v1", ".", "reset_default_graph", "(", ")", "# Build a static, pretend dataset.", "count_data", "=", "tf", ".", "cast", "(", "tf", ".", "concat", "(", "[", "tfd", ".", "Poisson", "(", "rate", "=", "15.", ")", ".", "sample", "(", "43", ")", ",", "tfd", ".", "Poisson", "(", "rate", "=", "25.", ")", ".", "sample", "(", "31", ")", "]", ",", "axis", "=", "0", ")", ",", "dtype", "=", "tf", ".", "float32", ")", "if", "tf", ".", "executing_eagerly", "(", ")", ":", "count_data", "=", "count_data", ".", "numpy", "(", ")", "else", ":", "with", "tf", ".", "compat", ".", "v1", ".", "Session", "(", ")", ":", "count_data", "=", "count_data", ".", "eval", "(", ")", "# Define a closure over our joint_log_prob.", "def", "unnormalized_log_posterior", "(", "lambda1", ",", "lambda2", ",", "tau", ")", ":", "return", "text_messages_joint_log_prob", "(", "count_data", ",", "lambda1", ",", "lambda2", ",", "tau", ")", "if", "tf", ".", "executing_eagerly", "(", ")", ":", "sample_chain", "=", "tf", ".", "function", "(", "tfp", ".", "mcmc", ".", "sample_chain", ")", "else", ":", "sample_chain", "=", "tfp", ".", "mcmc", ".", "sample_chain", "# Initialize the step_size. (It will be automatically adapted.)", "step_size", "=", "tf", ".", "compat", ".", "v2", ".", "Variable", "(", "name", "=", "'step_size'", ",", "initial_value", "=", "tf", ".", "constant", "(", "0.05", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "trainable", "=", "False", ")", "def", "computation", "(", ")", ":", "\"\"\"The benchmark computation.\"\"\"", "initial_chain_state", "=", "[", "tf", ".", "constant", "(", "count_data", ".", "mean", "(", ")", ",", "name", "=", "'init_lambda1'", ")", ",", "tf", ".", "constant", "(", "count_data", ".", "mean", "(", ")", ",", "name", "=", "'init_lambda2'", ")", ",", "tf", ".", "constant", "(", "0.5", ",", "name", "=", "'init_tau'", ")", ",", "]", "unconstraining_bijectors", "=", "[", "tfp", ".", "bijectors", ".", "Exp", "(", ")", ",", "# Maps a positive real to R.", "tfp", ".", "bijectors", ".", "Exp", "(", ")", ",", "# Maps a positive real to R.", "tfp", ".", "bijectors", ".", "Sigmoid", "(", ")", ",", "# Maps [0,1] to R.", "]", "_", ",", "kernel_results", "=", "sample_chain", "(", "num_results", "=", "num_results", ",", "num_burnin_steps", "=", "num_burnin_steps", ",", "current_state", "=", "initial_chain_state", ",", "kernel", "=", "tfp", ".", "mcmc", ".", "TransformedTransitionKernel", "(", "inner_kernel", "=", "tfp", ".", "mcmc", ".", "HamiltonianMonteCarlo", "(", "target_log_prob_fn", "=", "unnormalized_log_posterior", ",", "num_leapfrog_steps", "=", "num_leapfrog_steps", ",", "step_size", "=", "step_size", ",", "step_size_update_fn", "=", "tfp", ".", "mcmc", ".", "make_simple_step_size_update_policy", "(", "num_burnin_steps", ")", ",", "state_gradients_are_stopped", "=", "True", ")", ",", "bijector", "=", "unconstraining_bijectors", ")", ")", "return", "kernel_results", ".", "inner_results", ".", "is_accepted", "# Let's force evaluation of graph to ensure build time is not part of our time", "# trial.", "is_accepted_tensor", "=", "computation", "(", ")", "if", "not", "tf", ".", "executing_eagerly", "(", ")", ":", "session", "=", "tf", ".", "compat", ".", "v1", ".", "Session", "(", ")", "session", ".", "run", "(", "tf", ".", "compat", ".", "v1", ".", "global_variables_initializer", "(", ")", ")", "session", ".", "run", "(", "is_accepted_tensor", ")", "start_time", "=", "time", ".", "time", "(", ")", "if", "tf", ".", "executing_eagerly", "(", ")", ":", "is_accepted", "=", "computation", "(", ")", "else", ":", "is_accepted", "=", "session", ".", "run", "(", "is_accepted_tensor", ")", "wall_time", "=", "time", ".", "time", "(", ")", "-", "start_time", "num_accepted", "=", "np", ".", "sum", "(", "is_accepted", ")", "acceptance_rate", "=", "np", ".", "float32", "(", "num_accepted", ")", "/", "np", ".", "float32", "(", "num_results", ")", "return", "dict", "(", "iters", "=", "(", "num_results", "+", "num_burnin_steps", ")", "*", "num_leapfrog_steps", ",", "extras", "=", "{", "'acceptance_rate'", ":", "acceptance_rate", "}", ",", "wall_time", "=", "wall_time", ")"], "docstring": "Runs HMC on the text-messages unnormalized posterior.", "docstring_tokens": ["Runs", "HMC", "on", "the", "text", "-", "messages", "unnormalized", "posterior", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/text_messages_hmc.py#L64-L153", "partition": "test", "index": 749, "time": "2018-12-04 12:17:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/text_messages_hmc.py", "func_name": "text_messages_joint_log_prob", "original_string": "def text_messages_joint_log_prob(count_data, lambda_1, lambda_2, tau):\n  \"\"\"Joint log probability function.\"\"\"\n  alpha = (1. / tf.reduce_mean(input_tensor=count_data))\n  rv_lambda = tfd.Exponential(rate=alpha)\n\n  rv_tau = tfd.Uniform()\n\n  lambda_ = tf.gather(\n      [lambda_1, lambda_2],\n      indices=tf.cast(\n          tau * tf.cast(tf.size(input=count_data), dtype=tf.float32) <= tf.cast(\n              tf.range(tf.size(input=count_data)), dtype=tf.float32),\n          dtype=tf.int32))\n  rv_observation = tfd.Poisson(rate=lambda_)\n\n  return (rv_lambda.log_prob(lambda_1) + rv_lambda.log_prob(lambda_2) +\n          rv_tau.log_prob(tau) +\n          tf.reduce_sum(input_tensor=rv_observation.log_prob(count_data)))", "language": "python", "code": "def text_messages_joint_log_prob(count_data, lambda_1, lambda_2, tau):\n  \"\"\"Joint log probability function.\"\"\"\n  alpha = (1. / tf.reduce_mean(input_tensor=count_data))\n  rv_lambda = tfd.Exponential(rate=alpha)\n\n  rv_tau = tfd.Uniform()\n\n  lambda_ = tf.gather(\n      [lambda_1, lambda_2],\n      indices=tf.cast(\n          tau * tf.cast(tf.size(input=count_data), dtype=tf.float32) <= tf.cast(\n              tf.range(tf.size(input=count_data)), dtype=tf.float32),\n          dtype=tf.int32))\n  rv_observation = tfd.Poisson(rate=lambda_)\n\n  return (rv_lambda.log_prob(lambda_1) + rv_lambda.log_prob(lambda_2) +\n          rv_tau.log_prob(tau) +\n          tf.reduce_sum(input_tensor=rv_observation.log_prob(count_data)))", "code_tokens": ["def", "text_messages_joint_log_prob", "(", "count_data", ",", "lambda_1", ",", "lambda_2", ",", "tau", ")", ":", "alpha", "=", "(", "1.", "/", "tf", ".", "reduce_mean", "(", "input_tensor", "=", "count_data", ")", ")", "rv_lambda", "=", "tfd", ".", "Exponential", "(", "rate", "=", "alpha", ")", "rv_tau", "=", "tfd", ".", "Uniform", "(", ")", "lambda_", "=", "tf", ".", "gather", "(", "[", "lambda_1", ",", "lambda_2", "]", ",", "indices", "=", "tf", ".", "cast", "(", "tau", "*", "tf", ".", "cast", "(", "tf", ".", "size", "(", "input", "=", "count_data", ")", ",", "dtype", "=", "tf", ".", "float32", ")", "<=", "tf", ".", "cast", "(", "tf", ".", "range", "(", "tf", ".", "size", "(", "input", "=", "count_data", ")", ")", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "dtype", "=", "tf", ".", "int32", ")", ")", "rv_observation", "=", "tfd", ".", "Poisson", "(", "rate", "=", "lambda_", ")", "return", "(", "rv_lambda", ".", "log_prob", "(", "lambda_1", ")", "+", "rv_lambda", ".", "log_prob", "(", "lambda_2", ")", "+", "rv_tau", ".", "log_prob", "(", "tau", ")", "+", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "rv_observation", ".", "log_prob", "(", "count_data", ")", ")", ")"], "docstring": "Joint log probability function.", "docstring_tokens": ["Joint", "log", "probability", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/text_messages_hmc.py#L44-L61", "partition": "test", "index": 748, "time": "2018-12-04 12:17:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/lbfgs.py", "func_name": "_get_initial_state", "original_string": "def _get_initial_state(value_and_gradients_function,\n                       initial_position,\n                       num_correction_pairs,\n                       tolerance):\n  \"\"\"Create LBfgsOptimizerResults with initial state of search procedure.\"\"\"\n  init_args = bfgs_utils.get_initial_state_args(\n      value_and_gradients_function,\n      initial_position,\n      tolerance)\n  empty_queue = _make_empty_queue_for(num_correction_pairs, initial_position)\n  init_args.update(position_deltas=empty_queue, gradient_deltas=empty_queue)\n  return LBfgsOptimizerResults(**init_args)", "language": "python", "code": "def _get_initial_state(value_and_gradients_function,\n                       initial_position,\n                       num_correction_pairs,\n                       tolerance):\n  \"\"\"Create LBfgsOptimizerResults with initial state of search procedure.\"\"\"\n  init_args = bfgs_utils.get_initial_state_args(\n      value_and_gradients_function,\n      initial_position,\n      tolerance)\n  empty_queue = _make_empty_queue_for(num_correction_pairs, initial_position)\n  init_args.update(position_deltas=empty_queue, gradient_deltas=empty_queue)\n  return LBfgsOptimizerResults(**init_args)", "code_tokens": ["def", "_get_initial_state", "(", "value_and_gradients_function", ",", "initial_position", ",", "num_correction_pairs", ",", "tolerance", ")", ":", "init_args", "=", "bfgs_utils", ".", "get_initial_state_args", "(", "value_and_gradients_function", ",", "initial_position", ",", "tolerance", ")", "empty_queue", "=", "_make_empty_queue_for", "(", "num_correction_pairs", ",", "initial_position", ")", "init_args", ".", "update", "(", "position_deltas", "=", "empty_queue", ",", "gradient_deltas", "=", "empty_queue", ")", "return", "LBfgsOptimizerResults", "(", "*", "*", "init_args", ")"], "docstring": "Create LBfgsOptimizerResults with initial state of search procedure.", "docstring_tokens": ["Create", "LBfgsOptimizerResults", "with", "initial", "state", "of", "search", "procedure", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/lbfgs.py#L263-L274", "partition": "test", "index": 1073, "time": "2018-12-05 09:05:25"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/lbfgs.py", "func_name": "_get_search_direction", "original_string": "def _get_search_direction(state):\n  \"\"\"Computes the search direction to follow at the current state.\n\n  On the `k`-th iteration of the main L-BFGS algorithm, the state has collected\n  the most recent `m` correction pairs in position_deltas and gradient_deltas,\n  where `k = state.num_iterations` and `m = min(k, num_correction_pairs)`.\n\n  Assuming these, the code below is an implementation of the L-BFGS two-loop\n  recursion algorithm given by [Nocedal and Wright(2006)][1]:\n\n  ```None\n    q_direction = objective_gradient\n    for i in reversed(range(m)):  # First loop.\n      inv_rho[i] = gradient_deltas[i]^T * position_deltas[i]\n      alpha[i] = position_deltas[i]^T * q_direction / inv_rho[i]\n      q_direction = q_direction - alpha[i] * gradient_deltas[i]\n\n    kth_inv_hessian_factor = (gradient_deltas[-1]^T * position_deltas[-1] /\n                              gradient_deltas[-1]^T * gradient_deltas[-1])\n    r_direction = kth_inv_hessian_factor * I * q_direction\n\n    for i in range(m):  # Second loop.\n      beta = gradient_deltas[i]^T * r_direction / inv_rho[i]\n      r_direction = r_direction + position_deltas[i] * (alpha[i] - beta)\n\n    return -r_direction  # Approximates - H_k * objective_gradient.\n  ```\n\n  Args:\n    state: A `LBfgsOptimizerResults` tuple with the current state of the\n      search procedure.\n\n  Returns:\n    A real `Tensor` of the same shape as the `state.position`. The direction\n    along which to perform line search.\n  \"\"\"\n  # The number of correction pairs that have been collected so far.\n  num_elements = tf.minimum(\n      state.num_iterations,\n      distribution_util.prefer_static_shape(state.position_deltas)[0])\n\n  def _two_loop_algorithm():\n    \"\"\"L-BFGS two-loop algorithm.\"\"\"\n    # Correction pairs are always appended to the end, so only the latest\n    # `num_elements` vectors have valid position/gradient deltas.\n    position_deltas = state.position_deltas[-num_elements:]\n    gradient_deltas = state.gradient_deltas[-num_elements:]\n\n    # Pre-compute all `inv_rho[i]`s.\n    inv_rhos = tf.reduce_sum(\n        input_tensor=gradient_deltas * position_deltas, axis=-1)\n\n    def first_loop(acc, args):\n      _, q_direction = acc\n      position_delta, gradient_delta, inv_rho = args\n      alpha = tf.reduce_sum(\n          input_tensor=position_delta * q_direction, axis=-1) / inv_rho\n      direction_delta = tf.expand_dims(alpha, axis=-1) * gradient_delta\n      return (alpha, q_direction - direction_delta)\n\n    # Run first loop body computing and collecting `alpha[i]`s, while also\n    # computing the updated `q_direction` at each step.\n    zero = tf.zeros_like(inv_rhos[0])\n    alphas, q_directions = tf.scan(\n        first_loop, [position_deltas, gradient_deltas, inv_rhos],\n        initializer=(zero, state.objective_gradient), reverse=True)\n\n    # We use `H^0_k = gamma_k * I` as an estimate for the initial inverse\n    # hessian for the k-th iteration; then `r_direction = H^0_k * q_direction`.\n    gamma_k = inv_rhos[-1] / tf.reduce_sum(\n        input_tensor=gradient_deltas[-1] * gradient_deltas[-1], axis=-1)\n    r_direction = tf.expand_dims(gamma_k, axis=-1) * q_directions[0]\n\n    def second_loop(r_direction, args):\n      alpha, position_delta, gradient_delta, inv_rho = args\n      beta = tf.reduce_sum(\n          input_tensor=gradient_delta * r_direction, axis=-1) / inv_rho\n      direction_delta = tf.expand_dims(alpha - beta, axis=-1) * position_delta\n      return r_direction + direction_delta\n\n    # Finally, run second loop body computing the updated `r_direction` at each\n    # step.\n    r_directions = tf.scan(\n        second_loop, [alphas, position_deltas, gradient_deltas, inv_rhos],\n        initializer=r_direction)\n    return -r_directions[-1]\n\n  return prefer_static.cond(tf.equal(num_elements, 0),\n                            (lambda: -state.objective_gradient),\n                            _two_loop_algorithm)", "language": "python", "code": "def _get_search_direction(state):\n  \"\"\"Computes the search direction to follow at the current state.\n\n  On the `k`-th iteration of the main L-BFGS algorithm, the state has collected\n  the most recent `m` correction pairs in position_deltas and gradient_deltas,\n  where `k = state.num_iterations` and `m = min(k, num_correction_pairs)`.\n\n  Assuming these, the code below is an implementation of the L-BFGS two-loop\n  recursion algorithm given by [Nocedal and Wright(2006)][1]:\n\n  ```None\n    q_direction = objective_gradient\n    for i in reversed(range(m)):  # First loop.\n      inv_rho[i] = gradient_deltas[i]^T * position_deltas[i]\n      alpha[i] = position_deltas[i]^T * q_direction / inv_rho[i]\n      q_direction = q_direction - alpha[i] * gradient_deltas[i]\n\n    kth_inv_hessian_factor = (gradient_deltas[-1]^T * position_deltas[-1] /\n                              gradient_deltas[-1]^T * gradient_deltas[-1])\n    r_direction = kth_inv_hessian_factor * I * q_direction\n\n    for i in range(m):  # Second loop.\n      beta = gradient_deltas[i]^T * r_direction / inv_rho[i]\n      r_direction = r_direction + position_deltas[i] * (alpha[i] - beta)\n\n    return -r_direction  # Approximates - H_k * objective_gradient.\n  ```\n\n  Args:\n    state: A `LBfgsOptimizerResults` tuple with the current state of the\n      search procedure.\n\n  Returns:\n    A real `Tensor` of the same shape as the `state.position`. The direction\n    along which to perform line search.\n  \"\"\"\n  # The number of correction pairs that have been collected so far.\n  num_elements = tf.minimum(\n      state.num_iterations,\n      distribution_util.prefer_static_shape(state.position_deltas)[0])\n\n  def _two_loop_algorithm():\n    \"\"\"L-BFGS two-loop algorithm.\"\"\"\n    # Correction pairs are always appended to the end, so only the latest\n    # `num_elements` vectors have valid position/gradient deltas.\n    position_deltas = state.position_deltas[-num_elements:]\n    gradient_deltas = state.gradient_deltas[-num_elements:]\n\n    # Pre-compute all `inv_rho[i]`s.\n    inv_rhos = tf.reduce_sum(\n        input_tensor=gradient_deltas * position_deltas, axis=-1)\n\n    def first_loop(acc, args):\n      _, q_direction = acc\n      position_delta, gradient_delta, inv_rho = args\n      alpha = tf.reduce_sum(\n          input_tensor=position_delta * q_direction, axis=-1) / inv_rho\n      direction_delta = tf.expand_dims(alpha, axis=-1) * gradient_delta\n      return (alpha, q_direction - direction_delta)\n\n    # Run first loop body computing and collecting `alpha[i]`s, while also\n    # computing the updated `q_direction` at each step.\n    zero = tf.zeros_like(inv_rhos[0])\n    alphas, q_directions = tf.scan(\n        first_loop, [position_deltas, gradient_deltas, inv_rhos],\n        initializer=(zero, state.objective_gradient), reverse=True)\n\n    # We use `H^0_k = gamma_k * I` as an estimate for the initial inverse\n    # hessian for the k-th iteration; then `r_direction = H^0_k * q_direction`.\n    gamma_k = inv_rhos[-1] / tf.reduce_sum(\n        input_tensor=gradient_deltas[-1] * gradient_deltas[-1], axis=-1)\n    r_direction = tf.expand_dims(gamma_k, axis=-1) * q_directions[0]\n\n    def second_loop(r_direction, args):\n      alpha, position_delta, gradient_delta, inv_rho = args\n      beta = tf.reduce_sum(\n          input_tensor=gradient_delta * r_direction, axis=-1) / inv_rho\n      direction_delta = tf.expand_dims(alpha - beta, axis=-1) * position_delta\n      return r_direction + direction_delta\n\n    # Finally, run second loop body computing the updated `r_direction` at each\n    # step.\n    r_directions = tf.scan(\n        second_loop, [alphas, position_deltas, gradient_deltas, inv_rhos],\n        initializer=r_direction)\n    return -r_directions[-1]\n\n  return prefer_static.cond(tf.equal(num_elements, 0),\n                            (lambda: -state.objective_gradient),\n                            _two_loop_algorithm)", "code_tokens": ["def", "_get_search_direction", "(", "state", ")", ":", "# The number of correction pairs that have been collected so far.", "num_elements", "=", "tf", ".", "minimum", "(", "state", ".", "num_iterations", ",", "distribution_util", ".", "prefer_static_shape", "(", "state", ".", "position_deltas", ")", "[", "0", "]", ")", "def", "_two_loop_algorithm", "(", ")", ":", "\"\"\"L-BFGS two-loop algorithm.\"\"\"", "# Correction pairs are always appended to the end, so only the latest", "# `num_elements` vectors have valid position/gradient deltas.", "position_deltas", "=", "state", ".", "position_deltas", "[", "-", "num_elements", ":", "]", "gradient_deltas", "=", "state", ".", "gradient_deltas", "[", "-", "num_elements", ":", "]", "# Pre-compute all `inv_rho[i]`s.", "inv_rhos", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "gradient_deltas", "*", "position_deltas", ",", "axis", "=", "-", "1", ")", "def", "first_loop", "(", "acc", ",", "args", ")", ":", "_", ",", "q_direction", "=", "acc", "position_delta", ",", "gradient_delta", ",", "inv_rho", "=", "args", "alpha", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "position_delta", "*", "q_direction", ",", "axis", "=", "-", "1", ")", "/", "inv_rho", "direction_delta", "=", "tf", ".", "expand_dims", "(", "alpha", ",", "axis", "=", "-", "1", ")", "*", "gradient_delta", "return", "(", "alpha", ",", "q_direction", "-", "direction_delta", ")", "# Run first loop body computing and collecting `alpha[i]`s, while also", "# computing the updated `q_direction` at each step.", "zero", "=", "tf", ".", "zeros_like", "(", "inv_rhos", "[", "0", "]", ")", "alphas", ",", "q_directions", "=", "tf", ".", "scan", "(", "first_loop", ",", "[", "position_deltas", ",", "gradient_deltas", ",", "inv_rhos", "]", ",", "initializer", "=", "(", "zero", ",", "state", ".", "objective_gradient", ")", ",", "reverse", "=", "True", ")", "# We use `H^0_k = gamma_k * I` as an estimate for the initial inverse", "# hessian for the k-th iteration; then `r_direction = H^0_k * q_direction`.", "gamma_k", "=", "inv_rhos", "[", "-", "1", "]", "/", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "gradient_deltas", "[", "-", "1", "]", "*", "gradient_deltas", "[", "-", "1", "]", ",", "axis", "=", "-", "1", ")", "r_direction", "=", "tf", ".", "expand_dims", "(", "gamma_k", ",", "axis", "=", "-", "1", ")", "*", "q_directions", "[", "0", "]", "def", "second_loop", "(", "r_direction", ",", "args", ")", ":", "alpha", ",", "position_delta", ",", "gradient_delta", ",", "inv_rho", "=", "args", "beta", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "gradient_delta", "*", "r_direction", ",", "axis", "=", "-", "1", ")", "/", "inv_rho", "direction_delta", "=", "tf", ".", "expand_dims", "(", "alpha", "-", "beta", ",", "axis", "=", "-", "1", ")", "*", "position_delta", "return", "r_direction", "+", "direction_delta", "# Finally, run second loop body computing the updated `r_direction` at each", "# step.", "r_directions", "=", "tf", ".", "scan", "(", "second_loop", ",", "[", "alphas", ",", "position_deltas", ",", "gradient_deltas", ",", "inv_rhos", "]", ",", "initializer", "=", "r_direction", ")", "return", "-", "r_directions", "[", "-", "1", "]", "return", "prefer_static", ".", "cond", "(", "tf", ".", "equal", "(", "num_elements", ",", "0", ")", ",", "(", "lambda", ":", "-", "state", ".", "objective_gradient", ")", ",", "_two_loop_algorithm", ")"], "docstring": "Computes the search direction to follow at the current state.\n\n  On the `k`-th iteration of the main L-BFGS algorithm, the state has collected\n  the most recent `m` correction pairs in position_deltas and gradient_deltas,\n  where `k = state.num_iterations` and `m = min(k, num_correction_pairs)`.\n\n  Assuming these, the code below is an implementation of the L-BFGS two-loop\n  recursion algorithm given by [Nocedal and Wright(2006)][1]:\n\n  ```None\n    q_direction = objective_gradient\n    for i in reversed(range(m)):  # First loop.\n      inv_rho[i] = gradient_deltas[i]^T * position_deltas[i]\n      alpha[i] = position_deltas[i]^T * q_direction / inv_rho[i]\n      q_direction = q_direction - alpha[i] * gradient_deltas[i]\n\n    kth_inv_hessian_factor = (gradient_deltas[-1]^T * position_deltas[-1] /\n                              gradient_deltas[-1]^T * gradient_deltas[-1])\n    r_direction = kth_inv_hessian_factor * I * q_direction\n\n    for i in range(m):  # Second loop.\n      beta = gradient_deltas[i]^T * r_direction / inv_rho[i]\n      r_direction = r_direction + position_deltas[i] * (alpha[i] - beta)\n\n    return -r_direction  # Approximates - H_k * objective_gradient.\n  ```\n\n  Args:\n    state: A `LBfgsOptimizerResults` tuple with the current state of the\n      search procedure.\n\n  Returns:\n    A real `Tensor` of the same shape as the `state.position`. The direction\n    along which to perform line search.", "docstring_tokens": ["Computes", "the", "search", "direction", "to", "follow", "at", "the", "current", "state", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/lbfgs.py#L277-L366", "partition": "test", "index": 1074, "time": "2018-12-05 09:05:25"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/lbfgs.py", "func_name": "_make_empty_queue_for", "original_string": "def _make_empty_queue_for(k, element):\n  \"\"\"Creates a `tf.Tensor` suitable to hold `k` element-shaped tensors.\n\n  For example:\n\n  ```python\n    element = tf.constant([[0., 1., 2., 3., 4.],\n                           [5., 6., 7., 8., 9.]])\n\n    # A queue capable of holding 3 elements.\n    _make_empty_queue_for(3, element)\n    # => [[[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]]]\n  ```\n\n  Args:\n    k: A positive scalar integer, number of elements that each queue will hold.\n    element: A `tf.Tensor`, only its shape and dtype information are relevant.\n\n  Returns:\n    A zero-filed `tf.Tensor` of shape `(k,) + tf.shape(element)` and same dtype\n    as `element`.\n  \"\"\"\n  queue_shape = tf.concat(\n      [[k], distribution_util.prefer_static_shape(element)], axis=0)\n  return tf.zeros(queue_shape, dtype=element.dtype.base_dtype)", "language": "python", "code": "def _make_empty_queue_for(k, element):\n  \"\"\"Creates a `tf.Tensor` suitable to hold `k` element-shaped tensors.\n\n  For example:\n\n  ```python\n    element = tf.constant([[0., 1., 2., 3., 4.],\n                           [5., 6., 7., 8., 9.]])\n\n    # A queue capable of holding 3 elements.\n    _make_empty_queue_for(3, element)\n    # => [[[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]]]\n  ```\n\n  Args:\n    k: A positive scalar integer, number of elements that each queue will hold.\n    element: A `tf.Tensor`, only its shape and dtype information are relevant.\n\n  Returns:\n    A zero-filed `tf.Tensor` of shape `(k,) + tf.shape(element)` and same dtype\n    as `element`.\n  \"\"\"\n  queue_shape = tf.concat(\n      [[k], distribution_util.prefer_static_shape(element)], axis=0)\n  return tf.zeros(queue_shape, dtype=element.dtype.base_dtype)", "code_tokens": ["def", "_make_empty_queue_for", "(", "k", ",", "element", ")", ":", "queue_shape", "=", "tf", ".", "concat", "(", "[", "[", "k", "]", ",", "distribution_util", ".", "prefer_static_shape", "(", "element", ")", "]", ",", "axis", "=", "0", ")", "return", "tf", ".", "zeros", "(", "queue_shape", ",", "dtype", "=", "element", ".", "dtype", ".", "base_dtype", ")"], "docstring": "Creates a `tf.Tensor` suitable to hold `k` element-shaped tensors.\n\n  For example:\n\n  ```python\n    element = tf.constant([[0., 1., 2., 3., 4.],\n                           [5., 6., 7., 8., 9.]])\n\n    # A queue capable of holding 3 elements.\n    _make_empty_queue_for(3, element)\n    # => [[[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]]]\n  ```\n\n  Args:\n    k: A positive scalar integer, number of elements that each queue will hold.\n    element: A `tf.Tensor`, only its shape and dtype information are relevant.\n\n  Returns:\n    A zero-filed `tf.Tensor` of shape `(k,) + tf.shape(element)` and same dtype\n    as `element`.", "docstring_tokens": ["Creates", "a", "tf", ".", "Tensor", "suitable", "to", "hold", "k", "element", "-", "shaped", "tensors", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/lbfgs.py#L369-L400", "partition": "test", "index": 1075, "time": "2018-12-05 09:05:25"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/lbfgs.py", "func_name": "_queue_push", "original_string": "def _queue_push(queue, should_update, new_vecs):\n  \"\"\"Conditionally push new vectors into a batch of first-in-first-out queues.\n\n  The `queue` of shape `[k, ..., n]` can be thought of as a batch of queues,\n  each holding `k` n-D vectors; while `new_vecs` of shape `[..., n]` is a\n  fresh new batch of n-D vectors. The `should_update` batch of Boolean scalars,\n  i.e. shape `[...]`, indicates batch members whose corresponding n-D vector in\n  `new_vecs` should be added at the back of its queue, pushing out the\n  corresponding n-D vector from the front. Batch members in `new_vecs` for\n  which `should_update` is False are ignored.\n\n  Note: the choice of placing `k` at the dimension 0 of the queue is\n  constrained by the L-BFGS two-loop algorithm above. The algorithm uses\n  tf.scan to iterate over the `k` correction pairs simulatneously across all\n  batches, and tf.scan itself can only iterate over dimension 0.\n\n  For example:\n\n  ```python\n    k, b, n = (3, 2, 5)\n    queue = tf.reshape(tf.range(30), (k, b, n))\n    # => [[[ 0,  1,  2,  3,  4],\n    #      [ 5,  6,  7,  8,  9]],\n    #\n    #     [[10, 11, 12, 13, 14],\n    #      [15, 16, 17, 18, 19]],\n    #\n    #     [[20, 21, 22, 23, 24],\n    #      [25, 26, 27, 28, 29]]]\n\n    element = tf.reshape(tf.range(30, 40), (b, n))\n    # => [[30, 31, 32, 33, 34],\n          [35, 36, 37, 38, 39]]\n\n    should_update = tf.constant([True, False])  # Shape: (b,)\n\n    _queue_add(should_update, queue, element)\n    # => [[[10, 11, 12, 13, 14],\n    #      [ 5,  6,  7,  8,  9]],\n    #\n    #     [[20, 21, 22, 23, 24],\n    #      [15, 16, 17, 18, 19]],\n    #\n    #     [[30, 31, 32, 33, 34],\n    #      [25, 26, 27, 28, 29]]]\n  ```\n\n  Args:\n    queue: A `tf.Tensor` of shape `[k, ..., n]`; a batch of queues each with\n      `k` n-D vectors.\n    should_update: A Boolean `tf.Tensor` of shape `[...]` indicating batch\n      members where new vectors should be added to their queues.\n    new_vecs: A `tf.Tensor` of shape `[..., n]`; a batch of n-D vectors to add\n      at the end of their respective queues, pushing out the first element from\n      each.\n\n  Returns:\n    A new `tf.Tensor` of shape `[k, ..., n]`.\n  \"\"\"\n  new_queue = tf.concat([queue[1:], [new_vecs]], axis=0)\n  update_pattern = tf.broadcast_to(\n      should_update[tf.newaxis, ..., tf.newaxis],\n      distribution_util.prefer_static_shape(queue))\n  return tf.where(update_pattern, new_queue, queue)", "language": "python", "code": "def _queue_push(queue, should_update, new_vecs):\n  \"\"\"Conditionally push new vectors into a batch of first-in-first-out queues.\n\n  The `queue` of shape `[k, ..., n]` can be thought of as a batch of queues,\n  each holding `k` n-D vectors; while `new_vecs` of shape `[..., n]` is a\n  fresh new batch of n-D vectors. The `should_update` batch of Boolean scalars,\n  i.e. shape `[...]`, indicates batch members whose corresponding n-D vector in\n  `new_vecs` should be added at the back of its queue, pushing out the\n  corresponding n-D vector from the front. Batch members in `new_vecs` for\n  which `should_update` is False are ignored.\n\n  Note: the choice of placing `k` at the dimension 0 of the queue is\n  constrained by the L-BFGS two-loop algorithm above. The algorithm uses\n  tf.scan to iterate over the `k` correction pairs simulatneously across all\n  batches, and tf.scan itself can only iterate over dimension 0.\n\n  For example:\n\n  ```python\n    k, b, n = (3, 2, 5)\n    queue = tf.reshape(tf.range(30), (k, b, n))\n    # => [[[ 0,  1,  2,  3,  4],\n    #      [ 5,  6,  7,  8,  9]],\n    #\n    #     [[10, 11, 12, 13, 14],\n    #      [15, 16, 17, 18, 19]],\n    #\n    #     [[20, 21, 22, 23, 24],\n    #      [25, 26, 27, 28, 29]]]\n\n    element = tf.reshape(tf.range(30, 40), (b, n))\n    # => [[30, 31, 32, 33, 34],\n          [35, 36, 37, 38, 39]]\n\n    should_update = tf.constant([True, False])  # Shape: (b,)\n\n    _queue_add(should_update, queue, element)\n    # => [[[10, 11, 12, 13, 14],\n    #      [ 5,  6,  7,  8,  9]],\n    #\n    #     [[20, 21, 22, 23, 24],\n    #      [15, 16, 17, 18, 19]],\n    #\n    #     [[30, 31, 32, 33, 34],\n    #      [25, 26, 27, 28, 29]]]\n  ```\n\n  Args:\n    queue: A `tf.Tensor` of shape `[k, ..., n]`; a batch of queues each with\n      `k` n-D vectors.\n    should_update: A Boolean `tf.Tensor` of shape `[...]` indicating batch\n      members where new vectors should be added to their queues.\n    new_vecs: A `tf.Tensor` of shape `[..., n]`; a batch of n-D vectors to add\n      at the end of their respective queues, pushing out the first element from\n      each.\n\n  Returns:\n    A new `tf.Tensor` of shape `[k, ..., n]`.\n  \"\"\"\n  new_queue = tf.concat([queue[1:], [new_vecs]], axis=0)\n  update_pattern = tf.broadcast_to(\n      should_update[tf.newaxis, ..., tf.newaxis],\n      distribution_util.prefer_static_shape(queue))\n  return tf.where(update_pattern, new_queue, queue)", "code_tokens": ["def", "_queue_push", "(", "queue", ",", "should_update", ",", "new_vecs", ")", ":", "new_queue", "=", "tf", ".", "concat", "(", "[", "queue", "[", "1", ":", "]", ",", "[", "new_vecs", "]", "]", ",", "axis", "=", "0", ")", "update_pattern", "=", "tf", ".", "broadcast_to", "(", "should_update", "[", "tf", ".", "newaxis", ",", "...", ",", "tf", ".", "newaxis", "]", ",", "distribution_util", ".", "prefer_static_shape", "(", "queue", ")", ")", "return", "tf", ".", "where", "(", "update_pattern", ",", "new_queue", ",", "queue", ")"], "docstring": "Conditionally push new vectors into a batch of first-in-first-out queues.\n\n  The `queue` of shape `[k, ..., n]` can be thought of as a batch of queues,\n  each holding `k` n-D vectors; while `new_vecs` of shape `[..., n]` is a\n  fresh new batch of n-D vectors. The `should_update` batch of Boolean scalars,\n  i.e. shape `[...]`, indicates batch members whose corresponding n-D vector in\n  `new_vecs` should be added at the back of its queue, pushing out the\n  corresponding n-D vector from the front. Batch members in `new_vecs` for\n  which `should_update` is False are ignored.\n\n  Note: the choice of placing `k` at the dimension 0 of the queue is\n  constrained by the L-BFGS two-loop algorithm above. The algorithm uses\n  tf.scan to iterate over the `k` correction pairs simulatneously across all\n  batches, and tf.scan itself can only iterate over dimension 0.\n\n  For example:\n\n  ```python\n    k, b, n = (3, 2, 5)\n    queue = tf.reshape(tf.range(30), (k, b, n))\n    # => [[[ 0,  1,  2,  3,  4],\n    #      [ 5,  6,  7,  8,  9]],\n    #\n    #     [[10, 11, 12, 13, 14],\n    #      [15, 16, 17, 18, 19]],\n    #\n    #     [[20, 21, 22, 23, 24],\n    #      [25, 26, 27, 28, 29]]]\n\n    element = tf.reshape(tf.range(30, 40), (b, n))\n    # => [[30, 31, 32, 33, 34],\n          [35, 36, 37, 38, 39]]\n\n    should_update = tf.constant([True, False])  # Shape: (b,)\n\n    _queue_add(should_update, queue, element)\n    # => [[[10, 11, 12, 13, 14],\n    #      [ 5,  6,  7,  8,  9]],\n    #\n    #     [[20, 21, 22, 23, 24],\n    #      [15, 16, 17, 18, 19]],\n    #\n    #     [[30, 31, 32, 33, 34],\n    #      [25, 26, 27, 28, 29]]]\n  ```\n\n  Args:\n    queue: A `tf.Tensor` of shape `[k, ..., n]`; a batch of queues each with\n      `k` n-D vectors.\n    should_update: A Boolean `tf.Tensor` of shape `[...]` indicating batch\n      members where new vectors should be added to their queues.\n    new_vecs: A `tf.Tensor` of shape `[..., n]`; a batch of n-D vectors to add\n      at the end of their respective queues, pushing out the first element from\n      each.\n\n  Returns:\n    A new `tf.Tensor` of shape `[k, ..., n]`.", "docstring_tokens": ["Conditionally", "push", "new", "vectors", "into", "a", "batch", "of", "first", "-", "in", "-", "first", "-", "out", "queues", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/lbfgs.py#L403-L466", "partition": "test", "index": 1076, "time": "2018-12-05 09:05:25"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/lbfgs.py", "func_name": "minimize", "original_string": "def minimize(value_and_gradients_function,\n             initial_position,\n             num_correction_pairs=10,\n             tolerance=1e-8,\n             x_tolerance=0,\n             f_relative_tolerance=0,\n             initial_inverse_hessian_estimate=None,\n             max_iterations=50,\n             parallel_iterations=1,\n             stopping_condition=None,\n             name=None):\n  \"\"\"Applies the L-BFGS algorithm to minimize a differentiable function.\n\n  Performs unconstrained minimization of a differentiable function using the\n  L-BFGS scheme. See [Nocedal and Wright(2006)][1] for details of the algorithm.\n\n  ### Usage:\n\n  The following example demonstrates the L-BFGS optimizer attempting to find the\n  minimum for a simple high-dimensional quadratic objective function.\n\n  ```python\n    # A high-dimensional quadratic bowl.\n    ndims = 60\n    minimum = np.ones([ndims], dtype='float64')\n    scales = np.arange(ndims, dtype='float64') + 1.0\n\n    # The objective function and the gradient.\n    def quadratic(x):\n      value = tf.reduce_sum(scales * (x - minimum) ** 2)\n      return value, tf.gradients(value, x)[0]\n\n    start = np.arange(ndims, 0, -1, dtype='float64')\n    optim_results = tfp.optimizer.lbfgs_minimize(\n        quadratic, initial_position=start, num_correction_pairs=10,\n        tolerance=1e-8)\n\n    with tf.Session() as session:\n      results = session.run(optim_results)\n      # Check that the search converged\n      assert(results.converged)\n      # Check that the argmin is close to the actual value.\n      np.testing.assert_allclose(results.position, minimum)\n  ```\n\n  ### References:\n\n  [1] Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series\n      in Operations Research. pp 176-180. 2006\n\n  http://pages.mtu.edu/~struther/Courses/OLD/Sp2013/5630/Jorge_Nocedal_Numerical_optimization_267490.pdf\n\n  Args:\n    value_and_gradients_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a tuple of `Tensor`s of real dtype containing\n      the value of the function and its gradient at that point. The function\n      to be minimized. The input is of shape `[..., n]`, where `n` is the size\n      of the domain of input points, and all others are batching dimensions.\n      The first component of the return value is a real `Tensor` of matching\n      shape `[...]`. The second component (the gradient) is also of shape\n      `[..., n]` like the input value to the function.\n    initial_position: Real `Tensor` of shape `[..., n]`. The starting point, or\n      points when using batching dimensions, of the search procedure. At these\n      points the function value and the gradient norm should be finite.\n    num_correction_pairs: Positive integer. Specifies the maximum number of\n      (position_delta, gradient_delta) correction pairs to keep as implicit\n      approximation of the Hessian matrix.\n    tolerance: Scalar `Tensor` of real dtype. Specifies the gradient tolerance\n      for the procedure. If the supremum norm of the gradient vector is below\n      this number, the algorithm is stopped.\n    x_tolerance: Scalar `Tensor` of real dtype. If the absolute change in the\n      position between one iteration and the next is smaller than this number,\n      the algorithm is stopped.\n    f_relative_tolerance: Scalar `Tensor` of real dtype. If the relative change\n      in the objective value between one iteration and the next is smaller\n      than this value, the algorithm is stopped.\n    initial_inverse_hessian_estimate: None. Option currently not supported.\n    max_iterations: Scalar positive int32 `Tensor`. The maximum number of\n      iterations for L-BFGS updates.\n    parallel_iterations: Positive integer. The number of iterations allowed to\n      run in parallel.\n    stopping_condition: (Optional) A Python function that takes as input two\n      Boolean tensors of shape `[...]`, and returns a Boolean scalar tensor.\n      The input tensors are `converged` and `failed`, indicating the current\n      status of each respective batch member; the return value states whether\n      the algorithm should stop. The default is tfp.optimizer.converged_all\n      which only stops when all batch members have either converged or failed.\n      An alternative is tfp.optimizer.converged_any which stops as soon as one\n      batch member has converged, or when all have failed.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'minimize' is used.\n\n  Returns:\n    optimizer_results: A namedtuple containing the following items:\n      converged: Scalar boolean tensor indicating whether the minimum was\n        found within tolerance.\n      failed:  Scalar boolean tensor indicating whether a line search\n        step failed to find a suitable step size satisfying Wolfe\n        conditions. In the absence of any constraints on the\n        number of objective evaluations permitted, this value will\n        be the complement of `converged`. However, if there is\n        a constraint and the search stopped due to available\n        evaluations being exhausted, both `failed` and `converged`\n        will be simultaneously False.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A tensor containing the last argument value found\n        during the search. If the search converged, then\n        this value is the argmin of the objective function.\n      objective_value: A tensor containing the value of the objective\n        function at the `position`. If the search converged, then this is\n        the (local) minimum of the objective function.\n      objective_gradient: A tensor containing the gradient of the objective\n        function at the `position`. If the search converged the\n        max-norm of this tensor should be below the tolerance.\n      position_deltas: A tensor encoding information about the latest\n        changes in `position` during the algorithm execution.\n      gradient_deltas: A tensor encoding information about the latest\n        changes in `objective_gradient` during the algorithm execution.\n  \"\"\"\n  if initial_inverse_hessian_estimate is not None:\n    raise NotImplementedError(\n        'Support of initial_inverse_hessian_estimate arg not yet implemented')\n\n  if stopping_condition is None:\n    stopping_condition = bfgs_utils.converged_all\n\n  with tf.compat.v1.name_scope(name, 'minimize', [initial_position, tolerance]):\n    initial_position = tf.convert_to_tensor(\n        value=initial_position, name='initial_position')\n    dtype = initial_position.dtype.base_dtype\n    tolerance = tf.convert_to_tensor(\n        value=tolerance, dtype=dtype, name='grad_tolerance')\n    f_relative_tolerance = tf.convert_to_tensor(\n        value=f_relative_tolerance, dtype=dtype, name='f_relative_tolerance')\n    x_tolerance = tf.convert_to_tensor(\n        value=x_tolerance, dtype=dtype, name='x_tolerance')\n    max_iterations = tf.convert_to_tensor(\n        value=max_iterations, name='max_iterations')\n\n    # The `state` here is a `LBfgsOptimizerResults` tuple with values for the\n    # current state of the algorithm computation.\n    def _cond(state):\n      \"\"\"Continue if iterations remain and stopping condition is not met.\"\"\"\n      return ((state.num_iterations < max_iterations) &\n              tf.logical_not(stopping_condition(state.converged, state.failed)))\n\n    def _body(current_state):\n      \"\"\"Main optimization loop.\"\"\"\n      search_direction = _get_search_direction(current_state)\n\n      # TODO(b/120134934): Check if the derivative at the start point is not\n      # negative, if so then reset position/gradient deltas and recompute\n      # search direction.\n\n      next_state = bfgs_utils.line_search_step(\n          current_state,\n          value_and_gradients_function, search_direction,\n          tolerance, f_relative_tolerance, x_tolerance, stopping_condition)\n\n      # If not failed or converged, update the Hessian estimate.\n      should_update = ~(next_state.converged | next_state.failed)\n      state_after_inv_hessian_update = bfgs_utils.update_fields(\n          next_state,\n          position_deltas=_queue_push(\n              current_state.position_deltas, should_update,\n              next_state.position - current_state.position),\n          gradient_deltas=_queue_push(\n              current_state.gradient_deltas, should_update,\n              next_state.objective_gradient - current_state.objective_gradient))\n      return [state_after_inv_hessian_update]\n\n    initial_state = _get_initial_state(value_and_gradients_function,\n                                       initial_position,\n                                       num_correction_pairs,\n                                       tolerance)\n    return tf.while_loop(\n        cond=_cond,\n        body=_body,\n        loop_vars=[initial_state],\n        parallel_iterations=parallel_iterations)[0]", "language": "python", "code": "def minimize(value_and_gradients_function,\n             initial_position,\n             num_correction_pairs=10,\n             tolerance=1e-8,\n             x_tolerance=0,\n             f_relative_tolerance=0,\n             initial_inverse_hessian_estimate=None,\n             max_iterations=50,\n             parallel_iterations=1,\n             stopping_condition=None,\n             name=None):\n  \"\"\"Applies the L-BFGS algorithm to minimize a differentiable function.\n\n  Performs unconstrained minimization of a differentiable function using the\n  L-BFGS scheme. See [Nocedal and Wright(2006)][1] for details of the algorithm.\n\n  ### Usage:\n\n  The following example demonstrates the L-BFGS optimizer attempting to find the\n  minimum for a simple high-dimensional quadratic objective function.\n\n  ```python\n    # A high-dimensional quadratic bowl.\n    ndims = 60\n    minimum = np.ones([ndims], dtype='float64')\n    scales = np.arange(ndims, dtype='float64') + 1.0\n\n    # The objective function and the gradient.\n    def quadratic(x):\n      value = tf.reduce_sum(scales * (x - minimum) ** 2)\n      return value, tf.gradients(value, x)[0]\n\n    start = np.arange(ndims, 0, -1, dtype='float64')\n    optim_results = tfp.optimizer.lbfgs_minimize(\n        quadratic, initial_position=start, num_correction_pairs=10,\n        tolerance=1e-8)\n\n    with tf.Session() as session:\n      results = session.run(optim_results)\n      # Check that the search converged\n      assert(results.converged)\n      # Check that the argmin is close to the actual value.\n      np.testing.assert_allclose(results.position, minimum)\n  ```\n\n  ### References:\n\n  [1] Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series\n      in Operations Research. pp 176-180. 2006\n\n  http://pages.mtu.edu/~struther/Courses/OLD/Sp2013/5630/Jorge_Nocedal_Numerical_optimization_267490.pdf\n\n  Args:\n    value_and_gradients_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a tuple of `Tensor`s of real dtype containing\n      the value of the function and its gradient at that point. The function\n      to be minimized. The input is of shape `[..., n]`, where `n` is the size\n      of the domain of input points, and all others are batching dimensions.\n      The first component of the return value is a real `Tensor` of matching\n      shape `[...]`. The second component (the gradient) is also of shape\n      `[..., n]` like the input value to the function.\n    initial_position: Real `Tensor` of shape `[..., n]`. The starting point, or\n      points when using batching dimensions, of the search procedure. At these\n      points the function value and the gradient norm should be finite.\n    num_correction_pairs: Positive integer. Specifies the maximum number of\n      (position_delta, gradient_delta) correction pairs to keep as implicit\n      approximation of the Hessian matrix.\n    tolerance: Scalar `Tensor` of real dtype. Specifies the gradient tolerance\n      for the procedure. If the supremum norm of the gradient vector is below\n      this number, the algorithm is stopped.\n    x_tolerance: Scalar `Tensor` of real dtype. If the absolute change in the\n      position between one iteration and the next is smaller than this number,\n      the algorithm is stopped.\n    f_relative_tolerance: Scalar `Tensor` of real dtype. If the relative change\n      in the objective value between one iteration and the next is smaller\n      than this value, the algorithm is stopped.\n    initial_inverse_hessian_estimate: None. Option currently not supported.\n    max_iterations: Scalar positive int32 `Tensor`. The maximum number of\n      iterations for L-BFGS updates.\n    parallel_iterations: Positive integer. The number of iterations allowed to\n      run in parallel.\n    stopping_condition: (Optional) A Python function that takes as input two\n      Boolean tensors of shape `[...]`, and returns a Boolean scalar tensor.\n      The input tensors are `converged` and `failed`, indicating the current\n      status of each respective batch member; the return value states whether\n      the algorithm should stop. The default is tfp.optimizer.converged_all\n      which only stops when all batch members have either converged or failed.\n      An alternative is tfp.optimizer.converged_any which stops as soon as one\n      batch member has converged, or when all have failed.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'minimize' is used.\n\n  Returns:\n    optimizer_results: A namedtuple containing the following items:\n      converged: Scalar boolean tensor indicating whether the minimum was\n        found within tolerance.\n      failed:  Scalar boolean tensor indicating whether a line search\n        step failed to find a suitable step size satisfying Wolfe\n        conditions. In the absence of any constraints on the\n        number of objective evaluations permitted, this value will\n        be the complement of `converged`. However, if there is\n        a constraint and the search stopped due to available\n        evaluations being exhausted, both `failed` and `converged`\n        will be simultaneously False.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A tensor containing the last argument value found\n        during the search. If the search converged, then\n        this value is the argmin of the objective function.\n      objective_value: A tensor containing the value of the objective\n        function at the `position`. If the search converged, then this is\n        the (local) minimum of the objective function.\n      objective_gradient: A tensor containing the gradient of the objective\n        function at the `position`. If the search converged the\n        max-norm of this tensor should be below the tolerance.\n      position_deltas: A tensor encoding information about the latest\n        changes in `position` during the algorithm execution.\n      gradient_deltas: A tensor encoding information about the latest\n        changes in `objective_gradient` during the algorithm execution.\n  \"\"\"\n  if initial_inverse_hessian_estimate is not None:\n    raise NotImplementedError(\n        'Support of initial_inverse_hessian_estimate arg not yet implemented')\n\n  if stopping_condition is None:\n    stopping_condition = bfgs_utils.converged_all\n\n  with tf.compat.v1.name_scope(name, 'minimize', [initial_position, tolerance]):\n    initial_position = tf.convert_to_tensor(\n        value=initial_position, name='initial_position')\n    dtype = initial_position.dtype.base_dtype\n    tolerance = tf.convert_to_tensor(\n        value=tolerance, dtype=dtype, name='grad_tolerance')\n    f_relative_tolerance = tf.convert_to_tensor(\n        value=f_relative_tolerance, dtype=dtype, name='f_relative_tolerance')\n    x_tolerance = tf.convert_to_tensor(\n        value=x_tolerance, dtype=dtype, name='x_tolerance')\n    max_iterations = tf.convert_to_tensor(\n        value=max_iterations, name='max_iterations')\n\n    # The `state` here is a `LBfgsOptimizerResults` tuple with values for the\n    # current state of the algorithm computation.\n    def _cond(state):\n      \"\"\"Continue if iterations remain and stopping condition is not met.\"\"\"\n      return ((state.num_iterations < max_iterations) &\n              tf.logical_not(stopping_condition(state.converged, state.failed)))\n\n    def _body(current_state):\n      \"\"\"Main optimization loop.\"\"\"\n      search_direction = _get_search_direction(current_state)\n\n      # TODO(b/120134934): Check if the derivative at the start point is not\n      # negative, if so then reset position/gradient deltas and recompute\n      # search direction.\n\n      next_state = bfgs_utils.line_search_step(\n          current_state,\n          value_and_gradients_function, search_direction,\n          tolerance, f_relative_tolerance, x_tolerance, stopping_condition)\n\n      # If not failed or converged, update the Hessian estimate.\n      should_update = ~(next_state.converged | next_state.failed)\n      state_after_inv_hessian_update = bfgs_utils.update_fields(\n          next_state,\n          position_deltas=_queue_push(\n              current_state.position_deltas, should_update,\n              next_state.position - current_state.position),\n          gradient_deltas=_queue_push(\n              current_state.gradient_deltas, should_update,\n              next_state.objective_gradient - current_state.objective_gradient))\n      return [state_after_inv_hessian_update]\n\n    initial_state = _get_initial_state(value_and_gradients_function,\n                                       initial_position,\n                                       num_correction_pairs,\n                                       tolerance)\n    return tf.while_loop(\n        cond=_cond,\n        body=_body,\n        loop_vars=[initial_state],\n        parallel_iterations=parallel_iterations)[0]", "code_tokens": ["def", "minimize", "(", "value_and_gradients_function", ",", "initial_position", ",", "num_correction_pairs", "=", "10", ",", "tolerance", "=", "1e-8", ",", "x_tolerance", "=", "0", ",", "f_relative_tolerance", "=", "0", ",", "initial_inverse_hessian_estimate", "=", "None", ",", "max_iterations", "=", "50", ",", "parallel_iterations", "=", "1", ",", "stopping_condition", "=", "None", ",", "name", "=", "None", ")", ":", "if", "initial_inverse_hessian_estimate", "is", "not", "None", ":", "raise", "NotImplementedError", "(", "'Support of initial_inverse_hessian_estimate arg not yet implemented'", ")", "if", "stopping_condition", "is", "None", ":", "stopping_condition", "=", "bfgs_utils", ".", "converged_all", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'minimize'", ",", "[", "initial_position", ",", "tolerance", "]", ")", ":", "initial_position", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "initial_position", ",", "name", "=", "'initial_position'", ")", "dtype", "=", "initial_position", ".", "dtype", ".", "base_dtype", "tolerance", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "tolerance", ",", "dtype", "=", "dtype", ",", "name", "=", "'grad_tolerance'", ")", "f_relative_tolerance", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "f_relative_tolerance", ",", "dtype", "=", "dtype", ",", "name", "=", "'f_relative_tolerance'", ")", "x_tolerance", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x_tolerance", ",", "dtype", "=", "dtype", ",", "name", "=", "'x_tolerance'", ")", "max_iterations", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "max_iterations", ",", "name", "=", "'max_iterations'", ")", "# The `state` here is a `LBfgsOptimizerResults` tuple with values for the", "# current state of the algorithm computation.", "def", "_cond", "(", "state", ")", ":", "\"\"\"Continue if iterations remain and stopping condition is not met.\"\"\"", "return", "(", "(", "state", ".", "num_iterations", "<", "max_iterations", ")", "&", "tf", ".", "logical_not", "(", "stopping_condition", "(", "state", ".", "converged", ",", "state", ".", "failed", ")", ")", ")", "def", "_body", "(", "current_state", ")", ":", "\"\"\"Main optimization loop.\"\"\"", "search_direction", "=", "_get_search_direction", "(", "current_state", ")", "# TODO(b/120134934): Check if the derivative at the start point is not", "# negative, if so then reset position/gradient deltas and recompute", "# search direction.", "next_state", "=", "bfgs_utils", ".", "line_search_step", "(", "current_state", ",", "value_and_gradients_function", ",", "search_direction", ",", "tolerance", ",", "f_relative_tolerance", ",", "x_tolerance", ",", "stopping_condition", ")", "# If not failed or converged, update the Hessian estimate.", "should_update", "=", "~", "(", "next_state", ".", "converged", "|", "next_state", ".", "failed", ")", "state_after_inv_hessian_update", "=", "bfgs_utils", ".", "update_fields", "(", "next_state", ",", "position_deltas", "=", "_queue_push", "(", "current_state", ".", "position_deltas", ",", "should_update", ",", "next_state", ".", "position", "-", "current_state", ".", "position", ")", ",", "gradient_deltas", "=", "_queue_push", "(", "current_state", ".", "gradient_deltas", ",", "should_update", ",", "next_state", ".", "objective_gradient", "-", "current_state", ".", "objective_gradient", ")", ")", "return", "[", "state_after_inv_hessian_update", "]", "initial_state", "=", "_get_initial_state", "(", "value_and_gradients_function", ",", "initial_position", ",", "num_correction_pairs", ",", "tolerance", ")", "return", "tf", ".", "while_loop", "(", "cond", "=", "_cond", ",", "body", "=", "_body", ",", "loop_vars", "=", "[", "initial_state", "]", ",", "parallel_iterations", "=", "parallel_iterations", ")", "[", "0", "]"], "docstring": "Applies the L-BFGS algorithm to minimize a differentiable function.\n\n  Performs unconstrained minimization of a differentiable function using the\n  L-BFGS scheme. See [Nocedal and Wright(2006)][1] for details of the algorithm.\n\n  ### Usage:\n\n  The following example demonstrates the L-BFGS optimizer attempting to find the\n  minimum for a simple high-dimensional quadratic objective function.\n\n  ```python\n    # A high-dimensional quadratic bowl.\n    ndims = 60\n    minimum = np.ones([ndims], dtype='float64')\n    scales = np.arange(ndims, dtype='float64') + 1.0\n\n    # The objective function and the gradient.\n    def quadratic(x):\n      value = tf.reduce_sum(scales * (x - minimum) ** 2)\n      return value, tf.gradients(value, x)[0]\n\n    start = np.arange(ndims, 0, -1, dtype='float64')\n    optim_results = tfp.optimizer.lbfgs_minimize(\n        quadratic, initial_position=start, num_correction_pairs=10,\n        tolerance=1e-8)\n\n    with tf.Session() as session:\n      results = session.run(optim_results)\n      # Check that the search converged\n      assert(results.converged)\n      # Check that the argmin is close to the actual value.\n      np.testing.assert_allclose(results.position, minimum)\n  ```\n\n  ### References:\n\n  [1] Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series\n      in Operations Research. pp 176-180. 2006\n\n  http://pages.mtu.edu/~struther/Courses/OLD/Sp2013/5630/Jorge_Nocedal_Numerical_optimization_267490.pdf\n\n  Args:\n    value_and_gradients_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a tuple of `Tensor`s of real dtype containing\n      the value of the function and its gradient at that point. The function\n      to be minimized. The input is of shape `[..., n]`, where `n` is the size\n      of the domain of input points, and all others are batching dimensions.\n      The first component of the return value is a real `Tensor` of matching\n      shape `[...]`. The second component (the gradient) is also of shape\n      `[..., n]` like the input value to the function.\n    initial_position: Real `Tensor` of shape `[..., n]`. The starting point, or\n      points when using batching dimensions, of the search procedure. At these\n      points the function value and the gradient norm should be finite.\n    num_correction_pairs: Positive integer. Specifies the maximum number of\n      (position_delta, gradient_delta) correction pairs to keep as implicit\n      approximation of the Hessian matrix.\n    tolerance: Scalar `Tensor` of real dtype. Specifies the gradient tolerance\n      for the procedure. If the supremum norm of the gradient vector is below\n      this number, the algorithm is stopped.\n    x_tolerance: Scalar `Tensor` of real dtype. If the absolute change in the\n      position between one iteration and the next is smaller than this number,\n      the algorithm is stopped.\n    f_relative_tolerance: Scalar `Tensor` of real dtype. If the relative change\n      in the objective value between one iteration and the next is smaller\n      than this value, the algorithm is stopped.\n    initial_inverse_hessian_estimate: None. Option currently not supported.\n    max_iterations: Scalar positive int32 `Tensor`. The maximum number of\n      iterations for L-BFGS updates.\n    parallel_iterations: Positive integer. The number of iterations allowed to\n      run in parallel.\n    stopping_condition: (Optional) A Python function that takes as input two\n      Boolean tensors of shape `[...]`, and returns a Boolean scalar tensor.\n      The input tensors are `converged` and `failed`, indicating the current\n      status of each respective batch member; the return value states whether\n      the algorithm should stop. The default is tfp.optimizer.converged_all\n      which only stops when all batch members have either converged or failed.\n      An alternative is tfp.optimizer.converged_any which stops as soon as one\n      batch member has converged, or when all have failed.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'minimize' is used.\n\n  Returns:\n    optimizer_results: A namedtuple containing the following items:\n      converged: Scalar boolean tensor indicating whether the minimum was\n        found within tolerance.\n      failed:  Scalar boolean tensor indicating whether a line search\n        step failed to find a suitable step size satisfying Wolfe\n        conditions. In the absence of any constraints on the\n        number of objective evaluations permitted, this value will\n        be the complement of `converged`. However, if there is\n        a constraint and the search stopped due to available\n        evaluations being exhausted, both `failed` and `converged`\n        will be simultaneously False.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A tensor containing the last argument value found\n        during the search. If the search converged, then\n        this value is the argmin of the objective function.\n      objective_value: A tensor containing the value of the objective\n        function at the `position`. If the search converged, then this is\n        the (local) minimum of the objective function.\n      objective_gradient: A tensor containing the gradient of the objective\n        function at the `position`. If the search converged the\n        max-norm of this tensor should be below the tolerance.\n      position_deltas: A tensor encoding information about the latest\n        changes in `position` during the algorithm execution.\n      gradient_deltas: A tensor encoding information about the latest\n        changes in `objective_gradient` during the algorithm execution.", "docstring_tokens": ["Applies", "the", "L", "-", "BFGS", "algorithm", "to", "minimize", "a", "differentiable", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/lbfgs.py#L80-L260", "partition": "test", "index": 1072, "time": "2018-12-05 09:05:25"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/hidden_markov_model.py", "func_name": "_log_matrix_vector", "original_string": "def _log_matrix_vector(ms, vs):\n  \"\"\"Multiply tensor of matrices by vectors assuming values stored are logs.\"\"\"\n\n  return tf.reduce_logsumexp(input_tensor=ms + vs[..., tf.newaxis, :], axis=-1)", "language": "python", "code": "def _log_matrix_vector(ms, vs):\n  \"\"\"Multiply tensor of matrices by vectors assuming values stored are logs.\"\"\"\n\n  return tf.reduce_logsumexp(input_tensor=ms + vs[..., tf.newaxis, :], axis=-1)", "code_tokens": ["def", "_log_matrix_vector", "(", "ms", ",", "vs", ")", ":", "return", "tf", ".", "reduce_logsumexp", "(", "input_tensor", "=", "ms", "+", "vs", "[", "...", ",", "tf", ".", "newaxis", ",", ":", "]", ",", "axis", "=", "-", "1", ")"], "docstring": "Multiply tensor of matrices by vectors assuming values stored are logs.", "docstring_tokens": ["Multiply", "tensor", "of", "matrices", "by", "vectors", "assuming", "values", "stored", "are", "logs", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/hidden_markov_model.py#L914-L917", "partition": "test", "index": 687, "time": "2018-12-20 10:08:39"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/hidden_markov_model.py", "func_name": "HiddenMarkovModel.posterior_marginals", "original_string": "def posterior_marginals(self, observations, name=None):\n    \"\"\"Compute marginal posterior distribution for each state.\n\n    This function computes, for each time step, the marginal\n    conditional probability that the hidden Markov model was in\n    each possible state given the observations that were made\n    at each time step.\n    So if the hidden states are `z[0],...,z[num_steps - 1]` and\n    the observations are `x[0], ..., x[num_steps - 1]`, then\n    this function computes `P(z[i] | x[0], ..., x[num_steps - 1])`\n    for all `i` from `0` to `num_steps - 1`.\n\n    This operation is sometimes called smoothing. It uses a form\n    of the forward-backward algorithm.\n\n    Note: the behavior of this function is undefined if the\n    `observations` argument represents impossible observations\n    from the model.\n\n    Args:\n      observations: A tensor representing a batch of observations\n        made on the hidden Markov model.  The rightmost dimension of this tensor\n        gives the steps in a sequence of observations from a single sample from\n        the hidden Markov model. The size of this dimension should match the\n        `num_steps` parameter of the hidden Markov model object. The other\n        dimensions are the dimensions of the batch and these are broadcast with\n        the hidden Markov model's parameters.\n      name: Python `str` name prefixed to Ops created by this class.\n        Default value: \"HiddenMarkovModel\".\n\n    Returns:\n      posterior_marginal: A `Categorical` distribution object representing the\n        marginal probability of the hidden Markov model being in each state at\n        each step. The rightmost dimension of the `Categorical` distributions\n        batch will equal the `num_steps` parameter providing one marginal\n        distribution for each step. The other dimensions are the dimensions\n        corresponding to the batch of observations.\n\n    Raises:\n      ValueError: if rightmost dimension of `observations` does not\n      have size `num_steps`.\n    \"\"\"\n\n    with tf.name_scope(name or \"posterior_marginals\"):\n      with tf.control_dependencies(self._runtime_assertions):\n        observation_tensor_shape = tf.shape(input=observations)\n\n        with self._observation_shape_preconditions(observation_tensor_shape):\n          observation_batch_shape = observation_tensor_shape[\n              :-1 - self._underlying_event_rank]\n          observation_event_shape = observation_tensor_shape[\n              -1 - self._underlying_event_rank:]\n\n          batch_shape = tf.broadcast_dynamic_shape(observation_batch_shape,\n                                                   self.batch_shape_tensor())\n          log_init = tf.broadcast_to(self._log_init,\n                                     tf.concat([batch_shape,\n                                                [self._num_states]],\n                                               axis=0))\n          log_transition = self._log_trans\n\n          observations = tf.broadcast_to(observations,\n                                         tf.concat([batch_shape,\n                                                    observation_event_shape],\n                                                   axis=0))\n          observation_rank = tf.rank(observations)\n          underlying_event_rank = self._underlying_event_rank\n          observations = distribution_util.move_dimension(\n              observations, observation_rank - underlying_event_rank - 1, 0)\n          observations = tf.expand_dims(\n              observations,\n              observation_rank - underlying_event_rank)\n          observation_log_probs = self._observation_distribution.log_prob(\n              observations)\n\n          log_adjoint_prob = tf.zeros_like(log_init)\n\n          def forward_step(log_previous_step, log_prob_observation):\n            return _log_vector_matrix(log_previous_step,\n                                      log_transition) + log_prob_observation\n\n          log_prob = log_init + observation_log_probs[0]\n\n          forward_log_probs = tf.scan(forward_step, observation_log_probs[1:],\n                                      initializer=log_prob,\n                                      name=\"forward_log_probs\")\n\n          forward_log_probs = tf.concat([[log_prob], forward_log_probs], axis=0)\n\n          def backward_step(log_previous_step, log_prob_observation):\n            return _log_matrix_vector(log_transition,\n                                      log_prob_observation + log_previous_step)\n\n          backward_log_adjoint_probs = tf.scan(\n              backward_step,\n              observation_log_probs[1:],\n              initializer=log_adjoint_prob,\n              reverse=True,\n              name=\"backward_log_adjoint_probs\")\n\n          total_log_prob = tf.reduce_logsumexp(\n              input_tensor=forward_log_probs[-1], axis=-1)\n\n          backward_log_adjoint_probs = tf.concat([backward_log_adjoint_probs,\n                                                  [log_adjoint_prob]], axis=0)\n\n          log_likelihoods = forward_log_probs + backward_log_adjoint_probs\n\n          marginal_log_probs = distribution_util.move_dimension(\n              log_likelihoods - total_log_prob[..., tf.newaxis], 0, -2)\n\n          return categorical.Categorical(logits=marginal_log_probs)", "language": "python", "code": "def posterior_marginals(self, observations, name=None):\n    \"\"\"Compute marginal posterior distribution for each state.\n\n    This function computes, for each time step, the marginal\n    conditional probability that the hidden Markov model was in\n    each possible state given the observations that were made\n    at each time step.\n    So if the hidden states are `z[0],...,z[num_steps - 1]` and\n    the observations are `x[0], ..., x[num_steps - 1]`, then\n    this function computes `P(z[i] | x[0], ..., x[num_steps - 1])`\n    for all `i` from `0` to `num_steps - 1`.\n\n    This operation is sometimes called smoothing. It uses a form\n    of the forward-backward algorithm.\n\n    Note: the behavior of this function is undefined if the\n    `observations` argument represents impossible observations\n    from the model.\n\n    Args:\n      observations: A tensor representing a batch of observations\n        made on the hidden Markov model.  The rightmost dimension of this tensor\n        gives the steps in a sequence of observations from a single sample from\n        the hidden Markov model. The size of this dimension should match the\n        `num_steps` parameter of the hidden Markov model object. The other\n        dimensions are the dimensions of the batch and these are broadcast with\n        the hidden Markov model's parameters.\n      name: Python `str` name prefixed to Ops created by this class.\n        Default value: \"HiddenMarkovModel\".\n\n    Returns:\n      posterior_marginal: A `Categorical` distribution object representing the\n        marginal probability of the hidden Markov model being in each state at\n        each step. The rightmost dimension of the `Categorical` distributions\n        batch will equal the `num_steps` parameter providing one marginal\n        distribution for each step. The other dimensions are the dimensions\n        corresponding to the batch of observations.\n\n    Raises:\n      ValueError: if rightmost dimension of `observations` does not\n      have size `num_steps`.\n    \"\"\"\n\n    with tf.name_scope(name or \"posterior_marginals\"):\n      with tf.control_dependencies(self._runtime_assertions):\n        observation_tensor_shape = tf.shape(input=observations)\n\n        with self._observation_shape_preconditions(observation_tensor_shape):\n          observation_batch_shape = observation_tensor_shape[\n              :-1 - self._underlying_event_rank]\n          observation_event_shape = observation_tensor_shape[\n              -1 - self._underlying_event_rank:]\n\n          batch_shape = tf.broadcast_dynamic_shape(observation_batch_shape,\n                                                   self.batch_shape_tensor())\n          log_init = tf.broadcast_to(self._log_init,\n                                     tf.concat([batch_shape,\n                                                [self._num_states]],\n                                               axis=0))\n          log_transition = self._log_trans\n\n          observations = tf.broadcast_to(observations,\n                                         tf.concat([batch_shape,\n                                                    observation_event_shape],\n                                                   axis=0))\n          observation_rank = tf.rank(observations)\n          underlying_event_rank = self._underlying_event_rank\n          observations = distribution_util.move_dimension(\n              observations, observation_rank - underlying_event_rank - 1, 0)\n          observations = tf.expand_dims(\n              observations,\n              observation_rank - underlying_event_rank)\n          observation_log_probs = self._observation_distribution.log_prob(\n              observations)\n\n          log_adjoint_prob = tf.zeros_like(log_init)\n\n          def forward_step(log_previous_step, log_prob_observation):\n            return _log_vector_matrix(log_previous_step,\n                                      log_transition) + log_prob_observation\n\n          log_prob = log_init + observation_log_probs[0]\n\n          forward_log_probs = tf.scan(forward_step, observation_log_probs[1:],\n                                      initializer=log_prob,\n                                      name=\"forward_log_probs\")\n\n          forward_log_probs = tf.concat([[log_prob], forward_log_probs], axis=0)\n\n          def backward_step(log_previous_step, log_prob_observation):\n            return _log_matrix_vector(log_transition,\n                                      log_prob_observation + log_previous_step)\n\n          backward_log_adjoint_probs = tf.scan(\n              backward_step,\n              observation_log_probs[1:],\n              initializer=log_adjoint_prob,\n              reverse=True,\n              name=\"backward_log_adjoint_probs\")\n\n          total_log_prob = tf.reduce_logsumexp(\n              input_tensor=forward_log_probs[-1], axis=-1)\n\n          backward_log_adjoint_probs = tf.concat([backward_log_adjoint_probs,\n                                                  [log_adjoint_prob]], axis=0)\n\n          log_likelihoods = forward_log_probs + backward_log_adjoint_probs\n\n          marginal_log_probs = distribution_util.move_dimension(\n              log_likelihoods - total_log_prob[..., tf.newaxis], 0, -2)\n\n          return categorical.Categorical(logits=marginal_log_probs)", "code_tokens": ["def", "posterior_marginals", "(", "self", ",", "observations", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "\"posterior_marginals\"", ")", ":", "with", "tf", ".", "control_dependencies", "(", "self", ".", "_runtime_assertions", ")", ":", "observation_tensor_shape", "=", "tf", ".", "shape", "(", "input", "=", "observations", ")", "with", "self", ".", "_observation_shape_preconditions", "(", "observation_tensor_shape", ")", ":", "observation_batch_shape", "=", "observation_tensor_shape", "[", ":", "-", "1", "-", "self", ".", "_underlying_event_rank", "]", "observation_event_shape", "=", "observation_tensor_shape", "[", "-", "1", "-", "self", ".", "_underlying_event_rank", ":", "]", "batch_shape", "=", "tf", ".", "broadcast_dynamic_shape", "(", "observation_batch_shape", ",", "self", ".", "batch_shape_tensor", "(", ")", ")", "log_init", "=", "tf", ".", "broadcast_to", "(", "self", ".", "_log_init", ",", "tf", ".", "concat", "(", "[", "batch_shape", ",", "[", "self", ".", "_num_states", "]", "]", ",", "axis", "=", "0", ")", ")", "log_transition", "=", "self", ".", "_log_trans", "observations", "=", "tf", ".", "broadcast_to", "(", "observations", ",", "tf", ".", "concat", "(", "[", "batch_shape", ",", "observation_event_shape", "]", ",", "axis", "=", "0", ")", ")", "observation_rank", "=", "tf", ".", "rank", "(", "observations", ")", "underlying_event_rank", "=", "self", ".", "_underlying_event_rank", "observations", "=", "distribution_util", ".", "move_dimension", "(", "observations", ",", "observation_rank", "-", "underlying_event_rank", "-", "1", ",", "0", ")", "observations", "=", "tf", ".", "expand_dims", "(", "observations", ",", "observation_rank", "-", "underlying_event_rank", ")", "observation_log_probs", "=", "self", ".", "_observation_distribution", ".", "log_prob", "(", "observations", ")", "log_adjoint_prob", "=", "tf", ".", "zeros_like", "(", "log_init", ")", "def", "forward_step", "(", "log_previous_step", ",", "log_prob_observation", ")", ":", "return", "_log_vector_matrix", "(", "log_previous_step", ",", "log_transition", ")", "+", "log_prob_observation", "log_prob", "=", "log_init", "+", "observation_log_probs", "[", "0", "]", "forward_log_probs", "=", "tf", ".", "scan", "(", "forward_step", ",", "observation_log_probs", "[", "1", ":", "]", ",", "initializer", "=", "log_prob", ",", "name", "=", "\"forward_log_probs\"", ")", "forward_log_probs", "=", "tf", ".", "concat", "(", "[", "[", "log_prob", "]", ",", "forward_log_probs", "]", ",", "axis", "=", "0", ")", "def", "backward_step", "(", "log_previous_step", ",", "log_prob_observation", ")", ":", "return", "_log_matrix_vector", "(", "log_transition", ",", "log_prob_observation", "+", "log_previous_step", ")", "backward_log_adjoint_probs", "=", "tf", ".", "scan", "(", "backward_step", ",", "observation_log_probs", "[", "1", ":", "]", ",", "initializer", "=", "log_adjoint_prob", ",", "reverse", "=", "True", ",", "name", "=", "\"backward_log_adjoint_probs\"", ")", "total_log_prob", "=", "tf", ".", "reduce_logsumexp", "(", "input_tensor", "=", "forward_log_probs", "[", "-", "1", "]", ",", "axis", "=", "-", "1", ")", "backward_log_adjoint_probs", "=", "tf", ".", "concat", "(", "[", "backward_log_adjoint_probs", ",", "[", "log_adjoint_prob", "]", "]", ",", "axis", "=", "0", ")", "log_likelihoods", "=", "forward_log_probs", "+", "backward_log_adjoint_probs", "marginal_log_probs", "=", "distribution_util", ".", "move_dimension", "(", "log_likelihoods", "-", "total_log_prob", "[", "...", ",", "tf", ".", "newaxis", "]", ",", "0", ",", "-", "2", ")", "return", "categorical", ".", "Categorical", "(", "logits", "=", "marginal_log_probs", ")"], "docstring": "Compute marginal posterior distribution for each state.\n\n    This function computes, for each time step, the marginal\n    conditional probability that the hidden Markov model was in\n    each possible state given the observations that were made\n    at each time step.\n    So if the hidden states are `z[0],...,z[num_steps - 1]` and\n    the observations are `x[0], ..., x[num_steps - 1]`, then\n    this function computes `P(z[i] | x[0], ..., x[num_steps - 1])`\n    for all `i` from `0` to `num_steps - 1`.\n\n    This operation is sometimes called smoothing. It uses a form\n    of the forward-backward algorithm.\n\n    Note: the behavior of this function is undefined if the\n    `observations` argument represents impossible observations\n    from the model.\n\n    Args:\n      observations: A tensor representing a batch of observations\n        made on the hidden Markov model.  The rightmost dimension of this tensor\n        gives the steps in a sequence of observations from a single sample from\n        the hidden Markov model. The size of this dimension should match the\n        `num_steps` parameter of the hidden Markov model object. The other\n        dimensions are the dimensions of the batch and these are broadcast with\n        the hidden Markov model's parameters.\n      name: Python `str` name prefixed to Ops created by this class.\n        Default value: \"HiddenMarkovModel\".\n\n    Returns:\n      posterior_marginal: A `Categorical` distribution object representing the\n        marginal probability of the hidden Markov model being in each state at\n        each step. The rightmost dimension of the `Categorical` distributions\n        batch will equal the `num_steps` parameter providing one marginal\n        distribution for each step. The other dimensions are the dimensions\n        corresponding to the batch of observations.\n\n    Raises:\n      ValueError: if rightmost dimension of `observations` does not\n      have size `num_steps`.", "docstring_tokens": ["Compute", "marginal", "posterior", "distribution", "for", "each", "state", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/hidden_markov_model.py#L623-L734", "partition": "test", "index": 691, "time": "2018-12-20 10:08:39"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/bijector.py", "func_name": "_Mapping._deep_tuple", "original_string": "def _deep_tuple(self, x):\n    \"\"\"Converts nested `tuple`, `list`, or `dict` to nested `tuple`.\"\"\"\n    if isinstance(x, dict):\n      return self._deep_tuple(tuple(sorted(x.items())))\n    elif isinstance(x, (list, tuple)):\n      return tuple(map(self._deep_tuple, x))\n\n    return x", "language": "python", "code": "def _deep_tuple(self, x):\n    \"\"\"Converts nested `tuple`, `list`, or `dict` to nested `tuple`.\"\"\"\n    if isinstance(x, dict):\n      return self._deep_tuple(tuple(sorted(x.items())))\n    elif isinstance(x, (list, tuple)):\n      return tuple(map(self._deep_tuple, x))\n\n    return x", "code_tokens": ["def", "_deep_tuple", "(", "self", ",", "x", ")", ":", "if", "isinstance", "(", "x", ",", "dict", ")", ":", "return", "self", ".", "_deep_tuple", "(", "tuple", "(", "sorted", "(", "x", ".", "items", "(", ")", ")", ")", ")", "elif", "isinstance", "(", "x", ",", "(", "list", ",", "tuple", ")", ")", ":", "return", "tuple", "(", "map", "(", "self", ".", "_deep_tuple", ",", "x", ")", ")", "return", "x"], "docstring": "Converts nested `tuple`, `list`, or `dict` to nested `tuple`.", "docstring_tokens": ["Converts", "nested", "tuple", "list", "or", "dict", "to", "nested", "tuple", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/bijector.py#L122-L129", "partition": "test", "index": 791, "time": "2018-12-20 19:21:25"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/differential_evolution.py", "func_name": "minimize", "original_string": "def minimize(objective_function,\n             initial_population=None,\n             initial_position=None,\n             population_size=50,\n             population_stddev=1.,\n             max_iterations=100,\n             func_tolerance=0,\n             position_tolerance=1e-8,\n             differential_weight=0.5,\n             crossover_prob=0.9,\n             seed=None,\n             name=None):\n  \"\"\"Applies the Differential evolution algorithm to minimize a function.\n\n  Differential Evolution is an evolutionary optimization algorithm which works\n  on a set of candidate solutions called the population. It iteratively\n  improves the population by applying genetic operators of mutation and\n  recombination. The objective function `f` supplies the fitness of each\n  candidate. A candidate `s_1` is considered better than `s_2` if\n  `f(s_1) < f(s_2)`.\n\n  This method allows the user to either specify an initial population or a\n  single candidate solution. If a single solution is specified, a population\n  of the specified size is initialized by adding independent normal noise\n  to the candidate solution.\n\n  The implementation also supports a multi-part specification of the state. For\n  example, consider the objective function:\n\n  ```python\n  # x is a tensor of shape [n, m] while y is of shape [n].\n  def objective(x, y):\n    return tf.math.reduce_sum(x ** 2, axis=-1) + y ** 2\n  ```\n  The state in this case is specified by two input tensors `x` and `y`. To\n  apply the algorithm to this objective function, one would need to specify\n  either an initial population as a list of two tensors of shapes\n  `[population_size, k]` and `[population_size]`. The following code shows the\n  complete example:\n\n  ```python\n    population_size = 40\n    # With an initial population and a multi-part state.\n    initial_population = (tf.random.normal([population_size]),\n                          tf.random.normal([population_size]))\n    def easom_fn(x, y):\n      return -(tf.math.cos(x) * tf.math.cos(y) *\n               tf.math.exp(-(x-np.pi)**2 - (y-np.pi)**2))\n\n    optim_results = tfp.optimizers.differential_evolution_minimize(\n        easom_fn,\n        initial_population=initial_population,\n        seed=43210)\n\n    print (optim_results.converged)\n    print (optim_results.position)  # Should be (close to) [pi, pi].\n    print (optim_results.objective_value)    # Should be -1.\n\n\n    # With a single starting point\n    initial_position = (tf.constant(1.0), tf.constant(1.0))\n\n    optim_results = tfp.optimizers.differential_evolution_minimize(\n        easom_fn,\n        initial_position=initial_position,\n        population_size=40,\n        population_stddev=2.0,\n        seed=43210)\n  ```\n\n  Args:\n    objective_function: A Python callable that accepts a batch of possible\n      solutions and returns the values of the objective function at those\n      arguments as a rank 1 real `Tensor`. This specifies the function to be\n      minimized. The input to this callable may be either a single `Tensor`\n      or a Python `list` of `Tensor`s. The signature must match the format of\n      the argument `population`. (i.e. objective_function(*population) must\n      return the value of the function to be minimized).\n    initial_population: A real `Tensor` or Python list of `Tensor`s.\n      If a list, each `Tensor` must be of rank at least 1 and with a common\n      first dimension. The first dimension indexes into the candidate solutions\n      while the rest of the dimensions (if any) index into an individual\n      solution. The size of the population must be at least 4. This is a\n      requirement of the DE algorithm.\n    initial_position: A real `Tensor` of any shape. The seed solution used\n      to initialize the population of solutions. If this parameter is specified\n      then `initial_population` must not be specified.\n    population_size: A positive scalar int32 `Tensor` greater than 4. The\n      size of the population to evolve. This parameter is ignored if\n      `initial_population` is specified.\n      Default value: 50.\n    population_stddev: A positive scalar real `Tensor` of the same dtype\n      as `initial_position`. This parameter is ignored if `initial_population`\n      is specified. Used to generate the population from the `initial_position`\n      by adding random normal noise with zero mean and the specified standard\n      deviation.\n      Default value: 1.0\n    max_iterations: Positive scalar int32 `Tensor`. The maximum number of\n      generations to evolve the population for.\n      Default value: 100\n    func_tolerance: Scalar `Tensor` of the same dtype as the output of the\n      `objective_function`. The algorithm stops if the absolute difference\n      between the largest and the smallest objective function value in the\n      population is below this number.\n      Default value: 0\n    position_tolerance: Scalar `Tensor` of the same real dtype as\n      `initial_position` or `initial_population`. The algorithm terminates if\n      the largest absolute difference between the coordinates of the population\n      members is below this threshold.\n      Default value: 1e-8\n    differential_weight: Real scalar `Tensor`. Must be positive and less than\n      2.0. The parameter controlling the strength of mutation in the algorithm.\n      Default value: 0.5\n    crossover_prob: Real scalar `Tensor`. Must be between 0 and 1. The\n      probability of recombination per site.\n      Default value: 0.9\n    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n      applied.\n      Default value: None.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name\n      'differential_evolution_minimize' is used.\n      Default value: None\n\n  Returns:\n    optimizer_results: An object containing the following attributes:\n      converged: Scalar boolean `Tensor` indicating whether the minimum was\n        found within the specified tolerances.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A `Tensor` containing the best point found during the search.\n        If the search converged, then this value is the argmin of the\n        objective function within the specified tolerances.\n      objective_value: A `Tensor` containing the value of the objective\n        function at the `position`. If the search\n        converged, then this is the (local) minimum of\n        the objective function.\n      final_population: The final state of the population.\n      final_objective_values: The objective function evaluated at the\n        final population.\n      initial_population: The starting population.\n      initial_objective_values: The objective function evaluated at the\n        initial population.\n      num_iterations: The number of iterations of the main algorithm body.\n\n  Raises:\n    ValueError: If neither the initial population, nor the initial position\n      are specified or if both are specified.\n  \"\"\"\n\n  if initial_population is None and initial_position is None:\n    raise ValueError('Either the initial population or the initial position '\n                     'must be specified.')\n  if initial_population is not None and initial_position is not None:\n    raise ValueError('Only one of initial population or initial position '\n                     'should be specified')\n\n  with tf.compat.v1.name_scope(\n      name,\n      default_name='minimize',\n      values=[\n          initial_population, initial_position, population_size,\n          population_stddev, max_iterations, func_tolerance, position_tolerance,\n          differential_weight, crossover_prob\n      ]):\n    (\n        was_iterable,\n        population,\n        population_values,\n        max_iterations,\n        func_tolerance,\n        position_tolerance,\n        differential_weight,\n        crossover_prob\n    ) = _get_initial_args(objective_function,\n                          initial_population,\n                          initial_position,\n                          population_size,\n                          population_stddev,\n                          max_iterations,\n                          func_tolerance,\n                          position_tolerance,\n                          differential_weight,\n                          crossover_prob,\n                          seed)\n\n    def evolve_body(loop_vars):\n      \"\"\"Performs one step of the evolution.\"\"\"\n      next_population, next_population_values = one_step(\n          objective_function,\n          loop_vars.population,\n          population_values=loop_vars.population_values,\n          differential_weight=differential_weight,\n          crossover_prob=crossover_prob,\n          seed=seed)\n      converged = _check_convergence(next_population,\n                                     next_population_values,\n                                     func_tolerance,\n                                     position_tolerance)\n\n      failed = _check_failure(next_population_values)\n\n      return [_MinimizeLoopVars(\n          converged=converged,\n          failed=failed,\n          num_iterations=loop_vars.num_iterations+1,\n          population=next_population,\n          population_values=next_population_values)]\n\n    def evolve_cond(loop_vars):\n      should_stop = (\n          loop_vars.failed |\n          loop_vars.converged |\n          (max_iterations is not None and\n           loop_vars.num_iterations >= max_iterations))\n      return ~should_stop\n\n    initial_vars = _MinimizeLoopVars(\n        converged=tf.convert_to_tensor(value=False),\n        failed=tf.convert_to_tensor(value=False),\n        num_iterations=tf.convert_to_tensor(value=0),\n        population=population,\n        population_values=population_values)\n    final_state = tf.while_loop(\n        cond=evolve_cond, body=evolve_body, loop_vars=(initial_vars,))[0]\n    best_position, best_values = _find_best_in_population(\n        final_state.population,\n        final_state.population_values)\n    # Ensure we return a similar structure to what the user supplied.\n    final_population = final_state.population\n    if not was_iterable:\n      final_population = final_population[0]\n      best_position = best_position[0]\n    return DifferentialEvolutionOptimizerResults(\n        converged=final_state.converged,\n        failed=final_state.failed,\n        position=best_position,\n        objective_value=best_values,\n        final_population=final_population,\n        final_objective_values=final_state.population_values,\n        initial_population=population,\n        initial_objective_values=population_values,\n        num_iterations=final_state.num_iterations)", "language": "python", "code": "def minimize(objective_function,\n             initial_population=None,\n             initial_position=None,\n             population_size=50,\n             population_stddev=1.,\n             max_iterations=100,\n             func_tolerance=0,\n             position_tolerance=1e-8,\n             differential_weight=0.5,\n             crossover_prob=0.9,\n             seed=None,\n             name=None):\n  \"\"\"Applies the Differential evolution algorithm to minimize a function.\n\n  Differential Evolution is an evolutionary optimization algorithm which works\n  on a set of candidate solutions called the population. It iteratively\n  improves the population by applying genetic operators of mutation and\n  recombination. The objective function `f` supplies the fitness of each\n  candidate. A candidate `s_1` is considered better than `s_2` if\n  `f(s_1) < f(s_2)`.\n\n  This method allows the user to either specify an initial population or a\n  single candidate solution. If a single solution is specified, a population\n  of the specified size is initialized by adding independent normal noise\n  to the candidate solution.\n\n  The implementation also supports a multi-part specification of the state. For\n  example, consider the objective function:\n\n  ```python\n  # x is a tensor of shape [n, m] while y is of shape [n].\n  def objective(x, y):\n    return tf.math.reduce_sum(x ** 2, axis=-1) + y ** 2\n  ```\n  The state in this case is specified by two input tensors `x` and `y`. To\n  apply the algorithm to this objective function, one would need to specify\n  either an initial population as a list of two tensors of shapes\n  `[population_size, k]` and `[population_size]`. The following code shows the\n  complete example:\n\n  ```python\n    population_size = 40\n    # With an initial population and a multi-part state.\n    initial_population = (tf.random.normal([population_size]),\n                          tf.random.normal([population_size]))\n    def easom_fn(x, y):\n      return -(tf.math.cos(x) * tf.math.cos(y) *\n               tf.math.exp(-(x-np.pi)**2 - (y-np.pi)**2))\n\n    optim_results = tfp.optimizers.differential_evolution_minimize(\n        easom_fn,\n        initial_population=initial_population,\n        seed=43210)\n\n    print (optim_results.converged)\n    print (optim_results.position)  # Should be (close to) [pi, pi].\n    print (optim_results.objective_value)    # Should be -1.\n\n\n    # With a single starting point\n    initial_position = (tf.constant(1.0), tf.constant(1.0))\n\n    optim_results = tfp.optimizers.differential_evolution_minimize(\n        easom_fn,\n        initial_position=initial_position,\n        population_size=40,\n        population_stddev=2.0,\n        seed=43210)\n  ```\n\n  Args:\n    objective_function: A Python callable that accepts a batch of possible\n      solutions and returns the values of the objective function at those\n      arguments as a rank 1 real `Tensor`. This specifies the function to be\n      minimized. The input to this callable may be either a single `Tensor`\n      or a Python `list` of `Tensor`s. The signature must match the format of\n      the argument `population`. (i.e. objective_function(*population) must\n      return the value of the function to be minimized).\n    initial_population: A real `Tensor` or Python list of `Tensor`s.\n      If a list, each `Tensor` must be of rank at least 1 and with a common\n      first dimension. The first dimension indexes into the candidate solutions\n      while the rest of the dimensions (if any) index into an individual\n      solution. The size of the population must be at least 4. This is a\n      requirement of the DE algorithm.\n    initial_position: A real `Tensor` of any shape. The seed solution used\n      to initialize the population of solutions. If this parameter is specified\n      then `initial_population` must not be specified.\n    population_size: A positive scalar int32 `Tensor` greater than 4. The\n      size of the population to evolve. This parameter is ignored if\n      `initial_population` is specified.\n      Default value: 50.\n    population_stddev: A positive scalar real `Tensor` of the same dtype\n      as `initial_position`. This parameter is ignored if `initial_population`\n      is specified. Used to generate the population from the `initial_position`\n      by adding random normal noise with zero mean and the specified standard\n      deviation.\n      Default value: 1.0\n    max_iterations: Positive scalar int32 `Tensor`. The maximum number of\n      generations to evolve the population for.\n      Default value: 100\n    func_tolerance: Scalar `Tensor` of the same dtype as the output of the\n      `objective_function`. The algorithm stops if the absolute difference\n      between the largest and the smallest objective function value in the\n      population is below this number.\n      Default value: 0\n    position_tolerance: Scalar `Tensor` of the same real dtype as\n      `initial_position` or `initial_population`. The algorithm terminates if\n      the largest absolute difference between the coordinates of the population\n      members is below this threshold.\n      Default value: 1e-8\n    differential_weight: Real scalar `Tensor`. Must be positive and less than\n      2.0. The parameter controlling the strength of mutation in the algorithm.\n      Default value: 0.5\n    crossover_prob: Real scalar `Tensor`. Must be between 0 and 1. The\n      probability of recombination per site.\n      Default value: 0.9\n    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n      applied.\n      Default value: None.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name\n      'differential_evolution_minimize' is used.\n      Default value: None\n\n  Returns:\n    optimizer_results: An object containing the following attributes:\n      converged: Scalar boolean `Tensor` indicating whether the minimum was\n        found within the specified tolerances.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A `Tensor` containing the best point found during the search.\n        If the search converged, then this value is the argmin of the\n        objective function within the specified tolerances.\n      objective_value: A `Tensor` containing the value of the objective\n        function at the `position`. If the search\n        converged, then this is the (local) minimum of\n        the objective function.\n      final_population: The final state of the population.\n      final_objective_values: The objective function evaluated at the\n        final population.\n      initial_population: The starting population.\n      initial_objective_values: The objective function evaluated at the\n        initial population.\n      num_iterations: The number of iterations of the main algorithm body.\n\n  Raises:\n    ValueError: If neither the initial population, nor the initial position\n      are specified or if both are specified.\n  \"\"\"\n\n  if initial_population is None and initial_position is None:\n    raise ValueError('Either the initial population or the initial position '\n                     'must be specified.')\n  if initial_population is not None and initial_position is not None:\n    raise ValueError('Only one of initial population or initial position '\n                     'should be specified')\n\n  with tf.compat.v1.name_scope(\n      name,\n      default_name='minimize',\n      values=[\n          initial_population, initial_position, population_size,\n          population_stddev, max_iterations, func_tolerance, position_tolerance,\n          differential_weight, crossover_prob\n      ]):\n    (\n        was_iterable,\n        population,\n        population_values,\n        max_iterations,\n        func_tolerance,\n        position_tolerance,\n        differential_weight,\n        crossover_prob\n    ) = _get_initial_args(objective_function,\n                          initial_population,\n                          initial_position,\n                          population_size,\n                          population_stddev,\n                          max_iterations,\n                          func_tolerance,\n                          position_tolerance,\n                          differential_weight,\n                          crossover_prob,\n                          seed)\n\n    def evolve_body(loop_vars):\n      \"\"\"Performs one step of the evolution.\"\"\"\n      next_population, next_population_values = one_step(\n          objective_function,\n          loop_vars.population,\n          population_values=loop_vars.population_values,\n          differential_weight=differential_weight,\n          crossover_prob=crossover_prob,\n          seed=seed)\n      converged = _check_convergence(next_population,\n                                     next_population_values,\n                                     func_tolerance,\n                                     position_tolerance)\n\n      failed = _check_failure(next_population_values)\n\n      return [_MinimizeLoopVars(\n          converged=converged,\n          failed=failed,\n          num_iterations=loop_vars.num_iterations+1,\n          population=next_population,\n          population_values=next_population_values)]\n\n    def evolve_cond(loop_vars):\n      should_stop = (\n          loop_vars.failed |\n          loop_vars.converged |\n          (max_iterations is not None and\n           loop_vars.num_iterations >= max_iterations))\n      return ~should_stop\n\n    initial_vars = _MinimizeLoopVars(\n        converged=tf.convert_to_tensor(value=False),\n        failed=tf.convert_to_tensor(value=False),\n        num_iterations=tf.convert_to_tensor(value=0),\n        population=population,\n        population_values=population_values)\n    final_state = tf.while_loop(\n        cond=evolve_cond, body=evolve_body, loop_vars=(initial_vars,))[0]\n    best_position, best_values = _find_best_in_population(\n        final_state.population,\n        final_state.population_values)\n    # Ensure we return a similar structure to what the user supplied.\n    final_population = final_state.population\n    if not was_iterable:\n      final_population = final_population[0]\n      best_position = best_position[0]\n    return DifferentialEvolutionOptimizerResults(\n        converged=final_state.converged,\n        failed=final_state.failed,\n        position=best_position,\n        objective_value=best_values,\n        final_population=final_population,\n        final_objective_values=final_state.population_values,\n        initial_population=population,\n        initial_objective_values=population_values,\n        num_iterations=final_state.num_iterations)", "code_tokens": ["def", "minimize", "(", "objective_function", ",", "initial_population", "=", "None", ",", "initial_position", "=", "None", ",", "population_size", "=", "50", ",", "population_stddev", "=", "1.", ",", "max_iterations", "=", "100", ",", "func_tolerance", "=", "0", ",", "position_tolerance", "=", "1e-8", ",", "differential_weight", "=", "0.5", ",", "crossover_prob", "=", "0.9", ",", "seed", "=", "None", ",", "name", "=", "None", ")", ":", "if", "initial_population", "is", "None", "and", "initial_position", "is", "None", ":", "raise", "ValueError", "(", "'Either the initial population or the initial position '", "'must be specified.'", ")", "if", "initial_population", "is", "not", "None", "and", "initial_position", "is", "not", "None", ":", "raise", "ValueError", "(", "'Only one of initial population or initial position '", "'should be specified'", ")", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "default_name", "=", "'minimize'", ",", "values", "=", "[", "initial_population", ",", "initial_position", ",", "population_size", ",", "population_stddev", ",", "max_iterations", ",", "func_tolerance", ",", "position_tolerance", ",", "differential_weight", ",", "crossover_prob", "]", ")", ":", "(", "was_iterable", ",", "population", ",", "population_values", ",", "max_iterations", ",", "func_tolerance", ",", "position_tolerance", ",", "differential_weight", ",", "crossover_prob", ")", "=", "_get_initial_args", "(", "objective_function", ",", "initial_population", ",", "initial_position", ",", "population_size", ",", "population_stddev", ",", "max_iterations", ",", "func_tolerance", ",", "position_tolerance", ",", "differential_weight", ",", "crossover_prob", ",", "seed", ")", "def", "evolve_body", "(", "loop_vars", ")", ":", "\"\"\"Performs one step of the evolution.\"\"\"", "next_population", ",", "next_population_values", "=", "one_step", "(", "objective_function", ",", "loop_vars", ".", "population", ",", "population_values", "=", "loop_vars", ".", "population_values", ",", "differential_weight", "=", "differential_weight", ",", "crossover_prob", "=", "crossover_prob", ",", "seed", "=", "seed", ")", "converged", "=", "_check_convergence", "(", "next_population", ",", "next_population_values", ",", "func_tolerance", ",", "position_tolerance", ")", "failed", "=", "_check_failure", "(", "next_population_values", ")", "return", "[", "_MinimizeLoopVars", "(", "converged", "=", "converged", ",", "failed", "=", "failed", ",", "num_iterations", "=", "loop_vars", ".", "num_iterations", "+", "1", ",", "population", "=", "next_population", ",", "population_values", "=", "next_population_values", ")", "]", "def", "evolve_cond", "(", "loop_vars", ")", ":", "should_stop", "=", "(", "loop_vars", ".", "failed", "|", "loop_vars", ".", "converged", "|", "(", "max_iterations", "is", "not", "None", "and", "loop_vars", ".", "num_iterations", ">=", "max_iterations", ")", ")", "return", "~", "should_stop", "initial_vars", "=", "_MinimizeLoopVars", "(", "converged", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "False", ")", ",", "failed", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "False", ")", ",", "num_iterations", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "0", ")", ",", "population", "=", "population", ",", "population_values", "=", "population_values", ")", "final_state", "=", "tf", ".", "while_loop", "(", "cond", "=", "evolve_cond", ",", "body", "=", "evolve_body", ",", "loop_vars", "=", "(", "initial_vars", ",", ")", ")", "[", "0", "]", "best_position", ",", "best_values", "=", "_find_best_in_population", "(", "final_state", ".", "population", ",", "final_state", ".", "population_values", ")", "# Ensure we return a similar structure to what the user supplied.", "final_population", "=", "final_state", ".", "population", "if", "not", "was_iterable", ":", "final_population", "=", "final_population", "[", "0", "]", "best_position", "=", "best_position", "[", "0", "]", "return", "DifferentialEvolutionOptimizerResults", "(", "converged", "=", "final_state", ".", "converged", ",", "failed", "=", "final_state", ".", "failed", ",", "position", "=", "best_position", ",", "objective_value", "=", "best_values", ",", "final_population", "=", "final_population", ",", "final_objective_values", "=", "final_state", ".", "population_values", ",", "initial_population", "=", "population", ",", "initial_objective_values", "=", "population_values", ",", "num_iterations", "=", "final_state", ".", "num_iterations", ")"], "docstring": "Applies the Differential evolution algorithm to minimize a function.\n\n  Differential Evolution is an evolutionary optimization algorithm which works\n  on a set of candidate solutions called the population. It iteratively\n  improves the population by applying genetic operators of mutation and\n  recombination. The objective function `f` supplies the fitness of each\n  candidate. A candidate `s_1` is considered better than `s_2` if\n  `f(s_1) < f(s_2)`.\n\n  This method allows the user to either specify an initial population or a\n  single candidate solution. If a single solution is specified, a population\n  of the specified size is initialized by adding independent normal noise\n  to the candidate solution.\n\n  The implementation also supports a multi-part specification of the state. For\n  example, consider the objective function:\n\n  ```python\n  # x is a tensor of shape [n, m] while y is of shape [n].\n  def objective(x, y):\n    return tf.math.reduce_sum(x ** 2, axis=-1) + y ** 2\n  ```\n  The state in this case is specified by two input tensors `x` and `y`. To\n  apply the algorithm to this objective function, one would need to specify\n  either an initial population as a list of two tensors of shapes\n  `[population_size, k]` and `[population_size]`. The following code shows the\n  complete example:\n\n  ```python\n    population_size = 40\n    # With an initial population and a multi-part state.\n    initial_population = (tf.random.normal([population_size]),\n                          tf.random.normal([population_size]))\n    def easom_fn(x, y):\n      return -(tf.math.cos(x) * tf.math.cos(y) *\n               tf.math.exp(-(x-np.pi)**2 - (y-np.pi)**2))\n\n    optim_results = tfp.optimizers.differential_evolution_minimize(\n        easom_fn,\n        initial_population=initial_population,\n        seed=43210)\n\n    print (optim_results.converged)\n    print (optim_results.position)  # Should be (close to) [pi, pi].\n    print (optim_results.objective_value)    # Should be -1.\n\n\n    # With a single starting point\n    initial_position = (tf.constant(1.0), tf.constant(1.0))\n\n    optim_results = tfp.optimizers.differential_evolution_minimize(\n        easom_fn,\n        initial_position=initial_position,\n        population_size=40,\n        population_stddev=2.0,\n        seed=43210)\n  ```\n\n  Args:\n    objective_function: A Python callable that accepts a batch of possible\n      solutions and returns the values of the objective function at those\n      arguments as a rank 1 real `Tensor`. This specifies the function to be\n      minimized. The input to this callable may be either a single `Tensor`\n      or a Python `list` of `Tensor`s. The signature must match the format of\n      the argument `population`. (i.e. objective_function(*population) must\n      return the value of the function to be minimized).\n    initial_population: A real `Tensor` or Python list of `Tensor`s.\n      If a list, each `Tensor` must be of rank at least 1 and with a common\n      first dimension. The first dimension indexes into the candidate solutions\n      while the rest of the dimensions (if any) index into an individual\n      solution. The size of the population must be at least 4. This is a\n      requirement of the DE algorithm.\n    initial_position: A real `Tensor` of any shape. The seed solution used\n      to initialize the population of solutions. If this parameter is specified\n      then `initial_population` must not be specified.\n    population_size: A positive scalar int32 `Tensor` greater than 4. The\n      size of the population to evolve. This parameter is ignored if\n      `initial_population` is specified.\n      Default value: 50.\n    population_stddev: A positive scalar real `Tensor` of the same dtype\n      as `initial_position`. This parameter is ignored if `initial_population`\n      is specified. Used to generate the population from the `initial_position`\n      by adding random normal noise with zero mean and the specified standard\n      deviation.\n      Default value: 1.0\n    max_iterations: Positive scalar int32 `Tensor`. The maximum number of\n      generations to evolve the population for.\n      Default value: 100\n    func_tolerance: Scalar `Tensor` of the same dtype as the output of the\n      `objective_function`. The algorithm stops if the absolute difference\n      between the largest and the smallest objective function value in the\n      population is below this number.\n      Default value: 0\n    position_tolerance: Scalar `Tensor` of the same real dtype as\n      `initial_position` or `initial_population`. The algorithm terminates if\n      the largest absolute difference between the coordinates of the population\n      members is below this threshold.\n      Default value: 1e-8\n    differential_weight: Real scalar `Tensor`. Must be positive and less than\n      2.0. The parameter controlling the strength of mutation in the algorithm.\n      Default value: 0.5\n    crossover_prob: Real scalar `Tensor`. Must be between 0 and 1. The\n      probability of recombination per site.\n      Default value: 0.9\n    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n      applied.\n      Default value: None.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name\n      'differential_evolution_minimize' is used.\n      Default value: None\n\n  Returns:\n    optimizer_results: An object containing the following attributes:\n      converged: Scalar boolean `Tensor` indicating whether the minimum was\n        found within the specified tolerances.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A `Tensor` containing the best point found during the search.\n        If the search converged, then this value is the argmin of the\n        objective function within the specified tolerances.\n      objective_value: A `Tensor` containing the value of the objective\n        function at the `position`. If the search\n        converged, then this is the (local) minimum of\n        the objective function.\n      final_population: The final state of the population.\n      final_objective_values: The objective function evaluated at the\n        final population.\n      initial_population: The starting population.\n      initial_objective_values: The objective function evaluated at the\n        initial population.\n      num_iterations: The number of iterations of the main algorithm body.\n\n  Raises:\n    ValueError: If neither the initial population, nor the initial position\n      are specified or if both are specified.", "docstring_tokens": ["Applies", "the", "Differential", "evolution", "algorithm", "to", "minimize", "a", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/differential_evolution.py#L214-L456", "partition": "test", "index": 1086, "time": "2018-12-24 03:28:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/differential_evolution.py", "func_name": "one_step", "original_string": "def one_step(\n    objective_function,\n    population,\n    population_values=None,\n    differential_weight=0.5,\n    crossover_prob=0.9,\n    seed=None,\n    name=None):\n  \"\"\"Performs one step of the differential evolution algorithm.\n\n  Args:\n    objective_function:  A Python callable that accepts a batch of possible\n      solutions and returns the values of the objective function at those\n      arguments as a rank 1 real `Tensor`. This specifies the function to be\n      minimized. The input to this callable may be either a single `Tensor`\n      or a Python `list` of `Tensor`s. The signature must match the format of\n      the argument `population`. (i.e. objective_function(*population) must\n      return the value of the function to be minimized).\n    population:  `Tensor` or Python `list` of `Tensor`s representing the\n      current population vectors. Each `Tensor` must be of the same real dtype.\n      The first dimension indexes individual population members while the\n      rest of the dimensions are consumed by the value function. For example,\n      if the population is a single `Tensor` of shape [n, m1, m2], then `n` is\n      the population size and the output of `objective_function` applied to the\n      population is a `Tensor` of shape [n]. If the population is a python\n      list of `Tensor`s then each `Tensor` in the list should have the first\n      axis of a common size, say `n` and `objective_function(*population)`\n      should return a `Tensor of shape [n]. The population must have at least\n      4 members for the algorithm to work correctly.\n    population_values: A `Tensor` of rank 1 and real dtype. The result of\n      applying `objective_function` to the `population`. If not supplied it is\n      computed using the `objective_function`.\n      Default value: None.\n    differential_weight: Real scalar `Tensor`. Must be positive and less than\n      2.0. The parameter controlling the strength of mutation.\n      Default value: 0.5\n    crossover_prob: Real scalar `Tensor`. Must be between 0 and 1. The\n      probability of recombination per site.\n      Default value: 0.9\n    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n      applied.\n      Default value: None.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'one_step' is\n      used.\n      Default value: None\n\n  Returns:\n    A sequence containing the following elements (in order):\n    next_population: A `Tensor` or Python `list` of `Tensor`s of the same\n      structure as the input population. The population at the next generation.\n    next_population_values: A `Tensor` of same shape and dtype as input\n      `population_values`. The function values for the `next_population`.\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'one_step',\n      [population, population_values, differential_weight, crossover_prob]):\n    population, _ = _ensure_list(population)\n    if population_values is None:\n      population_values = objective_function(*population)\n    population_size = tf.shape(input=population[0])[0]\n    seed_stream = distributions.SeedStream(seed, salt='one_step')\n    mixing_indices = _get_mixing_indices(population_size, seed=seed_stream())\n    # Construct the mutated solution vectors. There is one for each member of\n    # the population.\n    mutants = _get_mutants(population,\n                           population_size,\n                           mixing_indices,\n                           differential_weight)\n    # Perform recombination between the parents and the mutants.\n    candidates = _binary_crossover(population,\n                                   population_size,\n                                   mutants,\n                                   crossover_prob,\n                                   seed=seed_stream())\n    candidate_values = objective_function(*candidates)\n    if population_values is None:\n      population_values = objective_function(*population)\n\n    infinity = tf.zeros_like(population_values) + np.inf\n\n    population_values = tf.where(\n        tf.math.is_nan(population_values), x=infinity, y=population_values)\n\n    to_replace = candidate_values < population_values\n    next_population = [\n        tf.where(to_replace, x=candidates_part, y=population_part)\n        for candidates_part, population_part in zip(candidates, population)\n    ]\n    next_values = tf.where(to_replace, x=candidate_values, y=population_values)\n\n  return next_population, next_values", "language": "python", "code": "def one_step(\n    objective_function,\n    population,\n    population_values=None,\n    differential_weight=0.5,\n    crossover_prob=0.9,\n    seed=None,\n    name=None):\n  \"\"\"Performs one step of the differential evolution algorithm.\n\n  Args:\n    objective_function:  A Python callable that accepts a batch of possible\n      solutions and returns the values of the objective function at those\n      arguments as a rank 1 real `Tensor`. This specifies the function to be\n      minimized. The input to this callable may be either a single `Tensor`\n      or a Python `list` of `Tensor`s. The signature must match the format of\n      the argument `population`. (i.e. objective_function(*population) must\n      return the value of the function to be minimized).\n    population:  `Tensor` or Python `list` of `Tensor`s representing the\n      current population vectors. Each `Tensor` must be of the same real dtype.\n      The first dimension indexes individual population members while the\n      rest of the dimensions are consumed by the value function. For example,\n      if the population is a single `Tensor` of shape [n, m1, m2], then `n` is\n      the population size and the output of `objective_function` applied to the\n      population is a `Tensor` of shape [n]. If the population is a python\n      list of `Tensor`s then each `Tensor` in the list should have the first\n      axis of a common size, say `n` and `objective_function(*population)`\n      should return a `Tensor of shape [n]. The population must have at least\n      4 members for the algorithm to work correctly.\n    population_values: A `Tensor` of rank 1 and real dtype. The result of\n      applying `objective_function` to the `population`. If not supplied it is\n      computed using the `objective_function`.\n      Default value: None.\n    differential_weight: Real scalar `Tensor`. Must be positive and less than\n      2.0. The parameter controlling the strength of mutation.\n      Default value: 0.5\n    crossover_prob: Real scalar `Tensor`. Must be between 0 and 1. The\n      probability of recombination per site.\n      Default value: 0.9\n    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n      applied.\n      Default value: None.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'one_step' is\n      used.\n      Default value: None\n\n  Returns:\n    A sequence containing the following elements (in order):\n    next_population: A `Tensor` or Python `list` of `Tensor`s of the same\n      structure as the input population. The population at the next generation.\n    next_population_values: A `Tensor` of same shape and dtype as input\n      `population_values`. The function values for the `next_population`.\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'one_step',\n      [population, population_values, differential_weight, crossover_prob]):\n    population, _ = _ensure_list(population)\n    if population_values is None:\n      population_values = objective_function(*population)\n    population_size = tf.shape(input=population[0])[0]\n    seed_stream = distributions.SeedStream(seed, salt='one_step')\n    mixing_indices = _get_mixing_indices(population_size, seed=seed_stream())\n    # Construct the mutated solution vectors. There is one for each member of\n    # the population.\n    mutants = _get_mutants(population,\n                           population_size,\n                           mixing_indices,\n                           differential_weight)\n    # Perform recombination between the parents and the mutants.\n    candidates = _binary_crossover(population,\n                                   population_size,\n                                   mutants,\n                                   crossover_prob,\n                                   seed=seed_stream())\n    candidate_values = objective_function(*candidates)\n    if population_values is None:\n      population_values = objective_function(*population)\n\n    infinity = tf.zeros_like(population_values) + np.inf\n\n    population_values = tf.where(\n        tf.math.is_nan(population_values), x=infinity, y=population_values)\n\n    to_replace = candidate_values < population_values\n    next_population = [\n        tf.where(to_replace, x=candidates_part, y=population_part)\n        for candidates_part, population_part in zip(candidates, population)\n    ]\n    next_values = tf.where(to_replace, x=candidate_values, y=population_values)\n\n  return next_population, next_values", "code_tokens": ["def", "one_step", "(", "objective_function", ",", "population", ",", "population_values", "=", "None", ",", "differential_weight", "=", "0.5", ",", "crossover_prob", "=", "0.9", ",", "seed", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'one_step'", ",", "[", "population", ",", "population_values", ",", "differential_weight", ",", "crossover_prob", "]", ")", ":", "population", ",", "_", "=", "_ensure_list", "(", "population", ")", "if", "population_values", "is", "None", ":", "population_values", "=", "objective_function", "(", "*", "population", ")", "population_size", "=", "tf", ".", "shape", "(", "input", "=", "population", "[", "0", "]", ")", "[", "0", "]", "seed_stream", "=", "distributions", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'one_step'", ")", "mixing_indices", "=", "_get_mixing_indices", "(", "population_size", ",", "seed", "=", "seed_stream", "(", ")", ")", "# Construct the mutated solution vectors. There is one for each member of", "# the population.", "mutants", "=", "_get_mutants", "(", "population", ",", "population_size", ",", "mixing_indices", ",", "differential_weight", ")", "# Perform recombination between the parents and the mutants.", "candidates", "=", "_binary_crossover", "(", "population", ",", "population_size", ",", "mutants", ",", "crossover_prob", ",", "seed", "=", "seed_stream", "(", ")", ")", "candidate_values", "=", "objective_function", "(", "*", "candidates", ")", "if", "population_values", "is", "None", ":", "population_values", "=", "objective_function", "(", "*", "population", ")", "infinity", "=", "tf", ".", "zeros_like", "(", "population_values", ")", "+", "np", ".", "inf", "population_values", "=", "tf", ".", "where", "(", "tf", ".", "math", ".", "is_nan", "(", "population_values", ")", ",", "x", "=", "infinity", ",", "y", "=", "population_values", ")", "to_replace", "=", "candidate_values", "<", "population_values", "next_population", "=", "[", "tf", ".", "where", "(", "to_replace", ",", "x", "=", "candidates_part", ",", "y", "=", "population_part", ")", "for", "candidates_part", ",", "population_part", "in", "zip", "(", "candidates", ",", "population", ")", "]", "next_values", "=", "tf", ".", "where", "(", "to_replace", ",", "x", "=", "candidate_values", ",", "y", "=", "population_values", ")", "return", "next_population", ",", "next_values"], "docstring": "Performs one step of the differential evolution algorithm.\n\n  Args:\n    objective_function:  A Python callable that accepts a batch of possible\n      solutions and returns the values of the objective function at those\n      arguments as a rank 1 real `Tensor`. This specifies the function to be\n      minimized. The input to this callable may be either a single `Tensor`\n      or a Python `list` of `Tensor`s. The signature must match the format of\n      the argument `population`. (i.e. objective_function(*population) must\n      return the value of the function to be minimized).\n    population:  `Tensor` or Python `list` of `Tensor`s representing the\n      current population vectors. Each `Tensor` must be of the same real dtype.\n      The first dimension indexes individual population members while the\n      rest of the dimensions are consumed by the value function. For example,\n      if the population is a single `Tensor` of shape [n, m1, m2], then `n` is\n      the population size and the output of `objective_function` applied to the\n      population is a `Tensor` of shape [n]. If the population is a python\n      list of `Tensor`s then each `Tensor` in the list should have the first\n      axis of a common size, say `n` and `objective_function(*population)`\n      should return a `Tensor of shape [n]. The population must have at least\n      4 members for the algorithm to work correctly.\n    population_values: A `Tensor` of rank 1 and real dtype. The result of\n      applying `objective_function` to the `population`. If not supplied it is\n      computed using the `objective_function`.\n      Default value: None.\n    differential_weight: Real scalar `Tensor`. Must be positive and less than\n      2.0. The parameter controlling the strength of mutation.\n      Default value: 0.5\n    crossover_prob: Real scalar `Tensor`. Must be between 0 and 1. The\n      probability of recombination per site.\n      Default value: 0.9\n    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n      applied.\n      Default value: None.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'one_step' is\n      used.\n      Default value: None\n\n  Returns:\n    A sequence containing the following elements (in order):\n    next_population: A `Tensor` or Python `list` of `Tensor`s of the same\n      structure as the input population. The population at the next generation.\n    next_population_values: A `Tensor` of same shape and dtype as input\n      `population_values`. The function values for the `next_population`.", "docstring_tokens": ["Performs", "one", "step", "of", "the", "differential", "evolution", "algorithm", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/differential_evolution.py#L120-L211", "partition": "test", "index": 1085, "time": "2018-12-24 03:28:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/differential_evolution.py", "func_name": "_ensure_list", "original_string": "def _ensure_list(tensor_or_list):\n  \"\"\"Converts the input arg to a list if it is not a list already.\n\n  Args:\n    tensor_or_list: A `Tensor` or a Python list of `Tensor`s. The argument to\n      convert to a list of `Tensor`s.\n\n  Returns:\n    A tuple of two elements. The first is a Python list of `Tensor`s containing\n    the original arguments. The second is a boolean indicating whether\n    the original argument was a list or tuple already.\n  \"\"\"\n  if isinstance(tensor_or_list, (list, tuple)):\n    return list(tensor_or_list), True\n  return [tensor_or_list], False", "language": "python", "code": "def _ensure_list(tensor_or_list):\n  \"\"\"Converts the input arg to a list if it is not a list already.\n\n  Args:\n    tensor_or_list: A `Tensor` or a Python list of `Tensor`s. The argument to\n      convert to a list of `Tensor`s.\n\n  Returns:\n    A tuple of two elements. The first is a Python list of `Tensor`s containing\n    the original arguments. The second is a boolean indicating whether\n    the original argument was a list or tuple already.\n  \"\"\"\n  if isinstance(tensor_or_list, (list, tuple)):\n    return list(tensor_or_list), True\n  return [tensor_or_list], False", "code_tokens": ["def", "_ensure_list", "(", "tensor_or_list", ")", ":", "if", "isinstance", "(", "tensor_or_list", ",", "(", "list", ",", "tuple", ")", ")", ":", "return", "list", "(", "tensor_or_list", ")", ",", "True", "return", "[", "tensor_or_list", "]", ",", "False"], "docstring": "Converts the input arg to a list if it is not a list already.\n\n  Args:\n    tensor_or_list: A `Tensor` or a Python list of `Tensor`s. The argument to\n      convert to a list of `Tensor`s.\n\n  Returns:\n    A tuple of two elements. The first is a Python list of `Tensor`s containing\n    the original arguments. The second is a boolean indicating whether\n    the original argument was a list or tuple already.", "docstring_tokens": ["Converts", "the", "input", "arg", "to", "a", "list", "if", "it", "is", "not", "a", "list", "already", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/differential_evolution.py#L771-L785", "partition": "test", "index": 1094, "time": "2018-12-24 03:28:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/differential_evolution.py", "func_name": "_binary_crossover", "original_string": "def _binary_crossover(population,\n                      population_size,\n                      mutants,\n                      crossover_prob,\n                      seed):\n  \"\"\"Performs recombination by binary crossover for the current population.\n\n  Let v_i denote the i'th component of the member v and m_i the corresponding\n  component of the mutant vector corresponding to v. Then the crossed over\n  vector w_i is determined by setting w_i =\n  (m_i with probability=crossover_prob else v_i). In addition, DE requires that\n  at least one of the components is crossed over (otherwise we end\n  up with no change). This is done by choosing on index say k randomly where\n  a force crossover is performed (i.e. w_k = m_k). This is the scheme\n  implemented in this function.\n\n  Args:\n    population: A Python list of `Tensor`s where each `Tensor` in the list\n      must be of rank at least 1 and all the elements must have a common\n      first dimension. The base population to cross over.\n    population_size: A scalar integer `Tensor`. The number of elements in the\n      population (i.e. size of the first dimension of any member of\n      `population`).\n    mutants: A Python list of `Tensor`s with the same structure as `population`.\n      The mutated population.\n    crossover_prob: A postive real scalar `Tensor` bounded above by 1.0. The\n      probability of a crossover being performed for each axis.\n    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n      applied.\n\n  Returns:\n    A list of `Tensor`s of the same structure, dtype and shape as `population`.\n    The recombined population.\n  \"\"\"\n  sizes = [tf.cast(tf.size(input=x), dtype=tf.float64) for x in population]\n  seed_stream = distributions.SeedStream(seed, salt='binary_crossover')\n  force_crossover_group = distributions.Categorical(sizes).sample(\n      [population_size, 1], seed=seed_stream())\n  recombinants = []\n  for i, population_part in enumerate(population):\n    pop_part_flat = tf.reshape(population_part, [population_size, -1])\n    mutant_part_flat = tf.reshape(mutants[i], [population_size, -1])\n    part_size = tf.size(input=population_part) // population_size\n    force_crossovers = tf.one_hot(\n        tf.random.uniform([population_size],\n                          minval=0,\n                          maxval=part_size,\n                          dtype=tf.int32,\n                          seed=seed_stream()),\n        part_size,\n        on_value=True,\n        off_value=False,\n        dtype=tf.bool)  # Tensor of shape [population_size, size]\n    group_mask = tf.math.equal(force_crossover_group, i)\n    force_crossovers &= group_mask\n    do_binary_crossover = tf.random.uniform(\n        [population_size, part_size],\n        dtype=crossover_prob.dtype.base_dtype,\n        seed=seed_stream()) < crossover_prob\n    do_binary_crossover |= force_crossovers\n    recombinant_flat = tf.where(\n        do_binary_crossover,\n        x=mutant_part_flat,\n        y=pop_part_flat)\n    recombinant = tf.reshape(recombinant_flat, tf.shape(input=population_part))\n    recombinants.append(recombinant)\n  return recombinants", "language": "python", "code": "def _binary_crossover(population,\n                      population_size,\n                      mutants,\n                      crossover_prob,\n                      seed):\n  \"\"\"Performs recombination by binary crossover for the current population.\n\n  Let v_i denote the i'th component of the member v and m_i the corresponding\n  component of the mutant vector corresponding to v. Then the crossed over\n  vector w_i is determined by setting w_i =\n  (m_i with probability=crossover_prob else v_i). In addition, DE requires that\n  at least one of the components is crossed over (otherwise we end\n  up with no change). This is done by choosing on index say k randomly where\n  a force crossover is performed (i.e. w_k = m_k). This is the scheme\n  implemented in this function.\n\n  Args:\n    population: A Python list of `Tensor`s where each `Tensor` in the list\n      must be of rank at least 1 and all the elements must have a common\n      first dimension. The base population to cross over.\n    population_size: A scalar integer `Tensor`. The number of elements in the\n      population (i.e. size of the first dimension of any member of\n      `population`).\n    mutants: A Python list of `Tensor`s with the same structure as `population`.\n      The mutated population.\n    crossover_prob: A postive real scalar `Tensor` bounded above by 1.0. The\n      probability of a crossover being performed for each axis.\n    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n      applied.\n\n  Returns:\n    A list of `Tensor`s of the same structure, dtype and shape as `population`.\n    The recombined population.\n  \"\"\"\n  sizes = [tf.cast(tf.size(input=x), dtype=tf.float64) for x in population]\n  seed_stream = distributions.SeedStream(seed, salt='binary_crossover')\n  force_crossover_group = distributions.Categorical(sizes).sample(\n      [population_size, 1], seed=seed_stream())\n  recombinants = []\n  for i, population_part in enumerate(population):\n    pop_part_flat = tf.reshape(population_part, [population_size, -1])\n    mutant_part_flat = tf.reshape(mutants[i], [population_size, -1])\n    part_size = tf.size(input=population_part) // population_size\n    force_crossovers = tf.one_hot(\n        tf.random.uniform([population_size],\n                          minval=0,\n                          maxval=part_size,\n                          dtype=tf.int32,\n                          seed=seed_stream()),\n        part_size,\n        on_value=True,\n        off_value=False,\n        dtype=tf.bool)  # Tensor of shape [population_size, size]\n    group_mask = tf.math.equal(force_crossover_group, i)\n    force_crossovers &= group_mask\n    do_binary_crossover = tf.random.uniform(\n        [population_size, part_size],\n        dtype=crossover_prob.dtype.base_dtype,\n        seed=seed_stream()) < crossover_prob\n    do_binary_crossover |= force_crossovers\n    recombinant_flat = tf.where(\n        do_binary_crossover,\n        x=mutant_part_flat,\n        y=pop_part_flat)\n    recombinant = tf.reshape(recombinant_flat, tf.shape(input=population_part))\n    recombinants.append(recombinant)\n  return recombinants", "code_tokens": ["def", "_binary_crossover", "(", "population", ",", "population_size", ",", "mutants", ",", "crossover_prob", ",", "seed", ")", ":", "sizes", "=", "[", "tf", ".", "cast", "(", "tf", ".", "size", "(", "input", "=", "x", ")", ",", "dtype", "=", "tf", ".", "float64", ")", "for", "x", "in", "population", "]", "seed_stream", "=", "distributions", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'binary_crossover'", ")", "force_crossover_group", "=", "distributions", ".", "Categorical", "(", "sizes", ")", ".", "sample", "(", "[", "population_size", ",", "1", "]", ",", "seed", "=", "seed_stream", "(", ")", ")", "recombinants", "=", "[", "]", "for", "i", ",", "population_part", "in", "enumerate", "(", "population", ")", ":", "pop_part_flat", "=", "tf", ".", "reshape", "(", "population_part", ",", "[", "population_size", ",", "-", "1", "]", ")", "mutant_part_flat", "=", "tf", ".", "reshape", "(", "mutants", "[", "i", "]", ",", "[", "population_size", ",", "-", "1", "]", ")", "part_size", "=", "tf", ".", "size", "(", "input", "=", "population_part", ")", "//", "population_size", "force_crossovers", "=", "tf", ".", "one_hot", "(", "tf", ".", "random", ".", "uniform", "(", "[", "population_size", "]", ",", "minval", "=", "0", ",", "maxval", "=", "part_size", ",", "dtype", "=", "tf", ".", "int32", ",", "seed", "=", "seed_stream", "(", ")", ")", ",", "part_size", ",", "on_value", "=", "True", ",", "off_value", "=", "False", ",", "dtype", "=", "tf", ".", "bool", ")", "# Tensor of shape [population_size, size]", "group_mask", "=", "tf", ".", "math", ".", "equal", "(", "force_crossover_group", ",", "i", ")", "force_crossovers", "&=", "group_mask", "do_binary_crossover", "=", "tf", ".", "random", ".", "uniform", "(", "[", "population_size", ",", "part_size", "]", ",", "dtype", "=", "crossover_prob", ".", "dtype", ".", "base_dtype", ",", "seed", "=", "seed_stream", "(", ")", ")", "<", "crossover_prob", "do_binary_crossover", "|=", "force_crossovers", "recombinant_flat", "=", "tf", ".", "where", "(", "do_binary_crossover", ",", "x", "=", "mutant_part_flat", ",", "y", "=", "pop_part_flat", ")", "recombinant", "=", "tf", ".", "reshape", "(", "recombinant_flat", ",", "tf", ".", "shape", "(", "input", "=", "population_part", ")", ")", "recombinants", ".", "append", "(", "recombinant", ")", "return", "recombinants"], "docstring": "Performs recombination by binary crossover for the current population.\n\n  Let v_i denote the i'th component of the member v and m_i the corresponding\n  component of the mutant vector corresponding to v. Then the crossed over\n  vector w_i is determined by setting w_i =\n  (m_i with probability=crossover_prob else v_i). In addition, DE requires that\n  at least one of the components is crossed over (otherwise we end\n  up with no change). This is done by choosing on index say k randomly where\n  a force crossover is performed (i.e. w_k = m_k). This is the scheme\n  implemented in this function.\n\n  Args:\n    population: A Python list of `Tensor`s where each `Tensor` in the list\n      must be of rank at least 1 and all the elements must have a common\n      first dimension. The base population to cross over.\n    population_size: A scalar integer `Tensor`. The number of elements in the\n      population (i.e. size of the first dimension of any member of\n      `population`).\n    mutants: A Python list of `Tensor`s with the same structure as `population`.\n      The mutated population.\n    crossover_prob: A postive real scalar `Tensor` bounded above by 1.0. The\n      probability of a crossover being performed for each axis.\n    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n      applied.\n\n  Returns:\n    A list of `Tensor`s of the same structure, dtype and shape as `population`.\n    The recombined population.", "docstring_tokens": ["Performs", "recombination", "by", "binary", "crossover", "for", "the", "current", "population", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/differential_evolution.py#L605-L671", "partition": "test", "index": 1091, "time": "2018-12-24 03:28:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/differential_evolution.py", "func_name": "_get_mixing_indices", "original_string": "def _get_mixing_indices(size, seed=None, name=None):\n  \"\"\"Generates an array of indices suitable for mutation operation.\n\n  The mutation operation in differential evolution requires that for every\n  element of the population, three distinct other elements be chosen to produce\n  a trial candidate. This function generates an array of shape [size, 3]\n  satisfying the properties that:\n    (a). array[i, :] does not contain the index 'i'.\n    (b). array[i, :] does not contain any overlapping indices.\n    (c). All elements in the array are between 0 and size - 1 inclusive.\n\n  Args:\n    size: Scalar integer `Tensor`. The number of samples as well as a the range\n      of the indices to sample from.\n    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n      applied.\n      Default value: `None`.\n    name:  Python `str` name prefixed to Ops created by this function.\n      Default value: 'get_mixing_indices'.\n\n  Returns:\n    sample: A `Tensor` of shape [size, 3] and same dtype as `size` containing\n    samples without replacement between 0 and size - 1 (inclusive) with the\n    `i`th row not including the number `i`.\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, default_name='get_mixing_indices', values=[size]):\n    size = tf.convert_to_tensor(value=size)\n    dtype = size.dtype\n    seed_stream = distributions.SeedStream(seed, salt='get_mixing_indices')\n    first = tf.random.uniform([size],\n                              maxval=size-1,\n                              dtype=dtype,\n                              seed=seed_stream())\n    second = tf.random.uniform([size],\n                               maxval=size-2,\n                               dtype=dtype,\n                               seed=seed_stream())\n    third = tf.random.uniform([size],\n                              maxval=size-3,\n                              dtype=dtype,\n                              seed=seed_stream())\n\n    # Shift second if it is on top of or to the right of first\n    second = tf.where(first < second, x=second, y=second + 1)\n    smaller = tf.math.minimum(first, second)\n    larger = tf.math.maximum(first, second)\n    # Shift the third one so it does not coincide with either the first or the\n    # second number. Assuming first < second, shift by 1 if the number is in\n    # [first, second) and by 2 if the number is greater than or equal to the\n    # second.\n    third = tf.where(third < smaller, x=third, y=third + 1)\n    third = tf.where(third < larger, x=third, y=third + 1)\n    sample = tf.stack([first, second, third], axis=1)\n    to_avoid = tf.expand_dims(tf.range(size), axis=-1)\n    sample = tf.where(sample < to_avoid, x=sample, y=sample + 1)\n    return sample", "language": "python", "code": "def _get_mixing_indices(size, seed=None, name=None):\n  \"\"\"Generates an array of indices suitable for mutation operation.\n\n  The mutation operation in differential evolution requires that for every\n  element of the population, three distinct other elements be chosen to produce\n  a trial candidate. This function generates an array of shape [size, 3]\n  satisfying the properties that:\n    (a). array[i, :] does not contain the index 'i'.\n    (b). array[i, :] does not contain any overlapping indices.\n    (c). All elements in the array are between 0 and size - 1 inclusive.\n\n  Args:\n    size: Scalar integer `Tensor`. The number of samples as well as a the range\n      of the indices to sample from.\n    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n      applied.\n      Default value: `None`.\n    name:  Python `str` name prefixed to Ops created by this function.\n      Default value: 'get_mixing_indices'.\n\n  Returns:\n    sample: A `Tensor` of shape [size, 3] and same dtype as `size` containing\n    samples without replacement between 0 and size - 1 (inclusive) with the\n    `i`th row not including the number `i`.\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, default_name='get_mixing_indices', values=[size]):\n    size = tf.convert_to_tensor(value=size)\n    dtype = size.dtype\n    seed_stream = distributions.SeedStream(seed, salt='get_mixing_indices')\n    first = tf.random.uniform([size],\n                              maxval=size-1,\n                              dtype=dtype,\n                              seed=seed_stream())\n    second = tf.random.uniform([size],\n                               maxval=size-2,\n                               dtype=dtype,\n                               seed=seed_stream())\n    third = tf.random.uniform([size],\n                              maxval=size-3,\n                              dtype=dtype,\n                              seed=seed_stream())\n\n    # Shift second if it is on top of or to the right of first\n    second = tf.where(first < second, x=second, y=second + 1)\n    smaller = tf.math.minimum(first, second)\n    larger = tf.math.maximum(first, second)\n    # Shift the third one so it does not coincide with either the first or the\n    # second number. Assuming first < second, shift by 1 if the number is in\n    # [first, second) and by 2 if the number is greater than or equal to the\n    # second.\n    third = tf.where(third < smaller, x=third, y=third + 1)\n    third = tf.where(third < larger, x=third, y=third + 1)\n    sample = tf.stack([first, second, third], axis=1)\n    to_avoid = tf.expand_dims(tf.range(size), axis=-1)\n    sample = tf.where(sample < to_avoid, x=sample, y=sample + 1)\n    return sample", "code_tokens": ["def", "_get_mixing_indices", "(", "size", ",", "seed", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "default_name", "=", "'get_mixing_indices'", ",", "values", "=", "[", "size", "]", ")", ":", "size", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "size", ")", "dtype", "=", "size", ".", "dtype", "seed_stream", "=", "distributions", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'get_mixing_indices'", ")", "first", "=", "tf", ".", "random", ".", "uniform", "(", "[", "size", "]", ",", "maxval", "=", "size", "-", "1", ",", "dtype", "=", "dtype", ",", "seed", "=", "seed_stream", "(", ")", ")", "second", "=", "tf", ".", "random", ".", "uniform", "(", "[", "size", "]", ",", "maxval", "=", "size", "-", "2", ",", "dtype", "=", "dtype", ",", "seed", "=", "seed_stream", "(", ")", ")", "third", "=", "tf", ".", "random", ".", "uniform", "(", "[", "size", "]", ",", "maxval", "=", "size", "-", "3", ",", "dtype", "=", "dtype", ",", "seed", "=", "seed_stream", "(", ")", ")", "# Shift second if it is on top of or to the right of first", "second", "=", "tf", ".", "where", "(", "first", "<", "second", ",", "x", "=", "second", ",", "y", "=", "second", "+", "1", ")", "smaller", "=", "tf", ".", "math", ".", "minimum", "(", "first", ",", "second", ")", "larger", "=", "tf", ".", "math", ".", "maximum", "(", "first", ",", "second", ")", "# Shift the third one so it does not coincide with either the first or the", "# second number. Assuming first < second, shift by 1 if the number is in", "# [first, second) and by 2 if the number is greater than or equal to the", "# second.", "third", "=", "tf", ".", "where", "(", "third", "<", "smaller", ",", "x", "=", "third", ",", "y", "=", "third", "+", "1", ")", "third", "=", "tf", ".", "where", "(", "third", "<", "larger", ",", "x", "=", "third", ",", "y", "=", "third", "+", "1", ")", "sample", "=", "tf", ".", "stack", "(", "[", "first", ",", "second", ",", "third", "]", ",", "axis", "=", "1", ")", "to_avoid", "=", "tf", ".", "expand_dims", "(", "tf", ".", "range", "(", "size", ")", ",", "axis", "=", "-", "1", ")", "sample", "=", "tf", ".", "where", "(", "sample", "<", "to_avoid", ",", "x", "=", "sample", ",", "y", "=", "sample", "+", "1", ")", "return", "sample"], "docstring": "Generates an array of indices suitable for mutation operation.\n\n  The mutation operation in differential evolution requires that for every\n  element of the population, three distinct other elements be chosen to produce\n  a trial candidate. This function generates an array of shape [size, 3]\n  satisfying the properties that:\n    (a). array[i, :] does not contain the index 'i'.\n    (b). array[i, :] does not contain any overlapping indices.\n    (c). All elements in the array are between 0 and size - 1 inclusive.\n\n  Args:\n    size: Scalar integer `Tensor`. The number of samples as well as a the range\n      of the indices to sample from.\n    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n      applied.\n      Default value: `None`.\n    name:  Python `str` name prefixed to Ops created by this function.\n      Default value: 'get_mixing_indices'.\n\n  Returns:\n    sample: A `Tensor` of shape [size, 3] and same dtype as `size` containing\n    samples without replacement between 0 and size - 1 (inclusive) with the\n    `i`th row not including the number `i`.", "docstring_tokens": ["Generates", "an", "array", "of", "indices", "suitable", "for", "mutation", "operation", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/differential_evolution.py#L712-L768", "partition": "test", "index": 1093, "time": "2018-12-24 03:28:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/differential_evolution.py", "func_name": "_check_convergence", "original_string": "def _check_convergence(population,\n                       population_values,\n                       func_tolerance,\n                       position_tolerance):\n  \"\"\"Checks whether the convergence criteria have been met.\"\"\"\n  # Check func tolerance\n  value_range = tf.math.abs(\n      tf.math.reduce_max(input_tensor=population_values) -\n      tf.math.reduce_min(input_tensor=population_values))\n  value_converged = value_range <= func_tolerance\n  # Ideally, we would compute the position convergence by computing the\n  # pairwise distance between every member of the population and checking if\n  # the maximum of those is less than the supplied tolerance. However, this is\n  # completely infeasible in terms of performance. We adopt a more conservative\n  # approach which checks the distance between the first population member\n  # with the rest of the population. If the largest such distance is less than\n  # half the supplied tolerance, we stop. The reason why this is sufficient is\n  # as follows. For any pair of distinct points (a, b) in the population, we\n  # have the relation:  |a - b| <= |x0 - a| + |x0 - b|, where x0 is any\n  # other point. In particular, let x0 be the first element of the population\n  # and suppose that the largest distance between this point and any other\n  # member is epsilon. Then, for any pair of points (a, b),\n  # |a - b| <= 2 * epsilon and hence, the maximum distance between any pair of\n  # points in the population is bounded above by twice the distance between\n  # the first point and other points.\n  half_tol = position_tolerance / 2\n  def part_converged(part):\n    return tf.math.reduce_max(input_tensor=tf.math.abs(part -\n                                                       part[0])) <= half_tol\n\n  x_converged = tf.math.reduce_all(\n      input_tensor=[part_converged(part) for part in population])\n  return value_converged | x_converged", "language": "python", "code": "def _check_convergence(population,\n                       population_values,\n                       func_tolerance,\n                       position_tolerance):\n  \"\"\"Checks whether the convergence criteria have been met.\"\"\"\n  # Check func tolerance\n  value_range = tf.math.abs(\n      tf.math.reduce_max(input_tensor=population_values) -\n      tf.math.reduce_min(input_tensor=population_values))\n  value_converged = value_range <= func_tolerance\n  # Ideally, we would compute the position convergence by computing the\n  # pairwise distance between every member of the population and checking if\n  # the maximum of those is less than the supplied tolerance. However, this is\n  # completely infeasible in terms of performance. We adopt a more conservative\n  # approach which checks the distance between the first population member\n  # with the rest of the population. If the largest such distance is less than\n  # half the supplied tolerance, we stop. The reason why this is sufficient is\n  # as follows. For any pair of distinct points (a, b) in the population, we\n  # have the relation:  |a - b| <= |x0 - a| + |x0 - b|, where x0 is any\n  # other point. In particular, let x0 be the first element of the population\n  # and suppose that the largest distance between this point and any other\n  # member is epsilon. Then, for any pair of points (a, b),\n  # |a - b| <= 2 * epsilon and hence, the maximum distance between any pair of\n  # points in the population is bounded above by twice the distance between\n  # the first point and other points.\n  half_tol = position_tolerance / 2\n  def part_converged(part):\n    return tf.math.reduce_max(input_tensor=tf.math.abs(part -\n                                                       part[0])) <= half_tol\n\n  x_converged = tf.math.reduce_all(\n      input_tensor=[part_converged(part) for part in population])\n  return value_converged | x_converged", "code_tokens": ["def", "_check_convergence", "(", "population", ",", "population_values", ",", "func_tolerance", ",", "position_tolerance", ")", ":", "# Check func tolerance", "value_range", "=", "tf", ".", "math", ".", "abs", "(", "tf", ".", "math", ".", "reduce_max", "(", "input_tensor", "=", "population_values", ")", "-", "tf", ".", "math", ".", "reduce_min", "(", "input_tensor", "=", "population_values", ")", ")", "value_converged", "=", "value_range", "<=", "func_tolerance", "# Ideally, we would compute the position convergence by computing the", "# pairwise distance between every member of the population and checking if", "# the maximum of those is less than the supplied tolerance. However, this is", "# completely infeasible in terms of performance. We adopt a more conservative", "# approach which checks the distance between the first population member", "# with the rest of the population. If the largest such distance is less than", "# half the supplied tolerance, we stop. The reason why this is sufficient is", "# as follows. For any pair of distinct points (a, b) in the population, we", "# have the relation:  |a - b| <= |x0 - a| + |x0 - b|, where x0 is any", "# other point. In particular, let x0 be the first element of the population", "# and suppose that the largest distance between this point and any other", "# member is epsilon. Then, for any pair of points (a, b),", "# |a - b| <= 2 * epsilon and hence, the maximum distance between any pair of", "# points in the population is bounded above by twice the distance between", "# the first point and other points.", "half_tol", "=", "position_tolerance", "/", "2", "def", "part_converged", "(", "part", ")", ":", "return", "tf", ".", "math", ".", "reduce_max", "(", "input_tensor", "=", "tf", ".", "math", ".", "abs", "(", "part", "-", "part", "[", "0", "]", ")", ")", "<=", "half_tol", "x_converged", "=", "tf", ".", "math", ".", "reduce_all", "(", "input_tensor", "=", "[", "part_converged", "(", "part", ")", "for", "part", "in", "population", "]", ")", "return", "value_converged", "|", "x_converged"], "docstring": "Checks whether the convergence criteria have been met.", "docstring_tokens": ["Checks", "whether", "the", "convergence", "criteria", "have", "been", "met", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/differential_evolution.py#L519-L551", "partition": "test", "index": 1089, "time": "2018-12-24 03:28:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/differential_evolution.py", "func_name": "_find_best_in_population", "original_string": "def _find_best_in_population(population, values):\n  \"\"\"Finds the population member with the lowest value.\"\"\"\n  best_value = tf.math.reduce_min(input_tensor=values)\n  best_index = tf.where(tf.math.equal(values, best_value))[0, 0]\n\n  return ([population_part[best_index] for population_part in population],\n          best_value)", "language": "python", "code": "def _find_best_in_population(population, values):\n  \"\"\"Finds the population member with the lowest value.\"\"\"\n  best_value = tf.math.reduce_min(input_tensor=values)\n  best_index = tf.where(tf.math.equal(values, best_value))[0, 0]\n\n  return ([population_part[best_index] for population_part in population],\n          best_value)", "code_tokens": ["def", "_find_best_in_population", "(", "population", ",", "values", ")", ":", "best_value", "=", "tf", ".", "math", ".", "reduce_min", "(", "input_tensor", "=", "values", ")", "best_index", "=", "tf", ".", "where", "(", "tf", ".", "math", ".", "equal", "(", "values", ",", "best_value", ")", ")", "[", "0", ",", "0", "]", "return", "(", "[", "population_part", "[", "best_index", "]", "for", "population_part", "in", "population", "]", ",", "best_value", ")"], "docstring": "Finds the population member with the lowest value.", "docstring_tokens": ["Finds", "the", "population", "member", "with", "the", "lowest", "value", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/differential_evolution.py#L510-L516", "partition": "test", "index": 1088, "time": "2018-12-24 03:28:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/differential_evolution.py", "func_name": "_get_initial_args", "original_string": "def _get_initial_args(objective_function,\n                      initial_population,\n                      initial_position,\n                      population_size,\n                      population_stddev,\n                      max_iterations,\n                      func_tolerance,\n                      position_tolerance,\n                      differential_weight,\n                      crossover_prob,\n                      seed):\n  \"\"\"Processes initial args.\"\"\"\n  was_iterable = False\n  if initial_position is not None:\n    initial_position, was_iterable = _ensure_list(initial_position)\n\n  if initial_population is not None:\n    initial_population, was_iterable = _ensure_list(initial_population)\n\n  population = _get_starting_population(initial_population,\n                                        initial_position,\n                                        population_size,\n                                        population_stddev,\n                                        seed=seed)\n\n  differential_weight = tf.convert_to_tensor(\n      value=differential_weight, dtype=population[0].dtype.base_dtype)\n\n  crossover_prob = tf.convert_to_tensor(value=crossover_prob)\n  population_values = objective_function(*population)\n  if max_iterations is not None:\n    max_iterations = tf.convert_to_tensor(value=max_iterations)\n  func_tolerance = tf.convert_to_tensor(\n      value=func_tolerance, dtype=population_values.dtype.base_dtype)\n  position_tolerance = tf.convert_to_tensor(\n      value=position_tolerance, dtype=population[0].dtype.base_dtype)\n  return (was_iterable,\n          population,\n          population_values,\n          max_iterations,\n          func_tolerance,\n          position_tolerance,\n          differential_weight,\n          crossover_prob)", "language": "python", "code": "def _get_initial_args(objective_function,\n                      initial_population,\n                      initial_position,\n                      population_size,\n                      population_stddev,\n                      max_iterations,\n                      func_tolerance,\n                      position_tolerance,\n                      differential_weight,\n                      crossover_prob,\n                      seed):\n  \"\"\"Processes initial args.\"\"\"\n  was_iterable = False\n  if initial_position is not None:\n    initial_position, was_iterable = _ensure_list(initial_position)\n\n  if initial_population is not None:\n    initial_population, was_iterable = _ensure_list(initial_population)\n\n  population = _get_starting_population(initial_population,\n                                        initial_position,\n                                        population_size,\n                                        population_stddev,\n                                        seed=seed)\n\n  differential_weight = tf.convert_to_tensor(\n      value=differential_weight, dtype=population[0].dtype.base_dtype)\n\n  crossover_prob = tf.convert_to_tensor(value=crossover_prob)\n  population_values = objective_function(*population)\n  if max_iterations is not None:\n    max_iterations = tf.convert_to_tensor(value=max_iterations)\n  func_tolerance = tf.convert_to_tensor(\n      value=func_tolerance, dtype=population_values.dtype.base_dtype)\n  position_tolerance = tf.convert_to_tensor(\n      value=position_tolerance, dtype=population[0].dtype.base_dtype)\n  return (was_iterable,\n          population,\n          population_values,\n          max_iterations,\n          func_tolerance,\n          position_tolerance,\n          differential_weight,\n          crossover_prob)", "code_tokens": ["def", "_get_initial_args", "(", "objective_function", ",", "initial_population", ",", "initial_position", ",", "population_size", ",", "population_stddev", ",", "max_iterations", ",", "func_tolerance", ",", "position_tolerance", ",", "differential_weight", ",", "crossover_prob", ",", "seed", ")", ":", "was_iterable", "=", "False", "if", "initial_position", "is", "not", "None", ":", "initial_position", ",", "was_iterable", "=", "_ensure_list", "(", "initial_position", ")", "if", "initial_population", "is", "not", "None", ":", "initial_population", ",", "was_iterable", "=", "_ensure_list", "(", "initial_population", ")", "population", "=", "_get_starting_population", "(", "initial_population", ",", "initial_position", ",", "population_size", ",", "population_stddev", ",", "seed", "=", "seed", ")", "differential_weight", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "differential_weight", ",", "dtype", "=", "population", "[", "0", "]", ".", "dtype", ".", "base_dtype", ")", "crossover_prob", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "crossover_prob", ")", "population_values", "=", "objective_function", "(", "*", "population", ")", "if", "max_iterations", "is", "not", "None", ":", "max_iterations", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "max_iterations", ")", "func_tolerance", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "func_tolerance", ",", "dtype", "=", "population_values", ".", "dtype", ".", "base_dtype", ")", "position_tolerance", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "position_tolerance", ",", "dtype", "=", "population", "[", "0", "]", ".", "dtype", ".", "base_dtype", ")", "return", "(", "was_iterable", ",", "population", ",", "population_values", ",", "max_iterations", ",", "func_tolerance", ",", "position_tolerance", ",", "differential_weight", ",", "crossover_prob", ")"], "docstring": "Processes initial args.", "docstring_tokens": ["Processes", "initial", "args", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/differential_evolution.py#L459-L502", "partition": "test", "index": 1087, "time": "2018-12-24 03:28:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/differential_evolution.py", "func_name": "_get_starting_population", "original_string": "def _get_starting_population(initial_population,\n                             initial_position,\n                             population_size,\n                             population_stddev,\n                             seed):\n  \"\"\"Constructs the initial population.\n\n  If an initial population is not already provided, this function constructs\n  a population by adding random normal noise to the initial position.\n\n  Args:\n    initial_population: None or a list of `Tensor`s. The initial population.\n    initial_position: None or a list of `Tensor`s. The initial position.\n      If initial_population is None, this argument must not be None.\n    population_size: Scalar integer `Tensor`. The number of members in the\n      population. If the initial population is not None, this parameter is\n      ignored.\n    population_stddev: A positive scalar real `Tensor` of the same dtype\n      as `initial_position` or `initial_population` (whichever is not None).\n      This parameter is ignored if `initial_population`\n      is specified. Used to generate the population from the\n      `initial_position` by adding random normal noise with zero mean and\n      the specified standard deviation.\n    seed: Seed for random number generation.\n\n  Returns:\n    A list of `Tensor`s. The initial population.\n  \"\"\"\n  if initial_population is not None:\n    return [tf.convert_to_tensor(value=part) for part in initial_population]\n  # Constructs the population by adding normal noise to the initial position.\n  seed_stream = distributions.SeedStream(seed, salt='get_starting_population')\n  population = []\n  for part in initial_position:\n    part = tf.convert_to_tensor(value=part)\n    part_event_shape = tf.shape(input=part)\n    # We only draw population_size-1 random vectors because we want to ensure\n    # that the supplied position is part of the population. The first member\n    # is set to be the initial_position.\n    population_part_shape = tf.concat([[population_size-1],\n                                       part_event_shape], axis=0)\n    population_part = tf.random.normal(population_part_shape,\n                                       stddev=population_stddev,\n                                       dtype=part.dtype.base_dtype,\n                                       seed=seed_stream())\n    population_part += part\n    population_part = tf.concat([[part], population_part], axis=0)\n    population.append(population_part)\n  return population", "language": "python", "code": "def _get_starting_population(initial_population,\n                             initial_position,\n                             population_size,\n                             population_stddev,\n                             seed):\n  \"\"\"Constructs the initial population.\n\n  If an initial population is not already provided, this function constructs\n  a population by adding random normal noise to the initial position.\n\n  Args:\n    initial_population: None or a list of `Tensor`s. The initial population.\n    initial_position: None or a list of `Tensor`s. The initial position.\n      If initial_population is None, this argument must not be None.\n    population_size: Scalar integer `Tensor`. The number of members in the\n      population. If the initial population is not None, this parameter is\n      ignored.\n    population_stddev: A positive scalar real `Tensor` of the same dtype\n      as `initial_position` or `initial_population` (whichever is not None).\n      This parameter is ignored if `initial_population`\n      is specified. Used to generate the population from the\n      `initial_position` by adding random normal noise with zero mean and\n      the specified standard deviation.\n    seed: Seed for random number generation.\n\n  Returns:\n    A list of `Tensor`s. The initial population.\n  \"\"\"\n  if initial_population is not None:\n    return [tf.convert_to_tensor(value=part) for part in initial_population]\n  # Constructs the population by adding normal noise to the initial position.\n  seed_stream = distributions.SeedStream(seed, salt='get_starting_population')\n  population = []\n  for part in initial_position:\n    part = tf.convert_to_tensor(value=part)\n    part_event_shape = tf.shape(input=part)\n    # We only draw population_size-1 random vectors because we want to ensure\n    # that the supplied position is part of the population. The first member\n    # is set to be the initial_position.\n    population_part_shape = tf.concat([[population_size-1],\n                                       part_event_shape], axis=0)\n    population_part = tf.random.normal(population_part_shape,\n                                       stddev=population_stddev,\n                                       dtype=part.dtype.base_dtype,\n                                       seed=seed_stream())\n    population_part += part\n    population_part = tf.concat([[part], population_part], axis=0)\n    population.append(population_part)\n  return population", "code_tokens": ["def", "_get_starting_population", "(", "initial_population", ",", "initial_position", ",", "population_size", ",", "population_stddev", ",", "seed", ")", ":", "if", "initial_population", "is", "not", "None", ":", "return", "[", "tf", ".", "convert_to_tensor", "(", "value", "=", "part", ")", "for", "part", "in", "initial_population", "]", "# Constructs the population by adding normal noise to the initial position.", "seed_stream", "=", "distributions", ".", "SeedStream", "(", "seed", ",", "salt", "=", "'get_starting_population'", ")", "population", "=", "[", "]", "for", "part", "in", "initial_position", ":", "part", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "part", ")", "part_event_shape", "=", "tf", ".", "shape", "(", "input", "=", "part", ")", "# We only draw population_size-1 random vectors because we want to ensure", "# that the supplied position is part of the population. The first member", "# is set to be the initial_position.", "population_part_shape", "=", "tf", ".", "concat", "(", "[", "[", "population_size", "-", "1", "]", ",", "part_event_shape", "]", ",", "axis", "=", "0", ")", "population_part", "=", "tf", ".", "random", ".", "normal", "(", "population_part_shape", ",", "stddev", "=", "population_stddev", ",", "dtype", "=", "part", ".", "dtype", ".", "base_dtype", ",", "seed", "=", "seed_stream", "(", ")", ")", "population_part", "+=", "part", "population_part", "=", "tf", ".", "concat", "(", "[", "[", "part", "]", ",", "population_part", "]", ",", "axis", "=", "0", ")", "population", ".", "append", "(", "population_part", ")", "return", "population"], "docstring": "Constructs the initial population.\n\n  If an initial population is not already provided, this function constructs\n  a population by adding random normal noise to the initial position.\n\n  Args:\n    initial_population: None or a list of `Tensor`s. The initial population.\n    initial_position: None or a list of `Tensor`s. The initial position.\n      If initial_population is None, this argument must not be None.\n    population_size: Scalar integer `Tensor`. The number of members in the\n      population. If the initial population is not None, this parameter is\n      ignored.\n    population_stddev: A positive scalar real `Tensor` of the same dtype\n      as `initial_position` or `initial_population` (whichever is not None).\n      This parameter is ignored if `initial_population`\n      is specified. Used to generate the population from the\n      `initial_position` by adding random normal noise with zero mean and\n      the specified standard deviation.\n    seed: Seed for random number generation.\n\n  Returns:\n    A list of `Tensor`s. The initial population.", "docstring_tokens": ["Constructs", "the", "initial", "population", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/differential_evolution.py#L554-L602", "partition": "test", "index": 1090, "time": "2018-12-24 03:28:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/differential_evolution.py", "func_name": "_get_mutants", "original_string": "def _get_mutants(population,\n                 population_size,\n                 mixing_indices,\n                 differential_weight):\n  \"\"\"Computes the mutatated vectors for each population member.\n\n  Args:\n    population:  Python `list` of `Tensor`s representing the\n      current population vectors. Each `Tensor` must be of the same real dtype.\n      The first dimension of each `Tensor` indexes individual\n      population members. For example, if the population is a list with a\n      single `Tensor` of shape [n, m1, m2], then `n` is the population size and\n      the shape of an individual solution is [m1, m2].\n      If there is more than one element in the population, then each `Tensor`\n      in the list should have the first axis of the same size.\n    population_size: Scalar integer `Tensor`. The size of the population.\n    mixing_indices: `Tensor` of integral dtype and shape [n, 3] where `n` is the\n      number of members in the population. Each element of the `Tensor` must be\n      a valid index into the first dimension of the population (i.e range\n      between `0` and `n-1` inclusive).\n    differential_weight: Real scalar `Tensor`. Must be positive and less than\n      2.0. The parameter controlling the strength of mutation.\n\n  Returns:\n    mutants: `Tensor` or Python `list` of `Tensor`s of the same shape and dtype\n      as the input population. The mutated vectors.\n  \"\"\"\n  mixing_indices = tf.reshape(mixing_indices, [-1])\n  weights = tf.stack([1.0, differential_weight, -differential_weight])\n  def _mutant_part(population_part):\n    donors = tf.gather(population_part, mixing_indices)\n    donors = tf.transpose(\n        a=tf.reshape(donors, [population_size, 3, -1]), perm=[0, 2, 1])\n    return tf.math.reduce_sum(input_tensor=donors * weights, axis=-1)\n\n  return [_mutant_part(population_part) for population_part in population]", "language": "python", "code": "def _get_mutants(population,\n                 population_size,\n                 mixing_indices,\n                 differential_weight):\n  \"\"\"Computes the mutatated vectors for each population member.\n\n  Args:\n    population:  Python `list` of `Tensor`s representing the\n      current population vectors. Each `Tensor` must be of the same real dtype.\n      The first dimension of each `Tensor` indexes individual\n      population members. For example, if the population is a list with a\n      single `Tensor` of shape [n, m1, m2], then `n` is the population size and\n      the shape of an individual solution is [m1, m2].\n      If there is more than one element in the population, then each `Tensor`\n      in the list should have the first axis of the same size.\n    population_size: Scalar integer `Tensor`. The size of the population.\n    mixing_indices: `Tensor` of integral dtype and shape [n, 3] where `n` is the\n      number of members in the population. Each element of the `Tensor` must be\n      a valid index into the first dimension of the population (i.e range\n      between `0` and `n-1` inclusive).\n    differential_weight: Real scalar `Tensor`. Must be positive and less than\n      2.0. The parameter controlling the strength of mutation.\n\n  Returns:\n    mutants: `Tensor` or Python `list` of `Tensor`s of the same shape and dtype\n      as the input population. The mutated vectors.\n  \"\"\"\n  mixing_indices = tf.reshape(mixing_indices, [-1])\n  weights = tf.stack([1.0, differential_weight, -differential_weight])\n  def _mutant_part(population_part):\n    donors = tf.gather(population_part, mixing_indices)\n    donors = tf.transpose(\n        a=tf.reshape(donors, [population_size, 3, -1]), perm=[0, 2, 1])\n    return tf.math.reduce_sum(input_tensor=donors * weights, axis=-1)\n\n  return [_mutant_part(population_part) for population_part in population]", "code_tokens": ["def", "_get_mutants", "(", "population", ",", "population_size", ",", "mixing_indices", ",", "differential_weight", ")", ":", "mixing_indices", "=", "tf", ".", "reshape", "(", "mixing_indices", ",", "[", "-", "1", "]", ")", "weights", "=", "tf", ".", "stack", "(", "[", "1.0", ",", "differential_weight", ",", "-", "differential_weight", "]", ")", "def", "_mutant_part", "(", "population_part", ")", ":", "donors", "=", "tf", ".", "gather", "(", "population_part", ",", "mixing_indices", ")", "donors", "=", "tf", ".", "transpose", "(", "a", "=", "tf", ".", "reshape", "(", "donors", ",", "[", "population_size", ",", "3", ",", "-", "1", "]", ")", ",", "perm", "=", "[", "0", ",", "2", ",", "1", "]", ")", "return", "tf", ".", "math", ".", "reduce_sum", "(", "input_tensor", "=", "donors", "*", "weights", ",", "axis", "=", "-", "1", ")", "return", "[", "_mutant_part", "(", "population_part", ")", "for", "population_part", "in", "population", "]"], "docstring": "Computes the mutatated vectors for each population member.\n\n  Args:\n    population:  Python `list` of `Tensor`s representing the\n      current population vectors. Each `Tensor` must be of the same real dtype.\n      The first dimension of each `Tensor` indexes individual\n      population members. For example, if the population is a list with a\n      single `Tensor` of shape [n, m1, m2], then `n` is the population size and\n      the shape of an individual solution is [m1, m2].\n      If there is more than one element in the population, then each `Tensor`\n      in the list should have the first axis of the same size.\n    population_size: Scalar integer `Tensor`. The size of the population.\n    mixing_indices: `Tensor` of integral dtype and shape [n, 3] where `n` is the\n      number of members in the population. Each element of the `Tensor` must be\n      a valid index into the first dimension of the population (i.e range\n      between `0` and `n-1` inclusive).\n    differential_weight: Real scalar `Tensor`. Must be positive and less than\n      2.0. The parameter controlling the strength of mutation.\n\n  Returns:\n    mutants: `Tensor` or Python `list` of `Tensor`s of the same shape and dtype\n      as the input population. The mutated vectors.", "docstring_tokens": ["Computes", "the", "mutatated", "vectors", "for", "each", "population", "member", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/differential_evolution.py#L674-L709", "partition": "test", "index": 1092, "time": "2018-12-24 03:28:23"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/distribution_layer.py", "func_name": "_event_size", "original_string": "def _event_size(event_shape, name=None):\n  \"\"\"Computes the number of elements in a tensor with shape `event_shape`.\n\n  Args:\n    event_shape: A tensor shape.\n    name: The name to use for the tensor op to compute the number of elements\n      (if such an op needs to be created).\n\n  Returns:\n    event_size: The number of elements in `tensor_shape`.  Returns a numpy int\n    when the number of elements can be computed immediately.  Otherwise, returns\n    a scalar tensor.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'event_size', [event_shape]):\n    event_shape = tf.convert_to_tensor(\n        value=event_shape, dtype=tf.int32, name='event_shape')\n\n    event_shape_const = tf.get_static_value(event_shape)\n    if event_shape_const is not None:\n      return np.prod(event_shape_const)\n    else:\n      return tf.reduce_prod(input_tensor=event_shape)", "language": "python", "code": "def _event_size(event_shape, name=None):\n  \"\"\"Computes the number of elements in a tensor with shape `event_shape`.\n\n  Args:\n    event_shape: A tensor shape.\n    name: The name to use for the tensor op to compute the number of elements\n      (if such an op needs to be created).\n\n  Returns:\n    event_size: The number of elements in `tensor_shape`.  Returns a numpy int\n    when the number of elements can be computed immediately.  Otherwise, returns\n    a scalar tensor.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'event_size', [event_shape]):\n    event_shape = tf.convert_to_tensor(\n        value=event_shape, dtype=tf.int32, name='event_shape')\n\n    event_shape_const = tf.get_static_value(event_shape)\n    if event_shape_const is not None:\n      return np.prod(event_shape_const)\n    else:\n      return tf.reduce_prod(input_tensor=event_shape)", "code_tokens": ["def", "_event_size", "(", "event_shape", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'event_size'", ",", "[", "event_shape", "]", ")", ":", "event_shape", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "event_shape", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "'event_shape'", ")", "event_shape_const", "=", "tf", ".", "get_static_value", "(", "event_shape", ")", "if", "event_shape_const", "is", "not", "None", ":", "return", "np", ".", "prod", "(", "event_shape_const", ")", "else", ":", "return", "tf", ".", "reduce_prod", "(", "input_tensor", "=", "event_shape", ")"], "docstring": "Computes the number of elements in a tensor with shape `event_shape`.\n\n  Args:\n    event_shape: A tensor shape.\n    name: The name to use for the tensor op to compute the number of elements\n      (if such an op needs to be created).\n\n  Returns:\n    event_size: The number of elements in `tensor_shape`.  Returns a numpy int\n    when the number of elements can be computed immediately.  Otherwise, returns\n    a scalar tensor.", "docstring_tokens": ["Computes", "the", "number", "of", "elements", "in", "a", "tensor", "with", "shape", "event_shape", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/distribution_layer.py#L65-L86", "partition": "test", "index": 624, "time": "2018-12-26 11:24:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/distribution_layer.py", "func_name": "MixtureSameFamily.params_size", "original_string": "def params_size(num_components, component_params_size, name=None):\n    \"\"\"Number of `params` needed to create a `MixtureSameFamily` distribution.\n\n    Arguments:\n      num_components: Number of component distributions in the mixture\n        distribution.\n      component_params_size: Number of parameters needed to create a single\n        component distribution.\n      name: The name to use for the op to compute the number of parameters\n        (if such an op needs to be created).\n\n    Returns:\n     params_size: The number of parameters needed to create the mixture\n       distribution.\n    \"\"\"\n    with tf.compat.v1.name_scope(name, 'MixtureSameFamily_params_size',\n                                 [num_components, component_params_size]):\n      num_components = tf.convert_to_tensor(\n          value=num_components, name='num_components', dtype_hint=tf.int32)\n      component_params_size = tf.convert_to_tensor(\n          value=component_params_size, name='component_params_size')\n\n      num_components = dist_util.prefer_static_value(num_components)\n      component_params_size = dist_util.prefer_static_value(\n          component_params_size)\n\n      return num_components + num_components * component_params_size", "language": "python", "code": "def params_size(num_components, component_params_size, name=None):\n    \"\"\"Number of `params` needed to create a `MixtureSameFamily` distribution.\n\n    Arguments:\n      num_components: Number of component distributions in the mixture\n        distribution.\n      component_params_size: Number of parameters needed to create a single\n        component distribution.\n      name: The name to use for the op to compute the number of parameters\n        (if such an op needs to be created).\n\n    Returns:\n     params_size: The number of parameters needed to create the mixture\n       distribution.\n    \"\"\"\n    with tf.compat.v1.name_scope(name, 'MixtureSameFamily_params_size',\n                                 [num_components, component_params_size]):\n      num_components = tf.convert_to_tensor(\n          value=num_components, name='num_components', dtype_hint=tf.int32)\n      component_params_size = tf.convert_to_tensor(\n          value=component_params_size, name='component_params_size')\n\n      num_components = dist_util.prefer_static_value(num_components)\n      component_params_size = dist_util.prefer_static_value(\n          component_params_size)\n\n      return num_components + num_components * component_params_size", "code_tokens": ["def", "params_size", "(", "num_components", ",", "component_params_size", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'MixtureSameFamily_params_size'", ",", "[", "num_components", ",", "component_params_size", "]", ")", ":", "num_components", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "num_components", ",", "name", "=", "'num_components'", ",", "dtype_hint", "=", "tf", ".", "int32", ")", "component_params_size", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "component_params_size", ",", "name", "=", "'component_params_size'", ")", "num_components", "=", "dist_util", ".", "prefer_static_value", "(", "num_components", ")", "component_params_size", "=", "dist_util", ".", "prefer_static_value", "(", "component_params_size", ")", "return", "num_components", "+", "num_components", "*", "component_params_size"], "docstring": "Number of `params` needed to create a `MixtureSameFamily` distribution.\n\n    Arguments:\n      num_components: Number of component distributions in the mixture\n        distribution.\n      component_params_size: Number of parameters needed to create a single\n        component distribution.\n      name: The name to use for the op to compute the number of parameters\n        (if such an op needs to be created).\n\n    Returns:\n     params_size: The number of parameters needed to create the mixture\n       distribution.", "docstring_tokens": ["Number", "of", "params", "needed", "to", "create", "a", "MixtureSameFamily", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/distribution_layer.py#L1451-L1477", "partition": "test", "index": 627, "time": "2019-01-04 13:37:17"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/forecast.py", "func_name": "one_step_predictive", "original_string": "def one_step_predictive(model, observed_time_series, parameter_samples):\n  \"\"\"Compute one-step-ahead predictive distributions for all timesteps.\n\n  Given samples from the posterior over parameters, return the predictive\n  distribution over observations at each time `T`, given observations up\n  through time `T-1`.\n\n  Args:\n    model: An instance of `StructuralTimeSeries` representing a\n      time-series model. This represents a joint distribution over\n      time-series and their parameters with batch shape `[b1, ..., bN]`.\n    observed_time_series: `float` `Tensor` of shape\n      `concat([sample_shape, model.batch_shape, [num_timesteps, 1]]) where\n      `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`\n      dimension may (optionally) be omitted if `num_timesteps > 1`. May\n      optionally be an instance of `tfp.sts.MaskedTimeSeries` including a\n      mask `Tensor` to encode the locations of missing observations.\n    parameter_samples: Python `list` of `Tensors` representing posterior samples\n      of model parameters, with shapes `[concat([[num_posterior_draws],\n      param.prior.batch_shape, param.prior.event_shape]) for param in\n      model.parameters]`. This may optionally also be a map (Python `dict`) of\n      parameter names to `Tensor` values.\n\n  Returns:\n    forecast_dist: a `tfd.MixtureSameFamily` instance with event shape\n      [num_timesteps] and\n      batch shape `concat([sample_shape, model.batch_shape])`, with\n      `num_posterior_draws` mixture components. The `t`th step represents the\n      forecast distribution `p(observed_time_series[t] |\n      observed_time_series[0:t-1], parameter_samples)`.\n\n  #### Examples\n\n  Suppose we've built a model and fit it to data using HMC:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)\n  ```\n\n  Passing the posterior samples into `one_step_predictive`, we construct a\n  one-step-ahead predictive distribution:\n\n  ```python\n    one_step_predictive_dist = tfp.sts.one_step_predictive(\n      model, observed_time_series, parameter_samples=samples)\n\n    predictive_means = one_step_predictive_dist.mean()\n    predictive_scales = one_step_predictive_dist.stddev()\n  ```\n\n  If using variational inference instead of HMC, we'd construct a forecast using\n  samples from the variational posterior:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series)\n\n    # OMITTED: take steps to optimize variational loss\n\n    samples = {k: q.sample(30) for (k, q) in variational_distributions.items()}\n    one_step_predictive_dist = tfp.sts.one_step_predictive(\n      model, observed_time_series, parameter_samples=samples)\n  ```\n\n  We can visualize the forecast by plotting:\n\n  ```python\n    from matplotlib import pylab as plt\n    def plot_one_step_predictive(observed_time_series,\n                                 forecast_mean,\n                                 forecast_scale):\n      plt.figure(figsize=(12, 6))\n      num_timesteps = forecast_mean.shape[-1]\n      c1, c2 = (0.12, 0.47, 0.71), (1.0, 0.5, 0.05)\n      plt.plot(observed_time_series, label=\"observed time series\", color=c1)\n      plt.plot(forecast_mean, label=\"one-step prediction\", color=c2)\n      plt.fill_between(np.arange(num_timesteps),\n                       forecast_mean - 2 * forecast_scale,\n                       forecast_mean + 2 * forecast_scale,\n                       alpha=0.1, color=c2)\n      plt.legend()\n\n    plot_one_step_predictive(observed_time_series,\n                             forecast_mean=predictive_means,\n                             forecast_scale=predictive_scales)\n  ```\n\n  To detect anomalous timesteps, we check whether the observed value at each\n  step is within a 95% predictive interval, i.e., two standard deviations from\n  the mean:\n\n  ```python\n    z_scores = ((observed_time_series[..., 1:] - predictive_means[..., :-1])\n                 / predictive_scales[..., :-1])\n    anomalous_timesteps = tf.boolean_mask(\n        tf.range(1, num_timesteps),\n        tf.abs(z_scores) > 2.0)\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      'one_step_predictive', values=[observed_time_series, parameter_samples]):\n\n    [\n        observed_time_series,\n        is_missing\n    ] = sts_util.canonicalize_observed_time_series_with_mask(\n        observed_time_series)\n\n    # Run filtering over the training timesteps to extract the\n    # predictive means and variances.\n    num_timesteps = dist_util.prefer_static_value(\n        tf.shape(input=observed_time_series))[-2]\n    lgssm = model.make_state_space_model(\n        num_timesteps=num_timesteps, param_vals=parameter_samples)\n    (_, _, _, _, _, observation_means, observation_covs\n    ) = lgssm.forward_filter(observed_time_series, mask=is_missing)\n\n    # Squeeze dims to convert from LGSSM's event shape `[num_timesteps, 1]`\n    # to a scalar time series.\n    return sts_util.mix_over_posterior_draws(\n        means=observation_means[..., 0],\n        variances=observation_covs[..., 0, 0])", "language": "python", "code": "def one_step_predictive(model, observed_time_series, parameter_samples):\n  \"\"\"Compute one-step-ahead predictive distributions for all timesteps.\n\n  Given samples from the posterior over parameters, return the predictive\n  distribution over observations at each time `T`, given observations up\n  through time `T-1`.\n\n  Args:\n    model: An instance of `StructuralTimeSeries` representing a\n      time-series model. This represents a joint distribution over\n      time-series and their parameters with batch shape `[b1, ..., bN]`.\n    observed_time_series: `float` `Tensor` of shape\n      `concat([sample_shape, model.batch_shape, [num_timesteps, 1]]) where\n      `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`\n      dimension may (optionally) be omitted if `num_timesteps > 1`. May\n      optionally be an instance of `tfp.sts.MaskedTimeSeries` including a\n      mask `Tensor` to encode the locations of missing observations.\n    parameter_samples: Python `list` of `Tensors` representing posterior samples\n      of model parameters, with shapes `[concat([[num_posterior_draws],\n      param.prior.batch_shape, param.prior.event_shape]) for param in\n      model.parameters]`. This may optionally also be a map (Python `dict`) of\n      parameter names to `Tensor` values.\n\n  Returns:\n    forecast_dist: a `tfd.MixtureSameFamily` instance with event shape\n      [num_timesteps] and\n      batch shape `concat([sample_shape, model.batch_shape])`, with\n      `num_posterior_draws` mixture components. The `t`th step represents the\n      forecast distribution `p(observed_time_series[t] |\n      observed_time_series[0:t-1], parameter_samples)`.\n\n  #### Examples\n\n  Suppose we've built a model and fit it to data using HMC:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)\n  ```\n\n  Passing the posterior samples into `one_step_predictive`, we construct a\n  one-step-ahead predictive distribution:\n\n  ```python\n    one_step_predictive_dist = tfp.sts.one_step_predictive(\n      model, observed_time_series, parameter_samples=samples)\n\n    predictive_means = one_step_predictive_dist.mean()\n    predictive_scales = one_step_predictive_dist.stddev()\n  ```\n\n  If using variational inference instead of HMC, we'd construct a forecast using\n  samples from the variational posterior:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series)\n\n    # OMITTED: take steps to optimize variational loss\n\n    samples = {k: q.sample(30) for (k, q) in variational_distributions.items()}\n    one_step_predictive_dist = tfp.sts.one_step_predictive(\n      model, observed_time_series, parameter_samples=samples)\n  ```\n\n  We can visualize the forecast by plotting:\n\n  ```python\n    from matplotlib import pylab as plt\n    def plot_one_step_predictive(observed_time_series,\n                                 forecast_mean,\n                                 forecast_scale):\n      plt.figure(figsize=(12, 6))\n      num_timesteps = forecast_mean.shape[-1]\n      c1, c2 = (0.12, 0.47, 0.71), (1.0, 0.5, 0.05)\n      plt.plot(observed_time_series, label=\"observed time series\", color=c1)\n      plt.plot(forecast_mean, label=\"one-step prediction\", color=c2)\n      plt.fill_between(np.arange(num_timesteps),\n                       forecast_mean - 2 * forecast_scale,\n                       forecast_mean + 2 * forecast_scale,\n                       alpha=0.1, color=c2)\n      plt.legend()\n\n    plot_one_step_predictive(observed_time_series,\n                             forecast_mean=predictive_means,\n                             forecast_scale=predictive_scales)\n  ```\n\n  To detect anomalous timesteps, we check whether the observed value at each\n  step is within a 95% predictive interval, i.e., two standard deviations from\n  the mean:\n\n  ```python\n    z_scores = ((observed_time_series[..., 1:] - predictive_means[..., :-1])\n                 / predictive_scales[..., :-1])\n    anomalous_timesteps = tf.boolean_mask(\n        tf.range(1, num_timesteps),\n        tf.abs(z_scores) > 2.0)\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      'one_step_predictive', values=[observed_time_series, parameter_samples]):\n\n    [\n        observed_time_series,\n        is_missing\n    ] = sts_util.canonicalize_observed_time_series_with_mask(\n        observed_time_series)\n\n    # Run filtering over the training timesteps to extract the\n    # predictive means and variances.\n    num_timesteps = dist_util.prefer_static_value(\n        tf.shape(input=observed_time_series))[-2]\n    lgssm = model.make_state_space_model(\n        num_timesteps=num_timesteps, param_vals=parameter_samples)\n    (_, _, _, _, _, observation_means, observation_covs\n    ) = lgssm.forward_filter(observed_time_series, mask=is_missing)\n\n    # Squeeze dims to convert from LGSSM's event shape `[num_timesteps, 1]`\n    # to a scalar time series.\n    return sts_util.mix_over_posterior_draws(\n        means=observation_means[..., 0],\n        variances=observation_covs[..., 0, 0])", "code_tokens": ["def", "one_step_predictive", "(", "model", ",", "observed_time_series", ",", "parameter_samples", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'one_step_predictive'", ",", "values", "=", "[", "observed_time_series", ",", "parameter_samples", "]", ")", ":", "[", "observed_time_series", ",", "is_missing", "]", "=", "sts_util", ".", "canonicalize_observed_time_series_with_mask", "(", "observed_time_series", ")", "# Run filtering over the training timesteps to extract the", "# predictive means and variances.", "num_timesteps", "=", "dist_util", ".", "prefer_static_value", "(", "tf", ".", "shape", "(", "input", "=", "observed_time_series", ")", ")", "[", "-", "2", "]", "lgssm", "=", "model", ".", "make_state_space_model", "(", "num_timesteps", "=", "num_timesteps", ",", "param_vals", "=", "parameter_samples", ")", "(", "_", ",", "_", ",", "_", ",", "_", ",", "_", ",", "observation_means", ",", "observation_covs", ")", "=", "lgssm", ".", "forward_filter", "(", "observed_time_series", ",", "mask", "=", "is_missing", ")", "# Squeeze dims to convert from LGSSM's event shape `[num_timesteps, 1]`", "# to a scalar time series.", "return", "sts_util", ".", "mix_over_posterior_draws", "(", "means", "=", "observation_means", "[", "...", ",", "0", "]", ",", "variances", "=", "observation_covs", "[", "...", ",", "0", ",", "0", "]", ")"], "docstring": "Compute one-step-ahead predictive distributions for all timesteps.\n\n  Given samples from the posterior over parameters, return the predictive\n  distribution over observations at each time `T`, given observations up\n  through time `T-1`.\n\n  Args:\n    model: An instance of `StructuralTimeSeries` representing a\n      time-series model. This represents a joint distribution over\n      time-series and their parameters with batch shape `[b1, ..., bN]`.\n    observed_time_series: `float` `Tensor` of shape\n      `concat([sample_shape, model.batch_shape, [num_timesteps, 1]]) where\n      `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`\n      dimension may (optionally) be omitted if `num_timesteps > 1`. May\n      optionally be an instance of `tfp.sts.MaskedTimeSeries` including a\n      mask `Tensor` to encode the locations of missing observations.\n    parameter_samples: Python `list` of `Tensors` representing posterior samples\n      of model parameters, with shapes `[concat([[num_posterior_draws],\n      param.prior.batch_shape, param.prior.event_shape]) for param in\n      model.parameters]`. This may optionally also be a map (Python `dict`) of\n      parameter names to `Tensor` values.\n\n  Returns:\n    forecast_dist: a `tfd.MixtureSameFamily` instance with event shape\n      [num_timesteps] and\n      batch shape `concat([sample_shape, model.batch_shape])`, with\n      `num_posterior_draws` mixture components. The `t`th step represents the\n      forecast distribution `p(observed_time_series[t] |\n      observed_time_series[0:t-1], parameter_samples)`.\n\n  #### Examples\n\n  Suppose we've built a model and fit it to data using HMC:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)\n  ```\n\n  Passing the posterior samples into `one_step_predictive`, we construct a\n  one-step-ahead predictive distribution:\n\n  ```python\n    one_step_predictive_dist = tfp.sts.one_step_predictive(\n      model, observed_time_series, parameter_samples=samples)\n\n    predictive_means = one_step_predictive_dist.mean()\n    predictive_scales = one_step_predictive_dist.stddev()\n  ```\n\n  If using variational inference instead of HMC, we'd construct a forecast using\n  samples from the variational posterior:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series)\n\n    # OMITTED: take steps to optimize variational loss\n\n    samples = {k: q.sample(30) for (k, q) in variational_distributions.items()}\n    one_step_predictive_dist = tfp.sts.one_step_predictive(\n      model, observed_time_series, parameter_samples=samples)\n  ```\n\n  We can visualize the forecast by plotting:\n\n  ```python\n    from matplotlib import pylab as plt\n    def plot_one_step_predictive(observed_time_series,\n                                 forecast_mean,\n                                 forecast_scale):\n      plt.figure(figsize=(12, 6))\n      num_timesteps = forecast_mean.shape[-1]\n      c1, c2 = (0.12, 0.47, 0.71), (1.0, 0.5, 0.05)\n      plt.plot(observed_time_series, label=\"observed time series\", color=c1)\n      plt.plot(forecast_mean, label=\"one-step prediction\", color=c2)\n      plt.fill_between(np.arange(num_timesteps),\n                       forecast_mean - 2 * forecast_scale,\n                       forecast_mean + 2 * forecast_scale,\n                       alpha=0.1, color=c2)\n      plt.legend()\n\n    plot_one_step_predictive(observed_time_series,\n                             forecast_mean=predictive_means,\n                             forecast_scale=predictive_scales)\n  ```\n\n  To detect anomalous timesteps, we check whether the observed value at each\n  step is within a 95% predictive interval, i.e., two standard deviations from\n  the mean:\n\n  ```python\n    z_scores = ((observed_time_series[..., 1:] - predictive_means[..., :-1])\n                 / predictive_scales[..., :-1])\n    anomalous_timesteps = tf.boolean_mask(\n        tf.range(1, num_timesteps),\n        tf.abs(z_scores) > 2.0)\n  ```", "docstring_tokens": ["Compute", "one", "-", "step", "-", "ahead", "predictive", "distributions", "for", "all", "timesteps", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/forecast.py#L35-L169", "partition": "test", "index": 619, "time": "2019-01-04 15:18:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/forecast.py", "func_name": "forecast", "original_string": "def forecast(model,\n             observed_time_series,\n             parameter_samples,\n             num_steps_forecast):\n  \"\"\"Construct predictive distribution over future observations.\n\n  Given samples from the posterior over parameters, return the predictive\n  distribution over future observations for num_steps_forecast timesteps.\n\n  Args:\n    model: An instance of `StructuralTimeSeries` representing a\n      time-series model. This represents a joint distribution over\n      time-series and their parameters with batch shape `[b1, ..., bN]`.\n    observed_time_series: `float` `Tensor` of shape\n      `concat([sample_shape, model.batch_shape, [num_timesteps, 1]])` where\n      `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`\n      dimension may (optionally) be omitted if `num_timesteps > 1`. May\n      optionally be an instance of `tfp.sts.MaskedTimeSeries` including a\n      mask `Tensor` to encode the locations of missing observations.\n    parameter_samples: Python `list` of `Tensors` representing posterior samples\n      of model parameters, with shapes `[concat([[num_posterior_draws],\n      param.prior.batch_shape, param.prior.event_shape]) for param in\n      model.parameters]`. This may optionally also be a map (Python `dict`) of\n      parameter names to `Tensor` values.\n    num_steps_forecast: scalar `int` `Tensor` number of steps to forecast.\n\n  Returns:\n    forecast_dist: a `tfd.MixtureSameFamily` instance with event shape\n      [num_steps_forecast, 1] and batch shape\n      `concat([sample_shape, model.batch_shape])`, with `num_posterior_draws`\n      mixture components.\n\n  #### Examples\n\n  Suppose we've built a model and fit it to data using HMC:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)\n  ```\n\n  Passing the posterior samples into `forecast`, we construct a forecast\n  distribution:\n\n  ```python\n    forecast_dist = tfp.sts.forecast(model, observed_time_series,\n                                     parameter_samples=samples,\n                                     num_steps_forecast=50)\n\n    forecast_mean = forecast_dist.mean()[..., 0]  # shape: [50]\n    forecast_scale = forecast_dist.stddev()[..., 0]  # shape: [50]\n    forecast_samples = forecast_dist.sample(10)[..., 0]  # shape: [10, 50]\n  ```\n\n  If using variational inference instead of HMC, we'd construct a forecast using\n  samples from the variational posterior:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series)\n\n    # OMITTED: take steps to optimize variational loss\n\n    samples = {k: q.sample(30) for (k, q) in variational_distributions.items()}\n    forecast_dist = tfp.sts.forecast(model, observed_time_series,\n                                         parameter_samples=samples,\n                                         num_steps_forecast=50)\n  ```\n\n  We can visualize the forecast by plotting:\n\n  ```python\n    from matplotlib import pylab as plt\n    def plot_forecast(observed_time_series,\n                      forecast_mean,\n                      forecast_scale,\n                      forecast_samples):\n      plt.figure(figsize=(12, 6))\n\n      num_steps = observed_time_series.shape[-1]\n      num_steps_forecast = forecast_mean.shape[-1]\n      num_steps_train = num_steps - num_steps_forecast\n\n      c1, c2 = (0.12, 0.47, 0.71), (1.0, 0.5, 0.05)\n      plt.plot(np.arange(num_steps), observed_time_series,\n               lw=2, color=c1, label='ground truth')\n\n      forecast_steps = np.arange(num_steps_train,\n                       num_steps_train+num_steps_forecast)\n      plt.plot(forecast_steps, forecast_samples.T, lw=1, color=c2, alpha=0.1)\n      plt.plot(forecast_steps, forecast_mean, lw=2, ls='--', color=c2,\n               label='forecast')\n      plt.fill_between(forecast_steps,\n                       forecast_mean - 2 * forecast_scale,\n                       forecast_mean + 2 * forecast_scale, color=c2, alpha=0.2)\n\n      plt.xlim([0, num_steps])\n      plt.legend()\n\n    plot_forecast(observed_time_series,\n                  forecast_mean=forecast_mean,\n                  forecast_scale=forecast_scale,\n                  forecast_samples=forecast_samples)\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      'forecast',\n      values=[observed_time_series, parameter_samples, num_steps_forecast]):\n    [\n        observed_time_series,\n        mask\n    ] = sts_util.canonicalize_observed_time_series_with_mask(\n        observed_time_series)\n\n    # Run filtering over the observed timesteps to extract the\n    # latent state posterior at timestep T+1 (i.e., the final\n    # filtering distribution, pushed through the transition model).\n    # This is the prior for the forecast model (\"today's prior\n    # is yesterday's posterior\").\n    num_observed_steps = dist_util.prefer_static_value(\n        tf.shape(input=observed_time_series))[-2]\n    observed_data_ssm = model.make_state_space_model(\n        num_timesteps=num_observed_steps, param_vals=parameter_samples)\n    (_, _, _, predictive_means, predictive_covs, _, _\n    ) = observed_data_ssm.forward_filter(observed_time_series, mask=mask)\n\n    # Build a batch of state-space models over the forecast period. Because\n    # we'll use MixtureSameFamily to mix over the posterior draws, we need to\n    # do some shenanigans to move the `[num_posterior_draws]` batch dimension\n    # from the leftmost to the rightmost side of the model's batch shape.\n    # TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an\n    # arbitrary axis, and eliminate `move_dimension` calls here.\n    parameter_samples = model._canonicalize_param_vals_as_map(parameter_samples)  # pylint: disable=protected-access\n    parameter_samples_with_reordered_batch_dimension = {\n        param.name: dist_util.move_dimension(\n            parameter_samples[param.name],\n            0, -(1 + _prefer_static_event_ndims(param.prior)))\n        for param in model.parameters}\n    forecast_prior = tfd.MultivariateNormalFullCovariance(\n        loc=dist_util.move_dimension(predictive_means[..., -1, :], 0, -2),\n        covariance_matrix=dist_util.move_dimension(\n            predictive_covs[..., -1, :, :], 0, -3))\n\n    # Ugly hack: because we moved `num_posterior_draws` to the trailing (rather\n    # than leading) dimension of parameters, the parameter batch shapes no\n    # longer broadcast against the `constant_offset` attribute used in `sts.Sum`\n    # models. We fix this by manually adding an extra broadcasting dim to\n    # `constant_offset` if present.\n    # The root cause of this hack is that we mucked with param dimensions above\n    # and are now passing params that are 'invalid' in the sense that they don't\n    # match the shapes of the model's param priors. The fix (as above) will be\n    # to update MixtureSameFamily so we can avoid changing param dimensions\n    # altogether.\n    # TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an\n    # arbitrary axis, and eliminate this hack.\n    kwargs = {}\n    if hasattr(model, 'constant_offset'):\n      kwargs['constant_offset'] = tf.convert_to_tensor(\n          value=model.constant_offset,\n          dtype=forecast_prior.dtype)[..., tf.newaxis]\n\n    # We assume that any STS model that has a `constant_offset` attribute\n    # will allow it to be overridden as a kwarg. This is currently just\n    # `sts.Sum`.\n    # TODO(b/120245392): when kwargs hack is removed, switch back to calling\n    # the public version of `_make_state_space_model`.\n    forecast_ssm = model._make_state_space_model(  # pylint: disable=protected-access\n        num_timesteps=num_steps_forecast,\n        param_map=parameter_samples_with_reordered_batch_dimension,\n        initial_state_prior=forecast_prior,\n        initial_step=num_observed_steps,\n        **kwargs)\n\n    num_posterior_draws = dist_util.prefer_static_value(\n        forecast_ssm.batch_shape_tensor())[-1]\n    return tfd.MixtureSameFamily(\n        mixture_distribution=tfd.Categorical(\n            logits=tf.zeros([num_posterior_draws], dtype=forecast_ssm.dtype)),\n        components_distribution=forecast_ssm)", "language": "python", "code": "def forecast(model,\n             observed_time_series,\n             parameter_samples,\n             num_steps_forecast):\n  \"\"\"Construct predictive distribution over future observations.\n\n  Given samples from the posterior over parameters, return the predictive\n  distribution over future observations for num_steps_forecast timesteps.\n\n  Args:\n    model: An instance of `StructuralTimeSeries` representing a\n      time-series model. This represents a joint distribution over\n      time-series and their parameters with batch shape `[b1, ..., bN]`.\n    observed_time_series: `float` `Tensor` of shape\n      `concat([sample_shape, model.batch_shape, [num_timesteps, 1]])` where\n      `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`\n      dimension may (optionally) be omitted if `num_timesteps > 1`. May\n      optionally be an instance of `tfp.sts.MaskedTimeSeries` including a\n      mask `Tensor` to encode the locations of missing observations.\n    parameter_samples: Python `list` of `Tensors` representing posterior samples\n      of model parameters, with shapes `[concat([[num_posterior_draws],\n      param.prior.batch_shape, param.prior.event_shape]) for param in\n      model.parameters]`. This may optionally also be a map (Python `dict`) of\n      parameter names to `Tensor` values.\n    num_steps_forecast: scalar `int` `Tensor` number of steps to forecast.\n\n  Returns:\n    forecast_dist: a `tfd.MixtureSameFamily` instance with event shape\n      [num_steps_forecast, 1] and batch shape\n      `concat([sample_shape, model.batch_shape])`, with `num_posterior_draws`\n      mixture components.\n\n  #### Examples\n\n  Suppose we've built a model and fit it to data using HMC:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)\n  ```\n\n  Passing the posterior samples into `forecast`, we construct a forecast\n  distribution:\n\n  ```python\n    forecast_dist = tfp.sts.forecast(model, observed_time_series,\n                                     parameter_samples=samples,\n                                     num_steps_forecast=50)\n\n    forecast_mean = forecast_dist.mean()[..., 0]  # shape: [50]\n    forecast_scale = forecast_dist.stddev()[..., 0]  # shape: [50]\n    forecast_samples = forecast_dist.sample(10)[..., 0]  # shape: [10, 50]\n  ```\n\n  If using variational inference instead of HMC, we'd construct a forecast using\n  samples from the variational posterior:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series)\n\n    # OMITTED: take steps to optimize variational loss\n\n    samples = {k: q.sample(30) for (k, q) in variational_distributions.items()}\n    forecast_dist = tfp.sts.forecast(model, observed_time_series,\n                                         parameter_samples=samples,\n                                         num_steps_forecast=50)\n  ```\n\n  We can visualize the forecast by plotting:\n\n  ```python\n    from matplotlib import pylab as plt\n    def plot_forecast(observed_time_series,\n                      forecast_mean,\n                      forecast_scale,\n                      forecast_samples):\n      plt.figure(figsize=(12, 6))\n\n      num_steps = observed_time_series.shape[-1]\n      num_steps_forecast = forecast_mean.shape[-1]\n      num_steps_train = num_steps - num_steps_forecast\n\n      c1, c2 = (0.12, 0.47, 0.71), (1.0, 0.5, 0.05)\n      plt.plot(np.arange(num_steps), observed_time_series,\n               lw=2, color=c1, label='ground truth')\n\n      forecast_steps = np.arange(num_steps_train,\n                       num_steps_train+num_steps_forecast)\n      plt.plot(forecast_steps, forecast_samples.T, lw=1, color=c2, alpha=0.1)\n      plt.plot(forecast_steps, forecast_mean, lw=2, ls='--', color=c2,\n               label='forecast')\n      plt.fill_between(forecast_steps,\n                       forecast_mean - 2 * forecast_scale,\n                       forecast_mean + 2 * forecast_scale, color=c2, alpha=0.2)\n\n      plt.xlim([0, num_steps])\n      plt.legend()\n\n    plot_forecast(observed_time_series,\n                  forecast_mean=forecast_mean,\n                  forecast_scale=forecast_scale,\n                  forecast_samples=forecast_samples)\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      'forecast',\n      values=[observed_time_series, parameter_samples, num_steps_forecast]):\n    [\n        observed_time_series,\n        mask\n    ] = sts_util.canonicalize_observed_time_series_with_mask(\n        observed_time_series)\n\n    # Run filtering over the observed timesteps to extract the\n    # latent state posterior at timestep T+1 (i.e., the final\n    # filtering distribution, pushed through the transition model).\n    # This is the prior for the forecast model (\"today's prior\n    # is yesterday's posterior\").\n    num_observed_steps = dist_util.prefer_static_value(\n        tf.shape(input=observed_time_series))[-2]\n    observed_data_ssm = model.make_state_space_model(\n        num_timesteps=num_observed_steps, param_vals=parameter_samples)\n    (_, _, _, predictive_means, predictive_covs, _, _\n    ) = observed_data_ssm.forward_filter(observed_time_series, mask=mask)\n\n    # Build a batch of state-space models over the forecast period. Because\n    # we'll use MixtureSameFamily to mix over the posterior draws, we need to\n    # do some shenanigans to move the `[num_posterior_draws]` batch dimension\n    # from the leftmost to the rightmost side of the model's batch shape.\n    # TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an\n    # arbitrary axis, and eliminate `move_dimension` calls here.\n    parameter_samples = model._canonicalize_param_vals_as_map(parameter_samples)  # pylint: disable=protected-access\n    parameter_samples_with_reordered_batch_dimension = {\n        param.name: dist_util.move_dimension(\n            parameter_samples[param.name],\n            0, -(1 + _prefer_static_event_ndims(param.prior)))\n        for param in model.parameters}\n    forecast_prior = tfd.MultivariateNormalFullCovariance(\n        loc=dist_util.move_dimension(predictive_means[..., -1, :], 0, -2),\n        covariance_matrix=dist_util.move_dimension(\n            predictive_covs[..., -1, :, :], 0, -3))\n\n    # Ugly hack: because we moved `num_posterior_draws` to the trailing (rather\n    # than leading) dimension of parameters, the parameter batch shapes no\n    # longer broadcast against the `constant_offset` attribute used in `sts.Sum`\n    # models. We fix this by manually adding an extra broadcasting dim to\n    # `constant_offset` if present.\n    # The root cause of this hack is that we mucked with param dimensions above\n    # and are now passing params that are 'invalid' in the sense that they don't\n    # match the shapes of the model's param priors. The fix (as above) will be\n    # to update MixtureSameFamily so we can avoid changing param dimensions\n    # altogether.\n    # TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an\n    # arbitrary axis, and eliminate this hack.\n    kwargs = {}\n    if hasattr(model, 'constant_offset'):\n      kwargs['constant_offset'] = tf.convert_to_tensor(\n          value=model.constant_offset,\n          dtype=forecast_prior.dtype)[..., tf.newaxis]\n\n    # We assume that any STS model that has a `constant_offset` attribute\n    # will allow it to be overridden as a kwarg. This is currently just\n    # `sts.Sum`.\n    # TODO(b/120245392): when kwargs hack is removed, switch back to calling\n    # the public version of `_make_state_space_model`.\n    forecast_ssm = model._make_state_space_model(  # pylint: disable=protected-access\n        num_timesteps=num_steps_forecast,\n        param_map=parameter_samples_with_reordered_batch_dimension,\n        initial_state_prior=forecast_prior,\n        initial_step=num_observed_steps,\n        **kwargs)\n\n    num_posterior_draws = dist_util.prefer_static_value(\n        forecast_ssm.batch_shape_tensor())[-1]\n    return tfd.MixtureSameFamily(\n        mixture_distribution=tfd.Categorical(\n            logits=tf.zeros([num_posterior_draws], dtype=forecast_ssm.dtype)),\n        components_distribution=forecast_ssm)", "code_tokens": ["def", "forecast", "(", "model", ",", "observed_time_series", ",", "parameter_samples", ",", "num_steps_forecast", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'forecast'", ",", "values", "=", "[", "observed_time_series", ",", "parameter_samples", ",", "num_steps_forecast", "]", ")", ":", "[", "observed_time_series", ",", "mask", "]", "=", "sts_util", ".", "canonicalize_observed_time_series_with_mask", "(", "observed_time_series", ")", "# Run filtering over the observed timesteps to extract the", "# latent state posterior at timestep T+1 (i.e., the final", "# filtering distribution, pushed through the transition model).", "# This is the prior for the forecast model (\"today's prior", "# is yesterday's posterior\").", "num_observed_steps", "=", "dist_util", ".", "prefer_static_value", "(", "tf", ".", "shape", "(", "input", "=", "observed_time_series", ")", ")", "[", "-", "2", "]", "observed_data_ssm", "=", "model", ".", "make_state_space_model", "(", "num_timesteps", "=", "num_observed_steps", ",", "param_vals", "=", "parameter_samples", ")", "(", "_", ",", "_", ",", "_", ",", "predictive_means", ",", "predictive_covs", ",", "_", ",", "_", ")", "=", "observed_data_ssm", ".", "forward_filter", "(", "observed_time_series", ",", "mask", "=", "mask", ")", "# Build a batch of state-space models over the forecast period. Because", "# we'll use MixtureSameFamily to mix over the posterior draws, we need to", "# do some shenanigans to move the `[num_posterior_draws]` batch dimension", "# from the leftmost to the rightmost side of the model's batch shape.", "# TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an", "# arbitrary axis, and eliminate `move_dimension` calls here.", "parameter_samples", "=", "model", ".", "_canonicalize_param_vals_as_map", "(", "parameter_samples", ")", "# pylint: disable=protected-access", "parameter_samples_with_reordered_batch_dimension", "=", "{", "param", ".", "name", ":", "dist_util", ".", "move_dimension", "(", "parameter_samples", "[", "param", ".", "name", "]", ",", "0", ",", "-", "(", "1", "+", "_prefer_static_event_ndims", "(", "param", ".", "prior", ")", ")", ")", "for", "param", "in", "model", ".", "parameters", "}", "forecast_prior", "=", "tfd", ".", "MultivariateNormalFullCovariance", "(", "loc", "=", "dist_util", ".", "move_dimension", "(", "predictive_means", "[", "...", ",", "-", "1", ",", ":", "]", ",", "0", ",", "-", "2", ")", ",", "covariance_matrix", "=", "dist_util", ".", "move_dimension", "(", "predictive_covs", "[", "...", ",", "-", "1", ",", ":", ",", ":", "]", ",", "0", ",", "-", "3", ")", ")", "# Ugly hack: because we moved `num_posterior_draws` to the trailing (rather", "# than leading) dimension of parameters, the parameter batch shapes no", "# longer broadcast against the `constant_offset` attribute used in `sts.Sum`", "# models. We fix this by manually adding an extra broadcasting dim to", "# `constant_offset` if present.", "# The root cause of this hack is that we mucked with param dimensions above", "# and are now passing params that are 'invalid' in the sense that they don't", "# match the shapes of the model's param priors. The fix (as above) will be", "# to update MixtureSameFamily so we can avoid changing param dimensions", "# altogether.", "# TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an", "# arbitrary axis, and eliminate this hack.", "kwargs", "=", "{", "}", "if", "hasattr", "(", "model", ",", "'constant_offset'", ")", ":", "kwargs", "[", "'constant_offset'", "]", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "model", ".", "constant_offset", ",", "dtype", "=", "forecast_prior", ".", "dtype", ")", "[", "...", ",", "tf", ".", "newaxis", "]", "# We assume that any STS model that has a `constant_offset` attribute", "# will allow it to be overridden as a kwarg. This is currently just", "# `sts.Sum`.", "# TODO(b/120245392): when kwargs hack is removed, switch back to calling", "# the public version of `_make_state_space_model`.", "forecast_ssm", "=", "model", ".", "_make_state_space_model", "(", "# pylint: disable=protected-access", "num_timesteps", "=", "num_steps_forecast", ",", "param_map", "=", "parameter_samples_with_reordered_batch_dimension", ",", "initial_state_prior", "=", "forecast_prior", ",", "initial_step", "=", "num_observed_steps", ",", "*", "*", "kwargs", ")", "num_posterior_draws", "=", "dist_util", ".", "prefer_static_value", "(", "forecast_ssm", ".", "batch_shape_tensor", "(", ")", ")", "[", "-", "1", "]", "return", "tfd", ".", "MixtureSameFamily", "(", "mixture_distribution", "=", "tfd", ".", "Categorical", "(", "logits", "=", "tf", ".", "zeros", "(", "[", "num_posterior_draws", "]", ",", "dtype", "=", "forecast_ssm", ".", "dtype", ")", ")", ",", "components_distribution", "=", "forecast_ssm", ")"], "docstring": "Construct predictive distribution over future observations.\n\n  Given samples from the posterior over parameters, return the predictive\n  distribution over future observations for num_steps_forecast timesteps.\n\n  Args:\n    model: An instance of `StructuralTimeSeries` representing a\n      time-series model. This represents a joint distribution over\n      time-series and their parameters with batch shape `[b1, ..., bN]`.\n    observed_time_series: `float` `Tensor` of shape\n      `concat([sample_shape, model.batch_shape, [num_timesteps, 1]])` where\n      `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`\n      dimension may (optionally) be omitted if `num_timesteps > 1`. May\n      optionally be an instance of `tfp.sts.MaskedTimeSeries` including a\n      mask `Tensor` to encode the locations of missing observations.\n    parameter_samples: Python `list` of `Tensors` representing posterior samples\n      of model parameters, with shapes `[concat([[num_posterior_draws],\n      param.prior.batch_shape, param.prior.event_shape]) for param in\n      model.parameters]`. This may optionally also be a map (Python `dict`) of\n      parameter names to `Tensor` values.\n    num_steps_forecast: scalar `int` `Tensor` number of steps to forecast.\n\n  Returns:\n    forecast_dist: a `tfd.MixtureSameFamily` instance with event shape\n      [num_steps_forecast, 1] and batch shape\n      `concat([sample_shape, model.batch_shape])`, with `num_posterior_draws`\n      mixture components.\n\n  #### Examples\n\n  Suppose we've built a model and fit it to data using HMC:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)\n  ```\n\n  Passing the posterior samples into `forecast`, we construct a forecast\n  distribution:\n\n  ```python\n    forecast_dist = tfp.sts.forecast(model, observed_time_series,\n                                     parameter_samples=samples,\n                                     num_steps_forecast=50)\n\n    forecast_mean = forecast_dist.mean()[..., 0]  # shape: [50]\n    forecast_scale = forecast_dist.stddev()[..., 0]  # shape: [50]\n    forecast_samples = forecast_dist.sample(10)[..., 0]  # shape: [10, 50]\n  ```\n\n  If using variational inference instead of HMC, we'd construct a forecast using\n  samples from the variational posterior:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series)\n\n    # OMITTED: take steps to optimize variational loss\n\n    samples = {k: q.sample(30) for (k, q) in variational_distributions.items()}\n    forecast_dist = tfp.sts.forecast(model, observed_time_series,\n                                         parameter_samples=samples,\n                                         num_steps_forecast=50)\n  ```\n\n  We can visualize the forecast by plotting:\n\n  ```python\n    from matplotlib import pylab as plt\n    def plot_forecast(observed_time_series,\n                      forecast_mean,\n                      forecast_scale,\n                      forecast_samples):\n      plt.figure(figsize=(12, 6))\n\n      num_steps = observed_time_series.shape[-1]\n      num_steps_forecast = forecast_mean.shape[-1]\n      num_steps_train = num_steps - num_steps_forecast\n\n      c1, c2 = (0.12, 0.47, 0.71), (1.0, 0.5, 0.05)\n      plt.plot(np.arange(num_steps), observed_time_series,\n               lw=2, color=c1, label='ground truth')\n\n      forecast_steps = np.arange(num_steps_train,\n                       num_steps_train+num_steps_forecast)\n      plt.plot(forecast_steps, forecast_samples.T, lw=1, color=c2, alpha=0.1)\n      plt.plot(forecast_steps, forecast_mean, lw=2, ls='--', color=c2,\n               label='forecast')\n      plt.fill_between(forecast_steps,\n                       forecast_mean - 2 * forecast_scale,\n                       forecast_mean + 2 * forecast_scale, color=c2, alpha=0.2)\n\n      plt.xlim([0, num_steps])\n      plt.legend()\n\n    plot_forecast(observed_time_series,\n                  forecast_mean=forecast_mean,\n                  forecast_scale=forecast_scale,\n                  forecast_samples=forecast_samples)\n  ```", "docstring_tokens": ["Construct", "predictive", "distribution", "over", "future", "observations", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/forecast.py#L172-L362", "partition": "test", "index": 620, "time": "2019-01-04 15:18:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/quantiles.py", "func_name": "quantiles", "original_string": "def quantiles(x,\n              num_quantiles,\n              axis=None,\n              interpolation=None,\n              keep_dims=False,\n              validate_args=False,\n              name=None):\n  \"\"\"Compute quantiles of `x` along `axis`.\n\n  The quantiles of a distribution are cut points dividing the range into\n  intervals with equal probabilities.\n\n  Given a vector `x` of samples, this function estimates the cut points by\n  returning `num_quantiles + 1` cut points, `(c0, ..., cn)`, such that, roughly\n  speaking, equal number of sample points lie in the `num_quantiles` intervals\n  `[c0, c1), [c1, c2), ..., [c_{n-1}, cn]`.  That is,\n\n  * About `1 / n` fraction of the data lies in `[c_{k-1}, c_k)`, `k = 1, ..., n`\n  * About `k / n` fraction of the data lies below `c_k`.\n  * `c0` is the sample minimum and `cn` is the maximum.\n\n  The exact number of data points in each interval depends on the size of\n  `x` (e.g. whether the size is divisible by `n`) and the `interpolation` kwarg.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.  If `axis` is not `None`,\n      `x` must have statically known number of dimensions.\n    num_quantiles:  Scalar `integer` `Tensor`.  The number of intervals the\n      returned `num_quantiles + 1` cut points divide the range into.\n    axis:  Optional `0-D` or `1-D` integer `Tensor` with constant values. The\n      axis that index independent samples over which to return the desired\n      percentile.  If `None` (the default), treat every dimension as a sample\n      dimension, returning a scalar.\n    interpolation : {'nearest', 'linear', 'lower', 'higher', 'midpoint'}.\n      Default value: 'nearest'.  This specifies the interpolation method to\n      use when the fractions `k / n` lie between two data points `i < j`:\n        * linear: i + (j - i) * fraction, where fraction is the fractional part\n          of the index surrounded by i and j.\n        * lower: `i`.\n        * higher: `j`.\n        * nearest: `i` or `j`, whichever is nearest.\n        * midpoint: (i + j) / 2. `linear` and `midpoint` interpolation do not\n          work with integer dtypes.\n    keep_dims:  Python `bool`. If `True`, the last dimension is kept with size 1\n      If `False`, the last dimension is removed from the output shape.\n    validate_args:  Whether to add runtime checks of argument validity. If\n      False, and arguments are incorrect, correct behavior is not guaranteed.\n    name:  A Python string name to give this `Op`.  Default is 'percentile'\n\n  Returns:\n    cut_points:  A `rank(x) + 1 - len(axis)` dimensional `Tensor` with same\n    `dtype` as `x` and shape `[num_quantiles + 1, ...]` where the trailing shape\n    is that of `x` without the dimensions in `axis` (unless `keep_dims is True`)\n\n  Raises:\n    ValueError:  If argument 'interpolation' is not an allowed type.\n    ValueError:  If interpolation type not compatible with `dtype`.\n\n  #### Examples\n\n  ```python\n  # Get quartiles of x with various interpolation choices.\n  x = [0.,  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.]\n\n  tfp.stats.quantiles(x, num_quantiles=4, interpolation='nearest')\n  ==> [  0.,   2.,   5.,   8.,  10.]\n\n  tfp.stats.quantiles(x, num_quantiles=4, interpolation='linear')\n  ==> [  0. ,   2.5,   5. ,   7.5,  10. ]\n\n  tfp.stats.quantiles(x, num_quantiles=4, interpolation='lower')\n  ==> [  0.,   2.,   5.,   7.,  10.]\n\n  # Get deciles of columns of an R x C data set.\n  data = load_my_columnar_data(...)\n  tfp.stats.quantiles(data, num_quantiles=10)\n  ==> Shape [11, C] Tensor\n  ```\n\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'quantiles', values=[x, num_quantiles, axis]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    return percentile(\n        x,\n        q=tf.linspace(\n            # percentile casts q to float64 before using it...so may as well use\n            # float64 here. Note that  using x.dtype won't work with linspace\n            # if x is integral type (which is anothe motivation for hard-coding\n            # float64).\n            tf.convert_to_tensor(value=0, dtype=tf.float64),\n            tf.convert_to_tensor(value=100, dtype=tf.float64),\n            num=num_quantiles + 1),\n        axis=axis,\n        interpolation=interpolation,\n        keep_dims=keep_dims,\n        validate_args=validate_args,\n        preserve_gradients=False)", "language": "python", "code": "def quantiles(x,\n              num_quantiles,\n              axis=None,\n              interpolation=None,\n              keep_dims=False,\n              validate_args=False,\n              name=None):\n  \"\"\"Compute quantiles of `x` along `axis`.\n\n  The quantiles of a distribution are cut points dividing the range into\n  intervals with equal probabilities.\n\n  Given a vector `x` of samples, this function estimates the cut points by\n  returning `num_quantiles + 1` cut points, `(c0, ..., cn)`, such that, roughly\n  speaking, equal number of sample points lie in the `num_quantiles` intervals\n  `[c0, c1), [c1, c2), ..., [c_{n-1}, cn]`.  That is,\n\n  * About `1 / n` fraction of the data lies in `[c_{k-1}, c_k)`, `k = 1, ..., n`\n  * About `k / n` fraction of the data lies below `c_k`.\n  * `c0` is the sample minimum and `cn` is the maximum.\n\n  The exact number of data points in each interval depends on the size of\n  `x` (e.g. whether the size is divisible by `n`) and the `interpolation` kwarg.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.  If `axis` is not `None`,\n      `x` must have statically known number of dimensions.\n    num_quantiles:  Scalar `integer` `Tensor`.  The number of intervals the\n      returned `num_quantiles + 1` cut points divide the range into.\n    axis:  Optional `0-D` or `1-D` integer `Tensor` with constant values. The\n      axis that index independent samples over which to return the desired\n      percentile.  If `None` (the default), treat every dimension as a sample\n      dimension, returning a scalar.\n    interpolation : {'nearest', 'linear', 'lower', 'higher', 'midpoint'}.\n      Default value: 'nearest'.  This specifies the interpolation method to\n      use when the fractions `k / n` lie between two data points `i < j`:\n        * linear: i + (j - i) * fraction, where fraction is the fractional part\n          of the index surrounded by i and j.\n        * lower: `i`.\n        * higher: `j`.\n        * nearest: `i` or `j`, whichever is nearest.\n        * midpoint: (i + j) / 2. `linear` and `midpoint` interpolation do not\n          work with integer dtypes.\n    keep_dims:  Python `bool`. If `True`, the last dimension is kept with size 1\n      If `False`, the last dimension is removed from the output shape.\n    validate_args:  Whether to add runtime checks of argument validity. If\n      False, and arguments are incorrect, correct behavior is not guaranteed.\n    name:  A Python string name to give this `Op`.  Default is 'percentile'\n\n  Returns:\n    cut_points:  A `rank(x) + 1 - len(axis)` dimensional `Tensor` with same\n    `dtype` as `x` and shape `[num_quantiles + 1, ...]` where the trailing shape\n    is that of `x` without the dimensions in `axis` (unless `keep_dims is True`)\n\n  Raises:\n    ValueError:  If argument 'interpolation' is not an allowed type.\n    ValueError:  If interpolation type not compatible with `dtype`.\n\n  #### Examples\n\n  ```python\n  # Get quartiles of x with various interpolation choices.\n  x = [0.,  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.]\n\n  tfp.stats.quantiles(x, num_quantiles=4, interpolation='nearest')\n  ==> [  0.,   2.,   5.,   8.,  10.]\n\n  tfp.stats.quantiles(x, num_quantiles=4, interpolation='linear')\n  ==> [  0. ,   2.5,   5. ,   7.5,  10. ]\n\n  tfp.stats.quantiles(x, num_quantiles=4, interpolation='lower')\n  ==> [  0.,   2.,   5.,   7.,  10.]\n\n  # Get deciles of columns of an R x C data set.\n  data = load_my_columnar_data(...)\n  tfp.stats.quantiles(data, num_quantiles=10)\n  ==> Shape [11, C] Tensor\n  ```\n\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'quantiles', values=[x, num_quantiles, axis]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    return percentile(\n        x,\n        q=tf.linspace(\n            # percentile casts q to float64 before using it...so may as well use\n            # float64 here. Note that  using x.dtype won't work with linspace\n            # if x is integral type (which is anothe motivation for hard-coding\n            # float64).\n            tf.convert_to_tensor(value=0, dtype=tf.float64),\n            tf.convert_to_tensor(value=100, dtype=tf.float64),\n            num=num_quantiles + 1),\n        axis=axis,\n        interpolation=interpolation,\n        keep_dims=keep_dims,\n        validate_args=validate_args,\n        preserve_gradients=False)", "code_tokens": ["def", "quantiles", "(", "x", ",", "num_quantiles", ",", "axis", "=", "None", ",", "interpolation", "=", "None", ",", "keep_dims", "=", "False", ",", "validate_args", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'quantiles'", ",", "values", "=", "[", "x", ",", "num_quantiles", ",", "axis", "]", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ")", "return", "percentile", "(", "x", ",", "q", "=", "tf", ".", "linspace", "(", "# percentile casts q to float64 before using it...so may as well use", "# float64 here. Note that  using x.dtype won't work with linspace", "# if x is integral type (which is anothe motivation for hard-coding", "# float64).", "tf", ".", "convert_to_tensor", "(", "value", "=", "0", ",", "dtype", "=", "tf", ".", "float64", ")", ",", "tf", ".", "convert_to_tensor", "(", "value", "=", "100", ",", "dtype", "=", "tf", ".", "float64", ")", ",", "num", "=", "num_quantiles", "+", "1", ")", ",", "axis", "=", "axis", ",", "interpolation", "=", "interpolation", ",", "keep_dims", "=", "keep_dims", ",", "validate_args", "=", "validate_args", ",", "preserve_gradients", "=", "False", ")"], "docstring": "Compute quantiles of `x` along `axis`.\n\n  The quantiles of a distribution are cut points dividing the range into\n  intervals with equal probabilities.\n\n  Given a vector `x` of samples, this function estimates the cut points by\n  returning `num_quantiles + 1` cut points, `(c0, ..., cn)`, such that, roughly\n  speaking, equal number of sample points lie in the `num_quantiles` intervals\n  `[c0, c1), [c1, c2), ..., [c_{n-1}, cn]`.  That is,\n\n  * About `1 / n` fraction of the data lies in `[c_{k-1}, c_k)`, `k = 1, ..., n`\n  * About `k / n` fraction of the data lies below `c_k`.\n  * `c0` is the sample minimum and `cn` is the maximum.\n\n  The exact number of data points in each interval depends on the size of\n  `x` (e.g. whether the size is divisible by `n`) and the `interpolation` kwarg.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.  If `axis` is not `None`,\n      `x` must have statically known number of dimensions.\n    num_quantiles:  Scalar `integer` `Tensor`.  The number of intervals the\n      returned `num_quantiles + 1` cut points divide the range into.\n    axis:  Optional `0-D` or `1-D` integer `Tensor` with constant values. The\n      axis that index independent samples over which to return the desired\n      percentile.  If `None` (the default), treat every dimension as a sample\n      dimension, returning a scalar.\n    interpolation : {'nearest', 'linear', 'lower', 'higher', 'midpoint'}.\n      Default value: 'nearest'.  This specifies the interpolation method to\n      use when the fractions `k / n` lie between two data points `i < j`:\n        * linear: i + (j - i) * fraction, where fraction is the fractional part\n          of the index surrounded by i and j.\n        * lower: `i`.\n        * higher: `j`.\n        * nearest: `i` or `j`, whichever is nearest.\n        * midpoint: (i + j) / 2. `linear` and `midpoint` interpolation do not\n          work with integer dtypes.\n    keep_dims:  Python `bool`. If `True`, the last dimension is kept with size 1\n      If `False`, the last dimension is removed from the output shape.\n    validate_args:  Whether to add runtime checks of argument validity. If\n      False, and arguments are incorrect, correct behavior is not guaranteed.\n    name:  A Python string name to give this `Op`.  Default is 'percentile'\n\n  Returns:\n    cut_points:  A `rank(x) + 1 - len(axis)` dimensional `Tensor` with same\n    `dtype` as `x` and shape `[num_quantiles + 1, ...]` where the trailing shape\n    is that of `x` without the dimensions in `axis` (unless `keep_dims is True`)\n\n  Raises:\n    ValueError:  If argument 'interpolation' is not an allowed type.\n    ValueError:  If interpolation type not compatible with `dtype`.\n\n  #### Examples\n\n  ```python\n  # Get quartiles of x with various interpolation choices.\n  x = [0.,  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.]\n\n  tfp.stats.quantiles(x, num_quantiles=4, interpolation='nearest')\n  ==> [  0.,   2.,   5.,   8.,  10.]\n\n  tfp.stats.quantiles(x, num_quantiles=4, interpolation='linear')\n  ==> [  0. ,   2.5,   5. ,   7.5,  10. ]\n\n  tfp.stats.quantiles(x, num_quantiles=4, interpolation='lower')\n  ==> [  0.,   2.,   5.,   7.,  10.]\n\n  # Get deciles of columns of an R x C data set.\n  data = load_my_columnar_data(...)\n  tfp.stats.quantiles(data, num_quantiles=10)\n  ==> Shape [11, C] Tensor\n  ```", "docstring_tokens": ["Compute", "quantiles", "of", "x", "along", "axis", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/quantiles.py#L626-L723", "partition": "test", "index": 1035, "time": "2019-01-08 17:39:45"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/linalg.py", "func_name": "_lu_reconstruct_assertions", "original_string": "def _lu_reconstruct_assertions(lower_upper, perm, validate_args):\n  \"\"\"Returns list of assertions related to `lu_reconstruct` assumptions.\"\"\"\n  assertions = []\n\n  message = 'Input `lower_upper` must have at least 2 dimensions.'\n  if lower_upper.shape.ndims is not None:\n    if lower_upper.shape.ndims < 2:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        tf.compat.v1.assert_rank_at_least(lower_upper, rank=2, message=message))\n\n  message = '`rank(lower_upper)` must equal `rank(perm) + 1`'\n  if lower_upper.shape.ndims is not None and perm.shape.ndims is not None:\n    if lower_upper.shape.ndims != perm.shape.ndims + 1:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        tf.compat.v1.assert_rank(\n            lower_upper, rank=tf.rank(perm) + 1, message=message))\n\n  message = '`lower_upper` must be square.'\n  if lower_upper.shape[:-2].is_fully_defined():\n    if lower_upper.shape[-2] != lower_upper.shape[-1]:\n      raise ValueError(message)\n  elif validate_args:\n    m, n = tf.split(tf.shape(input=lower_upper)[-2:], num_or_size_splits=2)\n    assertions.append(tf.compat.v1.assert_equal(m, n, message=message))\n\n  return assertions", "language": "python", "code": "def _lu_reconstruct_assertions(lower_upper, perm, validate_args):\n  \"\"\"Returns list of assertions related to `lu_reconstruct` assumptions.\"\"\"\n  assertions = []\n\n  message = 'Input `lower_upper` must have at least 2 dimensions.'\n  if lower_upper.shape.ndims is not None:\n    if lower_upper.shape.ndims < 2:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        tf.compat.v1.assert_rank_at_least(lower_upper, rank=2, message=message))\n\n  message = '`rank(lower_upper)` must equal `rank(perm) + 1`'\n  if lower_upper.shape.ndims is not None and perm.shape.ndims is not None:\n    if lower_upper.shape.ndims != perm.shape.ndims + 1:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        tf.compat.v1.assert_rank(\n            lower_upper, rank=tf.rank(perm) + 1, message=message))\n\n  message = '`lower_upper` must be square.'\n  if lower_upper.shape[:-2].is_fully_defined():\n    if lower_upper.shape[-2] != lower_upper.shape[-1]:\n      raise ValueError(message)\n  elif validate_args:\n    m, n = tf.split(tf.shape(input=lower_upper)[-2:], num_or_size_splits=2)\n    assertions.append(tf.compat.v1.assert_equal(m, n, message=message))\n\n  return assertions", "code_tokens": ["def", "_lu_reconstruct_assertions", "(", "lower_upper", ",", "perm", ",", "validate_args", ")", ":", "assertions", "=", "[", "]", "message", "=", "'Input `lower_upper` must have at least 2 dimensions.'", "if", "lower_upper", ".", "shape", ".", "ndims", "is", "not", "None", ":", "if", "lower_upper", ".", "shape", ".", "ndims", "<", "2", ":", "raise", "ValueError", "(", "message", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "tf", ".", "compat", ".", "v1", ".", "assert_rank_at_least", "(", "lower_upper", ",", "rank", "=", "2", ",", "message", "=", "message", ")", ")", "message", "=", "'`rank(lower_upper)` must equal `rank(perm) + 1`'", "if", "lower_upper", ".", "shape", ".", "ndims", "is", "not", "None", "and", "perm", ".", "shape", ".", "ndims", "is", "not", "None", ":", "if", "lower_upper", ".", "shape", ".", "ndims", "!=", "perm", ".", "shape", ".", "ndims", "+", "1", ":", "raise", "ValueError", "(", "message", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "tf", ".", "compat", ".", "v1", ".", "assert_rank", "(", "lower_upper", ",", "rank", "=", "tf", ".", "rank", "(", "perm", ")", "+", "1", ",", "message", "=", "message", ")", ")", "message", "=", "'`lower_upper` must be square.'", "if", "lower_upper", ".", "shape", "[", ":", "-", "2", "]", ".", "is_fully_defined", "(", ")", ":", "if", "lower_upper", ".", "shape", "[", "-", "2", "]", "!=", "lower_upper", ".", "shape", "[", "-", "1", "]", ":", "raise", "ValueError", "(", "message", ")", "elif", "validate_args", ":", "m", ",", "n", "=", "tf", ".", "split", "(", "tf", ".", "shape", "(", "input", "=", "lower_upper", ")", "[", "-", "2", ":", "]", ",", "num_or_size_splits", "=", "2", ")", "assertions", ".", "append", "(", "tf", ".", "compat", ".", "v1", ".", "assert_equal", "(", "m", ",", "n", ",", "message", "=", "message", ")", ")", "return", "assertions"], "docstring": "Returns list of assertions related to `lu_reconstruct` assumptions.", "docstring_tokens": ["Returns", "list", "of", "assertions", "related", "to", "lu_reconstruct", "assumptions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/linalg.py#L679-L708", "partition": "test", "index": 1109, "time": "2019-01-14 15:36:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/linalg.py", "func_name": "lu_solve", "original_string": "def lu_solve(lower_upper, perm, rhs,\n             validate_args=False,\n             name=None):\n  \"\"\"Solves systems of linear eqns `A X = RHS`, given LU factorizations.\n\n  Note: this function does not verify the implied matrix is actually invertible\n  nor is this condition checked even when `validate_args=True`.\n\n  Args:\n    lower_upper: `lu` as returned by `tf.linalg.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `lower_upper = L + U - eye`.\n    perm: `p` as returned by `tf.linag.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `perm = argmax(P)`.\n    rhs: Matrix-shaped float `Tensor` representing targets for which to solve;\n      `A X = RHS`. To handle vector cases, use:\n      `lu_solve(..., rhs[..., tf.newaxis])[..., 0]`.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness. Note: this function does not verify the implied matrix is\n      actually invertible, even when `validate_args=True`.\n      Default value: `False` (i.e., don't validate arguments).\n    name: Python `str` name given to ops managed by this object.\n      Default value: `None` (i.e., \"lu_solve\").\n\n  Returns:\n    x: The `X` in `A @ X = RHS`.\n\n  #### Examples\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  x = [[[1., 2],\n        [3, 4]],\n       [[7, 8],\n        [3, 4]]]\n  inv_x = tfp.math.lu_solve(*tf.linalg.lu(x), rhs=tf.eye(2))\n  tf.assert_near(tf.matrix_inverse(x), inv_x)\n  # ==> True\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, 'lu_solve', [lower_upper, perm, rhs]):\n    lower_upper = tf.convert_to_tensor(\n        value=lower_upper, dtype_hint=tf.float32, name='lower_upper')\n    perm = tf.convert_to_tensor(value=perm, dtype_hint=tf.int32, name='perm')\n    rhs = tf.convert_to_tensor(\n        value=rhs, dtype_hint=lower_upper.dtype, name='rhs')\n\n    assertions = _lu_solve_assertions(lower_upper, perm, rhs, validate_args)\n    if assertions:\n      with tf.control_dependencies(assertions):\n        lower_upper = tf.identity(lower_upper)\n        perm = tf.identity(perm)\n        rhs = tf.identity(rhs)\n\n    if rhs.shape.ndims == 2 and perm.shape.ndims == 1:\n      # Both rhs and perm have scalar batch_shape.\n      permuted_rhs = tf.gather(rhs, perm, axis=-2)\n    else:\n      # Either rhs or perm have non-scalar batch_shape or we can't determine\n      # this information statically.\n      rhs_shape = tf.shape(input=rhs)\n      broadcast_batch_shape = tf.broadcast_dynamic_shape(\n          rhs_shape[:-2],\n          tf.shape(input=perm)[:-1])\n      d, m = rhs_shape[-2], rhs_shape[-1]\n      rhs_broadcast_shape = tf.concat([broadcast_batch_shape, [d, m]], axis=0)\n\n      # Tile out rhs.\n      broadcast_rhs = tf.broadcast_to(rhs, rhs_broadcast_shape)\n      broadcast_rhs = tf.reshape(broadcast_rhs, [-1, d, m])\n\n      # Tile out perm and add batch indices.\n      broadcast_perm = tf.broadcast_to(perm, rhs_broadcast_shape[:-1])\n      broadcast_perm = tf.reshape(broadcast_perm, [-1, d])\n      broadcast_batch_size = tf.reduce_prod(input_tensor=broadcast_batch_shape)\n      broadcast_batch_indices = tf.broadcast_to(\n          tf.range(broadcast_batch_size)[:, tf.newaxis],\n          [broadcast_batch_size, d])\n      broadcast_perm = tf.stack([broadcast_batch_indices, broadcast_perm],\n                                axis=-1)\n\n      permuted_rhs = tf.gather_nd(broadcast_rhs, broadcast_perm)\n      permuted_rhs = tf.reshape(permuted_rhs, rhs_broadcast_shape)\n\n    lower = tf.linalg.set_diag(\n        tf.linalg.band_part(lower_upper, num_lower=-1, num_upper=0),\n        tf.ones(tf.shape(input=lower_upper)[:-1], dtype=lower_upper.dtype))\n    return linear_operator_util.matrix_triangular_solve_with_broadcast(\n        lower_upper,  # Only upper is accessed.\n        linear_operator_util.matrix_triangular_solve_with_broadcast(\n            lower, permuted_rhs),\n        lower=False)", "language": "python", "code": "def lu_solve(lower_upper, perm, rhs,\n             validate_args=False,\n             name=None):\n  \"\"\"Solves systems of linear eqns `A X = RHS`, given LU factorizations.\n\n  Note: this function does not verify the implied matrix is actually invertible\n  nor is this condition checked even when `validate_args=True`.\n\n  Args:\n    lower_upper: `lu` as returned by `tf.linalg.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `lower_upper = L + U - eye`.\n    perm: `p` as returned by `tf.linag.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `perm = argmax(P)`.\n    rhs: Matrix-shaped float `Tensor` representing targets for which to solve;\n      `A X = RHS`. To handle vector cases, use:\n      `lu_solve(..., rhs[..., tf.newaxis])[..., 0]`.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness. Note: this function does not verify the implied matrix is\n      actually invertible, even when `validate_args=True`.\n      Default value: `False` (i.e., don't validate arguments).\n    name: Python `str` name given to ops managed by this object.\n      Default value: `None` (i.e., \"lu_solve\").\n\n  Returns:\n    x: The `X` in `A @ X = RHS`.\n\n  #### Examples\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  x = [[[1., 2],\n        [3, 4]],\n       [[7, 8],\n        [3, 4]]]\n  inv_x = tfp.math.lu_solve(*tf.linalg.lu(x), rhs=tf.eye(2))\n  tf.assert_near(tf.matrix_inverse(x), inv_x)\n  # ==> True\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, 'lu_solve', [lower_upper, perm, rhs]):\n    lower_upper = tf.convert_to_tensor(\n        value=lower_upper, dtype_hint=tf.float32, name='lower_upper')\n    perm = tf.convert_to_tensor(value=perm, dtype_hint=tf.int32, name='perm')\n    rhs = tf.convert_to_tensor(\n        value=rhs, dtype_hint=lower_upper.dtype, name='rhs')\n\n    assertions = _lu_solve_assertions(lower_upper, perm, rhs, validate_args)\n    if assertions:\n      with tf.control_dependencies(assertions):\n        lower_upper = tf.identity(lower_upper)\n        perm = tf.identity(perm)\n        rhs = tf.identity(rhs)\n\n    if rhs.shape.ndims == 2 and perm.shape.ndims == 1:\n      # Both rhs and perm have scalar batch_shape.\n      permuted_rhs = tf.gather(rhs, perm, axis=-2)\n    else:\n      # Either rhs or perm have non-scalar batch_shape or we can't determine\n      # this information statically.\n      rhs_shape = tf.shape(input=rhs)\n      broadcast_batch_shape = tf.broadcast_dynamic_shape(\n          rhs_shape[:-2],\n          tf.shape(input=perm)[:-1])\n      d, m = rhs_shape[-2], rhs_shape[-1]\n      rhs_broadcast_shape = tf.concat([broadcast_batch_shape, [d, m]], axis=0)\n\n      # Tile out rhs.\n      broadcast_rhs = tf.broadcast_to(rhs, rhs_broadcast_shape)\n      broadcast_rhs = tf.reshape(broadcast_rhs, [-1, d, m])\n\n      # Tile out perm and add batch indices.\n      broadcast_perm = tf.broadcast_to(perm, rhs_broadcast_shape[:-1])\n      broadcast_perm = tf.reshape(broadcast_perm, [-1, d])\n      broadcast_batch_size = tf.reduce_prod(input_tensor=broadcast_batch_shape)\n      broadcast_batch_indices = tf.broadcast_to(\n          tf.range(broadcast_batch_size)[:, tf.newaxis],\n          [broadcast_batch_size, d])\n      broadcast_perm = tf.stack([broadcast_batch_indices, broadcast_perm],\n                                axis=-1)\n\n      permuted_rhs = tf.gather_nd(broadcast_rhs, broadcast_perm)\n      permuted_rhs = tf.reshape(permuted_rhs, rhs_broadcast_shape)\n\n    lower = tf.linalg.set_diag(\n        tf.linalg.band_part(lower_upper, num_lower=-1, num_upper=0),\n        tf.ones(tf.shape(input=lower_upper)[:-1], dtype=lower_upper.dtype))\n    return linear_operator_util.matrix_triangular_solve_with_broadcast(\n        lower_upper,  # Only upper is accessed.\n        linear_operator_util.matrix_triangular_solve_with_broadcast(\n            lower, permuted_rhs),\n        lower=False)", "code_tokens": ["def", "lu_solve", "(", "lower_upper", ",", "perm", ",", "rhs", ",", "validate_args", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'lu_solve'", ",", "[", "lower_upper", ",", "perm", ",", "rhs", "]", ")", ":", "lower_upper", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "lower_upper", ",", "dtype_hint", "=", "tf", ".", "float32", ",", "name", "=", "'lower_upper'", ")", "perm", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "perm", ",", "dtype_hint", "=", "tf", ".", "int32", ",", "name", "=", "'perm'", ")", "rhs", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "rhs", ",", "dtype_hint", "=", "lower_upper", ".", "dtype", ",", "name", "=", "'rhs'", ")", "assertions", "=", "_lu_solve_assertions", "(", "lower_upper", ",", "perm", ",", "rhs", ",", "validate_args", ")", "if", "assertions", ":", "with", "tf", ".", "control_dependencies", "(", "assertions", ")", ":", "lower_upper", "=", "tf", ".", "identity", "(", "lower_upper", ")", "perm", "=", "tf", ".", "identity", "(", "perm", ")", "rhs", "=", "tf", ".", "identity", "(", "rhs", ")", "if", "rhs", ".", "shape", ".", "ndims", "==", "2", "and", "perm", ".", "shape", ".", "ndims", "==", "1", ":", "# Both rhs and perm have scalar batch_shape.", "permuted_rhs", "=", "tf", ".", "gather", "(", "rhs", ",", "perm", ",", "axis", "=", "-", "2", ")", "else", ":", "# Either rhs or perm have non-scalar batch_shape or we can't determine", "# this information statically.", "rhs_shape", "=", "tf", ".", "shape", "(", "input", "=", "rhs", ")", "broadcast_batch_shape", "=", "tf", ".", "broadcast_dynamic_shape", "(", "rhs_shape", "[", ":", "-", "2", "]", ",", "tf", ".", "shape", "(", "input", "=", "perm", ")", "[", ":", "-", "1", "]", ")", "d", ",", "m", "=", "rhs_shape", "[", "-", "2", "]", ",", "rhs_shape", "[", "-", "1", "]", "rhs_broadcast_shape", "=", "tf", ".", "concat", "(", "[", "broadcast_batch_shape", ",", "[", "d", ",", "m", "]", "]", ",", "axis", "=", "0", ")", "# Tile out rhs.", "broadcast_rhs", "=", "tf", ".", "broadcast_to", "(", "rhs", ",", "rhs_broadcast_shape", ")", "broadcast_rhs", "=", "tf", ".", "reshape", "(", "broadcast_rhs", ",", "[", "-", "1", ",", "d", ",", "m", "]", ")", "# Tile out perm and add batch indices.", "broadcast_perm", "=", "tf", ".", "broadcast_to", "(", "perm", ",", "rhs_broadcast_shape", "[", ":", "-", "1", "]", ")", "broadcast_perm", "=", "tf", ".", "reshape", "(", "broadcast_perm", ",", "[", "-", "1", ",", "d", "]", ")", "broadcast_batch_size", "=", "tf", ".", "reduce_prod", "(", "input_tensor", "=", "broadcast_batch_shape", ")", "broadcast_batch_indices", "=", "tf", ".", "broadcast_to", "(", "tf", ".", "range", "(", "broadcast_batch_size", ")", "[", ":", ",", "tf", ".", "newaxis", "]", ",", "[", "broadcast_batch_size", ",", "d", "]", ")", "broadcast_perm", "=", "tf", ".", "stack", "(", "[", "broadcast_batch_indices", ",", "broadcast_perm", "]", ",", "axis", "=", "-", "1", ")", "permuted_rhs", "=", "tf", ".", "gather_nd", "(", "broadcast_rhs", ",", "broadcast_perm", ")", "permuted_rhs", "=", "tf", ".", "reshape", "(", "permuted_rhs", ",", "rhs_broadcast_shape", ")", "lower", "=", "tf", ".", "linalg", ".", "set_diag", "(", "tf", ".", "linalg", ".", "band_part", "(", "lower_upper", ",", "num_lower", "=", "-", "1", ",", "num_upper", "=", "0", ")", ",", "tf", ".", "ones", "(", "tf", ".", "shape", "(", "input", "=", "lower_upper", ")", "[", ":", "-", "1", "]", ",", "dtype", "=", "lower_upper", ".", "dtype", ")", ")", "return", "linear_operator_util", ".", "matrix_triangular_solve_with_broadcast", "(", "lower_upper", ",", "# Only upper is accessed.", "linear_operator_util", ".", "matrix_triangular_solve_with_broadcast", "(", "lower", ",", "permuted_rhs", ")", ",", "lower", "=", "False", ")"], "docstring": "Solves systems of linear eqns `A X = RHS`, given LU factorizations.\n\n  Note: this function does not verify the implied matrix is actually invertible\n  nor is this condition checked even when `validate_args=True`.\n\n  Args:\n    lower_upper: `lu` as returned by `tf.linalg.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `lower_upper = L + U - eye`.\n    perm: `p` as returned by `tf.linag.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `perm = argmax(P)`.\n    rhs: Matrix-shaped float `Tensor` representing targets for which to solve;\n      `A X = RHS`. To handle vector cases, use:\n      `lu_solve(..., rhs[..., tf.newaxis])[..., 0]`.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness. Note: this function does not verify the implied matrix is\n      actually invertible, even when `validate_args=True`.\n      Default value: `False` (i.e., don't validate arguments).\n    name: Python `str` name given to ops managed by this object.\n      Default value: `None` (i.e., \"lu_solve\").\n\n  Returns:\n    x: The `X` in `A @ X = RHS`.\n\n  #### Examples\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  x = [[[1., 2],\n        [3, 4]],\n       [[7, 8],\n        [3, 4]]]\n  inv_x = tfp.math.lu_solve(*tf.linalg.lu(x), rhs=tf.eye(2))\n  tf.assert_near(tf.matrix_inverse(x), inv_x)\n  # ==> True\n  ```", "docstring_tokens": ["Solves", "systems", "of", "linear", "eqns", "A", "X", "=", "RHS", "given", "LU", "factorizations", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/linalg.py#L448-L543", "partition": "test", "index": 1107, "time": "2019-01-14 16:55:33"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "expand_to_vector", "original_string": "def expand_to_vector(x, tensor_name=None, op_name=None, validate_args=False):\n  \"\"\"Transform a 0-D or 1-D `Tensor` to be 1-D.\n\n  For user convenience, many parts of the TensorFlow Probability API accept\n  inputs of rank 0 or 1 -- i.e., allowing an `event_shape` of `[5]` to be passed\n  to the API as either `5` or `[5]`.  This function can be used to transform\n  such an argument to always be 1-D.\n\n  NOTE: Python or NumPy values will be converted to `Tensor`s with standard type\n  inference/conversion.  In particular, an empty list or tuple will become an\n  empty `Tensor` with dtype `float32`.  Callers should convert values to\n  `Tensor`s before calling this function if different behavior is desired\n  (e.g. converting empty lists / other values to `Tensor`s with dtype `int32`).\n\n  Args:\n    x: A 0-D or 1-D `Tensor`.\n    tensor_name: Python `str` name for `Tensor`s created by this function.\n    op_name: Python `str` name for `Op`s created by this function.\n    validate_args: Python `bool, default `False`.  When `True`, arguments may be\n      checked for validity at execution time, possibly degrading runtime\n      performance.  When `False`, invalid inputs may silently render incorrect\n        outputs.\n  Returns:\n    vector: a 1-D `Tensor`.\n  \"\"\"\n  with tf.name_scope(op_name or \"expand_to_vector\"):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    ndims = tensorshape_util.rank(x.shape)\n\n    if ndims is None:\n      # Maybe expand ndims from 0 to 1.\n      if validate_args:\n        x = with_dependencies([\n            assert_util.assert_rank_at_most(\n                x, 1, message=\"Input is neither scalar nor vector.\")\n        ], x)\n      ndims = tf.rank(x)\n      expanded_shape = pick_vector(\n          tf.equal(ndims, 0), np.array([1], dtype=np.int32), tf.shape(input=x))\n      return tf.reshape(x, expanded_shape)\n\n    elif ndims == 0:\n      # Definitely expand ndims from 0 to 1.\n      x_const = tf.get_static_value(x)\n      if x_const is not None:\n        return tf.convert_to_tensor(\n            value=dtype_util.as_numpy_dtype(x.dtype)([x_const]),\n            name=tensor_name)\n\n      else:\n        return tf.reshape(x, [1])\n\n    elif ndims != 1:\n      raise ValueError(\"Input is neither scalar nor vector.\")\n\n    # ndims == 1\n    return x", "language": "python", "code": "def expand_to_vector(x, tensor_name=None, op_name=None, validate_args=False):\n  \"\"\"Transform a 0-D or 1-D `Tensor` to be 1-D.\n\n  For user convenience, many parts of the TensorFlow Probability API accept\n  inputs of rank 0 or 1 -- i.e., allowing an `event_shape` of `[5]` to be passed\n  to the API as either `5` or `[5]`.  This function can be used to transform\n  such an argument to always be 1-D.\n\n  NOTE: Python or NumPy values will be converted to `Tensor`s with standard type\n  inference/conversion.  In particular, an empty list or tuple will become an\n  empty `Tensor` with dtype `float32`.  Callers should convert values to\n  `Tensor`s before calling this function if different behavior is desired\n  (e.g. converting empty lists / other values to `Tensor`s with dtype `int32`).\n\n  Args:\n    x: A 0-D or 1-D `Tensor`.\n    tensor_name: Python `str` name for `Tensor`s created by this function.\n    op_name: Python `str` name for `Op`s created by this function.\n    validate_args: Python `bool, default `False`.  When `True`, arguments may be\n      checked for validity at execution time, possibly degrading runtime\n      performance.  When `False`, invalid inputs may silently render incorrect\n        outputs.\n  Returns:\n    vector: a 1-D `Tensor`.\n  \"\"\"\n  with tf.name_scope(op_name or \"expand_to_vector\"):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    ndims = tensorshape_util.rank(x.shape)\n\n    if ndims is None:\n      # Maybe expand ndims from 0 to 1.\n      if validate_args:\n        x = with_dependencies([\n            assert_util.assert_rank_at_most(\n                x, 1, message=\"Input is neither scalar nor vector.\")\n        ], x)\n      ndims = tf.rank(x)\n      expanded_shape = pick_vector(\n          tf.equal(ndims, 0), np.array([1], dtype=np.int32), tf.shape(input=x))\n      return tf.reshape(x, expanded_shape)\n\n    elif ndims == 0:\n      # Definitely expand ndims from 0 to 1.\n      x_const = tf.get_static_value(x)\n      if x_const is not None:\n        return tf.convert_to_tensor(\n            value=dtype_util.as_numpy_dtype(x.dtype)([x_const]),\n            name=tensor_name)\n\n      else:\n        return tf.reshape(x, [1])\n\n    elif ndims != 1:\n      raise ValueError(\"Input is neither scalar nor vector.\")\n\n    # ndims == 1\n    return x", "code_tokens": ["def", "expand_to_vector", "(", "x", ",", "tensor_name", "=", "None", ",", "op_name", "=", "None", ",", "validate_args", "=", "False", ")", ":", "with", "tf", ".", "name_scope", "(", "op_name", "or", "\"expand_to_vector\"", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "\"x\"", ")", "ndims", "=", "tensorshape_util", ".", "rank", "(", "x", ".", "shape", ")", "if", "ndims", "is", "None", ":", "# Maybe expand ndims from 0 to 1.", "if", "validate_args", ":", "x", "=", "with_dependencies", "(", "[", "assert_util", ".", "assert_rank_at_most", "(", "x", ",", "1", ",", "message", "=", "\"Input is neither scalar nor vector.\"", ")", "]", ",", "x", ")", "ndims", "=", "tf", ".", "rank", "(", "x", ")", "expanded_shape", "=", "pick_vector", "(", "tf", ".", "equal", "(", "ndims", ",", "0", ")", ",", "np", ".", "array", "(", "[", "1", "]", ",", "dtype", "=", "np", ".", "int32", ")", ",", "tf", ".", "shape", "(", "input", "=", "x", ")", ")", "return", "tf", ".", "reshape", "(", "x", ",", "expanded_shape", ")", "elif", "ndims", "==", "0", ":", "# Definitely expand ndims from 0 to 1.", "x_const", "=", "tf", ".", "get_static_value", "(", "x", ")", "if", "x_const", "is", "not", "None", ":", "return", "tf", ".", "convert_to_tensor", "(", "value", "=", "dtype_util", ".", "as_numpy_dtype", "(", "x", ".", "dtype", ")", "(", "[", "x_const", "]", ")", ",", "name", "=", "tensor_name", ")", "else", ":", "return", "tf", ".", "reshape", "(", "x", ",", "[", "1", "]", ")", "elif", "ndims", "!=", "1", ":", "raise", "ValueError", "(", "\"Input is neither scalar nor vector.\"", ")", "# ndims == 1", "return", "x"], "docstring": "Transform a 0-D or 1-D `Tensor` to be 1-D.\n\n  For user convenience, many parts of the TensorFlow Probability API accept\n  inputs of rank 0 or 1 -- i.e., allowing an `event_shape` of `[5]` to be passed\n  to the API as either `5` or `[5]`.  This function can be used to transform\n  such an argument to always be 1-D.\n\n  NOTE: Python or NumPy values will be converted to `Tensor`s with standard type\n  inference/conversion.  In particular, an empty list or tuple will become an\n  empty `Tensor` with dtype `float32`.  Callers should convert values to\n  `Tensor`s before calling this function if different behavior is desired\n  (e.g. converting empty lists / other values to `Tensor`s with dtype `int32`).\n\n  Args:\n    x: A 0-D or 1-D `Tensor`.\n    tensor_name: Python `str` name for `Tensor`s created by this function.\n    op_name: Python `str` name for `Op`s created by this function.\n    validate_args: Python `bool, default `False`.  When `True`, arguments may be\n      checked for validity at execution time, possibly degrading runtime\n      performance.  When `False`, invalid inputs may silently render incorrect\n        outputs.\n  Returns:\n    vector: a 1-D `Tensor`.", "docstring_tokens": ["Transform", "a", "0", "-", "D", "or", "1", "-", "D", "Tensor", "to", "be", "1", "-", "D", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L2103-L2159", "partition": "test", "index": 929, "time": "2019-01-15 10:54:26"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/linalg.py", "func_name": "lu_matrix_inverse", "original_string": "def lu_matrix_inverse(lower_upper, perm, validate_args=False, name=None):\n  \"\"\"Computes a matrix inverse given the matrix's LU decomposition.\n\n  This op is conceptually identical to,\n\n  ````python\n  inv_X = tf.lu_matrix_inverse(*tf.linalg.lu(X))\n  tf.assert_near(tf.matrix_inverse(X), inv_X)\n  # ==> True\n  ```\n\n  Note: this function does not verify the implied matrix is actually invertible\n  nor is this condition checked even when `validate_args=True`.\n\n  Args:\n    lower_upper: `lu` as returned by `tf.linalg.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `lower_upper = L + U - eye`.\n    perm: `p` as returned by `tf.linag.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `perm = argmax(P)`.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness. Note: this function does not verify the implied matrix is\n      actually invertible, even when `validate_args=True`.\n      Default value: `False` (i.e., don't validate arguments).\n    name: Python `str` name given to ops managed by this object.\n      Default value: `None` (i.e., \"lu_matrix_inverse\").\n\n  Returns:\n    inv_x: The matrix_inv, i.e.,\n      `tf.matrix_inverse(tfp.math.lu_reconstruct(lu, perm))`.\n\n  #### Examples\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  x = [[[3., 4], [1, 2]],\n       [[7., 8], [3, 4]]]\n  inv_x = tfp.math.lu_matrix_inverse(*tf.linalg.lu(x))\n  tf.assert_near(tf.matrix_inverse(x), inv_x)\n  # ==> True\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, 'lu_matrix_inverse', [lower_upper, perm]):\n    lower_upper = tf.convert_to_tensor(\n        value=lower_upper, dtype_hint=tf.float32, name='lower_upper')\n    perm = tf.convert_to_tensor(value=perm, dtype_hint=tf.int32, name='perm')\n    assertions = _lu_reconstruct_assertions(lower_upper, perm, validate_args)\n    if assertions:\n      with tf.control_dependencies(assertions):\n        lower_upper = tf.identity(lower_upper)\n        perm = tf.identity(perm)\n    shape = tf.shape(input=lower_upper)\n    return lu_solve(\n        lower_upper, perm,\n        rhs=tf.eye(shape[-1], batch_shape=shape[:-2], dtype=lower_upper.dtype),\n        validate_args=False)", "language": "python", "code": "def lu_matrix_inverse(lower_upper, perm, validate_args=False, name=None):\n  \"\"\"Computes a matrix inverse given the matrix's LU decomposition.\n\n  This op is conceptually identical to,\n\n  ````python\n  inv_X = tf.lu_matrix_inverse(*tf.linalg.lu(X))\n  tf.assert_near(tf.matrix_inverse(X), inv_X)\n  # ==> True\n  ```\n\n  Note: this function does not verify the implied matrix is actually invertible\n  nor is this condition checked even when `validate_args=True`.\n\n  Args:\n    lower_upper: `lu` as returned by `tf.linalg.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `lower_upper = L + U - eye`.\n    perm: `p` as returned by `tf.linag.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `perm = argmax(P)`.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness. Note: this function does not verify the implied matrix is\n      actually invertible, even when `validate_args=True`.\n      Default value: `False` (i.e., don't validate arguments).\n    name: Python `str` name given to ops managed by this object.\n      Default value: `None` (i.e., \"lu_matrix_inverse\").\n\n  Returns:\n    inv_x: The matrix_inv, i.e.,\n      `tf.matrix_inverse(tfp.math.lu_reconstruct(lu, perm))`.\n\n  #### Examples\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  x = [[[3., 4], [1, 2]],\n       [[7., 8], [3, 4]]]\n  inv_x = tfp.math.lu_matrix_inverse(*tf.linalg.lu(x))\n  tf.assert_near(tf.matrix_inverse(x), inv_x)\n  # ==> True\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, 'lu_matrix_inverse', [lower_upper, perm]):\n    lower_upper = tf.convert_to_tensor(\n        value=lower_upper, dtype_hint=tf.float32, name='lower_upper')\n    perm = tf.convert_to_tensor(value=perm, dtype_hint=tf.int32, name='perm')\n    assertions = _lu_reconstruct_assertions(lower_upper, perm, validate_args)\n    if assertions:\n      with tf.control_dependencies(assertions):\n        lower_upper = tf.identity(lower_upper)\n        perm = tf.identity(perm)\n    shape = tf.shape(input=lower_upper)\n    return lu_solve(\n        lower_upper, perm,\n        rhs=tf.eye(shape[-1], batch_shape=shape[:-2], dtype=lower_upper.dtype),\n        validate_args=False)", "code_tokens": ["def", "lu_matrix_inverse", "(", "lower_upper", ",", "perm", ",", "validate_args", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'lu_matrix_inverse'", ",", "[", "lower_upper", ",", "perm", "]", ")", ":", "lower_upper", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "lower_upper", ",", "dtype_hint", "=", "tf", ".", "float32", ",", "name", "=", "'lower_upper'", ")", "perm", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "perm", ",", "dtype_hint", "=", "tf", ".", "int32", ",", "name", "=", "'perm'", ")", "assertions", "=", "_lu_reconstruct_assertions", "(", "lower_upper", ",", "perm", ",", "validate_args", ")", "if", "assertions", ":", "with", "tf", ".", "control_dependencies", "(", "assertions", ")", ":", "lower_upper", "=", "tf", ".", "identity", "(", "lower_upper", ")", "perm", "=", "tf", ".", "identity", "(", "perm", ")", "shape", "=", "tf", ".", "shape", "(", "input", "=", "lower_upper", ")", "return", "lu_solve", "(", "lower_upper", ",", "perm", ",", "rhs", "=", "tf", ".", "eye", "(", "shape", "[", "-", "1", "]", ",", "batch_shape", "=", "shape", "[", ":", "-", "2", "]", ",", "dtype", "=", "lower_upper", ".", "dtype", ")", ",", "validate_args", "=", "False", ")"], "docstring": "Computes a matrix inverse given the matrix's LU decomposition.\n\n  This op is conceptually identical to,\n\n  ````python\n  inv_X = tf.lu_matrix_inverse(*tf.linalg.lu(X))\n  tf.assert_near(tf.matrix_inverse(X), inv_X)\n  # ==> True\n  ```\n\n  Note: this function does not verify the implied matrix is actually invertible\n  nor is this condition checked even when `validate_args=True`.\n\n  Args:\n    lower_upper: `lu` as returned by `tf.linalg.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `lower_upper = L + U - eye`.\n    perm: `p` as returned by `tf.linag.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `perm = argmax(P)`.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness. Note: this function does not verify the implied matrix is\n      actually invertible, even when `validate_args=True`.\n      Default value: `False` (i.e., don't validate arguments).\n    name: Python `str` name given to ops managed by this object.\n      Default value: `None` (i.e., \"lu_matrix_inverse\").\n\n  Returns:\n    inv_x: The matrix_inv, i.e.,\n      `tf.matrix_inverse(tfp.math.lu_reconstruct(lu, perm))`.\n\n  #### Examples\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  x = [[[3., 4], [1, 2]],\n       [[7., 8], [3, 4]]]\n  inv_x = tfp.math.lu_matrix_inverse(*tf.linalg.lu(x))\n  tf.assert_near(tf.matrix_inverse(x), inv_x)\n  # ==> True\n  ```", "docstring_tokens": ["Computes", "a", "matrix", "inverse", "given", "the", "matrix", "s", "LU", "decomposition", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/linalg.py#L546-L605", "partition": "test", "index": 1108, "time": "2019-01-16 17:51:16"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/bijector.py", "func_name": "_Mapping.remove", "original_string": "def remove(self, field):\n    \"\"\"To support weak referencing, removes cache key from the cache value.\"\"\"\n    return _Mapping(\n        x=None if field == \"x\" else self.x,\n        y=None if field == \"y\" else self.y,\n        ildj=self.ildj,\n        kwargs=self.kwargs)", "language": "python", "code": "def remove(self, field):\n    \"\"\"To support weak referencing, removes cache key from the cache value.\"\"\"\n    return _Mapping(\n        x=None if field == \"x\" else self.x,\n        y=None if field == \"y\" else self.y,\n        ildj=self.ildj,\n        kwargs=self.kwargs)", "code_tokens": ["def", "remove", "(", "self", ",", "field", ")", ":", "return", "_Mapping", "(", "x", "=", "None", "if", "field", "==", "\"x\"", "else", "self", ".", "x", ",", "y", "=", "None", "if", "field", "==", "\"y\"", "else", "self", ".", "y", ",", "ildj", "=", "self", ".", "ildj", ",", "kwargs", "=", "self", ".", "kwargs", ")"], "docstring": "To support weak referencing, removes cache key from the cache value.", "docstring_tokens": ["To", "support", "weak", "referencing", "removes", "cache", "key", "from", "the", "cache", "value", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/bijector.py#L104-L110", "partition": "test", "index": 789, "time": "2019-01-17 04:42:26"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "LinearGaussianStateSpaceModel.backward_smoothing_pass", "original_string": "def backward_smoothing_pass(self,\n                              filtered_means,\n                              filtered_covs,\n                              predicted_means,\n                              predicted_covs):\n    \"\"\"Run the backward pass in Kalman smoother.\n\n    The backward smoothing is using Rauch, Tung and Striebel smoother as\n    as discussed in section 18.3.2 of Kevin P. Murphy, 2012, Machine Learning:\n    A Probabilistic Perspective, The MIT Press. The inputs are returned by\n    `forward_filter` function.\n\n    Args:\n      filtered_means: Means of the per-timestep filtered marginal\n        distributions p(z_t | x_{:t}), as a Tensor of shape\n        `sample_shape(x) + batch_shape + [num_timesteps, latent_size]`.\n      filtered_covs: Covariances of the per-timestep filtered marginal\n        distributions p(z_t | x_{:t}), as a Tensor of shape\n        `batch_shape + [num_timesteps, latent_size, latent_size]`.\n      predicted_means: Means of the per-timestep predictive\n         distributions over latent states, p(z_{t+1} | x_{:t}), as a\n         Tensor of shape `sample_shape(x) + batch_shape +\n         [num_timesteps, latent_size]`.\n      predicted_covs: Covariances of the per-timestep predictive\n         distributions over latent states, p(z_{t+1} | x_{:t}), as a\n         Tensor of shape `batch_shape + [num_timesteps, latent_size,\n         latent_size]`.\n\n    Returns:\n      posterior_means: Means of the smoothed marginal distributions\n        p(z_t | x_{1:T}), as a Tensor of shape\n        `sample_shape(x) + batch_shape + [num_timesteps, latent_size]`,\n        which is of the same shape as filtered_means.\n      posterior_covs: Covariances of the smoothed marginal distributions\n        p(z_t | x_{1:T}), as a Tensor of shape\n        `batch_shape + [num_timesteps, latent_size, latent_size]`.\n        which is of the same shape as filtered_covs.\n    \"\"\"\n    with tf.name_scope(\"backward_pass\"):\n      filtered_means = tf.convert_to_tensor(\n          value=filtered_means, name=\"filtered_means\")\n      filtered_covs = tf.convert_to_tensor(\n          value=filtered_covs, name=\"filtered_covs\")\n      predicted_means = tf.convert_to_tensor(\n          value=predicted_means, name=\"predicted_means\")\n      predicted_covs = tf.convert_to_tensor(\n          value=predicted_covs, name=\"predicted_covs\")\n\n      # To scan over time dimension, we need to move 'num_timesteps' from the\n      # event shape to the initial dimension of the tensor.\n      filtered_means = distribution_util.move_dimension(filtered_means, -2, 0)\n      filtered_covs = distribution_util.move_dimension(filtered_covs, -3, 0)\n      predicted_means = distribution_util.move_dimension(predicted_means, -2, 0)\n      predicted_covs = distribution_util.move_dimension(predicted_covs, -3, 0)\n\n      # The means are assumed to be vectors. Adding a dummy index to\n      # ensure the `matmul` op working smoothly.\n      filtered_means = filtered_means[..., tf.newaxis]\n      predicted_means = predicted_means[..., tf.newaxis]\n\n      initial_backward_mean = predicted_means[-1, ...]\n      initial_backward_cov = predicted_covs[-1, ...]\n\n      num_timesteps = tf.shape(input=filtered_means)[0]\n      initial_state = BackwardPassState(\n          backward_mean=initial_backward_mean,\n          backward_cov=initial_backward_cov,\n          timestep=self.initial_step + num_timesteps - 1)\n\n      update_step_fn = build_backward_pass_step(\n          self.get_transition_matrix_for_timestep)\n\n      # For backward pass, it scans the `elems` from last to first.\n      posterior_states = tf.scan(update_step_fn,\n                                 elems=(filtered_means,\n                                        filtered_covs,\n                                        predicted_means,\n                                        predicted_covs),\n                                 initializer=initial_state,\n                                 reverse=True)\n\n      # Move the time dimension back into the event shape.\n      posterior_means = distribution_util.move_dimension(\n          posterior_states.backward_mean[..., 0], 0, -2)\n      posterior_covs = distribution_util.move_dimension(\n          posterior_states.backward_cov, 0, -3)\n\n      return (posterior_means, posterior_covs)", "language": "python", "code": "def backward_smoothing_pass(self,\n                              filtered_means,\n                              filtered_covs,\n                              predicted_means,\n                              predicted_covs):\n    \"\"\"Run the backward pass in Kalman smoother.\n\n    The backward smoothing is using Rauch, Tung and Striebel smoother as\n    as discussed in section 18.3.2 of Kevin P. Murphy, 2012, Machine Learning:\n    A Probabilistic Perspective, The MIT Press. The inputs are returned by\n    `forward_filter` function.\n\n    Args:\n      filtered_means: Means of the per-timestep filtered marginal\n        distributions p(z_t | x_{:t}), as a Tensor of shape\n        `sample_shape(x) + batch_shape + [num_timesteps, latent_size]`.\n      filtered_covs: Covariances of the per-timestep filtered marginal\n        distributions p(z_t | x_{:t}), as a Tensor of shape\n        `batch_shape + [num_timesteps, latent_size, latent_size]`.\n      predicted_means: Means of the per-timestep predictive\n         distributions over latent states, p(z_{t+1} | x_{:t}), as a\n         Tensor of shape `sample_shape(x) + batch_shape +\n         [num_timesteps, latent_size]`.\n      predicted_covs: Covariances of the per-timestep predictive\n         distributions over latent states, p(z_{t+1} | x_{:t}), as a\n         Tensor of shape `batch_shape + [num_timesteps, latent_size,\n         latent_size]`.\n\n    Returns:\n      posterior_means: Means of the smoothed marginal distributions\n        p(z_t | x_{1:T}), as a Tensor of shape\n        `sample_shape(x) + batch_shape + [num_timesteps, latent_size]`,\n        which is of the same shape as filtered_means.\n      posterior_covs: Covariances of the smoothed marginal distributions\n        p(z_t | x_{1:T}), as a Tensor of shape\n        `batch_shape + [num_timesteps, latent_size, latent_size]`.\n        which is of the same shape as filtered_covs.\n    \"\"\"\n    with tf.name_scope(\"backward_pass\"):\n      filtered_means = tf.convert_to_tensor(\n          value=filtered_means, name=\"filtered_means\")\n      filtered_covs = tf.convert_to_tensor(\n          value=filtered_covs, name=\"filtered_covs\")\n      predicted_means = tf.convert_to_tensor(\n          value=predicted_means, name=\"predicted_means\")\n      predicted_covs = tf.convert_to_tensor(\n          value=predicted_covs, name=\"predicted_covs\")\n\n      # To scan over time dimension, we need to move 'num_timesteps' from the\n      # event shape to the initial dimension of the tensor.\n      filtered_means = distribution_util.move_dimension(filtered_means, -2, 0)\n      filtered_covs = distribution_util.move_dimension(filtered_covs, -3, 0)\n      predicted_means = distribution_util.move_dimension(predicted_means, -2, 0)\n      predicted_covs = distribution_util.move_dimension(predicted_covs, -3, 0)\n\n      # The means are assumed to be vectors. Adding a dummy index to\n      # ensure the `matmul` op working smoothly.\n      filtered_means = filtered_means[..., tf.newaxis]\n      predicted_means = predicted_means[..., tf.newaxis]\n\n      initial_backward_mean = predicted_means[-1, ...]\n      initial_backward_cov = predicted_covs[-1, ...]\n\n      num_timesteps = tf.shape(input=filtered_means)[0]\n      initial_state = BackwardPassState(\n          backward_mean=initial_backward_mean,\n          backward_cov=initial_backward_cov,\n          timestep=self.initial_step + num_timesteps - 1)\n\n      update_step_fn = build_backward_pass_step(\n          self.get_transition_matrix_for_timestep)\n\n      # For backward pass, it scans the `elems` from last to first.\n      posterior_states = tf.scan(update_step_fn,\n                                 elems=(filtered_means,\n                                        filtered_covs,\n                                        predicted_means,\n                                        predicted_covs),\n                                 initializer=initial_state,\n                                 reverse=True)\n\n      # Move the time dimension back into the event shape.\n      posterior_means = distribution_util.move_dimension(\n          posterior_states.backward_mean[..., 0], 0, -2)\n      posterior_covs = distribution_util.move_dimension(\n          posterior_states.backward_cov, 0, -3)\n\n      return (posterior_means, posterior_covs)", "code_tokens": ["def", "backward_smoothing_pass", "(", "self", ",", "filtered_means", ",", "filtered_covs", ",", "predicted_means", ",", "predicted_covs", ")", ":", "with", "tf", ".", "name_scope", "(", "\"backward_pass\"", ")", ":", "filtered_means", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "filtered_means", ",", "name", "=", "\"filtered_means\"", ")", "filtered_covs", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "filtered_covs", ",", "name", "=", "\"filtered_covs\"", ")", "predicted_means", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "predicted_means", ",", "name", "=", "\"predicted_means\"", ")", "predicted_covs", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "predicted_covs", ",", "name", "=", "\"predicted_covs\"", ")", "# To scan over time dimension, we need to move 'num_timesteps' from the", "# event shape to the initial dimension of the tensor.", "filtered_means", "=", "distribution_util", ".", "move_dimension", "(", "filtered_means", ",", "-", "2", ",", "0", ")", "filtered_covs", "=", "distribution_util", ".", "move_dimension", "(", "filtered_covs", ",", "-", "3", ",", "0", ")", "predicted_means", "=", "distribution_util", ".", "move_dimension", "(", "predicted_means", ",", "-", "2", ",", "0", ")", "predicted_covs", "=", "distribution_util", ".", "move_dimension", "(", "predicted_covs", ",", "-", "3", ",", "0", ")", "# The means are assumed to be vectors. Adding a dummy index to", "# ensure the `matmul` op working smoothly.", "filtered_means", "=", "filtered_means", "[", "...", ",", "tf", ".", "newaxis", "]", "predicted_means", "=", "predicted_means", "[", "...", ",", "tf", ".", "newaxis", "]", "initial_backward_mean", "=", "predicted_means", "[", "-", "1", ",", "...", "]", "initial_backward_cov", "=", "predicted_covs", "[", "-", "1", ",", "...", "]", "num_timesteps", "=", "tf", ".", "shape", "(", "input", "=", "filtered_means", ")", "[", "0", "]", "initial_state", "=", "BackwardPassState", "(", "backward_mean", "=", "initial_backward_mean", ",", "backward_cov", "=", "initial_backward_cov", ",", "timestep", "=", "self", ".", "initial_step", "+", "num_timesteps", "-", "1", ")", "update_step_fn", "=", "build_backward_pass_step", "(", "self", ".", "get_transition_matrix_for_timestep", ")", "# For backward pass, it scans the `elems` from last to first.", "posterior_states", "=", "tf", ".", "scan", "(", "update_step_fn", ",", "elems", "=", "(", "filtered_means", ",", "filtered_covs", ",", "predicted_means", ",", "predicted_covs", ")", ",", "initializer", "=", "initial_state", ",", "reverse", "=", "True", ")", "# Move the time dimension back into the event shape.", "posterior_means", "=", "distribution_util", ".", "move_dimension", "(", "posterior_states", ".", "backward_mean", "[", "...", ",", "0", "]", ",", "0", ",", "-", "2", ")", "posterior_covs", "=", "distribution_util", ".", "move_dimension", "(", "posterior_states", ".", "backward_cov", ",", "0", ",", "-", "3", ")", "return", "(", "posterior_means", ",", "posterior_covs", ")"], "docstring": "Run the backward pass in Kalman smoother.\n\n    The backward smoothing is using Rauch, Tung and Striebel smoother as\n    as discussed in section 18.3.2 of Kevin P. Murphy, 2012, Machine Learning:\n    A Probabilistic Perspective, The MIT Press. The inputs are returned by\n    `forward_filter` function.\n\n    Args:\n      filtered_means: Means of the per-timestep filtered marginal\n        distributions p(z_t | x_{:t}), as a Tensor of shape\n        `sample_shape(x) + batch_shape + [num_timesteps, latent_size]`.\n      filtered_covs: Covariances of the per-timestep filtered marginal\n        distributions p(z_t | x_{:t}), as a Tensor of shape\n        `batch_shape + [num_timesteps, latent_size, latent_size]`.\n      predicted_means: Means of the per-timestep predictive\n         distributions over latent states, p(z_{t+1} | x_{:t}), as a\n         Tensor of shape `sample_shape(x) + batch_shape +\n         [num_timesteps, latent_size]`.\n      predicted_covs: Covariances of the per-timestep predictive\n         distributions over latent states, p(z_{t+1} | x_{:t}), as a\n         Tensor of shape `batch_shape + [num_timesteps, latent_size,\n         latent_size]`.\n\n    Returns:\n      posterior_means: Means of the smoothed marginal distributions\n        p(z_t | x_{1:T}), as a Tensor of shape\n        `sample_shape(x) + batch_shape + [num_timesteps, latent_size]`,\n        which is of the same shape as filtered_means.\n      posterior_covs: Covariances of the smoothed marginal distributions\n        p(z_t | x_{1:T}), as a Tensor of shape\n        `batch_shape + [num_timesteps, latent_size, latent_size]`.\n        which is of the same shape as filtered_covs.", "docstring_tokens": ["Run", "the", "backward", "pass", "in", "Kalman", "smoother", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L454-L541", "partition": "test", "index": 950, "time": "2019-01-18 08:43:06"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "backward_smoothing_update", "original_string": "def backward_smoothing_update(filtered_mean,\n                              filtered_cov,\n                              predicted_mean,\n                              predicted_cov,\n                              next_posterior_mean,\n                              next_posterior_cov,\n                              transition_matrix):\n  \"\"\"Backward update for a Kalman smoother.\n\n  Give the `filtered_mean` mu(t | t), `filtered_cov` sigma(t | t),\n  `predicted_mean` mu(t+1 | t) and `predicted_cov` sigma(t+1 | t),\n  as returns from the `forward_filter` function, as well as\n  `next_posterior_mean` mu(t+1 | 1:T) and `next_posterior_cov` sigma(t+1 | 1:T),\n  if the `transition_matrix` of states from time t to time t+1\n  is given as A(t+1), the 1 step backward smoothed distribution parameter\n  could be calculated as:\n  p(z(t) | Obs(1:T)) = N( mu(t | 1:T), sigma(t | 1:T)),\n  mu(t | 1:T) = mu(t | t) + J(t) * (mu(t+1 | 1:T) - mu(t+1 | t)),\n  sigma(t | 1:T) = sigma(t | t)\n                   + J(t) * (sigma(t+1 | 1:T) - sigma(t+1 | t) * J(t)',\n  J(t) = sigma(t | t) * A(t+1)' / sigma(t+1 | t),\n  where all the multiplications are matrix multiplication, and `/` is\n  the matrix inverse. J(t) is the backward Kalman gain matrix.\n\n  The algorithm can be intialized from mu(T | 1:T) and sigma(T | 1:T),\n  which are the last step parameters returned by forward_filter.\n\n\n  Args:\n    filtered_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t | t).\n    filtered_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t | t).\n    predicted_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t+1 | t).\n    predicted_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t+1 | t).\n    next_posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t+1 | 1:T).\n    next_posterior_cov: `Tensor` with event shape `[latent_size, latent_size]`\n      and batch shape `B`, containing sigma(t+1 | 1:T).\n    transition_matrix: `LinearOperator` with shape\n      `[latent_size, latent_size]` and batch shape broadcastable\n      to `B`.\n\n  Returns:\n    posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t | 1:T).\n    posterior_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t | 1:T).\n  \"\"\"\n  # Compute backward Kalman gain:\n  # J = F * T' * P^{-1}\n  # Since both F(iltered) and P(redictive) are cov matrices,\n  # thus self-adjoint, we can take the transpose.\n  # computation:\n  #      = (P^{-1} * T * F)'\n  #      = (P^{-1} * tmp_gain_cov) '\n  #      = (P \\ tmp_gain_cov)'\n  tmp_gain_cov = transition_matrix.matmul(filtered_cov)\n  predicted_cov_chol = tf.linalg.cholesky(predicted_cov)\n  gain_transpose = tf.linalg.cholesky_solve(predicted_cov_chol, tmp_gain_cov)\n\n  posterior_mean = (filtered_mean +\n                    tf.linalg.matmul(gain_transpose,\n                                     next_posterior_mean - predicted_mean,\n                                     adjoint_a=True))\n  posterior_cov = (\n      filtered_cov +\n      tf.linalg.matmul(gain_transpose,\n                       tf.linalg.matmul(\n                           next_posterior_cov - predicted_cov, gain_transpose),\n                       adjoint_a=True))\n\n  return (posterior_mean, posterior_cov)", "language": "python", "code": "def backward_smoothing_update(filtered_mean,\n                              filtered_cov,\n                              predicted_mean,\n                              predicted_cov,\n                              next_posterior_mean,\n                              next_posterior_cov,\n                              transition_matrix):\n  \"\"\"Backward update for a Kalman smoother.\n\n  Give the `filtered_mean` mu(t | t), `filtered_cov` sigma(t | t),\n  `predicted_mean` mu(t+1 | t) and `predicted_cov` sigma(t+1 | t),\n  as returns from the `forward_filter` function, as well as\n  `next_posterior_mean` mu(t+1 | 1:T) and `next_posterior_cov` sigma(t+1 | 1:T),\n  if the `transition_matrix` of states from time t to time t+1\n  is given as A(t+1), the 1 step backward smoothed distribution parameter\n  could be calculated as:\n  p(z(t) | Obs(1:T)) = N( mu(t | 1:T), sigma(t | 1:T)),\n  mu(t | 1:T) = mu(t | t) + J(t) * (mu(t+1 | 1:T) - mu(t+1 | t)),\n  sigma(t | 1:T) = sigma(t | t)\n                   + J(t) * (sigma(t+1 | 1:T) - sigma(t+1 | t) * J(t)',\n  J(t) = sigma(t | t) * A(t+1)' / sigma(t+1 | t),\n  where all the multiplications are matrix multiplication, and `/` is\n  the matrix inverse. J(t) is the backward Kalman gain matrix.\n\n  The algorithm can be intialized from mu(T | 1:T) and sigma(T | 1:T),\n  which are the last step parameters returned by forward_filter.\n\n\n  Args:\n    filtered_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t | t).\n    filtered_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t | t).\n    predicted_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t+1 | t).\n    predicted_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t+1 | t).\n    next_posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t+1 | 1:T).\n    next_posterior_cov: `Tensor` with event shape `[latent_size, latent_size]`\n      and batch shape `B`, containing sigma(t+1 | 1:T).\n    transition_matrix: `LinearOperator` with shape\n      `[latent_size, latent_size]` and batch shape broadcastable\n      to `B`.\n\n  Returns:\n    posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t | 1:T).\n    posterior_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t | 1:T).\n  \"\"\"\n  # Compute backward Kalman gain:\n  # J = F * T' * P^{-1}\n  # Since both F(iltered) and P(redictive) are cov matrices,\n  # thus self-adjoint, we can take the transpose.\n  # computation:\n  #      = (P^{-1} * T * F)'\n  #      = (P^{-1} * tmp_gain_cov) '\n  #      = (P \\ tmp_gain_cov)'\n  tmp_gain_cov = transition_matrix.matmul(filtered_cov)\n  predicted_cov_chol = tf.linalg.cholesky(predicted_cov)\n  gain_transpose = tf.linalg.cholesky_solve(predicted_cov_chol, tmp_gain_cov)\n\n  posterior_mean = (filtered_mean +\n                    tf.linalg.matmul(gain_transpose,\n                                     next_posterior_mean - predicted_mean,\n                                     adjoint_a=True))\n  posterior_cov = (\n      filtered_cov +\n      tf.linalg.matmul(gain_transpose,\n                       tf.linalg.matmul(\n                           next_posterior_cov - predicted_cov, gain_transpose),\n                       adjoint_a=True))\n\n  return (posterior_mean, posterior_cov)", "code_tokens": ["def", "backward_smoothing_update", "(", "filtered_mean", ",", "filtered_cov", ",", "predicted_mean", ",", "predicted_cov", ",", "next_posterior_mean", ",", "next_posterior_cov", ",", "transition_matrix", ")", ":", "# Compute backward Kalman gain:", "# J = F * T' * P^{-1}", "# Since both F(iltered) and P(redictive) are cov matrices,", "# thus self-adjoint, we can take the transpose.", "# computation:", "#      = (P^{-1} * T * F)'", "#      = (P^{-1} * tmp_gain_cov) '", "#      = (P \\ tmp_gain_cov)'", "tmp_gain_cov", "=", "transition_matrix", ".", "matmul", "(", "filtered_cov", ")", "predicted_cov_chol", "=", "tf", ".", "linalg", ".", "cholesky", "(", "predicted_cov", ")", "gain_transpose", "=", "tf", ".", "linalg", ".", "cholesky_solve", "(", "predicted_cov_chol", ",", "tmp_gain_cov", ")", "posterior_mean", "=", "(", "filtered_mean", "+", "tf", ".", "linalg", ".", "matmul", "(", "gain_transpose", ",", "next_posterior_mean", "-", "predicted_mean", ",", "adjoint_a", "=", "True", ")", ")", "posterior_cov", "=", "(", "filtered_cov", "+", "tf", ".", "linalg", ".", "matmul", "(", "gain_transpose", ",", "tf", ".", "linalg", ".", "matmul", "(", "next_posterior_cov", "-", "predicted_cov", ",", "gain_transpose", ")", ",", "adjoint_a", "=", "True", ")", ")", "return", "(", "posterior_mean", ",", "posterior_cov", ")"], "docstring": "Backward update for a Kalman smoother.\n\n  Give the `filtered_mean` mu(t | t), `filtered_cov` sigma(t | t),\n  `predicted_mean` mu(t+1 | t) and `predicted_cov` sigma(t+1 | t),\n  as returns from the `forward_filter` function, as well as\n  `next_posterior_mean` mu(t+1 | 1:T) and `next_posterior_cov` sigma(t+1 | 1:T),\n  if the `transition_matrix` of states from time t to time t+1\n  is given as A(t+1), the 1 step backward smoothed distribution parameter\n  could be calculated as:\n  p(z(t) | Obs(1:T)) = N( mu(t | 1:T), sigma(t | 1:T)),\n  mu(t | 1:T) = mu(t | t) + J(t) * (mu(t+1 | 1:T) - mu(t+1 | t)),\n  sigma(t | 1:T) = sigma(t | t)\n                   + J(t) * (sigma(t+1 | 1:T) - sigma(t+1 | t) * J(t)',\n  J(t) = sigma(t | t) * A(t+1)' / sigma(t+1 | t),\n  where all the multiplications are matrix multiplication, and `/` is\n  the matrix inverse. J(t) is the backward Kalman gain matrix.\n\n  The algorithm can be intialized from mu(T | 1:T) and sigma(T | 1:T),\n  which are the last step parameters returned by forward_filter.\n\n\n  Args:\n    filtered_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t | t).\n    filtered_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t | t).\n    predicted_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t+1 | t).\n    predicted_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t+1 | t).\n    next_posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t+1 | 1:T).\n    next_posterior_cov: `Tensor` with event shape `[latent_size, latent_size]`\n      and batch shape `B`, containing sigma(t+1 | 1:T).\n    transition_matrix: `LinearOperator` with shape\n      `[latent_size, latent_size]` and batch shape broadcastable\n      to `B`.\n\n  Returns:\n    posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t | 1:T).\n    posterior_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t | 1:T).", "docstring_tokens": ["Backward", "update", "for", "a", "Kalman", "smoother", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L1198-L1272", "partition": "test", "index": 941, "time": "2019-01-18 08:43:06"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "build_backward_pass_step", "original_string": "def build_backward_pass_step(get_transition_matrix_for_timestep):\n  \"\"\"Build a callable that perform one step for backward smoothing.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n\n  Returns:\n    backward_pass_step: a callable that updates a BackwardPassState\n      from timestep `t` to `t-1`.\n  \"\"\"\n\n  def backward_pass_step(state,\n                         filtered_parameters):\n    \"\"\"Run a single step of backward smoothing.\"\"\"\n\n    (filtered_mean, filtered_cov,\n     predicted_mean, predicted_cov) = filtered_parameters\n    transition_matrix = get_transition_matrix_for_timestep(state.timestep)\n\n    next_posterior_mean = state.backward_mean\n    next_posterior_cov = state.backward_cov\n\n    posterior_mean, posterior_cov = backward_smoothing_update(\n        filtered_mean,\n        filtered_cov,\n        predicted_mean,\n        predicted_cov,\n        next_posterior_mean,\n        next_posterior_cov,\n        transition_matrix)\n\n    return BackwardPassState(backward_mean=posterior_mean,\n                             backward_cov=posterior_cov,\n                             timestep=state.timestep-1)\n\n  return backward_pass_step", "language": "python", "code": "def build_backward_pass_step(get_transition_matrix_for_timestep):\n  \"\"\"Build a callable that perform one step for backward smoothing.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n\n  Returns:\n    backward_pass_step: a callable that updates a BackwardPassState\n      from timestep `t` to `t-1`.\n  \"\"\"\n\n  def backward_pass_step(state,\n                         filtered_parameters):\n    \"\"\"Run a single step of backward smoothing.\"\"\"\n\n    (filtered_mean, filtered_cov,\n     predicted_mean, predicted_cov) = filtered_parameters\n    transition_matrix = get_transition_matrix_for_timestep(state.timestep)\n\n    next_posterior_mean = state.backward_mean\n    next_posterior_cov = state.backward_cov\n\n    posterior_mean, posterior_cov = backward_smoothing_update(\n        filtered_mean,\n        filtered_cov,\n        predicted_mean,\n        predicted_cov,\n        next_posterior_mean,\n        next_posterior_cov,\n        transition_matrix)\n\n    return BackwardPassState(backward_mean=posterior_mean,\n                             backward_cov=posterior_cov,\n                             timestep=state.timestep-1)\n\n  return backward_pass_step", "code_tokens": ["def", "build_backward_pass_step", "(", "get_transition_matrix_for_timestep", ")", ":", "def", "backward_pass_step", "(", "state", ",", "filtered_parameters", ")", ":", "\"\"\"Run a single step of backward smoothing.\"\"\"", "(", "filtered_mean", ",", "filtered_cov", ",", "predicted_mean", ",", "predicted_cov", ")", "=", "filtered_parameters", "transition_matrix", "=", "get_transition_matrix_for_timestep", "(", "state", ".", "timestep", ")", "next_posterior_mean", "=", "state", ".", "backward_mean", "next_posterior_cov", "=", "state", ".", "backward_cov", "posterior_mean", ",", "posterior_cov", "=", "backward_smoothing_update", "(", "filtered_mean", ",", "filtered_cov", ",", "predicted_mean", ",", "predicted_cov", ",", "next_posterior_mean", ",", "next_posterior_cov", ",", "transition_matrix", ")", "return", "BackwardPassState", "(", "backward_mean", "=", "posterior_mean", ",", "backward_cov", "=", "posterior_cov", ",", "timestep", "=", "state", ".", "timestep", "-", "1", ")", "return", "backward_pass_step"], "docstring": "Build a callable that perform one step for backward smoothing.\n\n  Args:\n    get_transition_matrix_for_timestep: callable taking a timestep\n      as an integer `Tensor` argument, and returning a `LinearOperator`\n      of shape `[latent_size, latent_size]`.\n\n  Returns:\n    backward_pass_step: a callable that updates a BackwardPassState\n      from timestep `t` to `t-1`.", "docstring_tokens": ["Build", "a", "callable", "that", "perform", "one", "step", "for", "backward", "smoothing", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L1158-L1195", "partition": "test", "index": 940, "time": "2019-01-18 08:43:06"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "LinearGaussianStateSpaceModel.posterior_marginals", "original_string": "def posterior_marginals(self, x, mask=None):\n    \"\"\"Run a Kalman smoother to return posterior mean and cov.\n\n    Note that the returned values `smoothed_means` depend on the observed\n    time series `x`, while the `smoothed_covs` are independent\n    of the observed series; i.e., they depend only on the model itself.\n    This means that the mean values have shape `concat([sample_shape(x),\n    batch_shape, [num_timesteps, {latent/observation}_size]])`,\n    while the covariances have shape `concat[(batch_shape, [num_timesteps,\n    {latent/observation}_size, {latent/observation}_size]])`, which\n    does not depend on the sample shape.\n\n    This function only performs smoothing. If the user wants the\n    intermediate values, which are returned by filtering pass `forward_filter`,\n    one could get it by:\n    ```\n    (log_likelihoods,\n     filtered_means, filtered_covs,\n     predicted_means, predicted_covs,\n     observation_means, observation_covs) = model.forward_filter(x)\n    smoothed_means, smoothed_covs = model.backward_smoothing_pass(x)\n    ```\n    where `x` is an observation sequence.\n\n    Args:\n      x: a float-type `Tensor` with rightmost dimensions\n        `[num_timesteps, observation_size]` matching\n        `self.event_shape`. Additional dimensions must match or be\n        broadcastable to `self.batch_shape`; any further dimensions\n        are interpreted as a sample shape.\n      mask: optional bool-type `Tensor` with rightmost dimension\n        `[num_timesteps]`; `True` values specify that the value of `x`\n        at that timestep is masked, i.e., not conditioned on. Additional\n        dimensions must match or be broadcastable to `self.batch_shape`; any\n        further dimensions must match or be broadcastable to the sample\n        shape of `x`.\n        Default value: `None`.\n\n    Returns:\n      smoothed_means: Means of the per-timestep smoothed\n         distributions over latent states, p(x_{t} | x_{:T}), as a\n         Tensor of shape `sample_shape(x) + batch_shape +\n         [num_timesteps, observation_size]`.\n      smoothed_covs: Covariances of the per-timestep smoothed\n         distributions over latent states, p(x_{t} | x_{:T}), as a\n         Tensor of shape `sample_shape(mask) + batch_shape + [num_timesteps,\n         observation_size, observation_size]`. Note that the covariances depend\n         only on the model and the mask, not on the data, so this may have fewer\n         dimensions than `filtered_means`.\n    \"\"\"\n\n    with tf.name_scope(\"smooth\"):\n      x = tf.convert_to_tensor(value=x, name=\"x\")\n      (_, filtered_means, filtered_covs,\n       predicted_means, predicted_covs, _, _) = self.forward_filter(\n           x, mask=mask)\n\n      (smoothed_means, smoothed_covs) = self.backward_smoothing_pass(\n          filtered_means, filtered_covs,\n          predicted_means, predicted_covs)\n\n      return (smoothed_means, smoothed_covs)", "language": "python", "code": "def posterior_marginals(self, x, mask=None):\n    \"\"\"Run a Kalman smoother to return posterior mean and cov.\n\n    Note that the returned values `smoothed_means` depend on the observed\n    time series `x`, while the `smoothed_covs` are independent\n    of the observed series; i.e., they depend only on the model itself.\n    This means that the mean values have shape `concat([sample_shape(x),\n    batch_shape, [num_timesteps, {latent/observation}_size]])`,\n    while the covariances have shape `concat[(batch_shape, [num_timesteps,\n    {latent/observation}_size, {latent/observation}_size]])`, which\n    does not depend on the sample shape.\n\n    This function only performs smoothing. If the user wants the\n    intermediate values, which are returned by filtering pass `forward_filter`,\n    one could get it by:\n    ```\n    (log_likelihoods,\n     filtered_means, filtered_covs,\n     predicted_means, predicted_covs,\n     observation_means, observation_covs) = model.forward_filter(x)\n    smoothed_means, smoothed_covs = model.backward_smoothing_pass(x)\n    ```\n    where `x` is an observation sequence.\n\n    Args:\n      x: a float-type `Tensor` with rightmost dimensions\n        `[num_timesteps, observation_size]` matching\n        `self.event_shape`. Additional dimensions must match or be\n        broadcastable to `self.batch_shape`; any further dimensions\n        are interpreted as a sample shape.\n      mask: optional bool-type `Tensor` with rightmost dimension\n        `[num_timesteps]`; `True` values specify that the value of `x`\n        at that timestep is masked, i.e., not conditioned on. Additional\n        dimensions must match or be broadcastable to `self.batch_shape`; any\n        further dimensions must match or be broadcastable to the sample\n        shape of `x`.\n        Default value: `None`.\n\n    Returns:\n      smoothed_means: Means of the per-timestep smoothed\n         distributions over latent states, p(x_{t} | x_{:T}), as a\n         Tensor of shape `sample_shape(x) + batch_shape +\n         [num_timesteps, observation_size]`.\n      smoothed_covs: Covariances of the per-timestep smoothed\n         distributions over latent states, p(x_{t} | x_{:T}), as a\n         Tensor of shape `sample_shape(mask) + batch_shape + [num_timesteps,\n         observation_size, observation_size]`. Note that the covariances depend\n         only on the model and the mask, not on the data, so this may have fewer\n         dimensions than `filtered_means`.\n    \"\"\"\n\n    with tf.name_scope(\"smooth\"):\n      x = tf.convert_to_tensor(value=x, name=\"x\")\n      (_, filtered_means, filtered_covs,\n       predicted_means, predicted_covs, _, _) = self.forward_filter(\n           x, mask=mask)\n\n      (smoothed_means, smoothed_covs) = self.backward_smoothing_pass(\n          filtered_means, filtered_covs,\n          predicted_means, predicted_covs)\n\n      return (smoothed_means, smoothed_covs)", "code_tokens": ["def", "posterior_marginals", "(", "self", ",", "x", ",", "mask", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "\"smooth\"", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "\"x\"", ")", "(", "_", ",", "filtered_means", ",", "filtered_covs", ",", "predicted_means", ",", "predicted_covs", ",", "_", ",", "_", ")", "=", "self", ".", "forward_filter", "(", "x", ",", "mask", "=", "mask", ")", "(", "smoothed_means", ",", "smoothed_covs", ")", "=", "self", ".", "backward_smoothing_pass", "(", "filtered_means", ",", "filtered_covs", ",", "predicted_means", ",", "predicted_covs", ")", "return", "(", "smoothed_means", ",", "smoothed_covs", ")"], "docstring": "Run a Kalman smoother to return posterior mean and cov.\n\n    Note that the returned values `smoothed_means` depend on the observed\n    time series `x`, while the `smoothed_covs` are independent\n    of the observed series; i.e., they depend only on the model itself.\n    This means that the mean values have shape `concat([sample_shape(x),\n    batch_shape, [num_timesteps, {latent/observation}_size]])`,\n    while the covariances have shape `concat[(batch_shape, [num_timesteps,\n    {latent/observation}_size, {latent/observation}_size]])`, which\n    does not depend on the sample shape.\n\n    This function only performs smoothing. If the user wants the\n    intermediate values, which are returned by filtering pass `forward_filter`,\n    one could get it by:\n    ```\n    (log_likelihoods,\n     filtered_means, filtered_covs,\n     predicted_means, predicted_covs,\n     observation_means, observation_covs) = model.forward_filter(x)\n    smoothed_means, smoothed_covs = model.backward_smoothing_pass(x)\n    ```\n    where `x` is an observation sequence.\n\n    Args:\n      x: a float-type `Tensor` with rightmost dimensions\n        `[num_timesteps, observation_size]` matching\n        `self.event_shape`. Additional dimensions must match or be\n        broadcastable to `self.batch_shape`; any further dimensions\n        are interpreted as a sample shape.\n      mask: optional bool-type `Tensor` with rightmost dimension\n        `[num_timesteps]`; `True` values specify that the value of `x`\n        at that timestep is masked, i.e., not conditioned on. Additional\n        dimensions must match or be broadcastable to `self.batch_shape`; any\n        further dimensions must match or be broadcastable to the sample\n        shape of `x`.\n        Default value: `None`.\n\n    Returns:\n      smoothed_means: Means of the per-timestep smoothed\n         distributions over latent states, p(x_{t} | x_{:T}), as a\n         Tensor of shape `sample_shape(x) + batch_shape +\n         [num_timesteps, observation_size]`.\n      smoothed_covs: Covariances of the per-timestep smoothed\n         distributions over latent states, p(x_{t} | x_{:T}), as a\n         Tensor of shape `sample_shape(mask) + batch_shape + [num_timesteps,\n         observation_size, observation_size]`. Note that the covariances depend\n         only on the model and the mask, not on the data, so this may have fewer\n         dimensions than `filtered_means`.", "docstring_tokens": ["Run", "a", "Kalman", "smoother", "to", "return", "posterior", "mean", "and", "cov", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L914-L975", "partition": "test", "index": 952, "time": "2019-01-18 08:43:06"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/linalg.py", "func_name": "_lu_solve_assertions", "original_string": "def _lu_solve_assertions(lower_upper, perm, rhs, validate_args):\n  \"\"\"Returns list of assertions related to `lu_solve` assumptions.\"\"\"\n  assertions = _lu_reconstruct_assertions(lower_upper, perm, validate_args)\n\n  message = 'Input `rhs` must have at least 2 dimensions.'\n  if rhs.shape.ndims is not None:\n    if rhs.shape.ndims < 2:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        tf.compat.v1.assert_rank_at_least(rhs, rank=2, message=message))\n\n  message = '`lower_upper.shape[-1]` must equal `rhs.shape[-1]`.'\n  if (tf.compat.dimension_value(lower_upper.shape[-1]) is not None and\n      tf.compat.dimension_value(rhs.shape[-2]) is not None):\n    if lower_upper.shape[-1] != rhs.shape[-2]:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        tf.compat.v1.assert_equal(\n            tf.shape(input=lower_upper)[-1],\n            tf.shape(input=rhs)[-2],\n            message=message))\n\n  return assertions", "language": "python", "code": "def _lu_solve_assertions(lower_upper, perm, rhs, validate_args):\n  \"\"\"Returns list of assertions related to `lu_solve` assumptions.\"\"\"\n  assertions = _lu_reconstruct_assertions(lower_upper, perm, validate_args)\n\n  message = 'Input `rhs` must have at least 2 dimensions.'\n  if rhs.shape.ndims is not None:\n    if rhs.shape.ndims < 2:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        tf.compat.v1.assert_rank_at_least(rhs, rank=2, message=message))\n\n  message = '`lower_upper.shape[-1]` must equal `rhs.shape[-1]`.'\n  if (tf.compat.dimension_value(lower_upper.shape[-1]) is not None and\n      tf.compat.dimension_value(rhs.shape[-2]) is not None):\n    if lower_upper.shape[-1] != rhs.shape[-2]:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        tf.compat.v1.assert_equal(\n            tf.shape(input=lower_upper)[-1],\n            tf.shape(input=rhs)[-2],\n            message=message))\n\n  return assertions", "code_tokens": ["def", "_lu_solve_assertions", "(", "lower_upper", ",", "perm", ",", "rhs", ",", "validate_args", ")", ":", "assertions", "=", "_lu_reconstruct_assertions", "(", "lower_upper", ",", "perm", ",", "validate_args", ")", "message", "=", "'Input `rhs` must have at least 2 dimensions.'", "if", "rhs", ".", "shape", ".", "ndims", "is", "not", "None", ":", "if", "rhs", ".", "shape", ".", "ndims", "<", "2", ":", "raise", "ValueError", "(", "message", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "tf", ".", "compat", ".", "v1", ".", "assert_rank_at_least", "(", "rhs", ",", "rank", "=", "2", ",", "message", "=", "message", ")", ")", "message", "=", "'`lower_upper.shape[-1]` must equal `rhs.shape[-1]`.'", "if", "(", "tf", ".", "compat", ".", "dimension_value", "(", "lower_upper", ".", "shape", "[", "-", "1", "]", ")", "is", "not", "None", "and", "tf", ".", "compat", ".", "dimension_value", "(", "rhs", ".", "shape", "[", "-", "2", "]", ")", "is", "not", "None", ")", ":", "if", "lower_upper", ".", "shape", "[", "-", "1", "]", "!=", "rhs", ".", "shape", "[", "-", "2", "]", ":", "raise", "ValueError", "(", "message", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "tf", ".", "compat", ".", "v1", ".", "assert_equal", "(", "tf", ".", "shape", "(", "input", "=", "lower_upper", ")", "[", "-", "1", "]", ",", "tf", ".", "shape", "(", "input", "=", "rhs", ")", "[", "-", "2", "]", ",", "message", "=", "message", ")", ")", "return", "assertions"], "docstring": "Returns list of assertions related to `lu_solve` assumptions.", "docstring_tokens": ["Returns", "list", "of", "assertions", "related", "to", "lu_solve", "assumptions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/linalg.py#L711-L735", "partition": "test", "index": 1110, "time": "2019-01-18 13:57:43"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/program_transformations.py", "func_name": "make_value_setter", "original_string": "def make_value_setter(**model_kwargs):\n  \"\"\"Creates a value-setting interceptor.\n\n  This function creates an interceptor that sets values of Edward2 random\n  variable objects. This is useful for a range of tasks, including conditioning\n  on observed data, sampling from posterior predictive distributions, and as a\n  building block of inference primitives such as computing log joint\n  probabilities (see examples below).\n\n  Args:\n    **model_kwargs: dict of str to Tensor. Keys are the names of random\n      variables in the model to which this interceptor is being applied. Values\n      are Tensors to set their value to. Variables not included in this dict\n      will not be set and will maintain their existing value semantics (by\n      default, a sample from the parent-conditional distribution).\n\n  Returns:\n    set_values: function that sets the value of intercepted ops.\n\n  #### Examples\n\n  Consider for illustration a model with latent `z` and\n  observed `x`, and a corresponding trainable posterior model:\n\n  ```python\n  num_observations = 10\n  def model():\n    z = ed.Normal(loc=0, scale=1., name='z')  # log rate\n    x = ed.Poisson(rate=tf.exp(z) * tf.ones(num_observations), name='x')\n    return x\n\n  def variational_model():\n    return ed.Normal(loc=tf.Variable(0.),\n                     scale=tf.nn.softplus(tf.Variable(-4.)),\n                     name='z')  # for simplicity, match name of the model RV.\n  ```\n\n  We can use a value-setting interceptor to condition the model on observed\n  data. This approach is slightly more cumbersome than that of partially\n  evaluating the complete log-joint function, but has the potential advantage\n  that it returns a new model callable, which may be used to sample downstream\n  variables, passed into additional transformations, etc.\n\n  ```python\n  x_observed = np.array([6, 3, 1, 8, 7, 0, 6, 4, 7, 5])\n  def observed_model():\n    with ed.interception(make_value_setter(x=x_observed)):\n      model()\n  observed_log_joint_fn = ed.make_log_joint_fn(observed_model)\n\n  # After fixing 'x', the observed log joint is now only a function of 'z'.\n  # This enables us to define a variational lower bound,\n  # `E_q[ log p(x, z) - log q(z)]`, simply by evaluating the observed and\n  # variational log joints at variational samples.\n  variational_log_joint_fn = ed.make_log_joint_fn(variational_model)\n  with ed.tape() as variational_sample:  # Sample trace from variational model.\n    variational_model()\n  elbo_loss = -(observed_log_joint_fn(**variational_sample) -\n                variational_log_joint_fn(**variational_sample))\n  ```\n\n  After performing inference by minimizing the variational loss, a value-setting\n  interceptor enables simulation from the posterior predictive distribution:\n\n  ```python\n  with ed.tape() as posterior_samples:  # tape is a map {rv.name : rv}\n    variational_model()\n  with ed.interception(ed.make_value_setter(**posterior_samples)):\n    x = model()\n  # x is a sample from p(X | Z = z') where z' ~ q(z) (the variational model)\n  ```\n\n  As another example, using a value setter inside of `ed.tape` enables\n  computing the log joint probability, by setting all variables to\n  posterior values and then accumulating the log probs of those values under\n  the induced parent-conditional distributions. This is one way that we could\n  have implemented `ed.make_log_joint_fn`:\n\n  ```python\n  def make_log_joint_fn_demo(model):\n    def log_joint_fn(**model_kwargs):\n      with ed.tape() as model_tape:\n        with ed.make_value_setter(**model_kwargs):\n          model()\n\n      # accumulate sum_i log p(X_i = x_i | X_{:i-1} = x_{:i-1})\n      log_prob = 0.\n      for rv in model_tape.values():\n        log_prob += tf.reduce_sum(rv.log_prob(rv.value))\n\n      return log_prob\n    return log_joint_fn\n  ```\n\n  \"\"\"\n  def set_values(f, *args, **kwargs):\n    \"\"\"Sets random variable values to its aligned value.\"\"\"\n    name = kwargs.get(\"name\")\n    if name in model_kwargs:\n      kwargs[\"value\"] = model_kwargs[name]\n    return interceptable(f)(*args, **kwargs)\n  return set_values", "language": "python", "code": "def make_value_setter(**model_kwargs):\n  \"\"\"Creates a value-setting interceptor.\n\n  This function creates an interceptor that sets values of Edward2 random\n  variable objects. This is useful for a range of tasks, including conditioning\n  on observed data, sampling from posterior predictive distributions, and as a\n  building block of inference primitives such as computing log joint\n  probabilities (see examples below).\n\n  Args:\n    **model_kwargs: dict of str to Tensor. Keys are the names of random\n      variables in the model to which this interceptor is being applied. Values\n      are Tensors to set their value to. Variables not included in this dict\n      will not be set and will maintain their existing value semantics (by\n      default, a sample from the parent-conditional distribution).\n\n  Returns:\n    set_values: function that sets the value of intercepted ops.\n\n  #### Examples\n\n  Consider for illustration a model with latent `z` and\n  observed `x`, and a corresponding trainable posterior model:\n\n  ```python\n  num_observations = 10\n  def model():\n    z = ed.Normal(loc=0, scale=1., name='z')  # log rate\n    x = ed.Poisson(rate=tf.exp(z) * tf.ones(num_observations), name='x')\n    return x\n\n  def variational_model():\n    return ed.Normal(loc=tf.Variable(0.),\n                     scale=tf.nn.softplus(tf.Variable(-4.)),\n                     name='z')  # for simplicity, match name of the model RV.\n  ```\n\n  We can use a value-setting interceptor to condition the model on observed\n  data. This approach is slightly more cumbersome than that of partially\n  evaluating the complete log-joint function, but has the potential advantage\n  that it returns a new model callable, which may be used to sample downstream\n  variables, passed into additional transformations, etc.\n\n  ```python\n  x_observed = np.array([6, 3, 1, 8, 7, 0, 6, 4, 7, 5])\n  def observed_model():\n    with ed.interception(make_value_setter(x=x_observed)):\n      model()\n  observed_log_joint_fn = ed.make_log_joint_fn(observed_model)\n\n  # After fixing 'x', the observed log joint is now only a function of 'z'.\n  # This enables us to define a variational lower bound,\n  # `E_q[ log p(x, z) - log q(z)]`, simply by evaluating the observed and\n  # variational log joints at variational samples.\n  variational_log_joint_fn = ed.make_log_joint_fn(variational_model)\n  with ed.tape() as variational_sample:  # Sample trace from variational model.\n    variational_model()\n  elbo_loss = -(observed_log_joint_fn(**variational_sample) -\n                variational_log_joint_fn(**variational_sample))\n  ```\n\n  After performing inference by minimizing the variational loss, a value-setting\n  interceptor enables simulation from the posterior predictive distribution:\n\n  ```python\n  with ed.tape() as posterior_samples:  # tape is a map {rv.name : rv}\n    variational_model()\n  with ed.interception(ed.make_value_setter(**posterior_samples)):\n    x = model()\n  # x is a sample from p(X | Z = z') where z' ~ q(z) (the variational model)\n  ```\n\n  As another example, using a value setter inside of `ed.tape` enables\n  computing the log joint probability, by setting all variables to\n  posterior values and then accumulating the log probs of those values under\n  the induced parent-conditional distributions. This is one way that we could\n  have implemented `ed.make_log_joint_fn`:\n\n  ```python\n  def make_log_joint_fn_demo(model):\n    def log_joint_fn(**model_kwargs):\n      with ed.tape() as model_tape:\n        with ed.make_value_setter(**model_kwargs):\n          model()\n\n      # accumulate sum_i log p(X_i = x_i | X_{:i-1} = x_{:i-1})\n      log_prob = 0.\n      for rv in model_tape.values():\n        log_prob += tf.reduce_sum(rv.log_prob(rv.value))\n\n      return log_prob\n    return log_joint_fn\n  ```\n\n  \"\"\"\n  def set_values(f, *args, **kwargs):\n    \"\"\"Sets random variable values to its aligned value.\"\"\"\n    name = kwargs.get(\"name\")\n    if name in model_kwargs:\n      kwargs[\"value\"] = model_kwargs[name]\n    return interceptable(f)(*args, **kwargs)\n  return set_values", "code_tokens": ["def", "make_value_setter", "(", "*", "*", "model_kwargs", ")", ":", "def", "set_values", "(", "f", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "\"\"\"Sets random variable values to its aligned value.\"\"\"", "name", "=", "kwargs", ".", "get", "(", "\"name\"", ")", "if", "name", "in", "model_kwargs", ":", "kwargs", "[", "\"value\"", "]", "=", "model_kwargs", "[", "name", "]", "return", "interceptable", "(", "f", ")", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "set_values"], "docstring": "Creates a value-setting interceptor.\n\n  This function creates an interceptor that sets values of Edward2 random\n  variable objects. This is useful for a range of tasks, including conditioning\n  on observed data, sampling from posterior predictive distributions, and as a\n  building block of inference primitives such as computing log joint\n  probabilities (see examples below).\n\n  Args:\n    **model_kwargs: dict of str to Tensor. Keys are the names of random\n      variables in the model to which this interceptor is being applied. Values\n      are Tensors to set their value to. Variables not included in this dict\n      will not be set and will maintain their existing value semantics (by\n      default, a sample from the parent-conditional distribution).\n\n  Returns:\n    set_values: function that sets the value of intercepted ops.\n\n  #### Examples\n\n  Consider for illustration a model with latent `z` and\n  observed `x`, and a corresponding trainable posterior model:\n\n  ```python\n  num_observations = 10\n  def model():\n    z = ed.Normal(loc=0, scale=1., name='z')  # log rate\n    x = ed.Poisson(rate=tf.exp(z) * tf.ones(num_observations), name='x')\n    return x\n\n  def variational_model():\n    return ed.Normal(loc=tf.Variable(0.),\n                     scale=tf.nn.softplus(tf.Variable(-4.)),\n                     name='z')  # for simplicity, match name of the model RV.\n  ```\n\n  We can use a value-setting interceptor to condition the model on observed\n  data. This approach is slightly more cumbersome than that of partially\n  evaluating the complete log-joint function, but has the potential advantage\n  that it returns a new model callable, which may be used to sample downstream\n  variables, passed into additional transformations, etc.\n\n  ```python\n  x_observed = np.array([6, 3, 1, 8, 7, 0, 6, 4, 7, 5])\n  def observed_model():\n    with ed.interception(make_value_setter(x=x_observed)):\n      model()\n  observed_log_joint_fn = ed.make_log_joint_fn(observed_model)\n\n  # After fixing 'x', the observed log joint is now only a function of 'z'.\n  # This enables us to define a variational lower bound,\n  # `E_q[ log p(x, z) - log q(z)]`, simply by evaluating the observed and\n  # variational log joints at variational samples.\n  variational_log_joint_fn = ed.make_log_joint_fn(variational_model)\n  with ed.tape() as variational_sample:  # Sample trace from variational model.\n    variational_model()\n  elbo_loss = -(observed_log_joint_fn(**variational_sample) -\n                variational_log_joint_fn(**variational_sample))\n  ```\n\n  After performing inference by minimizing the variational loss, a value-setting\n  interceptor enables simulation from the posterior predictive distribution:\n\n  ```python\n  with ed.tape() as posterior_samples:  # tape is a map {rv.name : rv}\n    variational_model()\n  with ed.interception(ed.make_value_setter(**posterior_samples)):\n    x = model()\n  # x is a sample from p(X | Z = z') where z' ~ q(z) (the variational model)\n  ```\n\n  As another example, using a value setter inside of `ed.tape` enables\n  computing the log joint probability, by setting all variables to\n  posterior values and then accumulating the log probs of those values under\n  the induced parent-conditional distributions. This is one way that we could\n  have implemented `ed.make_log_joint_fn`:\n\n  ```python\n  def make_log_joint_fn_demo(model):\n    def log_joint_fn(**model_kwargs):\n      with ed.tape() as model_tape:\n        with ed.make_value_setter(**model_kwargs):\n          model()\n\n      # accumulate sum_i log p(X_i = x_i | X_{:i-1} = x_{:i-1})\n      log_prob = 0.\n      for rv in model_tape.values():\n        log_prob += tf.reduce_sum(rv.log_prob(rv.value))\n\n      return log_prob\n    return log_joint_fn\n  ```", "docstring_tokens": ["Creates", "a", "value", "-", "setting", "interceptor", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/program_transformations.py#L34-L135", "partition": "test", "index": 797, "time": "2019-01-18 14:42:48"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py", "func_name": "bisect", "original_string": "def bisect(value_and_gradients_function,\n           initial_left,\n           initial_right,\n           f_lim):\n  \"\"\"Bisects an interval and updates to satisfy opposite slope conditions.\n\n  Corresponds to the step U3 in [Hager and Zhang (2006)][2].\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple containing the value filed `f` of the\n      function and its derivative value field `df` at that point.\n      Alternatively, the function may representthe batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and return a tuple of two tensors of shape [n], the\n      function values and the corresponding derivatives at the input points.\n    initial_left: Return value of value_and_gradients_function at the left end\n      point of the current bracketing interval.\n    initial_right: Return value of value_and_gradients_function at the right end\n      point of the current bracketing interval.\n    f_lim: real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked for each batch member.\n\n  Returns:\n    A namedtuple containing the following fields:\n      iteration: An int32 scalar `Tensor`. The number of iterations performed.\n        Bounded above by `max_iterations` parameter.\n      stopped: A boolean scalar `Tensor`. True if the bisect algorithm\n        terminated.\n      failed: A scalar boolean tensor. Indicates whether the objective function\n        failed to produce a finite value.\n      num_evals: A scalar int32 tensor. The number of value and gradients\n        function evaluations.\n      left: Return value of value_and_gradients_function at the left end\n        point of the bracketing interval found.\n      right: Return value of value_and_gradients_function at the right end\n        point of the bracketing interval found.\n  \"\"\"\n  failed = ~is_finite(initial_left, initial_right)\n  needs_bisect = (initial_right.df < 0) & (initial_right.f > f_lim)\n  bisect_args = _IntermediateResult(\n      iteration=tf.convert_to_tensor(value=0),\n      stopped=failed | ~needs_bisect,\n      failed=failed,\n      num_evals=tf.convert_to_tensor(value=0),\n      left=initial_left,\n      right=initial_right)\n  return _bisect(value_and_gradients_function, bisect_args, f_lim)", "language": "python", "code": "def bisect(value_and_gradients_function,\n           initial_left,\n           initial_right,\n           f_lim):\n  \"\"\"Bisects an interval and updates to satisfy opposite slope conditions.\n\n  Corresponds to the step U3 in [Hager and Zhang (2006)][2].\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple containing the value filed `f` of the\n      function and its derivative value field `df` at that point.\n      Alternatively, the function may representthe batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and return a tuple of two tensors of shape [n], the\n      function values and the corresponding derivatives at the input points.\n    initial_left: Return value of value_and_gradients_function at the left end\n      point of the current bracketing interval.\n    initial_right: Return value of value_and_gradients_function at the right end\n      point of the current bracketing interval.\n    f_lim: real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked for each batch member.\n\n  Returns:\n    A namedtuple containing the following fields:\n      iteration: An int32 scalar `Tensor`. The number of iterations performed.\n        Bounded above by `max_iterations` parameter.\n      stopped: A boolean scalar `Tensor`. True if the bisect algorithm\n        terminated.\n      failed: A scalar boolean tensor. Indicates whether the objective function\n        failed to produce a finite value.\n      num_evals: A scalar int32 tensor. The number of value and gradients\n        function evaluations.\n      left: Return value of value_and_gradients_function at the left end\n        point of the bracketing interval found.\n      right: Return value of value_and_gradients_function at the right end\n        point of the bracketing interval found.\n  \"\"\"\n  failed = ~is_finite(initial_left, initial_right)\n  needs_bisect = (initial_right.df < 0) & (initial_right.f > f_lim)\n  bisect_args = _IntermediateResult(\n      iteration=tf.convert_to_tensor(value=0),\n      stopped=failed | ~needs_bisect,\n      failed=failed,\n      num_evals=tf.convert_to_tensor(value=0),\n      left=initial_left,\n      right=initial_right)\n  return _bisect(value_and_gradients_function, bisect_args, f_lim)", "code_tokens": ["def", "bisect", "(", "value_and_gradients_function", ",", "initial_left", ",", "initial_right", ",", "f_lim", ")", ":", "failed", "=", "~", "is_finite", "(", "initial_left", ",", "initial_right", ")", "needs_bisect", "=", "(", "initial_right", ".", "df", "<", "0", ")", "&", "(", "initial_right", ".", "f", ">", "f_lim", ")", "bisect_args", "=", "_IntermediateResult", "(", "iteration", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "0", ")", ",", "stopped", "=", "failed", "|", "~", "needs_bisect", ",", "failed", "=", "failed", ",", "num_evals", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "0", ")", ",", "left", "=", "initial_left", ",", "right", "=", "initial_right", ")", "return", "_bisect", "(", "value_and_gradients_function", ",", "bisect_args", ",", "f_lim", ")"], "docstring": "Bisects an interval and updates to satisfy opposite slope conditions.\n\n  Corresponds to the step U3 in [Hager and Zhang (2006)][2].\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple containing the value filed `f` of the\n      function and its derivative value field `df` at that point.\n      Alternatively, the function may representthe batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and return a tuple of two tensors of shape [n], the\n      function values and the corresponding derivatives at the input points.\n    initial_left: Return value of value_and_gradients_function at the left end\n      point of the current bracketing interval.\n    initial_right: Return value of value_and_gradients_function at the right end\n      point of the current bracketing interval.\n    f_lim: real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked for each batch member.\n\n  Returns:\n    A namedtuple containing the following fields:\n      iteration: An int32 scalar `Tensor`. The number of iterations performed.\n        Bounded above by `max_iterations` parameter.\n      stopped: A boolean scalar `Tensor`. True if the bisect algorithm\n        terminated.\n      failed: A scalar boolean tensor. Indicates whether the objective function\n        failed to produce a finite value.\n      num_evals: A scalar int32 tensor. The number of value and gradients\n        function evaluations.\n      left: Return value of value_and_gradients_function at the left end\n        point of the bracketing interval found.\n      right: Return value of value_and_gradients_function at the right end\n        point of the bracketing interval found.", "docstring_tokens": ["Bisects", "an", "interval", "and", "updates", "to", "satisfy", "opposite", "slope", "conditions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py#L548-L596", "partition": "test", "index": 1000, "time": "2019-01-22 10:43:59"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py", "func_name": "_bisect", "original_string": "def _bisect(value_and_gradients_function, initial_args, f_lim):\n  \"\"\"Actual implementation of bisect given initial_args in a _BracketResult.\"\"\"\n  def _loop_cond(curr):\n    # TODO(b/112524024): Also take into account max_iterations.\n    return ~tf.reduce_all(input_tensor=curr.stopped)\n\n  def _loop_body(curr):\n    \"\"\"Narrow down interval to satisfy opposite slope conditions.\"\"\"\n    mid = value_and_gradients_function((curr.left.x + curr.right.x) / 2)\n\n    # Fail if function values at mid point are no longer finite; or left/right\n    # points are so close to it that we can't distinguish them any more.\n    failed = (curr.failed | ~is_finite(mid) |\n              tf.equal(mid.x, curr.left.x) | tf.equal(mid.x, curr.right.x))\n\n    # If mid point has a negative slope and the function value at that point is\n    # small enough, we can use it as a new left end point to narrow down the\n    # interval. If mid point has a positive slope, then we have found a suitable\n    # right end point to bracket a minima within opposite slopes. Otherwise, the\n    # mid point has a negative slope but the function value at that point is too\n    # high to work as left end point, we are in the same situation in which we\n    # started the loop so we just update the right end point and continue.\n    to_update = ~(curr.stopped | failed)\n    update_left = (mid.df < 0) & (mid.f <= f_lim)\n    left = val_where(to_update & update_left, mid, curr.left)\n    right = val_where(to_update & ~update_left, mid, curr.right)\n\n    # We're done when the right end point has a positive slope.\n    stopped = curr.stopped | failed | (right.df >= 0)\n\n    return [_IntermediateResult(\n        iteration=curr.iteration,\n        stopped=stopped,\n        failed=failed,\n        num_evals=curr.num_evals + 1,\n        left=left,\n        right=right)]\n\n  # The interval needs updating if the right end point has a negative slope and\n  # the value of the function at that point is too high. It is not a valid left\n  # end point but along with the current left end point, it encloses another\n  # minima. The loop above tries to narrow the interval so that it satisfies the\n  # opposite slope conditions.\n  return tf.while_loop(\n      cond=_loop_cond, body=_loop_body, loop_vars=[initial_args])[0]", "language": "python", "code": "def _bisect(value_and_gradients_function, initial_args, f_lim):\n  \"\"\"Actual implementation of bisect given initial_args in a _BracketResult.\"\"\"\n  def _loop_cond(curr):\n    # TODO(b/112524024): Also take into account max_iterations.\n    return ~tf.reduce_all(input_tensor=curr.stopped)\n\n  def _loop_body(curr):\n    \"\"\"Narrow down interval to satisfy opposite slope conditions.\"\"\"\n    mid = value_and_gradients_function((curr.left.x + curr.right.x) / 2)\n\n    # Fail if function values at mid point are no longer finite; or left/right\n    # points are so close to it that we can't distinguish them any more.\n    failed = (curr.failed | ~is_finite(mid) |\n              tf.equal(mid.x, curr.left.x) | tf.equal(mid.x, curr.right.x))\n\n    # If mid point has a negative slope and the function value at that point is\n    # small enough, we can use it as a new left end point to narrow down the\n    # interval. If mid point has a positive slope, then we have found a suitable\n    # right end point to bracket a minima within opposite slopes. Otherwise, the\n    # mid point has a negative slope but the function value at that point is too\n    # high to work as left end point, we are in the same situation in which we\n    # started the loop so we just update the right end point and continue.\n    to_update = ~(curr.stopped | failed)\n    update_left = (mid.df < 0) & (mid.f <= f_lim)\n    left = val_where(to_update & update_left, mid, curr.left)\n    right = val_where(to_update & ~update_left, mid, curr.right)\n\n    # We're done when the right end point has a positive slope.\n    stopped = curr.stopped | failed | (right.df >= 0)\n\n    return [_IntermediateResult(\n        iteration=curr.iteration,\n        stopped=stopped,\n        failed=failed,\n        num_evals=curr.num_evals + 1,\n        left=left,\n        right=right)]\n\n  # The interval needs updating if the right end point has a negative slope and\n  # the value of the function at that point is too high. It is not a valid left\n  # end point but along with the current left end point, it encloses another\n  # minima. The loop above tries to narrow the interval so that it satisfies the\n  # opposite slope conditions.\n  return tf.while_loop(\n      cond=_loop_cond, body=_loop_body, loop_vars=[initial_args])[0]", "code_tokens": ["def", "_bisect", "(", "value_and_gradients_function", ",", "initial_args", ",", "f_lim", ")", ":", "def", "_loop_cond", "(", "curr", ")", ":", "# TODO(b/112524024): Also take into account max_iterations.", "return", "~", "tf", ".", "reduce_all", "(", "input_tensor", "=", "curr", ".", "stopped", ")", "def", "_loop_body", "(", "curr", ")", ":", "\"\"\"Narrow down interval to satisfy opposite slope conditions.\"\"\"", "mid", "=", "value_and_gradients_function", "(", "(", "curr", ".", "left", ".", "x", "+", "curr", ".", "right", ".", "x", ")", "/", "2", ")", "# Fail if function values at mid point are no longer finite; or left/right", "# points are so close to it that we can't distinguish them any more.", "failed", "=", "(", "curr", ".", "failed", "|", "~", "is_finite", "(", "mid", ")", "|", "tf", ".", "equal", "(", "mid", ".", "x", ",", "curr", ".", "left", ".", "x", ")", "|", "tf", ".", "equal", "(", "mid", ".", "x", ",", "curr", ".", "right", ".", "x", ")", ")", "# If mid point has a negative slope and the function value at that point is", "# small enough, we can use it as a new left end point to narrow down the", "# interval. If mid point has a positive slope, then we have found a suitable", "# right end point to bracket a minima within opposite slopes. Otherwise, the", "# mid point has a negative slope but the function value at that point is too", "# high to work as left end point, we are in the same situation in which we", "# started the loop so we just update the right end point and continue.", "to_update", "=", "~", "(", "curr", ".", "stopped", "|", "failed", ")", "update_left", "=", "(", "mid", ".", "df", "<", "0", ")", "&", "(", "mid", ".", "f", "<=", "f_lim", ")", "left", "=", "val_where", "(", "to_update", "&", "update_left", ",", "mid", ",", "curr", ".", "left", ")", "right", "=", "val_where", "(", "to_update", "&", "~", "update_left", ",", "mid", ",", "curr", ".", "right", ")", "# We're done when the right end point has a positive slope.", "stopped", "=", "curr", ".", "stopped", "|", "failed", "|", "(", "right", ".", "df", ">=", "0", ")", "return", "[", "_IntermediateResult", "(", "iteration", "=", "curr", ".", "iteration", ",", "stopped", "=", "stopped", ",", "failed", "=", "failed", ",", "num_evals", "=", "curr", ".", "num_evals", "+", "1", ",", "left", "=", "left", ",", "right", "=", "right", ")", "]", "# The interval needs updating if the right end point has a negative slope and", "# the value of the function at that point is too high. It is not a valid left", "# end point but along with the current left end point, it encloses another", "# minima. The loop above tries to narrow the interval so that it satisfies the", "# opposite slope conditions.", "return", "tf", ".", "while_loop", "(", "cond", "=", "_loop_cond", ",", "body", "=", "_loop_body", ",", "loop_vars", "=", "[", "initial_args", "]", ")", "[", "0", "]"], "docstring": "Actual implementation of bisect given initial_args in a _BracketResult.", "docstring_tokens": ["Actual", "implementation", "of", "bisect", "given", "initial_args", "in", "a", "_BracketResult", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py#L599-L643", "partition": "test", "index": 1001, "time": "2019-01-22 10:43:59"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py", "func_name": "is_finite", "original_string": "def is_finite(val_1, val_2=None):\n  \"\"\"Checks if the supplied values are finite.\n\n  Args:\n    val_1: A namedtuple instance with the function value and derivative,\n      as returned e.g. by value_and_gradients_function evaluations.\n    val_2: (Optional) A namedtuple instance with the function value and\n      derivative, as returned e.g. by value_and_gradients_function evaluations.\n\n  Returns:\n    is_finite: Scalar boolean `Tensor` indicating whether the function value\n      and the derivative in `val_1` (and optionally in `val_2`) are all finite.\n  \"\"\"\n  val_1_finite = tf.math.is_finite(val_1.f) & tf.math.is_finite(val_1.df)\n  if val_2 is not None:\n    return val_1_finite & tf.math.is_finite(val_2.f) & tf.math.is_finite(\n        val_2.df)\n  return val_1_finite", "language": "python", "code": "def is_finite(val_1, val_2=None):\n  \"\"\"Checks if the supplied values are finite.\n\n  Args:\n    val_1: A namedtuple instance with the function value and derivative,\n      as returned e.g. by value_and_gradients_function evaluations.\n    val_2: (Optional) A namedtuple instance with the function value and\n      derivative, as returned e.g. by value_and_gradients_function evaluations.\n\n  Returns:\n    is_finite: Scalar boolean `Tensor` indicating whether the function value\n      and the derivative in `val_1` (and optionally in `val_2`) are all finite.\n  \"\"\"\n  val_1_finite = tf.math.is_finite(val_1.f) & tf.math.is_finite(val_1.df)\n  if val_2 is not None:\n    return val_1_finite & tf.math.is_finite(val_2.f) & tf.math.is_finite(\n        val_2.df)\n  return val_1_finite", "code_tokens": ["def", "is_finite", "(", "val_1", ",", "val_2", "=", "None", ")", ":", "val_1_finite", "=", "tf", ".", "math", ".", "is_finite", "(", "val_1", ".", "f", ")", "&", "tf", ".", "math", ".", "is_finite", "(", "val_1", ".", "df", ")", "if", "val_2", "is", "not", "None", ":", "return", "val_1_finite", "&", "tf", ".", "math", ".", "is_finite", "(", "val_2", ".", "f", ")", "&", "tf", ".", "math", ".", "is_finite", "(", "val_2", ".", "df", ")", "return", "val_1_finite"], "docstring": "Checks if the supplied values are finite.\n\n  Args:\n    val_1: A namedtuple instance with the function value and derivative,\n      as returned e.g. by value_and_gradients_function evaluations.\n    val_2: (Optional) A namedtuple instance with the function value and\n      derivative, as returned e.g. by value_and_gradients_function evaluations.\n\n  Returns:\n    is_finite: Scalar boolean `Tensor` indicating whether the function value\n      and the derivative in `val_1` (and optionally in `val_2`) are all finite.", "docstring_tokens": ["Checks", "if", "the", "supplied", "values", "are", "finite", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py#L646-L663", "partition": "test", "index": 1002, "time": "2019-01-22 10:43:59"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/quantiles.py", "func_name": "find_bins", "original_string": "def find_bins(x,\n              edges,\n              extend_lower_interval=False,\n              extend_upper_interval=False,\n              dtype=None,\n              name=None):\n  \"\"\"Bin values into discrete intervals.\n\n  Given `edges = [c0, ..., cK]`, defining intervals\n  `I0 = [c0, c1)`, `I1 = [c1, c2)`, ..., `I_{K-1} = [c_{K-1}, cK]`,\n  This function returns `bins`, such that:\n  `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.\n    edges:  `Tensor` of same `dtype` as `x`.  The first dimension indexes edges\n      of intervals.  Must either be `1-D` or have\n      `x.shape[1:] == edges.shape[1:]`.  If `rank(edges) > 1`, `edges[k]`\n      designates a shape `edges.shape[1:]` `Tensor` of bin edges for the\n      corresponding dimensions of `x`.\n    extend_lower_interval:  Python `bool`.  If `True`, extend the lowest\n      interval `I0` to `(-inf, c1]`.\n    extend_upper_interval:  Python `bool`.  If `True`, extend the upper\n      interval `I_{K-1}` to `[c_{K-1}, +inf)`.\n    dtype: The output type (`int32` or `int64`). `Default value:` `x.dtype`.\n      This effects the output values when `x` is below/above the intervals,\n      which will be `-1/K+1` for `int` types and `NaN` for `float`s.\n      At indices where `x` is `NaN`, the output values will be `0` for `int`\n      types and `NaN` for floats.\n    name:  A Python string name to prepend to created ops. Default: 'find_bins'\n\n  Returns:\n    bins: `Tensor` with same `shape` as `x` and `dtype`.\n      Has whole number values.  `bins[i] = k` means the `x[i]` falls into the\n      `kth` bin, ie, `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Raises:\n    ValueError:  If `edges.shape[0]` is determined to be less than 2.\n\n  #### Examples\n\n  Cut a `1-D` array\n\n  ```python\n  x = [0., 5., 6., 10., 20.]\n  edges = [0., 5., 10.]\n  tfp.stats.find_bins(x, edges)\n  ==> [0., 0., 1., 1., np.nan]\n  ```\n\n  Cut `x` into its deciles\n\n  ```python\n  x = tf.random_uniform(shape=(100, 200))\n  decile_edges = tfp.stats.quantiles(x, num_quantiles=10)\n  bins = tfp.stats.find_bins(x, edges=decile_edges)\n  bins.shape\n  ==> (100, 200)\n  tf.reduce_mean(bins == 0.)\n  ==> approximately 0.1\n  tf.reduce_mean(bins == 1.)\n  ==> approximately 0.1\n  ```\n\n  \"\"\"\n  # TFP users may be surprised to see the \"action\" in the leftmost dim of\n  # edges, rather than the rightmost (event) dim.  Why?\n  # 1. Most likely you created edges by getting quantiles over samples, and\n  #    quantile/percentile return these edges in the leftmost (sample) dim.\n  # 2. Say you have event_shape = [5], then we expect the bin will be different\n  #    for all 5 events, so the index of the bin should not be in the event dim.\n  with tf.compat.v1.name_scope(\n      name, default_name='find_bins', values=[x, edges]):\n    in_type = dtype_util.common_dtype([x, edges],\n                                      preferred_dtype=tf.float32)\n    edges = tf.convert_to_tensor(value=edges, name='edges', dtype=in_type)\n    x = tf.convert_to_tensor(value=x, name='x', dtype=in_type)\n\n    if (tf.compat.dimension_value(edges.shape[0]) is not None and\n        tf.compat.dimension_value(edges.shape[0]) < 2):\n      raise ValueError(\n          'First dimension of `edges` must have length > 1 to index 1 or '\n          'more bin. Found: {}'.format(edges.shape))\n\n    flattening_x = edges.shape.ndims == 1 and x.shape.ndims > 1\n\n    if flattening_x:\n      x_orig_shape = tf.shape(input=x)\n      x = tf.reshape(x, [-1])\n\n    if dtype is None:\n      dtype = in_type\n    dtype = tf.as_dtype(dtype)\n\n    # Move first dims into the rightmost.\n    x_permed = distribution_util.rotate_transpose(x, shift=-1)\n    edges_permed = distribution_util.rotate_transpose(edges, shift=-1)\n\n    # If...\n    #   x_permed = [0, 1, 6., 10]\n    #   edges = [0, 5, 10.]\n    #   ==> almost_output = [0, 1, 2, 2]\n    searchsorted_type = dtype if dtype in [tf.int32, tf.int64] else None\n    almost_output_permed = tf.searchsorted(\n        sorted_sequence=edges_permed,\n        values=x_permed,\n        side='right',\n        out_type=searchsorted_type)\n    # Move the rightmost dims back to the leftmost.\n    almost_output = tf.cast(\n        distribution_util.rotate_transpose(almost_output_permed, shift=1),\n        dtype)\n\n    # In above example, we want [0, 0, 1, 1], so correct this here.\n    bins = tf.clip_by_value(almost_output - 1, tf.cast(0, dtype),\n                            tf.cast(tf.shape(input=edges)[0] - 2, dtype))\n\n    if not extend_lower_interval:\n      low_fill = np.nan if dtype.is_floating else -1\n      bins = tf.where(x < tf.expand_dims(edges[0], 0),\n                      tf.fill(tf.shape(input=x), tf.cast(low_fill, dtype)),\n                      bins)\n\n    if not extend_upper_interval:\n      up_fill = np.nan if dtype.is_floating else tf.shape(input=edges)[0] - 1\n      bins = tf.where(x > tf.expand_dims(edges[-1], 0),\n                      tf.fill(tf.shape(input=x), tf.cast(up_fill, dtype)), bins)\n\n    if flattening_x:\n      bins = tf.reshape(bins, x_orig_shape)\n\n    return bins", "language": "python", "code": "def find_bins(x,\n              edges,\n              extend_lower_interval=False,\n              extend_upper_interval=False,\n              dtype=None,\n              name=None):\n  \"\"\"Bin values into discrete intervals.\n\n  Given `edges = [c0, ..., cK]`, defining intervals\n  `I0 = [c0, c1)`, `I1 = [c1, c2)`, ..., `I_{K-1} = [c_{K-1}, cK]`,\n  This function returns `bins`, such that:\n  `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.\n    edges:  `Tensor` of same `dtype` as `x`.  The first dimension indexes edges\n      of intervals.  Must either be `1-D` or have\n      `x.shape[1:] == edges.shape[1:]`.  If `rank(edges) > 1`, `edges[k]`\n      designates a shape `edges.shape[1:]` `Tensor` of bin edges for the\n      corresponding dimensions of `x`.\n    extend_lower_interval:  Python `bool`.  If `True`, extend the lowest\n      interval `I0` to `(-inf, c1]`.\n    extend_upper_interval:  Python `bool`.  If `True`, extend the upper\n      interval `I_{K-1}` to `[c_{K-1}, +inf)`.\n    dtype: The output type (`int32` or `int64`). `Default value:` `x.dtype`.\n      This effects the output values when `x` is below/above the intervals,\n      which will be `-1/K+1` for `int` types and `NaN` for `float`s.\n      At indices where `x` is `NaN`, the output values will be `0` for `int`\n      types and `NaN` for floats.\n    name:  A Python string name to prepend to created ops. Default: 'find_bins'\n\n  Returns:\n    bins: `Tensor` with same `shape` as `x` and `dtype`.\n      Has whole number values.  `bins[i] = k` means the `x[i]` falls into the\n      `kth` bin, ie, `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Raises:\n    ValueError:  If `edges.shape[0]` is determined to be less than 2.\n\n  #### Examples\n\n  Cut a `1-D` array\n\n  ```python\n  x = [0., 5., 6., 10., 20.]\n  edges = [0., 5., 10.]\n  tfp.stats.find_bins(x, edges)\n  ==> [0., 0., 1., 1., np.nan]\n  ```\n\n  Cut `x` into its deciles\n\n  ```python\n  x = tf.random_uniform(shape=(100, 200))\n  decile_edges = tfp.stats.quantiles(x, num_quantiles=10)\n  bins = tfp.stats.find_bins(x, edges=decile_edges)\n  bins.shape\n  ==> (100, 200)\n  tf.reduce_mean(bins == 0.)\n  ==> approximately 0.1\n  tf.reduce_mean(bins == 1.)\n  ==> approximately 0.1\n  ```\n\n  \"\"\"\n  # TFP users may be surprised to see the \"action\" in the leftmost dim of\n  # edges, rather than the rightmost (event) dim.  Why?\n  # 1. Most likely you created edges by getting quantiles over samples, and\n  #    quantile/percentile return these edges in the leftmost (sample) dim.\n  # 2. Say you have event_shape = [5], then we expect the bin will be different\n  #    for all 5 events, so the index of the bin should not be in the event dim.\n  with tf.compat.v1.name_scope(\n      name, default_name='find_bins', values=[x, edges]):\n    in_type = dtype_util.common_dtype([x, edges],\n                                      preferred_dtype=tf.float32)\n    edges = tf.convert_to_tensor(value=edges, name='edges', dtype=in_type)\n    x = tf.convert_to_tensor(value=x, name='x', dtype=in_type)\n\n    if (tf.compat.dimension_value(edges.shape[0]) is not None and\n        tf.compat.dimension_value(edges.shape[0]) < 2):\n      raise ValueError(\n          'First dimension of `edges` must have length > 1 to index 1 or '\n          'more bin. Found: {}'.format(edges.shape))\n\n    flattening_x = edges.shape.ndims == 1 and x.shape.ndims > 1\n\n    if flattening_x:\n      x_orig_shape = tf.shape(input=x)\n      x = tf.reshape(x, [-1])\n\n    if dtype is None:\n      dtype = in_type\n    dtype = tf.as_dtype(dtype)\n\n    # Move first dims into the rightmost.\n    x_permed = distribution_util.rotate_transpose(x, shift=-1)\n    edges_permed = distribution_util.rotate_transpose(edges, shift=-1)\n\n    # If...\n    #   x_permed = [0, 1, 6., 10]\n    #   edges = [0, 5, 10.]\n    #   ==> almost_output = [0, 1, 2, 2]\n    searchsorted_type = dtype if dtype in [tf.int32, tf.int64] else None\n    almost_output_permed = tf.searchsorted(\n        sorted_sequence=edges_permed,\n        values=x_permed,\n        side='right',\n        out_type=searchsorted_type)\n    # Move the rightmost dims back to the leftmost.\n    almost_output = tf.cast(\n        distribution_util.rotate_transpose(almost_output_permed, shift=1),\n        dtype)\n\n    # In above example, we want [0, 0, 1, 1], so correct this here.\n    bins = tf.clip_by_value(almost_output - 1, tf.cast(0, dtype),\n                            tf.cast(tf.shape(input=edges)[0] - 2, dtype))\n\n    if not extend_lower_interval:\n      low_fill = np.nan if dtype.is_floating else -1\n      bins = tf.where(x < tf.expand_dims(edges[0], 0),\n                      tf.fill(tf.shape(input=x), tf.cast(low_fill, dtype)),\n                      bins)\n\n    if not extend_upper_interval:\n      up_fill = np.nan if dtype.is_floating else tf.shape(input=edges)[0] - 1\n      bins = tf.where(x > tf.expand_dims(edges[-1], 0),\n                      tf.fill(tf.shape(input=x), tf.cast(up_fill, dtype)), bins)\n\n    if flattening_x:\n      bins = tf.reshape(bins, x_orig_shape)\n\n    return bins", "code_tokens": ["def", "find_bins", "(", "x", ",", "edges", ",", "extend_lower_interval", "=", "False", ",", "extend_upper_interval", "=", "False", ",", "dtype", "=", "None", ",", "name", "=", "None", ")", ":", "# TFP users may be surprised to see the \"action\" in the leftmost dim of", "# edges, rather than the rightmost (event) dim.  Why?", "# 1. Most likely you created edges by getting quantiles over samples, and", "#    quantile/percentile return these edges in the leftmost (sample) dim.", "# 2. Say you have event_shape = [5], then we expect the bin will be different", "#    for all 5 events, so the index of the bin should not be in the event dim.", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "default_name", "=", "'find_bins'", ",", "values", "=", "[", "x", ",", "edges", "]", ")", ":", "in_type", "=", "dtype_util", ".", "common_dtype", "(", "[", "x", ",", "edges", "]", ",", "preferred_dtype", "=", "tf", ".", "float32", ")", "edges", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "edges", ",", "name", "=", "'edges'", ",", "dtype", "=", "in_type", ")", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ",", "dtype", "=", "in_type", ")", "if", "(", "tf", ".", "compat", ".", "dimension_value", "(", "edges", ".", "shape", "[", "0", "]", ")", "is", "not", "None", "and", "tf", ".", "compat", ".", "dimension_value", "(", "edges", ".", "shape", "[", "0", "]", ")", "<", "2", ")", ":", "raise", "ValueError", "(", "'First dimension of `edges` must have length > 1 to index 1 or '", "'more bin. Found: {}'", ".", "format", "(", "edges", ".", "shape", ")", ")", "flattening_x", "=", "edges", ".", "shape", ".", "ndims", "==", "1", "and", "x", ".", "shape", ".", "ndims", ">", "1", "if", "flattening_x", ":", "x_orig_shape", "=", "tf", ".", "shape", "(", "input", "=", "x", ")", "x", "=", "tf", ".", "reshape", "(", "x", ",", "[", "-", "1", "]", ")", "if", "dtype", "is", "None", ":", "dtype", "=", "in_type", "dtype", "=", "tf", ".", "as_dtype", "(", "dtype", ")", "# Move first dims into the rightmost.", "x_permed", "=", "distribution_util", ".", "rotate_transpose", "(", "x", ",", "shift", "=", "-", "1", ")", "edges_permed", "=", "distribution_util", ".", "rotate_transpose", "(", "edges", ",", "shift", "=", "-", "1", ")", "# If...", "#   x_permed = [0, 1, 6., 10]", "#   edges = [0, 5, 10.]", "#   ==> almost_output = [0, 1, 2, 2]", "searchsorted_type", "=", "dtype", "if", "dtype", "in", "[", "tf", ".", "int32", ",", "tf", ".", "int64", "]", "else", "None", "almost_output_permed", "=", "tf", ".", "searchsorted", "(", "sorted_sequence", "=", "edges_permed", ",", "values", "=", "x_permed", ",", "side", "=", "'right'", ",", "out_type", "=", "searchsorted_type", ")", "# Move the rightmost dims back to the leftmost.", "almost_output", "=", "tf", ".", "cast", "(", "distribution_util", ".", "rotate_transpose", "(", "almost_output_permed", ",", "shift", "=", "1", ")", ",", "dtype", ")", "# In above example, we want [0, 0, 1, 1], so correct this here.", "bins", "=", "tf", ".", "clip_by_value", "(", "almost_output", "-", "1", ",", "tf", ".", "cast", "(", "0", ",", "dtype", ")", ",", "tf", ".", "cast", "(", "tf", ".", "shape", "(", "input", "=", "edges", ")", "[", "0", "]", "-", "2", ",", "dtype", ")", ")", "if", "not", "extend_lower_interval", ":", "low_fill", "=", "np", ".", "nan", "if", "dtype", ".", "is_floating", "else", "-", "1", "bins", "=", "tf", ".", "where", "(", "x", "<", "tf", ".", "expand_dims", "(", "edges", "[", "0", "]", ",", "0", ")", ",", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "tf", ".", "cast", "(", "low_fill", ",", "dtype", ")", ")", ",", "bins", ")", "if", "not", "extend_upper_interval", ":", "up_fill", "=", "np", ".", "nan", "if", "dtype", ".", "is_floating", "else", "tf", ".", "shape", "(", "input", "=", "edges", ")", "[", "0", "]", "-", "1", "bins", "=", "tf", ".", "where", "(", "x", ">", "tf", ".", "expand_dims", "(", "edges", "[", "-", "1", "]", ",", "0", ")", ",", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "tf", ".", "cast", "(", "up_fill", ",", "dtype", ")", ")", ",", "bins", ")", "if", "flattening_x", ":", "bins", "=", "tf", ".", "reshape", "(", "bins", ",", "x_orig_shape", ")", "return", "bins"], "docstring": "Bin values into discrete intervals.\n\n  Given `edges = [c0, ..., cK]`, defining intervals\n  `I0 = [c0, c1)`, `I1 = [c1, c2)`, ..., `I_{K-1} = [c_{K-1}, cK]`,\n  This function returns `bins`, such that:\n  `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.\n    edges:  `Tensor` of same `dtype` as `x`.  The first dimension indexes edges\n      of intervals.  Must either be `1-D` or have\n      `x.shape[1:] == edges.shape[1:]`.  If `rank(edges) > 1`, `edges[k]`\n      designates a shape `edges.shape[1:]` `Tensor` of bin edges for the\n      corresponding dimensions of `x`.\n    extend_lower_interval:  Python `bool`.  If `True`, extend the lowest\n      interval `I0` to `(-inf, c1]`.\n    extend_upper_interval:  Python `bool`.  If `True`, extend the upper\n      interval `I_{K-1}` to `[c_{K-1}, +inf)`.\n    dtype: The output type (`int32` or `int64`). `Default value:` `x.dtype`.\n      This effects the output values when `x` is below/above the intervals,\n      which will be `-1/K+1` for `int` types and `NaN` for `float`s.\n      At indices where `x` is `NaN`, the output values will be `0` for `int`\n      types and `NaN` for floats.\n    name:  A Python string name to prepend to created ops. Default: 'find_bins'\n\n  Returns:\n    bins: `Tensor` with same `shape` as `x` and `dtype`.\n      Has whole number values.  `bins[i] = k` means the `x[i]` falls into the\n      `kth` bin, ie, `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Raises:\n    ValueError:  If `edges.shape[0]` is determined to be less than 2.\n\n  #### Examples\n\n  Cut a `1-D` array\n\n  ```python\n  x = [0., 5., 6., 10., 20.]\n  edges = [0., 5., 10.]\n  tfp.stats.find_bins(x, edges)\n  ==> [0., 0., 1., 1., np.nan]\n  ```\n\n  Cut `x` into its deciles\n\n  ```python\n  x = tf.random_uniform(shape=(100, 200))\n  decile_edges = tfp.stats.quantiles(x, num_quantiles=10)\n  bins = tfp.stats.find_bins(x, edges=decile_edges)\n  bins.shape\n  ==> (100, 200)\n  tf.reduce_mean(bins == 0.)\n  ==> approximately 0.1\n  tf.reduce_mean(bins == 1.)\n  ==> approximately 0.1\n  ```", "docstring_tokens": ["Bin", "values", "into", "discrete", "intervals", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/quantiles.py#L158-L289", "partition": "test", "index": 1033, "time": "2019-01-30 10:46:58"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/numeric.py", "func_name": "soft_threshold", "original_string": "def soft_threshold(x, threshold, name=None):\n  \"\"\"Soft Thresholding operator.\n\n  This operator is defined by the equations\n\n  ```none\n                                { x[i] - gamma,  x[i] >   gamma\n  SoftThreshold(x, gamma)[i] =  { 0,             x[i] ==  gamma\n                                { x[i] + gamma,  x[i] <  -gamma\n  ```\n\n  In the context of proximal gradient methods, we have\n\n  ```none\n  SoftThreshold(x, gamma) = prox_{gamma L1}(x)\n  ```\n\n  where `prox` is the proximity operator.  Thus the soft thresholding operator\n  is used in proximal gradient descent for optimizing a smooth function with\n  (non-smooth) L1 regularization, as outlined below.\n\n  The proximity operator is defined as:\n\n  ```none\n  prox_r(x) = argmin{ r(z) + 0.5 ||x - z||_2**2 : z },\n  ```\n\n  where `r` is a (weakly) convex function, not necessarily differentiable.\n  Because the L2 norm is strictly convex, the above argmin is unique.\n\n  One important application of the proximity operator is as follows.  Let `L` be\n  a convex and differentiable function with Lipschitz-continuous gradient.  Let\n  `R` be a convex lower semicontinuous function which is possibly\n  nondifferentiable.  Let `gamma` be an arbitrary positive real.  Then\n\n  ```none\n  x_star = argmin{ L(x) + R(x) : x }\n  ```\n\n  if and only if the fixed-point equation is satisfied:\n\n  ```none\n  x_star = prox_{gamma R}(x_star - gamma grad L(x_star))\n  ```\n\n  Proximal gradient descent thus typically consists of choosing an initial value\n  `x^{(0)}` and repeatedly applying the update\n\n  ```none\n  x^{(k+1)} = prox_{gamma^{(k)} R}(x^{(k)} - gamma^{(k)} grad L(x^{(k)}))\n  ```\n\n  where `gamma` is allowed to vary from iteration to iteration.  Specializing to\n  the case where `R(x) = ||x||_1`, we minimize `L(x) + ||x||_1` by repeatedly\n  applying the update\n\n  ```\n  x^{(k+1)} = SoftThreshold(x - gamma grad L(x^{(k)}), gamma)\n  ```\n\n  (This idea can also be extended to second-order approximations, although the\n  multivariate case does not have a known closed form like above.)\n\n  Args:\n    x: `float` `Tensor` representing the input to the SoftThreshold function.\n    threshold: nonnegative scalar, `float` `Tensor` representing the radius of\n      the interval on which each coordinate of SoftThreshold takes the value\n      zero.  Denoted `gamma` above.\n    name: Python string indicating the name of the TensorFlow operation.\n      Default value: `'soft_threshold'`.\n\n  Returns:\n    softthreshold: `float` `Tensor` with the same shape and dtype as `x`,\n      representing the value of the SoftThreshold function.\n\n  #### References\n\n  [1]: Yu, Yao-Liang. The Proximity Operator.\n       https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf\n\n  [2]: Wikipedia Contributors. Proximal gradient methods for learning.\n       _Wikipedia, The Free Encyclopedia_, 2018.\n       https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning\n\n  \"\"\"\n  # https://math.stackexchange.com/questions/471339/derivation-of-soft-thresholding-operator\n  with tf.compat.v1.name_scope(name, 'soft_threshold', [x, threshold]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    threshold = tf.convert_to_tensor(\n        value=threshold, dtype=x.dtype, name='threshold')\n    return tf.sign(x) * tf.maximum(tf.abs(x) - threshold, 0.)", "language": "python", "code": "def soft_threshold(x, threshold, name=None):\n  \"\"\"Soft Thresholding operator.\n\n  This operator is defined by the equations\n\n  ```none\n                                { x[i] - gamma,  x[i] >   gamma\n  SoftThreshold(x, gamma)[i] =  { 0,             x[i] ==  gamma\n                                { x[i] + gamma,  x[i] <  -gamma\n  ```\n\n  In the context of proximal gradient methods, we have\n\n  ```none\n  SoftThreshold(x, gamma) = prox_{gamma L1}(x)\n  ```\n\n  where `prox` is the proximity operator.  Thus the soft thresholding operator\n  is used in proximal gradient descent for optimizing a smooth function with\n  (non-smooth) L1 regularization, as outlined below.\n\n  The proximity operator is defined as:\n\n  ```none\n  prox_r(x) = argmin{ r(z) + 0.5 ||x - z||_2**2 : z },\n  ```\n\n  where `r` is a (weakly) convex function, not necessarily differentiable.\n  Because the L2 norm is strictly convex, the above argmin is unique.\n\n  One important application of the proximity operator is as follows.  Let `L` be\n  a convex and differentiable function with Lipschitz-continuous gradient.  Let\n  `R` be a convex lower semicontinuous function which is possibly\n  nondifferentiable.  Let `gamma` be an arbitrary positive real.  Then\n\n  ```none\n  x_star = argmin{ L(x) + R(x) : x }\n  ```\n\n  if and only if the fixed-point equation is satisfied:\n\n  ```none\n  x_star = prox_{gamma R}(x_star - gamma grad L(x_star))\n  ```\n\n  Proximal gradient descent thus typically consists of choosing an initial value\n  `x^{(0)}` and repeatedly applying the update\n\n  ```none\n  x^{(k+1)} = prox_{gamma^{(k)} R}(x^{(k)} - gamma^{(k)} grad L(x^{(k)}))\n  ```\n\n  where `gamma` is allowed to vary from iteration to iteration.  Specializing to\n  the case where `R(x) = ||x||_1`, we minimize `L(x) + ||x||_1` by repeatedly\n  applying the update\n\n  ```\n  x^{(k+1)} = SoftThreshold(x - gamma grad L(x^{(k)}), gamma)\n  ```\n\n  (This idea can also be extended to second-order approximations, although the\n  multivariate case does not have a known closed form like above.)\n\n  Args:\n    x: `float` `Tensor` representing the input to the SoftThreshold function.\n    threshold: nonnegative scalar, `float` `Tensor` representing the radius of\n      the interval on which each coordinate of SoftThreshold takes the value\n      zero.  Denoted `gamma` above.\n    name: Python string indicating the name of the TensorFlow operation.\n      Default value: `'soft_threshold'`.\n\n  Returns:\n    softthreshold: `float` `Tensor` with the same shape and dtype as `x`,\n      representing the value of the SoftThreshold function.\n\n  #### References\n\n  [1]: Yu, Yao-Liang. The Proximity Operator.\n       https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf\n\n  [2]: Wikipedia Contributors. Proximal gradient methods for learning.\n       _Wikipedia, The Free Encyclopedia_, 2018.\n       https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning\n\n  \"\"\"\n  # https://math.stackexchange.com/questions/471339/derivation-of-soft-thresholding-operator\n  with tf.compat.v1.name_scope(name, 'soft_threshold', [x, threshold]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    threshold = tf.convert_to_tensor(\n        value=threshold, dtype=x.dtype, name='threshold')\n    return tf.sign(x) * tf.maximum(tf.abs(x) - threshold, 0.)", "code_tokens": ["def", "soft_threshold", "(", "x", ",", "threshold", ",", "name", "=", "None", ")", ":", "# https://math.stackexchange.com/questions/471339/derivation-of-soft-thresholding-operator", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'soft_threshold'", ",", "[", "x", ",", "threshold", "]", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ")", "threshold", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "threshold", ",", "dtype", "=", "x", ".", "dtype", ",", "name", "=", "'threshold'", ")", "return", "tf", ".", "sign", "(", "x", ")", "*", "tf", ".", "maximum", "(", "tf", ".", "abs", "(", "x", ")", "-", "threshold", ",", "0.", ")"], "docstring": "Soft Thresholding operator.\n\n  This operator is defined by the equations\n\n  ```none\n                                { x[i] - gamma,  x[i] >   gamma\n  SoftThreshold(x, gamma)[i] =  { 0,             x[i] ==  gamma\n                                { x[i] + gamma,  x[i] <  -gamma\n  ```\n\n  In the context of proximal gradient methods, we have\n\n  ```none\n  SoftThreshold(x, gamma) = prox_{gamma L1}(x)\n  ```\n\n  where `prox` is the proximity operator.  Thus the soft thresholding operator\n  is used in proximal gradient descent for optimizing a smooth function with\n  (non-smooth) L1 regularization, as outlined below.\n\n  The proximity operator is defined as:\n\n  ```none\n  prox_r(x) = argmin{ r(z) + 0.5 ||x - z||_2**2 : z },\n  ```\n\n  where `r` is a (weakly) convex function, not necessarily differentiable.\n  Because the L2 norm is strictly convex, the above argmin is unique.\n\n  One important application of the proximity operator is as follows.  Let `L` be\n  a convex and differentiable function with Lipschitz-continuous gradient.  Let\n  `R` be a convex lower semicontinuous function which is possibly\n  nondifferentiable.  Let `gamma` be an arbitrary positive real.  Then\n\n  ```none\n  x_star = argmin{ L(x) + R(x) : x }\n  ```\n\n  if and only if the fixed-point equation is satisfied:\n\n  ```none\n  x_star = prox_{gamma R}(x_star - gamma grad L(x_star))\n  ```\n\n  Proximal gradient descent thus typically consists of choosing an initial value\n  `x^{(0)}` and repeatedly applying the update\n\n  ```none\n  x^{(k+1)} = prox_{gamma^{(k)} R}(x^{(k)} - gamma^{(k)} grad L(x^{(k)}))\n  ```\n\n  where `gamma` is allowed to vary from iteration to iteration.  Specializing to\n  the case where `R(x) = ||x||_1`, we minimize `L(x) + ||x||_1` by repeatedly\n  applying the update\n\n  ```\n  x^{(k+1)} = SoftThreshold(x - gamma grad L(x^{(k)}), gamma)\n  ```\n\n  (This idea can also be extended to second-order approximations, although the\n  multivariate case does not have a known closed form like above.)\n\n  Args:\n    x: `float` `Tensor` representing the input to the SoftThreshold function.\n    threshold: nonnegative scalar, `float` `Tensor` representing the radius of\n      the interval on which each coordinate of SoftThreshold takes the value\n      zero.  Denoted `gamma` above.\n    name: Python string indicating the name of the TensorFlow operation.\n      Default value: `'soft_threshold'`.\n\n  Returns:\n    softthreshold: `float` `Tensor` with the same shape and dtype as `x`,\n      representing the value of the SoftThreshold function.\n\n  #### References\n\n  [1]: Yu, Yao-Liang. The Proximity Operator.\n       https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf\n\n  [2]: Wikipedia Contributors. Proximal gradient methods for learning.\n       _Wikipedia, The Free Encyclopedia_, 2018.\n       https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning", "docstring_tokens": ["Soft", "Thresholding", "operator", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/numeric.py#L59-L149", "partition": "test", "index": 1096, "time": "2019-01-30 10:47:34"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/proximal_hessian_sparse.py", "func_name": "minimize", "original_string": "def minimize(grad_and_hessian_loss_fn,\n             x_start,\n             tolerance,\n             l1_regularizer,\n             l2_regularizer=None,\n             maximum_iterations=1,\n             maximum_full_sweeps_per_iteration=1,\n             learning_rate=None,\n             name=None):\n  \"\"\"Minimize using Hessian-informed proximal gradient descent.\n\n  This function solves the regularized minimization problem\n\n  ```none\n  argmin{ Loss(x)\n            + l1_regularizer * ||x||_1\n            + l2_regularizer * ||x||_2**2\n          : x in R^n }\n  ```\n\n  where `Loss` is a convex C^2 function (typically, `Loss` is the negative log\n  likelihood of a model and `x` is a vector of model coefficients).  The `Loss`\n  function does not need to be supplied directly, but this optimizer does need a\n  way to compute the gradient and Hessian of the Loss function at a given value\n  of `x`.  The gradient and Hessian are often computationally expensive, and\n  this optimizer calls them relatively few times compared with other algorithms.\n\n  Args:\n    grad_and_hessian_loss_fn: callable that takes as input a (batch of) `Tensor`\n      of the same shape and dtype as `x_start` and returns the triple\n      `(gradient_unregularized_loss, hessian_unregularized_loss_outer,\n      hessian_unregularized_loss_middle)` as defined in the argument spec of\n      `minimize_one_step`.\n    x_start: (Batch of) vector-shaped, `float` `Tensor` representing the initial\n      value of the argument to the `Loss` function.\n    tolerance: scalar, `float` `Tensor` representing the tolerance for each\n      optimization step; see the `tolerance` argument of\n      `minimize_one_step`.\n    l1_regularizer: scalar, `float` `Tensor` representing the weight of the L1\n      regularization term (see equation above).\n    l2_regularizer: scalar, `float` `Tensor` representing the weight of the L2\n      regularization term (see equation above).\n      Default value: `None` (i.e., no L2 regularization).\n    maximum_iterations: Python integer specifying the maximum number of\n      iterations of the outer loop of the optimizer.  After this many iterations\n      of the outer loop, the algorithm will terminate even if the return value\n      `optimal_x` has not converged.\n      Default value: `1`.\n    maximum_full_sweeps_per_iteration: Python integer specifying the maximum\n      number of sweeps allowed in each iteration of the outer loop of the\n      optimizer.  Passed as the `maximum_full_sweeps` argument to\n      `minimize_one_step`.\n      Default value: `1`.\n    learning_rate: scalar, `float` `Tensor` representing a multiplicative factor\n      used to dampen the proximal gradient descent steps.\n      Default value: `None` (i.e., factor is conceptually `1`).\n    name: Python string representing the name of the TensorFlow operation.\n      The default name is `\"minimize\"`.\n\n  Returns:\n    x: `Tensor` of the same shape and dtype as `x_start`, representing the\n      (batches of) computed values of `x` which minimizes `Loss(x)`.\n    is_converged: scalar, `bool` `Tensor` indicating whether the minimization\n      procedure converged within the specified number of iterations across all\n      batches.  Here convergence means that an iteration of the inner loop\n      (`minimize_one_step`) returns `True` for its `is_converged` output value.\n    iter: scalar, `int` `Tensor` indicating the actual number of iterations of\n      the outer loop of the optimizer completed (i.e., number of calls to\n      `minimize_one_step` before achieving convergence).\n\n  #### References\n\n  [1]: Jerome Friedman, Trevor Hastie and Rob Tibshirani. Regularization Paths\n       for Generalized Linear Models via Coordinate Descent. _Journal of\n       Statistical Software_, 33(1), 2010.\n       https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf\n\n  [2]: Guo-Xun Yuan, Chia-Hua Ho and Chih-Jen Lin. An Improved GLMNET for\n       L1-regularized Logistic Regression. _Journal of Machine Learning\n       Research_, 13, 2012.\n       http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf\n  \"\"\"\n  graph_deps = [\n      x_start,\n      l1_regularizer,\n      l2_regularizer,\n      maximum_iterations,\n      maximum_full_sweeps_per_iteration,\n      tolerance,\n      learning_rate,\n  ],\n  with tf.compat.v1.name_scope(name, 'minimize', graph_deps):\n\n    def _loop_cond(x_start, converged, iter_):\n      del x_start\n      return tf.logical_and(iter_ < maximum_iterations,\n                            tf.logical_not(converged))\n\n    def _loop_body(x_start, converged, iter_):  # pylint: disable=missing-docstring\n      g, h_outer, h_middle = grad_and_hessian_loss_fn(x_start)\n      x_start, converged, _ = minimize_one_step(\n          gradient_unregularized_loss=g,\n          hessian_unregularized_loss_outer=h_outer,\n          hessian_unregularized_loss_middle=h_middle,\n          x_start=x_start,\n          l1_regularizer=l1_regularizer,\n          l2_regularizer=l2_regularizer,\n          maximum_full_sweeps=maximum_full_sweeps_per_iteration,\n          tolerance=tolerance,\n          learning_rate=learning_rate)\n      return x_start, converged, iter_ + 1\n\n    return tf.while_loop(\n        cond=_loop_cond,\n        body=_loop_body,\n        loop_vars=[\n            x_start,\n            tf.zeros([], np.bool, name='converged'),\n            tf.zeros([], np.int32, name='iter'),\n        ])", "language": "python", "code": "def minimize(grad_and_hessian_loss_fn,\n             x_start,\n             tolerance,\n             l1_regularizer,\n             l2_regularizer=None,\n             maximum_iterations=1,\n             maximum_full_sweeps_per_iteration=1,\n             learning_rate=None,\n             name=None):\n  \"\"\"Minimize using Hessian-informed proximal gradient descent.\n\n  This function solves the regularized minimization problem\n\n  ```none\n  argmin{ Loss(x)\n            + l1_regularizer * ||x||_1\n            + l2_regularizer * ||x||_2**2\n          : x in R^n }\n  ```\n\n  where `Loss` is a convex C^2 function (typically, `Loss` is the negative log\n  likelihood of a model and `x` is a vector of model coefficients).  The `Loss`\n  function does not need to be supplied directly, but this optimizer does need a\n  way to compute the gradient and Hessian of the Loss function at a given value\n  of `x`.  The gradient and Hessian are often computationally expensive, and\n  this optimizer calls them relatively few times compared with other algorithms.\n\n  Args:\n    grad_and_hessian_loss_fn: callable that takes as input a (batch of) `Tensor`\n      of the same shape and dtype as `x_start` and returns the triple\n      `(gradient_unregularized_loss, hessian_unregularized_loss_outer,\n      hessian_unregularized_loss_middle)` as defined in the argument spec of\n      `minimize_one_step`.\n    x_start: (Batch of) vector-shaped, `float` `Tensor` representing the initial\n      value of the argument to the `Loss` function.\n    tolerance: scalar, `float` `Tensor` representing the tolerance for each\n      optimization step; see the `tolerance` argument of\n      `minimize_one_step`.\n    l1_regularizer: scalar, `float` `Tensor` representing the weight of the L1\n      regularization term (see equation above).\n    l2_regularizer: scalar, `float` `Tensor` representing the weight of the L2\n      regularization term (see equation above).\n      Default value: `None` (i.e., no L2 regularization).\n    maximum_iterations: Python integer specifying the maximum number of\n      iterations of the outer loop of the optimizer.  After this many iterations\n      of the outer loop, the algorithm will terminate even if the return value\n      `optimal_x` has not converged.\n      Default value: `1`.\n    maximum_full_sweeps_per_iteration: Python integer specifying the maximum\n      number of sweeps allowed in each iteration of the outer loop of the\n      optimizer.  Passed as the `maximum_full_sweeps` argument to\n      `minimize_one_step`.\n      Default value: `1`.\n    learning_rate: scalar, `float` `Tensor` representing a multiplicative factor\n      used to dampen the proximal gradient descent steps.\n      Default value: `None` (i.e., factor is conceptually `1`).\n    name: Python string representing the name of the TensorFlow operation.\n      The default name is `\"minimize\"`.\n\n  Returns:\n    x: `Tensor` of the same shape and dtype as `x_start`, representing the\n      (batches of) computed values of `x` which minimizes `Loss(x)`.\n    is_converged: scalar, `bool` `Tensor` indicating whether the minimization\n      procedure converged within the specified number of iterations across all\n      batches.  Here convergence means that an iteration of the inner loop\n      (`minimize_one_step`) returns `True` for its `is_converged` output value.\n    iter: scalar, `int` `Tensor` indicating the actual number of iterations of\n      the outer loop of the optimizer completed (i.e., number of calls to\n      `minimize_one_step` before achieving convergence).\n\n  #### References\n\n  [1]: Jerome Friedman, Trevor Hastie and Rob Tibshirani. Regularization Paths\n       for Generalized Linear Models via Coordinate Descent. _Journal of\n       Statistical Software_, 33(1), 2010.\n       https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf\n\n  [2]: Guo-Xun Yuan, Chia-Hua Ho and Chih-Jen Lin. An Improved GLMNET for\n       L1-regularized Logistic Regression. _Journal of Machine Learning\n       Research_, 13, 2012.\n       http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf\n  \"\"\"\n  graph_deps = [\n      x_start,\n      l1_regularizer,\n      l2_regularizer,\n      maximum_iterations,\n      maximum_full_sweeps_per_iteration,\n      tolerance,\n      learning_rate,\n  ],\n  with tf.compat.v1.name_scope(name, 'minimize', graph_deps):\n\n    def _loop_cond(x_start, converged, iter_):\n      del x_start\n      return tf.logical_and(iter_ < maximum_iterations,\n                            tf.logical_not(converged))\n\n    def _loop_body(x_start, converged, iter_):  # pylint: disable=missing-docstring\n      g, h_outer, h_middle = grad_and_hessian_loss_fn(x_start)\n      x_start, converged, _ = minimize_one_step(\n          gradient_unregularized_loss=g,\n          hessian_unregularized_loss_outer=h_outer,\n          hessian_unregularized_loss_middle=h_middle,\n          x_start=x_start,\n          l1_regularizer=l1_regularizer,\n          l2_regularizer=l2_regularizer,\n          maximum_full_sweeps=maximum_full_sweeps_per_iteration,\n          tolerance=tolerance,\n          learning_rate=learning_rate)\n      return x_start, converged, iter_ + 1\n\n    return tf.while_loop(\n        cond=_loop_cond,\n        body=_loop_body,\n        loop_vars=[\n            x_start,\n            tf.zeros([], np.bool, name='converged'),\n            tf.zeros([], np.int32, name='iter'),\n        ])", "code_tokens": ["def", "minimize", "(", "grad_and_hessian_loss_fn", ",", "x_start", ",", "tolerance", ",", "l1_regularizer", ",", "l2_regularizer", "=", "None", ",", "maximum_iterations", "=", "1", ",", "maximum_full_sweeps_per_iteration", "=", "1", ",", "learning_rate", "=", "None", ",", "name", "=", "None", ")", ":", "graph_deps", "=", "[", "x_start", ",", "l1_regularizer", ",", "l2_regularizer", ",", "maximum_iterations", ",", "maximum_full_sweeps_per_iteration", ",", "tolerance", ",", "learning_rate", ",", "]", ",", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'minimize'", ",", "graph_deps", ")", ":", "def", "_loop_cond", "(", "x_start", ",", "converged", ",", "iter_", ")", ":", "del", "x_start", "return", "tf", ".", "logical_and", "(", "iter_", "<", "maximum_iterations", ",", "tf", ".", "logical_not", "(", "converged", ")", ")", "def", "_loop_body", "(", "x_start", ",", "converged", ",", "iter_", ")", ":", "# pylint: disable=missing-docstring", "g", ",", "h_outer", ",", "h_middle", "=", "grad_and_hessian_loss_fn", "(", "x_start", ")", "x_start", ",", "converged", ",", "_", "=", "minimize_one_step", "(", "gradient_unregularized_loss", "=", "g", ",", "hessian_unregularized_loss_outer", "=", "h_outer", ",", "hessian_unregularized_loss_middle", "=", "h_middle", ",", "x_start", "=", "x_start", ",", "l1_regularizer", "=", "l1_regularizer", ",", "l2_regularizer", "=", "l2_regularizer", ",", "maximum_full_sweeps", "=", "maximum_full_sweeps_per_iteration", ",", "tolerance", "=", "tolerance", ",", "learning_rate", "=", "learning_rate", ")", "return", "x_start", ",", "converged", ",", "iter_", "+", "1", "return", "tf", ".", "while_loop", "(", "cond", "=", "_loop_cond", ",", "body", "=", "_loop_body", ",", "loop_vars", "=", "[", "x_start", ",", "tf", ".", "zeros", "(", "[", "]", ",", "np", ".", "bool", ",", "name", "=", "'converged'", ")", ",", "tf", ".", "zeros", "(", "[", "]", ",", "np", ".", "int32", ",", "name", "=", "'iter'", ")", ",", "]", ")"], "docstring": "Minimize using Hessian-informed proximal gradient descent.\n\n  This function solves the regularized minimization problem\n\n  ```none\n  argmin{ Loss(x)\n            + l1_regularizer * ||x||_1\n            + l2_regularizer * ||x||_2**2\n          : x in R^n }\n  ```\n\n  where `Loss` is a convex C^2 function (typically, `Loss` is the negative log\n  likelihood of a model and `x` is a vector of model coefficients).  The `Loss`\n  function does not need to be supplied directly, but this optimizer does need a\n  way to compute the gradient and Hessian of the Loss function at a given value\n  of `x`.  The gradient and Hessian are often computationally expensive, and\n  this optimizer calls them relatively few times compared with other algorithms.\n\n  Args:\n    grad_and_hessian_loss_fn: callable that takes as input a (batch of) `Tensor`\n      of the same shape and dtype as `x_start` and returns the triple\n      `(gradient_unregularized_loss, hessian_unregularized_loss_outer,\n      hessian_unregularized_loss_middle)` as defined in the argument spec of\n      `minimize_one_step`.\n    x_start: (Batch of) vector-shaped, `float` `Tensor` representing the initial\n      value of the argument to the `Loss` function.\n    tolerance: scalar, `float` `Tensor` representing the tolerance for each\n      optimization step; see the `tolerance` argument of\n      `minimize_one_step`.\n    l1_regularizer: scalar, `float` `Tensor` representing the weight of the L1\n      regularization term (see equation above).\n    l2_regularizer: scalar, `float` `Tensor` representing the weight of the L2\n      regularization term (see equation above).\n      Default value: `None` (i.e., no L2 regularization).\n    maximum_iterations: Python integer specifying the maximum number of\n      iterations of the outer loop of the optimizer.  After this many iterations\n      of the outer loop, the algorithm will terminate even if the return value\n      `optimal_x` has not converged.\n      Default value: `1`.\n    maximum_full_sweeps_per_iteration: Python integer specifying the maximum\n      number of sweeps allowed in each iteration of the outer loop of the\n      optimizer.  Passed as the `maximum_full_sweeps` argument to\n      `minimize_one_step`.\n      Default value: `1`.\n    learning_rate: scalar, `float` `Tensor` representing a multiplicative factor\n      used to dampen the proximal gradient descent steps.\n      Default value: `None` (i.e., factor is conceptually `1`).\n    name: Python string representing the name of the TensorFlow operation.\n      The default name is `\"minimize\"`.\n\n  Returns:\n    x: `Tensor` of the same shape and dtype as `x_start`, representing the\n      (batches of) computed values of `x` which minimizes `Loss(x)`.\n    is_converged: scalar, `bool` `Tensor` indicating whether the minimization\n      procedure converged within the specified number of iterations across all\n      batches.  Here convergence means that an iteration of the inner loop\n      (`minimize_one_step`) returns `True` for its `is_converged` output value.\n    iter: scalar, `int` `Tensor` indicating the actual number of iterations of\n      the outer loop of the optimizer completed (i.e., number of calls to\n      `minimize_one_step` before achieving convergence).\n\n  #### References\n\n  [1]: Jerome Friedman, Trevor Hastie and Rob Tibshirani. Regularization Paths\n       for Generalized Linear Models via Coordinate Descent. _Journal of\n       Statistical Software_, 33(1), 2010.\n       https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf\n\n  [2]: Guo-Xun Yuan, Chia-Hua Ho and Chih-Jen Lin. An Improved GLMNET for\n       L1-regularized Logistic Regression. _Journal of Machine Learning\n       Research_, 13, 2012.\n       http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf", "docstring_tokens": ["Minimize", "using", "Hessian", "-", "informed", "proximal", "gradient", "descent", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/proximal_hessian_sparse.py#L471-L590", "partition": "test", "index": 844, "time": "2019-01-30 10:47:34"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/linalg.py", "func_name": "_sparse_block_diag", "original_string": "def _sparse_block_diag(sp_a):\n  \"\"\"Returns a block diagonal rank 2 SparseTensor from a batch of SparseTensors.\n\n  Args:\n    sp_a: A rank 3 `SparseTensor` representing a batch of matrices.\n\n  Returns:\n    sp_block_diag_a: matrix-shaped, `float` `SparseTensor` with the same dtype\n    as `sparse_or_matrix`, of shape [B * M, B * N] where `sp_a` has shape\n    [B, M, N]. Each [M, N] batch of `sp_a` is lined up along the diagonal.\n  \"\"\"\n  # Construct the matrix [[M, N], [1, 0], [0, 1]] which would map the index\n  # (b, i, j) to (Mb + i, Nb + j). This effectively creates a block-diagonal\n  # matrix of dense shape [B * M, B * N].\n  # Note that this transformation doesn't increase the number of non-zero\n  # entries in the SparseTensor.\n  sp_a_shape = tf.convert_to_tensor(value=_get_shape(sp_a, tf.int64))\n  ind_mat = tf.concat([[sp_a_shape[-2:]], tf.eye(2, dtype=tf.int64)], axis=0)\n  indices = tf.matmul(sp_a.indices, ind_mat)\n  dense_shape = sp_a_shape[0] * sp_a_shape[1:]\n  return tf.SparseTensor(\n      indices=indices, values=sp_a.values, dense_shape=dense_shape)", "language": "python", "code": "def _sparse_block_diag(sp_a):\n  \"\"\"Returns a block diagonal rank 2 SparseTensor from a batch of SparseTensors.\n\n  Args:\n    sp_a: A rank 3 `SparseTensor` representing a batch of matrices.\n\n  Returns:\n    sp_block_diag_a: matrix-shaped, `float` `SparseTensor` with the same dtype\n    as `sparse_or_matrix`, of shape [B * M, B * N] where `sp_a` has shape\n    [B, M, N]. Each [M, N] batch of `sp_a` is lined up along the diagonal.\n  \"\"\"\n  # Construct the matrix [[M, N], [1, 0], [0, 1]] which would map the index\n  # (b, i, j) to (Mb + i, Nb + j). This effectively creates a block-diagonal\n  # matrix of dense shape [B * M, B * N].\n  # Note that this transformation doesn't increase the number of non-zero\n  # entries in the SparseTensor.\n  sp_a_shape = tf.convert_to_tensor(value=_get_shape(sp_a, tf.int64))\n  ind_mat = tf.concat([[sp_a_shape[-2:]], tf.eye(2, dtype=tf.int64)], axis=0)\n  indices = tf.matmul(sp_a.indices, ind_mat)\n  dense_shape = sp_a_shape[0] * sp_a_shape[1:]\n  return tf.SparseTensor(\n      indices=indices, values=sp_a.values, dense_shape=dense_shape)", "code_tokens": ["def", "_sparse_block_diag", "(", "sp_a", ")", ":", "# Construct the matrix [[M, N], [1, 0], [0, 1]] which would map the index", "# (b, i, j) to (Mb + i, Nb + j). This effectively creates a block-diagonal", "# matrix of dense shape [B * M, B * N].", "# Note that this transformation doesn't increase the number of non-zero", "# entries in the SparseTensor.", "sp_a_shape", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "_get_shape", "(", "sp_a", ",", "tf", ".", "int64", ")", ")", "ind_mat", "=", "tf", ".", "concat", "(", "[", "[", "sp_a_shape", "[", "-", "2", ":", "]", "]", ",", "tf", ".", "eye", "(", "2", ",", "dtype", "=", "tf", ".", "int64", ")", "]", ",", "axis", "=", "0", ")", "indices", "=", "tf", ".", "matmul", "(", "sp_a", ".", "indices", ",", "ind_mat", ")", "dense_shape", "=", "sp_a_shape", "[", "0", "]", "*", "sp_a_shape", "[", "1", ":", "]", "return", "tf", ".", "SparseTensor", "(", "indices", "=", "indices", ",", "values", "=", "sp_a", ".", "values", ",", "dense_shape", "=", "dense_shape", ")"], "docstring": "Returns a block diagonal rank 2 SparseTensor from a batch of SparseTensors.\n\n  Args:\n    sp_a: A rank 3 `SparseTensor` representing a batch of matrices.\n\n  Returns:\n    sp_block_diag_a: matrix-shaped, `float` `SparseTensor` with the same dtype\n    as `sparse_or_matrix`, of shape [B * M, B * N] where `sp_a` has shape\n    [B, M, N]. Each [M, N] batch of `sp_a` is lined up along the diagonal.", "docstring_tokens": ["Returns", "a", "block", "diagonal", "rank", "2", "SparseTensor", "from", "a", "batch", "of", "SparseTensors", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/linalg.py#L874-L895", "partition": "test", "index": 1111, "time": "2019-01-30 10:47:34"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/semilocal_linear_trend.py", "func_name": "semilocal_linear_trend_transition_noise", "original_string": "def semilocal_linear_trend_transition_noise(level_scale,\n                                            slope_mean,\n                                            slope_scale,\n                                            autoregressive_coef):\n  \"\"\"Build the transition noise model for a semi-local linear trend model.\"\"\"\n\n  # At each timestep, the stochasticity of `level` and `slope` are given\n  # by `level_scale` and `slope_scale` respectively.\n  broadcast_batch_shape = dist_util.get_broadcast_shape(\n      level_scale, slope_mean, slope_scale, autoregressive_coef)\n  broadcast_ones = tf.ones(broadcast_batch_shape, dtype=level_scale.dtype)\n  scale_diag = tf.stack([level_scale * broadcast_ones,\n                         slope_scale * broadcast_ones],\n                        axis=-1)\n\n  # We additionally fold in a bias term implementing the nonzero `slope_mean`.\n  # The overall `slope` update is (from `SemiLocalLinearTrend` docstring)\n  #   slope[t] = (slope_mean +\n  #               autoregressive_coef * (slope[t-1] - slope_mean) +\n  #               Normal(0., slope_scale))\n  # which we rewrite as\n  #   slope[t] = (\n  #    autoregressive_coef * slope[t-1] +                  # linear transition\n  #    Normal(loc=slope_mean - autoregressive_coef * slope_mean,  # noise bias\n  #           scale=slope_scale))                                 # noise scale\n  bias = tf.stack([tf.zeros_like(broadcast_ones),\n                   slope_mean * (1 - autoregressive_coef) * broadcast_ones],\n                  axis=-1)\n  return tfd.MultivariateNormalDiag(\n      loc=bias,\n      scale_diag=scale_diag)", "language": "python", "code": "def semilocal_linear_trend_transition_noise(level_scale,\n                                            slope_mean,\n                                            slope_scale,\n                                            autoregressive_coef):\n  \"\"\"Build the transition noise model for a semi-local linear trend model.\"\"\"\n\n  # At each timestep, the stochasticity of `level` and `slope` are given\n  # by `level_scale` and `slope_scale` respectively.\n  broadcast_batch_shape = dist_util.get_broadcast_shape(\n      level_scale, slope_mean, slope_scale, autoregressive_coef)\n  broadcast_ones = tf.ones(broadcast_batch_shape, dtype=level_scale.dtype)\n  scale_diag = tf.stack([level_scale * broadcast_ones,\n                         slope_scale * broadcast_ones],\n                        axis=-1)\n\n  # We additionally fold in a bias term implementing the nonzero `slope_mean`.\n  # The overall `slope` update is (from `SemiLocalLinearTrend` docstring)\n  #   slope[t] = (slope_mean +\n  #               autoregressive_coef * (slope[t-1] - slope_mean) +\n  #               Normal(0., slope_scale))\n  # which we rewrite as\n  #   slope[t] = (\n  #    autoregressive_coef * slope[t-1] +                  # linear transition\n  #    Normal(loc=slope_mean - autoregressive_coef * slope_mean,  # noise bias\n  #           scale=slope_scale))                                 # noise scale\n  bias = tf.stack([tf.zeros_like(broadcast_ones),\n                   slope_mean * (1 - autoregressive_coef) * broadcast_ones],\n                  axis=-1)\n  return tfd.MultivariateNormalDiag(\n      loc=bias,\n      scale_diag=scale_diag)", "code_tokens": ["def", "semilocal_linear_trend_transition_noise", "(", "level_scale", ",", "slope_mean", ",", "slope_scale", ",", "autoregressive_coef", ")", ":", "# At each timestep, the stochasticity of `level` and `slope` are given", "# by `level_scale` and `slope_scale` respectively.", "broadcast_batch_shape", "=", "dist_util", ".", "get_broadcast_shape", "(", "level_scale", ",", "slope_mean", ",", "slope_scale", ",", "autoregressive_coef", ")", "broadcast_ones", "=", "tf", ".", "ones", "(", "broadcast_batch_shape", ",", "dtype", "=", "level_scale", ".", "dtype", ")", "scale_diag", "=", "tf", ".", "stack", "(", "[", "level_scale", "*", "broadcast_ones", ",", "slope_scale", "*", "broadcast_ones", "]", ",", "axis", "=", "-", "1", ")", "# We additionally fold in a bias term implementing the nonzero `slope_mean`.", "# The overall `slope` update is (from `SemiLocalLinearTrend` docstring)", "#   slope[t] = (slope_mean +", "#               autoregressive_coef * (slope[t-1] - slope_mean) +", "#               Normal(0., slope_scale))", "# which we rewrite as", "#   slope[t] = (", "#    autoregressive_coef * slope[t-1] +                  # linear transition", "#    Normal(loc=slope_mean - autoregressive_coef * slope_mean,  # noise bias", "#           scale=slope_scale))                                 # noise scale", "bias", "=", "tf", ".", "stack", "(", "[", "tf", ".", "zeros_like", "(", "broadcast_ones", ")", ",", "slope_mean", "*", "(", "1", "-", "autoregressive_coef", ")", "*", "broadcast_ones", "]", ",", "axis", "=", "-", "1", ")", "return", "tfd", ".", "MultivariateNormalDiag", "(", "loc", "=", "bias", ",", "scale_diag", "=", "scale_diag", ")"], "docstring": "Build the transition noise model for a semi-local linear trend model.", "docstring_tokens": ["Build", "the", "transition", "noise", "model", "for", "a", "semi", "-", "local", "linear", "trend", "model", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/semilocal_linear_trend.py#L266-L296", "partition": "test", "index": 664, "time": "2019-01-30 10:59:04"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/semilocal_linear_trend.py", "func_name": "semilocal_linear_trend_transition_matrix", "original_string": "def semilocal_linear_trend_transition_matrix(autoregressive_coef):\n  \"\"\"Build the transition matrix for a semi-local linear trend model.\"\"\"\n  # We want to write the following 2 x 2 matrix:\n  #  [[1., 1., ],    # level(t+1) = level(t) + slope(t)\n  #   [0., ar_coef], # slope(t+1) = ar_coef * slope(t)\n  # but it's slightly tricky to properly incorporate the batch shape of\n  # autoregressive_coef. E.g., if autoregressive_coef has shape [4,6], we want\n  # to return shape [4, 6, 2, 2]. We do this by breaking the matrix into its\n  # fixed entries, written explicitly, and then the autoregressive_coef part\n  # which we add in after using a mask to broadcast to the correct matrix shape.\n\n  fixed_entries = tf.constant(\n      [[1., 1.],\n       [0., 0.]],\n      dtype=autoregressive_coef.dtype)\n\n  autoregressive_coef_mask = tf.constant([[0., 0.],\n                                          [0., 1.]],\n                                         dtype=autoregressive_coef.dtype)\n  bottom_right_entry = (autoregressive_coef[..., tf.newaxis, tf.newaxis] *\n                        autoregressive_coef_mask)\n  return tf.linalg.LinearOperatorFullMatrix(\n      fixed_entries + bottom_right_entry)", "language": "python", "code": "def semilocal_linear_trend_transition_matrix(autoregressive_coef):\n  \"\"\"Build the transition matrix for a semi-local linear trend model.\"\"\"\n  # We want to write the following 2 x 2 matrix:\n  #  [[1., 1., ],    # level(t+1) = level(t) + slope(t)\n  #   [0., ar_coef], # slope(t+1) = ar_coef * slope(t)\n  # but it's slightly tricky to properly incorporate the batch shape of\n  # autoregressive_coef. E.g., if autoregressive_coef has shape [4,6], we want\n  # to return shape [4, 6, 2, 2]. We do this by breaking the matrix into its\n  # fixed entries, written explicitly, and then the autoregressive_coef part\n  # which we add in after using a mask to broadcast to the correct matrix shape.\n\n  fixed_entries = tf.constant(\n      [[1., 1.],\n       [0., 0.]],\n      dtype=autoregressive_coef.dtype)\n\n  autoregressive_coef_mask = tf.constant([[0., 0.],\n                                          [0., 1.]],\n                                         dtype=autoregressive_coef.dtype)\n  bottom_right_entry = (autoregressive_coef[..., tf.newaxis, tf.newaxis] *\n                        autoregressive_coef_mask)\n  return tf.linalg.LinearOperatorFullMatrix(\n      fixed_entries + bottom_right_entry)", "code_tokens": ["def", "semilocal_linear_trend_transition_matrix", "(", "autoregressive_coef", ")", ":", "# We want to write the following 2 x 2 matrix:", "#  [[1., 1., ],    # level(t+1) = level(t) + slope(t)", "#   [0., ar_coef], # slope(t+1) = ar_coef * slope(t)", "# but it's slightly tricky to properly incorporate the batch shape of", "# autoregressive_coef. E.g., if autoregressive_coef has shape [4,6], we want", "# to return shape [4, 6, 2, 2]. We do this by breaking the matrix into its", "# fixed entries, written explicitly, and then the autoregressive_coef part", "# which we add in after using a mask to broadcast to the correct matrix shape.", "fixed_entries", "=", "tf", ".", "constant", "(", "[", "[", "1.", ",", "1.", "]", ",", "[", "0.", ",", "0.", "]", "]", ",", "dtype", "=", "autoregressive_coef", ".", "dtype", ")", "autoregressive_coef_mask", "=", "tf", ".", "constant", "(", "[", "[", "0.", ",", "0.", "]", ",", "[", "0.", ",", "1.", "]", "]", ",", "dtype", "=", "autoregressive_coef", ".", "dtype", ")", "bottom_right_entry", "=", "(", "autoregressive_coef", "[", "...", ",", "tf", ".", "newaxis", ",", "tf", ".", "newaxis", "]", "*", "autoregressive_coef_mask", ")", "return", "tf", ".", "linalg", ".", "LinearOperatorFullMatrix", "(", "fixed_entries", "+", "bottom_right_entry", ")"], "docstring": "Build the transition matrix for a semi-local linear trend model.", "docstring_tokens": ["Build", "the", "transition", "matrix", "for", "a", "semi", "-", "local", "linear", "trend", "model", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/semilocal_linear_trend.py#L241-L263", "partition": "test", "index": 663, "time": "2019-01-30 10:59:04"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py", "func_name": "bracket", "original_string": "def bracket(value_and_gradients_function,\n            search_interval,\n            f_lim,\n            max_iterations,\n            expansion_param=5.0):\n  \"\"\"Brackets the minimum given an initial starting point.\n\n  Applies the Hager Zhang bracketing algorithm to find an interval containing\n  a region with points satisfying Wolfe conditions. Uses the supplied initial\n  step size 'c', the right end point of the provided search interval, to find\n  such an interval. The only condition on 'c' is that it should be positive.\n  For more details see steps B0-B3 in [Hager and Zhang (2006)][2].\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple containing the value filed `f` of the\n      function and its derivative value field `df` at that point.\n      Alternatively, the function may representthe batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and return a tuple of two tensors of shape [n], the\n      function values and the corresponding derivatives at the input points.\n    search_interval: A namedtuple describing the current search interval,\n      must include the fields:\n      - converged: Boolean `Tensor` of shape [n], indicating batch members\n          where search has already converged. Interval for these batch members\n          wont be modified.\n      - failed: Boolean `Tensor` of shape [n], indicating batch members\n          where search has already failed. Interval for these batch members\n          wont be modified.\n      - iterations: Scalar int32 `Tensor`. Number of line search iterations\n          so far.\n      - func_evals: Scalar int32 `Tensor`. Number of function evaluations\n          so far.\n      - left: A namedtuple, as returned by value_and_gradients_function\n          evaluated at 0, the left end point of the current interval.\n      - right: A namedtuple, as returned by value_and_gradients_function,\n          of the right end point of the current interval (labelled 'c' above).\n    f_lim: real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked for each batch member.\n    max_iterations: Int32 scalar `Tensor`. The maximum number of iterations\n      permitted. The limit applies equally to all batch members.\n    expansion_param: Scalar positive `Tensor` of real dtype. Must be greater\n      than `1.`. Used to expand the initial interval in case it does not bracket\n      a minimum.\n\n  Returns:\n    A namedtuple with the following fields.\n      iteration: An int32 scalar `Tensor`. The number of iterations performed.\n        Bounded above by `max_iterations` parameter.\n      stopped: A boolean `Tensor` of shape [n]. True for those batch members\n        where the algorithm terminated before reaching `max_iterations`.\n      failed: A boolean `Tensor` of shape [n]. True for those batch members\n        where an error was encountered during bracketing.\n      num_evals: An int32 scalar `Tensor`. The number of times the objective\n        function was evaluated.\n      left: Return value of value_and_gradients_function at the updated left\n        end point of the interval found.\n      right: Return value of value_and_gradients_function at the updated right\n        end point of the interval found.\n  \"\"\"\n  already_stopped = search_interval.failed | search_interval.converged\n\n  # If the slope at right end point is positive, step B1 in [2], then the given\n  # initial points already bracket a minimum.\n  bracketed = search_interval.right.df >= 0\n\n  # Bisection is needed, step B2, if right end point almost works as a new left\n  # end point but the objective value is too high.\n  needs_bisect = (\n      search_interval.right.df < 0) & (search_interval.right.f > f_lim)\n\n  # In these three cases bracketing is already `stopped` and there is no need\n  # to perform further evaluations. Otherwise the bracketing loop is needed to\n  # expand the interval, step B3, until the conditions are met.\n  initial_args = _IntermediateResult(\n      iteration=search_interval.iterations,\n      stopped=already_stopped | bracketed | needs_bisect,\n      failed=search_interval.failed,\n      num_evals=search_interval.func_evals,\n      left=search_interval.left,\n      right=search_interval.right)\n\n  def _loop_cond(curr):\n    return (curr.iteration <\n            max_iterations) & ~tf.reduce_all(input_tensor=curr.stopped)\n\n  def _loop_body(curr):\n    \"\"\"Main body of bracketing loop.\"\"\"\n    # The loop maintains the invariant that curr.stopped is true if we have\n    # either: failed, successfully bracketed, or not yet bracketed but needs\n    # bisect. On the only remaining case, step B3 in [2]. case we need to\n    # expand and update the left/right values appropriately.\n    new_right = value_and_gradients_function(expansion_param * curr.right.x)\n    left = val_where(curr.stopped, curr.left, curr.right)\n    right = val_where(curr.stopped, curr.right, new_right)\n\n    # Updated the failed, bracketed, and needs_bisect conditions.\n    failed = curr.failed | ~is_finite(right)\n    bracketed = right.df >= 0\n    needs_bisect = (right.df < 0) & (right.f > f_lim)\n    return [_IntermediateResult(\n        iteration=curr.iteration + 1,\n        stopped=curr.stopped | failed | bracketed | needs_bisect,\n        failed=failed,\n        num_evals=curr.num_evals + 1,\n        left=left,\n        right=right)]\n\n  bracket_result = tf.while_loop(\n      cond=_loop_cond, body=_loop_body, loop_vars=[initial_args])[0]\n\n  # For entries where bisect is still needed, mark them as not yet stopped,\n  # reset the left end point, and run `_bisect` on them.\n  needs_bisect = (\n      (bracket_result.right.df < 0) & (bracket_result.right.f > f_lim))\n  stopped = already_stopped | bracket_result.failed | ~needs_bisect\n  left = val_where(stopped, bracket_result.left, search_interval.left)\n  bisect_args = bracket_result._replace(stopped=stopped, left=left)\n  return _bisect(value_and_gradients_function, bisect_args, f_lim)", "language": "python", "code": "def bracket(value_and_gradients_function,\n            search_interval,\n            f_lim,\n            max_iterations,\n            expansion_param=5.0):\n  \"\"\"Brackets the minimum given an initial starting point.\n\n  Applies the Hager Zhang bracketing algorithm to find an interval containing\n  a region with points satisfying Wolfe conditions. Uses the supplied initial\n  step size 'c', the right end point of the provided search interval, to find\n  such an interval. The only condition on 'c' is that it should be positive.\n  For more details see steps B0-B3 in [Hager and Zhang (2006)][2].\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple containing the value filed `f` of the\n      function and its derivative value field `df` at that point.\n      Alternatively, the function may representthe batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and return a tuple of two tensors of shape [n], the\n      function values and the corresponding derivatives at the input points.\n    search_interval: A namedtuple describing the current search interval,\n      must include the fields:\n      - converged: Boolean `Tensor` of shape [n], indicating batch members\n          where search has already converged. Interval for these batch members\n          wont be modified.\n      - failed: Boolean `Tensor` of shape [n], indicating batch members\n          where search has already failed. Interval for these batch members\n          wont be modified.\n      - iterations: Scalar int32 `Tensor`. Number of line search iterations\n          so far.\n      - func_evals: Scalar int32 `Tensor`. Number of function evaluations\n          so far.\n      - left: A namedtuple, as returned by value_and_gradients_function\n          evaluated at 0, the left end point of the current interval.\n      - right: A namedtuple, as returned by value_and_gradients_function,\n          of the right end point of the current interval (labelled 'c' above).\n    f_lim: real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked for each batch member.\n    max_iterations: Int32 scalar `Tensor`. The maximum number of iterations\n      permitted. The limit applies equally to all batch members.\n    expansion_param: Scalar positive `Tensor` of real dtype. Must be greater\n      than `1.`. Used to expand the initial interval in case it does not bracket\n      a minimum.\n\n  Returns:\n    A namedtuple with the following fields.\n      iteration: An int32 scalar `Tensor`. The number of iterations performed.\n        Bounded above by `max_iterations` parameter.\n      stopped: A boolean `Tensor` of shape [n]. True for those batch members\n        where the algorithm terminated before reaching `max_iterations`.\n      failed: A boolean `Tensor` of shape [n]. True for those batch members\n        where an error was encountered during bracketing.\n      num_evals: An int32 scalar `Tensor`. The number of times the objective\n        function was evaluated.\n      left: Return value of value_and_gradients_function at the updated left\n        end point of the interval found.\n      right: Return value of value_and_gradients_function at the updated right\n        end point of the interval found.\n  \"\"\"\n  already_stopped = search_interval.failed | search_interval.converged\n\n  # If the slope at right end point is positive, step B1 in [2], then the given\n  # initial points already bracket a minimum.\n  bracketed = search_interval.right.df >= 0\n\n  # Bisection is needed, step B2, if right end point almost works as a new left\n  # end point but the objective value is too high.\n  needs_bisect = (\n      search_interval.right.df < 0) & (search_interval.right.f > f_lim)\n\n  # In these three cases bracketing is already `stopped` and there is no need\n  # to perform further evaluations. Otherwise the bracketing loop is needed to\n  # expand the interval, step B3, until the conditions are met.\n  initial_args = _IntermediateResult(\n      iteration=search_interval.iterations,\n      stopped=already_stopped | bracketed | needs_bisect,\n      failed=search_interval.failed,\n      num_evals=search_interval.func_evals,\n      left=search_interval.left,\n      right=search_interval.right)\n\n  def _loop_cond(curr):\n    return (curr.iteration <\n            max_iterations) & ~tf.reduce_all(input_tensor=curr.stopped)\n\n  def _loop_body(curr):\n    \"\"\"Main body of bracketing loop.\"\"\"\n    # The loop maintains the invariant that curr.stopped is true if we have\n    # either: failed, successfully bracketed, or not yet bracketed but needs\n    # bisect. On the only remaining case, step B3 in [2]. case we need to\n    # expand and update the left/right values appropriately.\n    new_right = value_and_gradients_function(expansion_param * curr.right.x)\n    left = val_where(curr.stopped, curr.left, curr.right)\n    right = val_where(curr.stopped, curr.right, new_right)\n\n    # Updated the failed, bracketed, and needs_bisect conditions.\n    failed = curr.failed | ~is_finite(right)\n    bracketed = right.df >= 0\n    needs_bisect = (right.df < 0) & (right.f > f_lim)\n    return [_IntermediateResult(\n        iteration=curr.iteration + 1,\n        stopped=curr.stopped | failed | bracketed | needs_bisect,\n        failed=failed,\n        num_evals=curr.num_evals + 1,\n        left=left,\n        right=right)]\n\n  bracket_result = tf.while_loop(\n      cond=_loop_cond, body=_loop_body, loop_vars=[initial_args])[0]\n\n  # For entries where bisect is still needed, mark them as not yet stopped,\n  # reset the left end point, and run `_bisect` on them.\n  needs_bisect = (\n      (bracket_result.right.df < 0) & (bracket_result.right.f > f_lim))\n  stopped = already_stopped | bracket_result.failed | ~needs_bisect\n  left = val_where(stopped, bracket_result.left, search_interval.left)\n  bisect_args = bracket_result._replace(stopped=stopped, left=left)\n  return _bisect(value_and_gradients_function, bisect_args, f_lim)", "code_tokens": ["def", "bracket", "(", "value_and_gradients_function", ",", "search_interval", ",", "f_lim", ",", "max_iterations", ",", "expansion_param", "=", "5.0", ")", ":", "already_stopped", "=", "search_interval", ".", "failed", "|", "search_interval", ".", "converged", "# If the slope at right end point is positive, step B1 in [2], then the given", "# initial points already bracket a minimum.", "bracketed", "=", "search_interval", ".", "right", ".", "df", ">=", "0", "# Bisection is needed, step B2, if right end point almost works as a new left", "# end point but the objective value is too high.", "needs_bisect", "=", "(", "search_interval", ".", "right", ".", "df", "<", "0", ")", "&", "(", "search_interval", ".", "right", ".", "f", ">", "f_lim", ")", "# In these three cases bracketing is already `stopped` and there is no need", "# to perform further evaluations. Otherwise the bracketing loop is needed to", "# expand the interval, step B3, until the conditions are met.", "initial_args", "=", "_IntermediateResult", "(", "iteration", "=", "search_interval", ".", "iterations", ",", "stopped", "=", "already_stopped", "|", "bracketed", "|", "needs_bisect", ",", "failed", "=", "search_interval", ".", "failed", ",", "num_evals", "=", "search_interval", ".", "func_evals", ",", "left", "=", "search_interval", ".", "left", ",", "right", "=", "search_interval", ".", "right", ")", "def", "_loop_cond", "(", "curr", ")", ":", "return", "(", "curr", ".", "iteration", "<", "max_iterations", ")", "&", "~", "tf", ".", "reduce_all", "(", "input_tensor", "=", "curr", ".", "stopped", ")", "def", "_loop_body", "(", "curr", ")", ":", "\"\"\"Main body of bracketing loop.\"\"\"", "# The loop maintains the invariant that curr.stopped is true if we have", "# either: failed, successfully bracketed, or not yet bracketed but needs", "# bisect. On the only remaining case, step B3 in [2]. case we need to", "# expand and update the left/right values appropriately.", "new_right", "=", "value_and_gradients_function", "(", "expansion_param", "*", "curr", ".", "right", ".", "x", ")", "left", "=", "val_where", "(", "curr", ".", "stopped", ",", "curr", ".", "left", ",", "curr", ".", "right", ")", "right", "=", "val_where", "(", "curr", ".", "stopped", ",", "curr", ".", "right", ",", "new_right", ")", "# Updated the failed, bracketed, and needs_bisect conditions.", "failed", "=", "curr", ".", "failed", "|", "~", "is_finite", "(", "right", ")", "bracketed", "=", "right", ".", "df", ">=", "0", "needs_bisect", "=", "(", "right", ".", "df", "<", "0", ")", "&", "(", "right", ".", "f", ">", "f_lim", ")", "return", "[", "_IntermediateResult", "(", "iteration", "=", "curr", ".", "iteration", "+", "1", ",", "stopped", "=", "curr", ".", "stopped", "|", "failed", "|", "bracketed", "|", "needs_bisect", ",", "failed", "=", "failed", ",", "num_evals", "=", "curr", ".", "num_evals", "+", "1", ",", "left", "=", "left", ",", "right", "=", "right", ")", "]", "bracket_result", "=", "tf", ".", "while_loop", "(", "cond", "=", "_loop_cond", ",", "body", "=", "_loop_body", ",", "loop_vars", "=", "[", "initial_args", "]", ")", "[", "0", "]", "# For entries where bisect is still needed, mark them as not yet stopped,", "# reset the left end point, and run `_bisect` on them.", "needs_bisect", "=", "(", "(", "bracket_result", ".", "right", ".", "df", "<", "0", ")", "&", "(", "bracket_result", ".", "right", ".", "f", ">", "f_lim", ")", ")", "stopped", "=", "already_stopped", "|", "bracket_result", ".", "failed", "|", "~", "needs_bisect", "left", "=", "val_where", "(", "stopped", ",", "bracket_result", ".", "left", ",", "search_interval", ".", "left", ")", "bisect_args", "=", "bracket_result", ".", "_replace", "(", "stopped", "=", "stopped", ",", "left", "=", "left", ")", "return", "_bisect", "(", "value_and_gradients_function", ",", "bisect_args", ",", "f_lim", ")"], "docstring": "Brackets the minimum given an initial starting point.\n\n  Applies the Hager Zhang bracketing algorithm to find an interval containing\n  a region with points satisfying Wolfe conditions. Uses the supplied initial\n  step size 'c', the right end point of the provided search interval, to find\n  such an interval. The only condition on 'c' is that it should be positive.\n  For more details see steps B0-B3 in [Hager and Zhang (2006)][2].\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple containing the value filed `f` of the\n      function and its derivative value field `df` at that point.\n      Alternatively, the function may representthe batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and return a tuple of two tensors of shape [n], the\n      function values and the corresponding derivatives at the input points.\n    search_interval: A namedtuple describing the current search interval,\n      must include the fields:\n      - converged: Boolean `Tensor` of shape [n], indicating batch members\n          where search has already converged. Interval for these batch members\n          wont be modified.\n      - failed: Boolean `Tensor` of shape [n], indicating batch members\n          where search has already failed. Interval for these batch members\n          wont be modified.\n      - iterations: Scalar int32 `Tensor`. Number of line search iterations\n          so far.\n      - func_evals: Scalar int32 `Tensor`. Number of function evaluations\n          so far.\n      - left: A namedtuple, as returned by value_and_gradients_function\n          evaluated at 0, the left end point of the current interval.\n      - right: A namedtuple, as returned by value_and_gradients_function,\n          of the right end point of the current interval (labelled 'c' above).\n    f_lim: real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked for each batch member.\n    max_iterations: Int32 scalar `Tensor`. The maximum number of iterations\n      permitted. The limit applies equally to all batch members.\n    expansion_param: Scalar positive `Tensor` of real dtype. Must be greater\n      than `1.`. Used to expand the initial interval in case it does not bracket\n      a minimum.\n\n  Returns:\n    A namedtuple with the following fields.\n      iteration: An int32 scalar `Tensor`. The number of iterations performed.\n        Bounded above by `max_iterations` parameter.\n      stopped: A boolean `Tensor` of shape [n]. True for those batch members\n        where the algorithm terminated before reaching `max_iterations`.\n      failed: A boolean `Tensor` of shape [n]. True for those batch members\n        where an error was encountered during bracketing.\n      num_evals: An int32 scalar `Tensor`. The number of times the objective\n        function was evaluated.\n      left: Return value of value_and_gradients_function at the updated left\n        end point of the interval found.\n      right: Return value of value_and_gradients_function at the updated right\n        end point of the interval found.", "docstring_tokens": ["Brackets", "the", "minimum", "given", "an", "initial", "starting", "point", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py#L426-L545", "partition": "test", "index": 999, "time": "2019-02-01 02:04:01"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/numeric.py", "func_name": "clip_by_value_preserve_gradient", "original_string": "def clip_by_value_preserve_gradient(t, clip_value_min, clip_value_max,\n                                    name=None):\n  \"\"\"Clips values to a specified min and max while leaving gradient unaltered.\n\n  Like `tf.clip_by_value`, this function returns a tensor of the same type and\n  shape as input `t` but with values clamped to be no smaller than to\n  `clip_value_min` and no larger than `clip_value_max`. Unlike\n  `tf.clip_by_value`, the gradient is unaffected by this op, i.e.,\n\n  ```python\n  tf.gradients(tfp.math.clip_by_value_preserve_gradient(x), x)[0]\n  # ==> ones_like(x)\n  ```\n\n  Note: `clip_value_min` needs to be smaller or equal to `clip_value_max` for\n  correct results.\n\n  Args:\n    t: A `Tensor`.\n    clip_value_min: A scalar `Tensor`, or a `Tensor` with the same shape\n      as `t`. The minimum value to clip by.\n    clip_value_max: A scalar `Tensor`, or a `Tensor` with the same shape\n      as `t`. The maximum value to clip by.\n    name: A name for the operation (optional).\n      Default value: `'clip_by_value_preserve_gradient'`.\n\n  Returns:\n    clipped_t: A clipped `Tensor`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'clip_by_value_preserve_gradient',\n                               [t, clip_value_min, clip_value_max]):\n    t = tf.convert_to_tensor(value=t, name='t')\n    clip_t = tf.clip_by_value(t, clip_value_min, clip_value_max)\n    return t + tf.stop_gradient(clip_t - t)", "language": "python", "code": "def clip_by_value_preserve_gradient(t, clip_value_min, clip_value_max,\n                                    name=None):\n  \"\"\"Clips values to a specified min and max while leaving gradient unaltered.\n\n  Like `tf.clip_by_value`, this function returns a tensor of the same type and\n  shape as input `t` but with values clamped to be no smaller than to\n  `clip_value_min` and no larger than `clip_value_max`. Unlike\n  `tf.clip_by_value`, the gradient is unaffected by this op, i.e.,\n\n  ```python\n  tf.gradients(tfp.math.clip_by_value_preserve_gradient(x), x)[0]\n  # ==> ones_like(x)\n  ```\n\n  Note: `clip_value_min` needs to be smaller or equal to `clip_value_max` for\n  correct results.\n\n  Args:\n    t: A `Tensor`.\n    clip_value_min: A scalar `Tensor`, or a `Tensor` with the same shape\n      as `t`. The minimum value to clip by.\n    clip_value_max: A scalar `Tensor`, or a `Tensor` with the same shape\n      as `t`. The maximum value to clip by.\n    name: A name for the operation (optional).\n      Default value: `'clip_by_value_preserve_gradient'`.\n\n  Returns:\n    clipped_t: A clipped `Tensor`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'clip_by_value_preserve_gradient',\n                               [t, clip_value_min, clip_value_max]):\n    t = tf.convert_to_tensor(value=t, name='t')\n    clip_t = tf.clip_by_value(t, clip_value_min, clip_value_max)\n    return t + tf.stop_gradient(clip_t - t)", "code_tokens": ["def", "clip_by_value_preserve_gradient", "(", "t", ",", "clip_value_min", ",", "clip_value_max", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'clip_by_value_preserve_gradient'", ",", "[", "t", ",", "clip_value_min", ",", "clip_value_max", "]", ")", ":", "t", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "t", ",", "name", "=", "'t'", ")", "clip_t", "=", "tf", ".", "clip_by_value", "(", "t", ",", "clip_value_min", ",", "clip_value_max", ")", "return", "t", "+", "tf", ".", "stop_gradient", "(", "clip_t", "-", "t", ")"], "docstring": "Clips values to a specified min and max while leaving gradient unaltered.\n\n  Like `tf.clip_by_value`, this function returns a tensor of the same type and\n  shape as input `t` but with values clamped to be no smaller than to\n  `clip_value_min` and no larger than `clip_value_max`. Unlike\n  `tf.clip_by_value`, the gradient is unaffected by this op, i.e.,\n\n  ```python\n  tf.gradients(tfp.math.clip_by_value_preserve_gradient(x), x)[0]\n  # ==> ones_like(x)\n  ```\n\n  Note: `clip_value_min` needs to be smaller or equal to `clip_value_max` for\n  correct results.\n\n  Args:\n    t: A `Tensor`.\n    clip_value_min: A scalar `Tensor`, or a `Tensor` with the same shape\n      as `t`. The minimum value to clip by.\n    clip_value_max: A scalar `Tensor`, or a `Tensor` with the same shape\n      as `t`. The maximum value to clip by.\n    name: A name for the operation (optional).\n      Default value: `'clip_by_value_preserve_gradient'`.\n\n  Returns:\n    clipped_t: A clipped `Tensor`.", "docstring_tokens": ["Clips", "values", "to", "a", "specified", "min", "and", "max", "while", "leaving", "gradient", "unaltered", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/numeric.py#L152-L185", "partition": "test", "index": 1097, "time": "2019-02-01 20:01:49"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/mixture_same_family.py", "func_name": "_value_and_batch_jacobian", "original_string": "def _value_and_batch_jacobian(f, x):\n  \"\"\"Enables uniform interface to value and batch jacobian calculation.\n\n  Works in both eager and graph modes.\n\n  Arguments:\n    f: The scalar function to evaluate.\n    x: The value at which to compute the value and the batch jacobian.\n\n  Returns:\n    A tuple (f(x), J(x)), where J(x) is the batch jacobian.\n  \"\"\"\n  if tf.executing_eagerly():\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      value = f(x)\n    batch_jacobian = tape.batch_jacobian(value, x)\n  else:\n    value = f(x)\n    batch_jacobian = gradients.batch_jacobian(value, x)\n  return value, batch_jacobian", "language": "python", "code": "def _value_and_batch_jacobian(f, x):\n  \"\"\"Enables uniform interface to value and batch jacobian calculation.\n\n  Works in both eager and graph modes.\n\n  Arguments:\n    f: The scalar function to evaluate.\n    x: The value at which to compute the value and the batch jacobian.\n\n  Returns:\n    A tuple (f(x), J(x)), where J(x) is the batch jacobian.\n  \"\"\"\n  if tf.executing_eagerly():\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      value = f(x)\n    batch_jacobian = tape.batch_jacobian(value, x)\n  else:\n    value = f(x)\n    batch_jacobian = gradients.batch_jacobian(value, x)\n  return value, batch_jacobian", "code_tokens": ["def", "_value_and_batch_jacobian", "(", "f", ",", "x", ")", ":", "if", "tf", ".", "executing_eagerly", "(", ")", ":", "with", "tf", ".", "GradientTape", "(", ")", "as", "tape", ":", "tape", ".", "watch", "(", "x", ")", "value", "=", "f", "(", "x", ")", "batch_jacobian", "=", "tape", ".", "batch_jacobian", "(", "value", ",", "x", ")", "else", ":", "value", "=", "f", "(", "x", ")", "batch_jacobian", "=", "gradients", ".", "batch_jacobian", "(", "value", ",", "x", ")", "return", "value", ",", "batch_jacobian"], "docstring": "Enables uniform interface to value and batch jacobian calculation.\n\n  Works in both eager and graph modes.\n\n  Arguments:\n    f: The scalar function to evaluate.\n    x: The value at which to compute the value and the batch jacobian.\n\n  Returns:\n    A tuple (f(x), J(x)), where J(x) is the batch jacobian.", "docstring_tokens": ["Enables", "uniform", "interface", "to", "value", "and", "batch", "jacobian", "calculation", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/mixture_same_family.py#L542-L562", "partition": "test", "index": 757, "time": "2019-02-04 10:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/mixture_same_family.py", "func_name": "_prevent_2nd_derivative", "original_string": "def _prevent_2nd_derivative(x):\n  \"\"\"Disables computation of the second derivatives for a tensor.\n\n  NB: you need to apply a non-identity function to the output tensor for the\n  exception to be raised.\n\n  Arguments:\n    x: A tensor.\n\n  Returns:\n    A tensor with the same value and the same derivative as x, but that raises\n    LookupError when trying to compute the second derivatives.\n  \"\"\"\n  def grad(dy):\n    return array_ops.prevent_gradient(\n        dy, message=\"Second derivative is not implemented.\")\n\n  return tf.identity(x), grad", "language": "python", "code": "def _prevent_2nd_derivative(x):\n  \"\"\"Disables computation of the second derivatives for a tensor.\n\n  NB: you need to apply a non-identity function to the output tensor for the\n  exception to be raised.\n\n  Arguments:\n    x: A tensor.\n\n  Returns:\n    A tensor with the same value and the same derivative as x, but that raises\n    LookupError when trying to compute the second derivatives.\n  \"\"\"\n  def grad(dy):\n    return array_ops.prevent_gradient(\n        dy, message=\"Second derivative is not implemented.\")\n\n  return tf.identity(x), grad", "code_tokens": ["def", "_prevent_2nd_derivative", "(", "x", ")", ":", "def", "grad", "(", "dy", ")", ":", "return", "array_ops", ".", "prevent_gradient", "(", "dy", ",", "message", "=", "\"Second derivative is not implemented.\"", ")", "return", "tf", ".", "identity", "(", "x", ")", ",", "grad"], "docstring": "Disables computation of the second derivatives for a tensor.\n\n  NB: you need to apply a non-identity function to the output tensor for the\n  exception to be raised.\n\n  Arguments:\n    x: A tensor.\n\n  Returns:\n    A tensor with the same value and the same derivative as x, but that raises\n    LookupError when trying to compute the second derivatives.", "docstring_tokens": ["Disables", "computation", "of", "the", "second", "derivatives", "for", "a", "tensor", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/mixture_same_family.py#L566-L583", "partition": "test", "index": 758, "time": "2019-02-04 10:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/mixture_same_family.py", "func_name": "MixtureSameFamily._distributional_transform", "original_string": "def _distributional_transform(self, x):\n    \"\"\"Performs distributional transform of the mixture samples.\n\n    Distributional transform removes the parameters from samples of a\n    multivariate distribution by applying conditional CDFs:\n      (F(x_1), F(x_2 | x1_), ..., F(x_d | x_1, ..., x_d-1))\n    (the indexing is over the \"flattened\" event dimensions).\n    The result is a sample of product of Uniform[0, 1] distributions.\n\n    We assume that the components are factorized, so the conditional CDFs become\n      F(x_i | x_1, ..., x_i-1) = sum_k w_i^k F_k (x_i),\n    where w_i^k is the posterior mixture weight: for i > 0\n      w_i^k = w_k prob_k(x_1, ..., x_i-1) / sum_k' w_k' prob_k'(x_1, ..., x_i-1)\n    and w_0^k = w_k is the mixture probability of the k-th component.\n\n    Arguments:\n      x: Sample of mixture distribution\n\n    Returns:\n      Result of the distributional transform\n    \"\"\"\n\n    if tensorshape_util.rank(x.shape) is None:\n      # tf.nn.softmax raises an error when applied to inputs of undefined rank.\n      raise ValueError(\"Distributional transform does not support inputs of \"\n                       \"undefined rank.\")\n\n    # Obtain factorized components distribution and assert that it's\n    # a scalar distribution.\n    if isinstance(self._components_distribution, independent.Independent):\n      univariate_components = self._components_distribution.distribution\n    else:\n      univariate_components = self._components_distribution\n\n    with tf.control_dependencies([\n        assert_util.assert_equal(\n            univariate_components.is_scalar_event(),\n            True,\n            message=\"`univariate_components` must have scalar event\")\n    ]):\n      x_padded = self._pad_sample_dims(x)  # [S, B, 1, E]\n      log_prob_x = univariate_components.log_prob(x_padded)  # [S, B, k, E]\n      cdf_x = univariate_components.cdf(x_padded)  # [S, B, k, E]\n\n      # log prob_k (x_1, ..., x_i-1)\n      cumsum_log_prob_x = tf.reshape(\n          tf.math.cumsum(\n              # [S*prod(B)*k, prod(E)]\n              tf.reshape(log_prob_x, [-1, self._event_size]),\n              exclusive=True,\n              axis=-1),\n          tf.shape(input=log_prob_x))  # [S, B, k, E]\n\n      logits_mix_prob = distribution_utils.pad_mixture_dimensions(\n          self.mixture_distribution.logits, self, self.mixture_distribution,\n          self._event_ndims)  # [B, k, 1]\n\n      # Logits of the posterior weights: log w_k + log prob_k (x_1, ..., x_i-1)\n      log_posterior_weights_x = logits_mix_prob + cumsum_log_prob_x\n\n      component_axis = tensorshape_util.rank(x.shape) - self._event_ndims\n      posterior_weights_x = tf.nn.softmax(log_posterior_weights_x,\n                                          axis=component_axis)\n      return tf.reduce_sum(\n          input_tensor=posterior_weights_x * cdf_x, axis=component_axis)", "language": "python", "code": "def _distributional_transform(self, x):\n    \"\"\"Performs distributional transform of the mixture samples.\n\n    Distributional transform removes the parameters from samples of a\n    multivariate distribution by applying conditional CDFs:\n      (F(x_1), F(x_2 | x1_), ..., F(x_d | x_1, ..., x_d-1))\n    (the indexing is over the \"flattened\" event dimensions).\n    The result is a sample of product of Uniform[0, 1] distributions.\n\n    We assume that the components are factorized, so the conditional CDFs become\n      F(x_i | x_1, ..., x_i-1) = sum_k w_i^k F_k (x_i),\n    where w_i^k is the posterior mixture weight: for i > 0\n      w_i^k = w_k prob_k(x_1, ..., x_i-1) / sum_k' w_k' prob_k'(x_1, ..., x_i-1)\n    and w_0^k = w_k is the mixture probability of the k-th component.\n\n    Arguments:\n      x: Sample of mixture distribution\n\n    Returns:\n      Result of the distributional transform\n    \"\"\"\n\n    if tensorshape_util.rank(x.shape) is None:\n      # tf.nn.softmax raises an error when applied to inputs of undefined rank.\n      raise ValueError(\"Distributional transform does not support inputs of \"\n                       \"undefined rank.\")\n\n    # Obtain factorized components distribution and assert that it's\n    # a scalar distribution.\n    if isinstance(self._components_distribution, independent.Independent):\n      univariate_components = self._components_distribution.distribution\n    else:\n      univariate_components = self._components_distribution\n\n    with tf.control_dependencies([\n        assert_util.assert_equal(\n            univariate_components.is_scalar_event(),\n            True,\n            message=\"`univariate_components` must have scalar event\")\n    ]):\n      x_padded = self._pad_sample_dims(x)  # [S, B, 1, E]\n      log_prob_x = univariate_components.log_prob(x_padded)  # [S, B, k, E]\n      cdf_x = univariate_components.cdf(x_padded)  # [S, B, k, E]\n\n      # log prob_k (x_1, ..., x_i-1)\n      cumsum_log_prob_x = tf.reshape(\n          tf.math.cumsum(\n              # [S*prod(B)*k, prod(E)]\n              tf.reshape(log_prob_x, [-1, self._event_size]),\n              exclusive=True,\n              axis=-1),\n          tf.shape(input=log_prob_x))  # [S, B, k, E]\n\n      logits_mix_prob = distribution_utils.pad_mixture_dimensions(\n          self.mixture_distribution.logits, self, self.mixture_distribution,\n          self._event_ndims)  # [B, k, 1]\n\n      # Logits of the posterior weights: log w_k + log prob_k (x_1, ..., x_i-1)\n      log_posterior_weights_x = logits_mix_prob + cumsum_log_prob_x\n\n      component_axis = tensorshape_util.rank(x.shape) - self._event_ndims\n      posterior_weights_x = tf.nn.softmax(log_posterior_weights_x,\n                                          axis=component_axis)\n      return tf.reduce_sum(\n          input_tensor=posterior_weights_x * cdf_x, axis=component_axis)", "code_tokens": ["def", "_distributional_transform", "(", "self", ",", "x", ")", ":", "if", "tensorshape_util", ".", "rank", "(", "x", ".", "shape", ")", "is", "None", ":", "# tf.nn.softmax raises an error when applied to inputs of undefined rank.", "raise", "ValueError", "(", "\"Distributional transform does not support inputs of \"", "\"undefined rank.\"", ")", "# Obtain factorized components distribution and assert that it's", "# a scalar distribution.", "if", "isinstance", "(", "self", ".", "_components_distribution", ",", "independent", ".", "Independent", ")", ":", "univariate_components", "=", "self", ".", "_components_distribution", ".", "distribution", "else", ":", "univariate_components", "=", "self", ".", "_components_distribution", "with", "tf", ".", "control_dependencies", "(", "[", "assert_util", ".", "assert_equal", "(", "univariate_components", ".", "is_scalar_event", "(", ")", ",", "True", ",", "message", "=", "\"`univariate_components` must have scalar event\"", ")", "]", ")", ":", "x_padded", "=", "self", ".", "_pad_sample_dims", "(", "x", ")", "# [S, B, 1, E]", "log_prob_x", "=", "univariate_components", ".", "log_prob", "(", "x_padded", ")", "# [S, B, k, E]", "cdf_x", "=", "univariate_components", ".", "cdf", "(", "x_padded", ")", "# [S, B, k, E]", "# log prob_k (x_1, ..., x_i-1)", "cumsum_log_prob_x", "=", "tf", ".", "reshape", "(", "tf", ".", "math", ".", "cumsum", "(", "# [S*prod(B)*k, prod(E)]", "tf", ".", "reshape", "(", "log_prob_x", ",", "[", "-", "1", ",", "self", ".", "_event_size", "]", ")", ",", "exclusive", "=", "True", ",", "axis", "=", "-", "1", ")", ",", "tf", ".", "shape", "(", "input", "=", "log_prob_x", ")", ")", "# [S, B, k, E]", "logits_mix_prob", "=", "distribution_utils", ".", "pad_mixture_dimensions", "(", "self", ".", "mixture_distribution", ".", "logits", ",", "self", ",", "self", ".", "mixture_distribution", ",", "self", ".", "_event_ndims", ")", "# [B, k, 1]", "# Logits of the posterior weights: log w_k + log prob_k (x_1, ..., x_i-1)", "log_posterior_weights_x", "=", "logits_mix_prob", "+", "cumsum_log_prob_x", "component_axis", "=", "tensorshape_util", ".", "rank", "(", "x", ".", "shape", ")", "-", "self", ".", "_event_ndims", "posterior_weights_x", "=", "tf", ".", "nn", ".", "softmax", "(", "log_posterior_weights_x", ",", "axis", "=", "component_axis", ")", "return", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "posterior_weights_x", "*", "cdf_x", ",", "axis", "=", "component_axis", ")"], "docstring": "Performs distributional transform of the mixture samples.\n\n    Distributional transform removes the parameters from samples of a\n    multivariate distribution by applying conditional CDFs:\n      (F(x_1), F(x_2 | x1_), ..., F(x_d | x_1, ..., x_d-1))\n    (the indexing is over the \"flattened\" event dimensions).\n    The result is a sample of product of Uniform[0, 1] distributions.\n\n    We assume that the components are factorized, so the conditional CDFs become\n      F(x_i | x_1, ..., x_i-1) = sum_k w_i^k F_k (x_i),\n    where w_i^k is the posterior mixture weight: for i > 0\n      w_i^k = w_k prob_k(x_1, ..., x_i-1) / sum_k' w_k' prob_k'(x_1, ..., x_i-1)\n    and w_0^k = w_k is the mixture probability of the k-th component.\n\n    Arguments:\n      x: Sample of mixture distribution\n\n    Returns:\n      Result of the distributional transform", "docstring_tokens": ["Performs", "distributional", "transform", "of", "the", "mixture", "samples", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/mixture_same_family.py#L469-L533", "partition": "test", "index": 759, "time": "2019-02-04 10:32:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/multinomial.py", "func_name": "draw_sample", "original_string": "def draw_sample(num_samples, num_classes, logits, num_trials, dtype, seed):\n  \"\"\"Sample a multinomial.\n\n  The batch shape is given by broadcasting num_trials with\n  remove_last_dimension(logits).\n\n  Args:\n    num_samples: Python int or singleton integer Tensor: number of multinomial\n      samples to draw.\n    num_classes: Python int or singleton integer Tensor: number of classes.\n    logits: Floating Tensor with last dimension k, of (unnormalized) logit\n      probabilities per class.\n    num_trials: Tensor of number of categorical trials each multinomial consists\n      of.  num_trials[..., tf.newaxis] must broadcast with logits.\n    dtype: dtype at which to emit samples.\n    seed: Random seed.\n\n  Returns:\n    samples: Tensor of given dtype and shape [n] + batch_shape + [k].\n  \"\"\"\n  with tf.name_scope(\"multinomial.draw_sample\"):\n    # broadcast the num_trials and logits to same shape\n    num_trials = tf.ones_like(\n        logits[..., 0], dtype=num_trials.dtype) * num_trials\n    logits = tf.ones_like(\n        num_trials[..., tf.newaxis], dtype=logits.dtype) * logits\n\n    # flatten the total_count and logits\n    # flat_logits has shape [B1B2...Bm, num_classes]\n    flat_logits = tf.reshape(logits, [-1, num_classes])\n    flat_num_trials = num_samples * tf.reshape(num_trials, [-1])  # [B1B2...Bm]\n\n    # Computes each logits and num_trials situation by map_fn.\n\n    # Using just one batch tf.random.categorical call doesn't work because that\n    # requires num_trials to be the same across all members of the batch of\n    # logits.  This restriction makes sense for tf.random.categorical because\n    # for it, num_trials is part of the returned shape.  However, the\n    # multinomial sampler does not need that restriction, because it sums out\n    # exactly that dimension.\n\n    # One possibility would be to draw a batch categorical whose sample count is\n    # max(num_trials) and mask out the excess ones.  However, if the elements of\n    # num_trials vary widely, this can be wasteful of memory.\n\n    # TODO(b/123763054, b/112152209): Revisit the possibility of writing this\n    # with a batch categorical followed by batch unsorted_segment_sum, once both\n    # of those work and are memory-efficient enough.\n    def _sample_one_batch_member(args):\n      logits, num_cat_samples = args[0], args[1]  # [K], []\n      # x has shape [1, num_cat_samples = num_samples * num_trials]\n      x = tf.random.categorical(\n          logits[tf.newaxis, ...], num_cat_samples, seed=seed)\n      x = tf.reshape(x, shape=[num_samples, -1])  # [num_samples, num_trials]\n      x = tf.one_hot(\n          x, depth=num_classes)  # [num_samples, num_trials, num_classes]\n      x = tf.reduce_sum(input_tensor=x, axis=-2)  # [num_samples, num_classes]\n      return tf.cast(x, dtype=dtype)\n\n    x = tf.map_fn(\n        _sample_one_batch_member, [flat_logits, flat_num_trials],\n        dtype=dtype)  # [B1B2...Bm, num_samples, num_classes]\n\n    # reshape the results to proper shape\n    x = tf.transpose(a=x, perm=[1, 0, 2])\n    final_shape = tf.concat([[num_samples],\n                             tf.shape(input=num_trials), [num_classes]],\n                            axis=0)\n    x = tf.reshape(x, final_shape)\n\n    return x", "language": "python", "code": "def draw_sample(num_samples, num_classes, logits, num_trials, dtype, seed):\n  \"\"\"Sample a multinomial.\n\n  The batch shape is given by broadcasting num_trials with\n  remove_last_dimension(logits).\n\n  Args:\n    num_samples: Python int or singleton integer Tensor: number of multinomial\n      samples to draw.\n    num_classes: Python int or singleton integer Tensor: number of classes.\n    logits: Floating Tensor with last dimension k, of (unnormalized) logit\n      probabilities per class.\n    num_trials: Tensor of number of categorical trials each multinomial consists\n      of.  num_trials[..., tf.newaxis] must broadcast with logits.\n    dtype: dtype at which to emit samples.\n    seed: Random seed.\n\n  Returns:\n    samples: Tensor of given dtype and shape [n] + batch_shape + [k].\n  \"\"\"\n  with tf.name_scope(\"multinomial.draw_sample\"):\n    # broadcast the num_trials and logits to same shape\n    num_trials = tf.ones_like(\n        logits[..., 0], dtype=num_trials.dtype) * num_trials\n    logits = tf.ones_like(\n        num_trials[..., tf.newaxis], dtype=logits.dtype) * logits\n\n    # flatten the total_count and logits\n    # flat_logits has shape [B1B2...Bm, num_classes]\n    flat_logits = tf.reshape(logits, [-1, num_classes])\n    flat_num_trials = num_samples * tf.reshape(num_trials, [-1])  # [B1B2...Bm]\n\n    # Computes each logits and num_trials situation by map_fn.\n\n    # Using just one batch tf.random.categorical call doesn't work because that\n    # requires num_trials to be the same across all members of the batch of\n    # logits.  This restriction makes sense for tf.random.categorical because\n    # for it, num_trials is part of the returned shape.  However, the\n    # multinomial sampler does not need that restriction, because it sums out\n    # exactly that dimension.\n\n    # One possibility would be to draw a batch categorical whose sample count is\n    # max(num_trials) and mask out the excess ones.  However, if the elements of\n    # num_trials vary widely, this can be wasteful of memory.\n\n    # TODO(b/123763054, b/112152209): Revisit the possibility of writing this\n    # with a batch categorical followed by batch unsorted_segment_sum, once both\n    # of those work and are memory-efficient enough.\n    def _sample_one_batch_member(args):\n      logits, num_cat_samples = args[0], args[1]  # [K], []\n      # x has shape [1, num_cat_samples = num_samples * num_trials]\n      x = tf.random.categorical(\n          logits[tf.newaxis, ...], num_cat_samples, seed=seed)\n      x = tf.reshape(x, shape=[num_samples, -1])  # [num_samples, num_trials]\n      x = tf.one_hot(\n          x, depth=num_classes)  # [num_samples, num_trials, num_classes]\n      x = tf.reduce_sum(input_tensor=x, axis=-2)  # [num_samples, num_classes]\n      return tf.cast(x, dtype=dtype)\n\n    x = tf.map_fn(\n        _sample_one_batch_member, [flat_logits, flat_num_trials],\n        dtype=dtype)  # [B1B2...Bm, num_samples, num_classes]\n\n    # reshape the results to proper shape\n    x = tf.transpose(a=x, perm=[1, 0, 2])\n    final_shape = tf.concat([[num_samples],\n                             tf.shape(input=num_trials), [num_classes]],\n                            axis=0)\n    x = tf.reshape(x, final_shape)\n\n    return x", "code_tokens": ["def", "draw_sample", "(", "num_samples", ",", "num_classes", ",", "logits", ",", "num_trials", ",", "dtype", ",", "seed", ")", ":", "with", "tf", ".", "name_scope", "(", "\"multinomial.draw_sample\"", ")", ":", "# broadcast the num_trials and logits to same shape", "num_trials", "=", "tf", ".", "ones_like", "(", "logits", "[", "...", ",", "0", "]", ",", "dtype", "=", "num_trials", ".", "dtype", ")", "*", "num_trials", "logits", "=", "tf", ".", "ones_like", "(", "num_trials", "[", "...", ",", "tf", ".", "newaxis", "]", ",", "dtype", "=", "logits", ".", "dtype", ")", "*", "logits", "# flatten the total_count and logits", "# flat_logits has shape [B1B2...Bm, num_classes]", "flat_logits", "=", "tf", ".", "reshape", "(", "logits", ",", "[", "-", "1", ",", "num_classes", "]", ")", "flat_num_trials", "=", "num_samples", "*", "tf", ".", "reshape", "(", "num_trials", ",", "[", "-", "1", "]", ")", "# [B1B2...Bm]", "# Computes each logits and num_trials situation by map_fn.", "# Using just one batch tf.random.categorical call doesn't work because that", "# requires num_trials to be the same across all members of the batch of", "# logits.  This restriction makes sense for tf.random.categorical because", "# for it, num_trials is part of the returned shape.  However, the", "# multinomial sampler does not need that restriction, because it sums out", "# exactly that dimension.", "# One possibility would be to draw a batch categorical whose sample count is", "# max(num_trials) and mask out the excess ones.  However, if the elements of", "# num_trials vary widely, this can be wasteful of memory.", "# TODO(b/123763054, b/112152209): Revisit the possibility of writing this", "# with a batch categorical followed by batch unsorted_segment_sum, once both", "# of those work and are memory-efficient enough.", "def", "_sample_one_batch_member", "(", "args", ")", ":", "logits", ",", "num_cat_samples", "=", "args", "[", "0", "]", ",", "args", "[", "1", "]", "# [K], []", "# x has shape [1, num_cat_samples = num_samples * num_trials]", "x", "=", "tf", ".", "random", ".", "categorical", "(", "logits", "[", "tf", ".", "newaxis", ",", "...", "]", ",", "num_cat_samples", ",", "seed", "=", "seed", ")", "x", "=", "tf", ".", "reshape", "(", "x", ",", "shape", "=", "[", "num_samples", ",", "-", "1", "]", ")", "# [num_samples, num_trials]", "x", "=", "tf", ".", "one_hot", "(", "x", ",", "depth", "=", "num_classes", ")", "# [num_samples, num_trials, num_classes]", "x", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "x", ",", "axis", "=", "-", "2", ")", "# [num_samples, num_classes]", "return", "tf", ".", "cast", "(", "x", ",", "dtype", "=", "dtype", ")", "x", "=", "tf", ".", "map_fn", "(", "_sample_one_batch_member", ",", "[", "flat_logits", ",", "flat_num_trials", "]", ",", "dtype", "=", "dtype", ")", "# [B1B2...Bm, num_samples, num_classes]", "# reshape the results to proper shape", "x", "=", "tf", ".", "transpose", "(", "a", "=", "x", ",", "perm", "=", "[", "1", ",", "0", ",", "2", "]", ")", "final_shape", "=", "tf", ".", "concat", "(", "[", "[", "num_samples", "]", ",", "tf", ".", "shape", "(", "input", "=", "num_trials", ")", ",", "[", "num_classes", "]", "]", ",", "axis", "=", "0", ")", "x", "=", "tf", ".", "reshape", "(", "x", ",", "final_shape", ")", "return", "x"], "docstring": "Sample a multinomial.\n\n  The batch shape is given by broadcasting num_trials with\n  remove_last_dimension(logits).\n\n  Args:\n    num_samples: Python int or singleton integer Tensor: number of multinomial\n      samples to draw.\n    num_classes: Python int or singleton integer Tensor: number of classes.\n    logits: Floating Tensor with last dimension k, of (unnormalized) logit\n      probabilities per class.\n    num_trials: Tensor of number of categorical trials each multinomial consists\n      of.  num_trials[..., tf.newaxis] must broadcast with logits.\n    dtype: dtype at which to emit samples.\n    seed: Random seed.\n\n  Returns:\n    samples: Tensor of given dtype and shape [n] + batch_shape + [k].", "docstring_tokens": ["Sample", "a", "multinomial", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/multinomial.py#L285-L355", "partition": "test", "index": 1124, "time": "2019-02-05 09:59:01"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/blockwise.py", "func_name": "_validate_block_sizes", "original_string": "def _validate_block_sizes(block_sizes, bijectors, validate_args):\n  \"\"\"Helper to validate block sizes.\"\"\"\n  block_sizes_shape = block_sizes.shape\n  if tensorshape_util.is_fully_defined(block_sizes_shape):\n    if (tensorshape_util.rank(block_sizes_shape) != 1 or\n        (tensorshape_util.num_elements(block_sizes_shape) != len(bijectors))):\n      raise ValueError(\n          '`block_sizes` must be `None`, or a vector of the same length as '\n          '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of '\n          'length {}'.format(block_sizes_shape, len(bijectors)))\n    return block_sizes\n  elif validate_args:\n    message = ('`block_sizes` must be `None`, or a vector of the same length '\n               'as `bijectors`.')\n    with tf.control_dependencies([\n        assert_util.assert_equal(\n            tf.size(input=block_sizes), len(bijectors), message=message),\n        assert_util.assert_equal(tf.rank(block_sizes), 1)\n    ]):\n      return tf.identity(block_sizes)\n  else:\n    return block_sizes", "language": "python", "code": "def _validate_block_sizes(block_sizes, bijectors, validate_args):\n  \"\"\"Helper to validate block sizes.\"\"\"\n  block_sizes_shape = block_sizes.shape\n  if tensorshape_util.is_fully_defined(block_sizes_shape):\n    if (tensorshape_util.rank(block_sizes_shape) != 1 or\n        (tensorshape_util.num_elements(block_sizes_shape) != len(bijectors))):\n      raise ValueError(\n          '`block_sizes` must be `None`, or a vector of the same length as '\n          '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of '\n          'length {}'.format(block_sizes_shape, len(bijectors)))\n    return block_sizes\n  elif validate_args:\n    message = ('`block_sizes` must be `None`, or a vector of the same length '\n               'as `bijectors`.')\n    with tf.control_dependencies([\n        assert_util.assert_equal(\n            tf.size(input=block_sizes), len(bijectors), message=message),\n        assert_util.assert_equal(tf.rank(block_sizes), 1)\n    ]):\n      return tf.identity(block_sizes)\n  else:\n    return block_sizes", "code_tokens": ["def", "_validate_block_sizes", "(", "block_sizes", ",", "bijectors", ",", "validate_args", ")", ":", "block_sizes_shape", "=", "block_sizes", ".", "shape", "if", "tensorshape_util", ".", "is_fully_defined", "(", "block_sizes_shape", ")", ":", "if", "(", "tensorshape_util", ".", "rank", "(", "block_sizes_shape", ")", "!=", "1", "or", "(", "tensorshape_util", ".", "num_elements", "(", "block_sizes_shape", ")", "!=", "len", "(", "bijectors", ")", ")", ")", ":", "raise", "ValueError", "(", "'`block_sizes` must be `None`, or a vector of the same length as '", "'`bijectors`. Got a `Tensor` with shape {} and `bijectors` of '", "'length {}'", ".", "format", "(", "block_sizes_shape", ",", "len", "(", "bijectors", ")", ")", ")", "return", "block_sizes", "elif", "validate_args", ":", "message", "=", "(", "'`block_sizes` must be `None`, or a vector of the same length '", "'as `bijectors`.'", ")", "with", "tf", ".", "control_dependencies", "(", "[", "assert_util", ".", "assert_equal", "(", "tf", ".", "size", "(", "input", "=", "block_sizes", ")", ",", "len", "(", "bijectors", ")", ",", "message", "=", "message", ")", ",", "assert_util", ".", "assert_equal", "(", "tf", ".", "rank", "(", "block_sizes", ")", ",", "1", ")", "]", ")", ":", "return", "tf", ".", "identity", "(", "block_sizes", ")", "else", ":", "return", "block_sizes"], "docstring": "Helper to validate block sizes.", "docstring_tokens": ["Helper", "to", "validate", "block", "sizes", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/blockwise.py#L151-L172", "partition": "test", "index": 821, "time": "2019-02-06 15:17:46"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py", "func_name": "val_where", "original_string": "def val_where(cond, tval, fval):\n  \"\"\"Like tf.where but works on namedtuples.\"\"\"\n  if isinstance(tval, tf.Tensor):\n    return tf.where(cond, tval, fval)\n  elif isinstance(tval, tuple):\n    cls = type(tval)\n    return cls(*(val_where(cond, t, f) for t, f in zip(tval, fval)))\n  else:\n    raise Exception(TypeError)", "language": "python", "code": "def val_where(cond, tval, fval):\n  \"\"\"Like tf.where but works on namedtuples.\"\"\"\n  if isinstance(tval, tf.Tensor):\n    return tf.where(cond, tval, fval)\n  elif isinstance(tval, tuple):\n    cls = type(tval)\n    return cls(*(val_where(cond, t, f) for t, f in zip(tval, fval)))\n  else:\n    raise Exception(TypeError)", "code_tokens": ["def", "val_where", "(", "cond", ",", "tval", ",", "fval", ")", ":", "if", "isinstance", "(", "tval", ",", "tf", ".", "Tensor", ")", ":", "return", "tf", ".", "where", "(", "cond", ",", "tval", ",", "fval", ")", "elif", "isinstance", "(", "tval", ",", "tuple", ")", ":", "cls", "=", "type", "(", "tval", ")", "return", "cls", "(", "*", "(", "val_where", "(", "cond", ",", "t", ",", "f", ")", "for", "t", ",", "f", "in", "zip", "(", "tval", ",", "fval", ")", ")", ")", "else", ":", "raise", "Exception", "(", "TypeError", ")"], "docstring": "Like tf.where but works on namedtuples.", "docstring_tokens": ["Like", "tf", ".", "where", "but", "works", "on", "namedtuples", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py#L39-L47", "partition": "test", "index": 994, "time": "2019-02-07 09:39:48"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py", "func_name": "update", "original_string": "def update(value_and_gradients_function, val_left, val_right, val_trial, f_lim,\n           active=None):\n  \"\"\"Squeezes a bracketing interval containing the minimum.\n\n  Given an interval which brackets a minimum and a point in that interval,\n  finds a smaller nested interval which also brackets the minimum. If the\n  supplied point does not lie in the bracketing interval, the current interval\n  is returned.\n\n  The following description is given in terms of individual points evaluated on\n  a line function to be minimized. Note, however, the implementation also\n  accepts batches of points allowing to minimize multiple line functions at\n  once. See details on the docstring of `value_and_gradients_function` below.\n\n  The requirement of the interval bracketing a minimum is expressed through the\n  opposite slope conditions. Assume the left end point is 'a', the right\n  end point is 'b', the function to be minimized is 'f' and the derivative is\n  'df'. The update procedure relies on the following conditions being satisfied:\n\n  '''\n    f(a) <= f(0) + epsilon   (1)\n    df(a) < 0                (2)\n    df(b) > 0                (3)\n  '''\n\n  In the first condition, epsilon is a small positive constant. The condition\n  demands that the function at the left end point be not much bigger than the\n  starting point (i.e. 0). This is an easy to satisfy condition because by\n  assumption, we are in a direction where the function value is decreasing.\n  The second and third conditions together demand that there is at least one\n  zero of the derivative in between a and b.\n\n  In addition to the interval, the update algorithm requires a third point to\n  be supplied. Usually, this point would lie within the interval [a, b]. If the\n  point is outside this interval, the current interval is returned. If the\n  point lies within the interval, the behaviour of the function and derivative\n  value at this point is used to squeeze the original interval in a manner that\n  preserves the opposite slope conditions.\n\n  For further details of this component, see the procedure U0-U3 on page 123 of\n  the [Hager and Zhang (2006)][2] article.\n\n  Note that this function does not explicitly verify whether the opposite slope\n  conditions are satisfied for the supplied interval. It is assumed that this\n  is so.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns an object that can be converted to a namedtuple.\n      The namedtuple should have fields 'f' and 'df' that correspond to scalar\n      tensors of real dtype containing the value of the function and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      function values and derivatives at the input points.\n    val_left: Return value of value_and_gradients_function at the left\n      end point of the bracketing interval (labelles 'a' above).\n    val_right: Return value of value_and_gradients_function at the right\n      end point of the bracketing interval (labelles 'b' above).\n    val_trial: Return value of value_and_gradients_function at the trial point\n      to be used to shrink the interval (labelled 'c' above).\n    f_lim: real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked for each batch member.\n    active: optional boolean `Tensor` of shape [n]. Relevant in batching mode\n      only, indicates batch members on which the update procedure should be\n      applied. On non-active members the current left/right interval is returned\n      unmodified.\n\n  Returns:\n    A namedtuple containing the following fields:\n      iteration: An int32 scalar `Tensor`. The number of iterations performed\n        by the bisect algorithm.\n      stopped: A boolean `Tensor` of shape [n]. True for those batch members\n        where the bisection algorithm terminated.\n      failed: A boolean `Tensor` of shape [n]. True for those batch members\n        where an error was encountered.\n      num_evals: An int32 scalar `Tensor`. The number of times the objective\n        function was evaluated.\n      left: Return value of value_and_gradients_function at the updated left\n        end point of the interval found.\n      right: Return value of value_and_gradients_function at the updated right\n        end point of the interval found.\n  \"\"\"\n  # We should only update if the trial point is within the interval.\n  within_range = (val_left.x < val_trial.x) & (val_trial.x < val_right.x)\n  if active is not None:\n    within_range = within_range & active\n\n  # The new point is a valid left end point if it has negative slope\n  # and the value at the point is not too large.\n  valid_left = (val_trial.df < 0) & (val_trial.f <= f_lim)\n\n  # If the trial point has a negative slope but the value at that point\n  # is too high, bisect can narrow down an interval between the current left\n  # and the trial point.\n  needs_bisect = within_range & (val_trial.df < 0) & (val_trial.f > f_lim)\n\n  # Note that if `~valid_left` it is because either:\n  # - the slope at the trial point is positive, so it is a valid right\n  #   point, or\n  # - the needs_bisect condition is true.\n  # In both cases we want to keep the current left and replace right\n  # with the trial point.\n  left = val_where(within_range & valid_left, val_trial, val_left)\n  right = val_where(within_range & ~valid_left, val_trial, val_right)\n\n  bisect_args = _IntermediateResult(\n      iteration=tf.convert_to_tensor(value=0),\n      stopped=~needs_bisect,\n      failed=tf.zeros_like(within_range),  # i.e. all false.\n      num_evals=tf.convert_to_tensor(value=0),\n      left=left,\n      right=right)\n  return _bisect(value_and_gradients_function, bisect_args, f_lim)", "language": "python", "code": "def update(value_and_gradients_function, val_left, val_right, val_trial, f_lim,\n           active=None):\n  \"\"\"Squeezes a bracketing interval containing the minimum.\n\n  Given an interval which brackets a minimum and a point in that interval,\n  finds a smaller nested interval which also brackets the minimum. If the\n  supplied point does not lie in the bracketing interval, the current interval\n  is returned.\n\n  The following description is given in terms of individual points evaluated on\n  a line function to be minimized. Note, however, the implementation also\n  accepts batches of points allowing to minimize multiple line functions at\n  once. See details on the docstring of `value_and_gradients_function` below.\n\n  The requirement of the interval bracketing a minimum is expressed through the\n  opposite slope conditions. Assume the left end point is 'a', the right\n  end point is 'b', the function to be minimized is 'f' and the derivative is\n  'df'. The update procedure relies on the following conditions being satisfied:\n\n  '''\n    f(a) <= f(0) + epsilon   (1)\n    df(a) < 0                (2)\n    df(b) > 0                (3)\n  '''\n\n  In the first condition, epsilon is a small positive constant. The condition\n  demands that the function at the left end point be not much bigger than the\n  starting point (i.e. 0). This is an easy to satisfy condition because by\n  assumption, we are in a direction where the function value is decreasing.\n  The second and third conditions together demand that there is at least one\n  zero of the derivative in between a and b.\n\n  In addition to the interval, the update algorithm requires a third point to\n  be supplied. Usually, this point would lie within the interval [a, b]. If the\n  point is outside this interval, the current interval is returned. If the\n  point lies within the interval, the behaviour of the function and derivative\n  value at this point is used to squeeze the original interval in a manner that\n  preserves the opposite slope conditions.\n\n  For further details of this component, see the procedure U0-U3 on page 123 of\n  the [Hager and Zhang (2006)][2] article.\n\n  Note that this function does not explicitly verify whether the opposite slope\n  conditions are satisfied for the supplied interval. It is assumed that this\n  is so.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns an object that can be converted to a namedtuple.\n      The namedtuple should have fields 'f' and 'df' that correspond to scalar\n      tensors of real dtype containing the value of the function and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      function values and derivatives at the input points.\n    val_left: Return value of value_and_gradients_function at the left\n      end point of the bracketing interval (labelles 'a' above).\n    val_right: Return value of value_and_gradients_function at the right\n      end point of the bracketing interval (labelles 'b' above).\n    val_trial: Return value of value_and_gradients_function at the trial point\n      to be used to shrink the interval (labelled 'c' above).\n    f_lim: real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked for each batch member.\n    active: optional boolean `Tensor` of shape [n]. Relevant in batching mode\n      only, indicates batch members on which the update procedure should be\n      applied. On non-active members the current left/right interval is returned\n      unmodified.\n\n  Returns:\n    A namedtuple containing the following fields:\n      iteration: An int32 scalar `Tensor`. The number of iterations performed\n        by the bisect algorithm.\n      stopped: A boolean `Tensor` of shape [n]. True for those batch members\n        where the bisection algorithm terminated.\n      failed: A boolean `Tensor` of shape [n]. True for those batch members\n        where an error was encountered.\n      num_evals: An int32 scalar `Tensor`. The number of times the objective\n        function was evaluated.\n      left: Return value of value_and_gradients_function at the updated left\n        end point of the interval found.\n      right: Return value of value_and_gradients_function at the updated right\n        end point of the interval found.\n  \"\"\"\n  # We should only update if the trial point is within the interval.\n  within_range = (val_left.x < val_trial.x) & (val_trial.x < val_right.x)\n  if active is not None:\n    within_range = within_range & active\n\n  # The new point is a valid left end point if it has negative slope\n  # and the value at the point is not too large.\n  valid_left = (val_trial.df < 0) & (val_trial.f <= f_lim)\n\n  # If the trial point has a negative slope but the value at that point\n  # is too high, bisect can narrow down an interval between the current left\n  # and the trial point.\n  needs_bisect = within_range & (val_trial.df < 0) & (val_trial.f > f_lim)\n\n  # Note that if `~valid_left` it is because either:\n  # - the slope at the trial point is positive, so it is a valid right\n  #   point, or\n  # - the needs_bisect condition is true.\n  # In both cases we want to keep the current left and replace right\n  # with the trial point.\n  left = val_where(within_range & valid_left, val_trial, val_left)\n  right = val_where(within_range & ~valid_left, val_trial, val_right)\n\n  bisect_args = _IntermediateResult(\n      iteration=tf.convert_to_tensor(value=0),\n      stopped=~needs_bisect,\n      failed=tf.zeros_like(within_range),  # i.e. all false.\n      num_evals=tf.convert_to_tensor(value=0),\n      left=left,\n      right=right)\n  return _bisect(value_and_gradients_function, bisect_args, f_lim)", "code_tokens": ["def", "update", "(", "value_and_gradients_function", ",", "val_left", ",", "val_right", ",", "val_trial", ",", "f_lim", ",", "active", "=", "None", ")", ":", "# We should only update if the trial point is within the interval.", "within_range", "=", "(", "val_left", ".", "x", "<", "val_trial", ".", "x", ")", "&", "(", "val_trial", ".", "x", "<", "val_right", ".", "x", ")", "if", "active", "is", "not", "None", ":", "within_range", "=", "within_range", "&", "active", "# The new point is a valid left end point if it has negative slope", "# and the value at the point is not too large.", "valid_left", "=", "(", "val_trial", ".", "df", "<", "0", ")", "&", "(", "val_trial", ".", "f", "<=", "f_lim", ")", "# If the trial point has a negative slope but the value at that point", "# is too high, bisect can narrow down an interval between the current left", "# and the trial point.", "needs_bisect", "=", "within_range", "&", "(", "val_trial", ".", "df", "<", "0", ")", "&", "(", "val_trial", ".", "f", ">", "f_lim", ")", "# Note that if `~valid_left` it is because either:", "# - the slope at the trial point is positive, so it is a valid right", "#   point, or", "# - the needs_bisect condition is true.", "# In both cases we want to keep the current left and replace right", "# with the trial point.", "left", "=", "val_where", "(", "within_range", "&", "valid_left", ",", "val_trial", ",", "val_left", ")", "right", "=", "val_where", "(", "within_range", "&", "~", "valid_left", ",", "val_trial", ",", "val_right", ")", "bisect_args", "=", "_IntermediateResult", "(", "iteration", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "0", ")", ",", "stopped", "=", "~", "needs_bisect", ",", "failed", "=", "tf", ".", "zeros_like", "(", "within_range", ")", ",", "# i.e. all false.", "num_evals", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "0", ")", ",", "left", "=", "left", ",", "right", "=", "right", ")", "return", "_bisect", "(", "value_and_gradients_function", ",", "bisect_args", ",", "f_lim", ")"], "docstring": "Squeezes a bracketing interval containing the minimum.\n\n  Given an interval which brackets a minimum and a point in that interval,\n  finds a smaller nested interval which also brackets the minimum. If the\n  supplied point does not lie in the bracketing interval, the current interval\n  is returned.\n\n  The following description is given in terms of individual points evaluated on\n  a line function to be minimized. Note, however, the implementation also\n  accepts batches of points allowing to minimize multiple line functions at\n  once. See details on the docstring of `value_and_gradients_function` below.\n\n  The requirement of the interval bracketing a minimum is expressed through the\n  opposite slope conditions. Assume the left end point is 'a', the right\n  end point is 'b', the function to be minimized is 'f' and the derivative is\n  'df'. The update procedure relies on the following conditions being satisfied:\n\n  '''\n    f(a) <= f(0) + epsilon   (1)\n    df(a) < 0                (2)\n    df(b) > 0                (3)\n  '''\n\n  In the first condition, epsilon is a small positive constant. The condition\n  demands that the function at the left end point be not much bigger than the\n  starting point (i.e. 0). This is an easy to satisfy condition because by\n  assumption, we are in a direction where the function value is decreasing.\n  The second and third conditions together demand that there is at least one\n  zero of the derivative in between a and b.\n\n  In addition to the interval, the update algorithm requires a third point to\n  be supplied. Usually, this point would lie within the interval [a, b]. If the\n  point is outside this interval, the current interval is returned. If the\n  point lies within the interval, the behaviour of the function and derivative\n  value at this point is used to squeeze the original interval in a manner that\n  preserves the opposite slope conditions.\n\n  For further details of this component, see the procedure U0-U3 on page 123 of\n  the [Hager and Zhang (2006)][2] article.\n\n  Note that this function does not explicitly verify whether the opposite slope\n  conditions are satisfied for the supplied interval. It is assumed that this\n  is so.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns an object that can be converted to a namedtuple.\n      The namedtuple should have fields 'f' and 'df' that correspond to scalar\n      tensors of real dtype containing the value of the function and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      function values and derivatives at the input points.\n    val_left: Return value of value_and_gradients_function at the left\n      end point of the bracketing interval (labelles 'a' above).\n    val_right: Return value of value_and_gradients_function at the right\n      end point of the bracketing interval (labelles 'b' above).\n    val_trial: Return value of value_and_gradients_function at the trial point\n      to be used to shrink the interval (labelled 'c' above).\n    f_lim: real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked for each batch member.\n    active: optional boolean `Tensor` of shape [n]. Relevant in batching mode\n      only, indicates batch members on which the update procedure should be\n      applied. On non-active members the current left/right interval is returned\n      unmodified.\n\n  Returns:\n    A namedtuple containing the following fields:\n      iteration: An int32 scalar `Tensor`. The number of iterations performed\n        by the bisect algorithm.\n      stopped: A boolean `Tensor` of shape [n]. True for those batch members\n        where the bisection algorithm terminated.\n      failed: A boolean `Tensor` of shape [n]. True for those batch members\n        where an error was encountered.\n      num_evals: An int32 scalar `Tensor`. The number of times the objective\n        function was evaluated.\n      left: Return value of value_and_gradients_function at the updated left\n        end point of the interval found.\n      right: Return value of value_and_gradients_function at the updated right\n        end point of the interval found.", "docstring_tokens": ["Squeezes", "a", "bracketing", "interval", "containing", "the", "minimum", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py#L301-L423", "partition": "test", "index": 998, "time": "2019-02-13 01:51:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/monte_carlo.py", "func_name": "expectation_importance_sampler_logspace", "original_string": "def expectation_importance_sampler_logspace(\n    log_f,\n    log_p,\n    sampling_dist_q,\n    z=None,\n    n=None,\n    seed=None,\n    name='expectation_importance_sampler_logspace'):\n  r\"\"\"Importance sampling with a positive function, in log-space.\n\n  With \\\\(p(z) := exp^{log_p(z)}\\\\), and \\\\(f(z) = exp{log_f(z)}\\\\),\n  this `Op` returns\n\n  \\\\(Log[ n^{-1} sum_{i=1}^n [ f(z_i) p(z_i) / q(z_i) ] ],  z_i ~ q,\\\\)\n  \\\\(\\approx Log[ E_q[ f(Z) p(Z) / q(Z) ] ]\\\\)\n  \\\\(=       Log[E_p[f(Z)]]\\\\)\n\n  This integral is done in log-space with max-subtraction to better handle the\n  often extreme values that `f(z) p(z) / q(z)` can take on.\n\n  In contrast to `expectation_importance_sampler`, this `Op` returns values in\n  log-space.\n\n\n  User supplies either `Tensor` of samples `z`, or number of samples to draw `n`\n\n  Args:\n    log_f: Callable mapping samples from `sampling_dist_q` to `Tensors` with\n      shape broadcastable to `q.batch_shape`.\n      For example, `log_f` works \"just like\" `sampling_dist_q.log_prob`.\n    log_p:  Callable mapping samples from `sampling_dist_q` to `Tensors` with\n      shape broadcastable to `q.batch_shape`.\n      For example, `log_p` works \"just like\" `q.log_prob`.\n    sampling_dist_q:  The sampling distribution.\n      `tfp.distributions.Distribution`.\n      `float64` `dtype` recommended.\n      `log_p` and `q` should be supported on the same set.\n    z:  `Tensor` of samples from `q`, produced by `q.sample` for some `n`.\n    n:  Integer `Tensor`.  Number of samples to generate if `z` is not provided.\n    seed:  Python integer to seed the random number generator.\n    name:  A name to give this `Op`.\n\n  Returns:\n    Logarithm of the importance sampling estimate.  `Tensor` with `shape` equal\n      to batch shape of `q`, and `dtype` = `q.dtype`.\n  \"\"\"\n  q = sampling_dist_q\n  with tf.name_scope(name):\n    z = _get_samples(q, z, n, seed)\n    log_values = log_f(z) + log_p(z) - q.log_prob(z)\n    return _logspace_mean(log_values)", "language": "python", "code": "def expectation_importance_sampler_logspace(\n    log_f,\n    log_p,\n    sampling_dist_q,\n    z=None,\n    n=None,\n    seed=None,\n    name='expectation_importance_sampler_logspace'):\n  r\"\"\"Importance sampling with a positive function, in log-space.\n\n  With \\\\(p(z) := exp^{log_p(z)}\\\\), and \\\\(f(z) = exp{log_f(z)}\\\\),\n  this `Op` returns\n\n  \\\\(Log[ n^{-1} sum_{i=1}^n [ f(z_i) p(z_i) / q(z_i) ] ],  z_i ~ q,\\\\)\n  \\\\(\\approx Log[ E_q[ f(Z) p(Z) / q(Z) ] ]\\\\)\n  \\\\(=       Log[E_p[f(Z)]]\\\\)\n\n  This integral is done in log-space with max-subtraction to better handle the\n  often extreme values that `f(z) p(z) / q(z)` can take on.\n\n  In contrast to `expectation_importance_sampler`, this `Op` returns values in\n  log-space.\n\n\n  User supplies either `Tensor` of samples `z`, or number of samples to draw `n`\n\n  Args:\n    log_f: Callable mapping samples from `sampling_dist_q` to `Tensors` with\n      shape broadcastable to `q.batch_shape`.\n      For example, `log_f` works \"just like\" `sampling_dist_q.log_prob`.\n    log_p:  Callable mapping samples from `sampling_dist_q` to `Tensors` with\n      shape broadcastable to `q.batch_shape`.\n      For example, `log_p` works \"just like\" `q.log_prob`.\n    sampling_dist_q:  The sampling distribution.\n      `tfp.distributions.Distribution`.\n      `float64` `dtype` recommended.\n      `log_p` and `q` should be supported on the same set.\n    z:  `Tensor` of samples from `q`, produced by `q.sample` for some `n`.\n    n:  Integer `Tensor`.  Number of samples to generate if `z` is not provided.\n    seed:  Python integer to seed the random number generator.\n    name:  A name to give this `Op`.\n\n  Returns:\n    Logarithm of the importance sampling estimate.  `Tensor` with `shape` equal\n      to batch shape of `q`, and `dtype` = `q.dtype`.\n  \"\"\"\n  q = sampling_dist_q\n  with tf.name_scope(name):\n    z = _get_samples(q, z, n, seed)\n    log_values = log_f(z) + log_p(z) - q.log_prob(z)\n    return _logspace_mean(log_values)", "code_tokens": ["def", "expectation_importance_sampler_logspace", "(", "log_f", ",", "log_p", ",", "sampling_dist_q", ",", "z", "=", "None", ",", "n", "=", "None", ",", "seed", "=", "None", ",", "name", "=", "'expectation_importance_sampler_logspace'", ")", ":", "q", "=", "sampling_dist_q", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "z", "=", "_get_samples", "(", "q", ",", "z", ",", "n", ",", "seed", ")", "log_values", "=", "log_f", "(", "z", ")", "+", "log_p", "(", "z", ")", "-", "q", ".", "log_prob", "(", "z", ")", "return", "_logspace_mean", "(", "log_values", ")"], "docstring": "r\"\"\"Importance sampling with a positive function, in log-space.\n\n  With \\\\(p(z) := exp^{log_p(z)}\\\\), and \\\\(f(z) = exp{log_f(z)}\\\\),\n  this `Op` returns\n\n  \\\\(Log[ n^{-1} sum_{i=1}^n [ f(z_i) p(z_i) / q(z_i) ] ],  z_i ~ q,\\\\)\n  \\\\(\\approx Log[ E_q[ f(Z) p(Z) / q(Z) ] ]\\\\)\n  \\\\(=       Log[E_p[f(Z)]]\\\\)\n\n  This integral is done in log-space with max-subtraction to better handle the\n  often extreme values that `f(z) p(z) / q(z)` can take on.\n\n  In contrast to `expectation_importance_sampler`, this `Op` returns values in\n  log-space.\n\n\n  User supplies either `Tensor` of samples `z`, or number of samples to draw `n`\n\n  Args:\n    log_f: Callable mapping samples from `sampling_dist_q` to `Tensors` with\n      shape broadcastable to `q.batch_shape`.\n      For example, `log_f` works \"just like\" `sampling_dist_q.log_prob`.\n    log_p:  Callable mapping samples from `sampling_dist_q` to `Tensors` with\n      shape broadcastable to `q.batch_shape`.\n      For example, `log_p` works \"just like\" `q.log_prob`.\n    sampling_dist_q:  The sampling distribution.\n      `tfp.distributions.Distribution`.\n      `float64` `dtype` recommended.\n      `log_p` and `q` should be supported on the same set.\n    z:  `Tensor` of samples from `q`, produced by `q.sample` for some `n`.\n    n:  Integer `Tensor`.  Number of samples to generate if `z` is not provided.\n    seed:  Python integer to seed the random number generator.\n    name:  A name to give this `Op`.\n\n  Returns:\n    Logarithm of the importance sampling estimate.  `Tensor` with `shape` equal\n      to batch shape of `q`, and `dtype` = `q.dtype`.", "docstring_tokens": ["r", "Importance", "sampling", "with", "a", "positive", "function", "in", "log", "-", "space", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/monte_carlo.py#L102-L152", "partition": "test", "index": 1059, "time": "2019-02-14 17:05:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/categorical_to_discrete.py", "func_name": "_maybe_check_valid_map_values", "original_string": "def _maybe_check_valid_map_values(map_values, validate_args):\n  \"\"\"Validate `map_values` if `validate_args`==True.\"\"\"\n  assertions = []\n\n  message = 'Rank of map_values must be 1.'\n  if tensorshape_util.rank(map_values.shape) is not None:\n    if tensorshape_util.rank(map_values.shape) != 1:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(assert_util.assert_rank(map_values, 1, message=message))\n\n  message = 'Size of map_values must be greater than 0.'\n  if tensorshape_util.num_elements(map_values.shape) is not None:\n    if tensorshape_util.num_elements(map_values.shape) == 0:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        assert_util.assert_greater(\n            tf.size(input=map_values), 0, message=message))\n\n  if validate_args:\n    assertions.append(\n        assert_util.assert_equal(\n            tf.math.is_strictly_increasing(map_values),\n            True,\n            message='map_values is not strictly increasing.'))\n\n  return assertions", "language": "python", "code": "def _maybe_check_valid_map_values(map_values, validate_args):\n  \"\"\"Validate `map_values` if `validate_args`==True.\"\"\"\n  assertions = []\n\n  message = 'Rank of map_values must be 1.'\n  if tensorshape_util.rank(map_values.shape) is not None:\n    if tensorshape_util.rank(map_values.shape) != 1:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(assert_util.assert_rank(map_values, 1, message=message))\n\n  message = 'Size of map_values must be greater than 0.'\n  if tensorshape_util.num_elements(map_values.shape) is not None:\n    if tensorshape_util.num_elements(map_values.shape) == 0:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        assert_util.assert_greater(\n            tf.size(input=map_values), 0, message=message))\n\n  if validate_args:\n    assertions.append(\n        assert_util.assert_equal(\n            tf.math.is_strictly_increasing(map_values),\n            True,\n            message='map_values is not strictly increasing.'))\n\n  return assertions", "code_tokens": ["def", "_maybe_check_valid_map_values", "(", "map_values", ",", "validate_args", ")", ":", "assertions", "=", "[", "]", "message", "=", "'Rank of map_values must be 1.'", "if", "tensorshape_util", ".", "rank", "(", "map_values", ".", "shape", ")", "is", "not", "None", ":", "if", "tensorshape_util", ".", "rank", "(", "map_values", ".", "shape", ")", "!=", "1", ":", "raise", "ValueError", "(", "message", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "assert_util", ".", "assert_rank", "(", "map_values", ",", "1", ",", "message", "=", "message", ")", ")", "message", "=", "'Size of map_values must be greater than 0.'", "if", "tensorshape_util", ".", "num_elements", "(", "map_values", ".", "shape", ")", "is", "not", "None", ":", "if", "tensorshape_util", ".", "num_elements", "(", "map_values", ".", "shape", ")", "==", "0", ":", "raise", "ValueError", "(", "message", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "assert_util", ".", "assert_greater", "(", "tf", ".", "size", "(", "input", "=", "map_values", ")", ",", "0", ",", "message", "=", "message", ")", ")", "if", "validate_args", ":", "assertions", ".", "append", "(", "assert_util", ".", "assert_equal", "(", "tf", ".", "math", ".", "is_strictly_increasing", "(", "map_values", ")", ",", "True", ",", "message", "=", "'map_values is not strictly increasing.'", ")", ")", "return", "assertions"], "docstring": "Validate `map_values` if `validate_args`==True.", "docstring_tokens": ["Validate", "map_values", "if", "validate_args", "==", "True", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/categorical_to_discrete.py#L127-L154", "partition": "test", "index": 634, "time": "2019-02-14 19:28:09"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/util.py", "func_name": "trace_scan", "original_string": "def trace_scan(loop_fn,\n               initial_state,\n               elems,\n               trace_fn,\n               parallel_iterations=10,\n               name=None):\n  \"\"\"A simplified version of `tf.scan` that has configurable tracing.\n\n  This function repeatedly calls `loop_fn(state, elem)`, where `state` is the\n  `initial_state` during the first iteration, and the return value of `loop_fn`\n  for every iteration thereafter. `elem` is a slice of `elements` along the\n  first dimension, accessed in order. Additionally, it calls `trace_fn` on the\n  return value of `loop_fn`. The `Tensor`s in return values of `trace_fn` are\n  stacked and returned from this function, such that the first dimension of\n  those `Tensor`s matches the size of `elems`.\n\n  Args:\n    loop_fn: A callable that takes in a `Tensor` or a nested collection of\n      `Tensor`s with the same structure as `initial_state`, a slice of `elems`\n      and returns the same structure as `initial_state`.\n    initial_state: A `Tensor` or a nested collection of `Tensor`s passed to\n      `loop_fn` in the first iteration.\n    elems: A `Tensor` that is split along the first dimension and each element\n      of which is passed to `loop_fn`.\n    trace_fn: A callable that takes in the return value of `loop_fn` and returns\n      a `Tensor` or a nested collection of `Tensor`s.\n    parallel_iterations: Passed to the internal `tf.while_loop`.\n    name: Name scope used in this function. Default: 'trace_scan'.\n\n  Returns:\n    final_state: The final return value of `loop_fn`.\n    trace: The same structure as the return value of `trace_fn`, but with each\n      `Tensor` being a stack of the corresponding `Tensors` in the return value\n      of `trace_fn` for each slice of `elems`.\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'trace_scan', [initial_state, elems]), tf.compat.v1.variable_scope(\n          tf.compat.v1.get_variable_scope()) as vs:\n    if vs.caching_device is None and not tf.executing_eagerly():\n      vs.set_caching_device(lambda op: op.device)\n\n    initial_state = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, name='initial_state'),\n        initial_state)\n    elems = tf.convert_to_tensor(value=elems, name='elems')\n\n    static_length = elems.shape[0]\n    if tf.compat.dimension_value(static_length) is None:\n      length = tf.shape(input=elems)[0]\n    else:\n      length = tf.convert_to_tensor(\n          value=static_length, dtype=tf.int32, name='length')\n\n    # This is an TensorArray in part because of XLA, which had trouble with\n    # non-statically known indices. I.e. elems[i] errored, but\n    # elems_array.read(i) worked.\n    elems_array = tf.TensorArray(\n        elems.dtype, size=length, element_shape=elems.shape[1:])\n    elems_array = elems_array.unstack(elems)\n\n    trace_arrays = tf.nest.map_structure(\n        lambda x: tf.TensorArray(x.dtype, size=length, element_shape=x.shape),\n        trace_fn(initial_state))\n\n    def _body(i, state, trace_arrays):\n      state = loop_fn(state, elems_array.read(i))\n      trace_arrays = tf.nest.pack_sequence_as(trace_arrays, [\n          a.write(i, v) for a, v in zip(\n              tf.nest.flatten(trace_arrays), tf.nest.flatten(trace_fn(state)))\n      ])\n      return i + 1, state, trace_arrays\n\n    _, final_state, trace_arrays = tf.while_loop(\n        cond=lambda i, *args: i < length,\n        body=_body,\n        loop_vars=(0, initial_state, trace_arrays),\n        parallel_iterations=parallel_iterations)\n\n    stacked_trace = tf.nest.map_structure(lambda x: x.stack(), trace_arrays)\n\n    # Restore the static length if we know it.\n    def _merge_static_length(x):\n      x.set_shape(tf.TensorShape(static_length).concatenate(x.shape[1:]))\n      return x\n\n    stacked_trace = tf.nest.map_structure(_merge_static_length, stacked_trace)\n    return final_state, stacked_trace", "language": "python", "code": "def trace_scan(loop_fn,\n               initial_state,\n               elems,\n               trace_fn,\n               parallel_iterations=10,\n               name=None):\n  \"\"\"A simplified version of `tf.scan` that has configurable tracing.\n\n  This function repeatedly calls `loop_fn(state, elem)`, where `state` is the\n  `initial_state` during the first iteration, and the return value of `loop_fn`\n  for every iteration thereafter. `elem` is a slice of `elements` along the\n  first dimension, accessed in order. Additionally, it calls `trace_fn` on the\n  return value of `loop_fn`. The `Tensor`s in return values of `trace_fn` are\n  stacked and returned from this function, such that the first dimension of\n  those `Tensor`s matches the size of `elems`.\n\n  Args:\n    loop_fn: A callable that takes in a `Tensor` or a nested collection of\n      `Tensor`s with the same structure as `initial_state`, a slice of `elems`\n      and returns the same structure as `initial_state`.\n    initial_state: A `Tensor` or a nested collection of `Tensor`s passed to\n      `loop_fn` in the first iteration.\n    elems: A `Tensor` that is split along the first dimension and each element\n      of which is passed to `loop_fn`.\n    trace_fn: A callable that takes in the return value of `loop_fn` and returns\n      a `Tensor` or a nested collection of `Tensor`s.\n    parallel_iterations: Passed to the internal `tf.while_loop`.\n    name: Name scope used in this function. Default: 'trace_scan'.\n\n  Returns:\n    final_state: The final return value of `loop_fn`.\n    trace: The same structure as the return value of `trace_fn`, but with each\n      `Tensor` being a stack of the corresponding `Tensors` in the return value\n      of `trace_fn` for each slice of `elems`.\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'trace_scan', [initial_state, elems]), tf.compat.v1.variable_scope(\n          tf.compat.v1.get_variable_scope()) as vs:\n    if vs.caching_device is None and not tf.executing_eagerly():\n      vs.set_caching_device(lambda op: op.device)\n\n    initial_state = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, name='initial_state'),\n        initial_state)\n    elems = tf.convert_to_tensor(value=elems, name='elems')\n\n    static_length = elems.shape[0]\n    if tf.compat.dimension_value(static_length) is None:\n      length = tf.shape(input=elems)[0]\n    else:\n      length = tf.convert_to_tensor(\n          value=static_length, dtype=tf.int32, name='length')\n\n    # This is an TensorArray in part because of XLA, which had trouble with\n    # non-statically known indices. I.e. elems[i] errored, but\n    # elems_array.read(i) worked.\n    elems_array = tf.TensorArray(\n        elems.dtype, size=length, element_shape=elems.shape[1:])\n    elems_array = elems_array.unstack(elems)\n\n    trace_arrays = tf.nest.map_structure(\n        lambda x: tf.TensorArray(x.dtype, size=length, element_shape=x.shape),\n        trace_fn(initial_state))\n\n    def _body(i, state, trace_arrays):\n      state = loop_fn(state, elems_array.read(i))\n      trace_arrays = tf.nest.pack_sequence_as(trace_arrays, [\n          a.write(i, v) for a, v in zip(\n              tf.nest.flatten(trace_arrays), tf.nest.flatten(trace_fn(state)))\n      ])\n      return i + 1, state, trace_arrays\n\n    _, final_state, trace_arrays = tf.while_loop(\n        cond=lambda i, *args: i < length,\n        body=_body,\n        loop_vars=(0, initial_state, trace_arrays),\n        parallel_iterations=parallel_iterations)\n\n    stacked_trace = tf.nest.map_structure(lambda x: x.stack(), trace_arrays)\n\n    # Restore the static length if we know it.\n    def _merge_static_length(x):\n      x.set_shape(tf.TensorShape(static_length).concatenate(x.shape[1:]))\n      return x\n\n    stacked_trace = tf.nest.map_structure(_merge_static_length, stacked_trace)\n    return final_state, stacked_trace", "code_tokens": ["def", "trace_scan", "(", "loop_fn", ",", "initial_state", ",", "elems", ",", "trace_fn", ",", "parallel_iterations", "=", "10", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'trace_scan'", ",", "[", "initial_state", ",", "elems", "]", ")", ",", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "tf", ".", "compat", ".", "v1", ".", "get_variable_scope", "(", ")", ")", "as", "vs", ":", "if", "vs", ".", "caching_device", "is", "None", "and", "not", "tf", ".", "executing_eagerly", "(", ")", ":", "vs", ".", "set_caching_device", "(", "lambda", "op", ":", "op", ".", "device", ")", "initial_state", "=", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "x", ":", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'initial_state'", ")", ",", "initial_state", ")", "elems", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "elems", ",", "name", "=", "'elems'", ")", "static_length", "=", "elems", ".", "shape", "[", "0", "]", "if", "tf", ".", "compat", ".", "dimension_value", "(", "static_length", ")", "is", "None", ":", "length", "=", "tf", ".", "shape", "(", "input", "=", "elems", ")", "[", "0", "]", "else", ":", "length", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "static_length", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "'length'", ")", "# This is an TensorArray in part because of XLA, which had trouble with", "# non-statically known indices. I.e. elems[i] errored, but", "# elems_array.read(i) worked.", "elems_array", "=", "tf", ".", "TensorArray", "(", "elems", ".", "dtype", ",", "size", "=", "length", ",", "element_shape", "=", "elems", ".", "shape", "[", "1", ":", "]", ")", "elems_array", "=", "elems_array", ".", "unstack", "(", "elems", ")", "trace_arrays", "=", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "x", ":", "tf", ".", "TensorArray", "(", "x", ".", "dtype", ",", "size", "=", "length", ",", "element_shape", "=", "x", ".", "shape", ")", ",", "trace_fn", "(", "initial_state", ")", ")", "def", "_body", "(", "i", ",", "state", ",", "trace_arrays", ")", ":", "state", "=", "loop_fn", "(", "state", ",", "elems_array", ".", "read", "(", "i", ")", ")", "trace_arrays", "=", "tf", ".", "nest", ".", "pack_sequence_as", "(", "trace_arrays", ",", "[", "a", ".", "write", "(", "i", ",", "v", ")", "for", "a", ",", "v", "in", "zip", "(", "tf", ".", "nest", ".", "flatten", "(", "trace_arrays", ")", ",", "tf", ".", "nest", ".", "flatten", "(", "trace_fn", "(", "state", ")", ")", ")", "]", ")", "return", "i", "+", "1", ",", "state", ",", "trace_arrays", "_", ",", "final_state", ",", "trace_arrays", "=", "tf", ".", "while_loop", "(", "cond", "=", "lambda", "i", ",", "*", "args", ":", "i", "<", "length", ",", "body", "=", "_body", ",", "loop_vars", "=", "(", "0", ",", "initial_state", ",", "trace_arrays", ")", ",", "parallel_iterations", "=", "parallel_iterations", ")", "stacked_trace", "=", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "x", ":", "x", ".", "stack", "(", ")", ",", "trace_arrays", ")", "# Restore the static length if we know it.", "def", "_merge_static_length", "(", "x", ")", ":", "x", ".", "set_shape", "(", "tf", ".", "TensorShape", "(", "static_length", ")", ".", "concatenate", "(", "x", ".", "shape", "[", "1", ":", "]", ")", ")", "return", "x", "stacked_trace", "=", "tf", ".", "nest", ".", "map_structure", "(", "_merge_static_length", ",", "stacked_trace", ")", "return", "final_state", ",", "stacked_trace"], "docstring": "A simplified version of `tf.scan` that has configurable tracing.\n\n  This function repeatedly calls `loop_fn(state, elem)`, where `state` is the\n  `initial_state` during the first iteration, and the return value of `loop_fn`\n  for every iteration thereafter. `elem` is a slice of `elements` along the\n  first dimension, accessed in order. Additionally, it calls `trace_fn` on the\n  return value of `loop_fn`. The `Tensor`s in return values of `trace_fn` are\n  stacked and returned from this function, such that the first dimension of\n  those `Tensor`s matches the size of `elems`.\n\n  Args:\n    loop_fn: A callable that takes in a `Tensor` or a nested collection of\n      `Tensor`s with the same structure as `initial_state`, a slice of `elems`\n      and returns the same structure as `initial_state`.\n    initial_state: A `Tensor` or a nested collection of `Tensor`s passed to\n      `loop_fn` in the first iteration.\n    elems: A `Tensor` that is split along the first dimension and each element\n      of which is passed to `loop_fn`.\n    trace_fn: A callable that takes in the return value of `loop_fn` and returns\n      a `Tensor` or a nested collection of `Tensor`s.\n    parallel_iterations: Passed to the internal `tf.while_loop`.\n    name: Name scope used in this function. Default: 'trace_scan'.\n\n  Returns:\n    final_state: The final return value of `loop_fn`.\n    trace: The same structure as the return value of `trace_fn`, but with each\n      `Tensor` being a stack of the corresponding `Tensors` in the return value\n      of `trace_fn` for each slice of `elems`.", "docstring_tokens": ["A", "simplified", "version", "of", "tf", ".", "scan", "that", "has", "configurable", "tracing", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/util.py#L293-L379", "partition": "test", "index": 971, "time": "2019-02-15 12:33:05"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "with_dependencies", "original_string": "def with_dependencies(dependencies, output_tensor, name=None):\n  \"\"\"Produces the content of `output_tensor` only after `dependencies`.\n\n  In some cases, a user may want the output of an operation to be consumed\n  externally only after some other dependencies have run first. This function\n  returns `output_tensor`, but only after all operations in `dependencies` have\n  run. Note that this means that there is no guarantee that `output_tensor` will\n  be evaluated after any `dependencies` have run.\n\n  See also `tf.tuple` and `tf.group`.\n\n  Args:\n    dependencies: Iterable of operations to run before this op finishes.\n    output_tensor: A `Tensor` or `IndexedSlices` that will be returned.\n    name: (Optional) A name for this operation.\n\n  Returns:\n    output_with_deps: Same as `output_tensor` but with embedded dependencies.\n\n  Raises:\n    TypeError: if `output_tensor` is not a `Tensor` or `IndexedSlices`.\n  \"\"\"\n  if tf.executing_eagerly():\n    return output_tensor\n  with tf.name_scope(name or \"control_dependency\") as name:\n    with tf.control_dependencies(d for d in dependencies if d is not None):\n      output_tensor = tf.convert_to_tensor(value=output_tensor)\n      if isinstance(output_tensor, tf.Tensor):\n        return tf.identity(output_tensor, name=name)\n      else:\n        return tf.IndexedSlices(\n            tf.identity(output_tensor.values, name=name),\n            output_tensor.indices,\n            output_tensor.dense_shape)", "language": "python", "code": "def with_dependencies(dependencies, output_tensor, name=None):\n  \"\"\"Produces the content of `output_tensor` only after `dependencies`.\n\n  In some cases, a user may want the output of an operation to be consumed\n  externally only after some other dependencies have run first. This function\n  returns `output_tensor`, but only after all operations in `dependencies` have\n  run. Note that this means that there is no guarantee that `output_tensor` will\n  be evaluated after any `dependencies` have run.\n\n  See also `tf.tuple` and `tf.group`.\n\n  Args:\n    dependencies: Iterable of operations to run before this op finishes.\n    output_tensor: A `Tensor` or `IndexedSlices` that will be returned.\n    name: (Optional) A name for this operation.\n\n  Returns:\n    output_with_deps: Same as `output_tensor` but with embedded dependencies.\n\n  Raises:\n    TypeError: if `output_tensor` is not a `Tensor` or `IndexedSlices`.\n  \"\"\"\n  if tf.executing_eagerly():\n    return output_tensor\n  with tf.name_scope(name or \"control_dependency\") as name:\n    with tf.control_dependencies(d for d in dependencies if d is not None):\n      output_tensor = tf.convert_to_tensor(value=output_tensor)\n      if isinstance(output_tensor, tf.Tensor):\n        return tf.identity(output_tensor, name=name)\n      else:\n        return tf.IndexedSlices(\n            tf.identity(output_tensor.values, name=name),\n            output_tensor.indices,\n            output_tensor.dense_shape)", "code_tokens": ["def", "with_dependencies", "(", "dependencies", ",", "output_tensor", ",", "name", "=", "None", ")", ":", "if", "tf", ".", "executing_eagerly", "(", ")", ":", "return", "output_tensor", "with", "tf", ".", "name_scope", "(", "name", "or", "\"control_dependency\"", ")", "as", "name", ":", "with", "tf", ".", "control_dependencies", "(", "d", "for", "d", "in", "dependencies", "if", "d", "is", "not", "None", ")", ":", "output_tensor", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "output_tensor", ")", "if", "isinstance", "(", "output_tensor", ",", "tf", ".", "Tensor", ")", ":", "return", "tf", ".", "identity", "(", "output_tensor", ",", "name", "=", "name", ")", "else", ":", "return", "tf", ".", "IndexedSlices", "(", "tf", ".", "identity", "(", "output_tensor", ".", "values", ",", "name", "=", "name", ")", ",", "output_tensor", ".", "indices", ",", "output_tensor", ".", "dense_shape", ")"], "docstring": "Produces the content of `output_tensor` only after `dependencies`.\n\n  In some cases, a user may want the output of an operation to be consumed\n  externally only after some other dependencies have run first. This function\n  returns `output_tensor`, but only after all operations in `dependencies` have\n  run. Note that this means that there is no guarantee that `output_tensor` will\n  be evaluated after any `dependencies` have run.\n\n  See also `tf.tuple` and `tf.group`.\n\n  Args:\n    dependencies: Iterable of operations to run before this op finishes.\n    output_tensor: A `Tensor` or `IndexedSlices` that will be returned.\n    name: (Optional) A name for this operation.\n\n  Returns:\n    output_with_deps: Same as `output_tensor` but with embedded dependencies.\n\n  Raises:\n    TypeError: if `output_tensor` is not a `Tensor` or `IndexedSlices`.", "docstring_tokens": ["Produces", "the", "content", "of", "output_tensor", "only", "after", "dependencies", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L2162-L2195", "partition": "test", "index": 930, "time": "2019-02-16 14:36:35"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/prefer_static.py", "func_name": "case", "original_string": "def case(pred_fn_pairs, default=None, exclusive=False, name='smart_case'):\n  \"\"\"Like tf.case, except attempts to statically evaluate predicates.\n\n  If any predicate in `pred_fn_pairs` is a bool or has a constant value, the\n  associated callable will be called or omitted depending on its value.\n  Otherwise this functions like tf.case.\n\n  Args:\n    pred_fn_pairs: Dict or list of pairs of a boolean scalar tensor and a\n                   callable which returns a list of tensors.\n    default: Optional callable that returns a list of tensors.\n    exclusive: True iff at most one predicate is allowed to evaluate to `True`.\n    name: A name for this operation (optional).\n\n  Returns:\n    The tensors returned by the first pair whose predicate evaluated to True, or\n    those returned by `default` if none does.\n\n  Raises:\n    TypeError: If `pred_fn_pairs` is not a list/dictionary.\n    TypeError: If `pred_fn_pairs` is a list but does not contain 2-tuples.\n    TypeError: If `fns[i]` is not callable for any i, or `default` is not\n               callable.\n  \"\"\"\n  return control_flow_ops._case_helper(  # pylint: disable=protected-access\n      cond, pred_fn_pairs, default, exclusive, name, allow_python_preds=True)", "language": "python", "code": "def case(pred_fn_pairs, default=None, exclusive=False, name='smart_case'):\n  \"\"\"Like tf.case, except attempts to statically evaluate predicates.\n\n  If any predicate in `pred_fn_pairs` is a bool or has a constant value, the\n  associated callable will be called or omitted depending on its value.\n  Otherwise this functions like tf.case.\n\n  Args:\n    pred_fn_pairs: Dict or list of pairs of a boolean scalar tensor and a\n                   callable which returns a list of tensors.\n    default: Optional callable that returns a list of tensors.\n    exclusive: True iff at most one predicate is allowed to evaluate to `True`.\n    name: A name for this operation (optional).\n\n  Returns:\n    The tensors returned by the first pair whose predicate evaluated to True, or\n    those returned by `default` if none does.\n\n  Raises:\n    TypeError: If `pred_fn_pairs` is not a list/dictionary.\n    TypeError: If `pred_fn_pairs` is a list but does not contain 2-tuples.\n    TypeError: If `fns[i]` is not callable for any i, or `default` is not\n               callable.\n  \"\"\"\n  return control_flow_ops._case_helper(  # pylint: disable=protected-access\n      cond, pred_fn_pairs, default, exclusive, name, allow_python_preds=True)", "code_tokens": ["def", "case", "(", "pred_fn_pairs", ",", "default", "=", "None", ",", "exclusive", "=", "False", ",", "name", "=", "'smart_case'", ")", ":", "return", "control_flow_ops", ".", "_case_helper", "(", "# pylint: disable=protected-access", "cond", ",", "pred_fn_pairs", ",", "default", ",", "exclusive", ",", "name", ",", "allow_python_preds", "=", "True", ")"], "docstring": "Like tf.case, except attempts to statically evaluate predicates.\n\n  If any predicate in `pred_fn_pairs` is a bool or has a constant value, the\n  associated callable will be called or omitted depending on its value.\n  Otherwise this functions like tf.case.\n\n  Args:\n    pred_fn_pairs: Dict or list of pairs of a boolean scalar tensor and a\n                   callable which returns a list of tensors.\n    default: Optional callable that returns a list of tensors.\n    exclusive: True iff at most one predicate is allowed to evaluate to `True`.\n    name: A name for this operation (optional).\n\n  Returns:\n    The tensors returned by the first pair whose predicate evaluated to True, or\n    those returned by `default` if none does.\n\n  Raises:\n    TypeError: If `pred_fn_pairs` is not a list/dictionary.\n    TypeError: If `pred_fn_pairs` is a list but does not contain 2-tuples.\n    TypeError: If `fns[i]` is not callable for any i, or `default` is not\n               callable.", "docstring_tokens": ["Like", "tf", ".", "case", "except", "attempts", "to", "statically", "evaluate", "predicates", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/prefer_static.py#L154-L179", "partition": "test", "index": 900, "time": "2019-02-19 20:51:54"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/prefer_static.py", "func_name": "_get_static_predicate", "original_string": "def _get_static_predicate(pred):\n  \"\"\"Helper function for statically evaluating predicates in `cond`.\"\"\"\n  if pred in {0, 1}:  # Accept 1/0 as valid boolean values\n    pred_value = bool(pred)\n  elif isinstance(pred, bool):\n    pred_value = pred\n  elif isinstance(pred, tf.Tensor):\n    pred_value = tf.get_static_value(pred)\n\n    # TODO(jamieas): remove the dependency on `pywrap_tensorflow`.\n    # pylint: disable=protected-access\n    if pred_value is None:\n      pred_value = c_api.TF_TryEvaluateConstant_wrapper(pred.graph._c_graph,\n                                                        pred._as_tf_output())\n    # pylint: enable=protected-access\n\n  else:\n    raise TypeError('`pred` must be a Tensor, or a Python bool, or 1 or 0. '\n                    'Found instead: {}'.format(pred))\n  return pred_value", "language": "python", "code": "def _get_static_predicate(pred):\n  \"\"\"Helper function for statically evaluating predicates in `cond`.\"\"\"\n  if pred in {0, 1}:  # Accept 1/0 as valid boolean values\n    pred_value = bool(pred)\n  elif isinstance(pred, bool):\n    pred_value = pred\n  elif isinstance(pred, tf.Tensor):\n    pred_value = tf.get_static_value(pred)\n\n    # TODO(jamieas): remove the dependency on `pywrap_tensorflow`.\n    # pylint: disable=protected-access\n    if pred_value is None:\n      pred_value = c_api.TF_TryEvaluateConstant_wrapper(pred.graph._c_graph,\n                                                        pred._as_tf_output())\n    # pylint: enable=protected-access\n\n  else:\n    raise TypeError('`pred` must be a Tensor, or a Python bool, or 1 or 0. '\n                    'Found instead: {}'.format(pred))\n  return pred_value", "code_tokens": ["def", "_get_static_predicate", "(", "pred", ")", ":", "if", "pred", "in", "{", "0", ",", "1", "}", ":", "# Accept 1/0 as valid boolean values", "pred_value", "=", "bool", "(", "pred", ")", "elif", "isinstance", "(", "pred", ",", "bool", ")", ":", "pred_value", "=", "pred", "elif", "isinstance", "(", "pred", ",", "tf", ".", "Tensor", ")", ":", "pred_value", "=", "tf", ".", "get_static_value", "(", "pred", ")", "# TODO(jamieas): remove the dependency on `pywrap_tensorflow`.", "# pylint: disable=protected-access", "if", "pred_value", "is", "None", ":", "pred_value", "=", "c_api", ".", "TF_TryEvaluateConstant_wrapper", "(", "pred", ".", "graph", ".", "_c_graph", ",", "pred", ".", "_as_tf_output", "(", ")", ")", "# pylint: enable=protected-access", "else", ":", "raise", "TypeError", "(", "'`pred` must be a Tensor, or a Python bool, or 1 or 0. '", "'Found instead: {}'", ".", "format", "(", "pred", ")", ")", "return", "pred_value"], "docstring": "Helper function for statically evaluating predicates in `cond`.", "docstring_tokens": ["Helper", "function", "for", "statically", "evaluating", "predicates", "in", "cond", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/prefer_static.py#L78-L97", "partition": "test", "index": 898, "time": "2019-02-19 20:51:54"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/sparse.py", "func_name": "dense_to_sparse", "original_string": "def dense_to_sparse(x, ignore_value=None, name=None):\n  \"\"\"Converts dense `Tensor` to `SparseTensor`, dropping `ignore_value` cells.\n\n  Args:\n    x: A `Tensor`.\n    ignore_value: Entries in `x` equal to this value will be\n      absent from the return `SparseTensor`. If `None`, default value of\n      `x` dtype will be used (e.g. '' for `str`, 0 for `int`).\n    name: Python `str` prefix for ops created by this function.\n\n  Returns:\n    sparse_x: A `tf.SparseTensor` with the same shape as `x`.\n\n  Raises:\n    ValueError: when `x`'s rank is `None`.\n  \"\"\"\n  # Copied (with modifications) from:\n  # tensorflow/contrib/layers/python/ops/sparse_ops.py.\n  with tf.compat.v1.name_scope(name, 'dense_to_sparse', [x, ignore_value]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    if ignore_value is None:\n      if x.dtype.base_dtype == tf.string:\n        # Exception due to TF strings are converted to numpy objects by default.\n        ignore_value = ''\n      else:\n        ignore_value = x.dtype.as_numpy_dtype(0)\n      ignore_value = tf.cast(ignore_value, x.dtype, name='ignore_value')\n    indices = tf.where(tf.not_equal(x, ignore_value), name='indices')\n    return tf.SparseTensor(\n        indices=indices,\n        values=tf.gather_nd(x, indices, name='values'),\n        dense_shape=tf.shape(input=x, out_type=tf.int64, name='dense_shape'))", "language": "python", "code": "def dense_to_sparse(x, ignore_value=None, name=None):\n  \"\"\"Converts dense `Tensor` to `SparseTensor`, dropping `ignore_value` cells.\n\n  Args:\n    x: A `Tensor`.\n    ignore_value: Entries in `x` equal to this value will be\n      absent from the return `SparseTensor`. If `None`, default value of\n      `x` dtype will be used (e.g. '' for `str`, 0 for `int`).\n    name: Python `str` prefix for ops created by this function.\n\n  Returns:\n    sparse_x: A `tf.SparseTensor` with the same shape as `x`.\n\n  Raises:\n    ValueError: when `x`'s rank is `None`.\n  \"\"\"\n  # Copied (with modifications) from:\n  # tensorflow/contrib/layers/python/ops/sparse_ops.py.\n  with tf.compat.v1.name_scope(name, 'dense_to_sparse', [x, ignore_value]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    if ignore_value is None:\n      if x.dtype.base_dtype == tf.string:\n        # Exception due to TF strings are converted to numpy objects by default.\n        ignore_value = ''\n      else:\n        ignore_value = x.dtype.as_numpy_dtype(0)\n      ignore_value = tf.cast(ignore_value, x.dtype, name='ignore_value')\n    indices = tf.where(tf.not_equal(x, ignore_value), name='indices')\n    return tf.SparseTensor(\n        indices=indices,\n        values=tf.gather_nd(x, indices, name='values'),\n        dense_shape=tf.shape(input=x, out_type=tf.int64, name='dense_shape'))", "code_tokens": ["def", "dense_to_sparse", "(", "x", ",", "ignore_value", "=", "None", ",", "name", "=", "None", ")", ":", "# Copied (with modifications) from:", "# tensorflow/contrib/layers/python/ops/sparse_ops.py.", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'dense_to_sparse'", ",", "[", "x", ",", "ignore_value", "]", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ")", "if", "ignore_value", "is", "None", ":", "if", "x", ".", "dtype", ".", "base_dtype", "==", "tf", ".", "string", ":", "# Exception due to TF strings are converted to numpy objects by default.", "ignore_value", "=", "''", "else", ":", "ignore_value", "=", "x", ".", "dtype", ".", "as_numpy_dtype", "(", "0", ")", "ignore_value", "=", "tf", ".", "cast", "(", "ignore_value", ",", "x", ".", "dtype", ",", "name", "=", "'ignore_value'", ")", "indices", "=", "tf", ".", "where", "(", "tf", ".", "not_equal", "(", "x", ",", "ignore_value", ")", ",", "name", "=", "'indices'", ")", "return", "tf", ".", "SparseTensor", "(", "indices", "=", "indices", ",", "values", "=", "tf", ".", "gather_nd", "(", "x", ",", "indices", ",", "name", "=", "'values'", ")", ",", "dense_shape", "=", "tf", ".", "shape", "(", "input", "=", "x", ",", "out_type", "=", "tf", ".", "int64", ",", "name", "=", "'dense_shape'", ")", ")"], "docstring": "Converts dense `Tensor` to `SparseTensor`, dropping `ignore_value` cells.\n\n  Args:\n    x: A `Tensor`.\n    ignore_value: Entries in `x` equal to this value will be\n      absent from the return `SparseTensor`. If `None`, default value of\n      `x` dtype will be used (e.g. '' for `str`, 0 for `int`).\n    name: Python `str` prefix for ops created by this function.\n\n  Returns:\n    sparse_x: A `tf.SparseTensor` with the same shape as `x`.\n\n  Raises:\n    ValueError: when `x`'s rank is `None`.", "docstring_tokens": ["Converts", "dense", "Tensor", "to", "SparseTensor", "dropping", "ignore_value", "cells", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/sparse.py#L30-L61", "partition": "test", "index": 764, "time": "2019-02-20 09:45:13"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/reshape.py", "func_name": "_replace_event_shape_in_shape_tensor", "original_string": "def _replace_event_shape_in_shape_tensor(\n    input_shape, event_shape_in, event_shape_out, validate_args):\n  \"\"\"Replaces the rightmost dims in a `Tensor` representing a shape.\n\n  Args:\n    input_shape: a rank-1 `Tensor` of integers\n    event_shape_in: the event shape expected to be present in rightmost dims\n      of `shape_in`.\n    event_shape_out: the event shape with which to replace `event_shape_in` in\n      the rightmost dims of `input_shape`.\n    validate_args: Python `bool` indicating whether arguments should\n      be checked for correctness.\n\n  Returns:\n    output_shape: A rank-1 integer `Tensor` with the same contents as\n      `input_shape` except for the event dims, which are replaced with\n      `event_shape_out`.\n  \"\"\"\n  output_tensorshape, is_validated = _replace_event_shape_in_tensorshape(\n      tensorshape_util.constant_value_as_shape(input_shape),\n      event_shape_in,\n      event_shape_out)\n\n  # TODO(b/124240153): Remove map(tf.identity, deps) once tf.function\n  # correctly supports control_dependencies.\n  validation_dependencies = (\n      map(tf.identity, (event_shape_in, event_shape_out))\n      if validate_args else ())\n\n  if (tensorshape_util.is_fully_defined(output_tensorshape) and\n      (is_validated or not validate_args)):\n    with tf.control_dependencies(validation_dependencies):\n      output_shape = tf.convert_to_tensor(\n          value=output_tensorshape, name='output_shape', dtype_hint=tf.int32)\n    return output_shape, output_tensorshape\n\n  with tf.control_dependencies(validation_dependencies):\n    event_shape_in_ndims = (\n        tf.size(input=event_shape_in)\n        if tensorshape_util.num_elements(event_shape_in.shape) is None else\n        tensorshape_util.num_elements(event_shape_in.shape))\n    input_non_event_shape, input_event_shape = tf.split(\n        input_shape, num_or_size_splits=[-1, event_shape_in_ndims])\n\n  additional_assertions = []\n  if is_validated:\n    pass\n  elif validate_args:\n    # Check that `input_event_shape` and `event_shape_in` are compatible in the\n    # sense that they have equal entries in any position that isn't a `-1` in\n    # `event_shape_in`. Note that our validations at construction time ensure\n    # there is at most one such entry in `event_shape_in`.\n    mask = event_shape_in >= 0\n    explicit_input_event_shape = tf.boolean_mask(\n        tensor=input_event_shape, mask=mask)\n    explicit_event_shape_in = tf.boolean_mask(\n        tensor=event_shape_in, mask=mask)\n    additional_assertions.append(\n        assert_util.assert_equal(\n            explicit_input_event_shape,\n            explicit_event_shape_in,\n            message='Input `event_shape` does not match `event_shape_in`.'))\n    # We don't explicitly additionally verify\n    # `tf.size(input_shape) > tf.size(event_shape_in)` since `tf.split`\n    # already makes this assertion.\n\n  with tf.control_dependencies(additional_assertions):\n    output_shape = tf.concat([input_non_event_shape, event_shape_out], axis=0,\n                             name='output_shape')\n\n  return output_shape, output_tensorshape", "language": "python", "code": "def _replace_event_shape_in_shape_tensor(\n    input_shape, event_shape_in, event_shape_out, validate_args):\n  \"\"\"Replaces the rightmost dims in a `Tensor` representing a shape.\n\n  Args:\n    input_shape: a rank-1 `Tensor` of integers\n    event_shape_in: the event shape expected to be present in rightmost dims\n      of `shape_in`.\n    event_shape_out: the event shape with which to replace `event_shape_in` in\n      the rightmost dims of `input_shape`.\n    validate_args: Python `bool` indicating whether arguments should\n      be checked for correctness.\n\n  Returns:\n    output_shape: A rank-1 integer `Tensor` with the same contents as\n      `input_shape` except for the event dims, which are replaced with\n      `event_shape_out`.\n  \"\"\"\n  output_tensorshape, is_validated = _replace_event_shape_in_tensorshape(\n      tensorshape_util.constant_value_as_shape(input_shape),\n      event_shape_in,\n      event_shape_out)\n\n  # TODO(b/124240153): Remove map(tf.identity, deps) once tf.function\n  # correctly supports control_dependencies.\n  validation_dependencies = (\n      map(tf.identity, (event_shape_in, event_shape_out))\n      if validate_args else ())\n\n  if (tensorshape_util.is_fully_defined(output_tensorshape) and\n      (is_validated or not validate_args)):\n    with tf.control_dependencies(validation_dependencies):\n      output_shape = tf.convert_to_tensor(\n          value=output_tensorshape, name='output_shape', dtype_hint=tf.int32)\n    return output_shape, output_tensorshape\n\n  with tf.control_dependencies(validation_dependencies):\n    event_shape_in_ndims = (\n        tf.size(input=event_shape_in)\n        if tensorshape_util.num_elements(event_shape_in.shape) is None else\n        tensorshape_util.num_elements(event_shape_in.shape))\n    input_non_event_shape, input_event_shape = tf.split(\n        input_shape, num_or_size_splits=[-1, event_shape_in_ndims])\n\n  additional_assertions = []\n  if is_validated:\n    pass\n  elif validate_args:\n    # Check that `input_event_shape` and `event_shape_in` are compatible in the\n    # sense that they have equal entries in any position that isn't a `-1` in\n    # `event_shape_in`. Note that our validations at construction time ensure\n    # there is at most one such entry in `event_shape_in`.\n    mask = event_shape_in >= 0\n    explicit_input_event_shape = tf.boolean_mask(\n        tensor=input_event_shape, mask=mask)\n    explicit_event_shape_in = tf.boolean_mask(\n        tensor=event_shape_in, mask=mask)\n    additional_assertions.append(\n        assert_util.assert_equal(\n            explicit_input_event_shape,\n            explicit_event_shape_in,\n            message='Input `event_shape` does not match `event_shape_in`.'))\n    # We don't explicitly additionally verify\n    # `tf.size(input_shape) > tf.size(event_shape_in)` since `tf.split`\n    # already makes this assertion.\n\n  with tf.control_dependencies(additional_assertions):\n    output_shape = tf.concat([input_non_event_shape, event_shape_out], axis=0,\n                             name='output_shape')\n\n  return output_shape, output_tensorshape", "code_tokens": ["def", "_replace_event_shape_in_shape_tensor", "(", "input_shape", ",", "event_shape_in", ",", "event_shape_out", ",", "validate_args", ")", ":", "output_tensorshape", ",", "is_validated", "=", "_replace_event_shape_in_tensorshape", "(", "tensorshape_util", ".", "constant_value_as_shape", "(", "input_shape", ")", ",", "event_shape_in", ",", "event_shape_out", ")", "# TODO(b/124240153): Remove map(tf.identity, deps) once tf.function", "# correctly supports control_dependencies.", "validation_dependencies", "=", "(", "map", "(", "tf", ".", "identity", ",", "(", "event_shape_in", ",", "event_shape_out", ")", ")", "if", "validate_args", "else", "(", ")", ")", "if", "(", "tensorshape_util", ".", "is_fully_defined", "(", "output_tensorshape", ")", "and", "(", "is_validated", "or", "not", "validate_args", ")", ")", ":", "with", "tf", ".", "control_dependencies", "(", "validation_dependencies", ")", ":", "output_shape", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "output_tensorshape", ",", "name", "=", "'output_shape'", ",", "dtype_hint", "=", "tf", ".", "int32", ")", "return", "output_shape", ",", "output_tensorshape", "with", "tf", ".", "control_dependencies", "(", "validation_dependencies", ")", ":", "event_shape_in_ndims", "=", "(", "tf", ".", "size", "(", "input", "=", "event_shape_in", ")", "if", "tensorshape_util", ".", "num_elements", "(", "event_shape_in", ".", "shape", ")", "is", "None", "else", "tensorshape_util", ".", "num_elements", "(", "event_shape_in", ".", "shape", ")", ")", "input_non_event_shape", ",", "input_event_shape", "=", "tf", ".", "split", "(", "input_shape", ",", "num_or_size_splits", "=", "[", "-", "1", ",", "event_shape_in_ndims", "]", ")", "additional_assertions", "=", "[", "]", "if", "is_validated", ":", "pass", "elif", "validate_args", ":", "# Check that `input_event_shape` and `event_shape_in` are compatible in the", "# sense that they have equal entries in any position that isn't a `-1` in", "# `event_shape_in`. Note that our validations at construction time ensure", "# there is at most one such entry in `event_shape_in`.", "mask", "=", "event_shape_in", ">=", "0", "explicit_input_event_shape", "=", "tf", ".", "boolean_mask", "(", "tensor", "=", "input_event_shape", ",", "mask", "=", "mask", ")", "explicit_event_shape_in", "=", "tf", ".", "boolean_mask", "(", "tensor", "=", "event_shape_in", ",", "mask", "=", "mask", ")", "additional_assertions", ".", "append", "(", "assert_util", ".", "assert_equal", "(", "explicit_input_event_shape", ",", "explicit_event_shape_in", ",", "message", "=", "'Input `event_shape` does not match `event_shape_in`.'", ")", ")", "# We don't explicitly additionally verify", "# `tf.size(input_shape) > tf.size(event_shape_in)` since `tf.split`", "# already makes this assertion.", "with", "tf", ".", "control_dependencies", "(", "additional_assertions", ")", ":", "output_shape", "=", "tf", ".", "concat", "(", "[", "input_non_event_shape", ",", "event_shape_out", "]", ",", "axis", "=", "0", ",", "name", "=", "'output_shape'", ")", "return", "output_shape", ",", "output_tensorshape"], "docstring": "Replaces the rightmost dims in a `Tensor` representing a shape.\n\n  Args:\n    input_shape: a rank-1 `Tensor` of integers\n    event_shape_in: the event shape expected to be present in rightmost dims\n      of `shape_in`.\n    event_shape_out: the event shape with which to replace `event_shape_in` in\n      the rightmost dims of `input_shape`.\n    validate_args: Python `bool` indicating whether arguments should\n      be checked for correctness.\n\n  Returns:\n    output_shape: A rank-1 integer `Tensor` with the same contents as\n      `input_shape` except for the event dims, which are replaced with\n      `event_shape_out`.", "docstring_tokens": ["Replaces", "the", "rightmost", "dims", "in", "a", "Tensor", "representing", "a", "shape", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/reshape.py#L243-L313", "partition": "test", "index": 975, "time": "2019-02-23 12:18:26"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/reshape.py", "func_name": "_maybe_check_valid_shape", "original_string": "def _maybe_check_valid_shape(shape, validate_args):\n  \"\"\"Check that a shape Tensor is int-type and otherwise sane.\"\"\"\n  if not dtype_util.is_integer(shape.dtype):\n    raise TypeError('{} dtype ({}) should be `int`-like.'.format(\n        shape, dtype_util.name(shape.dtype)))\n\n  assertions = []\n\n  message = '`{}` rank should be <= 1.'\n  if tensorshape_util.rank(shape.shape) is not None:\n    if tensorshape_util.rank(shape.shape) > 1:\n      raise ValueError(message.format(shape))\n  elif validate_args:\n    assertions.append(assert_util.assert_less(\n        tf.rank(shape), 2, message=message.format(shape)))\n\n  shape_ = tf.get_static_value(shape)\n\n  message = '`{}` elements must have at most one `-1`.'\n  if shape_ is not None:\n    if sum(shape_ == -1) > 1:\n      raise ValueError(message.format(shape))\n  elif validate_args:\n    assertions.append(assert_util.assert_less(\n        tf.reduce_sum(input_tensor=tf.cast(tf.equal(shape, -1), tf.int32)),\n        2,\n        message=message.format(shape)))\n\n  message = '`{}` elements must be either positive integers or `-1`.'\n  if shape_ is not None:\n    if np.any(shape_ < -1):\n      raise ValueError(message.format(shape))\n  elif validate_args:\n    assertions.append(assert_util.assert_greater(\n        shape, -2, message=message.format(shape)))\n\n  return assertions", "language": "python", "code": "def _maybe_check_valid_shape(shape, validate_args):\n  \"\"\"Check that a shape Tensor is int-type and otherwise sane.\"\"\"\n  if not dtype_util.is_integer(shape.dtype):\n    raise TypeError('{} dtype ({}) should be `int`-like.'.format(\n        shape, dtype_util.name(shape.dtype)))\n\n  assertions = []\n\n  message = '`{}` rank should be <= 1.'\n  if tensorshape_util.rank(shape.shape) is not None:\n    if tensorshape_util.rank(shape.shape) > 1:\n      raise ValueError(message.format(shape))\n  elif validate_args:\n    assertions.append(assert_util.assert_less(\n        tf.rank(shape), 2, message=message.format(shape)))\n\n  shape_ = tf.get_static_value(shape)\n\n  message = '`{}` elements must have at most one `-1`.'\n  if shape_ is not None:\n    if sum(shape_ == -1) > 1:\n      raise ValueError(message.format(shape))\n  elif validate_args:\n    assertions.append(assert_util.assert_less(\n        tf.reduce_sum(input_tensor=tf.cast(tf.equal(shape, -1), tf.int32)),\n        2,\n        message=message.format(shape)))\n\n  message = '`{}` elements must be either positive integers or `-1`.'\n  if shape_ is not None:\n    if np.any(shape_ < -1):\n      raise ValueError(message.format(shape))\n  elif validate_args:\n    assertions.append(assert_util.assert_greater(\n        shape, -2, message=message.format(shape)))\n\n  return assertions", "code_tokens": ["def", "_maybe_check_valid_shape", "(", "shape", ",", "validate_args", ")", ":", "if", "not", "dtype_util", ".", "is_integer", "(", "shape", ".", "dtype", ")", ":", "raise", "TypeError", "(", "'{} dtype ({}) should be `int`-like.'", ".", "format", "(", "shape", ",", "dtype_util", ".", "name", "(", "shape", ".", "dtype", ")", ")", ")", "assertions", "=", "[", "]", "message", "=", "'`{}` rank should be <= 1.'", "if", "tensorshape_util", ".", "rank", "(", "shape", ".", "shape", ")", "is", "not", "None", ":", "if", "tensorshape_util", ".", "rank", "(", "shape", ".", "shape", ")", ">", "1", ":", "raise", "ValueError", "(", "message", ".", "format", "(", "shape", ")", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "assert_util", ".", "assert_less", "(", "tf", ".", "rank", "(", "shape", ")", ",", "2", ",", "message", "=", "message", ".", "format", "(", "shape", ")", ")", ")", "shape_", "=", "tf", ".", "get_static_value", "(", "shape", ")", "message", "=", "'`{}` elements must have at most one `-1`.'", "if", "shape_", "is", "not", "None", ":", "if", "sum", "(", "shape_", "==", "-", "1", ")", ">", "1", ":", "raise", "ValueError", "(", "message", ".", "format", "(", "shape", ")", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "assert_util", ".", "assert_less", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "cast", "(", "tf", ".", "equal", "(", "shape", ",", "-", "1", ")", ",", "tf", ".", "int32", ")", ")", ",", "2", ",", "message", "=", "message", ".", "format", "(", "shape", ")", ")", ")", "message", "=", "'`{}` elements must be either positive integers or `-1`.'", "if", "shape_", "is", "not", "None", ":", "if", "np", ".", "any", "(", "shape_", "<", "-", "1", ")", ":", "raise", "ValueError", "(", "message", ".", "format", "(", "shape", ")", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "assert_util", ".", "assert_greater", "(", "shape", ",", "-", "2", ",", "message", "=", "message", ".", "format", "(", "shape", ")", ")", ")", "return", "assertions"], "docstring": "Check that a shape Tensor is int-type and otherwise sane.", "docstring_tokens": ["Check", "that", "a", "shape", "Tensor", "is", "int", "-", "type", "and", "otherwise", "sane", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/reshape.py#L385-L421", "partition": "test", "index": 977, "time": "2019-02-23 12:18:26"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/distribution.py", "func_name": "_remove_dict_keys_with_value", "original_string": "def _remove_dict_keys_with_value(dict_, val):\n  \"\"\"Removes `dict` keys which have have `self` as value.\"\"\"\n  return {k: v for k, v in dict_.items() if v is not val}", "language": "python", "code": "def _remove_dict_keys_with_value(dict_, val):\n  \"\"\"Removes `dict` keys which have have `self` as value.\"\"\"\n  return {k: v for k, v in dict_.items() if v is not val}", "code_tokens": ["def", "_remove_dict_keys_with_value", "(", "dict_", ",", "val", ")", ":", "return", "{", "k", ":", "v", "for", "k", ",", "v", "in", "dict_", ".", "items", "(", ")", "if", "v", "is", "not", "val", "}"], "docstring": "Removes `dict` keys which have have `self` as value.", "docstring_tokens": ["Removes", "dict", "keys", "which", "have", "have", "self", "as", "value", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/distribution.py#L173-L175", "partition": "test", "index": 961, "time": "2019-02-25 13:39:04"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py", "func_name": "_satisfies_wolfe", "original_string": "def _satisfies_wolfe(val_0,\n                     val_c,\n                     f_lim,\n                     sufficient_decrease_param,\n                     curvature_param):\n  \"\"\"Checks whether the Wolfe or approx Wolfe conditions are satisfied.\n\n  The Wolfe conditions are a set of stopping criteria for an inexact line search\n  algorithm. Let f(a) be the function value along the search direction and\n  df(a) the derivative along the search direction evaluated a distance 'a'.\n  Here 'a' is the distance along the search direction. The Wolfe conditions are:\n\n    ```None\n      f(a) <= f(0) + delta * a * df(0)   (Armijo/Sufficient decrease condition)\n      df(a) >= sigma * df(0)             (Weak curvature condition)\n    ```\n  `delta` and `sigma` are two user supplied parameters satisfying:\n   `0 < delta < sigma <= 1.`. In the following, delta is called\n   `sufficient_decrease_param` and sigma is called `curvature_param`.\n\n  On a finite precision machine, the Wolfe conditions are difficult to satisfy\n  when one is close to the minimum. Hence, Hager-Zhang propose replacing\n  the sufficient decrease condition with the following condition on the\n  derivative in the vicinity of a minimum.\n\n    ```None\n      df(a) <= (2 * delta - 1) * df(0)  (Approx Wolfe sufficient decrease)\n    ```\n  This condition is only used if one is near the minimum. This is tested using\n\n    ```None\n      f(a) <= f(0) + epsilon * |f(0)|\n    ```\n  The following function checks both the Wolfe and approx Wolfe conditions.\n  Here, `epsilon` is a small positive constant. In the following, the argument\n  `f_lim` corresponds to the product: epsilon * |f(0)|.\n\n  Args:\n    val_0: A namedtuple, as returned by value_and_gradients_function\n      evaluated at 0.\n    val_c: A namedtuple, as returned by value_and_gradients_function\n      evaluated at the point to be tested.\n    f_lim: Scalar `Tensor` of real dtype. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to 'delta' in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager Zhang (2005)][1].\n\n  Returns:\n    is_satisfied: A scalar boolean `Tensor` which is True if either the\n      Wolfe or approximate Wolfe conditions are satisfied.\n  \"\"\"\n  exact_wolfe_suff_dec = (sufficient_decrease_param * val_0.df >=\n                          (val_c.f - val_0.f) / val_c.x)\n  wolfe_curvature = val_c.df >= curvature_param * val_0.df\n  exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature\n  approx_wolfe_applies = val_c.f <= f_lim\n  approx_wolfe_suff_dec = ((2 * sufficient_decrease_param - 1) * val_0.df\n                           >= val_c.df)\n  approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature\n  is_satisfied = exact_wolfe | approx_wolfe\n  return is_satisfied", "language": "python", "code": "def _satisfies_wolfe(val_0,\n                     val_c,\n                     f_lim,\n                     sufficient_decrease_param,\n                     curvature_param):\n  \"\"\"Checks whether the Wolfe or approx Wolfe conditions are satisfied.\n\n  The Wolfe conditions are a set of stopping criteria for an inexact line search\n  algorithm. Let f(a) be the function value along the search direction and\n  df(a) the derivative along the search direction evaluated a distance 'a'.\n  Here 'a' is the distance along the search direction. The Wolfe conditions are:\n\n    ```None\n      f(a) <= f(0) + delta * a * df(0)   (Armijo/Sufficient decrease condition)\n      df(a) >= sigma * df(0)             (Weak curvature condition)\n    ```\n  `delta` and `sigma` are two user supplied parameters satisfying:\n   `0 < delta < sigma <= 1.`. In the following, delta is called\n   `sufficient_decrease_param` and sigma is called `curvature_param`.\n\n  On a finite precision machine, the Wolfe conditions are difficult to satisfy\n  when one is close to the minimum. Hence, Hager-Zhang propose replacing\n  the sufficient decrease condition with the following condition on the\n  derivative in the vicinity of a minimum.\n\n    ```None\n      df(a) <= (2 * delta - 1) * df(0)  (Approx Wolfe sufficient decrease)\n    ```\n  This condition is only used if one is near the minimum. This is tested using\n\n    ```None\n      f(a) <= f(0) + epsilon * |f(0)|\n    ```\n  The following function checks both the Wolfe and approx Wolfe conditions.\n  Here, `epsilon` is a small positive constant. In the following, the argument\n  `f_lim` corresponds to the product: epsilon * |f(0)|.\n\n  Args:\n    val_0: A namedtuple, as returned by value_and_gradients_function\n      evaluated at 0.\n    val_c: A namedtuple, as returned by value_and_gradients_function\n      evaluated at the point to be tested.\n    f_lim: Scalar `Tensor` of real dtype. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to 'delta' in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager Zhang (2005)][1].\n\n  Returns:\n    is_satisfied: A scalar boolean `Tensor` which is True if either the\n      Wolfe or approximate Wolfe conditions are satisfied.\n  \"\"\"\n  exact_wolfe_suff_dec = (sufficient_decrease_param * val_0.df >=\n                          (val_c.f - val_0.f) / val_c.x)\n  wolfe_curvature = val_c.df >= curvature_param * val_0.df\n  exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature\n  approx_wolfe_applies = val_c.f <= f_lim\n  approx_wolfe_suff_dec = ((2 * sufficient_decrease_param - 1) * val_0.df\n                           >= val_c.df)\n  approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature\n  is_satisfied = exact_wolfe | approx_wolfe\n  return is_satisfied", "code_tokens": ["def", "_satisfies_wolfe", "(", "val_0", ",", "val_c", ",", "f_lim", ",", "sufficient_decrease_param", ",", "curvature_param", ")", ":", "exact_wolfe_suff_dec", "=", "(", "sufficient_decrease_param", "*", "val_0", ".", "df", ">=", "(", "val_c", ".", "f", "-", "val_0", ".", "f", ")", "/", "val_c", ".", "x", ")", "wolfe_curvature", "=", "val_c", ".", "df", ">=", "curvature_param", "*", "val_0", ".", "df", "exact_wolfe", "=", "exact_wolfe_suff_dec", "&", "wolfe_curvature", "approx_wolfe_applies", "=", "val_c", ".", "f", "<=", "f_lim", "approx_wolfe_suff_dec", "=", "(", "(", "2", "*", "sufficient_decrease_param", "-", "1", ")", "*", "val_0", ".", "df", ">=", "val_c", ".", "df", ")", "approx_wolfe", "=", "approx_wolfe_applies", "&", "approx_wolfe_suff_dec", "&", "wolfe_curvature", "is_satisfied", "=", "exact_wolfe", "|", "approx_wolfe", "return", "is_satisfied"], "docstring": "Checks whether the Wolfe or approx Wolfe conditions are satisfied.\n\n  The Wolfe conditions are a set of stopping criteria for an inexact line search\n  algorithm. Let f(a) be the function value along the search direction and\n  df(a) the derivative along the search direction evaluated a distance 'a'.\n  Here 'a' is the distance along the search direction. The Wolfe conditions are:\n\n    ```None\n      f(a) <= f(0) + delta * a * df(0)   (Armijo/Sufficient decrease condition)\n      df(a) >= sigma * df(0)             (Weak curvature condition)\n    ```\n  `delta` and `sigma` are two user supplied parameters satisfying:\n   `0 < delta < sigma <= 1.`. In the following, delta is called\n   `sufficient_decrease_param` and sigma is called `curvature_param`.\n\n  On a finite precision machine, the Wolfe conditions are difficult to satisfy\n  when one is close to the minimum. Hence, Hager-Zhang propose replacing\n  the sufficient decrease condition with the following condition on the\n  derivative in the vicinity of a minimum.\n\n    ```None\n      df(a) <= (2 * delta - 1) * df(0)  (Approx Wolfe sufficient decrease)\n    ```\n  This condition is only used if one is near the minimum. This is tested using\n\n    ```None\n      f(a) <= f(0) + epsilon * |f(0)|\n    ```\n  The following function checks both the Wolfe and approx Wolfe conditions.\n  Here, `epsilon` is a small positive constant. In the following, the argument\n  `f_lim` corresponds to the product: epsilon * |f(0)|.\n\n  Args:\n    val_0: A namedtuple, as returned by value_and_gradients_function\n      evaluated at 0.\n    val_c: A namedtuple, as returned by value_and_gradients_function\n      evaluated at the point to be tested.\n    f_lim: Scalar `Tensor` of real dtype. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to 'delta' in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager Zhang (2005)][1].\n\n  Returns:\n    is_satisfied: A scalar boolean `Tensor` which is True if either the\n      Wolfe or approximate Wolfe conditions are satisfied.", "docstring_tokens": ["Checks", "whether", "the", "Wolfe", "or", "approx", "Wolfe", "conditions", "are", "satisfied", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py#L666-L730", "partition": "test", "index": 1003, "time": "2019-02-26 06:40:26"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py", "func_name": "secant2", "original_string": "def secant2(value_and_gradients_function,\n            val_0,\n            search_interval,\n            f_lim,\n            sufficient_decrease_param=0.1,\n            curvature_param=0.9,\n            name=None):\n  \"\"\"Performs the secant square procedure of Hager Zhang.\n\n  Given an interval that brackets a root, this procedure performs an update of\n  both end points using two intermediate points generated using the secant\n  interpolation. For details see the steps S1-S4 in [Hager and Zhang (2006)][2].\n\n  The interval [a, b] must satisfy the opposite slope conditions described in\n  the documentation for `update`.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns an object that can be converted to a namedtuple.\n      The namedtuple should have fields 'f' and 'df' that correspond to scalar\n      tensors of real dtype containing the value of the function and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      function values and derivatives at the input points.\n    val_0: A namedtuple, as returned by value_and_gradients_function evaluated\n      at `0.`.\n    search_interval: A namedtuple describing the current search interval,\n      must include the fields:\n      - converged: Boolean `Tensor` of shape [n], indicating batch members\n          where search has already converged. Interval for these batch members\n          won't be modified.\n      - failed: Boolean `Tensor` of shape [n], indicating batch members\n          where search has already failed. Interval for these batch members\n          wont be modified.\n      - iterations: Scalar int32 `Tensor`. Number of line search iterations\n          so far.\n      - func_evals: Scalar int32 `Tensor`. Number of function evaluations\n          so far.\n      - left: A namedtuple, as returned by value_and_gradients_function,\n          of the left end point of the current search interval.\n      - right: A namedtuple, as returned by value_and_gradients_function,\n          of the right end point of the current search interval.\n    f_lim: Scalar `Tensor` of real dtype. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to 'delta' in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'secant2' is used.\n\n  Returns:\n    A namedtuple containing the following fields.\n      active: A boolean `Tensor` of shape [n]. Used internally by the procedure\n        to indicate batch members on which there is work left to do.\n      converged: A boolean `Tensor` of shape [n]. Indicates whether a point\n        satisfying the Wolfe conditions has been found. If this is True, the\n        interval will be degenerate (i.e. `left` and `right` below will be\n        identical).\n      failed: A boolean `Tensor` of shape [n]. Indicates if invalid function or\n        gradient values were encountered (i.e. infinity or NaNs).\n      num_evals: A scalar int32 `Tensor`. The total number of function\n        evaluations made.\n      left: Return value of value_and_gradients_function at the updated left\n        end point of the interval.\n      right: Return value of value_and_gradients_function at the updated right\n        end point of the interval.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'secant2', [\n      val_0, search_interval, f_lim, sufficient_decrease_param,\n      curvature_param]):\n    # This will always be s.t. left <= c <= right\n    val_c = value_and_gradients_function(\n        _secant(search_interval.left, search_interval.right))\n    failed = search_interval.failed | ~is_finite(val_c)\n    converged = search_interval.converged | (~failed & _satisfies_wolfe(\n        val_0, val_c, f_lim, sufficient_decrease_param, curvature_param))\n    new_converged = converged & ~search_interval.converged\n    val_left = val_where(new_converged, val_c, search_interval.left)\n    val_right = val_where(new_converged, val_c, search_interval.right)\n\n    initial_args = _Secant2Result(\n        active=~failed & ~converged,\n        converged=converged,\n        failed=failed,\n        num_evals=search_interval.func_evals + 1,\n        left=val_left,\n        right=val_right)\n\n    def _apply_secant2_inner():\n      return _secant2_inner(\n          value_and_gradients_function,\n          initial_args,\n          val_0,\n          val_c,\n          f_lim,\n          sufficient_decrease_param,\n          curvature_param)\n\n    return prefer_static.cond(\n        tf.reduce_any(input_tensor=initial_args.active),\n        _apply_secant2_inner,\n        lambda: initial_args)", "language": "python", "code": "def secant2(value_and_gradients_function,\n            val_0,\n            search_interval,\n            f_lim,\n            sufficient_decrease_param=0.1,\n            curvature_param=0.9,\n            name=None):\n  \"\"\"Performs the secant square procedure of Hager Zhang.\n\n  Given an interval that brackets a root, this procedure performs an update of\n  both end points using two intermediate points generated using the secant\n  interpolation. For details see the steps S1-S4 in [Hager and Zhang (2006)][2].\n\n  The interval [a, b] must satisfy the opposite slope conditions described in\n  the documentation for `update`.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns an object that can be converted to a namedtuple.\n      The namedtuple should have fields 'f' and 'df' that correspond to scalar\n      tensors of real dtype containing the value of the function and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      function values and derivatives at the input points.\n    val_0: A namedtuple, as returned by value_and_gradients_function evaluated\n      at `0.`.\n    search_interval: A namedtuple describing the current search interval,\n      must include the fields:\n      - converged: Boolean `Tensor` of shape [n], indicating batch members\n          where search has already converged. Interval for these batch members\n          won't be modified.\n      - failed: Boolean `Tensor` of shape [n], indicating batch members\n          where search has already failed. Interval for these batch members\n          wont be modified.\n      - iterations: Scalar int32 `Tensor`. Number of line search iterations\n          so far.\n      - func_evals: Scalar int32 `Tensor`. Number of function evaluations\n          so far.\n      - left: A namedtuple, as returned by value_and_gradients_function,\n          of the left end point of the current search interval.\n      - right: A namedtuple, as returned by value_and_gradients_function,\n          of the right end point of the current search interval.\n    f_lim: Scalar `Tensor` of real dtype. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to 'delta' in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'secant2' is used.\n\n  Returns:\n    A namedtuple containing the following fields.\n      active: A boolean `Tensor` of shape [n]. Used internally by the procedure\n        to indicate batch members on which there is work left to do.\n      converged: A boolean `Tensor` of shape [n]. Indicates whether a point\n        satisfying the Wolfe conditions has been found. If this is True, the\n        interval will be degenerate (i.e. `left` and `right` below will be\n        identical).\n      failed: A boolean `Tensor` of shape [n]. Indicates if invalid function or\n        gradient values were encountered (i.e. infinity or NaNs).\n      num_evals: A scalar int32 `Tensor`. The total number of function\n        evaluations made.\n      left: Return value of value_and_gradients_function at the updated left\n        end point of the interval.\n      right: Return value of value_and_gradients_function at the updated right\n        end point of the interval.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'secant2', [\n      val_0, search_interval, f_lim, sufficient_decrease_param,\n      curvature_param]):\n    # This will always be s.t. left <= c <= right\n    val_c = value_and_gradients_function(\n        _secant(search_interval.left, search_interval.right))\n    failed = search_interval.failed | ~is_finite(val_c)\n    converged = search_interval.converged | (~failed & _satisfies_wolfe(\n        val_0, val_c, f_lim, sufficient_decrease_param, curvature_param))\n    new_converged = converged & ~search_interval.converged\n    val_left = val_where(new_converged, val_c, search_interval.left)\n    val_right = val_where(new_converged, val_c, search_interval.right)\n\n    initial_args = _Secant2Result(\n        active=~failed & ~converged,\n        converged=converged,\n        failed=failed,\n        num_evals=search_interval.func_evals + 1,\n        left=val_left,\n        right=val_right)\n\n    def _apply_secant2_inner():\n      return _secant2_inner(\n          value_and_gradients_function,\n          initial_args,\n          val_0,\n          val_c,\n          f_lim,\n          sufficient_decrease_param,\n          curvature_param)\n\n    return prefer_static.cond(\n        tf.reduce_any(input_tensor=initial_args.active),\n        _apply_secant2_inner,\n        lambda: initial_args)", "code_tokens": ["def", "secant2", "(", "value_and_gradients_function", ",", "val_0", ",", "search_interval", ",", "f_lim", ",", "sufficient_decrease_param", "=", "0.1", ",", "curvature_param", "=", "0.9", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'secant2'", ",", "[", "val_0", ",", "search_interval", ",", "f_lim", ",", "sufficient_decrease_param", ",", "curvature_param", "]", ")", ":", "# This will always be s.t. left <= c <= right", "val_c", "=", "value_and_gradients_function", "(", "_secant", "(", "search_interval", ".", "left", ",", "search_interval", ".", "right", ")", ")", "failed", "=", "search_interval", ".", "failed", "|", "~", "is_finite", "(", "val_c", ")", "converged", "=", "search_interval", ".", "converged", "|", "(", "~", "failed", "&", "_satisfies_wolfe", "(", "val_0", ",", "val_c", ",", "f_lim", ",", "sufficient_decrease_param", ",", "curvature_param", ")", ")", "new_converged", "=", "converged", "&", "~", "search_interval", ".", "converged", "val_left", "=", "val_where", "(", "new_converged", ",", "val_c", ",", "search_interval", ".", "left", ")", "val_right", "=", "val_where", "(", "new_converged", ",", "val_c", ",", "search_interval", ".", "right", ")", "initial_args", "=", "_Secant2Result", "(", "active", "=", "~", "failed", "&", "~", "converged", ",", "converged", "=", "converged", ",", "failed", "=", "failed", ",", "num_evals", "=", "search_interval", ".", "func_evals", "+", "1", ",", "left", "=", "val_left", ",", "right", "=", "val_right", ")", "def", "_apply_secant2_inner", "(", ")", ":", "return", "_secant2_inner", "(", "value_and_gradients_function", ",", "initial_args", ",", "val_0", ",", "val_c", ",", "f_lim", ",", "sufficient_decrease_param", ",", "curvature_param", ")", "return", "prefer_static", ".", "cond", "(", "tf", ".", "reduce_any", "(", "input_tensor", "=", "initial_args", ".", "active", ")", ",", "_apply_secant2_inner", ",", "lambda", ":", "initial_args", ")"], "docstring": "Performs the secant square procedure of Hager Zhang.\n\n  Given an interval that brackets a root, this procedure performs an update of\n  both end points using two intermediate points generated using the secant\n  interpolation. For details see the steps S1-S4 in [Hager and Zhang (2006)][2].\n\n  The interval [a, b] must satisfy the opposite slope conditions described in\n  the documentation for `update`.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns an object that can be converted to a namedtuple.\n      The namedtuple should have fields 'f' and 'df' that correspond to scalar\n      tensors of real dtype containing the value of the function and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      function values and derivatives at the input points.\n    val_0: A namedtuple, as returned by value_and_gradients_function evaluated\n      at `0.`.\n    search_interval: A namedtuple describing the current search interval,\n      must include the fields:\n      - converged: Boolean `Tensor` of shape [n], indicating batch members\n          where search has already converged. Interval for these batch members\n          won't be modified.\n      - failed: Boolean `Tensor` of shape [n], indicating batch members\n          where search has already failed. Interval for these batch members\n          wont be modified.\n      - iterations: Scalar int32 `Tensor`. Number of line search iterations\n          so far.\n      - func_evals: Scalar int32 `Tensor`. Number of function evaluations\n          so far.\n      - left: A namedtuple, as returned by value_and_gradients_function,\n          of the left end point of the current search interval.\n      - right: A namedtuple, as returned by value_and_gradients_function,\n          of the right end point of the current search interval.\n    f_lim: Scalar `Tensor` of real dtype. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to 'delta' in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'secant2' is used.\n\n  Returns:\n    A namedtuple containing the following fields.\n      active: A boolean `Tensor` of shape [n]. Used internally by the procedure\n        to indicate batch members on which there is work left to do.\n      converged: A boolean `Tensor` of shape [n]. Indicates whether a point\n        satisfying the Wolfe conditions has been found. If this is True, the\n        interval will be degenerate (i.e. `left` and `right` below will be\n        identical).\n      failed: A boolean `Tensor` of shape [n]. Indicates if invalid function or\n        gradient values were encountered (i.e. infinity or NaNs).\n      num_evals: A scalar int32 `Tensor`. The total number of function\n        evaluations made.\n      left: Return value of value_and_gradients_function at the updated left\n        end point of the interval.\n      right: Return value of value_and_gradients_function at the updated right\n        end point of the interval.", "docstring_tokens": ["Performs", "the", "secant", "square", "procedure", "of", "Hager", "Zhang", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py#L60-L174", "partition": "test", "index": 995, "time": "2019-02-26 06:40:26"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py", "func_name": "_secant2_inner", "original_string": "def _secant2_inner(value_and_gradients_function,\n                   initial_args,\n                   val_0,\n                   val_c,\n                   f_lim,\n                   sufficient_decrease_param,\n                   curvature_param):\n  \"\"\"Helper function for secant square.\"\"\"\n  # Apply the `update` function on active branch members to squeeze their\n  # bracketing interval.\n  update_result = update(value_and_gradients_function,\n                         initial_args.left,\n                         initial_args.right,\n                         val_c,\n                         f_lim,\n                         active=initial_args.active)\n\n  # Update active and failed flags, update left/right on non-failed entries.\n  active = initial_args.active & ~update_result.failed\n  failed = initial_args.failed | update_result.failed\n  val_left = val_where(active, update_result.left, initial_args.left)\n  val_right = val_where(active, update_result.right, initial_args.right)\n\n  # Check if new `c` points should be generated.\n  updated_left = active & tf.equal(val_left.x, val_c.x)\n  updated_right = active & tf.equal(val_right.x, val_c.x)\n  is_new = updated_left | updated_right\n\n  next_c = tf.where(updated_left,\n                    _secant(initial_args.left, val_left),\n                    val_c.x)\n  next_c = tf.where(updated_right,\n                    _secant(initial_args.right, val_right),\n                    next_c)\n  in_range = (val_left.x <= next_c) & (next_c <= val_right.x)\n\n  # Figure out if an extra function evaluation is needed for new `c` points.\n  needs_extra_eval = tf.reduce_any(input_tensor=in_range & is_new)\n  num_evals = initial_args.num_evals + update_result.num_evals\n  num_evals = num_evals + tf.cast(needs_extra_eval, num_evals.dtype)\n\n  next_args = _Secant2Result(\n      active=active & in_range,  # No longer active if `c` is out of range.\n      converged=initial_args.converged,\n      failed=failed,\n      num_evals=num_evals,\n      left=val_left,\n      right=val_right)\n\n  def _apply_inner_update():\n    next_val_c = prefer_static.cond(\n        needs_extra_eval,\n        (lambda: value_and_gradients_function(next_c)),\n        (lambda: val_c))\n    return _secant2_inner_update(\n        value_and_gradients_function, next_args, val_0, next_val_c, f_lim,\n        sufficient_decrease_param, curvature_param)\n\n  return prefer_static.cond(\n      tf.reduce_any(input_tensor=next_args.active),\n      _apply_inner_update,\n      lambda: next_args)", "language": "python", "code": "def _secant2_inner(value_and_gradients_function,\n                   initial_args,\n                   val_0,\n                   val_c,\n                   f_lim,\n                   sufficient_decrease_param,\n                   curvature_param):\n  \"\"\"Helper function for secant square.\"\"\"\n  # Apply the `update` function on active branch members to squeeze their\n  # bracketing interval.\n  update_result = update(value_and_gradients_function,\n                         initial_args.left,\n                         initial_args.right,\n                         val_c,\n                         f_lim,\n                         active=initial_args.active)\n\n  # Update active and failed flags, update left/right on non-failed entries.\n  active = initial_args.active & ~update_result.failed\n  failed = initial_args.failed | update_result.failed\n  val_left = val_where(active, update_result.left, initial_args.left)\n  val_right = val_where(active, update_result.right, initial_args.right)\n\n  # Check if new `c` points should be generated.\n  updated_left = active & tf.equal(val_left.x, val_c.x)\n  updated_right = active & tf.equal(val_right.x, val_c.x)\n  is_new = updated_left | updated_right\n\n  next_c = tf.where(updated_left,\n                    _secant(initial_args.left, val_left),\n                    val_c.x)\n  next_c = tf.where(updated_right,\n                    _secant(initial_args.right, val_right),\n                    next_c)\n  in_range = (val_left.x <= next_c) & (next_c <= val_right.x)\n\n  # Figure out if an extra function evaluation is needed for new `c` points.\n  needs_extra_eval = tf.reduce_any(input_tensor=in_range & is_new)\n  num_evals = initial_args.num_evals + update_result.num_evals\n  num_evals = num_evals + tf.cast(needs_extra_eval, num_evals.dtype)\n\n  next_args = _Secant2Result(\n      active=active & in_range,  # No longer active if `c` is out of range.\n      converged=initial_args.converged,\n      failed=failed,\n      num_evals=num_evals,\n      left=val_left,\n      right=val_right)\n\n  def _apply_inner_update():\n    next_val_c = prefer_static.cond(\n        needs_extra_eval,\n        (lambda: value_and_gradients_function(next_c)),\n        (lambda: val_c))\n    return _secant2_inner_update(\n        value_and_gradients_function, next_args, val_0, next_val_c, f_lim,\n        sufficient_decrease_param, curvature_param)\n\n  return prefer_static.cond(\n      tf.reduce_any(input_tensor=next_args.active),\n      _apply_inner_update,\n      lambda: next_args)", "code_tokens": ["def", "_secant2_inner", "(", "value_and_gradients_function", ",", "initial_args", ",", "val_0", ",", "val_c", ",", "f_lim", ",", "sufficient_decrease_param", ",", "curvature_param", ")", ":", "# Apply the `update` function on active branch members to squeeze their", "# bracketing interval.", "update_result", "=", "update", "(", "value_and_gradients_function", ",", "initial_args", ".", "left", ",", "initial_args", ".", "right", ",", "val_c", ",", "f_lim", ",", "active", "=", "initial_args", ".", "active", ")", "# Update active and failed flags, update left/right on non-failed entries.", "active", "=", "initial_args", ".", "active", "&", "~", "update_result", ".", "failed", "failed", "=", "initial_args", ".", "failed", "|", "update_result", ".", "failed", "val_left", "=", "val_where", "(", "active", ",", "update_result", ".", "left", ",", "initial_args", ".", "left", ")", "val_right", "=", "val_where", "(", "active", ",", "update_result", ".", "right", ",", "initial_args", ".", "right", ")", "# Check if new `c` points should be generated.", "updated_left", "=", "active", "&", "tf", ".", "equal", "(", "val_left", ".", "x", ",", "val_c", ".", "x", ")", "updated_right", "=", "active", "&", "tf", ".", "equal", "(", "val_right", ".", "x", ",", "val_c", ".", "x", ")", "is_new", "=", "updated_left", "|", "updated_right", "next_c", "=", "tf", ".", "where", "(", "updated_left", ",", "_secant", "(", "initial_args", ".", "left", ",", "val_left", ")", ",", "val_c", ".", "x", ")", "next_c", "=", "tf", ".", "where", "(", "updated_right", ",", "_secant", "(", "initial_args", ".", "right", ",", "val_right", ")", ",", "next_c", ")", "in_range", "=", "(", "val_left", ".", "x", "<=", "next_c", ")", "&", "(", "next_c", "<=", "val_right", ".", "x", ")", "# Figure out if an extra function evaluation is needed for new `c` points.", "needs_extra_eval", "=", "tf", ".", "reduce_any", "(", "input_tensor", "=", "in_range", "&", "is_new", ")", "num_evals", "=", "initial_args", ".", "num_evals", "+", "update_result", ".", "num_evals", "num_evals", "=", "num_evals", "+", "tf", ".", "cast", "(", "needs_extra_eval", ",", "num_evals", ".", "dtype", ")", "next_args", "=", "_Secant2Result", "(", "active", "=", "active", "&", "in_range", ",", "# No longer active if `c` is out of range.", "converged", "=", "initial_args", ".", "converged", ",", "failed", "=", "failed", ",", "num_evals", "=", "num_evals", ",", "left", "=", "val_left", ",", "right", "=", "val_right", ")", "def", "_apply_inner_update", "(", ")", ":", "next_val_c", "=", "prefer_static", ".", "cond", "(", "needs_extra_eval", ",", "(", "lambda", ":", "value_and_gradients_function", "(", "next_c", ")", ")", ",", "(", "lambda", ":", "val_c", ")", ")", "return", "_secant2_inner_update", "(", "value_and_gradients_function", ",", "next_args", ",", "val_0", ",", "next_val_c", ",", "f_lim", ",", "sufficient_decrease_param", ",", "curvature_param", ")", "return", "prefer_static", ".", "cond", "(", "tf", ".", "reduce_any", "(", "input_tensor", "=", "next_args", ".", "active", ")", ",", "_apply_inner_update", ",", "lambda", ":", "next_args", ")"], "docstring": "Helper function for secant square.", "docstring_tokens": ["Helper", "function", "for", "secant", "square", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py#L177-L238", "partition": "test", "index": 996, "time": "2019-02-26 06:40:26"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py", "func_name": "_secant2_inner_update", "original_string": "def _secant2_inner_update(value_and_gradients_function,\n                          initial_args,\n                          val_0,\n                          val_c,\n                          f_lim,\n                          sufficient_decrease_param,\n                          curvature_param):\n  \"\"\"Helper function for secant-square step.\"\"\"\n  # Fail if `val_c` is no longer finite.\n  new_failed = initial_args.active & ~is_finite(val_c)\n  active = initial_args.active & ~new_failed\n  failed = initial_args.failed | new_failed\n\n  # We converge when we find a point satisfying the Wolfe conditions, in those\n  # cases we set `val_left = val_right = val_c`.\n  found_wolfe = active & _satisfies_wolfe(\n      val_0, val_c, f_lim, sufficient_decrease_param, curvature_param)\n  val_left = val_where(found_wolfe, val_c, initial_args.left)\n  val_right = val_where(found_wolfe, val_c, initial_args.right)\n  converged = initial_args.converged | found_wolfe\n  active = active & ~found_wolfe\n\n  # If any active batch members remain, we apply the `update` function to\n  # squeeze further their corresponding left/right bracketing interval.\n  def _apply_update():\n    update_result = update(\n        value_and_gradients_function, val_left, val_right, val_c, f_lim,\n        active=active)\n    return _Secant2Result(\n        active=tf.zeros_like(active),  # End of secant2, no actives anymore.\n        converged=converged,\n        failed=failed | update_result.failed,\n        num_evals=initial_args.num_evals + update_result.num_evals,\n        left=update_result.left,\n        right=update_result.right)\n\n  # Otherwise just return the current results.\n  def _default():\n    return _Secant2Result(\n        active=active,\n        converged=converged,\n        failed=failed,\n        num_evals=initial_args.num_evals,\n        left=val_left,\n        right=val_right)\n\n  return prefer_static.cond(\n      tf.reduce_any(input_tensor=active), _apply_update, _default)", "language": "python", "code": "def _secant2_inner_update(value_and_gradients_function,\n                          initial_args,\n                          val_0,\n                          val_c,\n                          f_lim,\n                          sufficient_decrease_param,\n                          curvature_param):\n  \"\"\"Helper function for secant-square step.\"\"\"\n  # Fail if `val_c` is no longer finite.\n  new_failed = initial_args.active & ~is_finite(val_c)\n  active = initial_args.active & ~new_failed\n  failed = initial_args.failed | new_failed\n\n  # We converge when we find a point satisfying the Wolfe conditions, in those\n  # cases we set `val_left = val_right = val_c`.\n  found_wolfe = active & _satisfies_wolfe(\n      val_0, val_c, f_lim, sufficient_decrease_param, curvature_param)\n  val_left = val_where(found_wolfe, val_c, initial_args.left)\n  val_right = val_where(found_wolfe, val_c, initial_args.right)\n  converged = initial_args.converged | found_wolfe\n  active = active & ~found_wolfe\n\n  # If any active batch members remain, we apply the `update` function to\n  # squeeze further their corresponding left/right bracketing interval.\n  def _apply_update():\n    update_result = update(\n        value_and_gradients_function, val_left, val_right, val_c, f_lim,\n        active=active)\n    return _Secant2Result(\n        active=tf.zeros_like(active),  # End of secant2, no actives anymore.\n        converged=converged,\n        failed=failed | update_result.failed,\n        num_evals=initial_args.num_evals + update_result.num_evals,\n        left=update_result.left,\n        right=update_result.right)\n\n  # Otherwise just return the current results.\n  def _default():\n    return _Secant2Result(\n        active=active,\n        converged=converged,\n        failed=failed,\n        num_evals=initial_args.num_evals,\n        left=val_left,\n        right=val_right)\n\n  return prefer_static.cond(\n      tf.reduce_any(input_tensor=active), _apply_update, _default)", "code_tokens": ["def", "_secant2_inner_update", "(", "value_and_gradients_function", ",", "initial_args", ",", "val_0", ",", "val_c", ",", "f_lim", ",", "sufficient_decrease_param", ",", "curvature_param", ")", ":", "# Fail if `val_c` is no longer finite.", "new_failed", "=", "initial_args", ".", "active", "&", "~", "is_finite", "(", "val_c", ")", "active", "=", "initial_args", ".", "active", "&", "~", "new_failed", "failed", "=", "initial_args", ".", "failed", "|", "new_failed", "# We converge when we find a point satisfying the Wolfe conditions, in those", "# cases we set `val_left = val_right = val_c`.", "found_wolfe", "=", "active", "&", "_satisfies_wolfe", "(", "val_0", ",", "val_c", ",", "f_lim", ",", "sufficient_decrease_param", ",", "curvature_param", ")", "val_left", "=", "val_where", "(", "found_wolfe", ",", "val_c", ",", "initial_args", ".", "left", ")", "val_right", "=", "val_where", "(", "found_wolfe", ",", "val_c", ",", "initial_args", ".", "right", ")", "converged", "=", "initial_args", ".", "converged", "|", "found_wolfe", "active", "=", "active", "&", "~", "found_wolfe", "# If any active batch members remain, we apply the `update` function to", "# squeeze further their corresponding left/right bracketing interval.", "def", "_apply_update", "(", ")", ":", "update_result", "=", "update", "(", "value_and_gradients_function", ",", "val_left", ",", "val_right", ",", "val_c", ",", "f_lim", ",", "active", "=", "active", ")", "return", "_Secant2Result", "(", "active", "=", "tf", ".", "zeros_like", "(", "active", ")", ",", "# End of secant2, no actives anymore.", "converged", "=", "converged", ",", "failed", "=", "failed", "|", "update_result", ".", "failed", ",", "num_evals", "=", "initial_args", ".", "num_evals", "+", "update_result", ".", "num_evals", ",", "left", "=", "update_result", ".", "left", ",", "right", "=", "update_result", ".", "right", ")", "# Otherwise just return the current results.", "def", "_default", "(", ")", ":", "return", "_Secant2Result", "(", "active", "=", "active", ",", "converged", "=", "converged", ",", "failed", "=", "failed", ",", "num_evals", "=", "initial_args", ".", "num_evals", ",", "left", "=", "val_left", ",", "right", "=", "val_right", ")", "return", "prefer_static", ".", "cond", "(", "tf", ".", "reduce_any", "(", "input_tensor", "=", "active", ")", ",", "_apply_update", ",", "_default", ")"], "docstring": "Helper function for secant-square step.", "docstring_tokens": ["Helper", "function", "for", "secant", "-", "square", "step", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py#L241-L288", "partition": "test", "index": 997, "time": "2019-02-26 06:40:26"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py", "func_name": "_secant", "original_string": "def _secant(val_a, val_b):\n  \"\"\"Returns the secant interpolation for the minimum.\n\n  The secant method is a technique for finding roots of nonlinear functions.\n  When finding the minimum, one applies the secant method to the derivative\n  of the function.\n  For an arbitrary function and a bounding interval, the secant approximation\n  can produce the next point which is outside the bounding interval. However,\n  with the assumption of opposite slope condtion on the interval [a,b] the new\n  point c is always bracketed by [a,b]. Note that by assumption,\n  f'(a) < 0 and f'(b) > 0.\n  Hence c is a weighted average of a and b and thus always in [a, b].\n\n  Args:\n    val_a: A namedtuple with the left end point, function value and derivative,\n      of the current interval (i.e. a).\n    val_b: A namedtuple with the right end point, function value and derivative,\n      of the current interval (i.e. b).\n\n  Returns:\n    approx_minimum: A scalar real `Tensor`. An approximation to the point\n      at which the derivative vanishes.\n  \"\"\"\n  return (val_a.x * val_b.df - val_b.x * val_a.df) / (val_b.df - val_a.df)", "language": "python", "code": "def _secant(val_a, val_b):\n  \"\"\"Returns the secant interpolation for the minimum.\n\n  The secant method is a technique for finding roots of nonlinear functions.\n  When finding the minimum, one applies the secant method to the derivative\n  of the function.\n  For an arbitrary function and a bounding interval, the secant approximation\n  can produce the next point which is outside the bounding interval. However,\n  with the assumption of opposite slope condtion on the interval [a,b] the new\n  point c is always bracketed by [a,b]. Note that by assumption,\n  f'(a) < 0 and f'(b) > 0.\n  Hence c is a weighted average of a and b and thus always in [a, b].\n\n  Args:\n    val_a: A namedtuple with the left end point, function value and derivative,\n      of the current interval (i.e. a).\n    val_b: A namedtuple with the right end point, function value and derivative,\n      of the current interval (i.e. b).\n\n  Returns:\n    approx_minimum: A scalar real `Tensor`. An approximation to the point\n      at which the derivative vanishes.\n  \"\"\"\n  return (val_a.x * val_b.df - val_b.x * val_a.df) / (val_b.df - val_a.df)", "code_tokens": ["def", "_secant", "(", "val_a", ",", "val_b", ")", ":", "return", "(", "val_a", ".", "x", "*", "val_b", ".", "df", "-", "val_b", ".", "x", "*", "val_a", ".", "df", ")", "/", "(", "val_b", ".", "df", "-", "val_a", ".", "df", ")"], "docstring": "Returns the secant interpolation for the minimum.\n\n  The secant method is a technique for finding roots of nonlinear functions.\n  When finding the minimum, one applies the secant method to the derivative\n  of the function.\n  For an arbitrary function and a bounding interval, the secant approximation\n  can produce the next point which is outside the bounding interval. However,\n  with the assumption of opposite slope condtion on the interval [a,b] the new\n  point c is always bracketed by [a,b]. Note that by assumption,\n  f'(a) < 0 and f'(b) > 0.\n  Hence c is a weighted average of a and b and thus always in [a, b].\n\n  Args:\n    val_a: A namedtuple with the left end point, function value and derivative,\n      of the current interval (i.e. a).\n    val_b: A namedtuple with the right end point, function value and derivative,\n      of the current interval (i.e. b).\n\n  Returns:\n    approx_minimum: A scalar real `Tensor`. An approximation to the point\n      at which the derivative vanishes.", "docstring_tokens": ["Returns", "the", "secant", "interpolation", "for", "the", "minimum", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py#L733-L756", "partition": "test", "index": 1004, "time": "2019-02-26 06:40:26"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/blockwise.py", "func_name": "_maybe_validate_distributions", "original_string": "def _maybe_validate_distributions(distributions, dtype_override, validate_args):\n  \"\"\"Checks that `distributions` satisfies all assumptions.\"\"\"\n  assertions = []\n\n  if not _is_iterable(distributions) or not distributions:\n    raise ValueError('`distributions` must be a list of one or more '\n                     'distributions.')\n\n  if dtype_override is None:\n    dts = [\n        dtype_util.base_dtype(d.dtype)\n        for d in distributions\n        if d.dtype is not None\n    ]\n    if dts[1:] != dts[:-1]:\n      raise TypeError('Distributions must have same dtype; found: {}.'.format(\n          set(dtype_util.name(dt) for dt in dts)))\n\n  # Validate event_ndims.\n  for d in distributions:\n    if tensorshape_util.rank(d.event_shape) is not None:\n      if tensorshape_util.rank(d.event_shape) != 1:\n        raise ValueError('`Distribution` must be vector variate, '\n                         'found event nimds: {}.'.format(\n                             tensorshape_util.rank(d.event_shape)))\n    elif validate_args:\n      assertions.append(\n          assert_util.assert_equal(\n              1, tf.size(input=d.event_shape_tensor()),\n              message='`Distribution` must be vector variate.'))\n\n  batch_shapes = [d.batch_shape for d in distributions]\n  if all(tensorshape_util.is_fully_defined(b) for b in batch_shapes):\n    if batch_shapes[1:] != batch_shapes[:-1]:\n      raise ValueError('Distributions must have the same `batch_shape`; '\n                       'found: {}.'.format(batch_shapes))\n  elif validate_args:\n    batch_shapes = [\n        tensorshape_util.as_list(d.batch_shape)  # pylint: disable=g-complex-comprehension\n        if tensorshape_util.is_fully_defined(d.batch_shape) else\n        d.batch_shape_tensor() for d in distributions\n    ]\n    assertions.extend(\n        assert_util.assert_equal(  # pylint: disable=g-complex-comprehension\n            b1, b2,\n            message='Distribution `batch_shape`s must be identical.')\n        for b1, b2 in zip(batch_shapes[1:], batch_shapes[:-1]))\n\n  return assertions", "language": "python", "code": "def _maybe_validate_distributions(distributions, dtype_override, validate_args):\n  \"\"\"Checks that `distributions` satisfies all assumptions.\"\"\"\n  assertions = []\n\n  if not _is_iterable(distributions) or not distributions:\n    raise ValueError('`distributions` must be a list of one or more '\n                     'distributions.')\n\n  if dtype_override is None:\n    dts = [\n        dtype_util.base_dtype(d.dtype)\n        for d in distributions\n        if d.dtype is not None\n    ]\n    if dts[1:] != dts[:-1]:\n      raise TypeError('Distributions must have same dtype; found: {}.'.format(\n          set(dtype_util.name(dt) for dt in dts)))\n\n  # Validate event_ndims.\n  for d in distributions:\n    if tensorshape_util.rank(d.event_shape) is not None:\n      if tensorshape_util.rank(d.event_shape) != 1:\n        raise ValueError('`Distribution` must be vector variate, '\n                         'found event nimds: {}.'.format(\n                             tensorshape_util.rank(d.event_shape)))\n    elif validate_args:\n      assertions.append(\n          assert_util.assert_equal(\n              1, tf.size(input=d.event_shape_tensor()),\n              message='`Distribution` must be vector variate.'))\n\n  batch_shapes = [d.batch_shape for d in distributions]\n  if all(tensorshape_util.is_fully_defined(b) for b in batch_shapes):\n    if batch_shapes[1:] != batch_shapes[:-1]:\n      raise ValueError('Distributions must have the same `batch_shape`; '\n                       'found: {}.'.format(batch_shapes))\n  elif validate_args:\n    batch_shapes = [\n        tensorshape_util.as_list(d.batch_shape)  # pylint: disable=g-complex-comprehension\n        if tensorshape_util.is_fully_defined(d.batch_shape) else\n        d.batch_shape_tensor() for d in distributions\n    ]\n    assertions.extend(\n        assert_util.assert_equal(  # pylint: disable=g-complex-comprehension\n            b1, b2,\n            message='Distribution `batch_shape`s must be identical.')\n        for b1, b2 in zip(batch_shapes[1:], batch_shapes[:-1]))\n\n  return assertions", "code_tokens": ["def", "_maybe_validate_distributions", "(", "distributions", ",", "dtype_override", ",", "validate_args", ")", ":", "assertions", "=", "[", "]", "if", "not", "_is_iterable", "(", "distributions", ")", "or", "not", "distributions", ":", "raise", "ValueError", "(", "'`distributions` must be a list of one or more '", "'distributions.'", ")", "if", "dtype_override", "is", "None", ":", "dts", "=", "[", "dtype_util", ".", "base_dtype", "(", "d", ".", "dtype", ")", "for", "d", "in", "distributions", "if", "d", ".", "dtype", "is", "not", "None", "]", "if", "dts", "[", "1", ":", "]", "!=", "dts", "[", ":", "-", "1", "]", ":", "raise", "TypeError", "(", "'Distributions must have same dtype; found: {}.'", ".", "format", "(", "set", "(", "dtype_util", ".", "name", "(", "dt", ")", "for", "dt", "in", "dts", ")", ")", ")", "# Validate event_ndims.", "for", "d", "in", "distributions", ":", "if", "tensorshape_util", ".", "rank", "(", "d", ".", "event_shape", ")", "is", "not", "None", ":", "if", "tensorshape_util", ".", "rank", "(", "d", ".", "event_shape", ")", "!=", "1", ":", "raise", "ValueError", "(", "'`Distribution` must be vector variate, '", "'found event nimds: {}.'", ".", "format", "(", "tensorshape_util", ".", "rank", "(", "d", ".", "event_shape", ")", ")", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "assert_util", ".", "assert_equal", "(", "1", ",", "tf", ".", "size", "(", "input", "=", "d", ".", "event_shape_tensor", "(", ")", ")", ",", "message", "=", "'`Distribution` must be vector variate.'", ")", ")", "batch_shapes", "=", "[", "d", ".", "batch_shape", "for", "d", "in", "distributions", "]", "if", "all", "(", "tensorshape_util", ".", "is_fully_defined", "(", "b", ")", "for", "b", "in", "batch_shapes", ")", ":", "if", "batch_shapes", "[", "1", ":", "]", "!=", "batch_shapes", "[", ":", "-", "1", "]", ":", "raise", "ValueError", "(", "'Distributions must have the same `batch_shape`; '", "'found: {}.'", ".", "format", "(", "batch_shapes", ")", ")", "elif", "validate_args", ":", "batch_shapes", "=", "[", "tensorshape_util", ".", "as_list", "(", "d", ".", "batch_shape", ")", "# pylint: disable=g-complex-comprehension", "if", "tensorshape_util", ".", "is_fully_defined", "(", "d", ".", "batch_shape", ")", "else", "d", ".", "batch_shape_tensor", "(", ")", "for", "d", "in", "distributions", "]", "assertions", ".", "extend", "(", "assert_util", ".", "assert_equal", "(", "# pylint: disable=g-complex-comprehension", "b1", ",", "b2", ",", "message", "=", "'Distribution `batch_shape`s must be identical.'", ")", "for", "b1", ",", "b2", "in", "zip", "(", "batch_shapes", "[", "1", ":", "]", ",", "batch_shapes", "[", ":", "-", "1", "]", ")", ")", "return", "assertions"], "docstring": "Checks that `distributions` satisfies all assumptions.", "docstring_tokens": ["Checks", "that", "distributions", "satisfies", "all", "assumptions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/blockwise.py#L177-L225", "partition": "test", "index": 1028, "time": "2019-02-28 14:30:01"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/linalg.py", "func_name": "matrix_rank", "original_string": "def matrix_rank(a, tol=None, validate_args=False, name=None):\n  \"\"\"Compute the matrix rank; the number of non-zero SVD singular values.\n\n  Arguments:\n    a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be\n      pseudo-inverted.\n    tol: Threshold below which the singular value is counted as \"zero\".\n      Default value: `None` (i.e., `eps * max(rows, cols) * max(singular_val)`).\n    validate_args: When `True`, additional assertions might be embedded in the\n      graph.\n      Default value: `False` (i.e., no graph assertions are added).\n    name: Python `str` prefixed to ops created by this function.\n      Default value: \"matrix_rank\".\n\n  Returns:\n    matrix_rank: (Batch of) `int32` scalars representing the number of non-zero\n      singular values.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'matrix_rank', [a, tol]):\n    a = tf.convert_to_tensor(value=a, dtype_hint=tf.float32, name='a')\n    assertions = _maybe_validate_matrix(a, validate_args)\n    if assertions:\n      with tf.control_dependencies(assertions):\n        a = tf.identity(a)\n    s = tf.linalg.svd(a, compute_uv=False)\n    if tol is None:\n      if a.shape[-2:].is_fully_defined():\n        m = np.max(a.shape[-2:].as_list())\n      else:\n        m = tf.reduce_max(input_tensor=tf.shape(input=a)[-2:])\n      eps = np.finfo(a.dtype.as_numpy_dtype).eps\n      tol = (eps * tf.cast(m, a.dtype) *\n             tf.reduce_max(input_tensor=s, axis=-1, keepdims=True))\n    return tf.reduce_sum(input_tensor=tf.cast(s > tol, tf.int32), axis=-1)", "language": "python", "code": "def matrix_rank(a, tol=None, validate_args=False, name=None):\n  \"\"\"Compute the matrix rank; the number of non-zero SVD singular values.\n\n  Arguments:\n    a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be\n      pseudo-inverted.\n    tol: Threshold below which the singular value is counted as \"zero\".\n      Default value: `None` (i.e., `eps * max(rows, cols) * max(singular_val)`).\n    validate_args: When `True`, additional assertions might be embedded in the\n      graph.\n      Default value: `False` (i.e., no graph assertions are added).\n    name: Python `str` prefixed to ops created by this function.\n      Default value: \"matrix_rank\".\n\n  Returns:\n    matrix_rank: (Batch of) `int32` scalars representing the number of non-zero\n      singular values.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'matrix_rank', [a, tol]):\n    a = tf.convert_to_tensor(value=a, dtype_hint=tf.float32, name='a')\n    assertions = _maybe_validate_matrix(a, validate_args)\n    if assertions:\n      with tf.control_dependencies(assertions):\n        a = tf.identity(a)\n    s = tf.linalg.svd(a, compute_uv=False)\n    if tol is None:\n      if a.shape[-2:].is_fully_defined():\n        m = np.max(a.shape[-2:].as_list())\n      else:\n        m = tf.reduce_max(input_tensor=tf.shape(input=a)[-2:])\n      eps = np.finfo(a.dtype.as_numpy_dtype).eps\n      tol = (eps * tf.cast(m, a.dtype) *\n             tf.reduce_max(input_tensor=s, axis=-1, keepdims=True))\n    return tf.reduce_sum(input_tensor=tf.cast(s > tol, tf.int32), axis=-1)", "code_tokens": ["def", "matrix_rank", "(", "a", ",", "tol", "=", "None", ",", "validate_args", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'matrix_rank'", ",", "[", "a", ",", "tol", "]", ")", ":", "a", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "a", ",", "dtype_hint", "=", "tf", ".", "float32", ",", "name", "=", "'a'", ")", "assertions", "=", "_maybe_validate_matrix", "(", "a", ",", "validate_args", ")", "if", "assertions", ":", "with", "tf", ".", "control_dependencies", "(", "assertions", ")", ":", "a", "=", "tf", ".", "identity", "(", "a", ")", "s", "=", "tf", ".", "linalg", ".", "svd", "(", "a", ",", "compute_uv", "=", "False", ")", "if", "tol", "is", "None", ":", "if", "a", ".", "shape", "[", "-", "2", ":", "]", ".", "is_fully_defined", "(", ")", ":", "m", "=", "np", ".", "max", "(", "a", ".", "shape", "[", "-", "2", ":", "]", ".", "as_list", "(", ")", ")", "else", ":", "m", "=", "tf", ".", "reduce_max", "(", "input_tensor", "=", "tf", ".", "shape", "(", "input", "=", "a", ")", "[", "-", "2", ":", "]", ")", "eps", "=", "np", ".", "finfo", "(", "a", ".", "dtype", ".", "as_numpy_dtype", ")", ".", "eps", "tol", "=", "(", "eps", "*", "tf", ".", "cast", "(", "m", ",", "a", ".", "dtype", ")", "*", "tf", ".", "reduce_max", "(", "input_tensor", "=", "s", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", ")", "return", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "cast", "(", "s", ">", "tol", ",", "tf", ".", "int32", ")", ",", "axis", "=", "-", "1", ")"], "docstring": "Compute the matrix rank; the number of non-zero SVD singular values.\n\n  Arguments:\n    a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be\n      pseudo-inverted.\n    tol: Threshold below which the singular value is counted as \"zero\".\n      Default value: `None` (i.e., `eps * max(rows, cols) * max(singular_val)`).\n    validate_args: When `True`, additional assertions might be embedded in the\n      graph.\n      Default value: `False` (i.e., no graph assertions are added).\n    name: Python `str` prefixed to ops created by this function.\n      Default value: \"matrix_rank\".\n\n  Returns:\n    matrix_rank: (Batch of) `int32` scalars representing the number of non-zero\n      singular values.", "docstring_tokens": ["Compute", "the", "matrix", "rank", ";", "the", "number", "of", "non", "-", "zero", "SVD", "singular", "values", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/linalg.py#L48-L81", "partition": "test", "index": 1105, "time": "2019-03-04 13:29:31"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/linalg.py", "func_name": "_maybe_validate_matrix", "original_string": "def _maybe_validate_matrix(a, validate_args):\n  \"\"\"Checks that input is a `float` matrix.\"\"\"\n  assertions = []\n  if not a.dtype.is_floating:\n    raise TypeError('Input `a` must have `float`-like `dtype` '\n                    '(saw {}).'.format(a.dtype.name))\n  if a.shape.ndims is not None:\n    if a.shape.ndims < 2:\n      raise ValueError('Input `a` must have at least 2 dimensions '\n                       '(saw: {}).'.format(a.shape.ndims))\n  elif validate_args:\n    assertions.append(tf.compat.v1.assert_rank_at_least(\n        a, rank=2, message='Input `a` must have at least 2 dimensions.'))\n  return assertions", "language": "python", "code": "def _maybe_validate_matrix(a, validate_args):\n  \"\"\"Checks that input is a `float` matrix.\"\"\"\n  assertions = []\n  if not a.dtype.is_floating:\n    raise TypeError('Input `a` must have `float`-like `dtype` '\n                    '(saw {}).'.format(a.dtype.name))\n  if a.shape.ndims is not None:\n    if a.shape.ndims < 2:\n      raise ValueError('Input `a` must have at least 2 dimensions '\n                       '(saw: {}).'.format(a.shape.ndims))\n  elif validate_args:\n    assertions.append(tf.compat.v1.assert_rank_at_least(\n        a, rank=2, message='Input `a` must have at least 2 dimensions.'))\n  return assertions", "code_tokens": ["def", "_maybe_validate_matrix", "(", "a", ",", "validate_args", ")", ":", "assertions", "=", "[", "]", "if", "not", "a", ".", "dtype", ".", "is_floating", ":", "raise", "TypeError", "(", "'Input `a` must have `float`-like `dtype` '", "'(saw {}).'", ".", "format", "(", "a", ".", "dtype", ".", "name", ")", ")", "if", "a", ".", "shape", ".", "ndims", "is", "not", "None", ":", "if", "a", ".", "shape", ".", "ndims", "<", "2", ":", "raise", "ValueError", "(", "'Input `a` must have at least 2 dimensions '", "'(saw: {}).'", ".", "format", "(", "a", ".", "shape", ".", "ndims", ")", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "tf", ".", "compat", ".", "v1", ".", "assert_rank_at_least", "(", "a", ",", "rank", "=", "2", ",", "message", "=", "'Input `a` must have at least 2 dimensions.'", ")", ")", "return", "assertions"], "docstring": "Checks that input is a `float` matrix.", "docstring_tokens": ["Checks", "that", "input", "is", "a", "float", "matrix", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/linalg.py#L898-L911", "partition": "test", "index": 1112, "time": "2019-03-04 13:29:31"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/transpose.py", "func_name": "Transpose._event_shape", "original_string": "def _event_shape(self, shape, static_perm_to_shape):\n    \"\"\"Helper for _forward and _inverse_event_shape.\"\"\"\n    rightmost_ = tf.get_static_value(self.rightmost_transposed_ndims)\n    if tensorshape_util.rank(shape) is None or rightmost_ is None:\n      return tf.TensorShape(None)\n    if tensorshape_util.rank(shape) < rightmost_:\n      raise ValueError('Invalid shape: min event ndims={} but got {}'.format(\n          rightmost_, shape))\n    perm_ = tf.get_static_value(self.perm, partial=True)\n    if perm_ is None:\n      return shape[:tensorshape_util.rank(shape) - rightmost_].concatenate(\n          [None] * int(rightmost_))\n    # We can use elimination to reidentify a single None dimension.\n    if sum(p is None for p in perm_) == 1:\n      present = np.argsort([-1 if p is None else p for p in perm_])\n      for i, p in enumerate(present[1:]):  # The -1 sorts to position 0.\n        if i != p:\n          perm_ = [i if p is None else p for p in perm_]\n          break\n    return shape[:tensorshape_util.rank(shape) - rightmost_].concatenate(\n        static_perm_to_shape(shape[tensorshape_util.rank(shape) - rightmost_:],\n                             perm_))", "language": "python", "code": "def _event_shape(self, shape, static_perm_to_shape):\n    \"\"\"Helper for _forward and _inverse_event_shape.\"\"\"\n    rightmost_ = tf.get_static_value(self.rightmost_transposed_ndims)\n    if tensorshape_util.rank(shape) is None or rightmost_ is None:\n      return tf.TensorShape(None)\n    if tensorshape_util.rank(shape) < rightmost_:\n      raise ValueError('Invalid shape: min event ndims={} but got {}'.format(\n          rightmost_, shape))\n    perm_ = tf.get_static_value(self.perm, partial=True)\n    if perm_ is None:\n      return shape[:tensorshape_util.rank(shape) - rightmost_].concatenate(\n          [None] * int(rightmost_))\n    # We can use elimination to reidentify a single None dimension.\n    if sum(p is None for p in perm_) == 1:\n      present = np.argsort([-1 if p is None else p for p in perm_])\n      for i, p in enumerate(present[1:]):  # The -1 sorts to position 0.\n        if i != p:\n          perm_ = [i if p is None else p for p in perm_]\n          break\n    return shape[:tensorshape_util.rank(shape) - rightmost_].concatenate(\n        static_perm_to_shape(shape[tensorshape_util.rank(shape) - rightmost_:],\n                             perm_))", "code_tokens": ["def", "_event_shape", "(", "self", ",", "shape", ",", "static_perm_to_shape", ")", ":", "rightmost_", "=", "tf", ".", "get_static_value", "(", "self", ".", "rightmost_transposed_ndims", ")", "if", "tensorshape_util", ".", "rank", "(", "shape", ")", "is", "None", "or", "rightmost_", "is", "None", ":", "return", "tf", ".", "TensorShape", "(", "None", ")", "if", "tensorshape_util", ".", "rank", "(", "shape", ")", "<", "rightmost_", ":", "raise", "ValueError", "(", "'Invalid shape: min event ndims={} but got {}'", ".", "format", "(", "rightmost_", ",", "shape", ")", ")", "perm_", "=", "tf", ".", "get_static_value", "(", "self", ".", "perm", ",", "partial", "=", "True", ")", "if", "perm_", "is", "None", ":", "return", "shape", "[", ":", "tensorshape_util", ".", "rank", "(", "shape", ")", "-", "rightmost_", "]", ".", "concatenate", "(", "[", "None", "]", "*", "int", "(", "rightmost_", ")", ")", "# We can use elimination to reidentify a single None dimension.", "if", "sum", "(", "p", "is", "None", "for", "p", "in", "perm_", ")", "==", "1", ":", "present", "=", "np", ".", "argsort", "(", "[", "-", "1", "if", "p", "is", "None", "else", "p", "for", "p", "in", "perm_", "]", ")", "for", "i", ",", "p", "in", "enumerate", "(", "present", "[", "1", ":", "]", ")", ":", "# The -1 sorts to position 0.", "if", "i", "!=", "p", ":", "perm_", "=", "[", "i", "if", "p", "is", "None", "else", "p", "for", "p", "in", "perm_", "]", "break", "return", "shape", "[", ":", "tensorshape_util", ".", "rank", "(", "shape", ")", "-", "rightmost_", "]", ".", "concatenate", "(", "static_perm_to_shape", "(", "shape", "[", "tensorshape_util", ".", "rank", "(", "shape", ")", "-", "rightmost_", ":", "]", ",", "perm_", ")", ")"], "docstring": "Helper for _forward and _inverse_event_shape.", "docstring_tokens": ["Helper", "for", "_forward", "and", "_inverse_event_shape", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/transpose.py#L184-L205", "partition": "test", "index": 933, "time": "2019-03-04 14:18:04"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/initializers.py", "func_name": "BlockwiseInitializer.from_config", "original_string": "def from_config(cls, config):\n    \"\"\"Instantiates an initializer from a configuration dictionary.\"\"\"\n    return cls(**{\n        'initializers': [tf.compat.v2.initializers.deserialize(init)\n                         for init in config.get('initializers', [])],\n        'sizes': config.get('sizes', []),\n        'validate_args': config.get('validate_args', False),\n    })", "language": "python", "code": "def from_config(cls, config):\n    \"\"\"Instantiates an initializer from a configuration dictionary.\"\"\"\n    return cls(**{\n        'initializers': [tf.compat.v2.initializers.deserialize(init)\n                         for init in config.get('initializers', [])],\n        'sizes': config.get('sizes', []),\n        'validate_args': config.get('validate_args', False),\n    })", "code_tokens": ["def", "from_config", "(", "cls", ",", "config", ")", ":", "return", "cls", "(", "*", "*", "{", "'initializers'", ":", "[", "tf", ".", "compat", ".", "v2", ".", "initializers", ".", "deserialize", "(", "init", ")", "for", "init", "in", "config", ".", "get", "(", "'initializers'", ",", "[", "]", ")", "]", ",", "'sizes'", ":", "config", ".", "get", "(", "'sizes'", ",", "[", "]", ")", ",", "'validate_args'", ":", "config", ".", "get", "(", "'validate_args'", ",", "False", ")", ",", "}", ")"], "docstring": "Instantiates an initializer from a configuration dictionary.", "docstring_tokens": ["Instantiates", "an", "initializer", "from", "a", "configuration", "dictionary", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/initializers.py#L119-L126", "partition": "test", "index": 874, "time": "2019-03-04 19:02:49"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/initializers.py", "func_name": "BlockwiseInitializer.get_config", "original_string": "def get_config(self):\n    \"\"\"Returns initializer configuration as a JSON-serializable dict.\"\"\"\n    return {\n        'initializers': [\n            tf.compat.v2.initializers.serialize(\n                tf.keras.initializers.get(init))\n            for init in self.initializers\n        ],\n        'sizes': self.sizes,\n        'validate_args': self.validate_args,\n    }", "language": "python", "code": "def get_config(self):\n    \"\"\"Returns initializer configuration as a JSON-serializable dict.\"\"\"\n    return {\n        'initializers': [\n            tf.compat.v2.initializers.serialize(\n                tf.keras.initializers.get(init))\n            for init in self.initializers\n        ],\n        'sizes': self.sizes,\n        'validate_args': self.validate_args,\n    }", "code_tokens": ["def", "get_config", "(", "self", ")", ":", "return", "{", "'initializers'", ":", "[", "tf", ".", "compat", ".", "v2", ".", "initializers", ".", "serialize", "(", "tf", ".", "keras", ".", "initializers", ".", "get", "(", "init", ")", ")", "for", "init", "in", "self", ".", "initializers", "]", ",", "'sizes'", ":", "self", ".", "sizes", ",", "'validate_args'", ":", "self", ".", "validate_args", ",", "}"], "docstring": "Returns initializer configuration as a JSON-serializable dict.", "docstring_tokens": ["Returns", "initializer", "configuration", "as", "a", "JSON", "-", "serializable", "dict", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/initializers.py#L106-L116", "partition": "test", "index": 873, "time": "2019-03-04 19:02:49"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/slicing.py", "func_name": "_apply_slice_sequence", "original_string": "def _apply_slice_sequence(dist, params_event_ndims, slice_overrides_seq):\n  \"\"\"Applies a sequence of slice or copy-with-overrides operations to `dist`.\"\"\"\n  for slices, overrides in slice_overrides_seq:\n    dist = _apply_single_step(dist, params_event_ndims, slices, overrides)\n  return dist", "language": "python", "code": "def _apply_slice_sequence(dist, params_event_ndims, slice_overrides_seq):\n  \"\"\"Applies a sequence of slice or copy-with-overrides operations to `dist`.\"\"\"\n  for slices, overrides in slice_overrides_seq:\n    dist = _apply_single_step(dist, params_event_ndims, slices, overrides)\n  return dist", "code_tokens": ["def", "_apply_slice_sequence", "(", "dist", ",", "params_event_ndims", ",", "slice_overrides_seq", ")", ":", "for", "slices", ",", "overrides", "in", "slice_overrides_seq", ":", "dist", "=", "_apply_single_step", "(", "dist", ",", "params_event_ndims", ",", "slices", ",", "overrides", ")", "return", "dist"], "docstring": "Applies a sequence of slice or copy-with-overrides operations to `dist`.", "docstring_tokens": ["Applies", "a", "sequence", "of", "slice", "or", "copy", "-", "with", "-", "overrides", "operations", "to", "dist", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/slicing.py#L158-L162", "partition": "test", "index": 890, "time": "2019-03-04 19:22:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/slicing.py", "func_name": "batch_slice", "original_string": "def batch_slice(dist, params_event_ndims, params_overrides, slices):\n  \"\"\"Slices `dist` along its batch dimensions. Helper for tfd.Distribution.\n\n  Args:\n    dist: A `tfd.Distribution` instance.\n    params_event_ndims: A `dict` of `str->int` indicating the number of\n      dimensions of a given parameter required to parameterize a single event.\n    params_overrides: A `dict` of parameter overrides. (e.g. from\n      `Distribution.copy`).\n    slices: A `slice` or `int` or `int` `Tensor` or `tf.newaxis` or `tuple`\n      thereof. (e.g. the argument of a `__getitem__` method).\n\n  Returns:\n    new_dist: A batch-sliced `tfd.Distribution`.\n  \"\"\"\n  if not isinstance(slices, collections.Sequence):\n    slices = (slices,)\n  # We track the history of slice and copy(**param_overrides) in order to trace\n  # back to the original distribution's source variables.\n  orig_dist, slice_overrides_seq = getattr(dist, PROVENANCE_ATTR, (dist, []))\n  slice_overrides_seq += [(slices, params_overrides)]\n  # Re-doing the full sequence of slice+copy override work here enables\n  # gradients all the way back to the original distribution's arguments.\n  dist = _apply_slice_sequence(orig_dist, params_event_ndims,\n                               slice_overrides_seq)\n  setattr(dist, PROVENANCE_ATTR, (orig_dist, slice_overrides_seq))\n  return dist", "language": "python", "code": "def batch_slice(dist, params_event_ndims, params_overrides, slices):\n  \"\"\"Slices `dist` along its batch dimensions. Helper for tfd.Distribution.\n\n  Args:\n    dist: A `tfd.Distribution` instance.\n    params_event_ndims: A `dict` of `str->int` indicating the number of\n      dimensions of a given parameter required to parameterize a single event.\n    params_overrides: A `dict` of parameter overrides. (e.g. from\n      `Distribution.copy`).\n    slices: A `slice` or `int` or `int` `Tensor` or `tf.newaxis` or `tuple`\n      thereof. (e.g. the argument of a `__getitem__` method).\n\n  Returns:\n    new_dist: A batch-sliced `tfd.Distribution`.\n  \"\"\"\n  if not isinstance(slices, collections.Sequence):\n    slices = (slices,)\n  # We track the history of slice and copy(**param_overrides) in order to trace\n  # back to the original distribution's source variables.\n  orig_dist, slice_overrides_seq = getattr(dist, PROVENANCE_ATTR, (dist, []))\n  slice_overrides_seq += [(slices, params_overrides)]\n  # Re-doing the full sequence of slice+copy override work here enables\n  # gradients all the way back to the original distribution's arguments.\n  dist = _apply_slice_sequence(orig_dist, params_event_ndims,\n                               slice_overrides_seq)\n  setattr(dist, PROVENANCE_ATTR, (orig_dist, slice_overrides_seq))\n  return dist", "code_tokens": ["def", "batch_slice", "(", "dist", ",", "params_event_ndims", ",", "params_overrides", ",", "slices", ")", ":", "if", "not", "isinstance", "(", "slices", ",", "collections", ".", "Sequence", ")", ":", "slices", "=", "(", "slices", ",", ")", "# We track the history of slice and copy(**param_overrides) in order to trace", "# back to the original distribution's source variables.", "orig_dist", ",", "slice_overrides_seq", "=", "getattr", "(", "dist", ",", "PROVENANCE_ATTR", ",", "(", "dist", ",", "[", "]", ")", ")", "slice_overrides_seq", "+=", "[", "(", "slices", ",", "params_overrides", ")", "]", "# Re-doing the full sequence of slice+copy override work here enables", "# gradients all the way back to the original distribution's arguments.", "dist", "=", "_apply_slice_sequence", "(", "orig_dist", ",", "params_event_ndims", ",", "slice_overrides_seq", ")", "setattr", "(", "dist", ",", "PROVENANCE_ATTR", ",", "(", "orig_dist", ",", "slice_overrides_seq", ")", ")", "return", "dist"], "docstring": "Slices `dist` along its batch dimensions. Helper for tfd.Distribution.\n\n  Args:\n    dist: A `tfd.Distribution` instance.\n    params_event_ndims: A `dict` of `str->int` indicating the number of\n      dimensions of a given parameter required to parameterize a single event.\n    params_overrides: A `dict` of parameter overrides. (e.g. from\n      `Distribution.copy`).\n    slices: A `slice` or `int` or `int` `Tensor` or `tf.newaxis` or `tuple`\n      thereof. (e.g. the argument of a `__getitem__` method).\n\n  Returns:\n    new_dist: A batch-sliced `tfd.Distribution`.", "docstring_tokens": ["Slices", "dist", "along", "its", "batch", "dimensions", ".", "Helper", "for", "tfd", ".", "Distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/slicing.py#L165-L191", "partition": "test", "index": 891, "time": "2019-03-04 19:22:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/slicing.py", "func_name": "_slice_single_param", "original_string": "def _slice_single_param(param, param_event_ndims, slices, dist_batch_shape):\n  \"\"\"Slices a single parameter of a distribution.\n\n  Args:\n    param: A `Tensor`, the original parameter to slice.\n    param_event_ndims: `int` event parameterization rank for this parameter.\n    slices: A `tuple` of normalized slices.\n    dist_batch_shape: The distribution's batch shape `Tensor`.\n\n  Returns:\n    new_param: A `Tensor`, batch-sliced according to slices.\n  \"\"\"\n  # Extend param shape with ones on the left to match dist_batch_shape.\n  param_shape = tf.shape(input=param)\n  insert_ones = tf.ones(\n      [tf.size(input=dist_batch_shape) + param_event_ndims - tf.rank(param)],\n      dtype=param_shape.dtype)\n  new_param_shape = tf.concat([insert_ones, param_shape], axis=0)\n  full_batch_param = tf.reshape(param, new_param_shape)\n  param_slices = []\n  # We separately track the batch axis from the parameter axis because we want\n  # them to align for positive indexing, and be offset by param_event_ndims for\n  # negative indexing.\n  param_dim_idx = 0\n  batch_dim_idx = 0\n  for slc in slices:\n    if slc is tf.newaxis:\n      param_slices.append(slc)\n      continue\n    if slc is Ellipsis:\n      if batch_dim_idx < 0:\n        raise ValueError('Found multiple `...` in slices {}'.format(slices))\n      param_slices.append(slc)\n      # Switch over to negative indexing for the broadcast check.\n      num_remaining_non_newaxis_slices = sum(\n          [s is not tf.newaxis for s in slices[slices.index(Ellipsis) + 1:]])\n      batch_dim_idx = -num_remaining_non_newaxis_slices\n      param_dim_idx = batch_dim_idx - param_event_ndims\n      continue\n    # Find the batch dimension sizes for both parameter and distribution.\n    param_dim_size = new_param_shape[param_dim_idx]\n    batch_dim_size = dist_batch_shape[batch_dim_idx]\n    is_broadcast = batch_dim_size > param_dim_size\n    # Slices are denoted by start:stop:step.\n    if isinstance(slc, slice):\n      start, stop, step = slc.start, slc.stop, slc.step\n      if start is not None:\n        start = tf.where(is_broadcast, 0, start)\n      if stop is not None:\n        stop = tf.where(is_broadcast, 1, stop)\n      if step is not None:\n        step = tf.where(is_broadcast, 1, step)\n      param_slices.append(slice(start, stop, step))\n    else:  # int, or int Tensor, e.g. d[d.batch_shape_tensor()[0] // 2]\n      param_slices.append(tf.where(is_broadcast, 0, slc))\n    param_dim_idx += 1\n    batch_dim_idx += 1\n  param_slices.extend([ALL_SLICE] * param_event_ndims)\n  return full_batch_param.__getitem__(param_slices)", "language": "python", "code": "def _slice_single_param(param, param_event_ndims, slices, dist_batch_shape):\n  \"\"\"Slices a single parameter of a distribution.\n\n  Args:\n    param: A `Tensor`, the original parameter to slice.\n    param_event_ndims: `int` event parameterization rank for this parameter.\n    slices: A `tuple` of normalized slices.\n    dist_batch_shape: The distribution's batch shape `Tensor`.\n\n  Returns:\n    new_param: A `Tensor`, batch-sliced according to slices.\n  \"\"\"\n  # Extend param shape with ones on the left to match dist_batch_shape.\n  param_shape = tf.shape(input=param)\n  insert_ones = tf.ones(\n      [tf.size(input=dist_batch_shape) + param_event_ndims - tf.rank(param)],\n      dtype=param_shape.dtype)\n  new_param_shape = tf.concat([insert_ones, param_shape], axis=0)\n  full_batch_param = tf.reshape(param, new_param_shape)\n  param_slices = []\n  # We separately track the batch axis from the parameter axis because we want\n  # them to align for positive indexing, and be offset by param_event_ndims for\n  # negative indexing.\n  param_dim_idx = 0\n  batch_dim_idx = 0\n  for slc in slices:\n    if slc is tf.newaxis:\n      param_slices.append(slc)\n      continue\n    if slc is Ellipsis:\n      if batch_dim_idx < 0:\n        raise ValueError('Found multiple `...` in slices {}'.format(slices))\n      param_slices.append(slc)\n      # Switch over to negative indexing for the broadcast check.\n      num_remaining_non_newaxis_slices = sum(\n          [s is not tf.newaxis for s in slices[slices.index(Ellipsis) + 1:]])\n      batch_dim_idx = -num_remaining_non_newaxis_slices\n      param_dim_idx = batch_dim_idx - param_event_ndims\n      continue\n    # Find the batch dimension sizes for both parameter and distribution.\n    param_dim_size = new_param_shape[param_dim_idx]\n    batch_dim_size = dist_batch_shape[batch_dim_idx]\n    is_broadcast = batch_dim_size > param_dim_size\n    # Slices are denoted by start:stop:step.\n    if isinstance(slc, slice):\n      start, stop, step = slc.start, slc.stop, slc.step\n      if start is not None:\n        start = tf.where(is_broadcast, 0, start)\n      if stop is not None:\n        stop = tf.where(is_broadcast, 1, stop)\n      if step is not None:\n        step = tf.where(is_broadcast, 1, step)\n      param_slices.append(slice(start, stop, step))\n    else:  # int, or int Tensor, e.g. d[d.batch_shape_tensor()[0] // 2]\n      param_slices.append(tf.where(is_broadcast, 0, slc))\n    param_dim_idx += 1\n    batch_dim_idx += 1\n  param_slices.extend([ALL_SLICE] * param_event_ndims)\n  return full_batch_param.__getitem__(param_slices)", "code_tokens": ["def", "_slice_single_param", "(", "param", ",", "param_event_ndims", ",", "slices", ",", "dist_batch_shape", ")", ":", "# Extend param shape with ones on the left to match dist_batch_shape.", "param_shape", "=", "tf", ".", "shape", "(", "input", "=", "param", ")", "insert_ones", "=", "tf", ".", "ones", "(", "[", "tf", ".", "size", "(", "input", "=", "dist_batch_shape", ")", "+", "param_event_ndims", "-", "tf", ".", "rank", "(", "param", ")", "]", ",", "dtype", "=", "param_shape", ".", "dtype", ")", "new_param_shape", "=", "tf", ".", "concat", "(", "[", "insert_ones", ",", "param_shape", "]", ",", "axis", "=", "0", ")", "full_batch_param", "=", "tf", ".", "reshape", "(", "param", ",", "new_param_shape", ")", "param_slices", "=", "[", "]", "# We separately track the batch axis from the parameter axis because we want", "# them to align for positive indexing, and be offset by param_event_ndims for", "# negative indexing.", "param_dim_idx", "=", "0", "batch_dim_idx", "=", "0", "for", "slc", "in", "slices", ":", "if", "slc", "is", "tf", ".", "newaxis", ":", "param_slices", ".", "append", "(", "slc", ")", "continue", "if", "slc", "is", "Ellipsis", ":", "if", "batch_dim_idx", "<", "0", ":", "raise", "ValueError", "(", "'Found multiple `...` in slices {}'", ".", "format", "(", "slices", ")", ")", "param_slices", ".", "append", "(", "slc", ")", "# Switch over to negative indexing for the broadcast check.", "num_remaining_non_newaxis_slices", "=", "sum", "(", "[", "s", "is", "not", "tf", ".", "newaxis", "for", "s", "in", "slices", "[", "slices", ".", "index", "(", "Ellipsis", ")", "+", "1", ":", "]", "]", ")", "batch_dim_idx", "=", "-", "num_remaining_non_newaxis_slices", "param_dim_idx", "=", "batch_dim_idx", "-", "param_event_ndims", "continue", "# Find the batch dimension sizes for both parameter and distribution.", "param_dim_size", "=", "new_param_shape", "[", "param_dim_idx", "]", "batch_dim_size", "=", "dist_batch_shape", "[", "batch_dim_idx", "]", "is_broadcast", "=", "batch_dim_size", ">", "param_dim_size", "# Slices are denoted by start:stop:step.", "if", "isinstance", "(", "slc", ",", "slice", ")", ":", "start", ",", "stop", ",", "step", "=", "slc", ".", "start", ",", "slc", ".", "stop", ",", "slc", ".", "step", "if", "start", "is", "not", "None", ":", "start", "=", "tf", ".", "where", "(", "is_broadcast", ",", "0", ",", "start", ")", "if", "stop", "is", "not", "None", ":", "stop", "=", "tf", ".", "where", "(", "is_broadcast", ",", "1", ",", "stop", ")", "if", "step", "is", "not", "None", ":", "step", "=", "tf", ".", "where", "(", "is_broadcast", ",", "1", ",", "step", ")", "param_slices", ".", "append", "(", "slice", "(", "start", ",", "stop", ",", "step", ")", ")", "else", ":", "# int, or int Tensor, e.g. d[d.batch_shape_tensor()[0] // 2]", "param_slices", ".", "append", "(", "tf", ".", "where", "(", "is_broadcast", ",", "0", ",", "slc", ")", ")", "param_dim_idx", "+=", "1", "batch_dim_idx", "+=", "1", "param_slices", ".", "extend", "(", "[", "ALL_SLICE", "]", "*", "param_event_ndims", ")", "return", "full_batch_param", ".", "__getitem__", "(", "param_slices", ")"], "docstring": "Slices a single parameter of a distribution.\n\n  Args:\n    param: A `Tensor`, the original parameter to slice.\n    param_event_ndims: `int` event parameterization rank for this parameter.\n    slices: A `tuple` of normalized slices.\n    dist_batch_shape: The distribution's batch shape `Tensor`.\n\n  Returns:\n    new_param: A `Tensor`, batch-sliced according to slices.", "docstring_tokens": ["Slices", "a", "single", "parameter", "of", "a", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/slicing.py#L46-L104", "partition": "test", "index": 887, "time": "2019-03-04 19:22:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/slicing.py", "func_name": "_slice_params_to_dict", "original_string": "def _slice_params_to_dict(dist, params_event_ndims, slices):\n  \"\"\"Computes the override dictionary of sliced parameters.\n\n  Args:\n    dist: The tfd.Distribution being batch-sliced.\n    params_event_ndims: Per-event parameter ranks, a `str->int` `dict`.\n    slices: Slices as received by __getitem__.\n\n  Returns:\n    overrides: `str->Tensor` `dict` of batch-sliced parameter overrides.\n  \"\"\"\n  override_dict = {}\n  for param_name, param_event_ndims in six.iteritems(params_event_ndims):\n    # Verify that either None or a legit value is in the parameters dict.\n    if param_name not in dist.parameters:\n      raise ValueError('Distribution {} is missing advertised '\n                       'parameter {}'.format(dist, param_name))\n    param = dist.parameters[param_name]\n    if param is None:\n      # some distributions have multiple possible parameterizations; this\n      # param was not provided\n      continue\n    dtype = None\n    if hasattr(dist, param_name):\n      attr = getattr(dist, param_name)\n      dtype = getattr(attr, 'dtype', None)\n    if dtype is None:\n      dtype = dist.dtype\n      warnings.warn('Unable to find property getter for parameter Tensor {} '\n                    'on {}, falling back to Distribution.dtype {}'.format(\n                        param_name, dist, dtype))\n    param = tf.convert_to_tensor(value=param, dtype=dtype)\n    override_dict[param_name] = _slice_single_param(param, param_event_ndims,\n                                                    slices,\n                                                    dist.batch_shape_tensor())\n  return override_dict", "language": "python", "code": "def _slice_params_to_dict(dist, params_event_ndims, slices):\n  \"\"\"Computes the override dictionary of sliced parameters.\n\n  Args:\n    dist: The tfd.Distribution being batch-sliced.\n    params_event_ndims: Per-event parameter ranks, a `str->int` `dict`.\n    slices: Slices as received by __getitem__.\n\n  Returns:\n    overrides: `str->Tensor` `dict` of batch-sliced parameter overrides.\n  \"\"\"\n  override_dict = {}\n  for param_name, param_event_ndims in six.iteritems(params_event_ndims):\n    # Verify that either None or a legit value is in the parameters dict.\n    if param_name not in dist.parameters:\n      raise ValueError('Distribution {} is missing advertised '\n                       'parameter {}'.format(dist, param_name))\n    param = dist.parameters[param_name]\n    if param is None:\n      # some distributions have multiple possible parameterizations; this\n      # param was not provided\n      continue\n    dtype = None\n    if hasattr(dist, param_name):\n      attr = getattr(dist, param_name)\n      dtype = getattr(attr, 'dtype', None)\n    if dtype is None:\n      dtype = dist.dtype\n      warnings.warn('Unable to find property getter for parameter Tensor {} '\n                    'on {}, falling back to Distribution.dtype {}'.format(\n                        param_name, dist, dtype))\n    param = tf.convert_to_tensor(value=param, dtype=dtype)\n    override_dict[param_name] = _slice_single_param(param, param_event_ndims,\n                                                    slices,\n                                                    dist.batch_shape_tensor())\n  return override_dict", "code_tokens": ["def", "_slice_params_to_dict", "(", "dist", ",", "params_event_ndims", ",", "slices", ")", ":", "override_dict", "=", "{", "}", "for", "param_name", ",", "param_event_ndims", "in", "six", ".", "iteritems", "(", "params_event_ndims", ")", ":", "# Verify that either None or a legit value is in the parameters dict.", "if", "param_name", "not", "in", "dist", ".", "parameters", ":", "raise", "ValueError", "(", "'Distribution {} is missing advertised '", "'parameter {}'", ".", "format", "(", "dist", ",", "param_name", ")", ")", "param", "=", "dist", ".", "parameters", "[", "param_name", "]", "if", "param", "is", "None", ":", "# some distributions have multiple possible parameterizations; this", "# param was not provided", "continue", "dtype", "=", "None", "if", "hasattr", "(", "dist", ",", "param_name", ")", ":", "attr", "=", "getattr", "(", "dist", ",", "param_name", ")", "dtype", "=", "getattr", "(", "attr", ",", "'dtype'", ",", "None", ")", "if", "dtype", "is", "None", ":", "dtype", "=", "dist", ".", "dtype", "warnings", ".", "warn", "(", "'Unable to find property getter for parameter Tensor {} '", "'on {}, falling back to Distribution.dtype {}'", ".", "format", "(", "param_name", ",", "dist", ",", "dtype", ")", ")", "param", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "param", ",", "dtype", "=", "dtype", ")", "override_dict", "[", "param_name", "]", "=", "_slice_single_param", "(", "param", ",", "param_event_ndims", ",", "slices", ",", "dist", ".", "batch_shape_tensor", "(", ")", ")", "return", "override_dict"], "docstring": "Computes the override dictionary of sliced parameters.\n\n  Args:\n    dist: The tfd.Distribution being batch-sliced.\n    params_event_ndims: Per-event parameter ranks, a `str->int` `dict`.\n    slices: Slices as received by __getitem__.\n\n  Returns:\n    overrides: `str->Tensor` `dict` of batch-sliced parameter overrides.", "docstring_tokens": ["Computes", "the", "override", "dictionary", "of", "sliced", "parameters", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/slicing.py#L107-L142", "partition": "test", "index": 888, "time": "2019-03-04 19:22:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/slicing.py", "func_name": "_apply_single_step", "original_string": "def _apply_single_step(dist, params_event_ndims, slices, params_overrides):\n  \"\"\"Applies a single slicing step to `dist`, returning a new instance.\"\"\"\n  if len(slices) == 1 and slices[0] == Ellipsis:\n    # The path used by Distribution.copy: batch_slice(...args..., Ellipsis)\n    override_dict = {}\n  else:\n    override_dict = _slice_params_to_dict(dist, params_event_ndims, slices)\n  override_dict.update(params_overrides)\n  parameters = dict(dist.parameters, **override_dict)\n  new_dist = type(dist)(**parameters)\n  return new_dist", "language": "python", "code": "def _apply_single_step(dist, params_event_ndims, slices, params_overrides):\n  \"\"\"Applies a single slicing step to `dist`, returning a new instance.\"\"\"\n  if len(slices) == 1 and slices[0] == Ellipsis:\n    # The path used by Distribution.copy: batch_slice(...args..., Ellipsis)\n    override_dict = {}\n  else:\n    override_dict = _slice_params_to_dict(dist, params_event_ndims, slices)\n  override_dict.update(params_overrides)\n  parameters = dict(dist.parameters, **override_dict)\n  new_dist = type(dist)(**parameters)\n  return new_dist", "code_tokens": ["def", "_apply_single_step", "(", "dist", ",", "params_event_ndims", ",", "slices", ",", "params_overrides", ")", ":", "if", "len", "(", "slices", ")", "==", "1", "and", "slices", "[", "0", "]", "==", "Ellipsis", ":", "# The path used by Distribution.copy: batch_slice(...args..., Ellipsis)", "override_dict", "=", "{", "}", "else", ":", "override_dict", "=", "_slice_params_to_dict", "(", "dist", ",", "params_event_ndims", ",", "slices", ")", "override_dict", ".", "update", "(", "params_overrides", ")", "parameters", "=", "dict", "(", "dist", ".", "parameters", ",", "*", "*", "override_dict", ")", "new_dist", "=", "type", "(", "dist", ")", "(", "*", "*", "parameters", ")", "return", "new_dist"], "docstring": "Applies a single slicing step to `dist`, returning a new instance.", "docstring_tokens": ["Applies", "a", "single", "slicing", "step", "to", "dist", "returning", "a", "new", "instance", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/slicing.py#L145-L155", "partition": "test", "index": 889, "time": "2019-03-04 19:22:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/hidden_markov_model.py", "func_name": "HiddenMarkovModel.posterior_mode", "original_string": "def posterior_mode(self, observations, name=None):\n    \"\"\"Compute maximum likelihood sequence of hidden states.\n\n    When this function is provided with a sequence of observations\n    `x[0], ..., x[num_steps - 1]`, it returns the sequence of hidden\n    states `z[0], ..., z[num_steps - 1]`, drawn from the underlying\n    Markov chain, that is most likely to yield those observations.\n\n    It uses the [Viterbi algorithm](\n    https://en.wikipedia.org/wiki/Viterbi_algorithm).\n\n    Note: the behavior of this function is undefined if the\n    `observations` argument represents impossible observations\n    from the model.\n\n    Note: if there isn't a unique most likely sequence then one\n    of the equally most likely sequences is chosen.\n\n    Args:\n      observations: A tensor representing a batch of observations made on the\n        hidden Markov model.  The rightmost dimensions of this tensor correspond\n        to the dimensions of the observation distributions of the underlying\n        Markov chain.  The next dimension from the right indexes the steps in a\n        sequence of observations from a single sample from the hidden Markov\n        model.  The size of this dimension should match the `num_steps`\n        parameter of the hidden Markov model object.  The other dimensions are\n        the dimensions of the batch and these are broadcast with the hidden\n        Markov model's parameters.\n      name: Python `str` name prefixed to Ops created by this class.\n        Default value: \"HiddenMarkovModel\".\n\n    Returns:\n      posterior_mode: A `Tensor` representing the most likely sequence of hidden\n        states. The rightmost dimension of this tensor will equal the\n        `num_steps` parameter providing one hidden state for each step. The\n        other dimensions are those of the batch.\n\n    Raises:\n      ValueError: if the `observations` tensor does not consist of\n      sequences of `num_steps` observations.\n\n    #### Examples\n\n    ```python\n    tfd = tfp.distributions\n\n    # A simple weather model.\n\n    # Represent a cold day with 0 and a hot day with 1.\n    # Suppose the first day of a sequence has a 0.8 chance of being cold.\n\n    initial_distribution = tfd.Categorical(probs=[0.8, 0.2])\n\n    # Suppose a cold day has a 30% chance of being followed by a hot day\n    # and a hot day has a 20% chance of being followed by a cold day.\n\n    transition_distribution = tfd.Categorical(probs=[[0.7, 0.3],\n                                                     [0.2, 0.8]])\n\n    # Suppose additionally that on each day the temperature is\n    # normally distributed with mean and standard deviation 0 and 5 on\n    # a cold day and mean and standard deviation 15 and 10 on a hot day.\n\n    observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])\n\n    # This gives the hidden Markov model:\n\n    model = tfd.HiddenMarkovModel(\n        initial_distribution=initial_distribution,\n        transition_distribution=transition_distribution,\n        observation_distribution=observation_distribution,\n        num_steps=7)\n\n    # Suppose we observe gradually rising temperatures over a week:\n    temps = [-2., 0., 2., 4., 6., 8., 10.]\n\n    # We can now compute the most probable sequence of hidden states:\n\n    model.posterior_mode(temps)\n\n    # The result is [0 0 0 0 0 1 1] telling us that the transition\n    # from \"cold\" to \"hot\" most likely happened between the\n    # 5th and 6th days.\n    ```\n    \"\"\"\n\n    with tf.name_scope(name or \"posterior_mode\"):\n      with tf.control_dependencies(self._runtime_assertions):\n        observation_tensor_shape = tf.shape(input=observations)\n\n        with self._observation_shape_preconditions(observation_tensor_shape):\n          observation_batch_shape = observation_tensor_shape[\n              :-1 - self._underlying_event_rank]\n          observation_event_shape = observation_tensor_shape[\n              -1 - self._underlying_event_rank:]\n\n          batch_shape = tf.broadcast_dynamic_shape(observation_batch_shape,\n                                                   self.batch_shape_tensor())\n          log_init = tf.broadcast_to(self._log_init,\n                                     tf.concat([batch_shape,\n                                                [self._num_states]],\n                                               axis=0))\n\n          observations = tf.broadcast_to(observations,\n                                         tf.concat([batch_shape,\n                                                    observation_event_shape],\n                                                   axis=0))\n          observation_rank = tf.rank(observations)\n          underlying_event_rank = self._underlying_event_rank\n          observations = distribution_util.move_dimension(\n              observations, observation_rank - underlying_event_rank - 1, 0)\n\n          # We need to compute the probability of each observation for\n          # each possible state.\n          # This requires inserting an extra index just before the\n          # observation event indices that will be broadcast with the\n          # last batch index in `observation_distribution`.\n          observations = tf.expand_dims(\n              observations,\n              observation_rank - underlying_event_rank)\n          observation_log_probs = self._observation_distribution.log_prob(\n              observations)\n\n          log_prob = log_init + observation_log_probs[0]\n\n          if self._num_steps == 1:\n            most_likely_end = tf.argmax(input=log_prob, axis=-1)\n            return most_likely_end[..., tf.newaxis]\n\n          def forward_step(previous_step_pair, log_prob_observation):\n            log_prob_previous = previous_step_pair[0]\n            log_prob = (log_prob_previous[..., tf.newaxis] +\n                        self._log_trans +\n                        log_prob_observation[..., tf.newaxis, :])\n            most_likely_given_successor = tf.argmax(input=log_prob, axis=-2)\n            max_log_p_given_successor = tf.reduce_max(input_tensor=log_prob,\n                                                      axis=-2)\n            return (max_log_p_given_successor, most_likely_given_successor)\n\n          forward_log_probs, all_most_likely_given_successor = tf.scan(\n              forward_step,\n              observation_log_probs[1:],\n              initializer=(log_prob,\n                           tf.zeros(tf.shape(input=log_init), dtype=tf.int64)),\n              name=\"forward_log_probs\")\n\n          most_likely_end = tf.argmax(input=forward_log_probs[-1], axis=-1)\n\n          # We require the operation that gives C from A and B where\n          # C[i...j] = A[i...j, B[i...j]]\n          # and A = most_likely_given_successor\n          #     B = most_likely_successor.\n          # tf.gather requires indices of known shape so instead we use\n          # reduction with tf.one_hot(B) to pick out elements from B\n          def backward_step(most_likely_successor, most_likely_given_successor):\n            return tf.reduce_sum(\n                input_tensor=(most_likely_given_successor *\n                              tf.one_hot(most_likely_successor,\n                                         self._num_states,\n                                         dtype=tf.int64)),\n                axis=-1)\n\n          backward_scan = tf.scan(\n              backward_step,\n              all_most_likely_given_successor,\n              most_likely_end,\n              reverse=True)\n          most_likely_sequences = tf.concat([backward_scan, [most_likely_end]],\n                                            axis=0)\n          return distribution_util.move_dimension(most_likely_sequences, 0, -1)", "language": "python", "code": "def posterior_mode(self, observations, name=None):\n    \"\"\"Compute maximum likelihood sequence of hidden states.\n\n    When this function is provided with a sequence of observations\n    `x[0], ..., x[num_steps - 1]`, it returns the sequence of hidden\n    states `z[0], ..., z[num_steps - 1]`, drawn from the underlying\n    Markov chain, that is most likely to yield those observations.\n\n    It uses the [Viterbi algorithm](\n    https://en.wikipedia.org/wiki/Viterbi_algorithm).\n\n    Note: the behavior of this function is undefined if the\n    `observations` argument represents impossible observations\n    from the model.\n\n    Note: if there isn't a unique most likely sequence then one\n    of the equally most likely sequences is chosen.\n\n    Args:\n      observations: A tensor representing a batch of observations made on the\n        hidden Markov model.  The rightmost dimensions of this tensor correspond\n        to the dimensions of the observation distributions of the underlying\n        Markov chain.  The next dimension from the right indexes the steps in a\n        sequence of observations from a single sample from the hidden Markov\n        model.  The size of this dimension should match the `num_steps`\n        parameter of the hidden Markov model object.  The other dimensions are\n        the dimensions of the batch and these are broadcast with the hidden\n        Markov model's parameters.\n      name: Python `str` name prefixed to Ops created by this class.\n        Default value: \"HiddenMarkovModel\".\n\n    Returns:\n      posterior_mode: A `Tensor` representing the most likely sequence of hidden\n        states. The rightmost dimension of this tensor will equal the\n        `num_steps` parameter providing one hidden state for each step. The\n        other dimensions are those of the batch.\n\n    Raises:\n      ValueError: if the `observations` tensor does not consist of\n      sequences of `num_steps` observations.\n\n    #### Examples\n\n    ```python\n    tfd = tfp.distributions\n\n    # A simple weather model.\n\n    # Represent a cold day with 0 and a hot day with 1.\n    # Suppose the first day of a sequence has a 0.8 chance of being cold.\n\n    initial_distribution = tfd.Categorical(probs=[0.8, 0.2])\n\n    # Suppose a cold day has a 30% chance of being followed by a hot day\n    # and a hot day has a 20% chance of being followed by a cold day.\n\n    transition_distribution = tfd.Categorical(probs=[[0.7, 0.3],\n                                                     [0.2, 0.8]])\n\n    # Suppose additionally that on each day the temperature is\n    # normally distributed with mean and standard deviation 0 and 5 on\n    # a cold day and mean and standard deviation 15 and 10 on a hot day.\n\n    observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])\n\n    # This gives the hidden Markov model:\n\n    model = tfd.HiddenMarkovModel(\n        initial_distribution=initial_distribution,\n        transition_distribution=transition_distribution,\n        observation_distribution=observation_distribution,\n        num_steps=7)\n\n    # Suppose we observe gradually rising temperatures over a week:\n    temps = [-2., 0., 2., 4., 6., 8., 10.]\n\n    # We can now compute the most probable sequence of hidden states:\n\n    model.posterior_mode(temps)\n\n    # The result is [0 0 0 0 0 1 1] telling us that the transition\n    # from \"cold\" to \"hot\" most likely happened between the\n    # 5th and 6th days.\n    ```\n    \"\"\"\n\n    with tf.name_scope(name or \"posterior_mode\"):\n      with tf.control_dependencies(self._runtime_assertions):\n        observation_tensor_shape = tf.shape(input=observations)\n\n        with self._observation_shape_preconditions(observation_tensor_shape):\n          observation_batch_shape = observation_tensor_shape[\n              :-1 - self._underlying_event_rank]\n          observation_event_shape = observation_tensor_shape[\n              -1 - self._underlying_event_rank:]\n\n          batch_shape = tf.broadcast_dynamic_shape(observation_batch_shape,\n                                                   self.batch_shape_tensor())\n          log_init = tf.broadcast_to(self._log_init,\n                                     tf.concat([batch_shape,\n                                                [self._num_states]],\n                                               axis=0))\n\n          observations = tf.broadcast_to(observations,\n                                         tf.concat([batch_shape,\n                                                    observation_event_shape],\n                                                   axis=0))\n          observation_rank = tf.rank(observations)\n          underlying_event_rank = self._underlying_event_rank\n          observations = distribution_util.move_dimension(\n              observations, observation_rank - underlying_event_rank - 1, 0)\n\n          # We need to compute the probability of each observation for\n          # each possible state.\n          # This requires inserting an extra index just before the\n          # observation event indices that will be broadcast with the\n          # last batch index in `observation_distribution`.\n          observations = tf.expand_dims(\n              observations,\n              observation_rank - underlying_event_rank)\n          observation_log_probs = self._observation_distribution.log_prob(\n              observations)\n\n          log_prob = log_init + observation_log_probs[0]\n\n          if self._num_steps == 1:\n            most_likely_end = tf.argmax(input=log_prob, axis=-1)\n            return most_likely_end[..., tf.newaxis]\n\n          def forward_step(previous_step_pair, log_prob_observation):\n            log_prob_previous = previous_step_pair[0]\n            log_prob = (log_prob_previous[..., tf.newaxis] +\n                        self._log_trans +\n                        log_prob_observation[..., tf.newaxis, :])\n            most_likely_given_successor = tf.argmax(input=log_prob, axis=-2)\n            max_log_p_given_successor = tf.reduce_max(input_tensor=log_prob,\n                                                      axis=-2)\n            return (max_log_p_given_successor, most_likely_given_successor)\n\n          forward_log_probs, all_most_likely_given_successor = tf.scan(\n              forward_step,\n              observation_log_probs[1:],\n              initializer=(log_prob,\n                           tf.zeros(tf.shape(input=log_init), dtype=tf.int64)),\n              name=\"forward_log_probs\")\n\n          most_likely_end = tf.argmax(input=forward_log_probs[-1], axis=-1)\n\n          # We require the operation that gives C from A and B where\n          # C[i...j] = A[i...j, B[i...j]]\n          # and A = most_likely_given_successor\n          #     B = most_likely_successor.\n          # tf.gather requires indices of known shape so instead we use\n          # reduction with tf.one_hot(B) to pick out elements from B\n          def backward_step(most_likely_successor, most_likely_given_successor):\n            return tf.reduce_sum(\n                input_tensor=(most_likely_given_successor *\n                              tf.one_hot(most_likely_successor,\n                                         self._num_states,\n                                         dtype=tf.int64)),\n                axis=-1)\n\n          backward_scan = tf.scan(\n              backward_step,\n              all_most_likely_given_successor,\n              most_likely_end,\n              reverse=True)\n          most_likely_sequences = tf.concat([backward_scan, [most_likely_end]],\n                                            axis=0)\n          return distribution_util.move_dimension(most_likely_sequences, 0, -1)", "code_tokens": ["def", "posterior_mode", "(", "self", ",", "observations", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "\"posterior_mode\"", ")", ":", "with", "tf", ".", "control_dependencies", "(", "self", ".", "_runtime_assertions", ")", ":", "observation_tensor_shape", "=", "tf", ".", "shape", "(", "input", "=", "observations", ")", "with", "self", ".", "_observation_shape_preconditions", "(", "observation_tensor_shape", ")", ":", "observation_batch_shape", "=", "observation_tensor_shape", "[", ":", "-", "1", "-", "self", ".", "_underlying_event_rank", "]", "observation_event_shape", "=", "observation_tensor_shape", "[", "-", "1", "-", "self", ".", "_underlying_event_rank", ":", "]", "batch_shape", "=", "tf", ".", "broadcast_dynamic_shape", "(", "observation_batch_shape", ",", "self", ".", "batch_shape_tensor", "(", ")", ")", "log_init", "=", "tf", ".", "broadcast_to", "(", "self", ".", "_log_init", ",", "tf", ".", "concat", "(", "[", "batch_shape", ",", "[", "self", ".", "_num_states", "]", "]", ",", "axis", "=", "0", ")", ")", "observations", "=", "tf", ".", "broadcast_to", "(", "observations", ",", "tf", ".", "concat", "(", "[", "batch_shape", ",", "observation_event_shape", "]", ",", "axis", "=", "0", ")", ")", "observation_rank", "=", "tf", ".", "rank", "(", "observations", ")", "underlying_event_rank", "=", "self", ".", "_underlying_event_rank", "observations", "=", "distribution_util", ".", "move_dimension", "(", "observations", ",", "observation_rank", "-", "underlying_event_rank", "-", "1", ",", "0", ")", "# We need to compute the probability of each observation for", "# each possible state.", "# This requires inserting an extra index just before the", "# observation event indices that will be broadcast with the", "# last batch index in `observation_distribution`.", "observations", "=", "tf", ".", "expand_dims", "(", "observations", ",", "observation_rank", "-", "underlying_event_rank", ")", "observation_log_probs", "=", "self", ".", "_observation_distribution", ".", "log_prob", "(", "observations", ")", "log_prob", "=", "log_init", "+", "observation_log_probs", "[", "0", "]", "if", "self", ".", "_num_steps", "==", "1", ":", "most_likely_end", "=", "tf", ".", "argmax", "(", "input", "=", "log_prob", ",", "axis", "=", "-", "1", ")", "return", "most_likely_end", "[", "...", ",", "tf", ".", "newaxis", "]", "def", "forward_step", "(", "previous_step_pair", ",", "log_prob_observation", ")", ":", "log_prob_previous", "=", "previous_step_pair", "[", "0", "]", "log_prob", "=", "(", "log_prob_previous", "[", "...", ",", "tf", ".", "newaxis", "]", "+", "self", ".", "_log_trans", "+", "log_prob_observation", "[", "...", ",", "tf", ".", "newaxis", ",", ":", "]", ")", "most_likely_given_successor", "=", "tf", ".", "argmax", "(", "input", "=", "log_prob", ",", "axis", "=", "-", "2", ")", "max_log_p_given_successor", "=", "tf", ".", "reduce_max", "(", "input_tensor", "=", "log_prob", ",", "axis", "=", "-", "2", ")", "return", "(", "max_log_p_given_successor", ",", "most_likely_given_successor", ")", "forward_log_probs", ",", "all_most_likely_given_successor", "=", "tf", ".", "scan", "(", "forward_step", ",", "observation_log_probs", "[", "1", ":", "]", ",", "initializer", "=", "(", "log_prob", ",", "tf", ".", "zeros", "(", "tf", ".", "shape", "(", "input", "=", "log_init", ")", ",", "dtype", "=", "tf", ".", "int64", ")", ")", ",", "name", "=", "\"forward_log_probs\"", ")", "most_likely_end", "=", "tf", ".", "argmax", "(", "input", "=", "forward_log_probs", "[", "-", "1", "]", ",", "axis", "=", "-", "1", ")", "# We require the operation that gives C from A and B where", "# C[i...j] = A[i...j, B[i...j]]", "# and A = most_likely_given_successor", "#     B = most_likely_successor.", "# tf.gather requires indices of known shape so instead we use", "# reduction with tf.one_hot(B) to pick out elements from B", "def", "backward_step", "(", "most_likely_successor", ",", "most_likely_given_successor", ")", ":", "return", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "(", "most_likely_given_successor", "*", "tf", ".", "one_hot", "(", "most_likely_successor", ",", "self", ".", "_num_states", ",", "dtype", "=", "tf", ".", "int64", ")", ")", ",", "axis", "=", "-", "1", ")", "backward_scan", "=", "tf", ".", "scan", "(", "backward_step", ",", "all_most_likely_given_successor", ",", "most_likely_end", ",", "reverse", "=", "True", ")", "most_likely_sequences", "=", "tf", ".", "concat", "(", "[", "backward_scan", ",", "[", "most_likely_end", "]", "]", ",", "axis", "=", "0", ")", "return", "distribution_util", ".", "move_dimension", "(", "most_likely_sequences", ",", "0", ",", "-", "1", ")"], "docstring": "Compute maximum likelihood sequence of hidden states.\n\n    When this function is provided with a sequence of observations\n    `x[0], ..., x[num_steps - 1]`, it returns the sequence of hidden\n    states `z[0], ..., z[num_steps - 1]`, drawn from the underlying\n    Markov chain, that is most likely to yield those observations.\n\n    It uses the [Viterbi algorithm](\n    https://en.wikipedia.org/wiki/Viterbi_algorithm).\n\n    Note: the behavior of this function is undefined if the\n    `observations` argument represents impossible observations\n    from the model.\n\n    Note: if there isn't a unique most likely sequence then one\n    of the equally most likely sequences is chosen.\n\n    Args:\n      observations: A tensor representing a batch of observations made on the\n        hidden Markov model.  The rightmost dimensions of this tensor correspond\n        to the dimensions of the observation distributions of the underlying\n        Markov chain.  The next dimension from the right indexes the steps in a\n        sequence of observations from a single sample from the hidden Markov\n        model.  The size of this dimension should match the `num_steps`\n        parameter of the hidden Markov model object.  The other dimensions are\n        the dimensions of the batch and these are broadcast with the hidden\n        Markov model's parameters.\n      name: Python `str` name prefixed to Ops created by this class.\n        Default value: \"HiddenMarkovModel\".\n\n    Returns:\n      posterior_mode: A `Tensor` representing the most likely sequence of hidden\n        states. The rightmost dimension of this tensor will equal the\n        `num_steps` parameter providing one hidden state for each step. The\n        other dimensions are those of the batch.\n\n    Raises:\n      ValueError: if the `observations` tensor does not consist of\n      sequences of `num_steps` observations.\n\n    #### Examples\n\n    ```python\n    tfd = tfp.distributions\n\n    # A simple weather model.\n\n    # Represent a cold day with 0 and a hot day with 1.\n    # Suppose the first day of a sequence has a 0.8 chance of being cold.\n\n    initial_distribution = tfd.Categorical(probs=[0.8, 0.2])\n\n    # Suppose a cold day has a 30% chance of being followed by a hot day\n    # and a hot day has a 20% chance of being followed by a cold day.\n\n    transition_distribution = tfd.Categorical(probs=[[0.7, 0.3],\n                                                     [0.2, 0.8]])\n\n    # Suppose additionally that on each day the temperature is\n    # normally distributed with mean and standard deviation 0 and 5 on\n    # a cold day and mean and standard deviation 15 and 10 on a hot day.\n\n    observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])\n\n    # This gives the hidden Markov model:\n\n    model = tfd.HiddenMarkovModel(\n        initial_distribution=initial_distribution,\n        transition_distribution=transition_distribution,\n        observation_distribution=observation_distribution,\n        num_steps=7)\n\n    # Suppose we observe gradually rising temperatures over a week:\n    temps = [-2., 0., 2., 4., 6., 8., 10.]\n\n    # We can now compute the most probable sequence of hidden states:\n\n    model.posterior_mode(temps)\n\n    # The result is [0 0 0 0 0 1 1] telling us that the transition\n    # from \"cold\" to \"hot\" most likely happened between the\n    # 5th and 6th days.\n    ```", "docstring_tokens": ["Compute", "maximum", "likelihood", "sequence", "of", "hidden", "states", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/hidden_markov_model.py#L736-L905", "partition": "test", "index": 692, "time": "2019-03-05 10:06:41"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/autoregressive.py", "func_name": "make_ar_transition_matrix", "original_string": "def make_ar_transition_matrix(coefficients):\n  \"\"\"Build transition matrix for an autoregressive StateSpaceModel.\n\n  When applied to a vector of previous values, this matrix computes\n  the expected new value (summing the previous states according to the\n  autoregressive coefficients) in the top dimension of the state space,\n  and moves all previous values down by one dimension, 'forgetting' the\n  final (least recent) value. That is, it looks like this:\n\n  ```\n  ar_matrix = [ coefs[0], coefs[1], ..., coefs[order]\n                1.,       0 ,       ..., 0.\n                0.,       1.,       ..., 0.\n                ...\n                0.,       0.,  ..., 1.,  0.            ]\n  ```\n\n  Args:\n    coefficients: float `Tensor` of shape `concat([batch_shape, [order]])`.\n\n  Returns:\n    ar_matrix: float `Tensor` with shape `concat([batch_shape,\n    [order, order]])`.\n  \"\"\"\n\n  top_row = tf.expand_dims(coefficients, -2)\n  coef_shape = dist_util.prefer_static_shape(coefficients)\n  batch_shape, order = coef_shape[:-1], coef_shape[-1]\n  remaining_rows = tf.concat([\n      tf.eye(order - 1, dtype=coefficients.dtype, batch_shape=batch_shape),\n      tf.zeros(tf.concat([batch_shape, (order - 1, 1)], axis=0),\n               dtype=coefficients.dtype)\n  ], axis=-1)\n  ar_matrix = tf.concat([top_row, remaining_rows], axis=-2)\n  return ar_matrix", "language": "python", "code": "def make_ar_transition_matrix(coefficients):\n  \"\"\"Build transition matrix for an autoregressive StateSpaceModel.\n\n  When applied to a vector of previous values, this matrix computes\n  the expected new value (summing the previous states according to the\n  autoregressive coefficients) in the top dimension of the state space,\n  and moves all previous values down by one dimension, 'forgetting' the\n  final (least recent) value. That is, it looks like this:\n\n  ```\n  ar_matrix = [ coefs[0], coefs[1], ..., coefs[order]\n                1.,       0 ,       ..., 0.\n                0.,       1.,       ..., 0.\n                ...\n                0.,       0.,  ..., 1.,  0.            ]\n  ```\n\n  Args:\n    coefficients: float `Tensor` of shape `concat([batch_shape, [order]])`.\n\n  Returns:\n    ar_matrix: float `Tensor` with shape `concat([batch_shape,\n    [order, order]])`.\n  \"\"\"\n\n  top_row = tf.expand_dims(coefficients, -2)\n  coef_shape = dist_util.prefer_static_shape(coefficients)\n  batch_shape, order = coef_shape[:-1], coef_shape[-1]\n  remaining_rows = tf.concat([\n      tf.eye(order - 1, dtype=coefficients.dtype, batch_shape=batch_shape),\n      tf.zeros(tf.concat([batch_shape, (order - 1, 1)], axis=0),\n               dtype=coefficients.dtype)\n  ], axis=-1)\n  ar_matrix = tf.concat([top_row, remaining_rows], axis=-2)\n  return ar_matrix", "code_tokens": ["def", "make_ar_transition_matrix", "(", "coefficients", ")", ":", "top_row", "=", "tf", ".", "expand_dims", "(", "coefficients", ",", "-", "2", ")", "coef_shape", "=", "dist_util", ".", "prefer_static_shape", "(", "coefficients", ")", "batch_shape", ",", "order", "=", "coef_shape", "[", ":", "-", "1", "]", ",", "coef_shape", "[", "-", "1", "]", "remaining_rows", "=", "tf", ".", "concat", "(", "[", "tf", ".", "eye", "(", "order", "-", "1", ",", "dtype", "=", "coefficients", ".", "dtype", ",", "batch_shape", "=", "batch_shape", ")", ",", "tf", ".", "zeros", "(", "tf", ".", "concat", "(", "[", "batch_shape", ",", "(", "order", "-", "1", ",", "1", ")", "]", ",", "axis", "=", "0", ")", ",", "dtype", "=", "coefficients", ".", "dtype", ")", "]", ",", "axis", "=", "-", "1", ")", "ar_matrix", "=", "tf", ".", "concat", "(", "[", "top_row", ",", "remaining_rows", "]", ",", "axis", "=", "-", "2", ")", "return", "ar_matrix"], "docstring": "Build transition matrix for an autoregressive StateSpaceModel.\n\n  When applied to a vector of previous values, this matrix computes\n  the expected new value (summing the previous states according to the\n  autoregressive coefficients) in the top dimension of the state space,\n  and moves all previous values down by one dimension, 'forgetting' the\n  final (least recent) value. That is, it looks like this:\n\n  ```\n  ar_matrix = [ coefs[0], coefs[1], ..., coefs[order]\n                1.,       0 ,       ..., 0.\n                0.,       1.,       ..., 0.\n                ...\n                0.,       0.,  ..., 1.,  0.            ]\n  ```\n\n  Args:\n    coefficients: float `Tensor` of shape `concat([batch_shape, [order]])`.\n\n  Returns:\n    ar_matrix: float `Tensor` with shape `concat([batch_shape,\n    [order, order]])`.", "docstring_tokens": ["Build", "transition", "matrix", "for", "an", "autoregressive", "StateSpaceModel", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/autoregressive.py#L223-L257", "partition": "test", "index": 832, "time": "2019-03-05 14:47:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/variational_gaussian_process.py", "func_name": "VariationalGaussianProcess.optimal_variational_posterior", "original_string": "def optimal_variational_posterior(\n      kernel,\n      inducing_index_points,\n      observation_index_points,\n      observations,\n      observation_noise_variance,\n      mean_fn=None,\n      jitter=1e-6,\n      name=None):\n    \"\"\"Model selection for optimal variational hyperparameters.\n\n    Given the full training set (parameterized by `observations` and\n    `observation_index_points`), compute the optimal variational\n    location and scale for the VGP. This is based of the method suggested\n    in [Titsias, 2009][1].\n\n    Args:\n      kernel: `PositiveSemidefiniteKernel`-like instance representing the\n        GP's covariance function.\n      inducing_index_points: `float` `Tensor` of locations of inducing points in\n        the index set. Shape has the form `[b1, ..., bB, e2, f1, ..., fF]`, just\n        like `observation_index_points`. The batch shape components needn't be\n        identical to those of `observation_index_points`, but must be broadcast\n        compatible with them.\n      observation_index_points: `float` `Tensor` representing finite (batch of)\n        vector(s) of points where observations are defined. Shape has the\n        form `[b1, ..., bB, e1, f1, ..., fF]` where `F` is the number of feature\n        dimensions and must equal `kernel.feature_ndims` and `e1` is the number\n        (size) of index points in each batch (we denote it `e1` to distinguish\n        it from the numer of inducing index points, denoted `e2` below).\n      observations: `float` `Tensor` representing collection, or batch of\n        collections, of observations corresponding to\n        `observation_index_points`. Shape has the form `[b1, ..., bB, e]`, which\n        must be brodcastable with the batch and example shapes of\n        `observation_index_points`. The batch shape `[b1, ..., bB]` must be\n        broadcastable with the shapes of all other batched parameters\n        (`kernel.batch_shape`, `observation_index_points`, etc.).\n      observation_noise_variance: `float` `Tensor` representing the variance\n        of the noise in the Normal likelihood distribution of the model. May be\n        batched, in which case the batch shape must be broadcastable with the\n        shapes of all other batched parameters (`kernel.batch_shape`,\n        `index_points`, etc.).\n        Default value: `0.`\n      mean_fn: Python `callable` that acts on index points to produce a (batch\n        of) vector(s) of mean values at those index points. Takes a `Tensor` of\n        shape `[b1, ..., bB, f1, ..., fF]` and returns a `Tensor` whose shape is\n        (broadcastable with) `[b1, ..., bB]`. Default value: `None` implies\n        constant zero function.\n      jitter: `float` scalar `Tensor` added to the diagonal of the covariance\n        matrix to ensure positive definiteness of the covariance matrix.\n        Default value: `1e-6`.\n      name: Python `str` name prefixed to Ops created by this class.\n        Default value: \"optimal_variational_posterior\".\n    Returns:\n      loc, scale: Tuple representing the variational location and scale.\n    Raises:\n      ValueError: if `mean_fn` is not `None` and is not callable.\n\n    #### References\n\n    [1]: Titsias, M. \"Variational Model Selection for Sparse Gaussian Process\n         Regression\", 2009.\n         http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf\n    \"\"\"\n\n    with tf.name_scope(name or 'optimal_variational_posterior'):\n      dtype = dtype_util.common_dtype(\n          [inducing_index_points,\n           observation_index_points,\n           observations,\n           observation_noise_variance,\n           jitter], tf.float32)\n\n      inducing_index_points = tf.convert_to_tensor(\n          value=inducing_index_points,\n          dtype=dtype, name='inducing_index_points')\n      observation_index_points = tf.convert_to_tensor(\n          value=observation_index_points, dtype=dtype,\n          name='observation_index_points')\n      observations = tf.convert_to_tensor(\n          value=observations, dtype=dtype, name='observations')\n      observation_noise_variance = tf.convert_to_tensor(\n          value=observation_noise_variance,\n          dtype=dtype,\n          name='observation_noise_variance')\n      jitter = tf.convert_to_tensor(\n          value=jitter, dtype=dtype, name='jitter')\n\n      # Default to a constant zero function.\n      if mean_fn is None:\n        mean_fn = lambda x: tf.zeros([1], dtype=dtype)\n      else:\n        if not callable(mean_fn):\n          raise ValueError('`mean_fn` must be a Python callable')\n\n      # z are the inducing points and x are the observation index points.\n      kzz = kernel.matrix(inducing_index_points, inducing_index_points)\n      kzx = kernel.matrix(inducing_index_points, observation_index_points)\n\n      noise_var_inv = tf.math.reciprocal(observation_noise_variance)\n\n      sigma_inv = _add_diagonal_shift(\n          kzz + noise_var_inv * tf.matmul(kzx, kzx, adjoint_b=True),\n          jitter)\n\n      chol_sigma_inv = tf.linalg.cholesky(sigma_inv)\n\n      kzx_lin_op = tf.linalg.LinearOperatorFullMatrix(kzx)\n      kzx_obs = kzx_lin_op.matvec(\n          observations - mean_fn(observation_index_points))\n      kzz_lin_op = tf.linalg.LinearOperatorFullMatrix(kzz)\n      loc = (mean_fn(inducing_index_points) +\n             noise_var_inv * kzz_lin_op.matvec(\n                 _solve_cholesky_factored_system_vec(chol_sigma_inv, kzx_obs)))\n\n      chol_sigma_inv_lin_op = tf.linalg.LinearOperatorLowerTriangular(\n          chol_sigma_inv)\n      scale = chol_sigma_inv_lin_op.solve(kzz)\n\n      return loc, scale", "language": "python", "code": "def optimal_variational_posterior(\n      kernel,\n      inducing_index_points,\n      observation_index_points,\n      observations,\n      observation_noise_variance,\n      mean_fn=None,\n      jitter=1e-6,\n      name=None):\n    \"\"\"Model selection for optimal variational hyperparameters.\n\n    Given the full training set (parameterized by `observations` and\n    `observation_index_points`), compute the optimal variational\n    location and scale for the VGP. This is based of the method suggested\n    in [Titsias, 2009][1].\n\n    Args:\n      kernel: `PositiveSemidefiniteKernel`-like instance representing the\n        GP's covariance function.\n      inducing_index_points: `float` `Tensor` of locations of inducing points in\n        the index set. Shape has the form `[b1, ..., bB, e2, f1, ..., fF]`, just\n        like `observation_index_points`. The batch shape components needn't be\n        identical to those of `observation_index_points`, but must be broadcast\n        compatible with them.\n      observation_index_points: `float` `Tensor` representing finite (batch of)\n        vector(s) of points where observations are defined. Shape has the\n        form `[b1, ..., bB, e1, f1, ..., fF]` where `F` is the number of feature\n        dimensions and must equal `kernel.feature_ndims` and `e1` is the number\n        (size) of index points in each batch (we denote it `e1` to distinguish\n        it from the numer of inducing index points, denoted `e2` below).\n      observations: `float` `Tensor` representing collection, or batch of\n        collections, of observations corresponding to\n        `observation_index_points`. Shape has the form `[b1, ..., bB, e]`, which\n        must be brodcastable with the batch and example shapes of\n        `observation_index_points`. The batch shape `[b1, ..., bB]` must be\n        broadcastable with the shapes of all other batched parameters\n        (`kernel.batch_shape`, `observation_index_points`, etc.).\n      observation_noise_variance: `float` `Tensor` representing the variance\n        of the noise in the Normal likelihood distribution of the model. May be\n        batched, in which case the batch shape must be broadcastable with the\n        shapes of all other batched parameters (`kernel.batch_shape`,\n        `index_points`, etc.).\n        Default value: `0.`\n      mean_fn: Python `callable` that acts on index points to produce a (batch\n        of) vector(s) of mean values at those index points. Takes a `Tensor` of\n        shape `[b1, ..., bB, f1, ..., fF]` and returns a `Tensor` whose shape is\n        (broadcastable with) `[b1, ..., bB]`. Default value: `None` implies\n        constant zero function.\n      jitter: `float` scalar `Tensor` added to the diagonal of the covariance\n        matrix to ensure positive definiteness of the covariance matrix.\n        Default value: `1e-6`.\n      name: Python `str` name prefixed to Ops created by this class.\n        Default value: \"optimal_variational_posterior\".\n    Returns:\n      loc, scale: Tuple representing the variational location and scale.\n    Raises:\n      ValueError: if `mean_fn` is not `None` and is not callable.\n\n    #### References\n\n    [1]: Titsias, M. \"Variational Model Selection for Sparse Gaussian Process\n         Regression\", 2009.\n         http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf\n    \"\"\"\n\n    with tf.name_scope(name or 'optimal_variational_posterior'):\n      dtype = dtype_util.common_dtype(\n          [inducing_index_points,\n           observation_index_points,\n           observations,\n           observation_noise_variance,\n           jitter], tf.float32)\n\n      inducing_index_points = tf.convert_to_tensor(\n          value=inducing_index_points,\n          dtype=dtype, name='inducing_index_points')\n      observation_index_points = tf.convert_to_tensor(\n          value=observation_index_points, dtype=dtype,\n          name='observation_index_points')\n      observations = tf.convert_to_tensor(\n          value=observations, dtype=dtype, name='observations')\n      observation_noise_variance = tf.convert_to_tensor(\n          value=observation_noise_variance,\n          dtype=dtype,\n          name='observation_noise_variance')\n      jitter = tf.convert_to_tensor(\n          value=jitter, dtype=dtype, name='jitter')\n\n      # Default to a constant zero function.\n      if mean_fn is None:\n        mean_fn = lambda x: tf.zeros([1], dtype=dtype)\n      else:\n        if not callable(mean_fn):\n          raise ValueError('`mean_fn` must be a Python callable')\n\n      # z are the inducing points and x are the observation index points.\n      kzz = kernel.matrix(inducing_index_points, inducing_index_points)\n      kzx = kernel.matrix(inducing_index_points, observation_index_points)\n\n      noise_var_inv = tf.math.reciprocal(observation_noise_variance)\n\n      sigma_inv = _add_diagonal_shift(\n          kzz + noise_var_inv * tf.matmul(kzx, kzx, adjoint_b=True),\n          jitter)\n\n      chol_sigma_inv = tf.linalg.cholesky(sigma_inv)\n\n      kzx_lin_op = tf.linalg.LinearOperatorFullMatrix(kzx)\n      kzx_obs = kzx_lin_op.matvec(\n          observations - mean_fn(observation_index_points))\n      kzz_lin_op = tf.linalg.LinearOperatorFullMatrix(kzz)\n      loc = (mean_fn(inducing_index_points) +\n             noise_var_inv * kzz_lin_op.matvec(\n                 _solve_cholesky_factored_system_vec(chol_sigma_inv, kzx_obs)))\n\n      chol_sigma_inv_lin_op = tf.linalg.LinearOperatorLowerTriangular(\n          chol_sigma_inv)\n      scale = chol_sigma_inv_lin_op.solve(kzz)\n\n      return loc, scale", "code_tokens": ["def", "optimal_variational_posterior", "(", "kernel", ",", "inducing_index_points", ",", "observation_index_points", ",", "observations", ",", "observation_noise_variance", ",", "mean_fn", "=", "None", ",", "jitter", "=", "1e-6", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "'optimal_variational_posterior'", ")", ":", "dtype", "=", "dtype_util", ".", "common_dtype", "(", "[", "inducing_index_points", ",", "observation_index_points", ",", "observations", ",", "observation_noise_variance", ",", "jitter", "]", ",", "tf", ".", "float32", ")", "inducing_index_points", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "inducing_index_points", ",", "dtype", "=", "dtype", ",", "name", "=", "'inducing_index_points'", ")", "observation_index_points", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "observation_index_points", ",", "dtype", "=", "dtype", ",", "name", "=", "'observation_index_points'", ")", "observations", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "observations", ",", "dtype", "=", "dtype", ",", "name", "=", "'observations'", ")", "observation_noise_variance", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "observation_noise_variance", ",", "dtype", "=", "dtype", ",", "name", "=", "'observation_noise_variance'", ")", "jitter", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "jitter", ",", "dtype", "=", "dtype", ",", "name", "=", "'jitter'", ")", "# Default to a constant zero function.", "if", "mean_fn", "is", "None", ":", "mean_fn", "=", "lambda", "x", ":", "tf", ".", "zeros", "(", "[", "1", "]", ",", "dtype", "=", "dtype", ")", "else", ":", "if", "not", "callable", "(", "mean_fn", ")", ":", "raise", "ValueError", "(", "'`mean_fn` must be a Python callable'", ")", "# z are the inducing points and x are the observation index points.", "kzz", "=", "kernel", ".", "matrix", "(", "inducing_index_points", ",", "inducing_index_points", ")", "kzx", "=", "kernel", ".", "matrix", "(", "inducing_index_points", ",", "observation_index_points", ")", "noise_var_inv", "=", "tf", ".", "math", ".", "reciprocal", "(", "observation_noise_variance", ")", "sigma_inv", "=", "_add_diagonal_shift", "(", "kzz", "+", "noise_var_inv", "*", "tf", ".", "matmul", "(", "kzx", ",", "kzx", ",", "adjoint_b", "=", "True", ")", ",", "jitter", ")", "chol_sigma_inv", "=", "tf", ".", "linalg", ".", "cholesky", "(", "sigma_inv", ")", "kzx_lin_op", "=", "tf", ".", "linalg", ".", "LinearOperatorFullMatrix", "(", "kzx", ")", "kzx_obs", "=", "kzx_lin_op", ".", "matvec", "(", "observations", "-", "mean_fn", "(", "observation_index_points", ")", ")", "kzz_lin_op", "=", "tf", ".", "linalg", ".", "LinearOperatorFullMatrix", "(", "kzz", ")", "loc", "=", "(", "mean_fn", "(", "inducing_index_points", ")", "+", "noise_var_inv", "*", "kzz_lin_op", ".", "matvec", "(", "_solve_cholesky_factored_system_vec", "(", "chol_sigma_inv", ",", "kzx_obs", ")", ")", ")", "chol_sigma_inv_lin_op", "=", "tf", ".", "linalg", ".", "LinearOperatorLowerTriangular", "(", "chol_sigma_inv", ")", "scale", "=", "chol_sigma_inv_lin_op", ".", "solve", "(", "kzz", ")", "return", "loc", ",", "scale"], "docstring": "Model selection for optimal variational hyperparameters.\n\n    Given the full training set (parameterized by `observations` and\n    `observation_index_points`), compute the optimal variational\n    location and scale for the VGP. This is based of the method suggested\n    in [Titsias, 2009][1].\n\n    Args:\n      kernel: `PositiveSemidefiniteKernel`-like instance representing the\n        GP's covariance function.\n      inducing_index_points: `float` `Tensor` of locations of inducing points in\n        the index set. Shape has the form `[b1, ..., bB, e2, f1, ..., fF]`, just\n        like `observation_index_points`. The batch shape components needn't be\n        identical to those of `observation_index_points`, but must be broadcast\n        compatible with them.\n      observation_index_points: `float` `Tensor` representing finite (batch of)\n        vector(s) of points where observations are defined. Shape has the\n        form `[b1, ..., bB, e1, f1, ..., fF]` where `F` is the number of feature\n        dimensions and must equal `kernel.feature_ndims` and `e1` is the number\n        (size) of index points in each batch (we denote it `e1` to distinguish\n        it from the numer of inducing index points, denoted `e2` below).\n      observations: `float` `Tensor` representing collection, or batch of\n        collections, of observations corresponding to\n        `observation_index_points`. Shape has the form `[b1, ..., bB, e]`, which\n        must be brodcastable with the batch and example shapes of\n        `observation_index_points`. The batch shape `[b1, ..., bB]` must be\n        broadcastable with the shapes of all other batched parameters\n        (`kernel.batch_shape`, `observation_index_points`, etc.).\n      observation_noise_variance: `float` `Tensor` representing the variance\n        of the noise in the Normal likelihood distribution of the model. May be\n        batched, in which case the batch shape must be broadcastable with the\n        shapes of all other batched parameters (`kernel.batch_shape`,\n        `index_points`, etc.).\n        Default value: `0.`\n      mean_fn: Python `callable` that acts on index points to produce a (batch\n        of) vector(s) of mean values at those index points. Takes a `Tensor` of\n        shape `[b1, ..., bB, f1, ..., fF]` and returns a `Tensor` whose shape is\n        (broadcastable with) `[b1, ..., bB]`. Default value: `None` implies\n        constant zero function.\n      jitter: `float` scalar `Tensor` added to the diagonal of the covariance\n        matrix to ensure positive definiteness of the covariance matrix.\n        Default value: `1e-6`.\n      name: Python `str` name prefixed to Ops created by this class.\n        Default value: \"optimal_variational_posterior\".\n    Returns:\n      loc, scale: Tuple representing the variational location and scale.\n    Raises:\n      ValueError: if `mean_fn` is not `None` and is not callable.\n\n    #### References\n\n    [1]: Titsias, M. \"Variational Model Selection for Sparse Gaussian Process\n         Regression\", 2009.\n         http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf", "docstring_tokens": ["Model", "selection", "for", "optimal", "variational", "hyperparameters", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/variational_gaussian_process.py#L845-L964", "partition": "test", "index": 1133, "time": "2019-03-05 18:34:56"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/variational_gaussian_process.py", "func_name": "VariationalGaussianProcess.variational_loss", "original_string": "def variational_loss(self,\n                       observations,\n                       observation_index_points=None,\n                       kl_weight=1.,\n                       name='variational_loss'):\n    \"\"\"Variational loss for the VGP.\n\n    Given `observations` and `observation_index_points`, compute the\n    negative variational lower bound as specified in [Hensman, 2013][1].\n\n    Args:\n      observations: `float` `Tensor` representing collection, or batch of\n        collections, of observations corresponding to\n        `observation_index_points`. Shape has the form `[b1, ..., bB, e]`, which\n        must be brodcastable with the batch and example shapes of\n        `observation_index_points`. The batch shape `[b1, ..., bB]` must be\n        broadcastable with the shapes of all other batched parameters\n        (`kernel.batch_shape`, `observation_index_points`, etc.).\n      observation_index_points: `float` `Tensor` representing finite (batch of)\n        vector(s) of points where observations are defined. Shape has the\n        form `[b1, ..., bB, e1, f1, ..., fF]` where `F` is the number of feature\n        dimensions and must equal `kernel.feature_ndims` and `e1` is the number\n        (size) of index points in each batch (we denote it `e1` to distinguish\n        it from the numer of inducing index points, denoted `e2` below). If\n        set to `None` uses `index_points` as the origin for observations.\n        Default value: None.\n      kl_weight: Amount by which to scale the KL divergence loss between prior\n        and posterior.\n        Default value: 1.\n      name: Python `str` name prefixed to Ops created by this class.\n        Default value: \"GaussianProcess\".\n    Returns:\n      loss: Scalar tensor representing the negative variational lower bound.\n        Can be directly used in a `tf.Optimizer`.\n    Raises:\n      ValueError: if `mean_fn` is not `None` and is not callable.\n\n    #### References\n\n    [1]: Hensman, J., Lawrence, N. \"Gaussian Processes for Big Data\", 2013\n         https://arxiv.org/abs/1309.6835\n    \"\"\"\n\n    with tf.name_scope(name or 'variational_gp_loss'):\n      if observation_index_points is None:\n        observation_index_points = self._index_points\n      observation_index_points = tf.convert_to_tensor(\n          value=observation_index_points, dtype=self._dtype,\n          name='observation_index_points')\n      observations = tf.convert_to_tensor(\n          value=observations, dtype=self._dtype, name='observations')\n      kl_weight = tf.convert_to_tensor(\n          value=kl_weight, dtype=self._dtype,\n          name='kl_weight')\n\n      # The variational loss is a negative ELBO. The ELBO can be broken down\n      # into three terms:\n      #  1. a likelihood term\n      #  2. a trace term arising from the covariance of the posterior predictive\n\n      kzx = self.kernel.matrix(self._inducing_index_points,\n                               observation_index_points)\n\n      kzx_linop = tf.linalg.LinearOperatorFullMatrix(kzx)\n      loc = (self._mean_fn(observation_index_points) +\n             kzx_linop.matvec(self._kzz_inv_varloc, adjoint=True))\n\n      likelihood = independent.Independent(\n          normal.Normal(\n              loc=loc,\n              scale=tf.sqrt(self._observation_noise_variance + self._jitter),\n              name='NormalLikelihood'),\n          reinterpreted_batch_ndims=1)\n      obs_ll = likelihood.log_prob(observations)\n\n      chol_kzz_linop = tf.linalg.LinearOperatorLowerTriangular(self._chol_kzz)\n      chol_kzz_inv_kzx = chol_kzz_linop.solve(kzx)\n      kzz_inv_kzx = chol_kzz_linop.solve(chol_kzz_inv_kzx, adjoint=True)\n\n      kxx_diag = tf.linalg.diag_part(\n          self.kernel.matrix(\n              observation_index_points, observation_index_points))\n      ktilde_trace_term = (\n          tf.reduce_sum(input_tensor=kxx_diag, axis=-1) -\n          tf.reduce_sum(input_tensor=chol_kzz_inv_kzx ** 2, axis=[-2, -1]))\n\n      # Tr(SB)\n      # where S = A A.T, A = variational_inducing_observations_scale\n      # and B = Kzz^-1 Kzx Kzx.T Kzz^-1\n      #\n      # Now Tr(SB) = Tr(A A.T Kzz^-1 Kzx Kzx.T Kzz^-1)\n      #            = Tr(A.T Kzz^-1 Kzx Kzx.T Kzz^-1 A)\n      #            = sum_ij (A.T Kzz^-1 Kzx)_{ij}^2\n      other_trace_term = tf.reduce_sum(\n          input_tensor=(\n              self._variational_inducing_observations_posterior.scale.matmul(\n                  kzz_inv_kzx) ** 2),\n          axis=[-2, -1])\n\n      trace_term = (.5 * (ktilde_trace_term + other_trace_term) /\n                    self._observation_noise_variance)\n\n      inducing_prior = gaussian_process.GaussianProcess(\n          kernel=self._kernel,\n          mean_fn=self._mean_fn,\n          index_points=self._inducing_index_points,\n          observation_noise_variance=self._observation_noise_variance)\n\n      kl_term = kl_weight * kullback_leibler.kl_divergence(\n          self._variational_inducing_observations_posterior,\n          inducing_prior)\n\n      lower_bound = (obs_ll - trace_term - kl_term)\n\n      return -tf.reduce_mean(input_tensor=lower_bound)", "language": "python", "code": "def variational_loss(self,\n                       observations,\n                       observation_index_points=None,\n                       kl_weight=1.,\n                       name='variational_loss'):\n    \"\"\"Variational loss for the VGP.\n\n    Given `observations` and `observation_index_points`, compute the\n    negative variational lower bound as specified in [Hensman, 2013][1].\n\n    Args:\n      observations: `float` `Tensor` representing collection, or batch of\n        collections, of observations corresponding to\n        `observation_index_points`. Shape has the form `[b1, ..., bB, e]`, which\n        must be brodcastable with the batch and example shapes of\n        `observation_index_points`. The batch shape `[b1, ..., bB]` must be\n        broadcastable with the shapes of all other batched parameters\n        (`kernel.batch_shape`, `observation_index_points`, etc.).\n      observation_index_points: `float` `Tensor` representing finite (batch of)\n        vector(s) of points where observations are defined. Shape has the\n        form `[b1, ..., bB, e1, f1, ..., fF]` where `F` is the number of feature\n        dimensions and must equal `kernel.feature_ndims` and `e1` is the number\n        (size) of index points in each batch (we denote it `e1` to distinguish\n        it from the numer of inducing index points, denoted `e2` below). If\n        set to `None` uses `index_points` as the origin for observations.\n        Default value: None.\n      kl_weight: Amount by which to scale the KL divergence loss between prior\n        and posterior.\n        Default value: 1.\n      name: Python `str` name prefixed to Ops created by this class.\n        Default value: \"GaussianProcess\".\n    Returns:\n      loss: Scalar tensor representing the negative variational lower bound.\n        Can be directly used in a `tf.Optimizer`.\n    Raises:\n      ValueError: if `mean_fn` is not `None` and is not callable.\n\n    #### References\n\n    [1]: Hensman, J., Lawrence, N. \"Gaussian Processes for Big Data\", 2013\n         https://arxiv.org/abs/1309.6835\n    \"\"\"\n\n    with tf.name_scope(name or 'variational_gp_loss'):\n      if observation_index_points is None:\n        observation_index_points = self._index_points\n      observation_index_points = tf.convert_to_tensor(\n          value=observation_index_points, dtype=self._dtype,\n          name='observation_index_points')\n      observations = tf.convert_to_tensor(\n          value=observations, dtype=self._dtype, name='observations')\n      kl_weight = tf.convert_to_tensor(\n          value=kl_weight, dtype=self._dtype,\n          name='kl_weight')\n\n      # The variational loss is a negative ELBO. The ELBO can be broken down\n      # into three terms:\n      #  1. a likelihood term\n      #  2. a trace term arising from the covariance of the posterior predictive\n\n      kzx = self.kernel.matrix(self._inducing_index_points,\n                               observation_index_points)\n\n      kzx_linop = tf.linalg.LinearOperatorFullMatrix(kzx)\n      loc = (self._mean_fn(observation_index_points) +\n             kzx_linop.matvec(self._kzz_inv_varloc, adjoint=True))\n\n      likelihood = independent.Independent(\n          normal.Normal(\n              loc=loc,\n              scale=tf.sqrt(self._observation_noise_variance + self._jitter),\n              name='NormalLikelihood'),\n          reinterpreted_batch_ndims=1)\n      obs_ll = likelihood.log_prob(observations)\n\n      chol_kzz_linop = tf.linalg.LinearOperatorLowerTriangular(self._chol_kzz)\n      chol_kzz_inv_kzx = chol_kzz_linop.solve(kzx)\n      kzz_inv_kzx = chol_kzz_linop.solve(chol_kzz_inv_kzx, adjoint=True)\n\n      kxx_diag = tf.linalg.diag_part(\n          self.kernel.matrix(\n              observation_index_points, observation_index_points))\n      ktilde_trace_term = (\n          tf.reduce_sum(input_tensor=kxx_diag, axis=-1) -\n          tf.reduce_sum(input_tensor=chol_kzz_inv_kzx ** 2, axis=[-2, -1]))\n\n      # Tr(SB)\n      # where S = A A.T, A = variational_inducing_observations_scale\n      # and B = Kzz^-1 Kzx Kzx.T Kzz^-1\n      #\n      # Now Tr(SB) = Tr(A A.T Kzz^-1 Kzx Kzx.T Kzz^-1)\n      #            = Tr(A.T Kzz^-1 Kzx Kzx.T Kzz^-1 A)\n      #            = sum_ij (A.T Kzz^-1 Kzx)_{ij}^2\n      other_trace_term = tf.reduce_sum(\n          input_tensor=(\n              self._variational_inducing_observations_posterior.scale.matmul(\n                  kzz_inv_kzx) ** 2),\n          axis=[-2, -1])\n\n      trace_term = (.5 * (ktilde_trace_term + other_trace_term) /\n                    self._observation_noise_variance)\n\n      inducing_prior = gaussian_process.GaussianProcess(\n          kernel=self._kernel,\n          mean_fn=self._mean_fn,\n          index_points=self._inducing_index_points,\n          observation_noise_variance=self._observation_noise_variance)\n\n      kl_term = kl_weight * kullback_leibler.kl_divergence(\n          self._variational_inducing_observations_posterior,\n          inducing_prior)\n\n      lower_bound = (obs_ll - trace_term - kl_term)\n\n      return -tf.reduce_mean(input_tensor=lower_bound)", "code_tokens": ["def", "variational_loss", "(", "self", ",", "observations", ",", "observation_index_points", "=", "None", ",", "kl_weight", "=", "1.", ",", "name", "=", "'variational_loss'", ")", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "'variational_gp_loss'", ")", ":", "if", "observation_index_points", "is", "None", ":", "observation_index_points", "=", "self", ".", "_index_points", "observation_index_points", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "observation_index_points", ",", "dtype", "=", "self", ".", "_dtype", ",", "name", "=", "'observation_index_points'", ")", "observations", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "observations", ",", "dtype", "=", "self", ".", "_dtype", ",", "name", "=", "'observations'", ")", "kl_weight", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "kl_weight", ",", "dtype", "=", "self", ".", "_dtype", ",", "name", "=", "'kl_weight'", ")", "# The variational loss is a negative ELBO. The ELBO can be broken down", "# into three terms:", "#  1. a likelihood term", "#  2. a trace term arising from the covariance of the posterior predictive", "kzx", "=", "self", ".", "kernel", ".", "matrix", "(", "self", ".", "_inducing_index_points", ",", "observation_index_points", ")", "kzx_linop", "=", "tf", ".", "linalg", ".", "LinearOperatorFullMatrix", "(", "kzx", ")", "loc", "=", "(", "self", ".", "_mean_fn", "(", "observation_index_points", ")", "+", "kzx_linop", ".", "matvec", "(", "self", ".", "_kzz_inv_varloc", ",", "adjoint", "=", "True", ")", ")", "likelihood", "=", "independent", ".", "Independent", "(", "normal", ".", "Normal", "(", "loc", "=", "loc", ",", "scale", "=", "tf", ".", "sqrt", "(", "self", ".", "_observation_noise_variance", "+", "self", ".", "_jitter", ")", ",", "name", "=", "'NormalLikelihood'", ")", ",", "reinterpreted_batch_ndims", "=", "1", ")", "obs_ll", "=", "likelihood", ".", "log_prob", "(", "observations", ")", "chol_kzz_linop", "=", "tf", ".", "linalg", ".", "LinearOperatorLowerTriangular", "(", "self", ".", "_chol_kzz", ")", "chol_kzz_inv_kzx", "=", "chol_kzz_linop", ".", "solve", "(", "kzx", ")", "kzz_inv_kzx", "=", "chol_kzz_linop", ".", "solve", "(", "chol_kzz_inv_kzx", ",", "adjoint", "=", "True", ")", "kxx_diag", "=", "tf", ".", "linalg", ".", "diag_part", "(", "self", ".", "kernel", ".", "matrix", "(", "observation_index_points", ",", "observation_index_points", ")", ")", "ktilde_trace_term", "=", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "kxx_diag", ",", "axis", "=", "-", "1", ")", "-", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "chol_kzz_inv_kzx", "**", "2", ",", "axis", "=", "[", "-", "2", ",", "-", "1", "]", ")", ")", "# Tr(SB)", "# where S = A A.T, A = variational_inducing_observations_scale", "# and B = Kzz^-1 Kzx Kzx.T Kzz^-1", "#", "# Now Tr(SB) = Tr(A A.T Kzz^-1 Kzx Kzx.T Kzz^-1)", "#            = Tr(A.T Kzz^-1 Kzx Kzx.T Kzz^-1 A)", "#            = sum_ij (A.T Kzz^-1 Kzx)_{ij}^2", "other_trace_term", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "(", "self", ".", "_variational_inducing_observations_posterior", ".", "scale", ".", "matmul", "(", "kzz_inv_kzx", ")", "**", "2", ")", ",", "axis", "=", "[", "-", "2", ",", "-", "1", "]", ")", "trace_term", "=", "(", ".5", "*", "(", "ktilde_trace_term", "+", "other_trace_term", ")", "/", "self", ".", "_observation_noise_variance", ")", "inducing_prior", "=", "gaussian_process", ".", "GaussianProcess", "(", "kernel", "=", "self", ".", "_kernel", ",", "mean_fn", "=", "self", ".", "_mean_fn", ",", "index_points", "=", "self", ".", "_inducing_index_points", ",", "observation_noise_variance", "=", "self", ".", "_observation_noise_variance", ")", "kl_term", "=", "kl_weight", "*", "kullback_leibler", ".", "kl_divergence", "(", "self", ".", "_variational_inducing_observations_posterior", ",", "inducing_prior", ")", "lower_bound", "=", "(", "obs_ll", "-", "trace_term", "-", "kl_term", ")", "return", "-", "tf", ".", "reduce_mean", "(", "input_tensor", "=", "lower_bound", ")"], "docstring": "Variational loss for the VGP.\n\n    Given `observations` and `observation_index_points`, compute the\n    negative variational lower bound as specified in [Hensman, 2013][1].\n\n    Args:\n      observations: `float` `Tensor` representing collection, or batch of\n        collections, of observations corresponding to\n        `observation_index_points`. Shape has the form `[b1, ..., bB, e]`, which\n        must be brodcastable with the batch and example shapes of\n        `observation_index_points`. The batch shape `[b1, ..., bB]` must be\n        broadcastable with the shapes of all other batched parameters\n        (`kernel.batch_shape`, `observation_index_points`, etc.).\n      observation_index_points: `float` `Tensor` representing finite (batch of)\n        vector(s) of points where observations are defined. Shape has the\n        form `[b1, ..., bB, e1, f1, ..., fF]` where `F` is the number of feature\n        dimensions and must equal `kernel.feature_ndims` and `e1` is the number\n        (size) of index points in each batch (we denote it `e1` to distinguish\n        it from the numer of inducing index points, denoted `e2` below). If\n        set to `None` uses `index_points` as the origin for observations.\n        Default value: None.\n      kl_weight: Amount by which to scale the KL divergence loss between prior\n        and posterior.\n        Default value: 1.\n      name: Python `str` name prefixed to Ops created by this class.\n        Default value: \"GaussianProcess\".\n    Returns:\n      loss: Scalar tensor representing the negative variational lower bound.\n        Can be directly used in a `tf.Optimizer`.\n    Raises:\n      ValueError: if `mean_fn` is not `None` and is not callable.\n\n    #### References\n\n    [1]: Hensman, J., Lawrence, N. \"Gaussian Processes for Big Data\", 2013\n         https://arxiv.org/abs/1309.6835", "docstring_tokens": ["Variational", "loss", "for", "the", "VGP", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/variational_gaussian_process.py#L728-L842", "partition": "test", "index": 1132, "time": "2019-03-05 18:34:56"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/quantiles.py", "func_name": "histogram", "original_string": "def histogram(x,\n              edges,\n              axis=None,\n              extend_lower_interval=False,\n              extend_upper_interval=False,\n              dtype=None,\n              name=None):\n  \"\"\"Count how often `x` falls in intervals defined by `edges`.\n\n  Given `edges = [c0, ..., cK]`, defining intervals\n  `I0 = [c0, c1)`, `I1 = [c1, c2)`, ..., `I_{K-1} = [c_{K-1}, cK]`,\n  This function counts how often `x` falls into each interval.\n\n  Values of `x` outside of the intervals cause errors.  Consider using\n  `extend_lower_interval`, `extend_upper_interval` to deal with this.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.  If `axis` is not\n      `None`, must have statically known number of dimensions. The\n      `axis` kwarg determines which dimensions index iid samples.\n      Other dimensions of `x` index \"events\" for which we will compute different\n      histograms.\n    edges:  `Tensor` of same `dtype` as `x`.  The first dimension indexes edges\n      of intervals.  Must either be `1-D` or have `edges.shape[1:]` the same\n      as the dimensions of `x` excluding `axis`.\n      If `rank(edges) > 1`, `edges[k]` designates a shape `edges.shape[1:]`\n      `Tensor` of interval edges for the corresponding dimensions of `x`.\n    axis:  Optional `0-D` or `1-D` integer `Tensor` with constant\n      values. The axis in `x` that index iid samples.\n      `Default value:` `None` (treat every dimension as sample dimension).\n    extend_lower_interval:  Python `bool`.  If `True`, extend the lowest\n      interval `I0` to `(-inf, c1]`.\n    extend_upper_interval:  Python `bool`.  If `True`, extend the upper\n      interval `I_{K-1}` to `[c_{K-1}, +inf)`.\n    dtype: The output type (`int32` or `int64`). `Default value:` `x.dtype`.\n    name:  A Python string name to prepend to created ops.\n      `Default value:` 'histogram'\n\n  Returns:\n    counts: `Tensor` of type `dtype` and, with\n      `~axis = [i for i in range(arr.ndim) if i not in axis]`,\n      `counts.shape = [edges.shape[0]] + x.shape[~axis]`.\n      With `I` a multi-index into `~axis`, `counts[k][I]` is the number of times\n      event(s) fell into the `kth` interval of `edges`.\n\n  #### Examples\n\n  ```python\n  # x.shape = [1000, 2]\n  # x[:, 0] ~ Uniform(0, 1), x[:, 1] ~ Uniform(1, 2).\n  x = tf.stack([tf.random_uniform([1000]), 1 + tf.random_uniform([1000])],\n               axis=-1)\n\n  # edges ==> bins [0, 0.5), [0.5, 1.0), [1.0, 1.5), [1.5, 2.0].\n  edges = [0., 0.5, 1.0, 1.5, 2.0]\n\n  tfp.stats.histogram(x, edges)\n  ==> approximately [500, 500, 500, 500]\n\n  tfp.stats.histogram(x, edges, axis=0)\n  ==> approximately [[500, 500, 0, 0], [0, 0, 500, 500]]\n  ```\n\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'histogram', values=[x, edges, axis]):\n\n    # Tensor conversions.\n    in_dtype = dtype_util.common_dtype([x, edges], preferred_dtype=tf.float32)\n\n    x = tf.convert_to_tensor(value=x, name='x', dtype=in_dtype)\n    edges = tf.convert_to_tensor(value=edges, name='edges', dtype=in_dtype)\n\n    # Move dims in axis to the left end as one flattened dim.\n    # After this, x.shape = [n_samples] + E.\n    if axis is None:\n      x = tf.reshape(x, shape=[-1])\n    else:\n      x_ndims = _get_static_ndims(\n          x, expect_static=True, expect_ndims_at_least=1)\n      axis = _make_static_axis_non_negative_list(axis, x_ndims)\n      if not axis:\n        raise ValueError('`axis` cannot be empty.  Found: {}'.format(axis))\n      x = _move_dims_to_flat_end(x, axis, x_ndims, right_end=False)\n\n    # bins.shape = x.shape = [n_samples] + E,\n    # and bins[i] is a shape E Tensor of the bins that sample `i` fell into.\n    # E is the \"event shape\", which is [] if axis is None.\n    bins = find_bins(\n        x,\n        edges=edges,\n        # If not extending intervals, then values outside the edges will return\n        # -1, which gives an error when fed to bincount.\n        extend_lower_interval=extend_lower_interval,\n        extend_upper_interval=extend_upper_interval,\n        dtype=tf.int32)\n\n    # TODO(b/124015136) Use standard tf.math.bincount once it supports `axis`.\n    counts = count_integers(\n        bins,\n        # Ensure we get correct output, even if x did not fall into every bin\n        minlength=tf.shape(input=edges)[0] - 1,\n        maxlength=tf.shape(input=edges)[0] - 1,\n        axis=0,\n        dtype=dtype or in_dtype)\n    n_edges = tf.compat.dimension_value(edges.shape[0])\n    if n_edges is not None:\n      counts.set_shape(\n          tf.TensorShape([n_edges - 1]).concatenate(counts.shape[1:]))\n    return counts", "language": "python", "code": "def histogram(x,\n              edges,\n              axis=None,\n              extend_lower_interval=False,\n              extend_upper_interval=False,\n              dtype=None,\n              name=None):\n  \"\"\"Count how often `x` falls in intervals defined by `edges`.\n\n  Given `edges = [c0, ..., cK]`, defining intervals\n  `I0 = [c0, c1)`, `I1 = [c1, c2)`, ..., `I_{K-1} = [c_{K-1}, cK]`,\n  This function counts how often `x` falls into each interval.\n\n  Values of `x` outside of the intervals cause errors.  Consider using\n  `extend_lower_interval`, `extend_upper_interval` to deal with this.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.  If `axis` is not\n      `None`, must have statically known number of dimensions. The\n      `axis` kwarg determines which dimensions index iid samples.\n      Other dimensions of `x` index \"events\" for which we will compute different\n      histograms.\n    edges:  `Tensor` of same `dtype` as `x`.  The first dimension indexes edges\n      of intervals.  Must either be `1-D` or have `edges.shape[1:]` the same\n      as the dimensions of `x` excluding `axis`.\n      If `rank(edges) > 1`, `edges[k]` designates a shape `edges.shape[1:]`\n      `Tensor` of interval edges for the corresponding dimensions of `x`.\n    axis:  Optional `0-D` or `1-D` integer `Tensor` with constant\n      values. The axis in `x` that index iid samples.\n      `Default value:` `None` (treat every dimension as sample dimension).\n    extend_lower_interval:  Python `bool`.  If `True`, extend the lowest\n      interval `I0` to `(-inf, c1]`.\n    extend_upper_interval:  Python `bool`.  If `True`, extend the upper\n      interval `I_{K-1}` to `[c_{K-1}, +inf)`.\n    dtype: The output type (`int32` or `int64`). `Default value:` `x.dtype`.\n    name:  A Python string name to prepend to created ops.\n      `Default value:` 'histogram'\n\n  Returns:\n    counts: `Tensor` of type `dtype` and, with\n      `~axis = [i for i in range(arr.ndim) if i not in axis]`,\n      `counts.shape = [edges.shape[0]] + x.shape[~axis]`.\n      With `I` a multi-index into `~axis`, `counts[k][I]` is the number of times\n      event(s) fell into the `kth` interval of `edges`.\n\n  #### Examples\n\n  ```python\n  # x.shape = [1000, 2]\n  # x[:, 0] ~ Uniform(0, 1), x[:, 1] ~ Uniform(1, 2).\n  x = tf.stack([tf.random_uniform([1000]), 1 + tf.random_uniform([1000])],\n               axis=-1)\n\n  # edges ==> bins [0, 0.5), [0.5, 1.0), [1.0, 1.5), [1.5, 2.0].\n  edges = [0., 0.5, 1.0, 1.5, 2.0]\n\n  tfp.stats.histogram(x, edges)\n  ==> approximately [500, 500, 500, 500]\n\n  tfp.stats.histogram(x, edges, axis=0)\n  ==> approximately [[500, 500, 0, 0], [0, 0, 500, 500]]\n  ```\n\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'histogram', values=[x, edges, axis]):\n\n    # Tensor conversions.\n    in_dtype = dtype_util.common_dtype([x, edges], preferred_dtype=tf.float32)\n\n    x = tf.convert_to_tensor(value=x, name='x', dtype=in_dtype)\n    edges = tf.convert_to_tensor(value=edges, name='edges', dtype=in_dtype)\n\n    # Move dims in axis to the left end as one flattened dim.\n    # After this, x.shape = [n_samples] + E.\n    if axis is None:\n      x = tf.reshape(x, shape=[-1])\n    else:\n      x_ndims = _get_static_ndims(\n          x, expect_static=True, expect_ndims_at_least=1)\n      axis = _make_static_axis_non_negative_list(axis, x_ndims)\n      if not axis:\n        raise ValueError('`axis` cannot be empty.  Found: {}'.format(axis))\n      x = _move_dims_to_flat_end(x, axis, x_ndims, right_end=False)\n\n    # bins.shape = x.shape = [n_samples] + E,\n    # and bins[i] is a shape E Tensor of the bins that sample `i` fell into.\n    # E is the \"event shape\", which is [] if axis is None.\n    bins = find_bins(\n        x,\n        edges=edges,\n        # If not extending intervals, then values outside the edges will return\n        # -1, which gives an error when fed to bincount.\n        extend_lower_interval=extend_lower_interval,\n        extend_upper_interval=extend_upper_interval,\n        dtype=tf.int32)\n\n    # TODO(b/124015136) Use standard tf.math.bincount once it supports `axis`.\n    counts = count_integers(\n        bins,\n        # Ensure we get correct output, even if x did not fall into every bin\n        minlength=tf.shape(input=edges)[0] - 1,\n        maxlength=tf.shape(input=edges)[0] - 1,\n        axis=0,\n        dtype=dtype or in_dtype)\n    n_edges = tf.compat.dimension_value(edges.shape[0])\n    if n_edges is not None:\n      counts.set_shape(\n          tf.TensorShape([n_edges - 1]).concatenate(counts.shape[1:]))\n    return counts", "code_tokens": ["def", "histogram", "(", "x", ",", "edges", ",", "axis", "=", "None", ",", "extend_lower_interval", "=", "False", ",", "extend_upper_interval", "=", "False", ",", "dtype", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'histogram'", ",", "values", "=", "[", "x", ",", "edges", ",", "axis", "]", ")", ":", "# Tensor conversions.", "in_dtype", "=", "dtype_util", ".", "common_dtype", "(", "[", "x", ",", "edges", "]", ",", "preferred_dtype", "=", "tf", ".", "float32", ")", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ",", "dtype", "=", "in_dtype", ")", "edges", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "edges", ",", "name", "=", "'edges'", ",", "dtype", "=", "in_dtype", ")", "# Move dims in axis to the left end as one flattened dim.", "# After this, x.shape = [n_samples] + E.", "if", "axis", "is", "None", ":", "x", "=", "tf", ".", "reshape", "(", "x", ",", "shape", "=", "[", "-", "1", "]", ")", "else", ":", "x_ndims", "=", "_get_static_ndims", "(", "x", ",", "expect_static", "=", "True", ",", "expect_ndims_at_least", "=", "1", ")", "axis", "=", "_make_static_axis_non_negative_list", "(", "axis", ",", "x_ndims", ")", "if", "not", "axis", ":", "raise", "ValueError", "(", "'`axis` cannot be empty.  Found: {}'", ".", "format", "(", "axis", ")", ")", "x", "=", "_move_dims_to_flat_end", "(", "x", ",", "axis", ",", "x_ndims", ",", "right_end", "=", "False", ")", "# bins.shape = x.shape = [n_samples] + E,", "# and bins[i] is a shape E Tensor of the bins that sample `i` fell into.", "# E is the \"event shape\", which is [] if axis is None.", "bins", "=", "find_bins", "(", "x", ",", "edges", "=", "edges", ",", "# If not extending intervals, then values outside the edges will return", "# -1, which gives an error when fed to bincount.", "extend_lower_interval", "=", "extend_lower_interval", ",", "extend_upper_interval", "=", "extend_upper_interval", ",", "dtype", "=", "tf", ".", "int32", ")", "# TODO(b/124015136) Use standard tf.math.bincount once it supports `axis`.", "counts", "=", "count_integers", "(", "bins", ",", "# Ensure we get correct output, even if x did not fall into every bin", "minlength", "=", "tf", ".", "shape", "(", "input", "=", "edges", ")", "[", "0", "]", "-", "1", ",", "maxlength", "=", "tf", ".", "shape", "(", "input", "=", "edges", ")", "[", "0", "]", "-", "1", ",", "axis", "=", "0", ",", "dtype", "=", "dtype", "or", "in_dtype", ")", "n_edges", "=", "tf", ".", "compat", ".", "dimension_value", "(", "edges", ".", "shape", "[", "0", "]", ")", "if", "n_edges", "is", "not", "None", ":", "counts", ".", "set_shape", "(", "tf", ".", "TensorShape", "(", "[", "n_edges", "-", "1", "]", ")", ".", "concatenate", "(", "counts", ".", "shape", "[", "1", ":", "]", ")", ")", "return", "counts"], "docstring": "Count how often `x` falls in intervals defined by `edges`.\n\n  Given `edges = [c0, ..., cK]`, defining intervals\n  `I0 = [c0, c1)`, `I1 = [c1, c2)`, ..., `I_{K-1} = [c_{K-1}, cK]`,\n  This function counts how often `x` falls into each interval.\n\n  Values of `x` outside of the intervals cause errors.  Consider using\n  `extend_lower_interval`, `extend_upper_interval` to deal with this.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.  If `axis` is not\n      `None`, must have statically known number of dimensions. The\n      `axis` kwarg determines which dimensions index iid samples.\n      Other dimensions of `x` index \"events\" for which we will compute different\n      histograms.\n    edges:  `Tensor` of same `dtype` as `x`.  The first dimension indexes edges\n      of intervals.  Must either be `1-D` or have `edges.shape[1:]` the same\n      as the dimensions of `x` excluding `axis`.\n      If `rank(edges) > 1`, `edges[k]` designates a shape `edges.shape[1:]`\n      `Tensor` of interval edges for the corresponding dimensions of `x`.\n    axis:  Optional `0-D` or `1-D` integer `Tensor` with constant\n      values. The axis in `x` that index iid samples.\n      `Default value:` `None` (treat every dimension as sample dimension).\n    extend_lower_interval:  Python `bool`.  If `True`, extend the lowest\n      interval `I0` to `(-inf, c1]`.\n    extend_upper_interval:  Python `bool`.  If `True`, extend the upper\n      interval `I_{K-1}` to `[c_{K-1}, +inf)`.\n    dtype: The output type (`int32` or `int64`). `Default value:` `x.dtype`.\n    name:  A Python string name to prepend to created ops.\n      `Default value:` 'histogram'\n\n  Returns:\n    counts: `Tensor` of type `dtype` and, with\n      `~axis = [i for i in range(arr.ndim) if i not in axis]`,\n      `counts.shape = [edges.shape[0]] + x.shape[~axis]`.\n      With `I` a multi-index into `~axis`, `counts[k][I]` is the number of times\n      event(s) fell into the `kth` interval of `edges`.\n\n  #### Examples\n\n  ```python\n  # x.shape = [1000, 2]\n  # x[:, 0] ~ Uniform(0, 1), x[:, 1] ~ Uniform(1, 2).\n  x = tf.stack([tf.random_uniform([1000]), 1 + tf.random_uniform([1000])],\n               axis=-1)\n\n  # edges ==> bins [0, 0.5), [0.5, 1.0), [1.0, 1.5), [1.5, 2.0].\n  edges = [0., 0.5, 1.0, 1.5, 2.0]\n\n  tfp.stats.histogram(x, edges)\n  ==> approximately [500, 500, 500, 500]\n\n  tfp.stats.histogram(x, edges, axis=0)\n  ==> approximately [[500, 500, 0, 0], [0, 0, 500, 500]]\n  ```", "docstring_tokens": ["Count", "how", "often", "x", "falls", "in", "intervals", "defined", "by", "edges", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/quantiles.py#L292-L400", "partition": "test", "index": 1034, "time": "2019-03-12 17:34:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/quantiles.py", "func_name": "count_integers", "original_string": "def count_integers(arr,\n                   weights=None,\n                   minlength=None,\n                   maxlength=None,\n                   axis=None,\n                   dtype=tf.int32,\n                   name=None):\n  \"\"\"Counts the number of occurrences of each value in an integer array `arr`.\n\n  Works like `tf.math.bincount`, but provides an `axis` kwarg that specifies\n  dimensions to reduce over.  With\n    `~axis = [i for i in range(arr.ndim) if i not in axis]`,\n  this function returns a `Tensor` of shape `[K] + arr.shape[~axis]`.\n\n  If `minlength` and `maxlength` are not given, `K = tf.reduce_max(arr) + 1`\n  if `arr` is non-empty, and 0 otherwise.\n  If `weights` are non-None, then index `i` of the output stores the sum of the\n  value in `weights` at each index where the corresponding value in `arr` is\n  `i`.\n\n  Args:\n    arr: An `int32` `Tensor` of non-negative values.\n    weights: If non-None, must be the same shape as arr. For each value in\n      `arr`, the bin will be incremented by the corresponding weight instead of\n      1.\n    minlength: If given, ensures the output has length at least `minlength`,\n      padding with zeros at the end if necessary.\n    maxlength: If given, skips values in `arr` that are equal or greater than\n      `maxlength`, ensuring that the output has length at most `maxlength`.\n    axis: A `0-D` or `1-D` `int32` `Tensor` (with static values) designating\n      dimensions in `arr` to reduce over.\n      `Default value:` `None`, meaning reduce over all dimensions.\n    dtype: If `weights` is None, determines the type of the output bins.\n    name: A name scope for the associated operations (optional).\n\n  Returns:\n    A vector with the same dtype as `weights` or the given `dtype`. The bin\n    values.\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'count_integers', values=[arr, weights, minlength, maxlength,\n                                      axis]):\n    if axis is None:\n      return tf.math.bincount(\n          arr,\n          weights=weights,\n          minlength=minlength,\n          maxlength=maxlength,\n          dtype=dtype)\n\n    arr = tf.convert_to_tensor(value=arr, dtype=tf.int32, name='arr')\n    arr_ndims = _get_static_ndims(arr, expect_static=True)\n\n    axis = _make_static_axis_non_negative_list(axis, arr_ndims)\n\n    # ~axis from docstring.  Dims in arr that are not in axis.\n    not_axis = sorted(set(range(arr_ndims)).difference(axis))\n\n    # If we're reducing over everything, just use standard bincount.\n    if not not_axis:\n      return tf.math.bincount(\n          arr,\n          weights=weights,\n          minlength=minlength,\n          maxlength=maxlength,\n          dtype=dtype)\n\n    # Move dims in ~axis to the left, so we can tf.map_fn bincount over them,\n    # Producing counts for every index I in ~axis.\n    # Thus, flat_arr is not totally flat, it just has the dims in ~axis\n    # flattened.\n    flat_arr = _move_dims_to_flat_end(arr, not_axis, arr_ndims, right_end=False)\n\n    # tf.map_fn over dim 0.\n    if weights is None:\n\n      def one_bincount(arr_slice):\n        return tf.math.bincount(\n            arr_slice,\n            weights=None,\n            minlength=minlength,\n            maxlength=maxlength,\n            dtype=dtype)\n\n      flat_counts = tf.map_fn(one_bincount, elems=flat_arr, dtype=dtype)\n    else:\n      weights = tf.convert_to_tensor(value=weights, name='weights')\n      _get_static_ndims(weights, expect_static=True, expect_ndims=arr_ndims)\n      flat_weights = _move_dims_to_flat_end(\n          weights, not_axis, arr_ndims, right_end=False)\n\n      def one_bincount(arr_and_weights_slices):\n        arr_slice, weights_slice = arr_and_weights_slices\n        return tf.math.bincount(\n            arr_slice,\n            weights=weights_slice,\n            minlength=minlength,\n            maxlength=maxlength,\n            dtype=dtype)\n\n      flat_counts = tf.map_fn(\n          one_bincount, elems=[flat_arr, flat_weights], dtype=weights.dtype)\n\n    # flat_counts.shape = [prod(~axis), K], because map_fn stacked on axis 0.\n    # bincount needs to have the K bins in axis 0, so transpose...\n    flat_counts_t = tf.transpose(a=flat_counts, perm=[1, 0])\n\n    # Throw in this assert, to ensure shape assumptions are correct.\n    _get_static_ndims(flat_counts_t, expect_ndims=2, expect_static=True)\n\n    # not_axis_shape = arr.shape[~axis]\n    not_axis_shape = tf.gather(tf.shape(input=arr), indices=not_axis)\n\n    # The first index of flat_counts_t indexes bins 0,..,K-1, the rest are ~axis\n    out_shape = tf.concat([[-1], not_axis_shape], axis=0)\n\n    return tf.reshape(flat_counts_t, out_shape)", "language": "python", "code": "def count_integers(arr,\n                   weights=None,\n                   minlength=None,\n                   maxlength=None,\n                   axis=None,\n                   dtype=tf.int32,\n                   name=None):\n  \"\"\"Counts the number of occurrences of each value in an integer array `arr`.\n\n  Works like `tf.math.bincount`, but provides an `axis` kwarg that specifies\n  dimensions to reduce over.  With\n    `~axis = [i for i in range(arr.ndim) if i not in axis]`,\n  this function returns a `Tensor` of shape `[K] + arr.shape[~axis]`.\n\n  If `minlength` and `maxlength` are not given, `K = tf.reduce_max(arr) + 1`\n  if `arr` is non-empty, and 0 otherwise.\n  If `weights` are non-None, then index `i` of the output stores the sum of the\n  value in `weights` at each index where the corresponding value in `arr` is\n  `i`.\n\n  Args:\n    arr: An `int32` `Tensor` of non-negative values.\n    weights: If non-None, must be the same shape as arr. For each value in\n      `arr`, the bin will be incremented by the corresponding weight instead of\n      1.\n    minlength: If given, ensures the output has length at least `minlength`,\n      padding with zeros at the end if necessary.\n    maxlength: If given, skips values in `arr` that are equal or greater than\n      `maxlength`, ensuring that the output has length at most `maxlength`.\n    axis: A `0-D` or `1-D` `int32` `Tensor` (with static values) designating\n      dimensions in `arr` to reduce over.\n      `Default value:` `None`, meaning reduce over all dimensions.\n    dtype: If `weights` is None, determines the type of the output bins.\n    name: A name scope for the associated operations (optional).\n\n  Returns:\n    A vector with the same dtype as `weights` or the given `dtype`. The bin\n    values.\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'count_integers', values=[arr, weights, minlength, maxlength,\n                                      axis]):\n    if axis is None:\n      return tf.math.bincount(\n          arr,\n          weights=weights,\n          minlength=minlength,\n          maxlength=maxlength,\n          dtype=dtype)\n\n    arr = tf.convert_to_tensor(value=arr, dtype=tf.int32, name='arr')\n    arr_ndims = _get_static_ndims(arr, expect_static=True)\n\n    axis = _make_static_axis_non_negative_list(axis, arr_ndims)\n\n    # ~axis from docstring.  Dims in arr that are not in axis.\n    not_axis = sorted(set(range(arr_ndims)).difference(axis))\n\n    # If we're reducing over everything, just use standard bincount.\n    if not not_axis:\n      return tf.math.bincount(\n          arr,\n          weights=weights,\n          minlength=minlength,\n          maxlength=maxlength,\n          dtype=dtype)\n\n    # Move dims in ~axis to the left, so we can tf.map_fn bincount over them,\n    # Producing counts for every index I in ~axis.\n    # Thus, flat_arr is not totally flat, it just has the dims in ~axis\n    # flattened.\n    flat_arr = _move_dims_to_flat_end(arr, not_axis, arr_ndims, right_end=False)\n\n    # tf.map_fn over dim 0.\n    if weights is None:\n\n      def one_bincount(arr_slice):\n        return tf.math.bincount(\n            arr_slice,\n            weights=None,\n            minlength=minlength,\n            maxlength=maxlength,\n            dtype=dtype)\n\n      flat_counts = tf.map_fn(one_bincount, elems=flat_arr, dtype=dtype)\n    else:\n      weights = tf.convert_to_tensor(value=weights, name='weights')\n      _get_static_ndims(weights, expect_static=True, expect_ndims=arr_ndims)\n      flat_weights = _move_dims_to_flat_end(\n          weights, not_axis, arr_ndims, right_end=False)\n\n      def one_bincount(arr_and_weights_slices):\n        arr_slice, weights_slice = arr_and_weights_slices\n        return tf.math.bincount(\n            arr_slice,\n            weights=weights_slice,\n            minlength=minlength,\n            maxlength=maxlength,\n            dtype=dtype)\n\n      flat_counts = tf.map_fn(\n          one_bincount, elems=[flat_arr, flat_weights], dtype=weights.dtype)\n\n    # flat_counts.shape = [prod(~axis), K], because map_fn stacked on axis 0.\n    # bincount needs to have the K bins in axis 0, so transpose...\n    flat_counts_t = tf.transpose(a=flat_counts, perm=[1, 0])\n\n    # Throw in this assert, to ensure shape assumptions are correct.\n    _get_static_ndims(flat_counts_t, expect_ndims=2, expect_static=True)\n\n    # not_axis_shape = arr.shape[~axis]\n    not_axis_shape = tf.gather(tf.shape(input=arr), indices=not_axis)\n\n    # The first index of flat_counts_t indexes bins 0,..,K-1, the rest are ~axis\n    out_shape = tf.concat([[-1], not_axis_shape], axis=0)\n\n    return tf.reshape(flat_counts_t, out_shape)", "code_tokens": ["def", "count_integers", "(", "arr", ",", "weights", "=", "None", ",", "minlength", "=", "None", ",", "maxlength", "=", "None", ",", "axis", "=", "None", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'count_integers'", ",", "values", "=", "[", "arr", ",", "weights", ",", "minlength", ",", "maxlength", ",", "axis", "]", ")", ":", "if", "axis", "is", "None", ":", "return", "tf", ".", "math", ".", "bincount", "(", "arr", ",", "weights", "=", "weights", ",", "minlength", "=", "minlength", ",", "maxlength", "=", "maxlength", ",", "dtype", "=", "dtype", ")", "arr", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "arr", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "'arr'", ")", "arr_ndims", "=", "_get_static_ndims", "(", "arr", ",", "expect_static", "=", "True", ")", "axis", "=", "_make_static_axis_non_negative_list", "(", "axis", ",", "arr_ndims", ")", "# ~axis from docstring.  Dims in arr that are not in axis.", "not_axis", "=", "sorted", "(", "set", "(", "range", "(", "arr_ndims", ")", ")", ".", "difference", "(", "axis", ")", ")", "# If we're reducing over everything, just use standard bincount.", "if", "not", "not_axis", ":", "return", "tf", ".", "math", ".", "bincount", "(", "arr", ",", "weights", "=", "weights", ",", "minlength", "=", "minlength", ",", "maxlength", "=", "maxlength", ",", "dtype", "=", "dtype", ")", "# Move dims in ~axis to the left, so we can tf.map_fn bincount over them,", "# Producing counts for every index I in ~axis.", "# Thus, flat_arr is not totally flat, it just has the dims in ~axis", "# flattened.", "flat_arr", "=", "_move_dims_to_flat_end", "(", "arr", ",", "not_axis", ",", "arr_ndims", ",", "right_end", "=", "False", ")", "# tf.map_fn over dim 0.", "if", "weights", "is", "None", ":", "def", "one_bincount", "(", "arr_slice", ")", ":", "return", "tf", ".", "math", ".", "bincount", "(", "arr_slice", ",", "weights", "=", "None", ",", "minlength", "=", "minlength", ",", "maxlength", "=", "maxlength", ",", "dtype", "=", "dtype", ")", "flat_counts", "=", "tf", ".", "map_fn", "(", "one_bincount", ",", "elems", "=", "flat_arr", ",", "dtype", "=", "dtype", ")", "else", ":", "weights", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "weights", ",", "name", "=", "'weights'", ")", "_get_static_ndims", "(", "weights", ",", "expect_static", "=", "True", ",", "expect_ndims", "=", "arr_ndims", ")", "flat_weights", "=", "_move_dims_to_flat_end", "(", "weights", ",", "not_axis", ",", "arr_ndims", ",", "right_end", "=", "False", ")", "def", "one_bincount", "(", "arr_and_weights_slices", ")", ":", "arr_slice", ",", "weights_slice", "=", "arr_and_weights_slices", "return", "tf", ".", "math", ".", "bincount", "(", "arr_slice", ",", "weights", "=", "weights_slice", ",", "minlength", "=", "minlength", ",", "maxlength", "=", "maxlength", ",", "dtype", "=", "dtype", ")", "flat_counts", "=", "tf", ".", "map_fn", "(", "one_bincount", ",", "elems", "=", "[", "flat_arr", ",", "flat_weights", "]", ",", "dtype", "=", "weights", ".", "dtype", ")", "# flat_counts.shape = [prod(~axis), K], because map_fn stacked on axis 0.", "# bincount needs to have the K bins in axis 0, so transpose...", "flat_counts_t", "=", "tf", ".", "transpose", "(", "a", "=", "flat_counts", ",", "perm", "=", "[", "1", ",", "0", "]", ")", "# Throw in this assert, to ensure shape assumptions are correct.", "_get_static_ndims", "(", "flat_counts_t", ",", "expect_ndims", "=", "2", ",", "expect_static", "=", "True", ")", "# not_axis_shape = arr.shape[~axis]", "not_axis_shape", "=", "tf", ".", "gather", "(", "tf", ".", "shape", "(", "input", "=", "arr", ")", ",", "indices", "=", "not_axis", ")", "# The first index of flat_counts_t indexes bins 0,..,K-1, the rest are ~axis", "out_shape", "=", "tf", ".", "concat", "(", "[", "[", "-", "1", "]", ",", "not_axis_shape", "]", ",", "axis", "=", "0", ")", "return", "tf", ".", "reshape", "(", "flat_counts_t", ",", "out_shape", ")"], "docstring": "Counts the number of occurrences of each value in an integer array `arr`.\n\n  Works like `tf.math.bincount`, but provides an `axis` kwarg that specifies\n  dimensions to reduce over.  With\n    `~axis = [i for i in range(arr.ndim) if i not in axis]`,\n  this function returns a `Tensor` of shape `[K] + arr.shape[~axis]`.\n\n  If `minlength` and `maxlength` are not given, `K = tf.reduce_max(arr) + 1`\n  if `arr` is non-empty, and 0 otherwise.\n  If `weights` are non-None, then index `i` of the output stores the sum of the\n  value in `weights` at each index where the corresponding value in `arr` is\n  `i`.\n\n  Args:\n    arr: An `int32` `Tensor` of non-negative values.\n    weights: If non-None, must be the same shape as arr. For each value in\n      `arr`, the bin will be incremented by the corresponding weight instead of\n      1.\n    minlength: If given, ensures the output has length at least `minlength`,\n      padding with zeros at the end if necessary.\n    maxlength: If given, skips values in `arr` that are equal or greater than\n      `maxlength`, ensuring that the output has length at most `maxlength`.\n    axis: A `0-D` or `1-D` `int32` `Tensor` (with static values) designating\n      dimensions in `arr` to reduce over.\n      `Default value:` `None`, meaning reduce over all dimensions.\n    dtype: If `weights` is None, determines the type of the output bins.\n    name: A name scope for the associated operations (optional).\n\n  Returns:\n    A vector with the same dtype as `weights` or the given `dtype`. The bin\n    values.", "docstring_tokens": ["Counts", "the", "number", "of", "occurrences", "of", "each", "value", "in", "an", "integer", "array", "arr", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/quantiles.py#L39-L155", "partition": "test", "index": 1032, "time": "2019-03-12 17:34:55"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/deterministic.py", "func_name": "_get_tol", "original_string": "def _get_tol(tol, dtype, validate_args):\n  \"\"\"Gets a Tensor of type `dtype`, 0 if `tol` is None, validation optional.\"\"\"\n  if tol is None:\n    return tf.convert_to_tensor(value=0, dtype=dtype)\n\n  tol = tf.convert_to_tensor(value=tol, dtype=dtype)\n  if validate_args:\n    tol = distribution_util.with_dependencies([\n        assert_util.assert_non_negative(\n            tol, message=\"Argument 'tol' must be non-negative\")\n    ], tol)\n  return tol", "language": "python", "code": "def _get_tol(tol, dtype, validate_args):\n  \"\"\"Gets a Tensor of type `dtype`, 0 if `tol` is None, validation optional.\"\"\"\n  if tol is None:\n    return tf.convert_to_tensor(value=0, dtype=dtype)\n\n  tol = tf.convert_to_tensor(value=tol, dtype=dtype)\n  if validate_args:\n    tol = distribution_util.with_dependencies([\n        assert_util.assert_non_negative(\n            tol, message=\"Argument 'tol' must be non-negative\")\n    ], tol)\n  return tol", "code_tokens": ["def", "_get_tol", "(", "tol", ",", "dtype", ",", "validate_args", ")", ":", "if", "tol", "is", "None", ":", "return", "tf", ".", "convert_to_tensor", "(", "value", "=", "0", ",", "dtype", "=", "dtype", ")", "tol", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "tol", ",", "dtype", "=", "dtype", ")", "if", "validate_args", ":", "tol", "=", "distribution_util", ".", "with_dependencies", "(", "[", "assert_util", ".", "assert_non_negative", "(", "tol", ",", "message", "=", "\"Argument 'tol' must be non-negative\"", ")", "]", ",", "tol", ")", "return", "tol"], "docstring": "Gets a Tensor of type `dtype`, 0 if `tol` is None, validation optional.", "docstring_tokens": ["Gets", "a", "Tensor", "of", "type", "dtype", "0", "if", "tol", "is", "None", "validation", "optional", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/deterministic.py#L41-L52", "partition": "test", "index": 1095, "time": "2019-03-13 06:39:41"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "LinearGaussianStateSpaceModel.latents_to_observations", "original_string": "def latents_to_observations(self, latent_means, latent_covs):\n    \"\"\"Push latent means and covariances forward through the observation model.\n\n    Args:\n      latent_means: float `Tensor` of shape `[..., num_timesteps, latent_size]`\n      latent_covs: float `Tensor` of shape\n        `[..., num_timesteps, latent_size, latent_size]`.\n\n    Returns:\n      observation_means: float `Tensor` of shape\n        `[..., num_timesteps, observation_size]`\n      observation_covs: float `Tensor` of shape\n        `[..., num_timesteps, observation_size, observation_size]`\n    \"\"\"\n\n    with tf.name_scope(\"latents_to_observations\"):\n\n      pushforward_latents_step = build_pushforward_latents_step(\n          self.get_observation_matrix_for_timestep,\n          self.get_observation_noise_for_timestep)\n\n      latent_means = distribution_util.move_dimension(\n          latent_means, source_idx=-2, dest_idx=0)\n      latent_means = latent_means[..., tf.newaxis]  # Make matmul happy.\n      latent_covs = distribution_util.move_dimension(\n          latent_covs, source_idx=-3, dest_idx=0)\n\n      (initial_observation_mean,\n       initial_observation_cov) = pushforward_latents_step(\n           _=None,  # Loop body ignores previous observations.\n           latent_t_mean_cov=(self.initial_step,\n                              latent_means[self.initial_step],\n                              latent_covs[self.initial_step]))\n\n      # TODO(davmre) this loop is embarassingly parallel; replace with `pfor`.\n      timesteps = tf.range(self.initial_step,\n                           self.initial_step + self.num_timesteps)\n      observation_means, observation_covs = tf.scan(\n          pushforward_latents_step,\n          elems=(timesteps, latent_means, latent_covs),\n          initializer=(initial_observation_mean, initial_observation_cov),\n          parallel_iterations=10000)\n\n      observation_means = distribution_util.move_dimension(\n          observation_means[..., 0], source_idx=0, dest_idx=-2)\n      observation_covs = distribution_util.move_dimension(\n          observation_covs, source_idx=0, dest_idx=-3)\n\n      return observation_means, observation_covs", "language": "python", "code": "def latents_to_observations(self, latent_means, latent_covs):\n    \"\"\"Push latent means and covariances forward through the observation model.\n\n    Args:\n      latent_means: float `Tensor` of shape `[..., num_timesteps, latent_size]`\n      latent_covs: float `Tensor` of shape\n        `[..., num_timesteps, latent_size, latent_size]`.\n\n    Returns:\n      observation_means: float `Tensor` of shape\n        `[..., num_timesteps, observation_size]`\n      observation_covs: float `Tensor` of shape\n        `[..., num_timesteps, observation_size, observation_size]`\n    \"\"\"\n\n    with tf.name_scope(\"latents_to_observations\"):\n\n      pushforward_latents_step = build_pushforward_latents_step(\n          self.get_observation_matrix_for_timestep,\n          self.get_observation_noise_for_timestep)\n\n      latent_means = distribution_util.move_dimension(\n          latent_means, source_idx=-2, dest_idx=0)\n      latent_means = latent_means[..., tf.newaxis]  # Make matmul happy.\n      latent_covs = distribution_util.move_dimension(\n          latent_covs, source_idx=-3, dest_idx=0)\n\n      (initial_observation_mean,\n       initial_observation_cov) = pushforward_latents_step(\n           _=None,  # Loop body ignores previous observations.\n           latent_t_mean_cov=(self.initial_step,\n                              latent_means[self.initial_step],\n                              latent_covs[self.initial_step]))\n\n      # TODO(davmre) this loop is embarassingly parallel; replace with `pfor`.\n      timesteps = tf.range(self.initial_step,\n                           self.initial_step + self.num_timesteps)\n      observation_means, observation_covs = tf.scan(\n          pushforward_latents_step,\n          elems=(timesteps, latent_means, latent_covs),\n          initializer=(initial_observation_mean, initial_observation_cov),\n          parallel_iterations=10000)\n\n      observation_means = distribution_util.move_dimension(\n          observation_means[..., 0], source_idx=0, dest_idx=-2)\n      observation_covs = distribution_util.move_dimension(\n          observation_covs, source_idx=0, dest_idx=-3)\n\n      return observation_means, observation_covs", "code_tokens": ["def", "latents_to_observations", "(", "self", ",", "latent_means", ",", "latent_covs", ")", ":", "with", "tf", ".", "name_scope", "(", "\"latents_to_observations\"", ")", ":", "pushforward_latents_step", "=", "build_pushforward_latents_step", "(", "self", ".", "get_observation_matrix_for_timestep", ",", "self", ".", "get_observation_noise_for_timestep", ")", "latent_means", "=", "distribution_util", ".", "move_dimension", "(", "latent_means", ",", "source_idx", "=", "-", "2", ",", "dest_idx", "=", "0", ")", "latent_means", "=", "latent_means", "[", "...", ",", "tf", ".", "newaxis", "]", "# Make matmul happy.", "latent_covs", "=", "distribution_util", ".", "move_dimension", "(", "latent_covs", ",", "source_idx", "=", "-", "3", ",", "dest_idx", "=", "0", ")", "(", "initial_observation_mean", ",", "initial_observation_cov", ")", "=", "pushforward_latents_step", "(", "_", "=", "None", ",", "# Loop body ignores previous observations.", "latent_t_mean_cov", "=", "(", "self", ".", "initial_step", ",", "latent_means", "[", "self", ".", "initial_step", "]", ",", "latent_covs", "[", "self", ".", "initial_step", "]", ")", ")", "# TODO(davmre) this loop is embarassingly parallel; replace with `pfor`.", "timesteps", "=", "tf", ".", "range", "(", "self", ".", "initial_step", ",", "self", ".", "initial_step", "+", "self", ".", "num_timesteps", ")", "observation_means", ",", "observation_covs", "=", "tf", ".", "scan", "(", "pushforward_latents_step", ",", "elems", "=", "(", "timesteps", ",", "latent_means", ",", "latent_covs", ")", ",", "initializer", "=", "(", "initial_observation_mean", ",", "initial_observation_cov", ")", ",", "parallel_iterations", "=", "10000", ")", "observation_means", "=", "distribution_util", ".", "move_dimension", "(", "observation_means", "[", "...", ",", "0", "]", ",", "source_idx", "=", "0", ",", "dest_idx", "=", "-", "2", ")", "observation_covs", "=", "distribution_util", ".", "move_dimension", "(", "observation_covs", ",", "source_idx", "=", "0", ",", "dest_idx", "=", "-", "3", ")", "return", "observation_means", ",", "observation_covs"], "docstring": "Push latent means and covariances forward through the observation model.\n\n    Args:\n      latent_means: float `Tensor` of shape `[..., num_timesteps, latent_size]`\n      latent_covs: float `Tensor` of shape\n        `[..., num_timesteps, latent_size, latent_size]`.\n\n    Returns:\n      observation_means: float `Tensor` of shape\n        `[..., num_timesteps, observation_size]`\n      observation_covs: float `Tensor` of shape\n        `[..., num_timesteps, observation_size, observation_size]`", "docstring_tokens": ["Push", "latent", "means", "and", "covariances", "forward", "through", "the", "observation", "model", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L1097-L1145", "partition": "test", "index": 955, "time": "2019-03-13 16:14:43"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/internal/util.py", "func_name": "mix_over_posterior_draws", "original_string": "def mix_over_posterior_draws(means, variances):\n  \"\"\"Construct a predictive normal distribution that mixes over posterior draws.\n\n  Args:\n    means: float `Tensor` of shape\n      `[num_posterior_draws, ..., num_timesteps]`.\n    variances: float `Tensor` of shape\n      `[num_posterior_draws, ..., num_timesteps]`.\n\n  Returns:\n    mixture_dist: `tfd.MixtureSameFamily(tfd.Independent(tfd.Normal))` instance\n      representing a uniform mixture over the posterior samples, with\n      `batch_shape = ...` and `event_shape = [num_timesteps]`.\n\n  \"\"\"\n  # The inputs `means`, `variances` have shape\n  #   `concat([\n  #      [num_posterior_draws],\n  #      sample_shape,\n  #      batch_shape,\n  #      [num_timesteps]])`\n  # Because MixtureSameFamily mixes over the rightmost batch dimension,\n  # we need to move the `num_posterior_draws` dimension to be rightmost\n  # in the batch shape. This requires use of `Independent` (to preserve\n  # `num_timesteps` as part of the event shape) and `move_dimension`.\n  # TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an\n  # arbitrary axis, and eliminate `move_dimension` calls here.\n\n  with tf.compat.v1.name_scope(\n      'mix_over_posterior_draws', values=[means, variances]):\n    num_posterior_draws = dist_util.prefer_static_value(\n        tf.shape(input=means))[0]\n\n    component_observations = tfd.Independent(\n        distribution=tfd.Normal(\n            loc=dist_util.move_dimension(means, 0, -2),\n            scale=tf.sqrt(dist_util.move_dimension(variances, 0, -2))),\n        reinterpreted_batch_ndims=1)\n\n    return tfd.MixtureSameFamily(\n        mixture_distribution=tfd.Categorical(\n            logits=tf.zeros([num_posterior_draws],\n                            dtype=component_observations.dtype)),\n        components_distribution=component_observations)", "language": "python", "code": "def mix_over_posterior_draws(means, variances):\n  \"\"\"Construct a predictive normal distribution that mixes over posterior draws.\n\n  Args:\n    means: float `Tensor` of shape\n      `[num_posterior_draws, ..., num_timesteps]`.\n    variances: float `Tensor` of shape\n      `[num_posterior_draws, ..., num_timesteps]`.\n\n  Returns:\n    mixture_dist: `tfd.MixtureSameFamily(tfd.Independent(tfd.Normal))` instance\n      representing a uniform mixture over the posterior samples, with\n      `batch_shape = ...` and `event_shape = [num_timesteps]`.\n\n  \"\"\"\n  # The inputs `means`, `variances` have shape\n  #   `concat([\n  #      [num_posterior_draws],\n  #      sample_shape,\n  #      batch_shape,\n  #      [num_timesteps]])`\n  # Because MixtureSameFamily mixes over the rightmost batch dimension,\n  # we need to move the `num_posterior_draws` dimension to be rightmost\n  # in the batch shape. This requires use of `Independent` (to preserve\n  # `num_timesteps` as part of the event shape) and `move_dimension`.\n  # TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an\n  # arbitrary axis, and eliminate `move_dimension` calls here.\n\n  with tf.compat.v1.name_scope(\n      'mix_over_posterior_draws', values=[means, variances]):\n    num_posterior_draws = dist_util.prefer_static_value(\n        tf.shape(input=means))[0]\n\n    component_observations = tfd.Independent(\n        distribution=tfd.Normal(\n            loc=dist_util.move_dimension(means, 0, -2),\n            scale=tf.sqrt(dist_util.move_dimension(variances, 0, -2))),\n        reinterpreted_batch_ndims=1)\n\n    return tfd.MixtureSameFamily(\n        mixture_distribution=tfd.Categorical(\n            logits=tf.zeros([num_posterior_draws],\n                            dtype=component_observations.dtype)),\n        components_distribution=component_observations)", "code_tokens": ["def", "mix_over_posterior_draws", "(", "means", ",", "variances", ")", ":", "# The inputs `means`, `variances` have shape", "#   `concat([", "#      [num_posterior_draws],", "#      sample_shape,", "#      batch_shape,", "#      [num_timesteps]])`", "# Because MixtureSameFamily mixes over the rightmost batch dimension,", "# we need to move the `num_posterior_draws` dimension to be rightmost", "# in the batch shape. This requires use of `Independent` (to preserve", "# `num_timesteps` as part of the event shape) and `move_dimension`.", "# TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an", "# arbitrary axis, and eliminate `move_dimension` calls here.", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'mix_over_posterior_draws'", ",", "values", "=", "[", "means", ",", "variances", "]", ")", ":", "num_posterior_draws", "=", "dist_util", ".", "prefer_static_value", "(", "tf", ".", "shape", "(", "input", "=", "means", ")", ")", "[", "0", "]", "component_observations", "=", "tfd", ".", "Independent", "(", "distribution", "=", "tfd", ".", "Normal", "(", "loc", "=", "dist_util", ".", "move_dimension", "(", "means", ",", "0", ",", "-", "2", ")", ",", "scale", "=", "tf", ".", "sqrt", "(", "dist_util", ".", "move_dimension", "(", "variances", ",", "0", ",", "-", "2", ")", ")", ")", ",", "reinterpreted_batch_ndims", "=", "1", ")", "return", "tfd", ".", "MixtureSameFamily", "(", "mixture_distribution", "=", "tfd", ".", "Categorical", "(", "logits", "=", "tf", ".", "zeros", "(", "[", "num_posterior_draws", "]", ",", "dtype", "=", "component_observations", ".", "dtype", ")", ")", ",", "components_distribution", "=", "component_observations", ")"], "docstring": "Construct a predictive normal distribution that mixes over posterior draws.\n\n  Args:\n    means: float `Tensor` of shape\n      `[num_posterior_draws, ..., num_timesteps]`.\n    variances: float `Tensor` of shape\n      `[num_posterior_draws, ..., num_timesteps]`.\n\n  Returns:\n    mixture_dist: `tfd.MixtureSameFamily(tfd.Independent(tfd.Normal))` instance\n      representing a uniform mixture over the posterior samples, with\n      `batch_shape = ...` and `event_shape = [num_timesteps]`.", "docstring_tokens": ["Construct", "a", "predictive", "normal", "distribution", "that", "mixes", "over", "posterior", "draws", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/internal/util.py#L332-L375", "partition": "test", "index": 709, "time": "2019-03-13 16:31:10"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/generative_adversarial_network.py", "func_name": "plot_generated_images", "original_string": "def plot_generated_images(images, fname):\n  \"\"\"Save a synthetic image as a PNG file.\n\n  Args:\n    images: samples of synthetic images generated by the generative network.\n    fname: Python `str`, filename to save the plot to.\n  \"\"\"\n  fig = plt.figure(figsize=(4, 4))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n\n  for i, image in enumerate(images):\n    ax = fig.add_subplot(4, 4, i + 1)\n    plt.axis('off')\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.imshow(image.reshape(IMAGE_SHAPE[:-1]), cmap='Greys_r')\n\n  fig.tight_layout()\n  plt.subplots_adjust(wspace=0.05, hspace=0.05)\n  canvas.print_figure(fname, format='png')", "language": "python", "code": "def plot_generated_images(images, fname):\n  \"\"\"Save a synthetic image as a PNG file.\n\n  Args:\n    images: samples of synthetic images generated by the generative network.\n    fname: Python `str`, filename to save the plot to.\n  \"\"\"\n  fig = plt.figure(figsize=(4, 4))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n\n  for i, image in enumerate(images):\n    ax = fig.add_subplot(4, 4, i + 1)\n    plt.axis('off')\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.imshow(image.reshape(IMAGE_SHAPE[:-1]), cmap='Greys_r')\n\n  fig.tight_layout()\n  plt.subplots_adjust(wspace=0.05, hspace=0.05)\n  canvas.print_figure(fname, format='png')", "code_tokens": ["def", "plot_generated_images", "(", "images", ",", "fname", ")", ":", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "4", ",", "4", ")", ")", "canvas", "=", "backend_agg", ".", "FigureCanvasAgg", "(", "fig", ")", "for", "i", ",", "image", "in", "enumerate", "(", "images", ")", ":", "ax", "=", "fig", ".", "add_subplot", "(", "4", ",", "4", ",", "i", "+", "1", ")", "plt", ".", "axis", "(", "'off'", ")", "ax", ".", "set_xticklabels", "(", "[", "]", ")", "ax", ".", "set_yticklabels", "(", "[", "]", ")", "ax", ".", "imshow", "(", "image", ".", "reshape", "(", "IMAGE_SHAPE", "[", ":", "-", "1", "]", ")", ",", "cmap", "=", "'Greys_r'", ")", "fig", ".", "tight_layout", "(", ")", "plt", ".", "subplots_adjust", "(", "wspace", "=", "0.05", ",", "hspace", "=", "0.05", ")", "canvas", ".", "print_figure", "(", "fname", ",", "format", "=", "'png'", ")"], "docstring": "Save a synthetic image as a PNG file.\n\n  Args:\n    images: samples of synthetic images generated by the generative network.\n    fname: Python `str`, filename to save the plot to.", "docstring_tokens": ["Save", "a", "synthetic", "image", "as", "a", "PNG", "file", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/generative_adversarial_network.py#L123-L142", "partition": "test", "index": 1099, "time": "2019-03-15 09:08:07"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/generative_adversarial_network.py", "func_name": "build_input_pipeline", "original_string": "def build_input_pipeline(train_images, batch_size):\n  \"\"\"Build an iterator over training batches.\"\"\"\n\n  training_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n  training_batches = training_dataset.shuffle(\n      50000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n  images = training_iterator.get_next()\n  return images", "language": "python", "code": "def build_input_pipeline(train_images, batch_size):\n  \"\"\"Build an iterator over training batches.\"\"\"\n\n  training_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n  training_batches = training_dataset.shuffle(\n      50000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n  images = training_iterator.get_next()\n  return images", "code_tokens": ["def", "build_input_pipeline", "(", "train_images", ",", "batch_size", ")", ":", "training_dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "train_images", ")", "training_batches", "=", "training_dataset", ".", "shuffle", "(", "50000", ",", "reshuffle_each_iteration", "=", "True", ")", ".", "repeat", "(", ")", ".", "batch", "(", "batch_size", ")", "training_iterator", "=", "tf", ".", "compat", ".", "v1", ".", "data", ".", "make_one_shot_iterator", "(", "training_batches", ")", "images", "=", "training_iterator", ".", "get_next", "(", ")", "return", "images"], "docstring": "Build an iterator over training batches.", "docstring_tokens": ["Build", "an", "iterator", "over", "training", "batches", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/generative_adversarial_network.py#L104-L112", "partition": "test", "index": 1098, "time": "2019-03-15 09:08:07"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/decomposition.py", "func_name": "decompose_forecast_by_component", "original_string": "def decompose_forecast_by_component(model, forecast_dist, parameter_samples):\n  \"\"\"Decompose a forecast distribution into contributions from each component.\n\n  Args:\n    model: An instance of `tfp.sts.Sum` representing a structural time series\n      model.\n    forecast_dist: A `Distribution` instance returned by `tfp.sts.forecast()`.\n      (specifically, must be a `tfd.MixtureSameFamily` over a\n      `tfd.LinearGaussianStateSpaceModel` parameterized by posterior samples).\n    parameter_samples: Python `list` of `Tensors` representing posterior samples\n      of model parameters, with shapes `[concat([[num_posterior_draws],\n      param.prior.batch_shape, param.prior.event_shape]) for param in\n      model.parameters]`. This may optionally also be a map (Python `dict`) of\n      parameter names to `Tensor` values.\n  Returns:\n    component_forecasts: A `collections.OrderedDict` instance mapping\n      component StructuralTimeSeries instances (elements of `model.components`)\n      to `tfd.Distribution` instances representing the marginal forecast for\n      each component. Each distribution has batch and event shape matching\n      `forecast_dist` (specifically, the event shape is\n      `[num_steps_forecast]`).\n\n  #### Examples\n\n  Suppose we've built a model, fit it to data, and constructed a forecast\n  distribution:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    num_steps_forecast = 50\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)\n    forecast_dist = tfp.sts.forecast(model, observed_time_series,\n                                 parameter_samples=samples,\n                                 num_steps_forecast=num_steps_forecast)\n  ```\n\n  To extract the forecast for individual components, pass the forecast\n  distribution into `decompose_forecast_by_components`:\n\n  ```python\n    component_forecasts = decompose_forecast_by_component(\n      model, forecast_dist, samples)\n\n    # Component mean and stddev have shape `[num_steps_forecast]`.\n    day_of_week_effect_mean = forecast_components[day_of_week].mean()\n    day_of_week_effect_stddev = forecast_components[day_of_week].stddev()\n  ```\n\n  Using the component forecasts, we can visualize the uncertainty for each\n  component:\n\n  ```\n  from matplotlib import pylab as plt\n  num_components = len(component_forecasts)\n  xs = np.arange(num_steps_forecast)\n  fig = plt.figure(figsize=(12, 3 * num_components))\n  for i, (component, component_dist) in enumerate(component_forecasts.items()):\n\n    # If in graph mode, replace `.numpy()` with `.eval()` or `sess.run()`.\n    component_mean = component_dist.mean().numpy()\n    component_stddev = component_dist.stddev().numpy()\n\n    ax = fig.add_subplot(num_components, 1, 1 + i)\n    ax.plot(xs, component_mean, lw=2)\n    ax.fill_between(xs,\n                    component_mean - 2 * component_stddev,\n                    component_mean + 2 * component_stddev,\n                    alpha=0.5)\n    ax.set_title(component.name)\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope('decompose_forecast_by_component'):\n    try:\n      forecast_lgssm = forecast_dist.components_distribution\n      forecast_latent_mean, _ = forecast_lgssm._joint_mean()  # pylint: disable=protected-access\n      forecast_latent_covs, _ = forecast_lgssm._joint_covariances()  # pylint: disable=protected-access\n    except AttributeError as e:\n      raise ValueError(\n          'Forecast distribution must be a MixtureSameFamily of'\n          'LinearGaussianStateSpaceModel distributions, such as returned by'\n          '`tfp.sts.forecast()`. (saw exception: {})'.format(e))\n\n    # Since `parameter_samples` will have sample shape `[num_posterior_draws]`,\n    # we need to move the `num_posterior_draws` dimension of the forecast\n    # moments from the trailing batch dimension, where it's currently put by\n    # `sts.forecast`, back to the leading (sample shape) dimension.\n    forecast_latent_mean = dist_util.move_dimension(\n        forecast_latent_mean, source_idx=-3, dest_idx=0)\n    forecast_latent_covs = dist_util.move_dimension(\n        forecast_latent_covs, source_idx=-4, dest_idx=0)\n\n    return _decompose_from_posterior_marginals(\n        model, forecast_latent_mean, forecast_latent_covs, parameter_samples)", "language": "python", "code": "def decompose_forecast_by_component(model, forecast_dist, parameter_samples):\n  \"\"\"Decompose a forecast distribution into contributions from each component.\n\n  Args:\n    model: An instance of `tfp.sts.Sum` representing a structural time series\n      model.\n    forecast_dist: A `Distribution` instance returned by `tfp.sts.forecast()`.\n      (specifically, must be a `tfd.MixtureSameFamily` over a\n      `tfd.LinearGaussianStateSpaceModel` parameterized by posterior samples).\n    parameter_samples: Python `list` of `Tensors` representing posterior samples\n      of model parameters, with shapes `[concat([[num_posterior_draws],\n      param.prior.batch_shape, param.prior.event_shape]) for param in\n      model.parameters]`. This may optionally also be a map (Python `dict`) of\n      parameter names to `Tensor` values.\n  Returns:\n    component_forecasts: A `collections.OrderedDict` instance mapping\n      component StructuralTimeSeries instances (elements of `model.components`)\n      to `tfd.Distribution` instances representing the marginal forecast for\n      each component. Each distribution has batch and event shape matching\n      `forecast_dist` (specifically, the event shape is\n      `[num_steps_forecast]`).\n\n  #### Examples\n\n  Suppose we've built a model, fit it to data, and constructed a forecast\n  distribution:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    num_steps_forecast = 50\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)\n    forecast_dist = tfp.sts.forecast(model, observed_time_series,\n                                 parameter_samples=samples,\n                                 num_steps_forecast=num_steps_forecast)\n  ```\n\n  To extract the forecast for individual components, pass the forecast\n  distribution into `decompose_forecast_by_components`:\n\n  ```python\n    component_forecasts = decompose_forecast_by_component(\n      model, forecast_dist, samples)\n\n    # Component mean and stddev have shape `[num_steps_forecast]`.\n    day_of_week_effect_mean = forecast_components[day_of_week].mean()\n    day_of_week_effect_stddev = forecast_components[day_of_week].stddev()\n  ```\n\n  Using the component forecasts, we can visualize the uncertainty for each\n  component:\n\n  ```\n  from matplotlib import pylab as plt\n  num_components = len(component_forecasts)\n  xs = np.arange(num_steps_forecast)\n  fig = plt.figure(figsize=(12, 3 * num_components))\n  for i, (component, component_dist) in enumerate(component_forecasts.items()):\n\n    # If in graph mode, replace `.numpy()` with `.eval()` or `sess.run()`.\n    component_mean = component_dist.mean().numpy()\n    component_stddev = component_dist.stddev().numpy()\n\n    ax = fig.add_subplot(num_components, 1, 1 + i)\n    ax.plot(xs, component_mean, lw=2)\n    ax.fill_between(xs,\n                    component_mean - 2 * component_stddev,\n                    component_mean + 2 * component_stddev,\n                    alpha=0.5)\n    ax.set_title(component.name)\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope('decompose_forecast_by_component'):\n    try:\n      forecast_lgssm = forecast_dist.components_distribution\n      forecast_latent_mean, _ = forecast_lgssm._joint_mean()  # pylint: disable=protected-access\n      forecast_latent_covs, _ = forecast_lgssm._joint_covariances()  # pylint: disable=protected-access\n    except AttributeError as e:\n      raise ValueError(\n          'Forecast distribution must be a MixtureSameFamily of'\n          'LinearGaussianStateSpaceModel distributions, such as returned by'\n          '`tfp.sts.forecast()`. (saw exception: {})'.format(e))\n\n    # Since `parameter_samples` will have sample shape `[num_posterior_draws]`,\n    # we need to move the `num_posterior_draws` dimension of the forecast\n    # moments from the trailing batch dimension, where it's currently put by\n    # `sts.forecast`, back to the leading (sample shape) dimension.\n    forecast_latent_mean = dist_util.move_dimension(\n        forecast_latent_mean, source_idx=-3, dest_idx=0)\n    forecast_latent_covs = dist_util.move_dimension(\n        forecast_latent_covs, source_idx=-4, dest_idx=0)\n\n    return _decompose_from_posterior_marginals(\n        model, forecast_latent_mean, forecast_latent_covs, parameter_samples)", "code_tokens": ["def", "decompose_forecast_by_component", "(", "model", ",", "forecast_dist", ",", "parameter_samples", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'decompose_forecast_by_component'", ")", ":", "try", ":", "forecast_lgssm", "=", "forecast_dist", ".", "components_distribution", "forecast_latent_mean", ",", "_", "=", "forecast_lgssm", ".", "_joint_mean", "(", ")", "# pylint: disable=protected-access", "forecast_latent_covs", ",", "_", "=", "forecast_lgssm", ".", "_joint_covariances", "(", ")", "# pylint: disable=protected-access", "except", "AttributeError", "as", "e", ":", "raise", "ValueError", "(", "'Forecast distribution must be a MixtureSameFamily of'", "'LinearGaussianStateSpaceModel distributions, such as returned by'", "'`tfp.sts.forecast()`. (saw exception: {})'", ".", "format", "(", "e", ")", ")", "# Since `parameter_samples` will have sample shape `[num_posterior_draws]`,", "# we need to move the `num_posterior_draws` dimension of the forecast", "# moments from the trailing batch dimension, where it's currently put by", "# `sts.forecast`, back to the leading (sample shape) dimension.", "forecast_latent_mean", "=", "dist_util", ".", "move_dimension", "(", "forecast_latent_mean", ",", "source_idx", "=", "-", "3", ",", "dest_idx", "=", "0", ")", "forecast_latent_covs", "=", "dist_util", ".", "move_dimension", "(", "forecast_latent_covs", ",", "source_idx", "=", "-", "4", ",", "dest_idx", "=", "0", ")", "return", "_decompose_from_posterior_marginals", "(", "model", ",", "forecast_latent_mean", ",", "forecast_latent_covs", ",", "parameter_samples", ")"], "docstring": "Decompose a forecast distribution into contributions from each component.\n\n  Args:\n    model: An instance of `tfp.sts.Sum` representing a structural time series\n      model.\n    forecast_dist: A `Distribution` instance returned by `tfp.sts.forecast()`.\n      (specifically, must be a `tfd.MixtureSameFamily` over a\n      `tfd.LinearGaussianStateSpaceModel` parameterized by posterior samples).\n    parameter_samples: Python `list` of `Tensors` representing posterior samples\n      of model parameters, with shapes `[concat([[num_posterior_draws],\n      param.prior.batch_shape, param.prior.event_shape]) for param in\n      model.parameters]`. This may optionally also be a map (Python `dict`) of\n      parameter names to `Tensor` values.\n  Returns:\n    component_forecasts: A `collections.OrderedDict` instance mapping\n      component StructuralTimeSeries instances (elements of `model.components`)\n      to `tfd.Distribution` instances representing the marginal forecast for\n      each component. Each distribution has batch and event shape matching\n      `forecast_dist` (specifically, the event shape is\n      `[num_steps_forecast]`).\n\n  #### Examples\n\n  Suppose we've built a model, fit it to data, and constructed a forecast\n  distribution:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    num_steps_forecast = 50\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)\n    forecast_dist = tfp.sts.forecast(model, observed_time_series,\n                                 parameter_samples=samples,\n                                 num_steps_forecast=num_steps_forecast)\n  ```\n\n  To extract the forecast for individual components, pass the forecast\n  distribution into `decompose_forecast_by_components`:\n\n  ```python\n    component_forecasts = decompose_forecast_by_component(\n      model, forecast_dist, samples)\n\n    # Component mean and stddev have shape `[num_steps_forecast]`.\n    day_of_week_effect_mean = forecast_components[day_of_week].mean()\n    day_of_week_effect_stddev = forecast_components[day_of_week].stddev()\n  ```\n\n  Using the component forecasts, we can visualize the uncertainty for each\n  component:\n\n  ```\n  from matplotlib import pylab as plt\n  num_components = len(component_forecasts)\n  xs = np.arange(num_steps_forecast)\n  fig = plt.figure(figsize=(12, 3 * num_components))\n  for i, (component, component_dist) in enumerate(component_forecasts.items()):\n\n    # If in graph mode, replace `.numpy()` with `.eval()` or `sess.run()`.\n    component_mean = component_dist.mean().numpy()\n    component_stddev = component_dist.stddev().numpy()\n\n    ax = fig.add_subplot(num_components, 1, 1 + i)\n    ax.plot(xs, component_mean, lw=2)\n    ax.fill_between(xs,\n                    component_mean - 2 * component_stddev,\n                    component_mean + 2 * component_stddev,\n                    alpha=0.5)\n    ax.set_title(component.name)\n  ```", "docstring_tokens": ["Decompose", "a", "forecast", "distribution", "into", "contributions", "from", "each", "component", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/decomposition.py#L222-L325", "partition": "test", "index": 763, "time": "2019-03-18 13:28:34"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/decomposition.py", "func_name": "decompose_by_component", "original_string": "def decompose_by_component(model, observed_time_series, parameter_samples):\n  \"\"\"Decompose an observed time series into contributions from each component.\n\n  This method decomposes a time series according to the posterior represention\n  of a structural time series model. In particular, it:\n    - Computes the posterior marginal mean and covariances over the additive\n      model's latent space.\n    - Decomposes the latent posterior into the marginal blocks for each\n      model component.\n    - Maps the per-component latent posteriors back through each component's\n      observation model, to generate the time series modeled by that component.\n\n  Args:\n    model: An instance of `tfp.sts.Sum` representing a structural time series\n      model.\n    observed_time_series: `float` `Tensor` of shape\n      `batch_shape + [num_timesteps, 1]` (omitting the trailing unit dimension\n      is also supported when `num_timesteps > 1`), specifying an observed time\n      series. May optionally be an instance of `tfp.sts.MaskedTimeSeries`, which\n      includes a mask `Tensor` to specify timesteps with missing observations.\n    parameter_samples: Python `list` of `Tensors` representing posterior\n      samples of model parameters, with shapes `[concat([\n      [num_posterior_draws], param.prior.batch_shape,\n      param.prior.event_shape]) for param in model.parameters]`. This may\n      optionally also be a map (Python `dict`) of parameter names to\n      `Tensor` values.\n  Returns:\n    component_dists: A `collections.OrderedDict` instance mapping\n      component StructuralTimeSeries instances (elements of `model.components`)\n      to `tfd.Distribution` instances representing the posterior marginal\n      distributions on the process modeled by each component. Each distribution\n      has batch shape matching that of `posterior_means`/`posterior_covs`, and\n      event shape of `[num_timesteps]`.\n\n  #### Examples\n\n  Suppose we've built a model and fit it to data:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    num_steps_forecast = 50\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)\n  ```\n\n  To extract the contributions of individual components, pass the time series\n  and sampled parameters into `decompose_by_component`:\n\n  ```python\n    component_dists = decompose_by_component(\n      model,\n      observed_time_series=observed_time_series,\n      parameter_samples=samples)\n\n    # Component mean and stddev have shape `[len(observed_time_series)]`.\n    day_of_week_effect_mean = component_dists[day_of_week].mean()\n    day_of_week_effect_stddev = component_dists[day_of_week].stddev()\n  ```\n\n  Using the component distributions, we can visualize the uncertainty for\n  each component:\n\n  ```\n  from matplotlib import pylab as plt\n  num_components = len(component_dists)\n  xs = np.arange(len(observed_time_series))\n  fig = plt.figure(figsize=(12, 3 * num_components))\n  for i, (component, component_dist) in enumerate(component_dists.items()):\n\n    # If in graph mode, replace `.numpy()` with `.eval()` or `sess.run()`.\n    component_mean = component_dist.mean().numpy()\n    component_stddev = component_dist.stddev().numpy()\n\n    ax = fig.add_subplot(num_components, 1, 1 + i)\n    ax.plot(xs, component_mean, lw=2)\n    ax.fill_between(xs,\n                    component_mean - 2 * component_stddev,\n                    component_mean + 2 * component_stddev,\n                    alpha=0.5)\n    ax.set_title(component.name)\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope('decompose_by_component',\n                               values=[observed_time_series]):\n    [\n        observed_time_series,\n        is_missing\n    ] = sts_util.canonicalize_observed_time_series_with_mask(\n        observed_time_series)\n\n    # Run smoothing over the training timesteps to extract the\n    # posterior on latents.\n    num_timesteps = dist_util.prefer_static_value(\n        tf.shape(input=observed_time_series))[-2]\n    ssm = model.make_state_space_model(num_timesteps=num_timesteps,\n                                       param_vals=parameter_samples)\n    posterior_means, posterior_covs = ssm.posterior_marginals(\n        observed_time_series, mask=is_missing)\n\n    return _decompose_from_posterior_marginals(\n        model, posterior_means, posterior_covs, parameter_samples)", "language": "python", "code": "def decompose_by_component(model, observed_time_series, parameter_samples):\n  \"\"\"Decompose an observed time series into contributions from each component.\n\n  This method decomposes a time series according to the posterior represention\n  of a structural time series model. In particular, it:\n    - Computes the posterior marginal mean and covariances over the additive\n      model's latent space.\n    - Decomposes the latent posterior into the marginal blocks for each\n      model component.\n    - Maps the per-component latent posteriors back through each component's\n      observation model, to generate the time series modeled by that component.\n\n  Args:\n    model: An instance of `tfp.sts.Sum` representing a structural time series\n      model.\n    observed_time_series: `float` `Tensor` of shape\n      `batch_shape + [num_timesteps, 1]` (omitting the trailing unit dimension\n      is also supported when `num_timesteps > 1`), specifying an observed time\n      series. May optionally be an instance of `tfp.sts.MaskedTimeSeries`, which\n      includes a mask `Tensor` to specify timesteps with missing observations.\n    parameter_samples: Python `list` of `Tensors` representing posterior\n      samples of model parameters, with shapes `[concat([\n      [num_posterior_draws], param.prior.batch_shape,\n      param.prior.event_shape]) for param in model.parameters]`. This may\n      optionally also be a map (Python `dict`) of parameter names to\n      `Tensor` values.\n  Returns:\n    component_dists: A `collections.OrderedDict` instance mapping\n      component StructuralTimeSeries instances (elements of `model.components`)\n      to `tfd.Distribution` instances representing the posterior marginal\n      distributions on the process modeled by each component. Each distribution\n      has batch shape matching that of `posterior_means`/`posterior_covs`, and\n      event shape of `[num_timesteps]`.\n\n  #### Examples\n\n  Suppose we've built a model and fit it to data:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    num_steps_forecast = 50\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)\n  ```\n\n  To extract the contributions of individual components, pass the time series\n  and sampled parameters into `decompose_by_component`:\n\n  ```python\n    component_dists = decompose_by_component(\n      model,\n      observed_time_series=observed_time_series,\n      parameter_samples=samples)\n\n    # Component mean and stddev have shape `[len(observed_time_series)]`.\n    day_of_week_effect_mean = component_dists[day_of_week].mean()\n    day_of_week_effect_stddev = component_dists[day_of_week].stddev()\n  ```\n\n  Using the component distributions, we can visualize the uncertainty for\n  each component:\n\n  ```\n  from matplotlib import pylab as plt\n  num_components = len(component_dists)\n  xs = np.arange(len(observed_time_series))\n  fig = plt.figure(figsize=(12, 3 * num_components))\n  for i, (component, component_dist) in enumerate(component_dists.items()):\n\n    # If in graph mode, replace `.numpy()` with `.eval()` or `sess.run()`.\n    component_mean = component_dist.mean().numpy()\n    component_stddev = component_dist.stddev().numpy()\n\n    ax = fig.add_subplot(num_components, 1, 1 + i)\n    ax.plot(xs, component_mean, lw=2)\n    ax.fill_between(xs,\n                    component_mean - 2 * component_stddev,\n                    component_mean + 2 * component_stddev,\n                    alpha=0.5)\n    ax.set_title(component.name)\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope('decompose_by_component',\n                               values=[observed_time_series]):\n    [\n        observed_time_series,\n        is_missing\n    ] = sts_util.canonicalize_observed_time_series_with_mask(\n        observed_time_series)\n\n    # Run smoothing over the training timesteps to extract the\n    # posterior on latents.\n    num_timesteps = dist_util.prefer_static_value(\n        tf.shape(input=observed_time_series))[-2]\n    ssm = model.make_state_space_model(num_timesteps=num_timesteps,\n                                       param_vals=parameter_samples)\n    posterior_means, posterior_covs = ssm.posterior_marginals(\n        observed_time_series, mask=is_missing)\n\n    return _decompose_from_posterior_marginals(\n        model, posterior_means, posterior_covs, parameter_samples)", "code_tokens": ["def", "decompose_by_component", "(", "model", ",", "observed_time_series", ",", "parameter_samples", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'decompose_by_component'", ",", "values", "=", "[", "observed_time_series", "]", ")", ":", "[", "observed_time_series", ",", "is_missing", "]", "=", "sts_util", ".", "canonicalize_observed_time_series_with_mask", "(", "observed_time_series", ")", "# Run smoothing over the training timesteps to extract the", "# posterior on latents.", "num_timesteps", "=", "dist_util", ".", "prefer_static_value", "(", "tf", ".", "shape", "(", "input", "=", "observed_time_series", ")", ")", "[", "-", "2", "]", "ssm", "=", "model", ".", "make_state_space_model", "(", "num_timesteps", "=", "num_timesteps", ",", "param_vals", "=", "parameter_samples", ")", "posterior_means", ",", "posterior_covs", "=", "ssm", ".", "posterior_marginals", "(", "observed_time_series", ",", "mask", "=", "is_missing", ")", "return", "_decompose_from_posterior_marginals", "(", "model", ",", "posterior_means", ",", "posterior_covs", ",", "parameter_samples", ")"], "docstring": "Decompose an observed time series into contributions from each component.\n\n  This method decomposes a time series according to the posterior represention\n  of a structural time series model. In particular, it:\n    - Computes the posterior marginal mean and covariances over the additive\n      model's latent space.\n    - Decomposes the latent posterior into the marginal blocks for each\n      model component.\n    - Maps the per-component latent posteriors back through each component's\n      observation model, to generate the time series modeled by that component.\n\n  Args:\n    model: An instance of `tfp.sts.Sum` representing a structural time series\n      model.\n    observed_time_series: `float` `Tensor` of shape\n      `batch_shape + [num_timesteps, 1]` (omitting the trailing unit dimension\n      is also supported when `num_timesteps > 1`), specifying an observed time\n      series. May optionally be an instance of `tfp.sts.MaskedTimeSeries`, which\n      includes a mask `Tensor` to specify timesteps with missing observations.\n    parameter_samples: Python `list` of `Tensors` representing posterior\n      samples of model parameters, with shapes `[concat([\n      [num_posterior_draws], param.prior.batch_shape,\n      param.prior.event_shape]) for param in model.parameters]`. This may\n      optionally also be a map (Python `dict`) of parameter names to\n      `Tensor` values.\n  Returns:\n    component_dists: A `collections.OrderedDict` instance mapping\n      component StructuralTimeSeries instances (elements of `model.components`)\n      to `tfd.Distribution` instances representing the posterior marginal\n      distributions on the process modeled by each component. Each distribution\n      has batch shape matching that of `posterior_means`/`posterior_covs`, and\n      event shape of `[num_timesteps]`.\n\n  #### Examples\n\n  Suppose we've built a model and fit it to data:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    num_steps_forecast = 50\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)\n  ```\n\n  To extract the contributions of individual components, pass the time series\n  and sampled parameters into `decompose_by_component`:\n\n  ```python\n    component_dists = decompose_by_component(\n      model,\n      observed_time_series=observed_time_series,\n      parameter_samples=samples)\n\n    # Component mean and stddev have shape `[len(observed_time_series)]`.\n    day_of_week_effect_mean = component_dists[day_of_week].mean()\n    day_of_week_effect_stddev = component_dists[day_of_week].stddev()\n  ```\n\n  Using the component distributions, we can visualize the uncertainty for\n  each component:\n\n  ```\n  from matplotlib import pylab as plt\n  num_components = len(component_dists)\n  xs = np.arange(len(observed_time_series))\n  fig = plt.figure(figsize=(12, 3 * num_components))\n  for i, (component, component_dist) in enumerate(component_dists.items()):\n\n    # If in graph mode, replace `.numpy()` with `.eval()` or `sess.run()`.\n    component_mean = component_dist.mean().numpy()\n    component_stddev = component_dist.stddev().numpy()\n\n    ax = fig.add_subplot(num_components, 1, 1 + i)\n    ax.plot(xs, component_mean, lw=2)\n    ax.fill_between(xs,\n                    component_mean - 2 * component_stddev,\n                    component_mean + 2 * component_stddev,\n                    alpha=0.5)\n    ax.set_title(component.name)\n  ```", "docstring_tokens": ["Decompose", "an", "observed", "time", "series", "into", "contributions", "from", "each", "component", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/decomposition.py#L109-L219", "partition": "test", "index": 762, "time": "2019-03-18 13:28:34"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/sum.py", "func_name": "Sum.make_component_state_space_models", "original_string": "def make_component_state_space_models(self,\n                                        num_timesteps,\n                                        param_vals,\n                                        initial_step=0):\n    \"\"\"Build an ordered list of Distribution instances for component models.\n\n    Args:\n      num_timesteps: Python `int` number of timesteps to model.\n      param_vals: a list of `Tensor` parameter values in order corresponding to\n        `self.parameters`, or a dict mapping from parameter names to values.\n      initial_step: optional `int` specifying the initial timestep to model.\n        This is relevant when the model contains time-varying components,\n        e.g., holidays or seasonality.\n\n    Returns:\n      component_ssms: a Python list of `LinearGaussianStateSpaceModel`\n        Distribution objects, in order corresponding to `self.components`.\n    \"\"\"\n\n    with tf.compat.v1.name_scope('make_component_state_space_models'):\n\n      # List the model parameters in canonical order\n      param_map = self._canonicalize_param_vals_as_map(param_vals)\n      param_vals_list = [param_map[p.name] for p in self.parameters]\n\n      # Build SSMs for each component model. We process the components in\n      # canonical order, extracting the parameters for each component from the\n      # (ordered) list of parameters.\n      remaining_param_vals = param_vals_list[1:]\n      component_ssms = []\n      for component in self.components:\n        num_parameters = len(component.parameters)\n        component_param_vals = remaining_param_vals[:num_parameters]\n        remaining_param_vals = remaining_param_vals[num_parameters:]\n\n        component_ssms.append(\n            component.make_state_space_model(\n                num_timesteps,\n                param_vals=component_param_vals,\n                initial_step=initial_step))\n\n    return component_ssms", "language": "python", "code": "def make_component_state_space_models(self,\n                                        num_timesteps,\n                                        param_vals,\n                                        initial_step=0):\n    \"\"\"Build an ordered list of Distribution instances for component models.\n\n    Args:\n      num_timesteps: Python `int` number of timesteps to model.\n      param_vals: a list of `Tensor` parameter values in order corresponding to\n        `self.parameters`, or a dict mapping from parameter names to values.\n      initial_step: optional `int` specifying the initial timestep to model.\n        This is relevant when the model contains time-varying components,\n        e.g., holidays or seasonality.\n\n    Returns:\n      component_ssms: a Python list of `LinearGaussianStateSpaceModel`\n        Distribution objects, in order corresponding to `self.components`.\n    \"\"\"\n\n    with tf.compat.v1.name_scope('make_component_state_space_models'):\n\n      # List the model parameters in canonical order\n      param_map = self._canonicalize_param_vals_as_map(param_vals)\n      param_vals_list = [param_map[p.name] for p in self.parameters]\n\n      # Build SSMs for each component model. We process the components in\n      # canonical order, extracting the parameters for each component from the\n      # (ordered) list of parameters.\n      remaining_param_vals = param_vals_list[1:]\n      component_ssms = []\n      for component in self.components:\n        num_parameters = len(component.parameters)\n        component_param_vals = remaining_param_vals[:num_parameters]\n        remaining_param_vals = remaining_param_vals[num_parameters:]\n\n        component_ssms.append(\n            component.make_state_space_model(\n                num_timesteps,\n                param_vals=component_param_vals,\n                initial_step=initial_step))\n\n    return component_ssms", "code_tokens": ["def", "make_component_state_space_models", "(", "self", ",", "num_timesteps", ",", "param_vals", ",", "initial_step", "=", "0", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'make_component_state_space_models'", ")", ":", "# List the model parameters in canonical order", "param_map", "=", "self", ".", "_canonicalize_param_vals_as_map", "(", "param_vals", ")", "param_vals_list", "=", "[", "param_map", "[", "p", ".", "name", "]", "for", "p", "in", "self", ".", "parameters", "]", "# Build SSMs for each component model. We process the components in", "# canonical order, extracting the parameters for each component from the", "# (ordered) list of parameters.", "remaining_param_vals", "=", "param_vals_list", "[", "1", ":", "]", "component_ssms", "=", "[", "]", "for", "component", "in", "self", ".", "components", ":", "num_parameters", "=", "len", "(", "component", ".", "parameters", ")", "component_param_vals", "=", "remaining_param_vals", "[", ":", "num_parameters", "]", "remaining_param_vals", "=", "remaining_param_vals", "[", "num_parameters", ":", "]", "component_ssms", ".", "append", "(", "component", ".", "make_state_space_model", "(", "num_timesteps", ",", "param_vals", "=", "component_param_vals", ",", "initial_step", "=", "initial_step", ")", ")", "return", "component_ssms"], "docstring": "Build an ordered list of Distribution instances for component models.\n\n    Args:\n      num_timesteps: Python `int` number of timesteps to model.\n      param_vals: a list of `Tensor` parameter values in order corresponding to\n        `self.parameters`, or a dict mapping from parameter names to values.\n      initial_step: optional `int` specifying the initial timestep to model.\n        This is relevant when the model contains time-varying components,\n        e.g., holidays or seasonality.\n\n    Returns:\n      component_ssms: a Python list of `LinearGaussianStateSpaceModel`\n        Distribution objects, in order corresponding to `self.components`.", "docstring_tokens": ["Build", "an", "ordered", "list", "of", "Distribution", "instances", "for", "component", "models", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/sum.py#L475-L516", "partition": "test", "index": 1041, "time": "2019-03-18 13:28:34"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/decomposition.py", "func_name": "_decompose_from_posterior_marginals", "original_string": "def _decompose_from_posterior_marginals(\n    model, posterior_means, posterior_covs, parameter_samples):\n  \"\"\"Utility method to decompose a joint posterior into components.\n\n  Args:\n    model: `tfp.sts.Sum` instance defining an additive STS model.\n    posterior_means: float `Tensor` of shape `concat(\n      [[num_posterior_draws], batch_shape, num_timesteps, latent_size])`\n      representing the posterior mean over latents in an\n      `AdditiveStateSpaceModel`.\n    posterior_covs: float `Tensor` of shape `concat(\n      [[num_posterior_draws], batch_shape, num_timesteps,\n      latent_size, latent_size])`\n      representing the posterior marginal covariances over latents in an\n      `AdditiveStateSpaceModel`.\n    parameter_samples: Python `list` of `Tensors` representing posterior\n      samples of model parameters, with shapes `[concat([\n      [num_posterior_draws], param.prior.batch_shape,\n      param.prior.event_shape]) for param in model.parameters]`. This may\n      optionally also be a map (Python `dict`) of parameter names to\n      `Tensor` values.\n\n  Returns:\n    component_dists: A `collections.OrderedDict` instance mapping\n      component StructuralTimeSeries instances (elements of `model.components`)\n      to `tfd.Distribution` instances representing the posterior marginal\n      distributions on the process modeled by each component. Each distribution\n      has batch shape matching that of `posterior_means`/`posterior_covs`, and\n      event shape of `[num_timesteps]`.\n  \"\"\"\n\n  try:\n    model.components\n  except AttributeError:\n    raise ValueError('Model decomposed into components must be an instance of'\n                     '`tfp.sts.Sum` (passed model {})'.format(model))\n\n  with tf.compat.v1.name_scope('decompose_from_posterior_marginals'):\n\n    # Extract the component means/covs from the joint latent posterior.\n    latent_sizes = [component.latent_size for component in model.components]\n    component_means = tf.split(posterior_means, latent_sizes, axis=-1)\n    component_covs = _split_covariance_into_marginals(\n        posterior_covs, latent_sizes)\n\n    # Instantiate per-component state space models, and use them to push the\n    # posterior means/covs through the observation model for each component.\n    num_timesteps = dist_util.prefer_static_value(\n        tf.shape(input=posterior_means))[-2]\n    component_ssms = model.make_component_state_space_models(\n        num_timesteps=num_timesteps,\n        param_vals=parameter_samples)\n    component_predictive_dists = collections.OrderedDict()\n    for (component, component_ssm,\n         component_mean, component_cov) in zip(model.components, component_ssms,\n                                               component_means, component_covs):\n      component_obs_mean, component_obs_cov = (\n          component_ssm.latents_to_observations(\n              latent_means=component_mean,\n              latent_covs=component_cov))\n\n      # Using the observation means and covs, build a mixture distribution\n      # that integrates over the posterior draws.\n      component_predictive_dists[component] = sts_util.mix_over_posterior_draws(\n          means=component_obs_mean[..., 0],\n          variances=component_obs_cov[..., 0, 0])\n  return component_predictive_dists", "language": "python", "code": "def _decompose_from_posterior_marginals(\n    model, posterior_means, posterior_covs, parameter_samples):\n  \"\"\"Utility method to decompose a joint posterior into components.\n\n  Args:\n    model: `tfp.sts.Sum` instance defining an additive STS model.\n    posterior_means: float `Tensor` of shape `concat(\n      [[num_posterior_draws], batch_shape, num_timesteps, latent_size])`\n      representing the posterior mean over latents in an\n      `AdditiveStateSpaceModel`.\n    posterior_covs: float `Tensor` of shape `concat(\n      [[num_posterior_draws], batch_shape, num_timesteps,\n      latent_size, latent_size])`\n      representing the posterior marginal covariances over latents in an\n      `AdditiveStateSpaceModel`.\n    parameter_samples: Python `list` of `Tensors` representing posterior\n      samples of model parameters, with shapes `[concat([\n      [num_posterior_draws], param.prior.batch_shape,\n      param.prior.event_shape]) for param in model.parameters]`. This may\n      optionally also be a map (Python `dict`) of parameter names to\n      `Tensor` values.\n\n  Returns:\n    component_dists: A `collections.OrderedDict` instance mapping\n      component StructuralTimeSeries instances (elements of `model.components`)\n      to `tfd.Distribution` instances representing the posterior marginal\n      distributions on the process modeled by each component. Each distribution\n      has batch shape matching that of `posterior_means`/`posterior_covs`, and\n      event shape of `[num_timesteps]`.\n  \"\"\"\n\n  try:\n    model.components\n  except AttributeError:\n    raise ValueError('Model decomposed into components must be an instance of'\n                     '`tfp.sts.Sum` (passed model {})'.format(model))\n\n  with tf.compat.v1.name_scope('decompose_from_posterior_marginals'):\n\n    # Extract the component means/covs from the joint latent posterior.\n    latent_sizes = [component.latent_size for component in model.components]\n    component_means = tf.split(posterior_means, latent_sizes, axis=-1)\n    component_covs = _split_covariance_into_marginals(\n        posterior_covs, latent_sizes)\n\n    # Instantiate per-component state space models, and use them to push the\n    # posterior means/covs through the observation model for each component.\n    num_timesteps = dist_util.prefer_static_value(\n        tf.shape(input=posterior_means))[-2]\n    component_ssms = model.make_component_state_space_models(\n        num_timesteps=num_timesteps,\n        param_vals=parameter_samples)\n    component_predictive_dists = collections.OrderedDict()\n    for (component, component_ssm,\n         component_mean, component_cov) in zip(model.components, component_ssms,\n                                               component_means, component_covs):\n      component_obs_mean, component_obs_cov = (\n          component_ssm.latents_to_observations(\n              latent_means=component_mean,\n              latent_covs=component_cov))\n\n      # Using the observation means and covs, build a mixture distribution\n      # that integrates over the posterior draws.\n      component_predictive_dists[component] = sts_util.mix_over_posterior_draws(\n          means=component_obs_mean[..., 0],\n          variances=component_obs_cov[..., 0, 0])\n  return component_predictive_dists", "code_tokens": ["def", "_decompose_from_posterior_marginals", "(", "model", ",", "posterior_means", ",", "posterior_covs", ",", "parameter_samples", ")", ":", "try", ":", "model", ".", "components", "except", "AttributeError", ":", "raise", "ValueError", "(", "'Model decomposed into components must be an instance of'", "'`tfp.sts.Sum` (passed model {})'", ".", "format", "(", "model", ")", ")", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'decompose_from_posterior_marginals'", ")", ":", "# Extract the component means/covs from the joint latent posterior.", "latent_sizes", "=", "[", "component", ".", "latent_size", "for", "component", "in", "model", ".", "components", "]", "component_means", "=", "tf", ".", "split", "(", "posterior_means", ",", "latent_sizes", ",", "axis", "=", "-", "1", ")", "component_covs", "=", "_split_covariance_into_marginals", "(", "posterior_covs", ",", "latent_sizes", ")", "# Instantiate per-component state space models, and use them to push the", "# posterior means/covs through the observation model for each component.", "num_timesteps", "=", "dist_util", ".", "prefer_static_value", "(", "tf", ".", "shape", "(", "input", "=", "posterior_means", ")", ")", "[", "-", "2", "]", "component_ssms", "=", "model", ".", "make_component_state_space_models", "(", "num_timesteps", "=", "num_timesteps", ",", "param_vals", "=", "parameter_samples", ")", "component_predictive_dists", "=", "collections", ".", "OrderedDict", "(", ")", "for", "(", "component", ",", "component_ssm", ",", "component_mean", ",", "component_cov", ")", "in", "zip", "(", "model", ".", "components", ",", "component_ssms", ",", "component_means", ",", "component_covs", ")", ":", "component_obs_mean", ",", "component_obs_cov", "=", "(", "component_ssm", ".", "latents_to_observations", "(", "latent_means", "=", "component_mean", ",", "latent_covs", "=", "component_cov", ")", ")", "# Using the observation means and covs, build a mixture distribution", "# that integrates over the posterior draws.", "component_predictive_dists", "[", "component", "]", "=", "sts_util", ".", "mix_over_posterior_draws", "(", "means", "=", "component_obs_mean", "[", "...", ",", "0", "]", ",", "variances", "=", "component_obs_cov", "[", "...", ",", "0", ",", "0", "]", ")", "return", "component_predictive_dists"], "docstring": "Utility method to decompose a joint posterior into components.\n\n  Args:\n    model: `tfp.sts.Sum` instance defining an additive STS model.\n    posterior_means: float `Tensor` of shape `concat(\n      [[num_posterior_draws], batch_shape, num_timesteps, latent_size])`\n      representing the posterior mean over latents in an\n      `AdditiveStateSpaceModel`.\n    posterior_covs: float `Tensor` of shape `concat(\n      [[num_posterior_draws], batch_shape, num_timesteps,\n      latent_size, latent_size])`\n      representing the posterior marginal covariances over latents in an\n      `AdditiveStateSpaceModel`.\n    parameter_samples: Python `list` of `Tensors` representing posterior\n      samples of model parameters, with shapes `[concat([\n      [num_posterior_draws], param.prior.batch_shape,\n      param.prior.event_shape]) for param in model.parameters]`. This may\n      optionally also be a map (Python `dict`) of parameter names to\n      `Tensor` values.\n\n  Returns:\n    component_dists: A `collections.OrderedDict` instance mapping\n      component StructuralTimeSeries instances (elements of `model.components`)\n      to `tfd.Distribution` instances representing the posterior marginal\n      distributions on the process modeled by each component. Each distribution\n      has batch shape matching that of `posterior_means`/`posterior_covs`, and\n      event shape of `[num_timesteps]`.", "docstring_tokens": ["Utility", "method", "to", "decompose", "a", "joint", "posterior", "into", "components", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/decomposition.py#L40-L106", "partition": "test", "index": 761, "time": "2019-03-18 13:28:34"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/decomposition.py", "func_name": "_split_covariance_into_marginals", "original_string": "def _split_covariance_into_marginals(covariance, block_sizes):\n  \"\"\"Split a covariance matrix into block-diagonal marginals of given sizes.\"\"\"\n  start_dim = 0\n  marginals = []\n  for size in block_sizes:\n    end_dim = start_dim + size\n    marginals.append(covariance[..., start_dim:end_dim, start_dim:end_dim])\n    start_dim = end_dim\n  return marginals", "language": "python", "code": "def _split_covariance_into_marginals(covariance, block_sizes):\n  \"\"\"Split a covariance matrix into block-diagonal marginals of given sizes.\"\"\"\n  start_dim = 0\n  marginals = []\n  for size in block_sizes:\n    end_dim = start_dim + size\n    marginals.append(covariance[..., start_dim:end_dim, start_dim:end_dim])\n    start_dim = end_dim\n  return marginals", "code_tokens": ["def", "_split_covariance_into_marginals", "(", "covariance", ",", "block_sizes", ")", ":", "start_dim", "=", "0", "marginals", "=", "[", "]", "for", "size", "in", "block_sizes", ":", "end_dim", "=", "start_dim", "+", "size", "marginals", ".", "append", "(", "covariance", "[", "...", ",", "start_dim", ":", "end_dim", ",", "start_dim", ":", "end_dim", "]", ")", "start_dim", "=", "end_dim", "return", "marginals"], "docstring": "Split a covariance matrix into block-diagonal marginals of given sizes.", "docstring_tokens": ["Split", "a", "covariance", "matrix", "into", "block", "-", "diagonal", "marginals", "of", "given", "sizes", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/decomposition.py#L29-L37", "partition": "test", "index": 760, "time": "2019-03-18 13:28:34"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/seasonal.py", "func_name": "build_effects_to_residuals_matrix", "original_string": "def build_effects_to_residuals_matrix(num_seasons, dtype):\n  \"\"\"Build change-of-basis matrices for constrained seasonal effects.\n\n  This method builds the matrix that transforms seasonal effects into\n  effect residuals (differences from the mean effect), and additionally\n  projects these residuals onto the subspace where the mean effect is zero.\n\n  See `ConstrainedSeasonalStateSpaceModel` for mathematical details.\n\n  Args:\n    num_seasons: scalar `int` number of seasons.\n    dtype: TensorFlow `dtype` for the returned values.\n  Returns:\n    effects_to_residuals: `Tensor` of shape\n      `[num_seasons-1, num_seasons]`, such that `differences_from_mean_effect =\n      matmul(effects_to_residuals, seasonal_effects)`.  In the\n      notation of `ConstrainedSeasonalStateSpaceModel`, this is\n      `effects_to_residuals = P * R`.\n    residuals_to_effects: the (pseudo)-inverse of the above; a\n      `Tensor` of shape `[num_seasons, num_seasons-1]`. In the\n      notation of `ConstrainedSeasonalStateSpaceModel`, this is\n      `residuals_to_effects = R^{-1} * P'`.\n  \"\"\"\n\n  # Build the matrix that converts effects `e_i` into differences from the mean\n  # effect `(e_i - sum(e_i)) / num_seasons`, with the mean effect in the last\n  # row so that the transformation is invertible.\n  effects_to_residuals_fullrank = np.eye(num_seasons) - 1./num_seasons\n  effects_to_residuals_fullrank[-1, :] = 1./num_seasons  # compute mean effect\n  residuals_to_effects_fullrank = np.linalg.inv(effects_to_residuals_fullrank)\n\n  # Drop the final dimension, effectively setting the mean effect to zero.\n  effects_to_residuals = effects_to_residuals_fullrank[:-1, :]\n  residuals_to_effects = residuals_to_effects_fullrank[:, :-1]\n\n  # Return Tensor values of the specified dtype.\n  effects_to_residuals = tf.cast(\n      effects_to_residuals, dtype=dtype, name='effects_to_residuals')\n  residuals_to_effects = tf.cast(\n      residuals_to_effects, dtype=dtype, name='residuals_to_effects')\n\n  return effects_to_residuals, residuals_to_effects", "language": "python", "code": "def build_effects_to_residuals_matrix(num_seasons, dtype):\n  \"\"\"Build change-of-basis matrices for constrained seasonal effects.\n\n  This method builds the matrix that transforms seasonal effects into\n  effect residuals (differences from the mean effect), and additionally\n  projects these residuals onto the subspace where the mean effect is zero.\n\n  See `ConstrainedSeasonalStateSpaceModel` for mathematical details.\n\n  Args:\n    num_seasons: scalar `int` number of seasons.\n    dtype: TensorFlow `dtype` for the returned values.\n  Returns:\n    effects_to_residuals: `Tensor` of shape\n      `[num_seasons-1, num_seasons]`, such that `differences_from_mean_effect =\n      matmul(effects_to_residuals, seasonal_effects)`.  In the\n      notation of `ConstrainedSeasonalStateSpaceModel`, this is\n      `effects_to_residuals = P * R`.\n    residuals_to_effects: the (pseudo)-inverse of the above; a\n      `Tensor` of shape `[num_seasons, num_seasons-1]`. In the\n      notation of `ConstrainedSeasonalStateSpaceModel`, this is\n      `residuals_to_effects = R^{-1} * P'`.\n  \"\"\"\n\n  # Build the matrix that converts effects `e_i` into differences from the mean\n  # effect `(e_i - sum(e_i)) / num_seasons`, with the mean effect in the last\n  # row so that the transformation is invertible.\n  effects_to_residuals_fullrank = np.eye(num_seasons) - 1./num_seasons\n  effects_to_residuals_fullrank[-1, :] = 1./num_seasons  # compute mean effect\n  residuals_to_effects_fullrank = np.linalg.inv(effects_to_residuals_fullrank)\n\n  # Drop the final dimension, effectively setting the mean effect to zero.\n  effects_to_residuals = effects_to_residuals_fullrank[:-1, :]\n  residuals_to_effects = residuals_to_effects_fullrank[:, :-1]\n\n  # Return Tensor values of the specified dtype.\n  effects_to_residuals = tf.cast(\n      effects_to_residuals, dtype=dtype, name='effects_to_residuals')\n  residuals_to_effects = tf.cast(\n      residuals_to_effects, dtype=dtype, name='residuals_to_effects')\n\n  return effects_to_residuals, residuals_to_effects", "code_tokens": ["def", "build_effects_to_residuals_matrix", "(", "num_seasons", ",", "dtype", ")", ":", "# Build the matrix that converts effects `e_i` into differences from the mean", "# effect `(e_i - sum(e_i)) / num_seasons`, with the mean effect in the last", "# row so that the transformation is invertible.", "effects_to_residuals_fullrank", "=", "np", ".", "eye", "(", "num_seasons", ")", "-", "1.", "/", "num_seasons", "effects_to_residuals_fullrank", "[", "-", "1", ",", ":", "]", "=", "1.", "/", "num_seasons", "# compute mean effect", "residuals_to_effects_fullrank", "=", "np", ".", "linalg", ".", "inv", "(", "effects_to_residuals_fullrank", ")", "# Drop the final dimension, effectively setting the mean effect to zero.", "effects_to_residuals", "=", "effects_to_residuals_fullrank", "[", ":", "-", "1", ",", ":", "]", "residuals_to_effects", "=", "residuals_to_effects_fullrank", "[", ":", ",", ":", "-", "1", "]", "# Return Tensor values of the specified dtype.", "effects_to_residuals", "=", "tf", ".", "cast", "(", "effects_to_residuals", ",", "dtype", "=", "dtype", ",", "name", "=", "'effects_to_residuals'", ")", "residuals_to_effects", "=", "tf", ".", "cast", "(", "residuals_to_effects", ",", "dtype", "=", "dtype", ",", "name", "=", "'residuals_to_effects'", ")", "return", "effects_to_residuals", ",", "residuals_to_effects"], "docstring": "Build change-of-basis matrices for constrained seasonal effects.\n\n  This method builds the matrix that transforms seasonal effects into\n  effect residuals (differences from the mean effect), and additionally\n  projects these residuals onto the subspace where the mean effect is zero.\n\n  See `ConstrainedSeasonalStateSpaceModel` for mathematical details.\n\n  Args:\n    num_seasons: scalar `int` number of seasons.\n    dtype: TensorFlow `dtype` for the returned values.\n  Returns:\n    effects_to_residuals: `Tensor` of shape\n      `[num_seasons-1, num_seasons]`, such that `differences_from_mean_effect =\n      matmul(effects_to_residuals, seasonal_effects)`.  In the\n      notation of `ConstrainedSeasonalStateSpaceModel`, this is\n      `effects_to_residuals = P * R`.\n    residuals_to_effects: the (pseudo)-inverse of the above; a\n      `Tensor` of shape `[num_seasons, num_seasons-1]`. In the\n      notation of `ConstrainedSeasonalStateSpaceModel`, this is\n      `residuals_to_effects = R^{-1} * P'`.", "docstring_tokens": ["Build", "change", "-", "of", "-", "basis", "matrices", "for", "constrained", "seasonal", "effects", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/seasonal.py#L529-L570", "partition": "test", "index": 1135, "time": "2019-03-18 17:07:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/seasonal.py", "func_name": "build_is_last_day_of_season", "original_string": "def build_is_last_day_of_season(num_steps_per_season):\n  \"\"\"Build utility method to compute whether the season is changing.\"\"\"\n  num_steps_per_cycle = np.sum(num_steps_per_season)\n  changepoints = np.cumsum(np.ravel(num_steps_per_season)) - 1\n  def is_last_day_of_season(t):\n    t_ = dist_util.maybe_get_static_value(t)\n    if t_ is not None:  # static case\n      step_in_cycle = t_ % num_steps_per_cycle\n      return any(step_in_cycle == changepoints)\n    else:\n      step_in_cycle = tf.math.floormod(t, num_steps_per_cycle)\n      return tf.reduce_any(\n          input_tensor=tf.equal(step_in_cycle, changepoints))\n  return is_last_day_of_season", "language": "python", "code": "def build_is_last_day_of_season(num_steps_per_season):\n  \"\"\"Build utility method to compute whether the season is changing.\"\"\"\n  num_steps_per_cycle = np.sum(num_steps_per_season)\n  changepoints = np.cumsum(np.ravel(num_steps_per_season)) - 1\n  def is_last_day_of_season(t):\n    t_ = dist_util.maybe_get_static_value(t)\n    if t_ is not None:  # static case\n      step_in_cycle = t_ % num_steps_per_cycle\n      return any(step_in_cycle == changepoints)\n    else:\n      step_in_cycle = tf.math.floormod(t, num_steps_per_cycle)\n      return tf.reduce_any(\n          input_tensor=tf.equal(step_in_cycle, changepoints))\n  return is_last_day_of_season", "code_tokens": ["def", "build_is_last_day_of_season", "(", "num_steps_per_season", ")", ":", "num_steps_per_cycle", "=", "np", ".", "sum", "(", "num_steps_per_season", ")", "changepoints", "=", "np", ".", "cumsum", "(", "np", ".", "ravel", "(", "num_steps_per_season", ")", ")", "-", "1", "def", "is_last_day_of_season", "(", "t", ")", ":", "t_", "=", "dist_util", ".", "maybe_get_static_value", "(", "t", ")", "if", "t_", "is", "not", "None", ":", "# static case", "step_in_cycle", "=", "t_", "%", "num_steps_per_cycle", "return", "any", "(", "step_in_cycle", "==", "changepoints", ")", "else", ":", "step_in_cycle", "=", "tf", ".", "math", ".", "floormod", "(", "t", ",", "num_steps_per_cycle", ")", "return", "tf", ".", "reduce_any", "(", "input_tensor", "=", "tf", ".", "equal", "(", "step_in_cycle", ",", "changepoints", ")", ")", "return", "is_last_day_of_season"], "docstring": "Build utility method to compute whether the season is changing.", "docstring_tokens": ["Build", "utility", "method", "to", "compute", "whether", "the", "season", "is", "changing", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/seasonal.py#L513-L526", "partition": "test", "index": 1134, "time": "2019-03-18 17:07:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/seasonal.py", "func_name": "build_constrained_seasonal_transition_noise", "original_string": "def build_constrained_seasonal_transition_noise(\n    drift_scale, num_seasons, is_last_day_of_season):\n  \"\"\"Build transition noise distribution for a ConstrainedSeasonalSSM.\"\"\"\n\n  # Conceptually, this method takes the noise covariance on effects L @ L'\n  # computed by `build_seasonal_transition_noise`, with scale factor\n  #       L = [ 0, 0, ..., 0\n  #             ...\n  #             0, 0, ..., drift_scale],\n  # and transforms it to act on the constrained-residual representation.\n  #\n  # The resulting noise covariance M @ M' is equivalent to\n  #    M @ M' = effects_to_residuals @ LL' @ residuals_to_effects\n  # where `@` is matrix multiplication. However because this matrix is\n  # rank-deficient, we can't take its Cholesky decomposition directly, so we'll\n  # construct its lower-triangular scale factor `M` by hand instead.\n  #\n  # Concretely, let `M = P @ R @ L` be the scale factor in the\n  # transformed space, with matrices `R`, `P` applying the reparameterization\n  # and zero-mean constraint respectively as defined in the\n  # \"Mathematical Details\" section of `ConstrainedSeasonalStateSpaceModel`. It's\n  # easy to see (*) that the implied covariance\n  # `M @ M' = P @ R @ L @ L' @ R' @ P'` is just the constant matrix\n  #  `M @ M' = [ 1, 1, ..., 1, 0\n  #              1, 1, ..., 1, 0\n  #              ...\n  #              1, 1, ..., 1, 0\n  #              0, 0, ..., 0, 0] * (drift_scale / num_seasons)**2`\n  # with zeros in the final row and column. So we can directly construct\n  # the lower-triangular factor\n  #  `Q = [ 1, 0, ...  0\n  #         1, 0, ..., 0\n  #         ...\n  #         1, 0, ..., 0\n  #         0, 0, ..., 0 ] * drift_scale/num_seasons`\n  # such that Q @ Q' = M @ M'. In practice, we don't reify the final row and\n  # column full of zeroes, i.e., we construct\n  # `Q[:num_seasons-1, :num_seasons-1]` as the scale-TriL covariance factor.\n  #\n  # (*) Argument: `L` is zero everywhere but the last column, so `R @ L` will be\n  # too. Since the last column of `R` is the constant `-1/num_seasons`, `R @ L`\n  # is simply the matrix with constant `-drift_scale/num_seasons` in the final\n  # column (except the final row, which is negated) and zero in all other\n  # columns, and `M = P @ R @ L` additionally zeroes out the final row. Then\n  # M @ M' is just the outer product of that final column with itself (since all\n  # other columns are zero), which gives the matrix shown above.\n\n  drift_scale_tril_nonzeros = tf.concat([\n      tf.ones([num_seasons - 1, 1], dtype=drift_scale.dtype),\n      tf.zeros([num_seasons - 1, num_seasons - 2], dtype=drift_scale.dtype)],\n                                        axis=-1)\n  drift_scale_tril = (drift_scale_tril_nonzeros *\n                      drift_scale[..., tf.newaxis, tf.newaxis] / num_seasons)\n\n  # Inject transition noise iff it is the last day of the season.\n  def seasonal_transition_noise(t):\n    noise_scale_tril = dist_util.pick_scalar_condition(\n        is_last_day_of_season(t),\n        drift_scale_tril,\n        tf.zeros_like(drift_scale_tril))\n    return tfd.MultivariateNormalTriL(\n        loc=tf.zeros(num_seasons-1, dtype=drift_scale.dtype),\n        scale_tril=noise_scale_tril)\n  return seasonal_transition_noise", "language": "python", "code": "def build_constrained_seasonal_transition_noise(\n    drift_scale, num_seasons, is_last_day_of_season):\n  \"\"\"Build transition noise distribution for a ConstrainedSeasonalSSM.\"\"\"\n\n  # Conceptually, this method takes the noise covariance on effects L @ L'\n  # computed by `build_seasonal_transition_noise`, with scale factor\n  #       L = [ 0, 0, ..., 0\n  #             ...\n  #             0, 0, ..., drift_scale],\n  # and transforms it to act on the constrained-residual representation.\n  #\n  # The resulting noise covariance M @ M' is equivalent to\n  #    M @ M' = effects_to_residuals @ LL' @ residuals_to_effects\n  # where `@` is matrix multiplication. However because this matrix is\n  # rank-deficient, we can't take its Cholesky decomposition directly, so we'll\n  # construct its lower-triangular scale factor `M` by hand instead.\n  #\n  # Concretely, let `M = P @ R @ L` be the scale factor in the\n  # transformed space, with matrices `R`, `P` applying the reparameterization\n  # and zero-mean constraint respectively as defined in the\n  # \"Mathematical Details\" section of `ConstrainedSeasonalStateSpaceModel`. It's\n  # easy to see (*) that the implied covariance\n  # `M @ M' = P @ R @ L @ L' @ R' @ P'` is just the constant matrix\n  #  `M @ M' = [ 1, 1, ..., 1, 0\n  #              1, 1, ..., 1, 0\n  #              ...\n  #              1, 1, ..., 1, 0\n  #              0, 0, ..., 0, 0] * (drift_scale / num_seasons)**2`\n  # with zeros in the final row and column. So we can directly construct\n  # the lower-triangular factor\n  #  `Q = [ 1, 0, ...  0\n  #         1, 0, ..., 0\n  #         ...\n  #         1, 0, ..., 0\n  #         0, 0, ..., 0 ] * drift_scale/num_seasons`\n  # such that Q @ Q' = M @ M'. In practice, we don't reify the final row and\n  # column full of zeroes, i.e., we construct\n  # `Q[:num_seasons-1, :num_seasons-1]` as the scale-TriL covariance factor.\n  #\n  # (*) Argument: `L` is zero everywhere but the last column, so `R @ L` will be\n  # too. Since the last column of `R` is the constant `-1/num_seasons`, `R @ L`\n  # is simply the matrix with constant `-drift_scale/num_seasons` in the final\n  # column (except the final row, which is negated) and zero in all other\n  # columns, and `M = P @ R @ L` additionally zeroes out the final row. Then\n  # M @ M' is just the outer product of that final column with itself (since all\n  # other columns are zero), which gives the matrix shown above.\n\n  drift_scale_tril_nonzeros = tf.concat([\n      tf.ones([num_seasons - 1, 1], dtype=drift_scale.dtype),\n      tf.zeros([num_seasons - 1, num_seasons - 2], dtype=drift_scale.dtype)],\n                                        axis=-1)\n  drift_scale_tril = (drift_scale_tril_nonzeros *\n                      drift_scale[..., tf.newaxis, tf.newaxis] / num_seasons)\n\n  # Inject transition noise iff it is the last day of the season.\n  def seasonal_transition_noise(t):\n    noise_scale_tril = dist_util.pick_scalar_condition(\n        is_last_day_of_season(t),\n        drift_scale_tril,\n        tf.zeros_like(drift_scale_tril))\n    return tfd.MultivariateNormalTriL(\n        loc=tf.zeros(num_seasons-1, dtype=drift_scale.dtype),\n        scale_tril=noise_scale_tril)\n  return seasonal_transition_noise", "code_tokens": ["def", "build_constrained_seasonal_transition_noise", "(", "drift_scale", ",", "num_seasons", ",", "is_last_day_of_season", ")", ":", "# Conceptually, this method takes the noise covariance on effects L @ L'", "# computed by `build_seasonal_transition_noise`, with scale factor", "#       L = [ 0, 0, ..., 0", "#             ...", "#             0, 0, ..., drift_scale],", "# and transforms it to act on the constrained-residual representation.", "#", "# The resulting noise covariance M @ M' is equivalent to", "#    M @ M' = effects_to_residuals @ LL' @ residuals_to_effects", "# where `@` is matrix multiplication. However because this matrix is", "# rank-deficient, we can't take its Cholesky decomposition directly, so we'll", "# construct its lower-triangular scale factor `M` by hand instead.", "#", "# Concretely, let `M = P @ R @ L` be the scale factor in the", "# transformed space, with matrices `R`, `P` applying the reparameterization", "# and zero-mean constraint respectively as defined in the", "# \"Mathematical Details\" section of `ConstrainedSeasonalStateSpaceModel`. It's", "# easy to see (*) that the implied covariance", "# `M @ M' = P @ R @ L @ L' @ R' @ P'` is just the constant matrix", "#  `M @ M' = [ 1, 1, ..., 1, 0", "#              1, 1, ..., 1, 0", "#              ...", "#              1, 1, ..., 1, 0", "#              0, 0, ..., 0, 0] * (drift_scale / num_seasons)**2`", "# with zeros in the final row and column. So we can directly construct", "# the lower-triangular factor", "#  `Q = [ 1, 0, ...  0", "#         1, 0, ..., 0", "#         ...", "#         1, 0, ..., 0", "#         0, 0, ..., 0 ] * drift_scale/num_seasons`", "# such that Q @ Q' = M @ M'. In practice, we don't reify the final row and", "# column full of zeroes, i.e., we construct", "# `Q[:num_seasons-1, :num_seasons-1]` as the scale-TriL covariance factor.", "#", "# (*) Argument: `L` is zero everywhere but the last column, so `R @ L` will be", "# too. Since the last column of `R` is the constant `-1/num_seasons`, `R @ L`", "# is simply the matrix with constant `-drift_scale/num_seasons` in the final", "# column (except the final row, which is negated) and zero in all other", "# columns, and `M = P @ R @ L` additionally zeroes out the final row. Then", "# M @ M' is just the outer product of that final column with itself (since all", "# other columns are zero), which gives the matrix shown above.", "drift_scale_tril_nonzeros", "=", "tf", ".", "concat", "(", "[", "tf", ".", "ones", "(", "[", "num_seasons", "-", "1", ",", "1", "]", ",", "dtype", "=", "drift_scale", ".", "dtype", ")", ",", "tf", ".", "zeros", "(", "[", "num_seasons", "-", "1", ",", "num_seasons", "-", "2", "]", ",", "dtype", "=", "drift_scale", ".", "dtype", ")", "]", ",", "axis", "=", "-", "1", ")", "drift_scale_tril", "=", "(", "drift_scale_tril_nonzeros", "*", "drift_scale", "[", "...", ",", "tf", ".", "newaxis", ",", "tf", ".", "newaxis", "]", "/", "num_seasons", ")", "# Inject transition noise iff it is the last day of the season.", "def", "seasonal_transition_noise", "(", "t", ")", ":", "noise_scale_tril", "=", "dist_util", ".", "pick_scalar_condition", "(", "is_last_day_of_season", "(", "t", ")", ",", "drift_scale_tril", ",", "tf", ".", "zeros_like", "(", "drift_scale_tril", ")", ")", "return", "tfd", ".", "MultivariateNormalTriL", "(", "loc", "=", "tf", ".", "zeros", "(", "num_seasons", "-", "1", ",", "dtype", "=", "drift_scale", ".", "dtype", ")", ",", "scale_tril", "=", "noise_scale_tril", ")", "return", "seasonal_transition_noise"], "docstring": "Build transition noise distribution for a ConstrainedSeasonalSSM.", "docstring_tokens": ["Build", "transition", "noise", "distribution", "for", "a", "ConstrainedSeasonalSSM", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/seasonal.py#L628-L691", "partition": "test", "index": 1138, "time": "2019-03-18 17:07:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/seasonal.py", "func_name": "build_seasonal_transition_matrix", "original_string": "def build_seasonal_transition_matrix(\n    num_seasons, is_last_day_of_season, dtype,\n    basis_change_matrix=None, basis_change_matrix_inv=None):\n  \"\"\"Build a function computing transitions for a seasonal effect model.\"\"\"\n\n  with tf.compat.v1.name_scope('build_seasonal_transition_matrix'):\n    # If the season is changing, the transition matrix permutes the latent\n    # state to shift all seasons up by a dimension, and sends the current\n    # season's effect to the bottom.\n    seasonal_permutation = np.concatenate(\n        [np.arange(1, num_seasons), [0]], axis=0)\n    seasonal_permutation_matrix = tf.constant(\n        np.eye(num_seasons)[seasonal_permutation], dtype=dtype)\n\n    # Optionally transform the transition matrix into a reparameterized space,\n    # enforcing the zero-sum constraint for ConstrainedSeasonalStateSpaceModel.\n    if basis_change_matrix is not None:\n      seasonal_permutation_matrix = tf.matmul(\n          basis_change_matrix,\n          tf.matmul(seasonal_permutation_matrix, basis_change_matrix_inv))\n\n    identity_matrix = tf.eye(\n        tf.shape(input=seasonal_permutation_matrix)[-1], dtype=dtype)\n\n    def seasonal_transition_matrix(t):\n      return tf.linalg.LinearOperatorFullMatrix(\n          matrix=dist_util.pick_scalar_condition(\n              is_last_day_of_season(t),\n              seasonal_permutation_matrix,\n              identity_matrix))\n\n  return seasonal_transition_matrix", "language": "python", "code": "def build_seasonal_transition_matrix(\n    num_seasons, is_last_day_of_season, dtype,\n    basis_change_matrix=None, basis_change_matrix_inv=None):\n  \"\"\"Build a function computing transitions for a seasonal effect model.\"\"\"\n\n  with tf.compat.v1.name_scope('build_seasonal_transition_matrix'):\n    # If the season is changing, the transition matrix permutes the latent\n    # state to shift all seasons up by a dimension, and sends the current\n    # season's effect to the bottom.\n    seasonal_permutation = np.concatenate(\n        [np.arange(1, num_seasons), [0]], axis=0)\n    seasonal_permutation_matrix = tf.constant(\n        np.eye(num_seasons)[seasonal_permutation], dtype=dtype)\n\n    # Optionally transform the transition matrix into a reparameterized space,\n    # enforcing the zero-sum constraint for ConstrainedSeasonalStateSpaceModel.\n    if basis_change_matrix is not None:\n      seasonal_permutation_matrix = tf.matmul(\n          basis_change_matrix,\n          tf.matmul(seasonal_permutation_matrix, basis_change_matrix_inv))\n\n    identity_matrix = tf.eye(\n        tf.shape(input=seasonal_permutation_matrix)[-1], dtype=dtype)\n\n    def seasonal_transition_matrix(t):\n      return tf.linalg.LinearOperatorFullMatrix(\n          matrix=dist_util.pick_scalar_condition(\n              is_last_day_of_season(t),\n              seasonal_permutation_matrix,\n              identity_matrix))\n\n  return seasonal_transition_matrix", "code_tokens": ["def", "build_seasonal_transition_matrix", "(", "num_seasons", ",", "is_last_day_of_season", ",", "dtype", ",", "basis_change_matrix", "=", "None", ",", "basis_change_matrix_inv", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'build_seasonal_transition_matrix'", ")", ":", "# If the season is changing, the transition matrix permutes the latent", "# state to shift all seasons up by a dimension, and sends the current", "# season's effect to the bottom.", "seasonal_permutation", "=", "np", ".", "concatenate", "(", "[", "np", ".", "arange", "(", "1", ",", "num_seasons", ")", ",", "[", "0", "]", "]", ",", "axis", "=", "0", ")", "seasonal_permutation_matrix", "=", "tf", ".", "constant", "(", "np", ".", "eye", "(", "num_seasons", ")", "[", "seasonal_permutation", "]", ",", "dtype", "=", "dtype", ")", "# Optionally transform the transition matrix into a reparameterized space,", "# enforcing the zero-sum constraint for ConstrainedSeasonalStateSpaceModel.", "if", "basis_change_matrix", "is", "not", "None", ":", "seasonal_permutation_matrix", "=", "tf", ".", "matmul", "(", "basis_change_matrix", ",", "tf", ".", "matmul", "(", "seasonal_permutation_matrix", ",", "basis_change_matrix_inv", ")", ")", "identity_matrix", "=", "tf", ".", "eye", "(", "tf", ".", "shape", "(", "input", "=", "seasonal_permutation_matrix", ")", "[", "-", "1", "]", ",", "dtype", "=", "dtype", ")", "def", "seasonal_transition_matrix", "(", "t", ")", ":", "return", "tf", ".", "linalg", ".", "LinearOperatorFullMatrix", "(", "matrix", "=", "dist_util", ".", "pick_scalar_condition", "(", "is_last_day_of_season", "(", "t", ")", ",", "seasonal_permutation_matrix", ",", "identity_matrix", ")", ")", "return", "seasonal_transition_matrix"], "docstring": "Build a function computing transitions for a seasonal effect model.", "docstring_tokens": ["Build", "a", "function", "computing", "transitions", "for", "a", "seasonal", "effect", "model", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/seasonal.py#L573-L604", "partition": "test", "index": 1136, "time": "2019-03-18 17:07:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/seasonal.py", "func_name": "build_seasonal_transition_noise", "original_string": "def build_seasonal_transition_noise(\n    drift_scale, num_seasons, is_last_day_of_season):\n  \"\"\"Build the transition noise model for a SeasonalStateSpaceModel.\"\"\"\n\n  # If the current season has just ended, increase the variance of its effect\n  # following drift_scale. (the just-ended seasonal effect will always be the\n  # bottom element of the vector). Otherwise, do nothing.\n  drift_scale_diag = tf.stack(\n      [tf.zeros_like(drift_scale)] * (num_seasons - 1) + [drift_scale],\n      axis=-1)\n  def seasonal_transition_noise(t):\n    noise_scale_diag = dist_util.pick_scalar_condition(\n        is_last_day_of_season(t),\n        drift_scale_diag,\n        tf.zeros_like(drift_scale_diag))\n    return tfd.MultivariateNormalDiag(\n        loc=tf.zeros(num_seasons, dtype=drift_scale.dtype),\n        scale_diag=noise_scale_diag)\n  return seasonal_transition_noise", "language": "python", "code": "def build_seasonal_transition_noise(\n    drift_scale, num_seasons, is_last_day_of_season):\n  \"\"\"Build the transition noise model for a SeasonalStateSpaceModel.\"\"\"\n\n  # If the current season has just ended, increase the variance of its effect\n  # following drift_scale. (the just-ended seasonal effect will always be the\n  # bottom element of the vector). Otherwise, do nothing.\n  drift_scale_diag = tf.stack(\n      [tf.zeros_like(drift_scale)] * (num_seasons - 1) + [drift_scale],\n      axis=-1)\n  def seasonal_transition_noise(t):\n    noise_scale_diag = dist_util.pick_scalar_condition(\n        is_last_day_of_season(t),\n        drift_scale_diag,\n        tf.zeros_like(drift_scale_diag))\n    return tfd.MultivariateNormalDiag(\n        loc=tf.zeros(num_seasons, dtype=drift_scale.dtype),\n        scale_diag=noise_scale_diag)\n  return seasonal_transition_noise", "code_tokens": ["def", "build_seasonal_transition_noise", "(", "drift_scale", ",", "num_seasons", ",", "is_last_day_of_season", ")", ":", "# If the current season has just ended, increase the variance of its effect", "# following drift_scale. (the just-ended seasonal effect will always be the", "# bottom element of the vector). Otherwise, do nothing.", "drift_scale_diag", "=", "tf", ".", "stack", "(", "[", "tf", ".", "zeros_like", "(", "drift_scale", ")", "]", "*", "(", "num_seasons", "-", "1", ")", "+", "[", "drift_scale", "]", ",", "axis", "=", "-", "1", ")", "def", "seasonal_transition_noise", "(", "t", ")", ":", "noise_scale_diag", "=", "dist_util", ".", "pick_scalar_condition", "(", "is_last_day_of_season", "(", "t", ")", ",", "drift_scale_diag", ",", "tf", ".", "zeros_like", "(", "drift_scale_diag", ")", ")", "return", "tfd", ".", "MultivariateNormalDiag", "(", "loc", "=", "tf", ".", "zeros", "(", "num_seasons", ",", "dtype", "=", "drift_scale", ".", "dtype", ")", ",", "scale_diag", "=", "noise_scale_diag", ")", "return", "seasonal_transition_noise"], "docstring": "Build the transition noise model for a SeasonalStateSpaceModel.", "docstring_tokens": ["Build", "the", "transition", "noise", "model", "for", "a", "SeasonalStateSpaceModel", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/seasonal.py#L607-L625", "partition": "test", "index": 1137, "time": "2019-03-18 17:07:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs_utils.py", "func_name": "converged_any", "original_string": "def converged_any(converged, failed):\n  \"\"\"Condition to stop when any batch member converges, or all have failed.\"\"\"\n  return (tf.reduce_any(input_tensor=converged) |\n          tf.reduce_all(input_tensor=failed))", "language": "python", "code": "def converged_any(converged, failed):\n  \"\"\"Condition to stop when any batch member converges, or all have failed.\"\"\"\n  return (tf.reduce_any(input_tensor=converged) |\n          tf.reduce_all(input_tensor=failed))", "code_tokens": ["def", "converged_any", "(", "converged", ",", "failed", ")", ":", "return", "(", "tf", ".", "reduce_any", "(", "input_tensor", "=", "converged", ")", "|", "tf", ".", "reduce_all", "(", "input_tensor", "=", "failed", ")", ")"], "docstring": "Condition to stop when any batch member converges, or all have failed.", "docstring_tokens": ["Condition", "to", "stop", "when", "any", "batch", "member", "converges", "or", "all", "have", "failed", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs_utils.py#L36-L39", "partition": "test", "index": 978, "time": "2019-03-20 09:50:24"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/util.py", "func_name": "enable_store_parameters_in_results", "original_string": "def enable_store_parameters_in_results(kernel):\n  \"\"\"Enables the `store_parameters_in_results` parameter in a chain of kernels.\n\n  This is a temporary utility for use during the transition period of the\n  parameter storage methods.\n\n  Args:\n    kernel: A TransitionKernel.\n\n  Returns:\n    kernel: The same kernel, but recreated with `store_parameters_in_results`\n        recursively set to `True` in its parameters and its inner kernels (as\n        appropriate).\n  \"\"\"\n  kernel_stack = []\n  while hasattr(kernel, 'parameters') and 'inner_kernel' in kernel.parameters:\n    kernel_stack.append(kernel)\n    kernel = kernel.parameters['inner_kernel']\n\n  def _recreate_kernel(kernel, parameters):\n    new_parameters = kernel.parameters.copy()\n    new_parameters.update(parameters)\n    if 'store_parameters_in_results' in new_parameters:\n      new_parameters['store_parameters_in_results'] = True\n    with deprecation.silence():\n      return type(kernel)(**new_parameters)\n\n  if hasattr(kernel, 'parameters'):\n    kernel = _recreate_kernel(kernel, {})\n\n  for outer_kernel in reversed(kernel_stack):\n    outer_kernel = _recreate_kernel(outer_kernel, {'inner_kernel': kernel})\n    kernel = outer_kernel\n\n  return kernel", "language": "python", "code": "def enable_store_parameters_in_results(kernel):\n  \"\"\"Enables the `store_parameters_in_results` parameter in a chain of kernels.\n\n  This is a temporary utility for use during the transition period of the\n  parameter storage methods.\n\n  Args:\n    kernel: A TransitionKernel.\n\n  Returns:\n    kernel: The same kernel, but recreated with `store_parameters_in_results`\n        recursively set to `True` in its parameters and its inner kernels (as\n        appropriate).\n  \"\"\"\n  kernel_stack = []\n  while hasattr(kernel, 'parameters') and 'inner_kernel' in kernel.parameters:\n    kernel_stack.append(kernel)\n    kernel = kernel.parameters['inner_kernel']\n\n  def _recreate_kernel(kernel, parameters):\n    new_parameters = kernel.parameters.copy()\n    new_parameters.update(parameters)\n    if 'store_parameters_in_results' in new_parameters:\n      new_parameters['store_parameters_in_results'] = True\n    with deprecation.silence():\n      return type(kernel)(**new_parameters)\n\n  if hasattr(kernel, 'parameters'):\n    kernel = _recreate_kernel(kernel, {})\n\n  for outer_kernel in reversed(kernel_stack):\n    outer_kernel = _recreate_kernel(outer_kernel, {'inner_kernel': kernel})\n    kernel = outer_kernel\n\n  return kernel", "code_tokens": ["def", "enable_store_parameters_in_results", "(", "kernel", ")", ":", "kernel_stack", "=", "[", "]", "while", "hasattr", "(", "kernel", ",", "'parameters'", ")", "and", "'inner_kernel'", "in", "kernel", ".", "parameters", ":", "kernel_stack", ".", "append", "(", "kernel", ")", "kernel", "=", "kernel", ".", "parameters", "[", "'inner_kernel'", "]", "def", "_recreate_kernel", "(", "kernel", ",", "parameters", ")", ":", "new_parameters", "=", "kernel", ".", "parameters", ".", "copy", "(", ")", "new_parameters", ".", "update", "(", "parameters", ")", "if", "'store_parameters_in_results'", "in", "new_parameters", ":", "new_parameters", "[", "'store_parameters_in_results'", "]", "=", "True", "with", "deprecation", ".", "silence", "(", ")", ":", "return", "type", "(", "kernel", ")", "(", "*", "*", "new_parameters", ")", "if", "hasattr", "(", "kernel", ",", "'parameters'", ")", ":", "kernel", "=", "_recreate_kernel", "(", "kernel", ",", "{", "}", ")", "for", "outer_kernel", "in", "reversed", "(", "kernel_stack", ")", ":", "outer_kernel", "=", "_recreate_kernel", "(", "outer_kernel", ",", "{", "'inner_kernel'", ":", "kernel", "}", ")", "kernel", "=", "outer_kernel", "return", "kernel"], "docstring": "Enables the `store_parameters_in_results` parameter in a chain of kernels.\n\n  This is a temporary utility for use during the transition period of the\n  parameter storage methods.\n\n  Args:\n    kernel: A TransitionKernel.\n\n  Returns:\n    kernel: The same kernel, but recreated with `store_parameters_in_results`\n        recursively set to `True` in its parameters and its inner kernels (as\n        appropriate).", "docstring_tokens": ["Enables", "the", "store_parameters_in_results", "parameter", "in", "a", "chain", "of", "kernels", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/util.py#L440-L474", "partition": "test", "index": 974, "time": "2019-03-22 09:53:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/util.py", "func_name": "make_innermost_getter", "original_string": "def make_innermost_getter(getter):\n  \"\"\"Wraps a getter so it applies to the inner-most results in `kernel_results`.\n\n  The wrapped getter unwraps `kernel_results` and returns the return value of\n  `getter` called with the first results without an `inner_results` attribute.\n\n  Args:\n    getter: A callable that takes Kernel results and returns some value.\n\n  Returns:\n    new_getter: A wrapped `getter`.\n  \"\"\"\n\n  @functools.wraps(getter)\n  def _new_getter(kernel_results, *args, **kwargs):\n    \"\"\"Wrapped getter.\"\"\"\n    results_stack = []\n    while hasattr(kernel_results, 'inner_results'):\n      results_stack.append(kernel_results)\n      kernel_results = kernel_results.inner_results\n\n    return getter(kernel_results, *args, **kwargs)\n\n  return _new_getter", "language": "python", "code": "def make_innermost_getter(getter):\n  \"\"\"Wraps a getter so it applies to the inner-most results in `kernel_results`.\n\n  The wrapped getter unwraps `kernel_results` and returns the return value of\n  `getter` called with the first results without an `inner_results` attribute.\n\n  Args:\n    getter: A callable that takes Kernel results and returns some value.\n\n  Returns:\n    new_getter: A wrapped `getter`.\n  \"\"\"\n\n  @functools.wraps(getter)\n  def _new_getter(kernel_results, *args, **kwargs):\n    \"\"\"Wrapped getter.\"\"\"\n    results_stack = []\n    while hasattr(kernel_results, 'inner_results'):\n      results_stack.append(kernel_results)\n      kernel_results = kernel_results.inner_results\n\n    return getter(kernel_results, *args, **kwargs)\n\n  return _new_getter", "code_tokens": ["def", "make_innermost_getter", "(", "getter", ")", ":", "@", "functools", ".", "wraps", "(", "getter", ")", "def", "_new_getter", "(", "kernel_results", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "\"\"\"Wrapped getter.\"\"\"", "results_stack", "=", "[", "]", "while", "hasattr", "(", "kernel_results", ",", "'inner_results'", ")", ":", "results_stack", ".", "append", "(", "kernel_results", ")", "kernel_results", "=", "kernel_results", ".", "inner_results", "return", "getter", "(", "kernel_results", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "_new_getter"], "docstring": "Wraps a getter so it applies to the inner-most results in `kernel_results`.\n\n  The wrapped getter unwraps `kernel_results` and returns the return value of\n  `getter` called with the first results without an `inner_results` attribute.\n\n  Args:\n    getter: A callable that takes Kernel results and returns some value.\n\n  Returns:\n    new_getter: A wrapped `getter`.", "docstring_tokens": ["Wraps", "a", "getter", "so", "it", "applies", "to", "the", "inner", "-", "most", "results", "in", "kernel_results", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/util.py#L414-L437", "partition": "test", "index": 973, "time": "2019-03-22 09:53:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/util.py", "func_name": "make_innermost_setter", "original_string": "def make_innermost_setter(setter):\n  \"\"\"Wraps a setter so it applies to the inner-most results in `kernel_results`.\n\n  The wrapped setter unwraps `kernel_results` and applies `setter` to the first\n  results without an `inner_results` attribute.\n\n  Args:\n    setter: A callable that takes the kernel results as well as some `*args` and\n      `**kwargs` and returns a modified copy of those kernel results.\n\n  Returns:\n    new_setter: A wrapped `setter`.\n  \"\"\"\n\n  @functools.wraps(setter)\n  def _new_setter(kernel_results, *args, **kwargs):\n    \"\"\"Wrapped setter.\"\"\"\n    results_stack = []\n    while hasattr(kernel_results, 'inner_results'):\n      results_stack.append(kernel_results)\n      kernel_results = kernel_results.inner_results\n\n    new_kernel_results = setter(kernel_results, *args, **kwargs)\n    for outer_results in reversed(results_stack):\n      new_kernel_results = outer_results._replace(\n          inner_results=new_kernel_results)\n\n    return new_kernel_results\n\n  return _new_setter", "language": "python", "code": "def make_innermost_setter(setter):\n  \"\"\"Wraps a setter so it applies to the inner-most results in `kernel_results`.\n\n  The wrapped setter unwraps `kernel_results` and applies `setter` to the first\n  results without an `inner_results` attribute.\n\n  Args:\n    setter: A callable that takes the kernel results as well as some `*args` and\n      `**kwargs` and returns a modified copy of those kernel results.\n\n  Returns:\n    new_setter: A wrapped `setter`.\n  \"\"\"\n\n  @functools.wraps(setter)\n  def _new_setter(kernel_results, *args, **kwargs):\n    \"\"\"Wrapped setter.\"\"\"\n    results_stack = []\n    while hasattr(kernel_results, 'inner_results'):\n      results_stack.append(kernel_results)\n      kernel_results = kernel_results.inner_results\n\n    new_kernel_results = setter(kernel_results, *args, **kwargs)\n    for outer_results in reversed(results_stack):\n      new_kernel_results = outer_results._replace(\n          inner_results=new_kernel_results)\n\n    return new_kernel_results\n\n  return _new_setter", "code_tokens": ["def", "make_innermost_setter", "(", "setter", ")", ":", "@", "functools", ".", "wraps", "(", "setter", ")", "def", "_new_setter", "(", "kernel_results", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "\"\"\"Wrapped setter.\"\"\"", "results_stack", "=", "[", "]", "while", "hasattr", "(", "kernel_results", ",", "'inner_results'", ")", ":", "results_stack", ".", "append", "(", "kernel_results", ")", "kernel_results", "=", "kernel_results", ".", "inner_results", "new_kernel_results", "=", "setter", "(", "kernel_results", ",", "*", "args", ",", "*", "*", "kwargs", ")", "for", "outer_results", "in", "reversed", "(", "results_stack", ")", ":", "new_kernel_results", "=", "outer_results", ".", "_replace", "(", "inner_results", "=", "new_kernel_results", ")", "return", "new_kernel_results", "return", "_new_setter"], "docstring": "Wraps a setter so it applies to the inner-most results in `kernel_results`.\n\n  The wrapped setter unwraps `kernel_results` and applies `setter` to the first\n  results without an `inner_results` attribute.\n\n  Args:\n    setter: A callable that takes the kernel results as well as some `*args` and\n      `**kwargs` and returns a modified copy of those kernel results.\n\n  Returns:\n    new_setter: A wrapped `setter`.", "docstring_tokens": ["Wraps", "a", "setter", "so", "it", "applies", "to", "the", "inner", "-", "most", "results", "in", "kernel_results", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/util.py#L382-L411", "partition": "test", "index": 972, "time": "2019-03-22 09:53:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/prefer_static.py", "func_name": "_copy_docstring", "original_string": "def _copy_docstring(original_fn, new_fn):\n  \"\"\"Wraps new_fn with the doc of original_fn.\"\"\"\n  original_spec = tf_inspect.getfullargspec(original_fn)\n  new_spec = tf_inspect.getfullargspec(new_fn)\n  if original_spec != new_spec:\n    raise ValueError(\n        'Arg specs do not match: original={}, new={}, fn={}'.format(\n            original_spec, new_spec, original_fn))\n  @decorator.decorator\n  def wrap(wrapped_fn, *args, **kwargs):\n    del wrapped_fn\n    return new_fn(*args, **kwargs)\n  return wrap(original_fn)", "language": "python", "code": "def _copy_docstring(original_fn, new_fn):\n  \"\"\"Wraps new_fn with the doc of original_fn.\"\"\"\n  original_spec = tf_inspect.getfullargspec(original_fn)\n  new_spec = tf_inspect.getfullargspec(new_fn)\n  if original_spec != new_spec:\n    raise ValueError(\n        'Arg specs do not match: original={}, new={}, fn={}'.format(\n            original_spec, new_spec, original_fn))\n  @decorator.decorator\n  def wrap(wrapped_fn, *args, **kwargs):\n    del wrapped_fn\n    return new_fn(*args, **kwargs)\n  return wrap(original_fn)", "code_tokens": ["def", "_copy_docstring", "(", "original_fn", ",", "new_fn", ")", ":", "original_spec", "=", "tf_inspect", ".", "getfullargspec", "(", "original_fn", ")", "new_spec", "=", "tf_inspect", ".", "getfullargspec", "(", "new_fn", ")", "if", "original_spec", "!=", "new_spec", ":", "raise", "ValueError", "(", "'Arg specs do not match: original={}, new={}, fn={}'", ".", "format", "(", "original_spec", ",", "new_spec", ",", "original_fn", ")", ")", "@", "decorator", ".", "decorator", "def", "wrap", "(", "wrapped_fn", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "del", "wrapped_fn", "return", "new_fn", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "wrap", "(", "original_fn", ")"], "docstring": "Wraps new_fn with the doc of original_fn.", "docstring_tokens": ["Wraps", "new_fn", "with", "the", "doc", "of", "original_fn", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/prefer_static.py#L59-L71", "partition": "test", "index": 897, "time": "2019-03-26 09:20:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/prefer_static.py", "func_name": "rank_from_shape", "original_string": "def rank_from_shape(shape_tensor_fn, tensorshape=None):\n  \"\"\"Computes `rank` given a `Tensor`'s `shape`.\"\"\"\n\n  if tensorshape is None:\n    shape_tensor = (shape_tensor_fn() if callable(shape_tensor_fn)\n                    else shape_tensor_fn)\n    if (hasattr(shape_tensor, 'shape') and\n        hasattr(shape_tensor.shape, 'num_elements')):\n      ndims_ = tensorshape_util.num_elements(shape_tensor.shape)\n    else:\n      ndims_ = len(shape_tensor)\n    ndims_fn = lambda: tf.size(input=shape_tensor)\n  else:\n    ndims_ = tensorshape_util.rank(tensorshape)\n    ndims_fn = lambda: tf.size(input=shape_tensor_fn()  # pylint: disable=g-long-lambda\n                               if callable(shape_tensor_fn)\n                               else shape_tensor_fn)\n  return ndims_fn() if ndims_ is None else ndims_", "language": "python", "code": "def rank_from_shape(shape_tensor_fn, tensorshape=None):\n  \"\"\"Computes `rank` given a `Tensor`'s `shape`.\"\"\"\n\n  if tensorshape is None:\n    shape_tensor = (shape_tensor_fn() if callable(shape_tensor_fn)\n                    else shape_tensor_fn)\n    if (hasattr(shape_tensor, 'shape') and\n        hasattr(shape_tensor.shape, 'num_elements')):\n      ndims_ = tensorshape_util.num_elements(shape_tensor.shape)\n    else:\n      ndims_ = len(shape_tensor)\n    ndims_fn = lambda: tf.size(input=shape_tensor)\n  else:\n    ndims_ = tensorshape_util.rank(tensorshape)\n    ndims_fn = lambda: tf.size(input=shape_tensor_fn()  # pylint: disable=g-long-lambda\n                               if callable(shape_tensor_fn)\n                               else shape_tensor_fn)\n  return ndims_fn() if ndims_ is None else ndims_", "code_tokens": ["def", "rank_from_shape", "(", "shape_tensor_fn", ",", "tensorshape", "=", "None", ")", ":", "if", "tensorshape", "is", "None", ":", "shape_tensor", "=", "(", "shape_tensor_fn", "(", ")", "if", "callable", "(", "shape_tensor_fn", ")", "else", "shape_tensor_fn", ")", "if", "(", "hasattr", "(", "shape_tensor", ",", "'shape'", ")", "and", "hasattr", "(", "shape_tensor", ".", "shape", ",", "'num_elements'", ")", ")", ":", "ndims_", "=", "tensorshape_util", ".", "num_elements", "(", "shape_tensor", ".", "shape", ")", "else", ":", "ndims_", "=", "len", "(", "shape_tensor", ")", "ndims_fn", "=", "lambda", ":", "tf", ".", "size", "(", "input", "=", "shape_tensor", ")", "else", ":", "ndims_", "=", "tensorshape_util", ".", "rank", "(", "tensorshape", ")", "ndims_fn", "=", "lambda", ":", "tf", ".", "size", "(", "input", "=", "shape_tensor_fn", "(", ")", "# pylint: disable=g-long-lambda", "if", "callable", "(", "shape_tensor_fn", ")", "else", "shape_tensor_fn", ")", "return", "ndims_fn", "(", ")", "if", "ndims_", "is", "None", "else", "ndims_"], "docstring": "Computes `rank` given a `Tensor`'s `shape`.", "docstring_tokens": ["Computes", "rank", "given", "a", "Tensor", "s", "shape", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/prefer_static.py#L100-L117", "partition": "test", "index": 899, "time": "2019-03-26 09:20:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/prefer_static.py", "func_name": "_prefer_static", "original_string": "def _prefer_static(original_fn, static_fn):\n  \"\"\"Wraps original_fn, preferring to call static_fn when inputs are static.\"\"\"\n  original_spec = tf_inspect.getfullargspec(original_fn)\n  static_spec = tf_inspect.getfullargspec(static_fn)\n  if original_spec != static_spec:\n    raise ValueError(\n        'Arg specs do not match: original={}, static={}, fn={}'.format(\n            original_spec, static_spec, original_fn))\n  @decorator.decorator\n  def wrap(wrapped_fn, *args, **kwargs):\n    del wrapped_fn\n    [args_, kwargs_], all_static = _maybe_get_static_args([args, kwargs])\n    if all_static:\n      return static_fn(*args_, **kwargs_)\n    return original_fn(*args, **kwargs)\n  return wrap(original_fn)", "language": "python", "code": "def _prefer_static(original_fn, static_fn):\n  \"\"\"Wraps original_fn, preferring to call static_fn when inputs are static.\"\"\"\n  original_spec = tf_inspect.getfullargspec(original_fn)\n  static_spec = tf_inspect.getfullargspec(static_fn)\n  if original_spec != static_spec:\n    raise ValueError(\n        'Arg specs do not match: original={}, static={}, fn={}'.format(\n            original_spec, static_spec, original_fn))\n  @decorator.decorator\n  def wrap(wrapped_fn, *args, **kwargs):\n    del wrapped_fn\n    [args_, kwargs_], all_static = _maybe_get_static_args([args, kwargs])\n    if all_static:\n      return static_fn(*args_, **kwargs_)\n    return original_fn(*args, **kwargs)\n  return wrap(original_fn)", "code_tokens": ["def", "_prefer_static", "(", "original_fn", ",", "static_fn", ")", ":", "original_spec", "=", "tf_inspect", ".", "getfullargspec", "(", "original_fn", ")", "static_spec", "=", "tf_inspect", ".", "getfullargspec", "(", "static_fn", ")", "if", "original_spec", "!=", "static_spec", ":", "raise", "ValueError", "(", "'Arg specs do not match: original={}, static={}, fn={}'", ".", "format", "(", "original_spec", ",", "static_spec", ",", "original_fn", ")", ")", "@", "decorator", ".", "decorator", "def", "wrap", "(", "wrapped_fn", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "del", "wrapped_fn", "[", "args_", ",", "kwargs_", "]", ",", "all_static", "=", "_maybe_get_static_args", "(", "[", "args", ",", "kwargs", "]", ")", "if", "all_static", ":", "return", "static_fn", "(", "*", "args_", ",", "*", "*", "kwargs_", ")", "return", "original_fn", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "wrap", "(", "original_fn", ")"], "docstring": "Wraps original_fn, preferring to call static_fn when inputs are static.", "docstring_tokens": ["Wraps", "original_fn", "preferring", "to", "call", "static_fn", "when", "inputs", "are", "static", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/prefer_static.py#L41-L56", "partition": "test", "index": 896, "time": "2019-03-26 09:20:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/distribution_layer.py", "func_name": "_get_convert_to_tensor_fn", "original_string": "def _get_convert_to_tensor_fn(identifier):\n  \"\"\"Return a convert-to-tensor func, given a name, config, callable, etc.\"\"\"\n  if identifier is None:\n    return None\n\n  if isinstance(identifier, six.string_types):\n    identifier = str(identifier)\n    return _deserialize(identifier)\n\n  if isinstance(identifier, dict):\n    return _deserialize(identifier)\n\n  if isinstance(identifier, property):\n    identifier = identifier.fget\n  if callable(identifier):\n    return identifier\n\n  raise ValueError('Could not interpret '\n                   'convert-to-tensor function identifier:', identifier)", "language": "python", "code": "def _get_convert_to_tensor_fn(identifier):\n  \"\"\"Return a convert-to-tensor func, given a name, config, callable, etc.\"\"\"\n  if identifier is None:\n    return None\n\n  if isinstance(identifier, six.string_types):\n    identifier = str(identifier)\n    return _deserialize(identifier)\n\n  if isinstance(identifier, dict):\n    return _deserialize(identifier)\n\n  if isinstance(identifier, property):\n    identifier = identifier.fget\n  if callable(identifier):\n    return identifier\n\n  raise ValueError('Could not interpret '\n                   'convert-to-tensor function identifier:', identifier)", "code_tokens": ["def", "_get_convert_to_tensor_fn", "(", "identifier", ")", ":", "if", "identifier", "is", "None", ":", "return", "None", "if", "isinstance", "(", "identifier", ",", "six", ".", "string_types", ")", ":", "identifier", "=", "str", "(", "identifier", ")", "return", "_deserialize", "(", "identifier", ")", "if", "isinstance", "(", "identifier", ",", "dict", ")", ":", "return", "_deserialize", "(", "identifier", ")", "if", "isinstance", "(", "identifier", ",", "property", ")", ":", "identifier", "=", "identifier", ".", "fget", "if", "callable", "(", "identifier", ")", ":", "return", "identifier", "raise", "ValueError", "(", "'Could not interpret '", "'convert-to-tensor function identifier:'", ",", "identifier", ")"], "docstring": "Return a convert-to-tensor func, given a name, config, callable, etc.", "docstring_tokens": ["Return", "a", "convert", "-", "to", "-", "tensor", "func", "given", "a", "name", "config", "callable", "etc", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/distribution_layer.py#L1912-L1930", "partition": "test", "index": 626, "time": "2019-03-26 17:06:31"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution.py", "func_name": "maybe_check_wont_broadcast", "original_string": "def maybe_check_wont_broadcast(flat_xs, validate_args):\n  \"\"\"Verifies that `parts` don't broadcast.\"\"\"\n  flat_xs = tuple(flat_xs)  # So we can receive generators.\n  if not validate_args:\n    # Note: we don't try static validation because it is theoretically\n    # possible that a user wants to take advantage of broadcasting.\n    # Only when `validate_args` is `True` do we enforce the validation.\n    return flat_xs\n  msg = 'Broadcasting probably indicates an error in model specification.'\n  s = tuple(x.shape for x in flat_xs)\n  if all(tensorshape_util.is_fully_defined(s_) for s_ in s):\n    if not all(a == b for a, b in zip(s[1:], s[:-1])):\n      raise ValueError(msg)\n    return flat_xs\n  assertions = [assert_util.assert_equal(a, b, message=msg)\n                for a, b in zip(s[1:], s[:-1])]\n  with tf.control_dependencies(assertions):\n    return tuple(tf.identity(x) for x in flat_xs)", "language": "python", "code": "def maybe_check_wont_broadcast(flat_xs, validate_args):\n  \"\"\"Verifies that `parts` don't broadcast.\"\"\"\n  flat_xs = tuple(flat_xs)  # So we can receive generators.\n  if not validate_args:\n    # Note: we don't try static validation because it is theoretically\n    # possible that a user wants to take advantage of broadcasting.\n    # Only when `validate_args` is `True` do we enforce the validation.\n    return flat_xs\n  msg = 'Broadcasting probably indicates an error in model specification.'\n  s = tuple(x.shape for x in flat_xs)\n  if all(tensorshape_util.is_fully_defined(s_) for s_ in s):\n    if not all(a == b for a, b in zip(s[1:], s[:-1])):\n      raise ValueError(msg)\n    return flat_xs\n  assertions = [assert_util.assert_equal(a, b, message=msg)\n                for a, b in zip(s[1:], s[:-1])]\n  with tf.control_dependencies(assertions):\n    return tuple(tf.identity(x) for x in flat_xs)", "code_tokens": ["def", "maybe_check_wont_broadcast", "(", "flat_xs", ",", "validate_args", ")", ":", "flat_xs", "=", "tuple", "(", "flat_xs", ")", "# So we can receive generators.", "if", "not", "validate_args", ":", "# Note: we don't try static validation because it is theoretically", "# possible that a user wants to take advantage of broadcasting.", "# Only when `validate_args` is `True` do we enforce the validation.", "return", "flat_xs", "msg", "=", "'Broadcasting probably indicates an error in model specification.'", "s", "=", "tuple", "(", "x", ".", "shape", "for", "x", "in", "flat_xs", ")", "if", "all", "(", "tensorshape_util", ".", "is_fully_defined", "(", "s_", ")", "for", "s_", "in", "s", ")", ":", "if", "not", "all", "(", "a", "==", "b", "for", "a", ",", "b", "in", "zip", "(", "s", "[", "1", ":", "]", ",", "s", "[", ":", "-", "1", "]", ")", ")", ":", "raise", "ValueError", "(", "msg", ")", "return", "flat_xs", "assertions", "=", "[", "assert_util", ".", "assert_equal", "(", "a", ",", "b", ",", "message", "=", "msg", ")", "for", "a", ",", "b", "in", "zip", "(", "s", "[", "1", ":", "]", ",", "s", "[", ":", "-", "1", "]", ")", "]", "with", "tf", ".", "control_dependencies", "(", "assertions", ")", ":", "return", "tuple", "(", "tf", ".", "identity", "(", "x", ")", "for", "x", "in", "flat_xs", ")"], "docstring": "Verifies that `parts` don't broadcast.", "docstring_tokens": ["Verifies", "that", "parts", "don", "t", "broadcast", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution.py#L324-L341", "partition": "test", "index": 822, "time": "2019-03-26 18:43:36"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/sample.py", "func_name": "_make_summary_statistic", "original_string": "def _make_summary_statistic(attr):\n  \"\"\"Factory for implementing summary statistics, eg, mean, stddev, mode.\"\"\"\n  def _fn(self, **kwargs):\n    \"\"\"Implements summary statistic, eg, mean, stddev, mode.\"\"\"\n    x = getattr(self.distribution, attr)(**kwargs)\n    shape = prefer_static.concat([\n        self.distribution.batch_shape_tensor(),\n        prefer_static.ones(prefer_static.rank_from_shape(self.sample_shape),\n                           dtype=self.sample_shape.dtype),\n        self.distribution.event_shape_tensor(),\n    ], axis=0)\n    x = tf.reshape(x, shape=shape)\n    shape = prefer_static.concat([\n        self.distribution.batch_shape_tensor(),\n        self.sample_shape,\n        self.distribution.event_shape_tensor(),\n    ], axis=0)\n    return tf.broadcast_to(x, shape)\n  return _fn", "language": "python", "code": "def _make_summary_statistic(attr):\n  \"\"\"Factory for implementing summary statistics, eg, mean, stddev, mode.\"\"\"\n  def _fn(self, **kwargs):\n    \"\"\"Implements summary statistic, eg, mean, stddev, mode.\"\"\"\n    x = getattr(self.distribution, attr)(**kwargs)\n    shape = prefer_static.concat([\n        self.distribution.batch_shape_tensor(),\n        prefer_static.ones(prefer_static.rank_from_shape(self.sample_shape),\n                           dtype=self.sample_shape.dtype),\n        self.distribution.event_shape_tensor(),\n    ], axis=0)\n    x = tf.reshape(x, shape=shape)\n    shape = prefer_static.concat([\n        self.distribution.batch_shape_tensor(),\n        self.sample_shape,\n        self.distribution.event_shape_tensor(),\n    ], axis=0)\n    return tf.broadcast_to(x, shape)\n  return _fn", "code_tokens": ["def", "_make_summary_statistic", "(", "attr", ")", ":", "def", "_fn", "(", "self", ",", "*", "*", "kwargs", ")", ":", "\"\"\"Implements summary statistic, eg, mean, stddev, mode.\"\"\"", "x", "=", "getattr", "(", "self", ".", "distribution", ",", "attr", ")", "(", "*", "*", "kwargs", ")", "shape", "=", "prefer_static", ".", "concat", "(", "[", "self", ".", "distribution", ".", "batch_shape_tensor", "(", ")", ",", "prefer_static", ".", "ones", "(", "prefer_static", ".", "rank_from_shape", "(", "self", ".", "sample_shape", ")", ",", "dtype", "=", "self", ".", "sample_shape", ".", "dtype", ")", ",", "self", ".", "distribution", ".", "event_shape_tensor", "(", ")", ",", "]", ",", "axis", "=", "0", ")", "x", "=", "tf", ".", "reshape", "(", "x", ",", "shape", "=", "shape", ")", "shape", "=", "prefer_static", ".", "concat", "(", "[", "self", ".", "distribution", ".", "batch_shape_tensor", "(", ")", ",", "self", ".", "sample_shape", ",", "self", ".", "distribution", ".", "event_shape_tensor", "(", ")", ",", "]", ",", "axis", "=", "0", ")", "return", "tf", ".", "broadcast_to", "(", "x", ",", "shape", ")", "return", "_fn"], "docstring": "Factory for implementing summary statistics, eg, mean, stddev, mode.", "docstring_tokens": ["Factory", "for", "implementing", "summary", "statistics", "eg", "mean", "stddev", "mode", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/sample.py#L34-L52", "partition": "test", "index": 778, "time": "2019-03-28 21:16:34"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/internal/missing_values_util.py", "func_name": "moments_of_masked_time_series", "original_string": "def moments_of_masked_time_series(time_series_tensor, broadcast_mask):\n  \"\"\"Compute mean and variance, accounting for a mask.\n\n  Args:\n    time_series_tensor: float `Tensor` time series of shape\n      `concat([batch_shape, [num_timesteps]])`.\n    broadcast_mask: bool `Tensor` of the same shape as `time_series`.\n  Returns:\n    mean: float `Tensor` of shape `batch_shape`.\n    variance: float `Tensor` of shape `batch_shape`.\n  \"\"\"\n  num_unmasked_entries = tf.cast(\n      tf.reduce_sum(input_tensor=tf.cast(~broadcast_mask, tf.int32), axis=-1),\n      time_series_tensor.dtype)\n\n  # Manually compute mean and variance, excluding masked entries.\n  mean = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      time_series_tensor), axis=-1) / num_unmasked_entries)\n  variance = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      (time_series_tensor - mean[..., tf.newaxis]) ** 2), axis=-1)\n              / num_unmasked_entries)\n  return mean, variance", "language": "python", "code": "def moments_of_masked_time_series(time_series_tensor, broadcast_mask):\n  \"\"\"Compute mean and variance, accounting for a mask.\n\n  Args:\n    time_series_tensor: float `Tensor` time series of shape\n      `concat([batch_shape, [num_timesteps]])`.\n    broadcast_mask: bool `Tensor` of the same shape as `time_series`.\n  Returns:\n    mean: float `Tensor` of shape `batch_shape`.\n    variance: float `Tensor` of shape `batch_shape`.\n  \"\"\"\n  num_unmasked_entries = tf.cast(\n      tf.reduce_sum(input_tensor=tf.cast(~broadcast_mask, tf.int32), axis=-1),\n      time_series_tensor.dtype)\n\n  # Manually compute mean and variance, excluding masked entries.\n  mean = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      time_series_tensor), axis=-1) / num_unmasked_entries)\n  variance = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      (time_series_tensor - mean[..., tf.newaxis]) ** 2), axis=-1)\n              / num_unmasked_entries)\n  return mean, variance", "code_tokens": ["def", "moments_of_masked_time_series", "(", "time_series_tensor", ",", "broadcast_mask", ")", ":", "num_unmasked_entries", "=", "tf", ".", "cast", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "cast", "(", "~", "broadcast_mask", ",", "tf", ".", "int32", ")", ",", "axis", "=", "-", "1", ")", ",", "time_series_tensor", ".", "dtype", ")", "# Manually compute mean and variance, excluding masked entries.", "mean", "=", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "where", "(", "broadcast_mask", ",", "tf", ".", "zeros_like", "(", "time_series_tensor", ")", ",", "time_series_tensor", ")", ",", "axis", "=", "-", "1", ")", "/", "num_unmasked_entries", ")", "variance", "=", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "where", "(", "broadcast_mask", ",", "tf", ".", "zeros_like", "(", "time_series_tensor", ")", ",", "(", "time_series_tensor", "-", "mean", "[", "...", ",", "tf", ".", "newaxis", "]", ")", "**", "2", ")", ",", "axis", "=", "-", "1", ")", "/", "num_unmasked_entries", ")", "return", "mean", ",", "variance"], "docstring": "Compute mean and variance, accounting for a mask.\n\n  Args:\n    time_series_tensor: float `Tensor` time series of shape\n      `concat([batch_shape, [num_timesteps]])`.\n    broadcast_mask: bool `Tensor` of the same shape as `time_series`.\n  Returns:\n    mean: float `Tensor` of shape `batch_shape`.\n    variance: float `Tensor` of shape `batch_shape`.", "docstring_tokens": ["Compute", "mean", "and", "variance", "accounting", "for", "a", "mask", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/internal/missing_values_util.py#L109-L134", "partition": "test", "index": 701, "time": "2019-04-02 12:11:33"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/internal/missing_values_util.py", "func_name": "initial_value_of_masked_time_series", "original_string": "def initial_value_of_masked_time_series(time_series_tensor, broadcast_mask):\n  \"\"\"Get the first unmasked entry of each time series in the batch.\n\n  Args:\n    time_series_tensor: float `Tensor` of shape [..., num_timesteps].\n    broadcast_mask: bool `Tensor` of same shape as `time_series`.\n  \"\"\"\n\n  num_timesteps = tf.shape(input=time_series_tensor)[-1]\n\n  # Compute the index of the first unmasked entry for each series in the batch.\n  unmasked_negindices = (\n      tf.cast(~broadcast_mask, tf.int32) *\n      tf.range(num_timesteps, 0, -1))\n  first_unmasked_indices = num_timesteps - tf.reduce_max(\n      input_tensor=unmasked_negindices, axis=-1)\n\n  if first_unmasked_indices.shape.ndims is None:\n    raise NotImplementedError(\n        'Cannot compute initial values of a masked time series with'\n        'dynamic rank.')  # `batch_gather` requires static rank\n\n  # Extract the initial value for each series in the batch.\n  return tf.squeeze(tf.compat.v1.batch_gather(\n      params=time_series_tensor,\n      indices=first_unmasked_indices[..., tf.newaxis]), axis=-1)", "language": "python", "code": "def initial_value_of_masked_time_series(time_series_tensor, broadcast_mask):\n  \"\"\"Get the first unmasked entry of each time series in the batch.\n\n  Args:\n    time_series_tensor: float `Tensor` of shape [..., num_timesteps].\n    broadcast_mask: bool `Tensor` of same shape as `time_series`.\n  \"\"\"\n\n  num_timesteps = tf.shape(input=time_series_tensor)[-1]\n\n  # Compute the index of the first unmasked entry for each series in the batch.\n  unmasked_negindices = (\n      tf.cast(~broadcast_mask, tf.int32) *\n      tf.range(num_timesteps, 0, -1))\n  first_unmasked_indices = num_timesteps - tf.reduce_max(\n      input_tensor=unmasked_negindices, axis=-1)\n\n  if first_unmasked_indices.shape.ndims is None:\n    raise NotImplementedError(\n        'Cannot compute initial values of a masked time series with'\n        'dynamic rank.')  # `batch_gather` requires static rank\n\n  # Extract the initial value for each series in the batch.\n  return tf.squeeze(tf.compat.v1.batch_gather(\n      params=time_series_tensor,\n      indices=first_unmasked_indices[..., tf.newaxis]), axis=-1)", "code_tokens": ["def", "initial_value_of_masked_time_series", "(", "time_series_tensor", ",", "broadcast_mask", ")", ":", "num_timesteps", "=", "tf", ".", "shape", "(", "input", "=", "time_series_tensor", ")", "[", "-", "1", "]", "# Compute the index of the first unmasked entry for each series in the batch.", "unmasked_negindices", "=", "(", "tf", ".", "cast", "(", "~", "broadcast_mask", ",", "tf", ".", "int32", ")", "*", "tf", ".", "range", "(", "num_timesteps", ",", "0", ",", "-", "1", ")", ")", "first_unmasked_indices", "=", "num_timesteps", "-", "tf", ".", "reduce_max", "(", "input_tensor", "=", "unmasked_negindices", ",", "axis", "=", "-", "1", ")", "if", "first_unmasked_indices", ".", "shape", ".", "ndims", "is", "None", ":", "raise", "NotImplementedError", "(", "'Cannot compute initial values of a masked time series with'", "'dynamic rank.'", ")", "# `batch_gather` requires static rank", "# Extract the initial value for each series in the batch.", "return", "tf", ".", "squeeze", "(", "tf", ".", "compat", ".", "v1", ".", "batch_gather", "(", "params", "=", "time_series_tensor", ",", "indices", "=", "first_unmasked_indices", "[", "...", ",", "tf", ".", "newaxis", "]", ")", ",", "axis", "=", "-", "1", ")"], "docstring": "Get the first unmasked entry of each time series in the batch.\n\n  Args:\n    time_series_tensor: float `Tensor` of shape [..., num_timesteps].\n    broadcast_mask: bool `Tensor` of same shape as `time_series`.", "docstring_tokens": ["Get", "the", "first", "unmasked", "entry", "of", "each", "time", "series", "in", "the", "batch", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/internal/missing_values_util.py#L137-L162", "partition": "test", "index": 702, "time": "2019-04-02 12:11:33"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/internal/util.py", "func_name": "canonicalize_observed_time_series_with_mask", "original_string": "def canonicalize_observed_time_series_with_mask(\n    maybe_masked_observed_time_series):\n  \"\"\"Extract a Tensor with canonical shape and optional mask.\n\n  Args:\n    maybe_masked_observed_time_series: a `Tensor`-like object with shape\n      `[..., num_timesteps]` or `[..., num_timesteps, 1]`, or a\n      `tfp.sts.MaskedTimeSeries` containing such an object.\n  Returns:\n    masked_time_series: a `tfp.sts.MaskedTimeSeries` namedtuple, in which\n      the `observed_time_series` is converted to `Tensor` with canonical shape\n      `[..., num_timesteps, 1]`, and `is_missing` is either `None` or a boolean\n      `Tensor`.\n  \"\"\"\n\n  with tf.compat.v1.name_scope('canonicalize_observed_time_series_with_mask'):\n    if hasattr(maybe_masked_observed_time_series, 'is_missing'):\n      observed_time_series = (\n          maybe_masked_observed_time_series.time_series)\n      is_missing = maybe_masked_observed_time_series.is_missing\n    else:\n      observed_time_series = maybe_masked_observed_time_series\n      is_missing = None\n\n    observed_time_series = tf.convert_to_tensor(value=observed_time_series,\n                                                name='observed_time_series')\n    observed_time_series = _maybe_expand_trailing_dim(observed_time_series)\n\n    if is_missing is not None:\n      is_missing = tf.convert_to_tensor(\n          value=is_missing, name='is_missing', dtype_hint=tf.bool)\n\n    return missing_values_util.MaskedTimeSeries(observed_time_series,\n                                                is_missing=is_missing)", "language": "python", "code": "def canonicalize_observed_time_series_with_mask(\n    maybe_masked_observed_time_series):\n  \"\"\"Extract a Tensor with canonical shape and optional mask.\n\n  Args:\n    maybe_masked_observed_time_series: a `Tensor`-like object with shape\n      `[..., num_timesteps]` or `[..., num_timesteps, 1]`, or a\n      `tfp.sts.MaskedTimeSeries` containing such an object.\n  Returns:\n    masked_time_series: a `tfp.sts.MaskedTimeSeries` namedtuple, in which\n      the `observed_time_series` is converted to `Tensor` with canonical shape\n      `[..., num_timesteps, 1]`, and `is_missing` is either `None` or a boolean\n      `Tensor`.\n  \"\"\"\n\n  with tf.compat.v1.name_scope('canonicalize_observed_time_series_with_mask'):\n    if hasattr(maybe_masked_observed_time_series, 'is_missing'):\n      observed_time_series = (\n          maybe_masked_observed_time_series.time_series)\n      is_missing = maybe_masked_observed_time_series.is_missing\n    else:\n      observed_time_series = maybe_masked_observed_time_series\n      is_missing = None\n\n    observed_time_series = tf.convert_to_tensor(value=observed_time_series,\n                                                name='observed_time_series')\n    observed_time_series = _maybe_expand_trailing_dim(observed_time_series)\n\n    if is_missing is not None:\n      is_missing = tf.convert_to_tensor(\n          value=is_missing, name='is_missing', dtype_hint=tf.bool)\n\n    return missing_values_util.MaskedTimeSeries(observed_time_series,\n                                                is_missing=is_missing)", "code_tokens": ["def", "canonicalize_observed_time_series_with_mask", "(", "maybe_masked_observed_time_series", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'canonicalize_observed_time_series_with_mask'", ")", ":", "if", "hasattr", "(", "maybe_masked_observed_time_series", ",", "'is_missing'", ")", ":", "observed_time_series", "=", "(", "maybe_masked_observed_time_series", ".", "time_series", ")", "is_missing", "=", "maybe_masked_observed_time_series", ".", "is_missing", "else", ":", "observed_time_series", "=", "maybe_masked_observed_time_series", "is_missing", "=", "None", "observed_time_series", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "observed_time_series", ",", "name", "=", "'observed_time_series'", ")", "observed_time_series", "=", "_maybe_expand_trailing_dim", "(", "observed_time_series", ")", "if", "is_missing", "is", "not", "None", ":", "is_missing", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "is_missing", ",", "name", "=", "'is_missing'", ",", "dtype_hint", "=", "tf", ".", "bool", ")", "return", "missing_values_util", ".", "MaskedTimeSeries", "(", "observed_time_series", ",", "is_missing", "=", "is_missing", ")"], "docstring": "Extract a Tensor with canonical shape and optional mask.\n\n  Args:\n    maybe_masked_observed_time_series: a `Tensor`-like object with shape\n      `[..., num_timesteps]` or `[..., num_timesteps, 1]`, or a\n      `tfp.sts.MaskedTimeSeries` containing such an object.\n  Returns:\n    masked_time_series: a `tfp.sts.MaskedTimeSeries` namedtuple, in which\n      the `observed_time_series` is converted to `Tensor` with canonical shape\n      `[..., num_timesteps, 1]`, and `is_missing` is either `None` or a boolean\n      `Tensor`.", "docstring_tokens": ["Extract", "a", "Tensor", "with", "canonical", "shape", "and", "optional", "mask", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/internal/util.py#L296-L329", "partition": "test", "index": 708, "time": "2019-04-02 12:11:33"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/masked_autoregressive.py", "func_name": "_create_masks", "original_string": "def _create_masks(degrees):\n  \"\"\"Returns a list of binary mask matrices enforcing autoregressivity.\"\"\"\n  return [\n      # Create input->hidden and hidden->hidden masks.\n      inp[:, np.newaxis] <= out\n      for inp, out in zip(degrees[:-1], degrees[1:])\n  ] + [\n      # Create hidden->output mask.\n      degrees[-1][:, np.newaxis] < degrees[0]\n  ]", "language": "python", "code": "def _create_masks(degrees):\n  \"\"\"Returns a list of binary mask matrices enforcing autoregressivity.\"\"\"\n  return [\n      # Create input->hidden and hidden->hidden masks.\n      inp[:, np.newaxis] <= out\n      for inp, out in zip(degrees[:-1], degrees[1:])\n  ] + [\n      # Create hidden->output mask.\n      degrees[-1][:, np.newaxis] < degrees[0]\n  ]", "code_tokens": ["def", "_create_masks", "(", "degrees", ")", ":", "return", "[", "# Create input->hidden and hidden->hidden masks.", "inp", "[", ":", ",", "np", ".", "newaxis", "]", "<=", "out", "for", "inp", ",", "out", "in", "zip", "(", "degrees", "[", ":", "-", "1", "]", ",", "degrees", "[", "1", ":", "]", ")", "]", "+", "[", "# Create hidden->output mask.", "degrees", "[", "-", "1", "]", "[", ":", ",", "np", ".", "newaxis", "]", "<", "degrees", "[", "0", "]", "]"], "docstring": "Returns a list of binary mask matrices enforcing autoregressivity.", "docstring_tokens": ["Returns", "a", "list", "of", "binary", "mask", "matrices", "enforcing", "autoregressivity", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/masked_autoregressive.py#L957-L966", "partition": "test", "index": 1120, "time": "2019-04-03 16:53:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/masked_autoregressive.py", "func_name": "_make_masked_initializer", "original_string": "def _make_masked_initializer(mask, initializer):\n  \"\"\"Returns a masked version of the given initializer.\"\"\"\n  initializer = tf.keras.initializers.get(initializer)\n  def masked_initializer(shape, dtype=None, partition_info=None):\n    # If no `partition_info` is given, then don't pass it to `initializer`, as\n    # `initializer` may be a `tf.compat.v2.initializers.Initializer` (which\n    # don't accept a `partition_info` argument).\n    if partition_info is None:\n      x = initializer(shape, dtype)\n    else:\n      x = initializer(shape, dtype, partition_info)\n    return tf.cast(mask, x.dtype) * x\n  return masked_initializer", "language": "python", "code": "def _make_masked_initializer(mask, initializer):\n  \"\"\"Returns a masked version of the given initializer.\"\"\"\n  initializer = tf.keras.initializers.get(initializer)\n  def masked_initializer(shape, dtype=None, partition_info=None):\n    # If no `partition_info` is given, then don't pass it to `initializer`, as\n    # `initializer` may be a `tf.compat.v2.initializers.Initializer` (which\n    # don't accept a `partition_info` argument).\n    if partition_info is None:\n      x = initializer(shape, dtype)\n    else:\n      x = initializer(shape, dtype, partition_info)\n    return tf.cast(mask, x.dtype) * x\n  return masked_initializer", "code_tokens": ["def", "_make_masked_initializer", "(", "mask", ",", "initializer", ")", ":", "initializer", "=", "tf", ".", "keras", ".", "initializers", ".", "get", "(", "initializer", ")", "def", "masked_initializer", "(", "shape", ",", "dtype", "=", "None", ",", "partition_info", "=", "None", ")", ":", "# If no `partition_info` is given, then don't pass it to `initializer`, as", "# `initializer` may be a `tf.compat.v2.initializers.Initializer` (which", "# don't accept a `partition_info` argument).", "if", "partition_info", "is", "None", ":", "x", "=", "initializer", "(", "shape", ",", "dtype", ")", "else", ":", "x", "=", "initializer", "(", "shape", ",", "dtype", ",", "partition_info", ")", "return", "tf", ".", "cast", "(", "mask", ",", "x", ".", "dtype", ")", "*", "x", "return", "masked_initializer"], "docstring": "Returns a masked version of the given initializer.", "docstring_tokens": ["Returns", "a", "masked", "version", "of", "the", "given", "initializer", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/masked_autoregressive.py#L969-L981", "partition": "test", "index": 1121, "time": "2019-04-03 16:53:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/masked_autoregressive.py", "func_name": "AutoregressiveLayer.build", "original_string": "def build(self, input_shape):\n    \"\"\"See tfkl.Layer.build.\"\"\"\n    if self._event_shape is None:\n      # `event_shape` wasn't specied at __init__, so infer from `input_shape`.\n      self._event_shape = [tf.compat.dimension_value(input_shape[-1])]\n      self._event_size = self._event_shape[-1]\n      self._event_ndims = len(self._event_shape)\n      # Should we throw if input_shape has rank > 2?\n\n    if input_shape[-1] != self._event_shape[-1]:\n      raise ValueError(\"Invalid final dimension of `input_shape`. \"\n                       \"Expected `{!r}`, but got `{!r}`\".format(\n                           self._event_shape[-1], input_shape[-1]))\n\n    # Construct the masks.\n    self._input_order = _create_input_order(\n        self._event_size, self._input_order_param)\n    self._masks = _create_masks(_create_degrees(\n        input_size=self._event_size,\n        hidden_units=self._hidden_units,\n        input_order=self._input_order,\n        hidden_degrees=self._hidden_degrees))\n\n    # In the final layer, we will produce `self._params` outputs for each of the\n    # `self._event_size` inputs to `AutoregressiveLayer`.  But `masks[-1]` has\n    # shape `[self._hidden_units[-1], self._event_size]`.  Thus, we need to\n    # expand the mask to `[hidden_units[-1], event_size * self._params]` such\n    # that all units for the same input are masked identically.  In particular,\n    # we tile the mask so the j-th element of `tf.unstack(output, axis=-1)` is a\n    # tensor of the j-th parameter/unit for each input.\n    #\n    # NOTE: Other orderings of the output could be faster -- should benchmark.\n    self._masks[-1] = np.reshape(\n        np.tile(self._masks[-1][..., tf.newaxis], [1, 1, self._params]),\n        [self._masks[-1].shape[0], self._event_size * self._params])\n\n    self._network = tf.keras.Sequential([\n        # Starting this model with an `InputLayer` ensures that Keras will build\n        # and propagate our `dtype` to each layer we add.\n        tf.keras.layers.InputLayer((self._event_size,), dtype=self.dtype)\n    ])\n\n    # Input-to-hidden, hidden-to-hidden, and hidden-to-output layers:\n    #  [..., self._event_size] -> [..., self._hidden_units[0]].\n    #  [..., self._hidden_units[k-1]] -> [..., self._hidden_units[k]].\n    #  [..., self._hidden_units[-1]] -> [..., event_size * self._params].\n    layer_output_sizes = self._hidden_units + [self._event_size * self._params]\n    for k in range(len(self._masks)):\n      self._network.add(tf.keras.layers.Dense(\n          layer_output_sizes[k],\n          kernel_initializer=_make_masked_initializer(\n              self._masks[k], self._kernel_initializer),\n          kernel_constraint=_make_masked_constraint(self._masks[k]),\n          activation=self._activation if k + 1 < len(self._masks) else None,\n          use_bias=self._use_bias,\n          **self._kwargs))\n\n    # Record that the layer has been built.\n    super(AutoregressiveLayer, self).build(input_shape)", "language": "python", "code": "def build(self, input_shape):\n    \"\"\"See tfkl.Layer.build.\"\"\"\n    if self._event_shape is None:\n      # `event_shape` wasn't specied at __init__, so infer from `input_shape`.\n      self._event_shape = [tf.compat.dimension_value(input_shape[-1])]\n      self._event_size = self._event_shape[-1]\n      self._event_ndims = len(self._event_shape)\n      # Should we throw if input_shape has rank > 2?\n\n    if input_shape[-1] != self._event_shape[-1]:\n      raise ValueError(\"Invalid final dimension of `input_shape`. \"\n                       \"Expected `{!r}`, but got `{!r}`\".format(\n                           self._event_shape[-1], input_shape[-1]))\n\n    # Construct the masks.\n    self._input_order = _create_input_order(\n        self._event_size, self._input_order_param)\n    self._masks = _create_masks(_create_degrees(\n        input_size=self._event_size,\n        hidden_units=self._hidden_units,\n        input_order=self._input_order,\n        hidden_degrees=self._hidden_degrees))\n\n    # In the final layer, we will produce `self._params` outputs for each of the\n    # `self._event_size` inputs to `AutoregressiveLayer`.  But `masks[-1]` has\n    # shape `[self._hidden_units[-1], self._event_size]`.  Thus, we need to\n    # expand the mask to `[hidden_units[-1], event_size * self._params]` such\n    # that all units for the same input are masked identically.  In particular,\n    # we tile the mask so the j-th element of `tf.unstack(output, axis=-1)` is a\n    # tensor of the j-th parameter/unit for each input.\n    #\n    # NOTE: Other orderings of the output could be faster -- should benchmark.\n    self._masks[-1] = np.reshape(\n        np.tile(self._masks[-1][..., tf.newaxis], [1, 1, self._params]),\n        [self._masks[-1].shape[0], self._event_size * self._params])\n\n    self._network = tf.keras.Sequential([\n        # Starting this model with an `InputLayer` ensures that Keras will build\n        # and propagate our `dtype` to each layer we add.\n        tf.keras.layers.InputLayer((self._event_size,), dtype=self.dtype)\n    ])\n\n    # Input-to-hidden, hidden-to-hidden, and hidden-to-output layers:\n    #  [..., self._event_size] -> [..., self._hidden_units[0]].\n    #  [..., self._hidden_units[k-1]] -> [..., self._hidden_units[k]].\n    #  [..., self._hidden_units[-1]] -> [..., event_size * self._params].\n    layer_output_sizes = self._hidden_units + [self._event_size * self._params]\n    for k in range(len(self._masks)):\n      self._network.add(tf.keras.layers.Dense(\n          layer_output_sizes[k],\n          kernel_initializer=_make_masked_initializer(\n              self._masks[k], self._kernel_initializer),\n          kernel_constraint=_make_masked_constraint(self._masks[k]),\n          activation=self._activation if k + 1 < len(self._masks) else None,\n          use_bias=self._use_bias,\n          **self._kwargs))\n\n    # Record that the layer has been built.\n    super(AutoregressiveLayer, self).build(input_shape)", "code_tokens": ["def", "build", "(", "self", ",", "input_shape", ")", ":", "if", "self", ".", "_event_shape", "is", "None", ":", "# `event_shape` wasn't specied at __init__, so infer from `input_shape`.", "self", ".", "_event_shape", "=", "[", "tf", ".", "compat", ".", "dimension_value", "(", "input_shape", "[", "-", "1", "]", ")", "]", "self", ".", "_event_size", "=", "self", ".", "_event_shape", "[", "-", "1", "]", "self", ".", "_event_ndims", "=", "len", "(", "self", ".", "_event_shape", ")", "# Should we throw if input_shape has rank > 2?", "if", "input_shape", "[", "-", "1", "]", "!=", "self", ".", "_event_shape", "[", "-", "1", "]", ":", "raise", "ValueError", "(", "\"Invalid final dimension of `input_shape`. \"", "\"Expected `{!r}`, but got `{!r}`\"", ".", "format", "(", "self", ".", "_event_shape", "[", "-", "1", "]", ",", "input_shape", "[", "-", "1", "]", ")", ")", "# Construct the masks.", "self", ".", "_input_order", "=", "_create_input_order", "(", "self", ".", "_event_size", ",", "self", ".", "_input_order_param", ")", "self", ".", "_masks", "=", "_create_masks", "(", "_create_degrees", "(", "input_size", "=", "self", ".", "_event_size", ",", "hidden_units", "=", "self", ".", "_hidden_units", ",", "input_order", "=", "self", ".", "_input_order", ",", "hidden_degrees", "=", "self", ".", "_hidden_degrees", ")", ")", "# In the final layer, we will produce `self._params` outputs for each of the", "# `self._event_size` inputs to `AutoregressiveLayer`.  But `masks[-1]` has", "# shape `[self._hidden_units[-1], self._event_size]`.  Thus, we need to", "# expand the mask to `[hidden_units[-1], event_size * self._params]` such", "# that all units for the same input are masked identically.  In particular,", "# we tile the mask so the j-th element of `tf.unstack(output, axis=-1)` is a", "# tensor of the j-th parameter/unit for each input.", "#", "# NOTE: Other orderings of the output could be faster -- should benchmark.", "self", ".", "_masks", "[", "-", "1", "]", "=", "np", ".", "reshape", "(", "np", ".", "tile", "(", "self", ".", "_masks", "[", "-", "1", "]", "[", "...", ",", "tf", ".", "newaxis", "]", ",", "[", "1", ",", "1", ",", "self", ".", "_params", "]", ")", ",", "[", "self", ".", "_masks", "[", "-", "1", "]", ".", "shape", "[", "0", "]", ",", "self", ".", "_event_size", "*", "self", ".", "_params", "]", ")", "self", ".", "_network", "=", "tf", ".", "keras", ".", "Sequential", "(", "[", "# Starting this model with an `InputLayer` ensures that Keras will build", "# and propagate our `dtype` to each layer we add.", "tf", ".", "keras", ".", "layers", ".", "InputLayer", "(", "(", "self", ".", "_event_size", ",", ")", ",", "dtype", "=", "self", ".", "dtype", ")", "]", ")", "# Input-to-hidden, hidden-to-hidden, and hidden-to-output layers:", "#  [..., self._event_size] -> [..., self._hidden_units[0]].", "#  [..., self._hidden_units[k-1]] -> [..., self._hidden_units[k]].", "#  [..., self._hidden_units[-1]] -> [..., event_size * self._params].", "layer_output_sizes", "=", "self", ".", "_hidden_units", "+", "[", "self", ".", "_event_size", "*", "self", ".", "_params", "]", "for", "k", "in", "range", "(", "len", "(", "self", ".", "_masks", ")", ")", ":", "self", ".", "_network", ".", "add", "(", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "layer_output_sizes", "[", "k", "]", ",", "kernel_initializer", "=", "_make_masked_initializer", "(", "self", ".", "_masks", "[", "k", "]", ",", "self", ".", "_kernel_initializer", ")", ",", "kernel_constraint", "=", "_make_masked_constraint", "(", "self", ".", "_masks", "[", "k", "]", ")", ",", "activation", "=", "self", ".", "_activation", "if", "k", "+", "1", "<", "len", "(", "self", ".", "_masks", ")", "else", "None", ",", "use_bias", "=", "self", ".", "_use_bias", ",", "*", "*", "self", ".", "_kwargs", ")", ")", "# Record that the layer has been built.", "super", "(", "AutoregressiveLayer", ",", "self", ")", ".", "build", "(", "input_shape", ")"], "docstring": "See tfkl.Layer.build.", "docstring_tokens": ["See", "tfkl", ".", "Layer", ".", "build", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/masked_autoregressive.py#L794-L852", "partition": "test", "index": 1122, "time": "2019-04-03 16:53:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/masked_autoregressive.py", "func_name": "AutoregressiveLayer.call", "original_string": "def call(self, x):\n    \"\"\"See tfkl.Layer.call.\"\"\"\n    with tf.compat.v2.name_scope(self.name or \"AutoregressiveLayer_call\"):\n      x = tf.convert_to_tensor(value=x, dtype=self.dtype, name=\"x\")\n      input_shape = tf.shape(input=x)\n      # TODO(b/67594795): Better support for dynamic shapes.\n      if tensorshape_util.rank(x.shape) == 1:\n        x = x[tf.newaxis, ...]\n      return tf.reshape(self._network(x),\n                        tf.concat([input_shape, [self._params]], axis=0))", "language": "python", "code": "def call(self, x):\n    \"\"\"See tfkl.Layer.call.\"\"\"\n    with tf.compat.v2.name_scope(self.name or \"AutoregressiveLayer_call\"):\n      x = tf.convert_to_tensor(value=x, dtype=self.dtype, name=\"x\")\n      input_shape = tf.shape(input=x)\n      # TODO(b/67594795): Better support for dynamic shapes.\n      if tensorshape_util.rank(x.shape) == 1:\n        x = x[tf.newaxis, ...]\n      return tf.reshape(self._network(x),\n                        tf.concat([input_shape, [self._params]], axis=0))", "code_tokens": ["def", "call", "(", "self", ",", "x", ")", ":", "with", "tf", ".", "compat", ".", "v2", ".", "name_scope", "(", "self", ".", "name", "or", "\"AutoregressiveLayer_call\"", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "dtype", "=", "self", ".", "dtype", ",", "name", "=", "\"x\"", ")", "input_shape", "=", "tf", ".", "shape", "(", "input", "=", "x", ")", "# TODO(b/67594795): Better support for dynamic shapes.", "if", "tensorshape_util", ".", "rank", "(", "x", ".", "shape", ")", "==", "1", ":", "x", "=", "x", "[", "tf", ".", "newaxis", ",", "...", "]", "return", "tf", ".", "reshape", "(", "self", ".", "_network", "(", "x", ")", ",", "tf", ".", "concat", "(", "[", "input_shape", ",", "[", "self", ".", "_params", "]", "]", ",", "axis", "=", "0", ")", ")"], "docstring": "See tfkl.Layer.call.", "docstring_tokens": ["See", "tfkl", ".", "Layer", ".", "call", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/masked_autoregressive.py#L854-L863", "partition": "test", "index": 1123, "time": "2019-04-03 16:53:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/masked_autoregressive.py", "func_name": "_create_input_order", "original_string": "def _create_input_order(input_size, input_order=\"left-to-right\"):\n  \"\"\"Returns a degree vectors for the input.\"\"\"\n  if isinstance(input_order, six.string_types):\n    if input_order == \"left-to-right\":\n      return np.arange(start=1, stop=input_size + 1)\n    elif input_order == \"right-to-left\":\n      return np.arange(start=input_size, stop=0, step=-1)\n    elif input_order == \"random\":\n      ret = np.arange(start=1, stop=input_size + 1)\n      np.random.shuffle(ret)\n      return ret\n  elif np.all(np.sort(input_order) == np.arange(1, input_size + 1)):\n    return np.array(input_order)\n\n  raise ValueError(\"Invalid input order: '{}'.\".format(input_order))", "language": "python", "code": "def _create_input_order(input_size, input_order=\"left-to-right\"):\n  \"\"\"Returns a degree vectors for the input.\"\"\"\n  if isinstance(input_order, six.string_types):\n    if input_order == \"left-to-right\":\n      return np.arange(start=1, stop=input_size + 1)\n    elif input_order == \"right-to-left\":\n      return np.arange(start=input_size, stop=0, step=-1)\n    elif input_order == \"random\":\n      ret = np.arange(start=1, stop=input_size + 1)\n      np.random.shuffle(ret)\n      return ret\n  elif np.all(np.sort(input_order) == np.arange(1, input_size + 1)):\n    return np.array(input_order)\n\n  raise ValueError(\"Invalid input order: '{}'.\".format(input_order))", "code_tokens": ["def", "_create_input_order", "(", "input_size", ",", "input_order", "=", "\"left-to-right\"", ")", ":", "if", "isinstance", "(", "input_order", ",", "six", ".", "string_types", ")", ":", "if", "input_order", "==", "\"left-to-right\"", ":", "return", "np", ".", "arange", "(", "start", "=", "1", ",", "stop", "=", "input_size", "+", "1", ")", "elif", "input_order", "==", "\"right-to-left\"", ":", "return", "np", ".", "arange", "(", "start", "=", "input_size", ",", "stop", "=", "0", ",", "step", "=", "-", "1", ")", "elif", "input_order", "==", "\"random\"", ":", "ret", "=", "np", ".", "arange", "(", "start", "=", "1", ",", "stop", "=", "input_size", "+", "1", ")", "np", ".", "random", ".", "shuffle", "(", "ret", ")", "return", "ret", "elif", "np", ".", "all", "(", "np", ".", "sort", "(", "input_order", ")", "==", "np", ".", "arange", "(", "1", ",", "input_size", "+", "1", ")", ")", ":", "return", "np", ".", "array", "(", "input_order", ")", "raise", "ValueError", "(", "\"Invalid input order: '{}'.\"", ".", "format", "(", "input_order", ")", ")"], "docstring": "Returns a degree vectors for the input.", "docstring_tokens": ["Returns", "a", "degree", "vectors", "for", "the", "input", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/masked_autoregressive.py#L886-L900", "partition": "test", "index": 1118, "time": "2019-04-03 16:53:32"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/masked_autoregressive.py", "func_name": "_create_degrees", "original_string": "def _create_degrees(input_size,\n                    hidden_units=None,\n                    input_order=\"left-to-right\",\n                    hidden_degrees=\"equal\"):\n  \"\"\"Returns a list of degree vectors, one for each input and hidden layer.\n\n  A unit with degree d can only receive input from units with degree < d. Output\n  units always have the same degree as their associated input unit.\n\n  Args:\n    input_size: Number of inputs.\n    hidden_units: list with the number of hidden units per layer. It does not\n      include the output layer. Each hidden unit size must be at least the size\n      of length (otherwise autoregressivity is not possible).\n    input_order: Order of degrees to the input units: 'random', 'left-to-right',\n      'right-to-left', or an array of an explicit order. For example,\n      'left-to-right' builds an autoregressive model\n      p(x) = p(x1) p(x2 | x1) ... p(xD | x<D).\n    hidden_degrees: Method for assigning degrees to the hidden units:\n      'equal', 'random'.  If 'equal', hidden units in each layer are allocated\n      equally (up to a remainder term) to each degree.  Default: 'equal'.\n\n  Raises:\n    ValueError: invalid input order.\n    ValueError: invalid hidden degrees.\n  \"\"\"\n  input_order = _create_input_order(input_size, input_order)\n  degrees = [input_order]\n\n  if hidden_units is None:\n    hidden_units = []\n\n  for units in hidden_units:\n    if isinstance(hidden_degrees, six.string_types):\n      if hidden_degrees == \"random\":\n        # samples from: [low, high)\n        degrees.append(\n            np.random.randint(low=min(np.min(degrees[-1]), input_size - 1),\n                              high=input_size,\n                              size=units))\n      elif hidden_degrees == \"equal\":\n        min_degree = min(np.min(degrees[-1]), input_size - 1)\n        degrees.append(np.maximum(\n            min_degree,\n            # Evenly divide the range `[1, input_size - 1]` in to `units + 1`\n            # segments, and pick the boundaries between the segments as degrees.\n            np.ceil(np.arange(1, units + 1)\n                    * (input_size - 1) / float(units + 1)).astype(np.int32)))\n    else:\n      raise ValueError('Invalid hidden order: \"{}\".'.format(hidden_degrees))\n\n  return degrees", "language": "python", "code": "def _create_degrees(input_size,\n                    hidden_units=None,\n                    input_order=\"left-to-right\",\n                    hidden_degrees=\"equal\"):\n  \"\"\"Returns a list of degree vectors, one for each input and hidden layer.\n\n  A unit with degree d can only receive input from units with degree < d. Output\n  units always have the same degree as their associated input unit.\n\n  Args:\n    input_size: Number of inputs.\n    hidden_units: list with the number of hidden units per layer. It does not\n      include the output layer. Each hidden unit size must be at least the size\n      of length (otherwise autoregressivity is not possible).\n    input_order: Order of degrees to the input units: 'random', 'left-to-right',\n      'right-to-left', or an array of an explicit order. For example,\n      'left-to-right' builds an autoregressive model\n      p(x) = p(x1) p(x2 | x1) ... p(xD | x<D).\n    hidden_degrees: Method for assigning degrees to the hidden units:\n      'equal', 'random'.  If 'equal', hidden units in each layer are allocated\n      equally (up to a remainder term) to each degree.  Default: 'equal'.\n\n  Raises:\n    ValueError: invalid input order.\n    ValueError: invalid hidden degrees.\n  \"\"\"\n  input_order = _create_input_order(input_size, input_order)\n  degrees = [input_order]\n\n  if hidden_units is None:\n    hidden_units = []\n\n  for units in hidden_units:\n    if isinstance(hidden_degrees, six.string_types):\n      if hidden_degrees == \"random\":\n        # samples from: [low, high)\n        degrees.append(\n            np.random.randint(low=min(np.min(degrees[-1]), input_size - 1),\n                              high=input_size,\n                              size=units))\n      elif hidden_degrees == \"equal\":\n        min_degree = min(np.min(degrees[-1]), input_size - 1)\n        degrees.append(np.maximum(\n            min_degree,\n            # Evenly divide the range `[1, input_size - 1]` in to `units + 1`\n            # segments, and pick the boundaries between the segments as degrees.\n            np.ceil(np.arange(1, units + 1)\n                    * (input_size - 1) / float(units + 1)).astype(np.int32)))\n    else:\n      raise ValueError('Invalid hidden order: \"{}\".'.format(hidden_degrees))\n\n  return degrees", "code_tokens": ["def", "_create_degrees", "(", "input_size", ",", "hidden_units", "=", "None", ",", "input_order", "=", "\"left-to-right\"", ",", "hidden_degrees", "=", "\"equal\"", ")", ":", "input_order", "=", "_create_input_order", "(", "input_size", ",", "input_order", ")", "degrees", "=", "[", "input_order", "]", "if", "hidden_units", "is", "None", ":", "hidden_units", "=", "[", "]", "for", "units", "in", "hidden_units", ":", "if", "isinstance", "(", "hidden_degrees", ",", "six", ".", "string_types", ")", ":", "if", "hidden_degrees", "==", "\"random\"", ":", "# samples from: [low, high)", "degrees", ".", "append", "(", "np", ".", "random", ".", "randint", "(", "low", "=", "min", "(", "np", ".", "min", "(", "degrees", "[", "-", "1", "]", ")", ",", "input_size", "-", "1", ")", ",", "high", "=", "input_size", ",", "size", "=", "units", ")", ")", "elif", "hidden_degrees", "==", "\"equal\"", ":", "min_degree", "=", "min", "(", "np", ".", "min", "(", "degrees", "[", "-", "1", "]", ")", ",", "input_size", "-", "1", ")", "degrees", ".", "append", "(", "np", ".", "maximum", "(", "min_degree", ",", "# Evenly divide the range `[1, input_size - 1]` in to `units + 1`", "# segments, and pick the boundaries between the segments as degrees.", "np", ".", "ceil", "(", "np", ".", "arange", "(", "1", ",", "units", "+", "1", ")", "*", "(", "input_size", "-", "1", ")", "/", "float", "(", "units", "+", "1", ")", ")", ".", "astype", "(", "np", ".", "int32", ")", ")", ")", "else", ":", "raise", "ValueError", "(", "'Invalid hidden order: \"{}\".'", ".", "format", "(", "hidden_degrees", ")", ")", "return", "degrees"], "docstring": "Returns a list of degree vectors, one for each input and hidden layer.\n\n  A unit with degree d can only receive input from units with degree < d. Output\n  units always have the same degree as their associated input unit.\n\n  Args:\n    input_size: Number of inputs.\n    hidden_units: list with the number of hidden units per layer. It does not\n      include the output layer. Each hidden unit size must be at least the size\n      of length (otherwise autoregressivity is not possible).\n    input_order: Order of degrees to the input units: 'random', 'left-to-right',\n      'right-to-left', or an array of an explicit order. For example,\n      'left-to-right' builds an autoregressive model\n      p(x) = p(x1) p(x2 | x1) ... p(xD | x<D).\n    hidden_degrees: Method for assigning degrees to the hidden units:\n      'equal', 'random'.  If 'equal', hidden units in each layer are allocated\n      equally (up to a remainder term) to each degree.  Default: 'equal'.\n\n  Raises:\n    ValueError: invalid input order.\n    ValueError: invalid hidden degrees.", "docstring_tokens": ["Returns", "a", "list", "of", "degree", "vectors", "one", "for", "each", "input", "and", "hidden", "layer", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/masked_autoregressive.py#L903-L954", "partition": "test", "index": 1119, "time": "2019-04-03 16:53:32"}
{"repo": "tensorflow/probability", "path": "experimental/neutra/neutra_kernel.py", "func_name": "NeuTra.one_step", "original_string": "def one_step(self, current_state, previous_kernel_results):\n    \"\"\"Runs one iteration of NeuTra.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s representing the\n        current state(s) of the Markov chain(s). The first `r` dimensions index\n        independent chains, `r = tf.rank(target_log_prob_fn(*current_state))`.\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.\n    \"\"\"\n\n    @tfp.mcmc.internal.util.make_innermost_setter\n    def set_num_leapfrog_steps(kernel_results, num_leapfrog_steps):\n      return kernel_results._replace(\n          accepted_results=kernel_results.accepted_results._replace(\n              num_leapfrog_steps=num_leapfrog_steps))\n\n    step_size = previous_kernel_results.new_step_size\n    previous_kernel_results = set_num_leapfrog_steps(\n        previous_kernel_results, self._num_leapfrog_steps(step_size))\n\n    new_state, kernel_results = self._kernel.one_step(\n        self._flatten_state(current_state), previous_kernel_results)\n    return self._unflatten_state(new_state), kernel_results", "language": "python", "code": "def one_step(self, current_state, previous_kernel_results):\n    \"\"\"Runs one iteration of NeuTra.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s representing the\n        current state(s) of the Markov chain(s). The first `r` dimensions index\n        independent chains, `r = tf.rank(target_log_prob_fn(*current_state))`.\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.\n    \"\"\"\n\n    @tfp.mcmc.internal.util.make_innermost_setter\n    def set_num_leapfrog_steps(kernel_results, num_leapfrog_steps):\n      return kernel_results._replace(\n          accepted_results=kernel_results.accepted_results._replace(\n              num_leapfrog_steps=num_leapfrog_steps))\n\n    step_size = previous_kernel_results.new_step_size\n    previous_kernel_results = set_num_leapfrog_steps(\n        previous_kernel_results, self._num_leapfrog_steps(step_size))\n\n    new_state, kernel_results = self._kernel.one_step(\n        self._flatten_state(current_state), previous_kernel_results)\n    return self._unflatten_state(new_state), kernel_results", "code_tokens": ["def", "one_step", "(", "self", ",", "current_state", ",", "previous_kernel_results", ")", ":", "@", "tfp", ".", "mcmc", ".", "internal", ".", "util", ".", "make_innermost_setter", "def", "set_num_leapfrog_steps", "(", "kernel_results", ",", "num_leapfrog_steps", ")", ":", "return", "kernel_results", ".", "_replace", "(", "accepted_results", "=", "kernel_results", ".", "accepted_results", ".", "_replace", "(", "num_leapfrog_steps", "=", "num_leapfrog_steps", ")", ")", "step_size", "=", "previous_kernel_results", ".", "new_step_size", "previous_kernel_results", "=", "set_num_leapfrog_steps", "(", "previous_kernel_results", ",", "self", ".", "_num_leapfrog_steps", "(", "step_size", ")", ")", "new_state", ",", "kernel_results", "=", "self", ".", "_kernel", ".", "one_step", "(", "self", ".", "_flatten_state", "(", "current_state", ")", ",", "previous_kernel_results", ")", "return", "self", ".", "_unflatten_state", "(", "new_state", ")", ",", "kernel_results"], "docstring": "Runs one iteration of NeuTra.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s representing the\n        current state(s) of the Markov chain(s). The first `r` dimensions index\n        independent chains, `r = tf.rank(target_log_prob_fn(*current_state))`.\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.", "docstring_tokens": ["Runs", "one", "iteration", "of", "NeuTra", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/neutra/neutra_kernel.py#L350-L381", "partition": "test", "index": 754, "time": "2019-04-05 12:12:34"}
{"repo": "tensorflow/probability", "path": "experimental/neutra/neutra_kernel.py", "func_name": "NeuTra.bootstrap_results", "original_string": "def bootstrap_results(self, state):\n    \"\"\"Trains the bijector and creates initial `previous_kernel_results`.\n\n    The supplied `state` is only used to determine the number of chains to run\n    in parallel_iterations\n\n    Args:\n      state: `Tensor` or Python `list` of `Tensor`s representing the initial\n        state(s) of the Markov chain(s). The first `r` dimensions index\n        independent chains, `r = tf.rank(target_log_prob_fn(*state))`.\n\n    Returns:\n      kernel_results: Instance of\n        `UncalibratedHamiltonianMonteCarloKernelResults` inside\n        `MetropolisHastingsResults` inside `TransformedTransitionKernelResults`\n        inside `SimpleStepSizeAdaptationResults`.\n    \"\"\"\n\n    def loss():\n      q = self._flattened_variational_distribution()\n      # TODO(siege): How to seed this?\n      samples = q.sample(self.train_batch_size)\n      return tf.reduce_mean(\n          input_tensor=q.log_prob(samples) -\n          self._flattened_target_log_prob(samples),\n          axis=-1)\n\n    lr = tf.convert_to_tensor(value=self.learning_rate, dtype=self._dtype)\n    dtype = lr.dtype\n\n    learning_rate = tf.compat.v2.optimizers.schedules.PiecewiseConstantDecay(\n        list(self.num_train_steps *\n             np.array([0.2, 0.8]).astype(dtype.as_numpy_dtype())),\n        [lr, lr * 0.1, lr * 0.01])\n\n    opt = tf.compat.v2.optimizers.Adam(learning_rate)\n\n    @tf.function(autograph=False)\n    def train_step():\n      with tf.GradientTape() as tape:\n        loss_val = loss()\n      vals = tape.watched_variables()\n      grads = tape.gradient(loss_val, vals)\n      grads_and_vals = list(zip(grads, vals))\n      opt.apply_gradients(grads_and_vals)\n      return loss_val\n\n    for step in range(self.num_train_steps):\n      loss_val = train_step()\n      tf.debugging.assert_all_finite(\n          loss_val, 'NeuTra loss is NaN at step {}'.format(step))\n      if self.train_debug_fn:\n        # pylint: disable=not-callable\n        self.train_debug_fn(self, step, loss_val)\n\n    state_parts = tf.nest.flatten(state)\n    flat_state_shapes = tf.nest.flatten(self.state_shape)\n    batch_shape = tf.shape(input=state_parts[0])[:-flat_state_shapes[0].ndims]\n\n    return self._kernel.bootstrap_results(\n        self._flattened_variational_distribution().sample(\n            batch_shape, seed=self.seed))", "language": "python", "code": "def bootstrap_results(self, state):\n    \"\"\"Trains the bijector and creates initial `previous_kernel_results`.\n\n    The supplied `state` is only used to determine the number of chains to run\n    in parallel_iterations\n\n    Args:\n      state: `Tensor` or Python `list` of `Tensor`s representing the initial\n        state(s) of the Markov chain(s). The first `r` dimensions index\n        independent chains, `r = tf.rank(target_log_prob_fn(*state))`.\n\n    Returns:\n      kernel_results: Instance of\n        `UncalibratedHamiltonianMonteCarloKernelResults` inside\n        `MetropolisHastingsResults` inside `TransformedTransitionKernelResults`\n        inside `SimpleStepSizeAdaptationResults`.\n    \"\"\"\n\n    def loss():\n      q = self._flattened_variational_distribution()\n      # TODO(siege): How to seed this?\n      samples = q.sample(self.train_batch_size)\n      return tf.reduce_mean(\n          input_tensor=q.log_prob(samples) -\n          self._flattened_target_log_prob(samples),\n          axis=-1)\n\n    lr = tf.convert_to_tensor(value=self.learning_rate, dtype=self._dtype)\n    dtype = lr.dtype\n\n    learning_rate = tf.compat.v2.optimizers.schedules.PiecewiseConstantDecay(\n        list(self.num_train_steps *\n             np.array([0.2, 0.8]).astype(dtype.as_numpy_dtype())),\n        [lr, lr * 0.1, lr * 0.01])\n\n    opt = tf.compat.v2.optimizers.Adam(learning_rate)\n\n    @tf.function(autograph=False)\n    def train_step():\n      with tf.GradientTape() as tape:\n        loss_val = loss()\n      vals = tape.watched_variables()\n      grads = tape.gradient(loss_val, vals)\n      grads_and_vals = list(zip(grads, vals))\n      opt.apply_gradients(grads_and_vals)\n      return loss_val\n\n    for step in range(self.num_train_steps):\n      loss_val = train_step()\n      tf.debugging.assert_all_finite(\n          loss_val, 'NeuTra loss is NaN at step {}'.format(step))\n      if self.train_debug_fn:\n        # pylint: disable=not-callable\n        self.train_debug_fn(self, step, loss_val)\n\n    state_parts = tf.nest.flatten(state)\n    flat_state_shapes = tf.nest.flatten(self.state_shape)\n    batch_shape = tf.shape(input=state_parts[0])[:-flat_state_shapes[0].ndims]\n\n    return self._kernel.bootstrap_results(\n        self._flattened_variational_distribution().sample(\n            batch_shape, seed=self.seed))", "code_tokens": ["def", "bootstrap_results", "(", "self", ",", "state", ")", ":", "def", "loss", "(", ")", ":", "q", "=", "self", ".", "_flattened_variational_distribution", "(", ")", "# TODO(siege): How to seed this?", "samples", "=", "q", ".", "sample", "(", "self", ".", "train_batch_size", ")", "return", "tf", ".", "reduce_mean", "(", "input_tensor", "=", "q", ".", "log_prob", "(", "samples", ")", "-", "self", ".", "_flattened_target_log_prob", "(", "samples", ")", ",", "axis", "=", "-", "1", ")", "lr", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "self", ".", "learning_rate", ",", "dtype", "=", "self", ".", "_dtype", ")", "dtype", "=", "lr", ".", "dtype", "learning_rate", "=", "tf", ".", "compat", ".", "v2", ".", "optimizers", ".", "schedules", ".", "PiecewiseConstantDecay", "(", "list", "(", "self", ".", "num_train_steps", "*", "np", ".", "array", "(", "[", "0.2", ",", "0.8", "]", ")", ".", "astype", "(", "dtype", ".", "as_numpy_dtype", "(", ")", ")", ")", ",", "[", "lr", ",", "lr", "*", "0.1", ",", "lr", "*", "0.01", "]", ")", "opt", "=", "tf", ".", "compat", ".", "v2", ".", "optimizers", ".", "Adam", "(", "learning_rate", ")", "@", "tf", ".", "function", "(", "autograph", "=", "False", ")", "def", "train_step", "(", ")", ":", "with", "tf", ".", "GradientTape", "(", ")", "as", "tape", ":", "loss_val", "=", "loss", "(", ")", "vals", "=", "tape", ".", "watched_variables", "(", ")", "grads", "=", "tape", ".", "gradient", "(", "loss_val", ",", "vals", ")", "grads_and_vals", "=", "list", "(", "zip", "(", "grads", ",", "vals", ")", ")", "opt", ".", "apply_gradients", "(", "grads_and_vals", ")", "return", "loss_val", "for", "step", "in", "range", "(", "self", ".", "num_train_steps", ")", ":", "loss_val", "=", "train_step", "(", ")", "tf", ".", "debugging", ".", "assert_all_finite", "(", "loss_val", ",", "'NeuTra loss is NaN at step {}'", ".", "format", "(", "step", ")", ")", "if", "self", ".", "train_debug_fn", ":", "# pylint: disable=not-callable", "self", ".", "train_debug_fn", "(", "self", ",", "step", ",", "loss_val", ")", "state_parts", "=", "tf", ".", "nest", ".", "flatten", "(", "state", ")", "flat_state_shapes", "=", "tf", ".", "nest", ".", "flatten", "(", "self", ".", "state_shape", ")", "batch_shape", "=", "tf", ".", "shape", "(", "input", "=", "state_parts", "[", "0", "]", ")", "[", ":", "-", "flat_state_shapes", "[", "0", "]", ".", "ndims", "]", "return", "self", ".", "_kernel", ".", "bootstrap_results", "(", "self", ".", "_flattened_variational_distribution", "(", ")", ".", "sample", "(", "batch_shape", ",", "seed", "=", "self", ".", "seed", ")", ")"], "docstring": "Trains the bijector and creates initial `previous_kernel_results`.\n\n    The supplied `state` is only used to determine the number of chains to run\n    in parallel_iterations\n\n    Args:\n      state: `Tensor` or Python `list` of `Tensor`s representing the initial\n        state(s) of the Markov chain(s). The first `r` dimensions index\n        independent chains, `r = tf.rank(target_log_prob_fn(*state))`.\n\n    Returns:\n      kernel_results: Instance of\n        `UncalibratedHamiltonianMonteCarloKernelResults` inside\n        `MetropolisHastingsResults` inside `TransformedTransitionKernelResults`\n        inside `SimpleStepSizeAdaptationResults`.", "docstring_tokens": ["Trains", "the", "bijector", "and", "creates", "initial", "previous_kernel_results", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/neutra/neutra_kernel.py#L383-L444", "partition": "test", "index": 755, "time": "2019-04-05 12:12:34"}
{"repo": "tensorflow/probability", "path": "experimental/neutra/neutra_kernel.py", "func_name": "make_iaf_stack", "original_string": "def make_iaf_stack(total_event_size,\n                   num_hidden_layers=2,\n                   seed=None,\n                   dtype=tf.float32):\n  \"\"\"Creates an stacked IAF bijector.\n\n  This bijector operates on vector-valued events.\n\n  Args:\n    total_event_size: Number of dimensions to operate over.\n    num_hidden_layers: How many hidden layers to use in each IAF.\n    seed: Random seed for the initializers.\n    dtype: DType for the variables.\n\n  Returns:\n    bijector: The created bijector.\n  \"\"\"\n\n  seed = tfd.SeedStream(seed, 'make_iaf_stack')\n\n  def make_iaf():\n    \"\"\"Create an IAF.\"\"\"\n    initializer = tf.compat.v2.keras.initializers.VarianceScaling(\n        2 * 0.01, seed=seed() % (2**31 - 1))\n\n    made = tfb.AutoregressiveLayer(\n        params=2,\n        event_shape=[total_event_size],\n        hidden_units=[total_event_size] * num_hidden_layers,\n        activation=tf.nn.elu,\n        kernel_initializer=initializer,\n        dtype=dtype)\n\n    def shift_and_scale(x):\n      # TODO(siege): Something is losing the static shape.\n      x.set_shape(\n          x.shape.merge_with([None] * (x.shape.ndims - 1) + [total_event_size]))\n      return tf.unstack(made(x), num=2, axis=-1)\n\n    return tfb.Invert(tfb.MaskedAutoregressiveFlow(shift_and_scale))\n\n  def make_swap():\n    \"\"\"Create an swap.\"\"\"\n    permutation = list(reversed(range(total_event_size)))\n    return tfb.Permute(permutation)\n\n  bijector = make_iaf()\n  bijector = make_swap()(bijector)\n  bijector = make_iaf()(bijector)\n  bijector = make_swap()(bijector)\n  bijector = make_iaf()(bijector)\n  bijector = make_swap()(bijector)\n\n  return bijector", "language": "python", "code": "def make_iaf_stack(total_event_size,\n                   num_hidden_layers=2,\n                   seed=None,\n                   dtype=tf.float32):\n  \"\"\"Creates an stacked IAF bijector.\n\n  This bijector operates on vector-valued events.\n\n  Args:\n    total_event_size: Number of dimensions to operate over.\n    num_hidden_layers: How many hidden layers to use in each IAF.\n    seed: Random seed for the initializers.\n    dtype: DType for the variables.\n\n  Returns:\n    bijector: The created bijector.\n  \"\"\"\n\n  seed = tfd.SeedStream(seed, 'make_iaf_stack')\n\n  def make_iaf():\n    \"\"\"Create an IAF.\"\"\"\n    initializer = tf.compat.v2.keras.initializers.VarianceScaling(\n        2 * 0.01, seed=seed() % (2**31 - 1))\n\n    made = tfb.AutoregressiveLayer(\n        params=2,\n        event_shape=[total_event_size],\n        hidden_units=[total_event_size] * num_hidden_layers,\n        activation=tf.nn.elu,\n        kernel_initializer=initializer,\n        dtype=dtype)\n\n    def shift_and_scale(x):\n      # TODO(siege): Something is losing the static shape.\n      x.set_shape(\n          x.shape.merge_with([None] * (x.shape.ndims - 1) + [total_event_size]))\n      return tf.unstack(made(x), num=2, axis=-1)\n\n    return tfb.Invert(tfb.MaskedAutoregressiveFlow(shift_and_scale))\n\n  def make_swap():\n    \"\"\"Create an swap.\"\"\"\n    permutation = list(reversed(range(total_event_size)))\n    return tfb.Permute(permutation)\n\n  bijector = make_iaf()\n  bijector = make_swap()(bijector)\n  bijector = make_iaf()(bijector)\n  bijector = make_swap()(bijector)\n  bijector = make_iaf()(bijector)\n  bijector = make_swap()(bijector)\n\n  return bijector", "code_tokens": ["def", "make_iaf_stack", "(", "total_event_size", ",", "num_hidden_layers", "=", "2", ",", "seed", "=", "None", ",", "dtype", "=", "tf", ".", "float32", ")", ":", "seed", "=", "tfd", ".", "SeedStream", "(", "seed", ",", "'make_iaf_stack'", ")", "def", "make_iaf", "(", ")", ":", "\"\"\"Create an IAF.\"\"\"", "initializer", "=", "tf", ".", "compat", ".", "v2", ".", "keras", ".", "initializers", ".", "VarianceScaling", "(", "2", "*", "0.01", ",", "seed", "=", "seed", "(", ")", "%", "(", "2", "**", "31", "-", "1", ")", ")", "made", "=", "tfb", ".", "AutoregressiveLayer", "(", "params", "=", "2", ",", "event_shape", "=", "[", "total_event_size", "]", ",", "hidden_units", "=", "[", "total_event_size", "]", "*", "num_hidden_layers", ",", "activation", "=", "tf", ".", "nn", ".", "elu", ",", "kernel_initializer", "=", "initializer", ",", "dtype", "=", "dtype", ")", "def", "shift_and_scale", "(", "x", ")", ":", "# TODO(siege): Something is losing the static shape.", "x", ".", "set_shape", "(", "x", ".", "shape", ".", "merge_with", "(", "[", "None", "]", "*", "(", "x", ".", "shape", ".", "ndims", "-", "1", ")", "+", "[", "total_event_size", "]", ")", ")", "return", "tf", ".", "unstack", "(", "made", "(", "x", ")", ",", "num", "=", "2", ",", "axis", "=", "-", "1", ")", "return", "tfb", ".", "Invert", "(", "tfb", ".", "MaskedAutoregressiveFlow", "(", "shift_and_scale", ")", ")", "def", "make_swap", "(", ")", ":", "\"\"\"Create an swap.\"\"\"", "permutation", "=", "list", "(", "reversed", "(", "range", "(", "total_event_size", ")", ")", ")", "return", "tfb", ".", "Permute", "(", "permutation", ")", "bijector", "=", "make_iaf", "(", ")", "bijector", "=", "make_swap", "(", ")", "(", "bijector", ")", "bijector", "=", "make_iaf", "(", ")", "(", "bijector", ")", "bijector", "=", "make_swap", "(", ")", "(", "bijector", ")", "bijector", "=", "make_iaf", "(", ")", "(", "bijector", ")", "bijector", "=", "make_swap", "(", ")", "(", "bijector", ")", "return", "bijector"], "docstring": "Creates an stacked IAF bijector.\n\n  This bijector operates on vector-valued events.\n\n  Args:\n    total_event_size: Number of dimensions to operate over.\n    num_hidden_layers: How many hidden layers to use in each IAF.\n    seed: Random seed for the initializers.\n    dtype: DType for the variables.\n\n  Returns:\n    bijector: The created bijector.", "docstring_tokens": ["Creates", "an", "stacked", "IAF", "bijector", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/neutra/neutra_kernel.py#L33-L86", "partition": "test", "index": 753, "time": "2019-04-05 12:12:34"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/backend/numpy/linalg.py", "func_name": "_matmul", "original_string": "def _matmul(a, b,\n            transpose_a=False, transpose_b=False,\n            adjoint_a=False, adjoint_b=False,\n            a_is_sparse=False, b_is_sparse=False,\n            name=None):  # pylint: disable=unused-argument\n  \"\"\"Numpy matmul wrapper.\"\"\"\n  if a_is_sparse or b_is_sparse:\n    raise NotImplementedError('Numpy backend does not support sparse matmul.')\n  if transpose_a or adjoint_a:\n    a = _matrix_transpose(a, conjugate=adjoint_a)\n  if transpose_b or adjoint_b:\n    b = _matrix_transpose(b, conjugate=adjoint_b)\n  return np.matmul(a, b)", "language": "python", "code": "def _matmul(a, b,\n            transpose_a=False, transpose_b=False,\n            adjoint_a=False, adjoint_b=False,\n            a_is_sparse=False, b_is_sparse=False,\n            name=None):  # pylint: disable=unused-argument\n  \"\"\"Numpy matmul wrapper.\"\"\"\n  if a_is_sparse or b_is_sparse:\n    raise NotImplementedError('Numpy backend does not support sparse matmul.')\n  if transpose_a or adjoint_a:\n    a = _matrix_transpose(a, conjugate=adjoint_a)\n  if transpose_b or adjoint_b:\n    b = _matrix_transpose(b, conjugate=adjoint_b)\n  return np.matmul(a, b)", "code_tokens": ["def", "_matmul", "(", "a", ",", "b", ",", "transpose_a", "=", "False", ",", "transpose_b", "=", "False", ",", "adjoint_a", "=", "False", ",", "adjoint_b", "=", "False", ",", "a_is_sparse", "=", "False", ",", "b_is_sparse", "=", "False", ",", "name", "=", "None", ")", ":", "# pylint: disable=unused-argument", "if", "a_is_sparse", "or", "b_is_sparse", ":", "raise", "NotImplementedError", "(", "'Numpy backend does not support sparse matmul.'", ")", "if", "transpose_a", "or", "adjoint_a", ":", "a", "=", "_matrix_transpose", "(", "a", ",", "conjugate", "=", "adjoint_a", ")", "if", "transpose_b", "or", "adjoint_b", ":", "b", "=", "_matrix_transpose", "(", "b", ",", "conjugate", "=", "adjoint_b", ")", "return", "np", ".", "matmul", "(", "a", ",", "b", ")"], "docstring": "Numpy matmul wrapper.", "docstring_tokens": ["Numpy", "matmul", "wrapper", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/backend/numpy/linalg.py#L97-L109", "partition": "test", "index": 875, "time": "2019-04-05 16:26:49"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/backend/numpy/misc.py", "func_name": "_sort", "original_string": "def _sort(values, axis=-1, direction='ASCENDING', stable=False, name=None):  # pylint: disable=unused-argument\n  \"\"\"Numpy implementation of `tf.sort`.\"\"\"\n  if direction == 'ASCENDING':\n    pass\n  elif direction == 'DESCENDING':\n    values = np.negative(values)\n  else:\n    raise ValueError('Unrecognized direction: {}.'.format(direction))\n  result = np.sort(values, axis, kind='stable' if stable else 'quicksort')\n  if direction == 'DESCENDING':\n    return np.negative(result)\n  return result", "language": "python", "code": "def _sort(values, axis=-1, direction='ASCENDING', stable=False, name=None):  # pylint: disable=unused-argument\n  \"\"\"Numpy implementation of `tf.sort`.\"\"\"\n  if direction == 'ASCENDING':\n    pass\n  elif direction == 'DESCENDING':\n    values = np.negative(values)\n  else:\n    raise ValueError('Unrecognized direction: {}.'.format(direction))\n  result = np.sort(values, axis, kind='stable' if stable else 'quicksort')\n  if direction == 'DESCENDING':\n    return np.negative(result)\n  return result", "code_tokens": ["def", "_sort", "(", "values", ",", "axis", "=", "-", "1", ",", "direction", "=", "'ASCENDING'", ",", "stable", "=", "False", ",", "name", "=", "None", ")", ":", "# pylint: disable=unused-argument", "if", "direction", "==", "'ASCENDING'", ":", "pass", "elif", "direction", "==", "'DESCENDING'", ":", "values", "=", "np", ".", "negative", "(", "values", ")", "else", ":", "raise", "ValueError", "(", "'Unrecognized direction: {}.'", ".", "format", "(", "direction", ")", ")", "result", "=", "np", ".", "sort", "(", "values", ",", "axis", ",", "kind", "=", "'stable'", "if", "stable", "else", "'quicksort'", ")", "if", "direction", "==", "'DESCENDING'", ":", "return", "np", ".", "negative", "(", "result", ")", "return", "result"], "docstring": "Numpy implementation of `tf.sort`.", "docstring_tokens": ["Numpy", "implementation", "of", "tf", ".", "sort", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/backend/numpy/misc.py#L60-L71", "partition": "test", "index": 740, "time": "2019-04-05 16:26:49"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/backend/numpy/misc.py", "func_name": "_argsort", "original_string": "def _argsort(values, axis=-1, direction='ASCENDING', stable=False, name=None):  # pylint: disable=unused-argument\n  \"\"\"Numpy implementation of `tf.argsort`.\"\"\"\n  if direction == 'ASCENDING':\n    pass\n  elif direction == 'DESCENDING':\n    values = np.negative(values)\n  else:\n    raise ValueError('Unrecognized direction: {}.'.format(direction))\n  return np.argsort(values, axis, kind='stable' if stable else 'quicksort')", "language": "python", "code": "def _argsort(values, axis=-1, direction='ASCENDING', stable=False, name=None):  # pylint: disable=unused-argument\n  \"\"\"Numpy implementation of `tf.argsort`.\"\"\"\n  if direction == 'ASCENDING':\n    pass\n  elif direction == 'DESCENDING':\n    values = np.negative(values)\n  else:\n    raise ValueError('Unrecognized direction: {}.'.format(direction))\n  return np.argsort(values, axis, kind='stable' if stable else 'quicksort')", "code_tokens": ["def", "_argsort", "(", "values", ",", "axis", "=", "-", "1", ",", "direction", "=", "'ASCENDING'", ",", "stable", "=", "False", ",", "name", "=", "None", ")", ":", "# pylint: disable=unused-argument", "if", "direction", "==", "'ASCENDING'", ":", "pass", "elif", "direction", "==", "'DESCENDING'", ":", "values", "=", "np", ".", "negative", "(", "values", ")", "else", ":", "raise", "ValueError", "(", "'Unrecognized direction: {}.'", ".", "format", "(", "direction", ")", ")", "return", "np", ".", "argsort", "(", "values", ",", "axis", ",", "kind", "=", "'stable'", "if", "stable", "else", "'quicksort'", ")"], "docstring": "Numpy implementation of `tf.argsort`.", "docstring_tokens": ["Numpy", "implementation", "of", "tf", ".", "argsort", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/backend/numpy/misc.py#L49-L57", "partition": "test", "index": 739, "time": "2019-04-05 16:26:49"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/assert_util.py", "func_name": "assert_rank_at_most", "original_string": "def assert_rank_at_most(x, rank, data=None, summarize=None, message=None,\n                        name=None):\n  \"\"\"Assert `x` has rank equal to `rank` or smaller.\n\n  Example of adding a dependency to an operation:\n\n  ```python\n  with tf.control_dependencies([tf.assert_rank_at_most(x, 2)]):\n    output = tf.reduce_sum(x)\n  ```\n\n  Args:\n    x:  Numeric `Tensor`.\n    rank:  Scalar `Tensor`.\n    data:  The tensors to print out if the condition is False.  Defaults to\n      error message and first few entries of `x`.\n    summarize: Print this many entries of each tensor.\n    message: A string to prefix to the default message.\n    name: A name for this operation (optional).\n      Defaults to \"assert_rank_at_most\".\n\n  Returns:\n    Op raising `InvalidArgumentError` unless `x` has specified rank or lower.\n    If static checks determine `x` has correct rank, a `no_op` is returned.\n\n  Raises:\n    ValueError:  If static checks determine `x` has wrong rank.\n  \"\"\"\n  with tf.compat.v2.name_scope(name or 'assert_rank_at_most'):\n    return tf.compat.v1.assert_less_equal(\n        tf.rank(x), rank, data=data, summarize=summarize, message=message)", "language": "python", "code": "def assert_rank_at_most(x, rank, data=None, summarize=None, message=None,\n                        name=None):\n  \"\"\"Assert `x` has rank equal to `rank` or smaller.\n\n  Example of adding a dependency to an operation:\n\n  ```python\n  with tf.control_dependencies([tf.assert_rank_at_most(x, 2)]):\n    output = tf.reduce_sum(x)\n  ```\n\n  Args:\n    x:  Numeric `Tensor`.\n    rank:  Scalar `Tensor`.\n    data:  The tensors to print out if the condition is False.  Defaults to\n      error message and first few entries of `x`.\n    summarize: Print this many entries of each tensor.\n    message: A string to prefix to the default message.\n    name: A name for this operation (optional).\n      Defaults to \"assert_rank_at_most\".\n\n  Returns:\n    Op raising `InvalidArgumentError` unless `x` has specified rank or lower.\n    If static checks determine `x` has correct rank, a `no_op` is returned.\n\n  Raises:\n    ValueError:  If static checks determine `x` has wrong rank.\n  \"\"\"\n  with tf.compat.v2.name_scope(name or 'assert_rank_at_most'):\n    return tf.compat.v1.assert_less_equal(\n        tf.rank(x), rank, data=data, summarize=summarize, message=message)", "code_tokens": ["def", "assert_rank_at_most", "(", "x", ",", "rank", ",", "data", "=", "None", ",", "summarize", "=", "None", ",", "message", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v2", ".", "name_scope", "(", "name", "or", "'assert_rank_at_most'", ")", ":", "return", "tf", ".", "compat", ".", "v1", ".", "assert_less_equal", "(", "tf", ".", "rank", "(", "x", ")", ",", "rank", ",", "data", "=", "data", ",", "summarize", "=", "summarize", ",", "message", "=", "message", ")"], "docstring": "Assert `x` has rank equal to `rank` or smaller.\n\n  Example of adding a dependency to an operation:\n\n  ```python\n  with tf.control_dependencies([tf.assert_rank_at_most(x, 2)]):\n    output = tf.reduce_sum(x)\n  ```\n\n  Args:\n    x:  Numeric `Tensor`.\n    rank:  Scalar `Tensor`.\n    data:  The tensors to print out if the condition is False.  Defaults to\n      error message and first few entries of `x`.\n    summarize: Print this many entries of each tensor.\n    message: A string to prefix to the default message.\n    name: A name for this operation (optional).\n      Defaults to \"assert_rank_at_most\".\n\n  Returns:\n    Op raising `InvalidArgumentError` unless `x` has specified rank or lower.\n    If static checks determine `x` has correct rank, a `no_op` is returned.\n\n  Raises:\n    ValueError:  If static checks determine `x` has wrong rank.", "docstring_tokens": ["Assert", "x", "has", "rank", "equal", "to", "rank", "or", "smaller", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/assert_util.py#L76-L106", "partition": "test", "index": 623, "time": "2019-04-08 12:05:35"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution_sequential.py", "func_name": "JointDistributionSequential._entropy", "original_string": "def _entropy(self):\n    \"\"\"Shannon entropy in nats.\"\"\"\n    if any(self._dist_fn_args):\n      raise ValueError(\n          'Can only compute entropy when all distributions are independent.')\n    return sum(joint_distribution_lib.maybe_check_wont_broadcast(\n        (d().entropy() for d in self._dist_fn_wrapped),\n        self.validate_args))", "language": "python", "code": "def _entropy(self):\n    \"\"\"Shannon entropy in nats.\"\"\"\n    if any(self._dist_fn_args):\n      raise ValueError(\n          'Can only compute entropy when all distributions are independent.')\n    return sum(joint_distribution_lib.maybe_check_wont_broadcast(\n        (d().entropy() for d in self._dist_fn_wrapped),\n        self.validate_args))", "code_tokens": ["def", "_entropy", "(", "self", ")", ":", "if", "any", "(", "self", ".", "_dist_fn_args", ")", ":", "raise", "ValueError", "(", "'Can only compute entropy when all distributions are independent.'", ")", "return", "sum", "(", "joint_distribution_lib", ".", "maybe_check_wont_broadcast", "(", "(", "d", "(", ")", ".", "entropy", "(", ")", "for", "d", "in", "self", ".", "_dist_fn_wrapped", ")", ",", "self", ".", "validate_args", ")", ")"], "docstring": "Shannon entropy in nats.", "docstring_tokens": ["Shannon", "entropy", "in", "nats", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution_sequential.py#L333-L340", "partition": "test", "index": 718, "time": "2019-04-08 12:42:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution_sequential.py", "func_name": "JointDistributionSequential._resolve_graph", "original_string": "def _resolve_graph(self, distribution_names=None, leaf_name='x'):\n    \"\"\"Creates a `tuple` of `tuple`s of dependencies.\n\n    This function is **experimental**. That said, we encourage its use\n    and ask that you report problems to `tfprobability@tensorflow.org`.\n\n    Args:\n      distribution_names: `list` of `str` or `None` names corresponding to each\n        of `model` elements. (`None`s are expanding into the\n        appropriate `str`.)\n      leaf_name: `str` used when no maker depends on a particular\n        `model` element.\n\n    Returns:\n      graph: `tuple` of `(str tuple)` pairs representing the name of each\n        distribution (maker) and the names of its dependencies.\n\n    #### Example\n\n    ```python\n    d = tfd.JointDistributionSequential([\n                     tfd.Independent(tfd.Exponential(rate=[100, 120]), 1),\n        lambda    e: tfd.Gamma(concentration=e[..., 0], rate=e[..., 1]),\n                     tfd.Normal(loc=0, scale=2.),\n        lambda n, g: tfd.Normal(loc=n, scale=g),\n    ])\n    d._resolve_graph()\n    # ==> (\n    #       ('e', ()),\n    #       ('g', ('e',)),\n    #       ('n', ()),\n    #       ('x', ('n', 'g')),\n    #     )\n    ```\n\n    \"\"\"\n    # This function additionally depends on:\n    #   self._dist_fn_args\n    #   self._dist_fn_wrapped\n    # TODO(b/129008220): Robustify this procedure. Eg, handle collisions better,\n    # ignore args prefixed with `_`.\n    if distribution_names is None or any(self._dist_fn_args):\n      distribution_names = _resolve_distribution_names(\n          self._dist_fn_args, distribution_names, leaf_name)\n    if len(set(distribution_names)) != len(distribution_names):\n      raise ValueError('Distribution names must be unique: {}'.format(\n          distribution_names))\n    if len(distribution_names) != len(self._dist_fn_wrapped):\n      raise ValueError('Distribution names must be 1:1 with `rvs`.')\n    return tuple(zip(distribution_names,\n                     tuple(() if a is None else a for a in self._dist_fn_args)))", "language": "python", "code": "def _resolve_graph(self, distribution_names=None, leaf_name='x'):\n    \"\"\"Creates a `tuple` of `tuple`s of dependencies.\n\n    This function is **experimental**. That said, we encourage its use\n    and ask that you report problems to `tfprobability@tensorflow.org`.\n\n    Args:\n      distribution_names: `list` of `str` or `None` names corresponding to each\n        of `model` elements. (`None`s are expanding into the\n        appropriate `str`.)\n      leaf_name: `str` used when no maker depends on a particular\n        `model` element.\n\n    Returns:\n      graph: `tuple` of `(str tuple)` pairs representing the name of each\n        distribution (maker) and the names of its dependencies.\n\n    #### Example\n\n    ```python\n    d = tfd.JointDistributionSequential([\n                     tfd.Independent(tfd.Exponential(rate=[100, 120]), 1),\n        lambda    e: tfd.Gamma(concentration=e[..., 0], rate=e[..., 1]),\n                     tfd.Normal(loc=0, scale=2.),\n        lambda n, g: tfd.Normal(loc=n, scale=g),\n    ])\n    d._resolve_graph()\n    # ==> (\n    #       ('e', ()),\n    #       ('g', ('e',)),\n    #       ('n', ()),\n    #       ('x', ('n', 'g')),\n    #     )\n    ```\n\n    \"\"\"\n    # This function additionally depends on:\n    #   self._dist_fn_args\n    #   self._dist_fn_wrapped\n    # TODO(b/129008220): Robustify this procedure. Eg, handle collisions better,\n    # ignore args prefixed with `_`.\n    if distribution_names is None or any(self._dist_fn_args):\n      distribution_names = _resolve_distribution_names(\n          self._dist_fn_args, distribution_names, leaf_name)\n    if len(set(distribution_names)) != len(distribution_names):\n      raise ValueError('Distribution names must be unique: {}'.format(\n          distribution_names))\n    if len(distribution_names) != len(self._dist_fn_wrapped):\n      raise ValueError('Distribution names must be 1:1 with `rvs`.')\n    return tuple(zip(distribution_names,\n                     tuple(() if a is None else a for a in self._dist_fn_args)))", "code_tokens": ["def", "_resolve_graph", "(", "self", ",", "distribution_names", "=", "None", ",", "leaf_name", "=", "'x'", ")", ":", "# This function additionally depends on:", "#   self._dist_fn_args", "#   self._dist_fn_wrapped", "# TODO(b/129008220): Robustify this procedure. Eg, handle collisions better,", "# ignore args prefixed with `_`.", "if", "distribution_names", "is", "None", "or", "any", "(", "self", ".", "_dist_fn_args", ")", ":", "distribution_names", "=", "_resolve_distribution_names", "(", "self", ".", "_dist_fn_args", ",", "distribution_names", ",", "leaf_name", ")", "if", "len", "(", "set", "(", "distribution_names", ")", ")", "!=", "len", "(", "distribution_names", ")", ":", "raise", "ValueError", "(", "'Distribution names must be unique: {}'", ".", "format", "(", "distribution_names", ")", ")", "if", "len", "(", "distribution_names", ")", "!=", "len", "(", "self", ".", "_dist_fn_wrapped", ")", ":", "raise", "ValueError", "(", "'Distribution names must be 1:1 with `rvs`.'", ")", "return", "tuple", "(", "zip", "(", "distribution_names", ",", "tuple", "(", "(", ")", "if", "a", "is", "None", "else", "a", "for", "a", "in", "self", ".", "_dist_fn_args", ")", ")", ")"], "docstring": "Creates a `tuple` of `tuple`s of dependencies.\n\n    This function is **experimental**. That said, we encourage its use\n    and ask that you report problems to `tfprobability@tensorflow.org`.\n\n    Args:\n      distribution_names: `list` of `str` or `None` names corresponding to each\n        of `model` elements. (`None`s are expanding into the\n        appropriate `str`.)\n      leaf_name: `str` used when no maker depends on a particular\n        `model` element.\n\n    Returns:\n      graph: `tuple` of `(str tuple)` pairs representing the name of each\n        distribution (maker) and the names of its dependencies.\n\n    #### Example\n\n    ```python\n    d = tfd.JointDistributionSequential([\n                     tfd.Independent(tfd.Exponential(rate=[100, 120]), 1),\n        lambda    e: tfd.Gamma(concentration=e[..., 0], rate=e[..., 1]),\n                     tfd.Normal(loc=0, scale=2.),\n        lambda n, g: tfd.Normal(loc=n, scale=g),\n    ])\n    d._resolve_graph()\n    # ==> (\n    #       ('e', ()),\n    #       ('g', ('e',)),\n    #       ('n', ()),\n    #       ('x', ('n', 'g')),\n    #     )\n    ```", "docstring_tokens": ["Creates", "a", "tuple", "of", "tuple", "s", "of", "dependencies", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution_sequential.py#L276-L326", "partition": "test", "index": 717, "time": "2019-04-08 12:42:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution_sequential.py", "func_name": "_kl_joint_joint", "original_string": "def _kl_joint_joint(d0, d1, name=None):\n  \"\"\"Calculate the KL divergence between two `JointDistributionSequential`s.\n\n  Args:\n    d0: instance of a `JointDistributionSequential` object.\n    d1: instance of a `JointDistributionSequential` object.\n    name: (optional) Name to use for created operations.\n      Default value: `\"kl_joint_joint\"`.\n\n  Returns:\n    kl_joint_joint: `Tensor` The sum of KL divergences between elemental\n      distributions of two joint distributions.\n\n  Raises:\n    ValueError: when joint distributions have a different number of elemental\n      distributions.\n    ValueError: when either joint distribution has a distribution with dynamic\n      dependency, i.e., when either joint distribution is not a collection of\n      independent distributions.\n  \"\"\"\n  if len(d0._dist_fn_wrapped) != len(d1._dist_fn_wrapped):  # pylint: disable=protected-access\n    raise ValueError(\n        'Can only compute KL divergence between when each has the'\n        'same number of component distributions.')\n  if (not all(a is None for a in d0._dist_fn_args) or  # pylint: disable=protected-access\n      not all(a is None for a in d1._dist_fn_args)):  # pylint: disable=protected-access\n    raise ValueError(\n        'Can only compute KL divergence when all distributions are '\n        'independent.')\n  with tf.name_scope(name or 'kl_jointseq_jointseq'):\n    return sum(kullback_leibler.kl_divergence(d0_(), d1_())\n               for d0_, d1_ in zip(d0._dist_fn_wrapped, d1._dist_fn_wrapped))", "language": "python", "code": "def _kl_joint_joint(d0, d1, name=None):\n  \"\"\"Calculate the KL divergence between two `JointDistributionSequential`s.\n\n  Args:\n    d0: instance of a `JointDistributionSequential` object.\n    d1: instance of a `JointDistributionSequential` object.\n    name: (optional) Name to use for created operations.\n      Default value: `\"kl_joint_joint\"`.\n\n  Returns:\n    kl_joint_joint: `Tensor` The sum of KL divergences between elemental\n      distributions of two joint distributions.\n\n  Raises:\n    ValueError: when joint distributions have a different number of elemental\n      distributions.\n    ValueError: when either joint distribution has a distribution with dynamic\n      dependency, i.e., when either joint distribution is not a collection of\n      independent distributions.\n  \"\"\"\n  if len(d0._dist_fn_wrapped) != len(d1._dist_fn_wrapped):  # pylint: disable=protected-access\n    raise ValueError(\n        'Can only compute KL divergence between when each has the'\n        'same number of component distributions.')\n  if (not all(a is None for a in d0._dist_fn_args) or  # pylint: disable=protected-access\n      not all(a is None for a in d1._dist_fn_args)):  # pylint: disable=protected-access\n    raise ValueError(\n        'Can only compute KL divergence when all distributions are '\n        'independent.')\n  with tf.name_scope(name or 'kl_jointseq_jointseq'):\n    return sum(kullback_leibler.kl_divergence(d0_(), d1_())\n               for d0_, d1_ in zip(d0._dist_fn_wrapped, d1._dist_fn_wrapped))", "code_tokens": ["def", "_kl_joint_joint", "(", "d0", ",", "d1", ",", "name", "=", "None", ")", ":", "if", "len", "(", "d0", ".", "_dist_fn_wrapped", ")", "!=", "len", "(", "d1", ".", "_dist_fn_wrapped", ")", ":", "# pylint: disable=protected-access", "raise", "ValueError", "(", "'Can only compute KL divergence between when each has the'", "'same number of component distributions.'", ")", "if", "(", "not", "all", "(", "a", "is", "None", "for", "a", "in", "d0", ".", "_dist_fn_args", ")", "or", "# pylint: disable=protected-access", "not", "all", "(", "a", "is", "None", "for", "a", "in", "d1", ".", "_dist_fn_args", ")", ")", ":", "# pylint: disable=protected-access", "raise", "ValueError", "(", "'Can only compute KL divergence when all distributions are '", "'independent.'", ")", "with", "tf", ".", "name_scope", "(", "name", "or", "'kl_jointseq_jointseq'", ")", ":", "return", "sum", "(", "kullback_leibler", ".", "kl_divergence", "(", "d0_", "(", ")", ",", "d1_", "(", ")", ")", "for", "d0_", ",", "d1_", "in", "zip", "(", "d0", ".", "_dist_fn_wrapped", ",", "d1", ".", "_dist_fn_wrapped", ")", ")"], "docstring": "Calculate the KL divergence between two `JointDistributionSequential`s.\n\n  Args:\n    d0: instance of a `JointDistributionSequential` object.\n    d1: instance of a `JointDistributionSequential` object.\n    name: (optional) Name to use for created operations.\n      Default value: `\"kl_joint_joint\"`.\n\n  Returns:\n    kl_joint_joint: `Tensor` The sum of KL divergences between elemental\n      distributions of two joint distributions.\n\n  Raises:\n    ValueError: when joint distributions have a different number of elemental\n      distributions.\n    ValueError: when either joint distribution has a distribution with dynamic\n      dependency, i.e., when either joint distribution is not a collection of\n      independent distributions.", "docstring_tokens": ["Calculate", "the", "KL", "divergence", "between", "two", "JointDistributionSequential", "s", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution_sequential.py#L492-L523", "partition": "test", "index": 715, "time": "2019-04-08 12:42:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution_sequential.py", "func_name": "_get_required_args", "original_string": "def _get_required_args(fn):\n  \"\"\"Returns the distribution's required args.\"\"\"\n  argspec = tf_inspect.getfullargspec(fn)\n  args = argspec.args\n  if tf_inspect.isclass(fn):\n    args = args[1:]  # Remove the `self` arg.\n  if argspec.defaults:\n    # Remove the args which have defaults. By convention we only feed\n    # *required args*. This means some distributions must always be wrapped\n    # with a `lambda`, e.g., `lambda logits: tfd.Bernoulli(logits=logits)`\n    # or `lambda probs: tfd.Bernoulli(probs=probs)`.\n    args = args[:-len(argspec.defaults)]\n  return tuple(args)", "language": "python", "code": "def _get_required_args(fn):\n  \"\"\"Returns the distribution's required args.\"\"\"\n  argspec = tf_inspect.getfullargspec(fn)\n  args = argspec.args\n  if tf_inspect.isclass(fn):\n    args = args[1:]  # Remove the `self` arg.\n  if argspec.defaults:\n    # Remove the args which have defaults. By convention we only feed\n    # *required args*. This means some distributions must always be wrapped\n    # with a `lambda`, e.g., `lambda logits: tfd.Bernoulli(logits=logits)`\n    # or `lambda probs: tfd.Bernoulli(probs=probs)`.\n    args = args[:-len(argspec.defaults)]\n  return tuple(args)", "code_tokens": ["def", "_get_required_args", "(", "fn", ")", ":", "argspec", "=", "tf_inspect", ".", "getfullargspec", "(", "fn", ")", "args", "=", "argspec", ".", "args", "if", "tf_inspect", ".", "isclass", "(", "fn", ")", ":", "args", "=", "args", "[", "1", ":", "]", "# Remove the `self` arg.", "if", "argspec", ".", "defaults", ":", "# Remove the args which have defaults. By convention we only feed", "# *required args*. This means some distributions must always be wrapped", "# with a `lambda`, e.g., `lambda logits: tfd.Bernoulli(logits=logits)`", "# or `lambda probs: tfd.Bernoulli(probs=probs)`.", "args", "=", "args", "[", ":", "-", "len", "(", "argspec", ".", "defaults", ")", "]", "return", "tuple", "(", "args", ")"], "docstring": "Returns the distribution's required args.", "docstring_tokens": ["Returns", "the", "distribution", "s", "required", "args", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution_sequential.py#L475-L487", "partition": "test", "index": 714, "time": "2019-04-08 12:42:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution_sequential.py", "func_name": "_resolve_distribution_names", "original_string": "def _resolve_distribution_names(dist_fn_args, dist_names, leaf_name):\n  \"\"\"Uses arg names to resolve distribution names.\"\"\"\n  if dist_names is None:\n    dist_names = []\n  else:\n    dist_names = dist_names.copy()\n  n = len(dist_fn_args)\n  dist_names.extend([None]*(n - len(dist_names)))\n  for i_, args in enumerate(reversed(dist_fn_args)):\n    if not args:\n      continue  # There's no args to analyze.\n    i = n - i_ - 1\n    for j, arg_name in enumerate(args):\n      dist_names[i - j - 1] = arg_name\n  j = 0\n  for i_ in range(len(dist_names)):\n    i = n - i_ - 1\n    if dist_names[i] is None:\n      dist_names[i] = leaf_name if j == 0 else leaf_name + str(j)\n      j += 1\n  return tuple(dist_names)", "language": "python", "code": "def _resolve_distribution_names(dist_fn_args, dist_names, leaf_name):\n  \"\"\"Uses arg names to resolve distribution names.\"\"\"\n  if dist_names is None:\n    dist_names = []\n  else:\n    dist_names = dist_names.copy()\n  n = len(dist_fn_args)\n  dist_names.extend([None]*(n - len(dist_names)))\n  for i_, args in enumerate(reversed(dist_fn_args)):\n    if not args:\n      continue  # There's no args to analyze.\n    i = n - i_ - 1\n    for j, arg_name in enumerate(args):\n      dist_names[i - j - 1] = arg_name\n  j = 0\n  for i_ in range(len(dist_names)):\n    i = n - i_ - 1\n    if dist_names[i] is None:\n      dist_names[i] = leaf_name if j == 0 else leaf_name + str(j)\n      j += 1\n  return tuple(dist_names)", "code_tokens": ["def", "_resolve_distribution_names", "(", "dist_fn_args", ",", "dist_names", ",", "leaf_name", ")", ":", "if", "dist_names", "is", "None", ":", "dist_names", "=", "[", "]", "else", ":", "dist_names", "=", "dist_names", ".", "copy", "(", ")", "n", "=", "len", "(", "dist_fn_args", ")", "dist_names", ".", "extend", "(", "[", "None", "]", "*", "(", "n", "-", "len", "(", "dist_names", ")", ")", ")", "for", "i_", ",", "args", "in", "enumerate", "(", "reversed", "(", "dist_fn_args", ")", ")", ":", "if", "not", "args", ":", "continue", "# There's no args to analyze.", "i", "=", "n", "-", "i_", "-", "1", "for", "j", ",", "arg_name", "in", "enumerate", "(", "args", ")", ":", "dist_names", "[", "i", "-", "j", "-", "1", "]", "=", "arg_name", "j", "=", "0", "for", "i_", "in", "range", "(", "len", "(", "dist_names", ")", ")", ":", "i", "=", "n", "-", "i_", "-", "1", "if", "dist_names", "[", "i", "]", "is", "None", ":", "dist_names", "[", "i", "]", "=", "leaf_name", "if", "j", "==", "0", "else", "leaf_name", "+", "str", "(", "j", ")", "j", "+=", "1", "return", "tuple", "(", "dist_names", ")"], "docstring": "Uses arg names to resolve distribution names.", "docstring_tokens": ["Uses", "arg", "names", "to", "resolve", "distribution", "names", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution_sequential.py#L452-L472", "partition": "test", "index": 713, "time": "2019-04-08 12:42:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution_sequential.py", "func_name": "_make_summary_statistic", "original_string": "def _make_summary_statistic(attr):\n  \"\"\"Factory for making summary statistics, eg, mean, mode, stddev.\"\"\"\n  def _fn(self):\n    if any(self._dist_fn_args):  # pylint: disable=protected-access\n      raise ValueError(\n          'Can only compute ' + attr + ' when all distributions are '\n          'independent; {}'.format(self.model))\n    return self._unflatten(getattr(d(), attr)() for d in self._dist_fn_wrapped)  # pylint: disable=protected-access\n  return _fn", "language": "python", "code": "def _make_summary_statistic(attr):\n  \"\"\"Factory for making summary statistics, eg, mean, mode, stddev.\"\"\"\n  def _fn(self):\n    if any(self._dist_fn_args):  # pylint: disable=protected-access\n      raise ValueError(\n          'Can only compute ' + attr + ' when all distributions are '\n          'independent; {}'.format(self.model))\n    return self._unflatten(getattr(d(), attr)() for d in self._dist_fn_wrapped)  # pylint: disable=protected-access\n  return _fn", "code_tokens": ["def", "_make_summary_statistic", "(", "attr", ")", ":", "def", "_fn", "(", "self", ")", ":", "if", "any", "(", "self", ".", "_dist_fn_args", ")", ":", "# pylint: disable=protected-access", "raise", "ValueError", "(", "'Can only compute '", "+", "attr", "+", "' when all distributions are '", "'independent; {}'", ".", "format", "(", "self", ".", "model", ")", ")", "return", "self", ".", "_unflatten", "(", "getattr", "(", "d", "(", ")", ",", "attr", ")", "(", ")", "for", "d", "in", "self", ".", "_dist_fn_wrapped", ")", "# pylint: disable=protected-access", "return", "_fn"], "docstring": "Factory for making summary statistics, eg, mean, mode, stddev.", "docstring_tokens": ["Factory", "for", "making", "summary", "statistics", "eg", "mean", "mode", "stddev", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution_sequential.py#L38-L46", "partition": "test", "index": 711, "time": "2019-04-08 12:42:08"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution_sequential.py", "func_name": "_unify_call_signature", "original_string": "def _unify_call_signature(i, dist_fn):\n  \"\"\"Creates `dist_fn_wrapped` which calls `dist_fn` with all prev nodes.\n\n  Args:\n    i: Python `int` corresponding to position in topologically sorted DAG.\n    dist_fn: Python `callable` which takes a subset of previously constructed\n      distributions (in reverse order) and produces a new distribution instance.\n\n  Returns:\n    dist_fn_wrapped: Python `callable` which takes all previous distributions\n      (in non reverse order) and produces a  new distribution instance.\n    args: `tuple` of `str` representing the arg names of `dist_fn` (and in non\n      wrapped, \"natural\" order). `None` is returned only if the input is not a\n      `callable`.\n  \"\"\"\n  if distribution_util.is_distribution_instance(dist_fn):\n    return (lambda *_: dist_fn), None\n\n  if not callable(dist_fn):\n    raise TypeError('{} must be either `tfd.Distribution`-like or '\n                    '`callable`.'.format(dist_fn))\n\n  args = _get_required_args(dist_fn)\n  if not args:\n    return (lambda *_: dist_fn()), ()\n\n  @functools.wraps(dist_fn)\n  def dist_fn_wrapped(*xs):\n    \"\"\"Calls `dist_fn` with reversed and truncated args.\"\"\"\n    if i != len(xs):\n      raise ValueError(\n          'Internal Error: Unexpected number of inputs provided to {}-th '\n          'distribution maker (dist_fn: {}, expected: {}, saw: {}).'.format(\n              i, dist_fn, i, len(xs)))\n    if len(xs) < len(args):\n      raise ValueError(\n          'Internal Error: Too few inputs provided to {}-th distribution maker '\n          '(dist_fn: {}, expected: {}, saw: {}).'.format(\n              i, dist_fn, len(args), len(xs)))\n    return dist_fn(*reversed(xs[-len(args):]))\n  return dist_fn_wrapped, args", "language": "python", "code": "def _unify_call_signature(i, dist_fn):\n  \"\"\"Creates `dist_fn_wrapped` which calls `dist_fn` with all prev nodes.\n\n  Args:\n    i: Python `int` corresponding to position in topologically sorted DAG.\n    dist_fn: Python `callable` which takes a subset of previously constructed\n      distributions (in reverse order) and produces a new distribution instance.\n\n  Returns:\n    dist_fn_wrapped: Python `callable` which takes all previous distributions\n      (in non reverse order) and produces a  new distribution instance.\n    args: `tuple` of `str` representing the arg names of `dist_fn` (and in non\n      wrapped, \"natural\" order). `None` is returned only if the input is not a\n      `callable`.\n  \"\"\"\n  if distribution_util.is_distribution_instance(dist_fn):\n    return (lambda *_: dist_fn), None\n\n  if not callable(dist_fn):\n    raise TypeError('{} must be either `tfd.Distribution`-like or '\n                    '`callable`.'.format(dist_fn))\n\n  args = _get_required_args(dist_fn)\n  if not args:\n    return (lambda *_: dist_fn()), ()\n\n  @functools.wraps(dist_fn)\n  def dist_fn_wrapped(*xs):\n    \"\"\"Calls `dist_fn` with reversed and truncated args.\"\"\"\n    if i != len(xs):\n      raise ValueError(\n          'Internal Error: Unexpected number of inputs provided to {}-th '\n          'distribution maker (dist_fn: {}, expected: {}, saw: {}).'.format(\n              i, dist_fn, i, len(xs)))\n    if len(xs) < len(args):\n      raise ValueError(\n          'Internal Error: Too few inputs provided to {}-th distribution maker '\n          '(dist_fn: {}, expected: {}, saw: {}).'.format(\n              i, dist_fn, len(args), len(xs)))\n    return dist_fn(*reversed(xs[-len(args):]))\n  return dist_fn_wrapped, args", "code_tokens": ["def", "_unify_call_signature", "(", "i", ",", "dist_fn", ")", ":", "if", "distribution_util", ".", "is_distribution_instance", "(", "dist_fn", ")", ":", "return", "(", "lambda", "*", "_", ":", "dist_fn", ")", ",", "None", "if", "not", "callable", "(", "dist_fn", ")", ":", "raise", "TypeError", "(", "'{} must be either `tfd.Distribution`-like or '", "'`callable`.'", ".", "format", "(", "dist_fn", ")", ")", "args", "=", "_get_required_args", "(", "dist_fn", ")", "if", "not", "args", ":", "return", "(", "lambda", "*", "_", ":", "dist_fn", "(", ")", ")", ",", "(", ")", "@", "functools", ".", "wraps", "(", "dist_fn", ")", "def", "dist_fn_wrapped", "(", "*", "xs", ")", ":", "\"\"\"Calls `dist_fn` with reversed and truncated args.\"\"\"", "if", "i", "!=", "len", "(", "xs", ")", ":", "raise", "ValueError", "(", "'Internal Error: Unexpected number of inputs provided to {}-th '", "'distribution maker (dist_fn: {}, expected: {}, saw: {}).'", ".", "format", "(", "i", ",", "dist_fn", ",", "i", ",", "len", "(", "xs", ")", ")", ")", "if", "len", "(", "xs", ")", "<", "len", "(", "args", ")", ":", "raise", "ValueError", "(", "'Internal Error: Too few inputs provided to {}-th distribution maker '", "'(dist_fn: {}, expected: {}, saw: {}).'", ".", "format", "(", "i", ",", "dist_fn", ",", "len", "(", "args", ")", ",", "len", "(", "xs", ")", ")", ")", "return", "dist_fn", "(", "*", "reversed", "(", "xs", "[", "-", "len", "(", "args", ")", ":", "]", ")", ")", "return", "dist_fn_wrapped", ",", "args"], "docstring": "Creates `dist_fn_wrapped` which calls `dist_fn` with all prev nodes.\n\n  Args:\n    i: Python `int` corresponding to position in topologically sorted DAG.\n    dist_fn: Python `callable` which takes a subset of previously constructed\n      distributions (in reverse order) and produces a new distribution instance.\n\n  Returns:\n    dist_fn_wrapped: Python `callable` which takes all previous distributions\n      (in non reverse order) and produces a  new distribution instance.\n    args: `tuple` of `str` representing the arg names of `dist_fn` (and in non\n      wrapped, \"natural\" order). `None` is returned only if the input is not a\n      `callable`.", "docstring_tokens": ["Creates", "dist_fn_wrapped", "which", "calls", "dist_fn", "with", "all", "prev", "nodes", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution_sequential.py#L409-L449", "partition": "test", "index": 712, "time": "2019-04-08 12:42:08"}
{"repo": "tensorflow/probability", "path": "experimental/fun_mcmc/fun_mcmc_lib.py", "func_name": "trace", "original_string": "def trace(state: State, fn: TransitionOperator, num_steps: IntTensor,\n          trace_fn: Callable[[State, TensorNest], TensorNest]\n         ) -> Tuple[State, TensorNest]:\n  \"\"\"`TransitionOperator` that runs `fn` repeatedly and traces its outputs.\n\n  Args:\n    state: A nest of `Tensor`s or None.\n    fn: A `TransitionOperator`.\n    num_steps: Number of steps to run the function for. Must be greater than 1.\n    trace_fn: Callable that the unpacked outputs of `fn` and returns a nest of\n      `Tensor`s. These will be stacked and returned.\n\n  Returns:\n    state: The final state returned by `fn`.\n    traces: Stacked outputs of `trace_fn`.\n  \"\"\"\n\n  def fn_wrapper(args, _):\n    return tf.nest.map_structure(tf.convert_to_tensor, call_fn(fn, args[0]))\n\n  def trace_fn_wrapper(args):\n    return tf.nest.map_structure(tf.convert_to_tensor, call_fn(trace_fn, args))\n\n  state = call_fn(fn, state)\n  first_trace = trace_fn_wrapper(state)\n\n  state, full_trace = mcmc_util.trace_scan(\n      fn_wrapper, state, tf.ones(num_steps - 1), trace_fn=trace_fn_wrapper)\n\n  prepend = lambda x, y: tf.concat(  # pylint: disable=g-long-lambda\n      [tf.convert_to_tensor(value=x)[tf.newaxis], y], 0)\n\n  return state, tf.nest.map_structure(prepend, first_trace, full_trace)", "language": "python", "code": "def trace(state: State, fn: TransitionOperator, num_steps: IntTensor,\n          trace_fn: Callable[[State, TensorNest], TensorNest]\n         ) -> Tuple[State, TensorNest]:\n  \"\"\"`TransitionOperator` that runs `fn` repeatedly and traces its outputs.\n\n  Args:\n    state: A nest of `Tensor`s or None.\n    fn: A `TransitionOperator`.\n    num_steps: Number of steps to run the function for. Must be greater than 1.\n    trace_fn: Callable that the unpacked outputs of `fn` and returns a nest of\n      `Tensor`s. These will be stacked and returned.\n\n  Returns:\n    state: The final state returned by `fn`.\n    traces: Stacked outputs of `trace_fn`.\n  \"\"\"\n\n  def fn_wrapper(args, _):\n    return tf.nest.map_structure(tf.convert_to_tensor, call_fn(fn, args[0]))\n\n  def trace_fn_wrapper(args):\n    return tf.nest.map_structure(tf.convert_to_tensor, call_fn(trace_fn, args))\n\n  state = call_fn(fn, state)\n  first_trace = trace_fn_wrapper(state)\n\n  state, full_trace = mcmc_util.trace_scan(\n      fn_wrapper, state, tf.ones(num_steps - 1), trace_fn=trace_fn_wrapper)\n\n  prepend = lambda x, y: tf.concat(  # pylint: disable=g-long-lambda\n      [tf.convert_to_tensor(value=x)[tf.newaxis], y], 0)\n\n  return state, tf.nest.map_structure(prepend, first_trace, full_trace)", "code_tokens": ["def", "trace", "(", "state", ":", "State", ",", "fn", ":", "TransitionOperator", ",", "num_steps", ":", "IntTensor", ",", "trace_fn", ":", "Callable", "[", "[", "State", ",", "TensorNest", "]", ",", "TensorNest", "]", ")", "->", "Tuple", "[", "State", ",", "TensorNest", "]", ":", "def", "fn_wrapper", "(", "args", ",", "_", ")", ":", "return", "tf", ".", "nest", ".", "map_structure", "(", "tf", ".", "convert_to_tensor", ",", "call_fn", "(", "fn", ",", "args", "[", "0", "]", ")", ")", "def", "trace_fn_wrapper", "(", "args", ")", ":", "return", "tf", ".", "nest", ".", "map_structure", "(", "tf", ".", "convert_to_tensor", ",", "call_fn", "(", "trace_fn", ",", "args", ")", ")", "state", "=", "call_fn", "(", "fn", ",", "state", ")", "first_trace", "=", "trace_fn_wrapper", "(", "state", ")", "state", ",", "full_trace", "=", "mcmc_util", ".", "trace_scan", "(", "fn_wrapper", ",", "state", ",", "tf", ".", "ones", "(", "num_steps", "-", "1", ")", ",", "trace_fn", "=", "trace_fn_wrapper", ")", "prepend", "=", "lambda", "x", ",", "y", ":", "tf", ".", "concat", "(", "# pylint: disable=g-long-lambda", "[", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ")", "[", "tf", ".", "newaxis", "]", ",", "y", "]", ",", "0", ")", "return", "state", ",", "tf", ".", "nest", ".", "map_structure", "(", "prepend", ",", "first_trace", ",", "full_trace", ")"], "docstring": "`TransitionOperator` that runs `fn` repeatedly and traces its outputs.\n\n  Args:\n    state: A nest of `Tensor`s or None.\n    fn: A `TransitionOperator`.\n    num_steps: Number of steps to run the function for. Must be greater than 1.\n    trace_fn: Callable that the unpacked outputs of `fn` and returns a nest of\n      `Tensor`s. These will be stacked and returned.\n\n  Returns:\n    state: The final state returned by `fn`.\n    traces: Stacked outputs of `trace_fn`.", "docstring_tokens": ["TransitionOperator", "that", "runs", "fn", "repeatedly", "and", "traces", "its", "outputs", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/fun_mcmc/fun_mcmc_lib.py#L87-L119", "partition": "test", "index": 635, "time": "2019-04-08 15:37:18"}
{"repo": "tensorflow/probability", "path": "experimental/fun_mcmc/fun_mcmc_lib.py", "func_name": "maybe_broadcast_structure", "original_string": "def maybe_broadcast_structure(from_structure: Any, to_structure: Any) -> Any:\n  \"\"\"Maybe broadcasts `from_structure` to `to_structure`.\n\n  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    from_structure: A structure.\n    to_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.\n  \"\"\"\n  flat_from = tf.nest.flatten(from_structure)\n  flat_to = tf.nest.flatten(to_structure)\n  if len(flat_from) == 1:\n    flat_from *= len(flat_to)\n  return tf.nest.pack_sequence_as(to_structure, flat_from)", "language": "python", "code": "def maybe_broadcast_structure(from_structure: Any, to_structure: Any) -> Any:\n  \"\"\"Maybe broadcasts `from_structure` to `to_structure`.\n\n  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    from_structure: A structure.\n    to_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.\n  \"\"\"\n  flat_from = tf.nest.flatten(from_structure)\n  flat_to = tf.nest.flatten(to_structure)\n  if len(flat_from) == 1:\n    flat_from *= len(flat_to)\n  return tf.nest.pack_sequence_as(to_structure, flat_from)", "code_tokens": ["def", "maybe_broadcast_structure", "(", "from_structure", ":", "Any", ",", "to_structure", ":", "Any", ")", "->", "Any", ":", "flat_from", "=", "tf", ".", "nest", ".", "flatten", "(", "from_structure", ")", "flat_to", "=", "tf", ".", "nest", ".", "flatten", "(", "to_structure", ")", "if", "len", "(", "flat_from", ")", "==", "1", ":", "flat_from", "*=", "len", "(", "flat_to", ")", "return", "tf", ".", "nest", ".", "pack_sequence_as", "(", "to_structure", ",", "flat_from", ")"], "docstring": "Maybe broadcasts `from_structure` to `to_structure`.\n\n  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    from_structure: A structure.\n    to_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.", "docstring_tokens": ["Maybe", "broadcasts", "from_structure", "to", "to_structure", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/fun_mcmc/fun_mcmc_lib.py#L160-L178", "partition": "test", "index": 638, "time": "2019-04-08 15:37:18"}
{"repo": "tensorflow/probability", "path": "experimental/fun_mcmc/fun_mcmc_lib.py", "func_name": "sign_adaptation", "original_string": "def sign_adaptation(control: FloatNest,\n                    output: FloatTensor,\n                    set_point: FloatTensor,\n                    adaptation_rate: FloatTensor = 0.01) -> FloatNest:\n  \"\"\"A function to do simple sign-based control of a variable.\n\n  ```\n  control = control * (1. + adaptation_rate) ** sign(output - set_point)\n  ```\n\n  Args:\n    control: The control variable.\n    output: The output variable.\n    set_point: The set point for `output`. This function will adjust `control`\n      so that `output` matches `set_point`.\n    adaptation_rate: Adaptation rate.\n\n  Returns:\n    control: New control.\n  \"\"\"\n\n  def _get_new_control(control, output, set_point):\n    new_control = mcmc_util.choose(output > set_point,\n                                   control * (1. + adaptation_rate),\n                                   control / (1. + adaptation_rate))\n    return new_control\n\n  output = maybe_broadcast_structure(output, control)\n  set_point = maybe_broadcast_structure(set_point, control)\n\n  return tf.nest.map_structure(_get_new_control, control, output, set_point)", "language": "python", "code": "def sign_adaptation(control: FloatNest,\n                    output: FloatTensor,\n                    set_point: FloatTensor,\n                    adaptation_rate: FloatTensor = 0.01) -> FloatNest:\n  \"\"\"A function to do simple sign-based control of a variable.\n\n  ```\n  control = control * (1. + adaptation_rate) ** sign(output - set_point)\n  ```\n\n  Args:\n    control: The control variable.\n    output: The output variable.\n    set_point: The set point for `output`. This function will adjust `control`\n      so that `output` matches `set_point`.\n    adaptation_rate: Adaptation rate.\n\n  Returns:\n    control: New control.\n  \"\"\"\n\n  def _get_new_control(control, output, set_point):\n    new_control = mcmc_util.choose(output > set_point,\n                                   control * (1. + adaptation_rate),\n                                   control / (1. + adaptation_rate))\n    return new_control\n\n  output = maybe_broadcast_structure(output, control)\n  set_point = maybe_broadcast_structure(set_point, control)\n\n  return tf.nest.map_structure(_get_new_control, control, output, set_point)", "code_tokens": ["def", "sign_adaptation", "(", "control", ":", "FloatNest", ",", "output", ":", "FloatTensor", ",", "set_point", ":", "FloatTensor", ",", "adaptation_rate", ":", "FloatTensor", "=", "0.01", ")", "->", "FloatNest", ":", "def", "_get_new_control", "(", "control", ",", "output", ",", "set_point", ")", ":", "new_control", "=", "mcmc_util", ".", "choose", "(", "output", ">", "set_point", ",", "control", "*", "(", "1.", "+", "adaptation_rate", ")", ",", "control", "/", "(", "1.", "+", "adaptation_rate", ")", ")", "return", "new_control", "output", "=", "maybe_broadcast_structure", "(", "output", ",", "control", ")", "set_point", "=", "maybe_broadcast_structure", "(", "set_point", ",", "control", ")", "return", "tf", ".", "nest", ".", "map_structure", "(", "_get_new_control", ",", "control", ",", "output", ",", "set_point", ")"], "docstring": "A function to do simple sign-based control of a variable.\n\n  ```\n  control = control * (1. + adaptation_rate) ** sign(output - set_point)\n  ```\n\n  Args:\n    control: The control variable.\n    output: The output variable.\n    set_point: The set point for `output`. This function will adjust `control`\n      so that `output` matches `set_point`.\n    adaptation_rate: Adaptation rate.\n\n  Returns:\n    control: New control.", "docstring_tokens": ["A", "function", "to", "do", "simple", "sign", "-", "based", "control", "of", "a", "variable", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/fun_mcmc/fun_mcmc_lib.py#L520-L550", "partition": "test", "index": 643, "time": "2019-04-08 15:37:18"}
{"repo": "tensorflow/probability", "path": "experimental/fun_mcmc/fun_mcmc_lib.py", "func_name": "hamiltonian_monte_carlo", "original_string": "def hamiltonian_monte_carlo(\n    hmc_state: HamiltonianMonteCarloState,\n    target_log_prob_fn: PotentialFn,\n    step_size: Any,\n    num_leapfrog_steps: IntTensor,\n    momentum: State = None,\n    kinetic_energy_fn: PotentialFn = None,\n    momentum_sample_fn: MomentumSampleFn = None,\n    leapfrog_trace_fn: Callable[[LeapFrogStepState, LeapFrogStepExtras],\n                                TensorNest] = lambda *args: (),\n    seed=None,\n) -> Tuple[HamiltonianMonteCarloState, HamiltonianMonteCarloExtra]:\n  \"\"\"Hamiltonian Monte Carlo `TransitionOperator`.\n\n  #### Example\n\n  ```python\n  step_size = 0.2\n  num_steps = 2000\n  num_leapfrog_steps = 10\n  state = tf.ones([16, 2])\n\n  base_mean = [1., 0]\n  base_cov = [[1, 0.5], [0.5, 1]]\n\n  bijector = tfb.Softplus()\n  base_dist = tfd.MultivariateNormalFullCovariance(\n      loc=base_mean, covariance_matrix=base_cov)\n  target_dist = bijector(base_dist)\n\n  def orig_target_log_prob_fn(x):\n    return target_dist.log_prob(x), ()\n\n  target_log_prob_fn, state = fun_mcmc.transform_log_prob_fn(\n      orig_target_log_prob_fn, bijector, state)\n\n  kernel = tf.function(lambda state: fun_mcmc.hamiltonian_monte_carlo(\n      state,\n      step_size=step_size,\n      num_leapfrog_steps=num_leapfrog_steps,\n      target_log_prob_fn=target_log_prob_fn,\n      seed=tfp_test_util.test_seed()))\n\n  _, chain = fun_mcmc.trace(\n      state=fun_mcmc.HamiltonianMonteCarloState(\n          state=state,\n          state_grads=None,\n          target_log_prob=None,\n          state_extra=None),\n      fn=kernel,\n      num_steps=num_steps,\n      trace_fn=lambda state, extra: state.state_extra[0])\n  ```\n\n  Args:\n    hmc_state: HamiltonianMonteCarloState.\n    target_log_prob_fn: Target log prob fn.\n    step_size: Step size, structure broadcastable to the `target_log_prob_fn`\n      state.\n    num_leapfrog_steps: Number of leapfrog steps to take.\n    momentum: Initial momentum, passed to `momentum_sample_fn`. Default: zeroes.\n    kinetic_energy_fn: Kinetic energy function.\n    momentum_sample_fn: Sampler for the momentum.\n    leapfrog_trace_fn: Trace function for the leapfrog integrator.\n    seed: For reproducibility.\n\n  Returns:\n    hmc_state: HamiltonianMonteCarloState\n    hmc_extra: HamiltonianMonteCarloExtra\n  \"\"\"\n  state = hmc_state.state\n  state_grads = hmc_state.state_grads\n  target_log_prob = hmc_state.target_log_prob\n  state_extra = hmc_state.state_extra\n\n  if kinetic_energy_fn is None:\n\n    # pylint: disable=function-redefined\n    def kinetic_energy_fn(*momentum):\n      return tf.add_n([\n          tf.reduce_sum(input_tensor=tf.square(x), axis=-1) / 2.\n          for x in tf.nest.flatten(momentum)\n      ]), ()\n\n  if momentum_sample_fn is None:\n\n    # pylint: disable=function-redefined\n    def momentum_sample_fn(*momentum):\n      ret = tf.nest.map_structure(\n          lambda x: tf.random.normal(tf.shape(input=x), dtype=x.dtype),\n          momentum)\n      if len(ret) == 1:\n        return ret[0]\n      else:\n        return ret\n\n  if momentum is None:\n    momentum = call_fn(momentum_sample_fn,\n                       tf.nest.map_structure(tf.zeros_like, state))\n  if target_log_prob is None:\n    target_log_prob, state_extra, state_grads = call_and_grads(\n        target_log_prob_fn, state)\n\n  kinetic_energy, _ = call_fn(kinetic_energy_fn, momentum)\n  current_energy = -target_log_prob + kinetic_energy\n  current_state = HamiltonianMonteCarloState(\n      state=state,\n      state_grads=state_grads,\n      state_extra=state_extra,\n      target_log_prob=target_log_prob)\n\n  def leapfrog_wrapper(leapfrog_state, target_log_prob, state_extra):\n    \"\"\"Leapfrog wrapper that tracks extra state.\"\"\"\n    del target_log_prob\n    del state_extra\n\n    leapfrog_state, leapfrog_extra = leapfrog_step(\n        leapfrog_state,\n        step_size=step_size,\n        target_log_prob_fn=target_log_prob_fn,\n        kinetic_energy_fn=kinetic_energy_fn)\n\n    return [\n        leapfrog_state, leapfrog_extra.target_log_prob,\n        leapfrog_extra.state_extra\n    ], leapfrog_extra\n\n  def leapfrog_trace_wrapper_fn(args, leapfrog_extra):\n    return leapfrog_trace_fn(args[0], leapfrog_extra)\n\n  leapfrog_wrapper_state = (LeapFrogStepState(state, state_grads, momentum),\n                            target_log_prob, state_extra)\n\n  [[leapfrog_state, target_log_prob, state_extra], _], leapfrog_trace = trace(\n      leapfrog_wrapper_state,\n      leapfrog_wrapper,\n      num_leapfrog_steps,\n      trace_fn=leapfrog_trace_wrapper_fn)\n\n  kinetic_energy, _ = call_fn(kinetic_energy_fn, leapfrog_state.momentum)\n  proposed_energy = -target_log_prob + kinetic_energy\n  proposed_state = HamiltonianMonteCarloState(\n      state=leapfrog_state.state,\n      state_grads=leapfrog_state.state_grads,\n      target_log_prob=target_log_prob,\n      state_extra=state_extra)\n\n  energy_change = proposed_energy - current_energy\n  hmc_state, is_accepted, _ = metropolis_hastings_step(\n      current_state, proposed_state, energy_change, seed=seed)\n\n  hmc_state = hmc_state  # type: HamiltonianMonteCarloState\n  return hmc_state, HamiltonianMonteCarloExtra(\n      is_accepted=is_accepted,\n      proposed_hmc_state=proposed_state,\n      log_accept_ratio=-energy_change,\n      leapfrog_trace=leapfrog_trace)", "language": "python", "code": "def hamiltonian_monte_carlo(\n    hmc_state: HamiltonianMonteCarloState,\n    target_log_prob_fn: PotentialFn,\n    step_size: Any,\n    num_leapfrog_steps: IntTensor,\n    momentum: State = None,\n    kinetic_energy_fn: PotentialFn = None,\n    momentum_sample_fn: MomentumSampleFn = None,\n    leapfrog_trace_fn: Callable[[LeapFrogStepState, LeapFrogStepExtras],\n                                TensorNest] = lambda *args: (),\n    seed=None,\n) -> Tuple[HamiltonianMonteCarloState, HamiltonianMonteCarloExtra]:\n  \"\"\"Hamiltonian Monte Carlo `TransitionOperator`.\n\n  #### Example\n\n  ```python\n  step_size = 0.2\n  num_steps = 2000\n  num_leapfrog_steps = 10\n  state = tf.ones([16, 2])\n\n  base_mean = [1., 0]\n  base_cov = [[1, 0.5], [0.5, 1]]\n\n  bijector = tfb.Softplus()\n  base_dist = tfd.MultivariateNormalFullCovariance(\n      loc=base_mean, covariance_matrix=base_cov)\n  target_dist = bijector(base_dist)\n\n  def orig_target_log_prob_fn(x):\n    return target_dist.log_prob(x), ()\n\n  target_log_prob_fn, state = fun_mcmc.transform_log_prob_fn(\n      orig_target_log_prob_fn, bijector, state)\n\n  kernel = tf.function(lambda state: fun_mcmc.hamiltonian_monte_carlo(\n      state,\n      step_size=step_size,\n      num_leapfrog_steps=num_leapfrog_steps,\n      target_log_prob_fn=target_log_prob_fn,\n      seed=tfp_test_util.test_seed()))\n\n  _, chain = fun_mcmc.trace(\n      state=fun_mcmc.HamiltonianMonteCarloState(\n          state=state,\n          state_grads=None,\n          target_log_prob=None,\n          state_extra=None),\n      fn=kernel,\n      num_steps=num_steps,\n      trace_fn=lambda state, extra: state.state_extra[0])\n  ```\n\n  Args:\n    hmc_state: HamiltonianMonteCarloState.\n    target_log_prob_fn: Target log prob fn.\n    step_size: Step size, structure broadcastable to the `target_log_prob_fn`\n      state.\n    num_leapfrog_steps: Number of leapfrog steps to take.\n    momentum: Initial momentum, passed to `momentum_sample_fn`. Default: zeroes.\n    kinetic_energy_fn: Kinetic energy function.\n    momentum_sample_fn: Sampler for the momentum.\n    leapfrog_trace_fn: Trace function for the leapfrog integrator.\n    seed: For reproducibility.\n\n  Returns:\n    hmc_state: HamiltonianMonteCarloState\n    hmc_extra: HamiltonianMonteCarloExtra\n  \"\"\"\n  state = hmc_state.state\n  state_grads = hmc_state.state_grads\n  target_log_prob = hmc_state.target_log_prob\n  state_extra = hmc_state.state_extra\n\n  if kinetic_energy_fn is None:\n\n    # pylint: disable=function-redefined\n    def kinetic_energy_fn(*momentum):\n      return tf.add_n([\n          tf.reduce_sum(input_tensor=tf.square(x), axis=-1) / 2.\n          for x in tf.nest.flatten(momentum)\n      ]), ()\n\n  if momentum_sample_fn is None:\n\n    # pylint: disable=function-redefined\n    def momentum_sample_fn(*momentum):\n      ret = tf.nest.map_structure(\n          lambda x: tf.random.normal(tf.shape(input=x), dtype=x.dtype),\n          momentum)\n      if len(ret) == 1:\n        return ret[0]\n      else:\n        return ret\n\n  if momentum is None:\n    momentum = call_fn(momentum_sample_fn,\n                       tf.nest.map_structure(tf.zeros_like, state))\n  if target_log_prob is None:\n    target_log_prob, state_extra, state_grads = call_and_grads(\n        target_log_prob_fn, state)\n\n  kinetic_energy, _ = call_fn(kinetic_energy_fn, momentum)\n  current_energy = -target_log_prob + kinetic_energy\n  current_state = HamiltonianMonteCarloState(\n      state=state,\n      state_grads=state_grads,\n      state_extra=state_extra,\n      target_log_prob=target_log_prob)\n\n  def leapfrog_wrapper(leapfrog_state, target_log_prob, state_extra):\n    \"\"\"Leapfrog wrapper that tracks extra state.\"\"\"\n    del target_log_prob\n    del state_extra\n\n    leapfrog_state, leapfrog_extra = leapfrog_step(\n        leapfrog_state,\n        step_size=step_size,\n        target_log_prob_fn=target_log_prob_fn,\n        kinetic_energy_fn=kinetic_energy_fn)\n\n    return [\n        leapfrog_state, leapfrog_extra.target_log_prob,\n        leapfrog_extra.state_extra\n    ], leapfrog_extra\n\n  def leapfrog_trace_wrapper_fn(args, leapfrog_extra):\n    return leapfrog_trace_fn(args[0], leapfrog_extra)\n\n  leapfrog_wrapper_state = (LeapFrogStepState(state, state_grads, momentum),\n                            target_log_prob, state_extra)\n\n  [[leapfrog_state, target_log_prob, state_extra], _], leapfrog_trace = trace(\n      leapfrog_wrapper_state,\n      leapfrog_wrapper,\n      num_leapfrog_steps,\n      trace_fn=leapfrog_trace_wrapper_fn)\n\n  kinetic_energy, _ = call_fn(kinetic_energy_fn, leapfrog_state.momentum)\n  proposed_energy = -target_log_prob + kinetic_energy\n  proposed_state = HamiltonianMonteCarloState(\n      state=leapfrog_state.state,\n      state_grads=leapfrog_state.state_grads,\n      target_log_prob=target_log_prob,\n      state_extra=state_extra)\n\n  energy_change = proposed_energy - current_energy\n  hmc_state, is_accepted, _ = metropolis_hastings_step(\n      current_state, proposed_state, energy_change, seed=seed)\n\n  hmc_state = hmc_state  # type: HamiltonianMonteCarloState\n  return hmc_state, HamiltonianMonteCarloExtra(\n      is_accepted=is_accepted,\n      proposed_hmc_state=proposed_state,\n      log_accept_ratio=-energy_change,\n      leapfrog_trace=leapfrog_trace)", "code_tokens": ["def", "hamiltonian_monte_carlo", "(", "hmc_state", ":", "HamiltonianMonteCarloState", ",", "target_log_prob_fn", ":", "PotentialFn", ",", "step_size", ":", "Any", ",", "num_leapfrog_steps", ":", "IntTensor", ",", "momentum", ":", "State", "=", "None", ",", "kinetic_energy_fn", ":", "PotentialFn", "=", "None", ",", "momentum_sample_fn", ":", "MomentumSampleFn", "=", "None", ",", "leapfrog_trace_fn", ":", "Callable", "[", "[", "LeapFrogStepState", ",", "LeapFrogStepExtras", "]", ",", "TensorNest", "]", "=", "lambda", "*", "args", ":", "(", ")", ",", "seed", "=", "None", ",", ")", "->", "Tuple", "[", "HamiltonianMonteCarloState", ",", "HamiltonianMonteCarloExtra", "]", ":", "state", "=", "hmc_state", ".", "state", "state_grads", "=", "hmc_state", ".", "state_grads", "target_log_prob", "=", "hmc_state", ".", "target_log_prob", "state_extra", "=", "hmc_state", ".", "state_extra", "if", "kinetic_energy_fn", "is", "None", ":", "# pylint: disable=function-redefined", "def", "kinetic_energy_fn", "(", "*", "momentum", ")", ":", "return", "tf", ".", "add_n", "(", "[", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "square", "(", "x", ")", ",", "axis", "=", "-", "1", ")", "/", "2.", "for", "x", "in", "tf", ".", "nest", ".", "flatten", "(", "momentum", ")", "]", ")", ",", "(", ")", "if", "momentum_sample_fn", "is", "None", ":", "# pylint: disable=function-redefined", "def", "momentum_sample_fn", "(", "*", "momentum", ")", ":", "ret", "=", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "x", ":", "tf", ".", "random", ".", "normal", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "dtype", "=", "x", ".", "dtype", ")", ",", "momentum", ")", "if", "len", "(", "ret", ")", "==", "1", ":", "return", "ret", "[", "0", "]", "else", ":", "return", "ret", "if", "momentum", "is", "None", ":", "momentum", "=", "call_fn", "(", "momentum_sample_fn", ",", "tf", ".", "nest", ".", "map_structure", "(", "tf", ".", "zeros_like", ",", "state", ")", ")", "if", "target_log_prob", "is", "None", ":", "target_log_prob", ",", "state_extra", ",", "state_grads", "=", "call_and_grads", "(", "target_log_prob_fn", ",", "state", ")", "kinetic_energy", ",", "_", "=", "call_fn", "(", "kinetic_energy_fn", ",", "momentum", ")", "current_energy", "=", "-", "target_log_prob", "+", "kinetic_energy", "current_state", "=", "HamiltonianMonteCarloState", "(", "state", "=", "state", ",", "state_grads", "=", "state_grads", ",", "state_extra", "=", "state_extra", ",", "target_log_prob", "=", "target_log_prob", ")", "def", "leapfrog_wrapper", "(", "leapfrog_state", ",", "target_log_prob", ",", "state_extra", ")", ":", "\"\"\"Leapfrog wrapper that tracks extra state.\"\"\"", "del", "target_log_prob", "del", "state_extra", "leapfrog_state", ",", "leapfrog_extra", "=", "leapfrog_step", "(", "leapfrog_state", ",", "step_size", "=", "step_size", ",", "target_log_prob_fn", "=", "target_log_prob_fn", ",", "kinetic_energy_fn", "=", "kinetic_energy_fn", ")", "return", "[", "leapfrog_state", ",", "leapfrog_extra", ".", "target_log_prob", ",", "leapfrog_extra", ".", "state_extra", "]", ",", "leapfrog_extra", "def", "leapfrog_trace_wrapper_fn", "(", "args", ",", "leapfrog_extra", ")", ":", "return", "leapfrog_trace_fn", "(", "args", "[", "0", "]", ",", "leapfrog_extra", ")", "leapfrog_wrapper_state", "=", "(", "LeapFrogStepState", "(", "state", ",", "state_grads", ",", "momentum", ")", ",", "target_log_prob", ",", "state_extra", ")", "[", "[", "leapfrog_state", ",", "target_log_prob", ",", "state_extra", "]", ",", "_", "]", ",", "leapfrog_trace", "=", "trace", "(", "leapfrog_wrapper_state", ",", "leapfrog_wrapper", ",", "num_leapfrog_steps", ",", "trace_fn", "=", "leapfrog_trace_wrapper_fn", ")", "kinetic_energy", ",", "_", "=", "call_fn", "(", "kinetic_energy_fn", ",", "leapfrog_state", ".", "momentum", ")", "proposed_energy", "=", "-", "target_log_prob", "+", "kinetic_energy", "proposed_state", "=", "HamiltonianMonteCarloState", "(", "state", "=", "leapfrog_state", ".", "state", ",", "state_grads", "=", "leapfrog_state", ".", "state_grads", ",", "target_log_prob", "=", "target_log_prob", ",", "state_extra", "=", "state_extra", ")", "energy_change", "=", "proposed_energy", "-", "current_energy", "hmc_state", ",", "is_accepted", ",", "_", "=", "metropolis_hastings_step", "(", "current_state", ",", "proposed_state", ",", "energy_change", ",", "seed", "=", "seed", ")", "hmc_state", "=", "hmc_state", "# type: HamiltonianMonteCarloState", "return", "hmc_state", ",", "HamiltonianMonteCarloExtra", "(", "is_accepted", "=", "is_accepted", ",", "proposed_hmc_state", "=", "proposed_state", ",", "log_accept_ratio", "=", "-", "energy_change", ",", "leapfrog_trace", "=", "leapfrog_trace", ")"], "docstring": "Hamiltonian Monte Carlo `TransitionOperator`.\n\n  #### Example\n\n  ```python\n  step_size = 0.2\n  num_steps = 2000\n  num_leapfrog_steps = 10\n  state = tf.ones([16, 2])\n\n  base_mean = [1., 0]\n  base_cov = [[1, 0.5], [0.5, 1]]\n\n  bijector = tfb.Softplus()\n  base_dist = tfd.MultivariateNormalFullCovariance(\n      loc=base_mean, covariance_matrix=base_cov)\n  target_dist = bijector(base_dist)\n\n  def orig_target_log_prob_fn(x):\n    return target_dist.log_prob(x), ()\n\n  target_log_prob_fn, state = fun_mcmc.transform_log_prob_fn(\n      orig_target_log_prob_fn, bijector, state)\n\n  kernel = tf.function(lambda state: fun_mcmc.hamiltonian_monte_carlo(\n      state,\n      step_size=step_size,\n      num_leapfrog_steps=num_leapfrog_steps,\n      target_log_prob_fn=target_log_prob_fn,\n      seed=tfp_test_util.test_seed()))\n\n  _, chain = fun_mcmc.trace(\n      state=fun_mcmc.HamiltonianMonteCarloState(\n          state=state,\n          state_grads=None,\n          target_log_prob=None,\n          state_extra=None),\n      fn=kernel,\n      num_steps=num_steps,\n      trace_fn=lambda state, extra: state.state_extra[0])\n  ```\n\n  Args:\n    hmc_state: HamiltonianMonteCarloState.\n    target_log_prob_fn: Target log prob fn.\n    step_size: Step size, structure broadcastable to the `target_log_prob_fn`\n      state.\n    num_leapfrog_steps: Number of leapfrog steps to take.\n    momentum: Initial momentum, passed to `momentum_sample_fn`. Default: zeroes.\n    kinetic_energy_fn: Kinetic energy function.\n    momentum_sample_fn: Sampler for the momentum.\n    leapfrog_trace_fn: Trace function for the leapfrog integrator.\n    seed: For reproducibility.\n\n  Returns:\n    hmc_state: HamiltonianMonteCarloState\n    hmc_extra: HamiltonianMonteCarloExtra", "docstring_tokens": ["Hamiltonian", "Monte", "Carlo", "TransitionOperator", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/fun_mcmc/fun_mcmc_lib.py#L361-L517", "partition": "test", "index": 642, "time": "2019-04-08 15:37:18"}
{"repo": "tensorflow/probability", "path": "experimental/fun_mcmc/fun_mcmc_lib.py", "func_name": "metropolis_hastings_step", "original_string": "def metropolis_hastings_step(current_state: State,\n                             proposed_state: State,\n                             energy_change: FloatTensor,\n                             seed=None) -> Tuple[State, tf.Tensor, tf.Tensor]:\n  \"\"\"Metropolis-Hastings step.\n\n  This probabilistically chooses between `current_state` and `proposed_state`\n  based on the `energy_change` so as to preserve detailed balance.\n\n  Energy change is the negative of `log_accept_ratio`.\n\n  Args:\n    current_state: Current state.\n    proposed_state: Proposed state.\n    energy_change: E(proposed_state) - E(previous_state).\n    seed: For reproducibility.\n\n  Returns:\n    new_state: The chosen state.\n    is_accepted: Whether the proposed state was accepted.\n    log_uniform: The random number that was used to select between the two\n      states.\n  \"\"\"\n  flat_current = tf.nest.flatten(current_state)\n  flat_proposed = nest.flatten_up_to(current_state, proposed_state)\n  # Impute the None's in the current state.\n  flat_current = [\n      p if c is None else c for p, c in zip(flat_proposed, flat_current)\n  ]\n  current_state = tf.nest.pack_sequence_as(current_state, flat_current)\n\n  current_state = tf.nest.map_structure(tf.convert_to_tensor, current_state)\n  proposed_state = tf.nest.map_structure(tf.convert_to_tensor, proposed_state)\n  energy_change = tf.convert_to_tensor(value=energy_change)\n\n  log_accept_ratio = -energy_change\n\n  log_uniform = tf.math.log(\n      tf.random.uniform(\n          shape=tf.shape(input=log_accept_ratio),\n          dtype=log_accept_ratio.dtype.base_dtype,\n          seed=seed))\n  is_accepted = log_uniform < log_accept_ratio\n\n  next_state = mcmc_util.choose(\n      is_accepted, proposed_state, current_state, name='choose_next_state')\n  return next_state, is_accepted, log_uniform", "language": "python", "code": "def metropolis_hastings_step(current_state: State,\n                             proposed_state: State,\n                             energy_change: FloatTensor,\n                             seed=None) -> Tuple[State, tf.Tensor, tf.Tensor]:\n  \"\"\"Metropolis-Hastings step.\n\n  This probabilistically chooses between `current_state` and `proposed_state`\n  based on the `energy_change` so as to preserve detailed balance.\n\n  Energy change is the negative of `log_accept_ratio`.\n\n  Args:\n    current_state: Current state.\n    proposed_state: Proposed state.\n    energy_change: E(proposed_state) - E(previous_state).\n    seed: For reproducibility.\n\n  Returns:\n    new_state: The chosen state.\n    is_accepted: Whether the proposed state was accepted.\n    log_uniform: The random number that was used to select between the two\n      states.\n  \"\"\"\n  flat_current = tf.nest.flatten(current_state)\n  flat_proposed = nest.flatten_up_to(current_state, proposed_state)\n  # Impute the None's in the current state.\n  flat_current = [\n      p if c is None else c for p, c in zip(flat_proposed, flat_current)\n  ]\n  current_state = tf.nest.pack_sequence_as(current_state, flat_current)\n\n  current_state = tf.nest.map_structure(tf.convert_to_tensor, current_state)\n  proposed_state = tf.nest.map_structure(tf.convert_to_tensor, proposed_state)\n  energy_change = tf.convert_to_tensor(value=energy_change)\n\n  log_accept_ratio = -energy_change\n\n  log_uniform = tf.math.log(\n      tf.random.uniform(\n          shape=tf.shape(input=log_accept_ratio),\n          dtype=log_accept_ratio.dtype.base_dtype,\n          seed=seed))\n  is_accepted = log_uniform < log_accept_ratio\n\n  next_state = mcmc_util.choose(\n      is_accepted, proposed_state, current_state, name='choose_next_state')\n  return next_state, is_accepted, log_uniform", "code_tokens": ["def", "metropolis_hastings_step", "(", "current_state", ":", "State", ",", "proposed_state", ":", "State", ",", "energy_change", ":", "FloatTensor", ",", "seed", "=", "None", ")", "->", "Tuple", "[", "State", ",", "tf", ".", "Tensor", ",", "tf", ".", "Tensor", "]", ":", "flat_current", "=", "tf", ".", "nest", ".", "flatten", "(", "current_state", ")", "flat_proposed", "=", "nest", ".", "flatten_up_to", "(", "current_state", ",", "proposed_state", ")", "# Impute the None's in the current state.", "flat_current", "=", "[", "p", "if", "c", "is", "None", "else", "c", "for", "p", ",", "c", "in", "zip", "(", "flat_proposed", ",", "flat_current", ")", "]", "current_state", "=", "tf", ".", "nest", ".", "pack_sequence_as", "(", "current_state", ",", "flat_current", ")", "current_state", "=", "tf", ".", "nest", ".", "map_structure", "(", "tf", ".", "convert_to_tensor", ",", "current_state", ")", "proposed_state", "=", "tf", ".", "nest", ".", "map_structure", "(", "tf", ".", "convert_to_tensor", ",", "proposed_state", ")", "energy_change", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "energy_change", ")", "log_accept_ratio", "=", "-", "energy_change", "log_uniform", "=", "tf", ".", "math", ".", "log", "(", "tf", ".", "random", ".", "uniform", "(", "shape", "=", "tf", ".", "shape", "(", "input", "=", "log_accept_ratio", ")", ",", "dtype", "=", "log_accept_ratio", ".", "dtype", ".", "base_dtype", ",", "seed", "=", "seed", ")", ")", "is_accepted", "=", "log_uniform", "<", "log_accept_ratio", "next_state", "=", "mcmc_util", ".", "choose", "(", "is_accepted", ",", "proposed_state", ",", "current_state", ",", "name", "=", "'choose_next_state'", ")", "return", "next_state", ",", "is_accepted", ",", "log_uniform"], "docstring": "Metropolis-Hastings step.\n\n  This probabilistically chooses between `current_state` and `proposed_state`\n  based on the `energy_change` so as to preserve detailed balance.\n\n  Energy change is the negative of `log_accept_ratio`.\n\n  Args:\n    current_state: Current state.\n    proposed_state: Proposed state.\n    energy_change: E(proposed_state) - E(previous_state).\n    seed: For reproducibility.\n\n  Returns:\n    new_state: The chosen state.\n    is_accepted: Whether the proposed state was accepted.\n    log_uniform: The random number that was used to select between the two\n      states.", "docstring_tokens": ["Metropolis", "-", "Hastings", "step", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/fun_mcmc/fun_mcmc_lib.py#L299-L345", "partition": "test", "index": 641, "time": "2019-04-08 15:37:18"}
{"repo": "tensorflow/probability", "path": "experimental/fun_mcmc/fun_mcmc_lib.py", "func_name": "leapfrog_step", "original_string": "def leapfrog_step(leapfrog_step_state: LeapFrogStepState,\n                  step_size: FloatTensor, target_log_prob_fn: PotentialFn,\n                  kinetic_energy_fn: PotentialFn\n                 ) -> Tuple[LeapFrogStepState, LeapFrogStepExtras]:\n  \"\"\"Leapfrog `TransitionOperator`.\n\n  Args:\n    leapfrog_step_state: LeapFrogStepState.\n    step_size: Step size, structure broadcastable to the `target_log_prob_fn`\n      state.\n    target_log_prob_fn: Target log prob fn.\n    kinetic_energy_fn: Kinetic energy fn.\n\n  Returns:\n    leapfrog_step_state: LeapFrogStepState.\n    leapfrog_step_extras: LeapFrogStepExtras.\n  \"\"\"\n  state = leapfrog_step_state.state\n  state_grads = leapfrog_step_state.state_grads\n  momentum = leapfrog_step_state.momentum\n  step_size = maybe_broadcast_structure(step_size, state)\n\n  state = tf.nest.map_structure(tf.convert_to_tensor, state)\n  momentum = tf.nest.map_structure(tf.convert_to_tensor, momentum)\n  state = tf.nest.map_structure(tf.convert_to_tensor, state)\n\n  if state_grads is None:\n    _, _, state_grads = call_and_grads(target_log_prob_fn, state)\n  else:\n    state_grads = tf.nest.map_structure(tf.convert_to_tensor, state_grads)\n\n  momentum = tf.nest.map_structure(lambda m, sg, s: m + 0.5 * sg * s, momentum,\n                                   state_grads, step_size)\n\n  kinetic_energy, kinetic_energy_extra, momentum_grads = call_and_grads(\n      kinetic_energy_fn, momentum)\n\n  state = tf.nest.map_structure(lambda x, mg, s: x + mg * s, state,\n                                momentum_grads, step_size)\n\n  target_log_prob, state_extra, state_grads = call_and_grads(\n      target_log_prob_fn, state)\n\n  momentum = tf.nest.map_structure(lambda m, sg, s: m + 0.5 * sg * s, momentum,\n                                   state_grads, step_size)\n\n  return LeapFrogStepState(state, state_grads, momentum), LeapFrogStepExtras(\n      target_log_prob, state_extra, kinetic_energy, kinetic_energy_extra)", "language": "python", "code": "def leapfrog_step(leapfrog_step_state: LeapFrogStepState,\n                  step_size: FloatTensor, target_log_prob_fn: PotentialFn,\n                  kinetic_energy_fn: PotentialFn\n                 ) -> Tuple[LeapFrogStepState, LeapFrogStepExtras]:\n  \"\"\"Leapfrog `TransitionOperator`.\n\n  Args:\n    leapfrog_step_state: LeapFrogStepState.\n    step_size: Step size, structure broadcastable to the `target_log_prob_fn`\n      state.\n    target_log_prob_fn: Target log prob fn.\n    kinetic_energy_fn: Kinetic energy fn.\n\n  Returns:\n    leapfrog_step_state: LeapFrogStepState.\n    leapfrog_step_extras: LeapFrogStepExtras.\n  \"\"\"\n  state = leapfrog_step_state.state\n  state_grads = leapfrog_step_state.state_grads\n  momentum = leapfrog_step_state.momentum\n  step_size = maybe_broadcast_structure(step_size, state)\n\n  state = tf.nest.map_structure(tf.convert_to_tensor, state)\n  momentum = tf.nest.map_structure(tf.convert_to_tensor, momentum)\n  state = tf.nest.map_structure(tf.convert_to_tensor, state)\n\n  if state_grads is None:\n    _, _, state_grads = call_and_grads(target_log_prob_fn, state)\n  else:\n    state_grads = tf.nest.map_structure(tf.convert_to_tensor, state_grads)\n\n  momentum = tf.nest.map_structure(lambda m, sg, s: m + 0.5 * sg * s, momentum,\n                                   state_grads, step_size)\n\n  kinetic_energy, kinetic_energy_extra, momentum_grads = call_and_grads(\n      kinetic_energy_fn, momentum)\n\n  state = tf.nest.map_structure(lambda x, mg, s: x + mg * s, state,\n                                momentum_grads, step_size)\n\n  target_log_prob, state_extra, state_grads = call_and_grads(\n      target_log_prob_fn, state)\n\n  momentum = tf.nest.map_structure(lambda m, sg, s: m + 0.5 * sg * s, momentum,\n                                   state_grads, step_size)\n\n  return LeapFrogStepState(state, state_grads, momentum), LeapFrogStepExtras(\n      target_log_prob, state_extra, kinetic_energy, kinetic_energy_extra)", "code_tokens": ["def", "leapfrog_step", "(", "leapfrog_step_state", ":", "LeapFrogStepState", ",", "step_size", ":", "FloatTensor", ",", "target_log_prob_fn", ":", "PotentialFn", ",", "kinetic_energy_fn", ":", "PotentialFn", ")", "->", "Tuple", "[", "LeapFrogStepState", ",", "LeapFrogStepExtras", "]", ":", "state", "=", "leapfrog_step_state", ".", "state", "state_grads", "=", "leapfrog_step_state", ".", "state_grads", "momentum", "=", "leapfrog_step_state", ".", "momentum", "step_size", "=", "maybe_broadcast_structure", "(", "step_size", ",", "state", ")", "state", "=", "tf", ".", "nest", ".", "map_structure", "(", "tf", ".", "convert_to_tensor", ",", "state", ")", "momentum", "=", "tf", ".", "nest", ".", "map_structure", "(", "tf", ".", "convert_to_tensor", ",", "momentum", ")", "state", "=", "tf", ".", "nest", ".", "map_structure", "(", "tf", ".", "convert_to_tensor", ",", "state", ")", "if", "state_grads", "is", "None", ":", "_", ",", "_", ",", "state_grads", "=", "call_and_grads", "(", "target_log_prob_fn", ",", "state", ")", "else", ":", "state_grads", "=", "tf", ".", "nest", ".", "map_structure", "(", "tf", ".", "convert_to_tensor", ",", "state_grads", ")", "momentum", "=", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "m", ",", "sg", ",", "s", ":", "m", "+", "0.5", "*", "sg", "*", "s", ",", "momentum", ",", "state_grads", ",", "step_size", ")", "kinetic_energy", ",", "kinetic_energy_extra", ",", "momentum_grads", "=", "call_and_grads", "(", "kinetic_energy_fn", ",", "momentum", ")", "state", "=", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "x", ",", "mg", ",", "s", ":", "x", "+", "mg", "*", "s", ",", "state", ",", "momentum_grads", ",", "step_size", ")", "target_log_prob", ",", "state_extra", ",", "state_grads", "=", "call_and_grads", "(", "target_log_prob_fn", ",", "state", ")", "momentum", "=", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "m", ",", "sg", ",", "s", ":", "m", "+", "0.5", "*", "sg", "*", "s", ",", "momentum", ",", "state_grads", ",", "step_size", ")", "return", "LeapFrogStepState", "(", "state", ",", "state_grads", ",", "momentum", ")", ",", "LeapFrogStepExtras", "(", "target_log_prob", ",", "state_extra", ",", "kinetic_energy", ",", "kinetic_energy_extra", ")"], "docstring": "Leapfrog `TransitionOperator`.\n\n  Args:\n    leapfrog_step_state: LeapFrogStepState.\n    step_size: Step size, structure broadcastable to the `target_log_prob_fn`\n      state.\n    target_log_prob_fn: Target log prob fn.\n    kinetic_energy_fn: Kinetic energy fn.\n\n  Returns:\n    leapfrog_step_state: LeapFrogStepState.\n    leapfrog_step_extras: LeapFrogStepExtras.", "docstring_tokens": ["Leapfrog", "TransitionOperator", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/fun_mcmc/fun_mcmc_lib.py#L249-L296", "partition": "test", "index": 640, "time": "2019-04-08 15:37:18"}
{"repo": "tensorflow/probability", "path": "experimental/fun_mcmc/fun_mcmc_lib.py", "func_name": "transform_log_prob_fn", "original_string": "def transform_log_prob_fn(log_prob_fn: PotentialFn,\n                          bijector: BijectorNest,\n                          init_state: State = None\n                         ) -> Union[PotentialFn, Tuple[PotentialFn, State]]:\n  \"\"\"Transforms a log-prob function using a bijector.\n\n  This takes a log-prob function and creates a new log-prob function that now\n  takes takes state in the domain of the bijector, forward transforms that state\n  and calls the original log-prob function. It then returns the log-probability\n  that correctly accounts for this transformation.\n\n  The forward-transformed state is pre-pended to the original log-prob\n  function's extra returns and returned as the new extra return.\n\n  For convenience you can also pass the initial state (in the original space),\n  and this function will return the inverse transformed as the 2nd return value.\n  You'd use this to initialize MCMC operators that operate in the transformed\n  space.\n\n  Args:\n    log_prob_fn: Log prob fn.\n    bijector: Bijector(s), must be of the same structure as the `log_prob_fn`\n      inputs.\n    init_state: Initial state, in the original space.\n\n  Returns:\n    transformed_log_prob_fn: Transformed log prob fn.\n    transformed_init_state: If `init_state` is provided. Initial state in the\n      transformed space.\n  \"\"\"\n\n  def wrapper(*args):\n    \"\"\"Transformed wrapper.\"\"\"\n    bijector_ = bijector\n\n    args = tf.nest.map_structure(lambda x: 0. + x, args)\n    if len(args) == 1:\n      args = args[0]\n    elif isinstance(bijector_, list):\n      bijector_ = tuple(bijector_)\n\n    original_space_args = tf.nest.map_structure(lambda b, x: b.forward(x),\n                                                bijector_, args)\n    original_space_args = original_space_args  # type: Tuple[Any]\n    original_space_log_prob, extra = call_fn(log_prob_fn, original_space_args)\n    event_ndims = tf.nest.map_structure(\n        lambda x: tf.rank(x) - tf.rank(original_space_log_prob), args)\n\n    return original_space_log_prob + sum(\n        tf.nest.flatten(\n            tf.nest.map_structure(\n                lambda b, x, e: b.forward_log_det_jacobian(x, event_ndims=e),\n                bijector_, args, event_ndims))), [original_space_args, extra]\n\n  if init_state is None:\n    return wrapper\n  else:\n    return wrapper, tf.nest.map_structure(lambda b, s: b.inverse(s), bijector,\n                                          init_state)", "language": "python", "code": "def transform_log_prob_fn(log_prob_fn: PotentialFn,\n                          bijector: BijectorNest,\n                          init_state: State = None\n                         ) -> Union[PotentialFn, Tuple[PotentialFn, State]]:\n  \"\"\"Transforms a log-prob function using a bijector.\n\n  This takes a log-prob function and creates a new log-prob function that now\n  takes takes state in the domain of the bijector, forward transforms that state\n  and calls the original log-prob function. It then returns the log-probability\n  that correctly accounts for this transformation.\n\n  The forward-transformed state is pre-pended to the original log-prob\n  function's extra returns and returned as the new extra return.\n\n  For convenience you can also pass the initial state (in the original space),\n  and this function will return the inverse transformed as the 2nd return value.\n  You'd use this to initialize MCMC operators that operate in the transformed\n  space.\n\n  Args:\n    log_prob_fn: Log prob fn.\n    bijector: Bijector(s), must be of the same structure as the `log_prob_fn`\n      inputs.\n    init_state: Initial state, in the original space.\n\n  Returns:\n    transformed_log_prob_fn: Transformed log prob fn.\n    transformed_init_state: If `init_state` is provided. Initial state in the\n      transformed space.\n  \"\"\"\n\n  def wrapper(*args):\n    \"\"\"Transformed wrapper.\"\"\"\n    bijector_ = bijector\n\n    args = tf.nest.map_structure(lambda x: 0. + x, args)\n    if len(args) == 1:\n      args = args[0]\n    elif isinstance(bijector_, list):\n      bijector_ = tuple(bijector_)\n\n    original_space_args = tf.nest.map_structure(lambda b, x: b.forward(x),\n                                                bijector_, args)\n    original_space_args = original_space_args  # type: Tuple[Any]\n    original_space_log_prob, extra = call_fn(log_prob_fn, original_space_args)\n    event_ndims = tf.nest.map_structure(\n        lambda x: tf.rank(x) - tf.rank(original_space_log_prob), args)\n\n    return original_space_log_prob + sum(\n        tf.nest.flatten(\n            tf.nest.map_structure(\n                lambda b, x, e: b.forward_log_det_jacobian(x, event_ndims=e),\n                bijector_, args, event_ndims))), [original_space_args, extra]\n\n  if init_state is None:\n    return wrapper\n  else:\n    return wrapper, tf.nest.map_structure(lambda b, s: b.inverse(s), bijector,\n                                          init_state)", "code_tokens": ["def", "transform_log_prob_fn", "(", "log_prob_fn", ":", "PotentialFn", ",", "bijector", ":", "BijectorNest", ",", "init_state", ":", "State", "=", "None", ")", "->", "Union", "[", "PotentialFn", ",", "Tuple", "[", "PotentialFn", ",", "State", "]", "]", ":", "def", "wrapper", "(", "*", "args", ")", ":", "\"\"\"Transformed wrapper.\"\"\"", "bijector_", "=", "bijector", "args", "=", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "x", ":", "0.", "+", "x", ",", "args", ")", "if", "len", "(", "args", ")", "==", "1", ":", "args", "=", "args", "[", "0", "]", "elif", "isinstance", "(", "bijector_", ",", "list", ")", ":", "bijector_", "=", "tuple", "(", "bijector_", ")", "original_space_args", "=", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "b", ",", "x", ":", "b", ".", "forward", "(", "x", ")", ",", "bijector_", ",", "args", ")", "original_space_args", "=", "original_space_args", "# type: Tuple[Any]", "original_space_log_prob", ",", "extra", "=", "call_fn", "(", "log_prob_fn", ",", "original_space_args", ")", "event_ndims", "=", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "x", ":", "tf", ".", "rank", "(", "x", ")", "-", "tf", ".", "rank", "(", "original_space_log_prob", ")", ",", "args", ")", "return", "original_space_log_prob", "+", "sum", "(", "tf", ".", "nest", ".", "flatten", "(", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "b", ",", "x", ",", "e", ":", "b", ".", "forward_log_det_jacobian", "(", "x", ",", "event_ndims", "=", "e", ")", ",", "bijector_", ",", "args", ",", "event_ndims", ")", ")", ")", ",", "[", "original_space_args", ",", "extra", "]", "if", "init_state", "is", "None", ":", "return", "wrapper", "else", ":", "return", "wrapper", ",", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "b", ",", "s", ":", "b", ".", "inverse", "(", "s", ")", ",", "bijector", ",", "init_state", ")"], "docstring": "Transforms a log-prob function using a bijector.\n\n  This takes a log-prob function and creates a new log-prob function that now\n  takes takes state in the domain of the bijector, forward transforms that state\n  and calls the original log-prob function. It then returns the log-probability\n  that correctly accounts for this transformation.\n\n  The forward-transformed state is pre-pended to the original log-prob\n  function's extra returns and returned as the new extra return.\n\n  For convenience you can also pass the initial state (in the original space),\n  and this function will return the inverse transformed as the 2nd return value.\n  You'd use this to initialize MCMC operators that operate in the transformed\n  space.\n\n  Args:\n    log_prob_fn: Log prob fn.\n    bijector: Bijector(s), must be of the same structure as the `log_prob_fn`\n      inputs.\n    init_state: Initial state, in the original space.\n\n  Returns:\n    transformed_log_prob_fn: Transformed log prob fn.\n    transformed_init_state: If `init_state` is provided. Initial state in the\n      transformed space.", "docstring_tokens": ["Transforms", "a", "log", "-", "prob", "function", "using", "a", "bijector", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/fun_mcmc/fun_mcmc_lib.py#L181-L239", "partition": "test", "index": 639, "time": "2019-04-08 15:37:18"}
{"repo": "tensorflow/probability", "path": "experimental/fun_mcmc/fun_mcmc_lib.py", "func_name": "call_and_grads", "original_string": "def call_and_grads(fn: TransitionOperator, args: Union[Tuple[Any], Any]\n                  ) -> Tuple[tf.Tensor, TensorNest, TensorNest]:\n  \"\"\"Calls `fn` and returns the gradients with respect to `fn`'s first output.\n\n  Args:\n    fn: A `TransitionOperator`.\n    args: Arguments to `fn`\n\n  Returns:\n    ret: First output of `fn`.\n    extra: Second output of `fn`.\n    grads: Gradients of `ret` with respect to `args`.\n  \"\"\"\n  with tf.GradientTape() as tape:\n    tape.watch(args)\n    ret, extra = call_fn(fn, args)\n  grads = tape.gradient(ret, args)\n  return ret, extra, grads", "language": "python", "code": "def call_and_grads(fn: TransitionOperator, args: Union[Tuple[Any], Any]\n                  ) -> Tuple[tf.Tensor, TensorNest, TensorNest]:\n  \"\"\"Calls `fn` and returns the gradients with respect to `fn`'s first output.\n\n  Args:\n    fn: A `TransitionOperator`.\n    args: Arguments to `fn`\n\n  Returns:\n    ret: First output of `fn`.\n    extra: Second output of `fn`.\n    grads: Gradients of `ret` with respect to `args`.\n  \"\"\"\n  with tf.GradientTape() as tape:\n    tape.watch(args)\n    ret, extra = call_fn(fn, args)\n  grads = tape.gradient(ret, args)\n  return ret, extra, grads", "code_tokens": ["def", "call_and_grads", "(", "fn", ":", "TransitionOperator", ",", "args", ":", "Union", "[", "Tuple", "[", "Any", "]", ",", "Any", "]", ")", "->", "Tuple", "[", "tf", ".", "Tensor", ",", "TensorNest", ",", "TensorNest", "]", ":", "with", "tf", ".", "GradientTape", "(", ")", "as", "tape", ":", "tape", ".", "watch", "(", "args", ")", "ret", ",", "extra", "=", "call_fn", "(", "fn", ",", "args", ")", "grads", "=", "tape", ".", "gradient", "(", "ret", ",", "args", ")", "return", "ret", ",", "extra", ",", "grads"], "docstring": "Calls `fn` and returns the gradients with respect to `fn`'s first output.\n\n  Args:\n    fn: A `TransitionOperator`.\n    args: Arguments to `fn`\n\n  Returns:\n    ret: First output of `fn`.\n    extra: Second output of `fn`.\n    grads: Gradients of `ret` with respect to `args`.", "docstring_tokens": ["Calls", "fn", "and", "returns", "the", "gradients", "with", "respect", "to", "fn", "s", "first", "output", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/fun_mcmc/fun_mcmc_lib.py#L140-L157", "partition": "test", "index": 637, "time": "2019-04-08 15:37:18"}
{"repo": "tensorflow/probability", "path": "experimental/fun_mcmc/fun_mcmc_lib.py", "func_name": "call_fn", "original_string": "def call_fn(fn: TransitionOperator, args: Union[Tuple[Any], Any]) -> Any:\n  \"\"\"Calls a transition operator with args, unpacking args if its a sequence.\n\n  Args:\n    fn: A `TransitionOperator`.\n    args: Arguments to `fn`\n\n  Returns:\n    ret: Return value of `fn`.\n  \"\"\"\n\n  if isinstance(args, (list, tuple)) and not mcmc_util.is_namedtuple_like(args):\n    args = args  # type: Tuple[Any]\n    return fn(*args)\n  else:\n    return fn(args)", "language": "python", "code": "def call_fn(fn: TransitionOperator, args: Union[Tuple[Any], Any]) -> Any:\n  \"\"\"Calls a transition operator with args, unpacking args if its a sequence.\n\n  Args:\n    fn: A `TransitionOperator`.\n    args: Arguments to `fn`\n\n  Returns:\n    ret: Return value of `fn`.\n  \"\"\"\n\n  if isinstance(args, (list, tuple)) and not mcmc_util.is_namedtuple_like(args):\n    args = args  # type: Tuple[Any]\n    return fn(*args)\n  else:\n    return fn(args)", "code_tokens": ["def", "call_fn", "(", "fn", ":", "TransitionOperator", ",", "args", ":", "Union", "[", "Tuple", "[", "Any", "]", ",", "Any", "]", ")", "->", "Any", ":", "if", "isinstance", "(", "args", ",", "(", "list", ",", "tuple", ")", ")", "and", "not", "mcmc_util", ".", "is_namedtuple_like", "(", "args", ")", ":", "args", "=", "args", "# type: Tuple[Any]", "return", "fn", "(", "*", "args", ")", "else", ":", "return", "fn", "(", "args", ")"], "docstring": "Calls a transition operator with args, unpacking args if its a sequence.\n\n  Args:\n    fn: A `TransitionOperator`.\n    args: Arguments to `fn`\n\n  Returns:\n    ret: Return value of `fn`.", "docstring_tokens": ["Calls", "a", "transition", "operator", "with", "args", "unpacking", "args", "if", "its", "a", "sequence", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/fun_mcmc/fun_mcmc_lib.py#L122-L137", "partition": "test", "index": 636, "time": "2019-04-08 15:37:18"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/regression.py", "func_name": "_zero_dimensional_mvndiag", "original_string": "def _zero_dimensional_mvndiag(dtype):\n  \"\"\"Build a zero-dimensional MVNDiag object.\"\"\"\n  dummy_mvndiag = tfd.MultivariateNormalDiag(\n      scale_diag=tf.ones([0], dtype=dtype))\n  dummy_mvndiag.covariance = lambda: dummy_mvndiag.variance()[..., tf.newaxis]\n  return dummy_mvndiag", "language": "python", "code": "def _zero_dimensional_mvndiag(dtype):\n  \"\"\"Build a zero-dimensional MVNDiag object.\"\"\"\n  dummy_mvndiag = tfd.MultivariateNormalDiag(\n      scale_diag=tf.ones([0], dtype=dtype))\n  dummy_mvndiag.covariance = lambda: dummy_mvndiag.variance()[..., tf.newaxis]\n  return dummy_mvndiag", "code_tokens": ["def", "_zero_dimensional_mvndiag", "(", "dtype", ")", ":", "dummy_mvndiag", "=", "tfd", ".", "MultivariateNormalDiag", "(", "scale_diag", "=", "tf", ".", "ones", "(", "[", "0", "]", ",", "dtype", "=", "dtype", ")", ")", "dummy_mvndiag", ".", "covariance", "=", "lambda", ":", "dummy_mvndiag", ".", "variance", "(", ")", "[", "...", ",", "tf", ".", "newaxis", "]", "return", "dummy_mvndiag"], "docstring": "Build a zero-dimensional MVNDiag object.", "docstring_tokens": ["Build", "a", "zero", "-", "dimensional", "MVNDiag", "object", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/regression.py#L32-L37", "partition": "test", "index": 1125, "time": "2019-04-10 13:48:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/regression.py", "func_name": "SparseLinearRegression.params_to_weights", "original_string": "def params_to_weights(self,\n                        global_scale_variance,\n                        global_scale_noncentered,\n                        local_scale_variances,\n                        local_scales_noncentered,\n                        weights_noncentered):\n    \"\"\"Build regression weights from model parameters.\"\"\"\n    global_scale = (global_scale_noncentered *\n                    tf.sqrt(global_scale_variance) *\n                    self.weights_prior_scale)\n\n    local_scales = local_scales_noncentered * tf.sqrt(local_scale_variances)\n    return weights_noncentered * local_scales * global_scale[..., tf.newaxis]", "language": "python", "code": "def params_to_weights(self,\n                        global_scale_variance,\n                        global_scale_noncentered,\n                        local_scale_variances,\n                        local_scales_noncentered,\n                        weights_noncentered):\n    \"\"\"Build regression weights from model parameters.\"\"\"\n    global_scale = (global_scale_noncentered *\n                    tf.sqrt(global_scale_variance) *\n                    self.weights_prior_scale)\n\n    local_scales = local_scales_noncentered * tf.sqrt(local_scale_variances)\n    return weights_noncentered * local_scales * global_scale[..., tf.newaxis]", "code_tokens": ["def", "params_to_weights", "(", "self", ",", "global_scale_variance", ",", "global_scale_noncentered", ",", "local_scale_variances", ",", "local_scales_noncentered", ",", "weights_noncentered", ")", ":", "global_scale", "=", "(", "global_scale_noncentered", "*", "tf", ".", "sqrt", "(", "global_scale_variance", ")", "*", "self", ".", "weights_prior_scale", ")", "local_scales", "=", "local_scales_noncentered", "*", "tf", ".", "sqrt", "(", "local_scale_variances", ")", "return", "weights_noncentered", "*", "local_scales", "*", "global_scale", "[", "...", ",", "tf", ".", "newaxis", "]"], "docstring": "Build regression weights from model parameters.", "docstring_tokens": ["Build", "regression", "weights", "from", "model", "parameters", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/regression.py#L474-L486", "partition": "test", "index": 1127, "time": "2019-04-10 13:48:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/regression.py", "func_name": "_observe_timeseries_fn", "original_string": "def _observe_timeseries_fn(timeseries):\n  \"\"\"Build an observation_noise_fn that observes a Tensor timeseries.\"\"\"\n  def observation_noise_fn(t):\n    current_slice = timeseries[..., t, :]\n    return tfd.MultivariateNormalDiag(\n        loc=current_slice,\n        scale_diag=tf.zeros_like(current_slice))\n  return observation_noise_fn", "language": "python", "code": "def _observe_timeseries_fn(timeseries):\n  \"\"\"Build an observation_noise_fn that observes a Tensor timeseries.\"\"\"\n  def observation_noise_fn(t):\n    current_slice = timeseries[..., t, :]\n    return tfd.MultivariateNormalDiag(\n        loc=current_slice,\n        scale_diag=tf.zeros_like(current_slice))\n  return observation_noise_fn", "code_tokens": ["def", "_observe_timeseries_fn", "(", "timeseries", ")", ":", "def", "observation_noise_fn", "(", "t", ")", ":", "current_slice", "=", "timeseries", "[", "...", ",", "t", ",", ":", "]", "return", "tfd", ".", "MultivariateNormalDiag", "(", "loc", "=", "current_slice", ",", "scale_diag", "=", "tf", ".", "zeros_like", "(", "current_slice", ")", ")", "return", "observation_noise_fn"], "docstring": "Build an observation_noise_fn that observes a Tensor timeseries.", "docstring_tokens": ["Build", "an", "observation_noise_fn", "that", "observes", "a", "Tensor", "timeseries", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/regression.py#L40-L47", "partition": "test", "index": 1126, "time": "2019-04-10 13:48:37"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/dtype_util.py", "func_name": "size", "original_string": "def size(dtype):\n  \"\"\"Returns the number of bytes to represent this `dtype`.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'size'):\n    return dtype.size\n  return np.dtype(dtype).itemsize", "language": "python", "code": "def size(dtype):\n  \"\"\"Returns the number of bytes to represent this `dtype`.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'size'):\n    return dtype.size\n  return np.dtype(dtype).itemsize", "code_tokens": ["def", "size", "(", "dtype", ")", ":", "dtype", "=", "tf", ".", "as_dtype", "(", "dtype", ")", "if", "hasattr", "(", "dtype", ",", "'size'", ")", ":", "return", "dtype", ".", "size", "return", "np", ".", "dtype", "(", "dtype", ")", ".", "itemsize"], "docstring": "Returns the number of bytes to represent this `dtype`.", "docstring_tokens": ["Returns", "the", "number", "of", "bytes", "to", "represent", "this", "dtype", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/dtype_util.py#L142-L147", "partition": "test", "index": 855, "time": "2019-04-11 13:17:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/dtype_util.py", "func_name": "max", "original_string": "def max(dtype):  # pylint: disable=redefined-builtin\n  \"\"\"Returns the maximum representable value in this data type.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'max'):\n    return dtype.max\n  use_finfo = is_floating(dtype) or is_complex(dtype)\n  return np.finfo(dtype).max if use_finfo else np.iinfo(dtype).max", "language": "python", "code": "def max(dtype):  # pylint: disable=redefined-builtin\n  \"\"\"Returns the maximum representable value in this data type.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'max'):\n    return dtype.max\n  use_finfo = is_floating(dtype) or is_complex(dtype)\n  return np.finfo(dtype).max if use_finfo else np.iinfo(dtype).max", "code_tokens": ["def", "max", "(", "dtype", ")", ":", "# pylint: disable=redefined-builtin", "dtype", "=", "tf", ".", "as_dtype", "(", "dtype", ")", "if", "hasattr", "(", "dtype", ",", "'max'", ")", ":", "return", "dtype", ".", "max", "use_finfo", "=", "is_floating", "(", "dtype", ")", "or", "is_complex", "(", "dtype", ")", "return", "np", ".", "finfo", "(", "dtype", ")", ".", "max", "if", "use_finfo", "else", "np", ".", "iinfo", "(", "dtype", ")", ".", "max"], "docstring": "Returns the maximum representable value in this data type.", "docstring_tokens": ["Returns", "the", "maximum", "representable", "value", "in", "this", "data", "type", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/dtype_util.py#L114-L120", "partition": "test", "index": 853, "time": "2019-04-11 13:17:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/dtype_util.py", "func_name": "is_complex", "original_string": "def is_complex(dtype):\n  \"\"\"Returns whether this is a complex floating point type.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'is_complex'):\n    return dtype.is_complex\n  return np.issubdtype(np.dtype(dtype), np.complex)", "language": "python", "code": "def is_complex(dtype):\n  \"\"\"Returns whether this is a complex floating point type.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'is_complex'):\n    return dtype.is_complex\n  return np.issubdtype(np.dtype(dtype), np.complex)", "code_tokens": ["def", "is_complex", "(", "dtype", ")", ":", "dtype", "=", "tf", ".", "as_dtype", "(", "dtype", ")", "if", "hasattr", "(", "dtype", ",", "'is_complex'", ")", ":", "return", "dtype", ".", "is_complex", "return", "np", ".", "issubdtype", "(", "np", ".", "dtype", "(", "dtype", ")", ",", "np", ".", "complex", ")"], "docstring": "Returns whether this is a complex floating point type.", "docstring_tokens": ["Returns", "whether", "this", "is", "a", "complex", "floating", "point", "type", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/dtype_util.py#L90-L95", "partition": "test", "index": 852, "time": "2019-04-11 13:17:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/dtype_util.py", "func_name": "name", "original_string": "def name(dtype):\n  \"\"\"Returns the string name for this `dtype`.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'name'):\n    return dtype.name\n  if hasattr(dtype, '__name__'):\n    return dtype.__name__\n  return str(dtype)", "language": "python", "code": "def name(dtype):\n  \"\"\"Returns the string name for this `dtype`.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'name'):\n    return dtype.name\n  if hasattr(dtype, '__name__'):\n    return dtype.__name__\n  return str(dtype)", "code_tokens": ["def", "name", "(", "dtype", ")", ":", "dtype", "=", "tf", ".", "as_dtype", "(", "dtype", ")", "if", "hasattr", "(", "dtype", ",", "'name'", ")", ":", "return", "dtype", ".", "name", "if", "hasattr", "(", "dtype", ",", "'__name__'", ")", ":", "return", "dtype", ".", "__name__", "return", "str", "(", "dtype", ")"], "docstring": "Returns the string name for this `dtype`.", "docstring_tokens": ["Returns", "the", "string", "name", "for", "this", "dtype", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/dtype_util.py#L132-L139", "partition": "test", "index": 854, "time": "2019-04-11 13:17:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/dtype_util.py", "func_name": "base_dtype", "original_string": "def base_dtype(dtype):\n  \"\"\"Returns a non-reference `dtype` based on this `dtype`.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'base_dtype'):\n    return dtype.base_dtype\n  return dtype", "language": "python", "code": "def base_dtype(dtype):\n  \"\"\"Returns a non-reference `dtype` based on this `dtype`.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'base_dtype'):\n    return dtype.base_dtype\n  return dtype", "code_tokens": ["def", "base_dtype", "(", "dtype", ")", ":", "dtype", "=", "tf", ".", "as_dtype", "(", "dtype", ")", "if", "hasattr", "(", "dtype", ",", "'base_dtype'", ")", ":", "return", "dtype", ".", "base_dtype", "return", "dtype"], "docstring": "Returns a non-reference `dtype` based on this `dtype`.", "docstring_tokens": ["Returns", "a", "non", "-", "reference", "dtype", "based", "on", "this", "dtype", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/dtype_util.py#L52-L57", "partition": "test", "index": 850, "time": "2019-04-11 13:17:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/dtype_util.py", "func_name": "as_numpy_dtype", "original_string": "def as_numpy_dtype(dtype):\n  \"\"\"Returns a `np.dtype` based on this `dtype`.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'as_numpy_dtype'):\n    return dtype.as_numpy_dtype\n  return dtype", "language": "python", "code": "def as_numpy_dtype(dtype):\n  \"\"\"Returns a `np.dtype` based on this `dtype`.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'as_numpy_dtype'):\n    return dtype.as_numpy_dtype\n  return dtype", "code_tokens": ["def", "as_numpy_dtype", "(", "dtype", ")", ":", "dtype", "=", "tf", ".", "as_dtype", "(", "dtype", ")", "if", "hasattr", "(", "dtype", ",", "'as_numpy_dtype'", ")", ":", "return", "dtype", ".", "as_numpy_dtype", "return", "dtype"], "docstring": "Returns a `np.dtype` based on this `dtype`.", "docstring_tokens": ["Returns", "a", "np", ".", "dtype", "based", "on", "this", "dtype", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/dtype_util.py#L44-L49", "partition": "test", "index": 849, "time": "2019-04-11 13:17:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/dtype_util.py", "func_name": "is_bool", "original_string": "def is_bool(dtype):\n  \"\"\"Returns whether this is a boolean data type.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'is_bool'):\n    return dtype.is_bool\n  # We use `kind` because:\n  # np.issubdtype(np.uint8, np.bool) == True.\n  return np.dtype(dtype).kind == 'b'", "language": "python", "code": "def is_bool(dtype):\n  \"\"\"Returns whether this is a boolean data type.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'is_bool'):\n    return dtype.is_bool\n  # We use `kind` because:\n  # np.issubdtype(np.uint8, np.bool) == True.\n  return np.dtype(dtype).kind == 'b'", "code_tokens": ["def", "is_bool", "(", "dtype", ")", ":", "dtype", "=", "tf", ".", "as_dtype", "(", "dtype", ")", "if", "hasattr", "(", "dtype", ",", "'is_bool'", ")", ":", "return", "dtype", ".", "is_bool", "# We use `kind` because:", "# np.issubdtype(np.uint8, np.bool) == True.", "return", "np", ".", "dtype", "(", "dtype", ")", ".", "kind", "==", "'b'"], "docstring": "Returns whether this is a boolean data type.", "docstring_tokens": ["Returns", "whether", "this", "is", "a", "boolean", "data", "type", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/dtype_util.py#L80-L87", "partition": "test", "index": 851, "time": "2019-04-11 13:17:12"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/tensorshape_util.py", "func_name": "concatenate", "original_string": "def concatenate(x, other):\n  \"\"\"Returns the concatenation of the dimension in `x` and `other`.\n\n  *Note:* If either `x` or `other` is completely unknown, concatenation will\n  discard information about the other shape. In future, we might support\n  concatenation that preserves this information for use with slicing.\n\n  For more details, see `help(tf.TensorShape.concatenate)`.\n\n  Args:\n    x: object representing a shape; convertible to `tf.TensorShape`.\n    other: object representing a shape; convertible to `tf.TensorShape`.\n\n  Returns:\n    new_shape: an object like `x` whose elements are the concatenation of the\n      dimensions in `x` and `other`.\n  \"\"\"\n  return type(x)(tf.TensorShape(x).concatenate(other))", "language": "python", "code": "def concatenate(x, other):\n  \"\"\"Returns the concatenation of the dimension in `x` and `other`.\n\n  *Note:* If either `x` or `other` is completely unknown, concatenation will\n  discard information about the other shape. In future, we might support\n  concatenation that preserves this information for use with slicing.\n\n  For more details, see `help(tf.TensorShape.concatenate)`.\n\n  Args:\n    x: object representing a shape; convertible to `tf.TensorShape`.\n    other: object representing a shape; convertible to `tf.TensorShape`.\n\n  Returns:\n    new_shape: an object like `x` whose elements are the concatenation of the\n      dimensions in `x` and `other`.\n  \"\"\"\n  return type(x)(tf.TensorShape(x).concatenate(other))", "code_tokens": ["def", "concatenate", "(", "x", ",", "other", ")", ":", "return", "type", "(", "x", ")", "(", "tf", ".", "TensorShape", "(", "x", ")", ".", "concatenate", "(", "other", ")", ")"], "docstring": "Returns the concatenation of the dimension in `x` and `other`.\n\n  *Note:* If either `x` or `other` is completely unknown, concatenation will\n  discard information about the other shape. In future, we might support\n  concatenation that preserves this information for use with slicing.\n\n  For more details, see `help(tf.TensorShape.concatenate)`.\n\n  Args:\n    x: object representing a shape; convertible to `tf.TensorShape`.\n    other: object representing a shape; convertible to `tf.TensorShape`.\n\n  Returns:\n    new_shape: an object like `x` whose elements are the concatenation of the\n      dimensions in `x` and `other`.", "docstring_tokens": ["Returns", "the", "concatenation", "of", "the", "dimension", "in", "x", "and", "other", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/tensorshape_util.py#L99-L116", "partition": "test", "index": 934, "time": "2019-04-11 15:42:42"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/tensorshape_util.py", "func_name": "dims", "original_string": "def dims(x):\n  \"\"\"Returns a list of dimension sizes, or `None` if `rank` is unknown.\n\n  For more details, see `help(tf.TensorShape.dims)`.\n\n  Args:\n    x: object representing a shape; convertible to `tf.TensorShape`.\n\n  Returns:\n    shape_as_list: list of sizes or `None` values representing each\n      dimensions size if known. A size is `tf.Dimension` if input is a\n      `tf.TensorShape` and an `int` otherwise.\n  \"\"\"\n  if isinstance(x, tf.TensorShape):\n    return x.dims\n  r = tf.TensorShape(x).dims\n  return None if r is None else list(map(tf.compat.dimension_value, r))", "language": "python", "code": "def dims(x):\n  \"\"\"Returns a list of dimension sizes, or `None` if `rank` is unknown.\n\n  For more details, see `help(tf.TensorShape.dims)`.\n\n  Args:\n    x: object representing a shape; convertible to `tf.TensorShape`.\n\n  Returns:\n    shape_as_list: list of sizes or `None` values representing each\n      dimensions size if known. A size is `tf.Dimension` if input is a\n      `tf.TensorShape` and an `int` otherwise.\n  \"\"\"\n  if isinstance(x, tf.TensorShape):\n    return x.dims\n  r = tf.TensorShape(x).dims\n  return None if r is None else list(map(tf.compat.dimension_value, r))", "code_tokens": ["def", "dims", "(", "x", ")", ":", "if", "isinstance", "(", "x", ",", "tf", ".", "TensorShape", ")", ":", "return", "x", ".", "dims", "r", "=", "tf", ".", "TensorShape", "(", "x", ")", ".", "dims", "return", "None", "if", "r", "is", "None", "else", "list", "(", "map", "(", "tf", ".", "compat", ".", "dimension_value", ",", "r", ")", ")"], "docstring": "Returns a list of dimension sizes, or `None` if `rank` is unknown.\n\n  For more details, see `help(tf.TensorShape.dims)`.\n\n  Args:\n    x: object representing a shape; convertible to `tf.TensorShape`.\n\n  Returns:\n    shape_as_list: list of sizes or `None` values representing each\n      dimensions size if known. A size is `tf.Dimension` if input is a\n      `tf.TensorShape` and an `int` otherwise.", "docstring_tokens": ["Returns", "a", "list", "of", "dimension", "sizes", "or", "None", "if", "rank", "is", "unknown", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/tensorshape_util.py#L144-L160", "partition": "test", "index": 935, "time": "2019-04-11 15:42:42"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/tensorshape_util.py", "func_name": "merge_with", "original_string": "def merge_with(x, other):\n  \"\"\"Returns a shape combining the information in `x` and `other`.\n\n  The dimensions in `x` and `other` are merged elementwise, according to the\n  rules defined for `tf.Dimension.merge_with()`.\n\n  For more details, see `help(tf.TensorShape.merge_with)`.\n\n  Args:\n    x: object representing a shape; convertible to `tf.TensorShape`.\n    other: object representing a shape; convertible to `tf.TensorShape`.\n\n  Returns:\n    merged_shape: shape having `type(x)` containing the combined information of\n      `x` and `other`.\n\n  Raises:\n    ValueError: If `x` and `other` are not compatible.\n  \"\"\"\n  return type(x)(tf.TensorShape(x).merge_with(other))", "language": "python", "code": "def merge_with(x, other):\n  \"\"\"Returns a shape combining the information in `x` and `other`.\n\n  The dimensions in `x` and `other` are merged elementwise, according to the\n  rules defined for `tf.Dimension.merge_with()`.\n\n  For more details, see `help(tf.TensorShape.merge_with)`.\n\n  Args:\n    x: object representing a shape; convertible to `tf.TensorShape`.\n    other: object representing a shape; convertible to `tf.TensorShape`.\n\n  Returns:\n    merged_shape: shape having `type(x)` containing the combined information of\n      `x` and `other`.\n\n  Raises:\n    ValueError: If `x` and `other` are not compatible.\n  \"\"\"\n  return type(x)(tf.TensorShape(x).merge_with(other))", "code_tokens": ["def", "merge_with", "(", "x", ",", "other", ")", ":", "return", "type", "(", "x", ")", "(", "tf", ".", "TensorShape", "(", "x", ")", ".", "merge_with", "(", "other", ")", ")"], "docstring": "Returns a shape combining the information in `x` and `other`.\n\n  The dimensions in `x` and `other` are merged elementwise, according to the\n  rules defined for `tf.Dimension.merge_with()`.\n\n  For more details, see `help(tf.TensorShape.merge_with)`.\n\n  Args:\n    x: object representing a shape; convertible to `tf.TensorShape`.\n    other: object representing a shape; convertible to `tf.TensorShape`.\n\n  Returns:\n    merged_shape: shape having `type(x)` containing the combined information of\n      `x` and `other`.\n\n  Raises:\n    ValueError: If `x` and `other` are not compatible.", "docstring_tokens": ["Returns", "a", "shape", "combining", "the", "information", "in", "x", "and", "other", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/tensorshape_util.py#L192-L211", "partition": "test", "index": 936, "time": "2019-04-11 15:42:42"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/tensorshape_util.py", "func_name": "with_rank_at_least", "original_string": "def with_rank_at_least(x, rank):  # pylint: disable=redefined-outer-name\n  \"\"\"Returns a shape based on `x` with at least the given `rank`.\n\n  For more details, see `help(tf.TensorShape.with_rank_at_least)`.\n\n  Args:\n    x: object representing a shape; convertible to `tf.TensorShape`.\n    rank: An `int` representing the minimum rank of `x` or else an assertion is\n      raised.\n\n  Returns:\n    shape: a shape having `type(x)` but guaranteed to have at least the given\n      rank (or else an assertion was raised).\n\n  Raises:\n    ValueError: If `x` does not represent a shape with at least the given\n      `rank`.\n  \"\"\"\n  return type(x)(tf.TensorShape(x).with_rank_at_least(rank))", "language": "python", "code": "def with_rank_at_least(x, rank):  # pylint: disable=redefined-outer-name\n  \"\"\"Returns a shape based on `x` with at least the given `rank`.\n\n  For more details, see `help(tf.TensorShape.with_rank_at_least)`.\n\n  Args:\n    x: object representing a shape; convertible to `tf.TensorShape`.\n    rank: An `int` representing the minimum rank of `x` or else an assertion is\n      raised.\n\n  Returns:\n    shape: a shape having `type(x)` but guaranteed to have at least the given\n      rank (or else an assertion was raised).\n\n  Raises:\n    ValueError: If `x` does not represent a shape with at least the given\n      `rank`.\n  \"\"\"\n  return type(x)(tf.TensorShape(x).with_rank_at_least(rank))", "code_tokens": ["def", "with_rank_at_least", "(", "x", ",", "rank", ")", ":", "# pylint: disable=redefined-outer-name", "return", "type", "(", "x", ")", "(", "tf", ".", "TensorShape", "(", "x", ")", ".", "with_rank_at_least", "(", "rank", ")", ")"], "docstring": "Returns a shape based on `x` with at least the given `rank`.\n\n  For more details, see `help(tf.TensorShape.with_rank_at_least)`.\n\n  Args:\n    x: object representing a shape; convertible to `tf.TensorShape`.\n    rank: An `int` representing the minimum rank of `x` or else an assertion is\n      raised.\n\n  Returns:\n    shape: a shape having `type(x)` but guaranteed to have at least the given\n      rank (or else an assertion was raised).\n\n  Raises:\n    ValueError: If `x` does not represent a shape with at least the given\n      `rank`.", "docstring_tokens": ["Returns", "a", "shape", "based", "on", "x", "with", "at", "least", "the", "given", "rank", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/tensorshape_util.py#L285-L303", "partition": "test", "index": 937, "time": "2019-04-11 15:42:42"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/positive_semidefinite_kernels/internal/util.py", "func_name": "maybe_get_common_dtype", "original_string": "def maybe_get_common_dtype(arg_list):\n  \"\"\"Return common dtype of arg_list, or None.\n\n  Args:\n    arg_list: an iterable of items which are either `None` or have a `dtype`\n      property.\n\n  Returns:\n    dtype: The common dtype of items in `arg_list`, or `None` if the list is\n      empty or all items are `None`.\n  \"\"\"\n  # Note that `all` defaults to `True` if `arg_list` is empty.\n  if all(a is None for a in arg_list):\n    return None\n  return dtype_util.common_dtype(arg_list, tf.float32)", "language": "python", "code": "def maybe_get_common_dtype(arg_list):\n  \"\"\"Return common dtype of arg_list, or None.\n\n  Args:\n    arg_list: an iterable of items which are either `None` or have a `dtype`\n      property.\n\n  Returns:\n    dtype: The common dtype of items in `arg_list`, or `None` if the list is\n      empty or all items are `None`.\n  \"\"\"\n  # Note that `all` defaults to `True` if `arg_list` is empty.\n  if all(a is None for a in arg_list):\n    return None\n  return dtype_util.common_dtype(arg_list, tf.float32)", "code_tokens": ["def", "maybe_get_common_dtype", "(", "arg_list", ")", ":", "# Note that `all` defaults to `True` if `arg_list` is empty.", "if", "all", "(", "a", "is", "None", "for", "a", "in", "arg_list", ")", ":", "return", "None", "return", "dtype_util", ".", "common_dtype", "(", "arg_list", ",", "tf", ".", "float32", ")"], "docstring": "Return common dtype of arg_list, or None.\n\n  Args:\n    arg_list: an iterable of items which are either `None` or have a `dtype`\n      property.\n\n  Returns:\n    dtype: The common dtype of items in `arg_list`, or `None` if the list is\n      empty or all items are `None`.", "docstring_tokens": ["Return", "common", "dtype", "of", "arg_list", "or", "None", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/positive_semidefinite_kernels/internal/util.py#L157-L171", "partition": "test", "index": 1071, "time": "2019-04-12 10:35:41"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/backend/numpy/math.py", "func_name": "_max_mask_non_finite", "original_string": "def _max_mask_non_finite(x, axis=-1, keepdims=False, mask=0):\n  \"\"\"Returns `max` or `mask` if `max` is not finite.\"\"\"\n  m = np.max(x, axis=_astuple(axis), keepdims=keepdims)\n  needs_masking = ~np.isfinite(m)\n  if needs_masking.ndim > 0:\n    m[needs_masking] = mask\n  elif needs_masking:\n    m = mask\n  return m", "language": "python", "code": "def _max_mask_non_finite(x, axis=-1, keepdims=False, mask=0):\n  \"\"\"Returns `max` or `mask` if `max` is not finite.\"\"\"\n  m = np.max(x, axis=_astuple(axis), keepdims=keepdims)\n  needs_masking = ~np.isfinite(m)\n  if needs_masking.ndim > 0:\n    m[needs_masking] = mask\n  elif needs_masking:\n    m = mask\n  return m", "code_tokens": ["def", "_max_mask_non_finite", "(", "x", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "False", ",", "mask", "=", "0", ")", ":", "m", "=", "np", ".", "max", "(", "x", ",", "axis", "=", "_astuple", "(", "axis", ")", ",", "keepdims", "=", "keepdims", ")", "needs_masking", "=", "~", "np", ".", "isfinite", "(", "m", ")", "if", "needs_masking", ".", "ndim", ">", "0", ":", "m", "[", "needs_masking", "]", "=", "mask", "elif", "needs_masking", ":", "m", "=", "mask", "return", "m"], "docstring": "Returns `max` or `mask` if `max` is not finite.", "docstring_tokens": ["Returns", "max", "or", "mask", "if", "max", "is", "not", "finite", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/backend/numpy/math.py#L172-L180", "partition": "test", "index": 621, "time": "2019-04-15 11:33:02"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/assert_util.py", "func_name": "assert_finite", "original_string": "def assert_finite(x, data=None, summarize=None, message=None, name=None):\n  \"\"\"Assert all elements of `x` are finite.\n\n  Args:\n    x:  Numeric `Tensor`.\n    data:  The tensors to print out if the condition is False.  Defaults to\n      error message and first few entries of `x`.\n    summarize: Print this many entries of each tensor.\n    message: A string to prefix to the default message.\n    name: A name for this operation (optional).\n      Defaults to \"assert_finite\".\n\n  Returns:\n    Op raising `InvalidArgumentError` unless `x` has specified rank or lower.\n    If static checks determine `x` has correct rank, a `no_op` is returned.\n\n  Raises:\n    ValueError:  If static checks determine `x` has wrong rank.\n  \"\"\"\n  with tf.compat.v2.name_scope(name or 'assert_finite'):\n    x_ = tf.get_static_value(x)\n    if x_ is not None:\n      if ~np.all(np.isfinite(x_)):\n        raise ValueError(message)\n      return x\n    assertion = tf.compat.v1.assert_equal(\n        tf.math.is_finite(x), tf.ones_like(x, tf.bool),\n        data=data, summarize=summarize, message=message)\n    with tf.control_dependencies([assertion]):\n      return tf.identity(x)", "language": "python", "code": "def assert_finite(x, data=None, summarize=None, message=None, name=None):\n  \"\"\"Assert all elements of `x` are finite.\n\n  Args:\n    x:  Numeric `Tensor`.\n    data:  The tensors to print out if the condition is False.  Defaults to\n      error message and first few entries of `x`.\n    summarize: Print this many entries of each tensor.\n    message: A string to prefix to the default message.\n    name: A name for this operation (optional).\n      Defaults to \"assert_finite\".\n\n  Returns:\n    Op raising `InvalidArgumentError` unless `x` has specified rank or lower.\n    If static checks determine `x` has correct rank, a `no_op` is returned.\n\n  Raises:\n    ValueError:  If static checks determine `x` has wrong rank.\n  \"\"\"\n  with tf.compat.v2.name_scope(name or 'assert_finite'):\n    x_ = tf.get_static_value(x)\n    if x_ is not None:\n      if ~np.all(np.isfinite(x_)):\n        raise ValueError(message)\n      return x\n    assertion = tf.compat.v1.assert_equal(\n        tf.math.is_finite(x), tf.ones_like(x, tf.bool),\n        data=data, summarize=summarize, message=message)\n    with tf.control_dependencies([assertion]):\n      return tf.identity(x)", "code_tokens": ["def", "assert_finite", "(", "x", ",", "data", "=", "None", ",", "summarize", "=", "None", ",", "message", "=", "None", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v2", ".", "name_scope", "(", "name", "or", "'assert_finite'", ")", ":", "x_", "=", "tf", ".", "get_static_value", "(", "x", ")", "if", "x_", "is", "not", "None", ":", "if", "~", "np", ".", "all", "(", "np", ".", "isfinite", "(", "x_", ")", ")", ":", "raise", "ValueError", "(", "message", ")", "return", "x", "assertion", "=", "tf", ".", "compat", ".", "v1", ".", "assert_equal", "(", "tf", ".", "math", ".", "is_finite", "(", "x", ")", ",", "tf", ".", "ones_like", "(", "x", ",", "tf", ".", "bool", ")", ",", "data", "=", "data", ",", "summarize", "=", "summarize", ",", "message", "=", "message", ")", "with", "tf", ".", "control_dependencies", "(", "[", "assertion", "]", ")", ":", "return", "tf", ".", "identity", "(", "x", ")"], "docstring": "Assert all elements of `x` are finite.\n\n  Args:\n    x:  Numeric `Tensor`.\n    data:  The tensors to print out if the condition is False.  Defaults to\n      error message and first few entries of `x`.\n    summarize: Print this many entries of each tensor.\n    message: A string to prefix to the default message.\n    name: A name for this operation (optional).\n      Defaults to \"assert_finite\".\n\n  Returns:\n    Op raising `InvalidArgumentError` unless `x` has specified rank or lower.\n    If static checks determine `x` has correct rank, a `no_op` is returned.\n\n  Raises:\n    ValueError:  If static checks determine `x` has wrong rank.", "docstring_tokens": ["Assert", "all", "elements", "of", "x", "are", "finite", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/assert_util.py#L44-L73", "partition": "test", "index": 622, "time": "2019-04-15 16:18:29"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/dtype_util.py", "func_name": "_assert_same_base_type", "original_string": "def _assert_same_base_type(items, expected_type=None):\n  r\"\"\"Asserts all items are of the same base type.\n\n  Args:\n    items: List of graph items (e.g., `Variable`, `Tensor`, `SparseTensor`,\n        `Operation`, or `IndexedSlices`). Can include `None` elements, which\n        will be ignored.\n    expected_type: Expected type. If not specified, assert all items are\n        of the same base type.\n\n  Returns:\n    Validated type, or none if neither expected_type nor items provided.\n\n  Raises:\n    ValueError: If any types do not match.\n  \"\"\"\n  original_expected_type = expected_type\n  mismatch = False\n  for item in items:\n    if item is not None:\n      item_type = base_dtype(item.dtype)\n      if not expected_type:\n        expected_type = item_type\n      elif expected_type != item_type:\n        mismatch = True\n        break\n  if mismatch:\n    # Loop back through and build up an informative error message (this is very\n    # slow, so we don't do it unless we found an error above).\n    expected_type = original_expected_type\n    original_item_str = None\n    get_name = lambda x: x.name if hasattr(x, 'name') else str(x)\n    for item in items:\n      if item is not None:\n        item_type = base_dtype(item.dtype)\n        if not expected_type:\n          expected_type = item_type\n          original_item_str = get_name(item)\n        elif expected_type != item_type:\n          raise ValueError(\n              '{}, type={}, must be of the same type ({}){}.'.format(\n                  get_name(item),\n                  item_type,\n                  expected_type,\n                  ((' as {}'.format(original_item_str))\n                   if original_item_str else '')))\n    return expected_type  # Should be unreachable\n  else:\n    return expected_type", "language": "python", "code": "def _assert_same_base_type(items, expected_type=None):\n  r\"\"\"Asserts all items are of the same base type.\n\n  Args:\n    items: List of graph items (e.g., `Variable`, `Tensor`, `SparseTensor`,\n        `Operation`, or `IndexedSlices`). Can include `None` elements, which\n        will be ignored.\n    expected_type: Expected type. If not specified, assert all items are\n        of the same base type.\n\n  Returns:\n    Validated type, or none if neither expected_type nor items provided.\n\n  Raises:\n    ValueError: If any types do not match.\n  \"\"\"\n  original_expected_type = expected_type\n  mismatch = False\n  for item in items:\n    if item is not None:\n      item_type = base_dtype(item.dtype)\n      if not expected_type:\n        expected_type = item_type\n      elif expected_type != item_type:\n        mismatch = True\n        break\n  if mismatch:\n    # Loop back through and build up an informative error message (this is very\n    # slow, so we don't do it unless we found an error above).\n    expected_type = original_expected_type\n    original_item_str = None\n    get_name = lambda x: x.name if hasattr(x, 'name') else str(x)\n    for item in items:\n      if item is not None:\n        item_type = base_dtype(item.dtype)\n        if not expected_type:\n          expected_type = item_type\n          original_item_str = get_name(item)\n        elif expected_type != item_type:\n          raise ValueError(\n              '{}, type={}, must be of the same type ({}){}.'.format(\n                  get_name(item),\n                  item_type,\n                  expected_type,\n                  ((' as {}'.format(original_item_str))\n                   if original_item_str else '')))\n    return expected_type  # Should be unreachable\n  else:\n    return expected_type", "code_tokens": ["def", "_assert_same_base_type", "(", "items", ",", "expected_type", "=", "None", ")", ":", "original_expected_type", "=", "expected_type", "mismatch", "=", "False", "for", "item", "in", "items", ":", "if", "item", "is", "not", "None", ":", "item_type", "=", "base_dtype", "(", "item", ".", "dtype", ")", "if", "not", "expected_type", ":", "expected_type", "=", "item_type", "elif", "expected_type", "!=", "item_type", ":", "mismatch", "=", "True", "break", "if", "mismatch", ":", "# Loop back through and build up an informative error message (this is very", "# slow, so we don't do it unless we found an error above).", "expected_type", "=", "original_expected_type", "original_item_str", "=", "None", "get_name", "=", "lambda", "x", ":", "x", ".", "name", "if", "hasattr", "(", "x", ",", "'name'", ")", "else", "str", "(", "x", ")", "for", "item", "in", "items", ":", "if", "item", "is", "not", "None", ":", "item_type", "=", "base_dtype", "(", "item", ".", "dtype", ")", "if", "not", "expected_type", ":", "expected_type", "=", "item_type", "original_item_str", "=", "get_name", "(", "item", ")", "elif", "expected_type", "!=", "item_type", ":", "raise", "ValueError", "(", "'{}, type={}, must be of the same type ({}){}.'", ".", "format", "(", "get_name", "(", "item", ")", ",", "item_type", ",", "expected_type", ",", "(", "(", "' as {}'", ".", "format", "(", "original_item_str", ")", ")", "if", "original_item_str", "else", "''", ")", ")", ")", "return", "expected_type", "# Should be unreachable", "else", ":", "return", "expected_type"], "docstring": "r\"\"\"Asserts all items are of the same base type.\n\n  Args:\n    items: List of graph items (e.g., `Variable`, `Tensor`, `SparseTensor`,\n        `Operation`, or `IndexedSlices`). Can include `None` elements, which\n        will be ignored.\n    expected_type: Expected type. If not specified, assert all items are\n        of the same base type.\n\n  Returns:\n    Validated type, or none if neither expected_type nor items provided.\n\n  Raises:\n    ValueError: If any types do not match.", "docstring_tokens": ["r", "Asserts", "all", "items", "are", "of", "the", "same", "base", "type", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/dtype_util.py#L150-L198", "partition": "test", "index": 856, "time": "2019-04-16 11:55:46"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/dtype_util.py", "func_name": "assert_same_float_dtype", "original_string": "def assert_same_float_dtype(tensors=None, dtype=None):\n  \"\"\"Validate and return float type based on `tensors` and `dtype`.\n\n  For ops such as matrix multiplication, inputs and weights must be of the\n  same float type. This function validates that all `tensors` are the same type,\n  validates that type is `dtype` (if supplied), and returns the type. Type must\n  be a floating point type. If neither `tensors` nor `dtype` is supplied,\n  the function will return `dtypes.float32`.\n\n  Args:\n    tensors: Tensors of input values. Can include `None` elements, which will\n      be ignored.\n    dtype: Expected type.\n\n  Returns:\n    Validated type.\n\n  Raises:\n    ValueError: if neither `tensors` nor `dtype` is supplied, or result is not\n      float, or the common type of the inputs is not a floating point type.\n  \"\"\"\n  if tensors:\n    dtype = _assert_same_base_type(tensors, dtype)\n  if not dtype:\n    dtype = tf.float32\n  elif not is_floating(dtype):\n    raise ValueError('Expected floating point type, got {}.'.format(dtype))\n  return dtype", "language": "python", "code": "def assert_same_float_dtype(tensors=None, dtype=None):\n  \"\"\"Validate and return float type based on `tensors` and `dtype`.\n\n  For ops such as matrix multiplication, inputs and weights must be of the\n  same float type. This function validates that all `tensors` are the same type,\n  validates that type is `dtype` (if supplied), and returns the type. Type must\n  be a floating point type. If neither `tensors` nor `dtype` is supplied,\n  the function will return `dtypes.float32`.\n\n  Args:\n    tensors: Tensors of input values. Can include `None` elements, which will\n      be ignored.\n    dtype: Expected type.\n\n  Returns:\n    Validated type.\n\n  Raises:\n    ValueError: if neither `tensors` nor `dtype` is supplied, or result is not\n      float, or the common type of the inputs is not a floating point type.\n  \"\"\"\n  if tensors:\n    dtype = _assert_same_base_type(tensors, dtype)\n  if not dtype:\n    dtype = tf.float32\n  elif not is_floating(dtype):\n    raise ValueError('Expected floating point type, got {}.'.format(dtype))\n  return dtype", "code_tokens": ["def", "assert_same_float_dtype", "(", "tensors", "=", "None", ",", "dtype", "=", "None", ")", ":", "if", "tensors", ":", "dtype", "=", "_assert_same_base_type", "(", "tensors", ",", "dtype", ")", "if", "not", "dtype", ":", "dtype", "=", "tf", ".", "float32", "elif", "not", "is_floating", "(", "dtype", ")", ":", "raise", "ValueError", "(", "'Expected floating point type, got {}.'", ".", "format", "(", "dtype", ")", ")", "return", "dtype"], "docstring": "Validate and return float type based on `tensors` and `dtype`.\n\n  For ops such as matrix multiplication, inputs and weights must be of the\n  same float type. This function validates that all `tensors` are the same type,\n  validates that type is `dtype` (if supplied), and returns the type. Type must\n  be a floating point type. If neither `tensors` nor `dtype` is supplied,\n  the function will return `dtypes.float32`.\n\n  Args:\n    tensors: Tensors of input values. Can include `None` elements, which will\n      be ignored.\n    dtype: Expected type.\n\n  Returns:\n    Validated type.\n\n  Raises:\n    ValueError: if neither `tensors` nor `dtype` is supplied, or result is not\n      float, or the common type of the inputs is not a floating point type.", "docstring_tokens": ["Validate", "and", "return", "float", "type", "based", "on", "tensors", "and", "dtype", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/dtype_util.py#L201-L228", "partition": "test", "index": 857, "time": "2019-04-16 11:55:46"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/backend/numpy/internal/utils.py", "func_name": "common_dtype", "original_string": "def common_dtype(args_list, preferred_dtype=None):\n  \"\"\"Returns explict dtype from `args_list` if exists, else preferred_dtype.\"\"\"\n  dtype = None\n  preferred_dtype = (None if preferred_dtype is None\n                     else tf.as_dtype(preferred_dtype))\n  for a in tf.nest.flatten(args_list):\n    if hasattr(a, 'dtype'):\n      dt = tf.as_dtype(a.dtype)\n    else:\n      continue\n    if dtype is None:\n      dtype = dt\n    elif dtype != dt:\n      raise TypeError('Found incompatible dtypes, {} and {}.'.format(dtype, dt))\n  if dtype is None and preferred_dtype is None:\n    return None\n  return (preferred_dtype if dtype is None else dtype).as_numpy_dtype", "language": "python", "code": "def common_dtype(args_list, preferred_dtype=None):\n  \"\"\"Returns explict dtype from `args_list` if exists, else preferred_dtype.\"\"\"\n  dtype = None\n  preferred_dtype = (None if preferred_dtype is None\n                     else tf.as_dtype(preferred_dtype))\n  for a in tf.nest.flatten(args_list):\n    if hasattr(a, 'dtype'):\n      dt = tf.as_dtype(a.dtype)\n    else:\n      continue\n    if dtype is None:\n      dtype = dt\n    elif dtype != dt:\n      raise TypeError('Found incompatible dtypes, {} and {}.'.format(dtype, dt))\n  if dtype is None and preferred_dtype is None:\n    return None\n  return (preferred_dtype if dtype is None else dtype).as_numpy_dtype", "code_tokens": ["def", "common_dtype", "(", "args_list", ",", "preferred_dtype", "=", "None", ")", ":", "dtype", "=", "None", "preferred_dtype", "=", "(", "None", "if", "preferred_dtype", "is", "None", "else", "tf", ".", "as_dtype", "(", "preferred_dtype", ")", ")", "for", "a", "in", "tf", ".", "nest", ".", "flatten", "(", "args_list", ")", ":", "if", "hasattr", "(", "a", ",", "'dtype'", ")", ":", "dt", "=", "tf", ".", "as_dtype", "(", "a", ".", "dtype", ")", "else", ":", "continue", "if", "dtype", "is", "None", ":", "dtype", "=", "dt", "elif", "dtype", "!=", "dt", ":", "raise", "TypeError", "(", "'Found incompatible dtypes, {} and {}.'", ".", "format", "(", "dtype", ",", "dt", ")", ")", "if", "dtype", "is", "None", "and", "preferred_dtype", "is", "None", ":", "return", "None", "return", "(", "preferred_dtype", "if", "dtype", "is", "None", "else", "dtype", ")", ".", "as_numpy_dtype"], "docstring": "Returns explict dtype from `args_list` if exists, else preferred_dtype.", "docstring_tokens": ["Returns", "explict", "dtype", "from", "args_list", "if", "exists", "else", "preferred_dtype", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/backend/numpy/internal/utils.py#L58-L74", "partition": "test", "index": 777, "time": "2019-04-16 15:04:11"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/gaussian_process_regression_model.py", "func_name": "_validate_observation_data", "original_string": "def _validate_observation_data(\n    kernel, observation_index_points, observations):\n  \"\"\"Ensure that observation data and locations have consistent shapes.\n\n  This basically means that the batch shapes are broadcastable. We can only\n  ensure this when those shapes are fully statically defined.\n\n\n  Args:\n    kernel: The GP kernel.\n    observation_index_points: the observation data locations in the index set.\n    observations: the observation data.\n\n  Raises:\n    ValueError: if the observations' batch shapes are not broadcastable.\n  \"\"\"\n  # Check that observation index points and observation counts broadcast.\n  ndims = kernel.feature_ndims\n  if (tensorshape_util.is_fully_defined(\n      observation_index_points.shape[:-ndims]) and\n      tensorshape_util.is_fully_defined(observations.shape)):\n    index_point_count = observation_index_points.shape[:-ndims]\n    observation_count = observations.shape\n    try:\n      tf.broadcast_static_shape(index_point_count, observation_count)\n    except ValueError:\n      # Re-raise with our own more contextual error message.\n      raise ValueError(\n          'Observation index point and observation counts are not '\n          'broadcastable: {} and {}, respectively.'.format(\n              index_point_count, observation_count))", "language": "python", "code": "def _validate_observation_data(\n    kernel, observation_index_points, observations):\n  \"\"\"Ensure that observation data and locations have consistent shapes.\n\n  This basically means that the batch shapes are broadcastable. We can only\n  ensure this when those shapes are fully statically defined.\n\n\n  Args:\n    kernel: The GP kernel.\n    observation_index_points: the observation data locations in the index set.\n    observations: the observation data.\n\n  Raises:\n    ValueError: if the observations' batch shapes are not broadcastable.\n  \"\"\"\n  # Check that observation index points and observation counts broadcast.\n  ndims = kernel.feature_ndims\n  if (tensorshape_util.is_fully_defined(\n      observation_index_points.shape[:-ndims]) and\n      tensorshape_util.is_fully_defined(observations.shape)):\n    index_point_count = observation_index_points.shape[:-ndims]\n    observation_count = observations.shape\n    try:\n      tf.broadcast_static_shape(index_point_count, observation_count)\n    except ValueError:\n      # Re-raise with our own more contextual error message.\n      raise ValueError(\n          'Observation index point and observation counts are not '\n          'broadcastable: {} and {}, respectively.'.format(\n              index_point_count, observation_count))", "code_tokens": ["def", "_validate_observation_data", "(", "kernel", ",", "observation_index_points", ",", "observations", ")", ":", "# Check that observation index points and observation counts broadcast.", "ndims", "=", "kernel", ".", "feature_ndims", "if", "(", "tensorshape_util", ".", "is_fully_defined", "(", "observation_index_points", ".", "shape", "[", ":", "-", "ndims", "]", ")", "and", "tensorshape_util", ".", "is_fully_defined", "(", "observations", ".", "shape", ")", ")", ":", "index_point_count", "=", "observation_index_points", ".", "shape", "[", ":", "-", "ndims", "]", "observation_count", "=", "observations", ".", "shape", "try", ":", "tf", ".", "broadcast_static_shape", "(", "index_point_count", ",", "observation_count", ")", "except", "ValueError", ":", "# Re-raise with our own more contextual error message.", "raise", "ValueError", "(", "'Observation index point and observation counts are not '", "'broadcastable: {} and {}, respectively.'", ".", "format", "(", "index_point_count", ",", "observation_count", ")", ")"], "docstring": "Ensure that observation data and locations have consistent shapes.\n\n  This basically means that the batch shapes are broadcastable. We can only\n  ensure this when those shapes are fully statically defined.\n\n\n  Args:\n    kernel: The GP kernel.\n    observation_index_points: the observation data locations in the index set.\n    observations: the observation data.\n\n  Raises:\n    ValueError: if the observations' batch shapes are not broadcastable.", "docstring_tokens": ["Ensure", "that", "observation", "data", "and", "locations", "have", "consistent", "shapes", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/gaussian_process_regression_model.py#L73-L103", "partition": "test", "index": 1140, "time": "2019-04-18 12:46:59"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/gaussian_process_regression_model.py", "func_name": "_is_empty_observation_data", "original_string": "def _is_empty_observation_data(\n    feature_ndims, observation_index_points, observations):\n  \"\"\"Returns `True` if given observation data is empty.\n\n  Emptiness means either\n    1. Both `observation_index_points` and `observations` are `None`, or\n    2. the \"number of observations\" shape is 0. The shape of\n    `observation_index_points` is `[..., N, f1, ..., fF]`, where `N` is the\n    number of observations and the `f`s are feature dims. Thus, we look at the\n    shape element just to the left of the leftmost feature dim. If that shape is\n    zero, we consider the data empty.\n\n  We don't check the shape of observations; validations are checked elsewhere in\n  the calling code, to ensure these shapes are consistent.\n\n  Args:\n    feature_ndims: the number of feature dims, as reported by the GP kernel.\n    observation_index_points: the observation data locations in the index set.\n    observations: the observation data.\n\n  Returns:\n    is_empty: True if the data were deemed to be empty.\n  \"\"\"\n  # If both input locations and observations are `None`, we consider this\n  # \"empty\" observation data.\n  if observation_index_points is None and observations is None:\n    return True\n  num_obs = tf.compat.dimension_value(\n      observation_index_points.shape[-(feature_ndims + 1)])\n  if num_obs is not None and num_obs == 0:\n    return True\n  return False", "language": "python", "code": "def _is_empty_observation_data(\n    feature_ndims, observation_index_points, observations):\n  \"\"\"Returns `True` if given observation data is empty.\n\n  Emptiness means either\n    1. Both `observation_index_points` and `observations` are `None`, or\n    2. the \"number of observations\" shape is 0. The shape of\n    `observation_index_points` is `[..., N, f1, ..., fF]`, where `N` is the\n    number of observations and the `f`s are feature dims. Thus, we look at the\n    shape element just to the left of the leftmost feature dim. If that shape is\n    zero, we consider the data empty.\n\n  We don't check the shape of observations; validations are checked elsewhere in\n  the calling code, to ensure these shapes are consistent.\n\n  Args:\n    feature_ndims: the number of feature dims, as reported by the GP kernel.\n    observation_index_points: the observation data locations in the index set.\n    observations: the observation data.\n\n  Returns:\n    is_empty: True if the data were deemed to be empty.\n  \"\"\"\n  # If both input locations and observations are `None`, we consider this\n  # \"empty\" observation data.\n  if observation_index_points is None and observations is None:\n    return True\n  num_obs = tf.compat.dimension_value(\n      observation_index_points.shape[-(feature_ndims + 1)])\n  if num_obs is not None and num_obs == 0:\n    return True\n  return False", "code_tokens": ["def", "_is_empty_observation_data", "(", "feature_ndims", ",", "observation_index_points", ",", "observations", ")", ":", "# If both input locations and observations are `None`, we consider this", "# \"empty\" observation data.", "if", "observation_index_points", "is", "None", "and", "observations", "is", "None", ":", "return", "True", "num_obs", "=", "tf", ".", "compat", ".", "dimension_value", "(", "observation_index_points", ".", "shape", "[", "-", "(", "feature_ndims", "+", "1", ")", "]", ")", "if", "num_obs", "is", "not", "None", "and", "num_obs", "==", "0", ":", "return", "True", "return", "False"], "docstring": "Returns `True` if given observation data is empty.\n\n  Emptiness means either\n    1. Both `observation_index_points` and `observations` are `None`, or\n    2. the \"number of observations\" shape is 0. The shape of\n    `observation_index_points` is `[..., N, f1, ..., fF]`, where `N` is the\n    number of observations and the `f`s are feature dims. Thus, we look at the\n    shape element just to the left of the leftmost feature dim. If that shape is\n    zero, we consider the data empty.\n\n  We don't check the shape of observations; validations are checked elsewhere in\n  the calling code, to ensure these shapes are consistent.\n\n  Args:\n    feature_ndims: the number of feature dims, as reported by the GP kernel.\n    observation_index_points: the observation data locations in the index set.\n    observations: the observation data.\n\n  Returns:\n    is_empty: True if the data were deemed to be empty.", "docstring_tokens": ["Returns", "True", "if", "given", "observation", "data", "is", "empty", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/gaussian_process_regression_model.py#L39-L70", "partition": "test", "index": 1139, "time": "2019-04-18 12:46:59"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution_named.py", "func_name": "JointDistributionNamed._build", "original_string": "def _build(self, model):\n    \"\"\"Creates `dist_fn`, `dist_fn_wrapped`, `dist_fn_args`, `dist_fn_name`.\"\"\"\n    if not _is_dict_like(model):\n      raise TypeError('`model` must be convertible to `dict` (saw: {}).'.format(\n          type(model).__name__))\n    [\n        self._dist_fn,\n        self._dist_fn_wrapped,\n        self._dist_fn_args,\n        self._dist_fn_name,  # JointDistributionSequential doesn't have this.\n    ] = _prob_chain_rule_flatten(model)", "language": "python", "code": "def _build(self, model):\n    \"\"\"Creates `dist_fn`, `dist_fn_wrapped`, `dist_fn_args`, `dist_fn_name`.\"\"\"\n    if not _is_dict_like(model):\n      raise TypeError('`model` must be convertible to `dict` (saw: {}).'.format(\n          type(model).__name__))\n    [\n        self._dist_fn,\n        self._dist_fn_wrapped,\n        self._dist_fn_args,\n        self._dist_fn_name,  # JointDistributionSequential doesn't have this.\n    ] = _prob_chain_rule_flatten(model)", "code_tokens": ["def", "_build", "(", "self", ",", "model", ")", ":", "if", "not", "_is_dict_like", "(", "model", ")", ":", "raise", "TypeError", "(", "'`model` must be convertible to `dict` (saw: {}).'", ".", "format", "(", "type", "(", "model", ")", ".", "__name__", ")", ")", "[", "self", ".", "_dist_fn", ",", "self", ".", "_dist_fn_wrapped", ",", "self", ".", "_dist_fn_args", ",", "self", ".", "_dist_fn_name", ",", "# JointDistributionSequential doesn't have this.", "]", "=", "_prob_chain_rule_flatten", "(", "model", ")"], "docstring": "Creates `dist_fn`, `dist_fn_wrapped`, `dist_fn_args`, `dist_fn_name`.", "docstring_tokens": ["Creates", "dist_fn", "dist_fn_wrapped", "dist_fn_args", "dist_fn_name", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution_named.py#L172-L182", "partition": "test", "index": 1131, "time": "2019-04-18 14:33:43"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution_named.py", "func_name": "_prob_chain_rule_flatten", "original_string": "def _prob_chain_rule_flatten(named_makers):\n  \"\"\"Creates lists of callables suitable for JDSeq.\"\"\"\n  def _make(dist_fn, args):\n    if args is None:\n      return lambda *_: dist_fn\n    if not args:\n      return lambda *_: dist_fn()\n    def _fn(*xs):\n      kwargs = dict(zip(args, reversed(xs[-len(args):])))\n      kwargs.pop('_', None)\n      return dist_fn(**kwargs)\n    return _fn\n  named_makers = _convert_to_dict(named_makers)\n  g = {k: (None if distribution_util.is_distribution_instance(v)\n           else joint_distribution_sequential._get_required_args(v))  # pylint: disable=protected-access\n       for k, v in named_makers.items()}\n  g = _best_order(g)\n  dist_fn_name, dist_fn_args = zip(*g)\n  dist_fn_args = tuple(None if a is None else tuple(a) for a in dist_fn_args)\n  dist_fn_wrapped = tuple(_make(named_makers[name], parents)\n                          for (name, parents) in g)\n  dist_fn = tuple(named_makers.get(n) for n in dist_fn_name)\n  return dist_fn, dist_fn_wrapped, dist_fn_args, dist_fn_name", "language": "python", "code": "def _prob_chain_rule_flatten(named_makers):\n  \"\"\"Creates lists of callables suitable for JDSeq.\"\"\"\n  def _make(dist_fn, args):\n    if args is None:\n      return lambda *_: dist_fn\n    if not args:\n      return lambda *_: dist_fn()\n    def _fn(*xs):\n      kwargs = dict(zip(args, reversed(xs[-len(args):])))\n      kwargs.pop('_', None)\n      return dist_fn(**kwargs)\n    return _fn\n  named_makers = _convert_to_dict(named_makers)\n  g = {k: (None if distribution_util.is_distribution_instance(v)\n           else joint_distribution_sequential._get_required_args(v))  # pylint: disable=protected-access\n       for k, v in named_makers.items()}\n  g = _best_order(g)\n  dist_fn_name, dist_fn_args = zip(*g)\n  dist_fn_args = tuple(None if a is None else tuple(a) for a in dist_fn_args)\n  dist_fn_wrapped = tuple(_make(named_makers[name], parents)\n                          for (name, parents) in g)\n  dist_fn = tuple(named_makers.get(n) for n in dist_fn_name)\n  return dist_fn, dist_fn_wrapped, dist_fn_args, dist_fn_name", "code_tokens": ["def", "_prob_chain_rule_flatten", "(", "named_makers", ")", ":", "def", "_make", "(", "dist_fn", ",", "args", ")", ":", "if", "args", "is", "None", ":", "return", "lambda", "*", "_", ":", "dist_fn", "if", "not", "args", ":", "return", "lambda", "*", "_", ":", "dist_fn", "(", ")", "def", "_fn", "(", "*", "xs", ")", ":", "kwargs", "=", "dict", "(", "zip", "(", "args", ",", "reversed", "(", "xs", "[", "-", "len", "(", "args", ")", ":", "]", ")", ")", ")", "kwargs", ".", "pop", "(", "'_'", ",", "None", ")", "return", "dist_fn", "(", "*", "*", "kwargs", ")", "return", "_fn", "named_makers", "=", "_convert_to_dict", "(", "named_makers", ")", "g", "=", "{", "k", ":", "(", "None", "if", "distribution_util", ".", "is_distribution_instance", "(", "v", ")", "else", "joint_distribution_sequential", ".", "_get_required_args", "(", "v", ")", ")", "# pylint: disable=protected-access", "for", "k", ",", "v", "in", "named_makers", ".", "items", "(", ")", "}", "g", "=", "_best_order", "(", "g", ")", "dist_fn_name", ",", "dist_fn_args", "=", "zip", "(", "*", "g", ")", "dist_fn_args", "=", "tuple", "(", "None", "if", "a", "is", "None", "else", "tuple", "(", "a", ")", "for", "a", "in", "dist_fn_args", ")", "dist_fn_wrapped", "=", "tuple", "(", "_make", "(", "named_makers", "[", "name", "]", ",", "parents", ")", "for", "(", "name", ",", "parents", ")", "in", "g", ")", "dist_fn", "=", "tuple", "(", "named_makers", ".", "get", "(", "n", ")", "for", "n", "in", "dist_fn_name", ")", "return", "dist_fn", ",", "dist_fn_wrapped", ",", "dist_fn_args", ",", "dist_fn_name"], "docstring": "Creates lists of callables suitable for JDSeq.", "docstring_tokens": ["Creates", "lists", "of", "callables", "suitable", "for", "JDSeq", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution_named.py#L245-L267", "partition": "test", "index": 1130, "time": "2019-04-18 14:33:43"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution_sequential.py", "func_name": "JointDistributionSequential._build", "original_string": "def _build(self, model):\n    \"\"\"Creates `dist_fn`, `dist_fn_wrapped`, `dist_fn_args`.\"\"\"\n    if not isinstance(model, collections.Sequence):\n      raise TypeError('`model` must be `list`-like (saw: {}).'.format(\n          type(model).__name__))\n    self._dist_fn = model\n    self._dist_fn_wrapped, self._dist_fn_args = zip(*[\n        _unify_call_signature(i, dist_fn)\n        for i, dist_fn in enumerate(model)])", "language": "python", "code": "def _build(self, model):\n    \"\"\"Creates `dist_fn`, `dist_fn_wrapped`, `dist_fn_args`.\"\"\"\n    if not isinstance(model, collections.Sequence):\n      raise TypeError('`model` must be `list`-like (saw: {}).'.format(\n          type(model).__name__))\n    self._dist_fn = model\n    self._dist_fn_wrapped, self._dist_fn_args = zip(*[\n        _unify_call_signature(i, dist_fn)\n        for i, dist_fn in enumerate(model)])", "code_tokens": ["def", "_build", "(", "self", ",", "model", ")", ":", "if", "not", "isinstance", "(", "model", ",", "collections", ".", "Sequence", ")", ":", "raise", "TypeError", "(", "'`model` must be `list`-like (saw: {}).'", ".", "format", "(", "type", "(", "model", ")", ".", "__name__", ")", ")", "self", ".", "_dist_fn", "=", "model", "self", ".", "_dist_fn_wrapped", ",", "self", ".", "_dist_fn_args", "=", "zip", "(", "*", "[", "_unify_call_signature", "(", "i", ",", "dist_fn", ")", "for", "i", ",", "dist_fn", "in", "enumerate", "(", "model", ")", "]", ")"], "docstring": "Creates `dist_fn`, `dist_fn_wrapped`, `dist_fn_args`.", "docstring_tokens": ["Creates", "dist_fn", "dist_fn_wrapped", "dist_fn_args", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution_sequential.py#L210-L218", "partition": "test", "index": 716, "time": "2019-04-18 14:33:43"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution_named.py", "func_name": "_depth", "original_string": "def _depth(g):\n  \"\"\"Computes the number of edges on longest path from node to root.\"\"\"\n  def _explore(v):\n    if v.depth < 0:\n      v.depth = ((1 + max([-1] + [_explore(annotated_graph[u])\n                                  for u in v.parents]))\n                 if v.parents else 0)\n    return v.depth\n  annotated_graph = {k: _Node(k, v) for k, v in g.items()}\n  for v in annotated_graph.values():\n    _explore(v)\n  return annotated_graph", "language": "python", "code": "def _depth(g):\n  \"\"\"Computes the number of edges on longest path from node to root.\"\"\"\n  def _explore(v):\n    if v.depth < 0:\n      v.depth = ((1 + max([-1] + [_explore(annotated_graph[u])\n                                  for u in v.parents]))\n                 if v.parents else 0)\n    return v.depth\n  annotated_graph = {k: _Node(k, v) for k, v in g.items()}\n  for v in annotated_graph.values():\n    _explore(v)\n  return annotated_graph", "code_tokens": ["def", "_depth", "(", "g", ")", ":", "def", "_explore", "(", "v", ")", ":", "if", "v", ".", "depth", "<", "0", ":", "v", ".", "depth", "=", "(", "(", "1", "+", "max", "(", "[", "-", "1", "]", "+", "[", "_explore", "(", "annotated_graph", "[", "u", "]", ")", "for", "u", "in", "v", ".", "parents", "]", ")", ")", "if", "v", ".", "parents", "else", "0", ")", "return", "v", ".", "depth", "annotated_graph", "=", "{", "k", ":", "_Node", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "g", ".", "items", "(", ")", "}", "for", "v", "in", "annotated_graph", ".", "values", "(", ")", ":", "_explore", "(", "v", ")", "return", "annotated_graph"], "docstring": "Computes the number of edges on longest path from node to root.", "docstring_tokens": ["Computes", "the", "number", "of", "edges", "on", "longest", "path", "from", "node", "to", "root", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution_named.py#L204-L215", "partition": "test", "index": 1128, "time": "2019-04-18 14:33:43"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution_named.py", "func_name": "_best_order", "original_string": "def _best_order(g):\n  \"\"\"Creates tuple of str tuple-str pairs representing resolved & sorted DAG.\"\"\"\n  def _explore(u):\n    \"\"\"Recursive function to ascend up through unvisited dependencies.\"\"\"\n    if u.depth < 0:\n      return  # Already visited.\n    if not u.parents:\n      result.append((u.name, u.parents))\n      u.depth = -1  # Mark visited.\n      return\n    b = (u.name, [])\n    result.append(b)\n    u.depth = -1  # Mark visited.\n    d = 0\n    for v in sorted((g.get(p) for p in u.parents), key=lambda v: v.depth):\n      n0 = len(result)\n      _explore(v)\n      n1 = len(result)\n      b[1].extend(['_']*d + [v.name])\n      d = n1 - n0 - 1\n  g = _depth(g)\n  result = []\n  for u in sorted(g.values(), key=lambda v: v.depth, reverse=True):\n    _explore(u)\n  return tuple(reversed(result))", "language": "python", "code": "def _best_order(g):\n  \"\"\"Creates tuple of str tuple-str pairs representing resolved & sorted DAG.\"\"\"\n  def _explore(u):\n    \"\"\"Recursive function to ascend up through unvisited dependencies.\"\"\"\n    if u.depth < 0:\n      return  # Already visited.\n    if not u.parents:\n      result.append((u.name, u.parents))\n      u.depth = -1  # Mark visited.\n      return\n    b = (u.name, [])\n    result.append(b)\n    u.depth = -1  # Mark visited.\n    d = 0\n    for v in sorted((g.get(p) for p in u.parents), key=lambda v: v.depth):\n      n0 = len(result)\n      _explore(v)\n      n1 = len(result)\n      b[1].extend(['_']*d + [v.name])\n      d = n1 - n0 - 1\n  g = _depth(g)\n  result = []\n  for u in sorted(g.values(), key=lambda v: v.depth, reverse=True):\n    _explore(u)\n  return tuple(reversed(result))", "code_tokens": ["def", "_best_order", "(", "g", ")", ":", "def", "_explore", "(", "u", ")", ":", "\"\"\"Recursive function to ascend up through unvisited dependencies.\"\"\"", "if", "u", ".", "depth", "<", "0", ":", "return", "# Already visited.", "if", "not", "u", ".", "parents", ":", "result", ".", "append", "(", "(", "u", ".", "name", ",", "u", ".", "parents", ")", ")", "u", ".", "depth", "=", "-", "1", "# Mark visited.", "return", "b", "=", "(", "u", ".", "name", ",", "[", "]", ")", "result", ".", "append", "(", "b", ")", "u", ".", "depth", "=", "-", "1", "# Mark visited.", "d", "=", "0", "for", "v", "in", "sorted", "(", "(", "g", ".", "get", "(", "p", ")", "for", "p", "in", "u", ".", "parents", ")", ",", "key", "=", "lambda", "v", ":", "v", ".", "depth", ")", ":", "n0", "=", "len", "(", "result", ")", "_explore", "(", "v", ")", "n1", "=", "len", "(", "result", ")", "b", "[", "1", "]", ".", "extend", "(", "[", "'_'", "]", "*", "d", "+", "[", "v", ".", "name", "]", ")", "d", "=", "n1", "-", "n0", "-", "1", "g", "=", "_depth", "(", "g", ")", "result", "=", "[", "]", "for", "u", "in", "sorted", "(", "g", ".", "values", "(", ")", ",", "key", "=", "lambda", "v", ":", "v", ".", "depth", ",", "reverse", "=", "True", ")", ":", "_explore", "(", "u", ")", "return", "tuple", "(", "reversed", "(", "result", ")", ")"], "docstring": "Creates tuple of str tuple-str pairs representing resolved & sorted DAG.", "docstring_tokens": ["Creates", "tuple", "of", "str", "tuple", "-", "str", "pairs", "representing", "resolved", "&", "sorted", "DAG", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution_named.py#L218-L242", "partition": "test", "index": 1129, "time": "2019-04-18 14:33:43"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/nest_util.py", "func_name": "broadcast_structure", "original_string": "def broadcast_structure(to_structure, from_structure):\n  \"\"\"Broadcasts `from_structure` to `to_structure`.\n\n  This is useful for downstream usage of `zip` or `tf.nest.map_structure`.\n\n  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    to_structure: A structure.\n    from_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.\n\n  #### Example:\n\n  ```python\n  a_structure = ['a', 'b', 'c']\n  b_structure = broadcast_structure(a_structure, 'd')\n  # -> ['d', 'd', 'd']\n  c_structure = tf.nest.map_structure(\n      lambda a, b: a + b, a_structure, b_structure)\n  # -> ['ad', 'bd', 'cd']\n  ```\n  \"\"\"\n  from_parts = tf.nest.flatten(from_structure)\n  if len(from_parts) == 1:\n    from_structure = tf.nest.map_structure(lambda _: from_parts[0],\n                                           to_structure)\n  return from_structure", "language": "python", "code": "def broadcast_structure(to_structure, from_structure):\n  \"\"\"Broadcasts `from_structure` to `to_structure`.\n\n  This is useful for downstream usage of `zip` or `tf.nest.map_structure`.\n\n  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    to_structure: A structure.\n    from_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.\n\n  #### Example:\n\n  ```python\n  a_structure = ['a', 'b', 'c']\n  b_structure = broadcast_structure(a_structure, 'd')\n  # -> ['d', 'd', 'd']\n  c_structure = tf.nest.map_structure(\n      lambda a, b: a + b, a_structure, b_structure)\n  # -> ['ad', 'bd', 'cd']\n  ```\n  \"\"\"\n  from_parts = tf.nest.flatten(from_structure)\n  if len(from_parts) == 1:\n    from_structure = tf.nest.map_structure(lambda _: from_parts[0],\n                                           to_structure)\n  return from_structure", "code_tokens": ["def", "broadcast_structure", "(", "to_structure", ",", "from_structure", ")", ":", "from_parts", "=", "tf", ".", "nest", ".", "flatten", "(", "from_structure", ")", "if", "len", "(", "from_parts", ")", "==", "1", ":", "from_structure", "=", "tf", ".", "nest", ".", "map_structure", "(", "lambda", "_", ":", "from_parts", "[", "0", "]", ",", "to_structure", ")", "return", "from_structure"], "docstring": "Broadcasts `from_structure` to `to_structure`.\n\n  This is useful for downstream usage of `zip` or `tf.nest.map_structure`.\n\n  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    to_structure: A structure.\n    from_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.\n\n  #### Example:\n\n  ```python\n  a_structure = ['a', 'b', 'c']\n  b_structure = broadcast_structure(a_structure, 'd')\n  # -> ['d', 'd', 'd']\n  c_structure = tf.nest.map_structure(\n      lambda a, b: a + b, a_structure, b_structure)\n  # -> ['ad', 'bd', 'cd']\n  ```", "docstring_tokens": ["Broadcasts", "from_structure", "to", "to_structure", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/nest_util.py#L36-L67", "partition": "test", "index": 812, "time": "2019-04-18 17:33:17"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/nest_util.py", "func_name": "_nested_convert_to_tensor", "original_string": "def _nested_convert_to_tensor(struct, dtype=None, name=None):\n  \"\"\"Eagerly converts struct to Tensor, recursing upon failure.\"\"\"\n  if dtype is not None or not tf.nest.is_nested(struct):\n    return tf.convert_to_tensor(struct, dtype=dtype)\n\n  if _maybe_convertible_to_tensor(struct):\n    try:\n      # Try converting the structure wholesale.\n      return tf.convert_to_tensor(value=struct, name=name)\n    except (ValueError, TypeError):\n      # Unfortunately Eager/Graph mode don't agree on the error type.\n      pass\n  # Try converting all of its children.\n  shallow_struct = _get_shallow_structure(struct)\n  return nest.map_structure_up_to(\n      shallow_struct, lambda s: _nested_convert_to_tensor(s, name=name), struct)", "language": "python", "code": "def _nested_convert_to_tensor(struct, dtype=None, name=None):\n  \"\"\"Eagerly converts struct to Tensor, recursing upon failure.\"\"\"\n  if dtype is not None or not tf.nest.is_nested(struct):\n    return tf.convert_to_tensor(struct, dtype=dtype)\n\n  if _maybe_convertible_to_tensor(struct):\n    try:\n      # Try converting the structure wholesale.\n      return tf.convert_to_tensor(value=struct, name=name)\n    except (ValueError, TypeError):\n      # Unfortunately Eager/Graph mode don't agree on the error type.\n      pass\n  # Try converting all of its children.\n  shallow_struct = _get_shallow_structure(struct)\n  return nest.map_structure_up_to(\n      shallow_struct, lambda s: _nested_convert_to_tensor(s, name=name), struct)", "code_tokens": ["def", "_nested_convert_to_tensor", "(", "struct", ",", "dtype", "=", "None", ",", "name", "=", "None", ")", ":", "if", "dtype", "is", "not", "None", "or", "not", "tf", ".", "nest", ".", "is_nested", "(", "struct", ")", ":", "return", "tf", ".", "convert_to_tensor", "(", "struct", ",", "dtype", "=", "dtype", ")", "if", "_maybe_convertible_to_tensor", "(", "struct", ")", ":", "try", ":", "# Try converting the structure wholesale.", "return", "tf", ".", "convert_to_tensor", "(", "value", "=", "struct", ",", "name", "=", "name", ")", "except", "(", "ValueError", ",", "TypeError", ")", ":", "# Unfortunately Eager/Graph mode don't agree on the error type.", "pass", "# Try converting all of its children.", "shallow_struct", "=", "_get_shallow_structure", "(", "struct", ")", "return", "nest", ".", "map_structure_up_to", "(", "shallow_struct", ",", "lambda", "s", ":", "_nested_convert_to_tensor", "(", "s", ",", "name", "=", "name", ")", ",", "struct", ")"], "docstring": "Eagerly converts struct to Tensor, recursing upon failure.", "docstring_tokens": ["Eagerly", "converts", "struct", "to", "Tensor", "recursing", "upon", "failure", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/nest_util.py#L98-L113", "partition": "test", "index": 813, "time": "2019-04-18 17:33:17"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/nest_util.py", "func_name": "convert_args_to_tensor", "original_string": "def convert_args_to_tensor(args, dtype=None, name=None):\n  \"\"\"Converts `args` to `Tensor`s.\n\n  Use this when it is necessary to convert user-provided arguments that will\n  then be passed to user-provided callables.\n\n  When `dtype` is `None` this function behaves as follows:\n\n  1A. If the top-level structure is a `list`/`tuple` but not a `namedtuple`,\n      then it is left as is and only its elements are converted to `Tensor`s.\n\n  2A. The sub-structures are converted to `Tensor`s eagerly. E.g. if `args` is\n      `{'arg': [[1], [2]]}` it is converted to\n      `{'arg': tf.constant([[1], [2]])}`. If the conversion fails, it will\n      attempt to recurse into its children.\n\n  When `dtype` is specified, it acts as both a structural and numeric type\n  constraint. `dtype` can be a single `DType`, `None` or a nested collection\n  thereof. The conversion rule becomes as follows:\n\n  1B. The return value of this function will have the same structure as `dtype`.\n\n  2B. If the leaf of `dtype` is a concrete `DType`, then the corresponding\n      sub-structure in `args` is converted to a `Tensor`.\n\n  3B. If the leaf of `dtype` is `None`, then the corresponding sub-structure is\n      converted eagerly as described in the rule 2A above.\n\n  Args:\n    args: Arguments to convert to `Tensor`s.\n    dtype: Optional structure/numeric type constraint.\n    name: Optional name-scope to use.\n\n  Returns:\n    args: Converted `args`.\n\n  #### Examples.\n\n  This table shows some useful conversion cases. `T` means `Tensor`, `NT` means\n  `namedtuple` and `CNT` means a `namedtuple` with a `Tensor`-conversion\n  function registered.\n\n  |     args     |    dtype   |       output       |\n  |:------------:|:----------:|:------------------:|\n  | `{\"a\": 1}`   | `None`     | `{\"a\": T(1)}`      |\n  | `T(1)`       | `None`     | `T(1)`             |\n  | `[1]`        | `None`     | `[T(1)]`           |\n  | `[1]`        | `tf.int32` | `T([1])`           |\n  | `[[T(1)]]`   | `None`     | `[T([1])]`         |\n  | `[[T(1)]]`   | `[[None]]` | `[[T(1)]]`         |\n  | `NT(1, 2)`   | `None`     | `NT(T(1), T(2))`   |\n  | `NT(1, 2)`   | `tf.int32` | `T([1, 2])`        |\n  | `CNT(1, 2)`  | `None`     | `T(...)`           |\n  | `[[1, [2]]]` | `None`     | `[[T(1), T([2])]]` |\n\n  \"\"\"\n  if dtype is None:\n    if expand_as_args(args) or _expand_as_kwargs(args):\n      shallow_args = _get_shallow_structure(args)\n      return nest.map_structure_up_to(\n          shallow_args, lambda s: _nested_convert_to_tensor(s, name=name), args)\n    else:\n      return _nested_convert_to_tensor(args, name=name)\n  else:\n    return nest.map_structure_up_to(\n        dtype, lambda s, dtype: _nested_convert_to_tensor(s, dtype, name), args,\n        dtype)", "language": "python", "code": "def convert_args_to_tensor(args, dtype=None, name=None):\n  \"\"\"Converts `args` to `Tensor`s.\n\n  Use this when it is necessary to convert user-provided arguments that will\n  then be passed to user-provided callables.\n\n  When `dtype` is `None` this function behaves as follows:\n\n  1A. If the top-level structure is a `list`/`tuple` but not a `namedtuple`,\n      then it is left as is and only its elements are converted to `Tensor`s.\n\n  2A. The sub-structures are converted to `Tensor`s eagerly. E.g. if `args` is\n      `{'arg': [[1], [2]]}` it is converted to\n      `{'arg': tf.constant([[1], [2]])}`. If the conversion fails, it will\n      attempt to recurse into its children.\n\n  When `dtype` is specified, it acts as both a structural and numeric type\n  constraint. `dtype` can be a single `DType`, `None` or a nested collection\n  thereof. The conversion rule becomes as follows:\n\n  1B. The return value of this function will have the same structure as `dtype`.\n\n  2B. If the leaf of `dtype` is a concrete `DType`, then the corresponding\n      sub-structure in `args` is converted to a `Tensor`.\n\n  3B. If the leaf of `dtype` is `None`, then the corresponding sub-structure is\n      converted eagerly as described in the rule 2A above.\n\n  Args:\n    args: Arguments to convert to `Tensor`s.\n    dtype: Optional structure/numeric type constraint.\n    name: Optional name-scope to use.\n\n  Returns:\n    args: Converted `args`.\n\n  #### Examples.\n\n  This table shows some useful conversion cases. `T` means `Tensor`, `NT` means\n  `namedtuple` and `CNT` means a `namedtuple` with a `Tensor`-conversion\n  function registered.\n\n  |     args     |    dtype   |       output       |\n  |:------------:|:----------:|:------------------:|\n  | `{\"a\": 1}`   | `None`     | `{\"a\": T(1)}`      |\n  | `T(1)`       | `None`     | `T(1)`             |\n  | `[1]`        | `None`     | `[T(1)]`           |\n  | `[1]`        | `tf.int32` | `T([1])`           |\n  | `[[T(1)]]`   | `None`     | `[T([1])]`         |\n  | `[[T(1)]]`   | `[[None]]` | `[[T(1)]]`         |\n  | `NT(1, 2)`   | `None`     | `NT(T(1), T(2))`   |\n  | `NT(1, 2)`   | `tf.int32` | `T([1, 2])`        |\n  | `CNT(1, 2)`  | `None`     | `T(...)`           |\n  | `[[1, [2]]]` | `None`     | `[[T(1), T([2])]]` |\n\n  \"\"\"\n  if dtype is None:\n    if expand_as_args(args) or _expand_as_kwargs(args):\n      shallow_args = _get_shallow_structure(args)\n      return nest.map_structure_up_to(\n          shallow_args, lambda s: _nested_convert_to_tensor(s, name=name), args)\n    else:\n      return _nested_convert_to_tensor(args, name=name)\n  else:\n    return nest.map_structure_up_to(\n        dtype, lambda s, dtype: _nested_convert_to_tensor(s, dtype, name), args,\n        dtype)", "code_tokens": ["def", "convert_args_to_tensor", "(", "args", ",", "dtype", "=", "None", ",", "name", "=", "None", ")", ":", "if", "dtype", "is", "None", ":", "if", "expand_as_args", "(", "args", ")", "or", "_expand_as_kwargs", "(", "args", ")", ":", "shallow_args", "=", "_get_shallow_structure", "(", "args", ")", "return", "nest", ".", "map_structure_up_to", "(", "shallow_args", ",", "lambda", "s", ":", "_nested_convert_to_tensor", "(", "s", ",", "name", "=", "name", ")", ",", "args", ")", "else", ":", "return", "_nested_convert_to_tensor", "(", "args", ",", "name", "=", "name", ")", "else", ":", "return", "nest", ".", "map_structure_up_to", "(", "dtype", ",", "lambda", "s", ",", "dtype", ":", "_nested_convert_to_tensor", "(", "s", ",", "dtype", ",", "name", ")", ",", "args", ",", "dtype", ")"], "docstring": "Converts `args` to `Tensor`s.\n\n  Use this when it is necessary to convert user-provided arguments that will\n  then be passed to user-provided callables.\n\n  When `dtype` is `None` this function behaves as follows:\n\n  1A. If the top-level structure is a `list`/`tuple` but not a `namedtuple`,\n      then it is left as is and only its elements are converted to `Tensor`s.\n\n  2A. The sub-structures are converted to `Tensor`s eagerly. E.g. if `args` is\n      `{'arg': [[1], [2]]}` it is converted to\n      `{'arg': tf.constant([[1], [2]])}`. If the conversion fails, it will\n      attempt to recurse into its children.\n\n  When `dtype` is specified, it acts as both a structural and numeric type\n  constraint. `dtype` can be a single `DType`, `None` or a nested collection\n  thereof. The conversion rule becomes as follows:\n\n  1B. The return value of this function will have the same structure as `dtype`.\n\n  2B. If the leaf of `dtype` is a concrete `DType`, then the corresponding\n      sub-structure in `args` is converted to a `Tensor`.\n\n  3B. If the leaf of `dtype` is `None`, then the corresponding sub-structure is\n      converted eagerly as described in the rule 2A above.\n\n  Args:\n    args: Arguments to convert to `Tensor`s.\n    dtype: Optional structure/numeric type constraint.\n    name: Optional name-scope to use.\n\n  Returns:\n    args: Converted `args`.\n\n  #### Examples.\n\n  This table shows some useful conversion cases. `T` means `Tensor`, `NT` means\n  `namedtuple` and `CNT` means a `namedtuple` with a `Tensor`-conversion\n  function registered.\n\n  |     args     |    dtype   |       output       |\n  |:------------:|:----------:|:------------------:|\n  | `{\"a\": 1}`   | `None`     | `{\"a\": T(1)}`      |\n  | `T(1)`       | `None`     | `T(1)`             |\n  | `[1]`        | `None`     | `[T(1)]`           |\n  | `[1]`        | `tf.int32` | `T([1])`           |\n  | `[[T(1)]]`   | `None`     | `[T([1])]`         |\n  | `[[T(1)]]`   | `[[None]]` | `[[T(1)]]`         |\n  | `NT(1, 2)`   | `None`     | `NT(T(1), T(2))`   |\n  | `NT(1, 2)`   | `tf.int32` | `T([1, 2])`        |\n  | `CNT(1, 2)`  | `None`     | `T(...)`           |\n  | `[[1, [2]]]` | `None`     | `[[T(1), T([2])]]` |", "docstring_tokens": ["Converts", "args", "to", "Tensor", "s", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/nest_util.py#L116-L182", "partition": "test", "index": 814, "time": "2019-04-18 17:33:17"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/nest_util.py", "func_name": "call_fn", "original_string": "def call_fn(fn, args):\n  \"\"\"Calls `fn` with `args`, possibly expanding `args`.\n\n  Use this function when calling a user-provided callable using user-provided\n  arguments.\n\n  The expansion rules are as follows:\n\n  `fn(*args)` if `args` is a `list` or a `tuple`, but not a `namedtuple`.\n  `fn(**args)` if `args` is a `dict`.\n  `fn(args)` otherwise.\n\n  Args:\n    fn: A callable that takes either `args` as an argument(s).\n    args: Arguments to `fn`.\n\n  Returns:\n    result: Return value of `fn`.\n  \"\"\"\n\n  if expand_as_args(args):\n    return fn(*args)\n  elif _expand_as_kwargs(args):\n    return fn(**args)\n  else:\n    return fn(args)", "language": "python", "code": "def call_fn(fn, args):\n  \"\"\"Calls `fn` with `args`, possibly expanding `args`.\n\n  Use this function when calling a user-provided callable using user-provided\n  arguments.\n\n  The expansion rules are as follows:\n\n  `fn(*args)` if `args` is a `list` or a `tuple`, but not a `namedtuple`.\n  `fn(**args)` if `args` is a `dict`.\n  `fn(args)` otherwise.\n\n  Args:\n    fn: A callable that takes either `args` as an argument(s).\n    args: Arguments to `fn`.\n\n  Returns:\n    result: Return value of `fn`.\n  \"\"\"\n\n  if expand_as_args(args):\n    return fn(*args)\n  elif _expand_as_kwargs(args):\n    return fn(**args)\n  else:\n    return fn(args)", "code_tokens": ["def", "call_fn", "(", "fn", ",", "args", ")", ":", "if", "expand_as_args", "(", "args", ")", ":", "return", "fn", "(", "*", "args", ")", "elif", "_expand_as_kwargs", "(", "args", ")", ":", "return", "fn", "(", "*", "*", "args", ")", "else", ":", "return", "fn", "(", "args", ")"], "docstring": "Calls `fn` with `args`, possibly expanding `args`.\n\n  Use this function when calling a user-provided callable using user-provided\n  arguments.\n\n  The expansion rules are as follows:\n\n  `fn(*args)` if `args` is a `list` or a `tuple`, but not a `namedtuple`.\n  `fn(**args)` if `args` is a `dict`.\n  `fn(args)` otherwise.\n\n  Args:\n    fn: A callable that takes either `args` as an argument(s).\n    args: Arguments to `fn`.\n\n  Returns:\n    result: Return value of `fn`.", "docstring_tokens": ["Calls", "fn", "with", "args", "possibly", "expanding", "args", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/nest_util.py#L185-L210", "partition": "test", "index": 815, "time": "2019-04-18 17:33:17"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/gaussian_process.py", "func_name": "GaussianProcess._get_index_points", "original_string": "def _get_index_points(self, index_points=None):\n    \"\"\"Return `index_points` if not None, else `self._index_points`.\n\n    Args:\n      index_points: if given, this is what is returned; else,\n      `self._index_points`\n\n    Returns:\n      index_points: the given arg, if not None, else the class member\n      `self._index_points`.\n\n    Rases:\n      ValueError: if `index_points` and `self._index_points` are both `None`.\n    \"\"\"\n    if self._index_points is None and index_points is None:\n      raise ValueError(\n          'This GaussianProcess instance was not instantiated with a value for '\n          'index_points. One must therefore be provided when calling sample, '\n          'log_prob, and other such methods. In particular, one can\\'t compute '\n          'KL divergences to/from an instance of `GaussianProccess` with '\n          'unspecified `index_points` directly. Instead, use the '\n          '`get_marginal_distribution` function, which takes `index_points` as '\n          'an argument and returns a `Normal` or '\n          '`MultivariateNormalLinearOperator` instance, whose KL can be '\n          'computed.')\n    return index_points if index_points is not None else self._index_points", "language": "python", "code": "def _get_index_points(self, index_points=None):\n    \"\"\"Return `index_points` if not None, else `self._index_points`.\n\n    Args:\n      index_points: if given, this is what is returned; else,\n      `self._index_points`\n\n    Returns:\n      index_points: the given arg, if not None, else the class member\n      `self._index_points`.\n\n    Rases:\n      ValueError: if `index_points` and `self._index_points` are both `None`.\n    \"\"\"\n    if self._index_points is None and index_points is None:\n      raise ValueError(\n          'This GaussianProcess instance was not instantiated with a value for '\n          'index_points. One must therefore be provided when calling sample, '\n          'log_prob, and other such methods. In particular, one can\\'t compute '\n          'KL divergences to/from an instance of `GaussianProccess` with '\n          'unspecified `index_points` directly. Instead, use the '\n          '`get_marginal_distribution` function, which takes `index_points` as '\n          'an argument and returns a `Normal` or '\n          '`MultivariateNormalLinearOperator` instance, whose KL can be '\n          'computed.')\n    return index_points if index_points is not None else self._index_points", "code_tokens": ["def", "_get_index_points", "(", "self", ",", "index_points", "=", "None", ")", ":", "if", "self", ".", "_index_points", "is", "None", "and", "index_points", "is", "None", ":", "raise", "ValueError", "(", "'This GaussianProcess instance was not instantiated with a value for '", "'index_points. One must therefore be provided when calling sample, '", "'log_prob, and other such methods. In particular, one can\\'t compute '", "'KL divergences to/from an instance of `GaussianProccess` with '", "'unspecified `index_points` directly. Instead, use the '", "'`get_marginal_distribution` function, which takes `index_points` as '", "'an argument and returns a `Normal` or '", "'`MultivariateNormalLinearOperator` instance, whose KL can be '", "'computed.'", ")", "return", "index_points", "if", "index_points", "is", "not", "None", "else", "self", ".", "_index_points"], "docstring": "Return `index_points` if not None, else `self._index_points`.\n\n    Args:\n      index_points: if given, this is what is returned; else,\n      `self._index_points`\n\n    Returns:\n      index_points: the given arg, if not None, else the class member\n      `self._index_points`.\n\n    Rases:\n      ValueError: if `index_points` and `self._index_points` are both `None`.", "docstring_tokens": ["Return", "index_points", "if", "not", "None", "else", "self", ".", "_index_points", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/gaussian_process.py#L388-L413", "partition": "test", "index": 752, "time": "2019-04-19 10:27:51"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/joint_distribution_coroutine.py", "func_name": "JointDistributionCoroutine._flat_sample_distributions", "original_string": "def _flat_sample_distributions(self, sample_shape=(), seed=None, value=None):\n    \"\"\"Executes `model`, creating both samples and distributions.\"\"\"\n    ds = []\n    values_out = []\n    seed = seed_stream.SeedStream('JointDistributionCoroutine', seed)\n    gen = self._model()\n    index = 0\n    d = next(gen)\n    try:\n      while True:\n        actual_distribution = d.distribution if isinstance(d, self.Root) else d\n        ds.append(actual_distribution)\n        if (value is not None and len(value) > index and\n            value[index] is not None):\n          seed()\n          next_value = value[index]\n        else:\n          next_value = actual_distribution.sample(\n              sample_shape=sample_shape if isinstance(d, self.Root) else (),\n              seed=seed())\n        values_out.append(next_value)\n        index += 1\n        d = gen.send(next_value)\n    except StopIteration:\n      pass\n    return ds, values_out", "language": "python", "code": "def _flat_sample_distributions(self, sample_shape=(), seed=None, value=None):\n    \"\"\"Executes `model`, creating both samples and distributions.\"\"\"\n    ds = []\n    values_out = []\n    seed = seed_stream.SeedStream('JointDistributionCoroutine', seed)\n    gen = self._model()\n    index = 0\n    d = next(gen)\n    try:\n      while True:\n        actual_distribution = d.distribution if isinstance(d, self.Root) else d\n        ds.append(actual_distribution)\n        if (value is not None and len(value) > index and\n            value[index] is not None):\n          seed()\n          next_value = value[index]\n        else:\n          next_value = actual_distribution.sample(\n              sample_shape=sample_shape if isinstance(d, self.Root) else (),\n              seed=seed())\n        values_out.append(next_value)\n        index += 1\n        d = gen.send(next_value)\n    except StopIteration:\n      pass\n    return ds, values_out", "code_tokens": ["def", "_flat_sample_distributions", "(", "self", ",", "sample_shape", "=", "(", ")", ",", "seed", "=", "None", ",", "value", "=", "None", ")", ":", "ds", "=", "[", "]", "values_out", "=", "[", "]", "seed", "=", "seed_stream", ".", "SeedStream", "(", "'JointDistributionCoroutine'", ",", "seed", ")", "gen", "=", "self", ".", "_model", "(", ")", "index", "=", "0", "d", "=", "next", "(", "gen", ")", "try", ":", "while", "True", ":", "actual_distribution", "=", "d", ".", "distribution", "if", "isinstance", "(", "d", ",", "self", ".", "Root", ")", "else", "d", "ds", ".", "append", "(", "actual_distribution", ")", "if", "(", "value", "is", "not", "None", "and", "len", "(", "value", ")", ">", "index", "and", "value", "[", "index", "]", "is", "not", "None", ")", ":", "seed", "(", ")", "next_value", "=", "value", "[", "index", "]", "else", ":", "next_value", "=", "actual_distribution", ".", "sample", "(", "sample_shape", "=", "sample_shape", "if", "isinstance", "(", "d", ",", "self", ".", "Root", ")", "else", "(", ")", ",", "seed", "=", "seed", "(", ")", ")", "values_out", ".", "append", "(", "next_value", ")", "index", "+=", "1", "d", "=", "gen", ".", "send", "(", "next_value", ")", "except", "StopIteration", ":", "pass", "return", "ds", ",", "values_out"], "docstring": "Executes `model`, creating both samples and distributions.", "docstring_tokens": ["Executes", "model", "creating", "both", "samples", "and", "distributions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/joint_distribution_coroutine.py#L170-L195", "partition": "test", "index": 837, "time": "2019-04-19 16:25:06"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/distribution.py", "func_name": "_recursively_replace_dict_for_pretty_dict", "original_string": "def _recursively_replace_dict_for_pretty_dict(x):\n  \"\"\"Recursively replace `dict`s with `_PrettyDict`.\"\"\"\n  # We use \"PrettyDict\" because collections.OrderedDict repr/str has the word\n  # \"OrderedDict\" in it. We only want to print \"OrderedDict\" if in fact the\n  # input really is an OrderedDict.\n  if isinstance(x, dict):\n    return _PrettyDict({\n        k: _recursively_replace_dict_for_pretty_dict(v)\n        for k, v in x.items()})\n  if (isinstance(x, collections.Sequence) and\n      not isinstance(x, six.string_types)):\n    args = (_recursively_replace_dict_for_pretty_dict(x_) for x_ in x)\n    is_named_tuple = (isinstance(x, tuple) and\n                      hasattr(x, \"_asdict\") and\n                      hasattr(x, \"_fields\"))\n    return type(x)(*args) if is_named_tuple else type(x)(args)\n  if isinstance(x, collections.Mapping):\n    return type(x)(**{k: _recursively_replace_dict_for_pretty_dict(v)\n                      for k, v in x.items()})\n  return x", "language": "python", "code": "def _recursively_replace_dict_for_pretty_dict(x):\n  \"\"\"Recursively replace `dict`s with `_PrettyDict`.\"\"\"\n  # We use \"PrettyDict\" because collections.OrderedDict repr/str has the word\n  # \"OrderedDict\" in it. We only want to print \"OrderedDict\" if in fact the\n  # input really is an OrderedDict.\n  if isinstance(x, dict):\n    return _PrettyDict({\n        k: _recursively_replace_dict_for_pretty_dict(v)\n        for k, v in x.items()})\n  if (isinstance(x, collections.Sequence) and\n      not isinstance(x, six.string_types)):\n    args = (_recursively_replace_dict_for_pretty_dict(x_) for x_ in x)\n    is_named_tuple = (isinstance(x, tuple) and\n                      hasattr(x, \"_asdict\") and\n                      hasattr(x, \"_fields\"))\n    return type(x)(*args) if is_named_tuple else type(x)(args)\n  if isinstance(x, collections.Mapping):\n    return type(x)(**{k: _recursively_replace_dict_for_pretty_dict(v)\n                      for k, v in x.items()})\n  return x", "code_tokens": ["def", "_recursively_replace_dict_for_pretty_dict", "(", "x", ")", ":", "# We use \"PrettyDict\" because collections.OrderedDict repr/str has the word", "# \"OrderedDict\" in it. We only want to print \"OrderedDict\" if in fact the", "# input really is an OrderedDict.", "if", "isinstance", "(", "x", ",", "dict", ")", ":", "return", "_PrettyDict", "(", "{", "k", ":", "_recursively_replace_dict_for_pretty_dict", "(", "v", ")", "for", "k", ",", "v", "in", "x", ".", "items", "(", ")", "}", ")", "if", "(", "isinstance", "(", "x", ",", "collections", ".", "Sequence", ")", "and", "not", "isinstance", "(", "x", ",", "six", ".", "string_types", ")", ")", ":", "args", "=", "(", "_recursively_replace_dict_for_pretty_dict", "(", "x_", ")", "for", "x_", "in", "x", ")", "is_named_tuple", "=", "(", "isinstance", "(", "x", ",", "tuple", ")", "and", "hasattr", "(", "x", ",", "\"_asdict\"", ")", "and", "hasattr", "(", "x", ",", "\"_fields\"", ")", ")", "return", "type", "(", "x", ")", "(", "*", "args", ")", "if", "is_named_tuple", "else", "type", "(", "x", ")", "(", "args", ")", "if", "isinstance", "(", "x", ",", "collections", ".", "Mapping", ")", ":", "return", "type", "(", "x", ")", "(", "*", "*", "{", "k", ":", "_recursively_replace_dict_for_pretty_dict", "(", "v", ")", "for", "k", ",", "v", "in", "x", ".", "items", "(", ")", "}", ")", "return", "x"], "docstring": "Recursively replace `dict`s with `_PrettyDict`.", "docstring_tokens": ["Recursively", "replace", "dict", "s", "with", "_PrettyDict", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/distribution.py#L1392-L1411", "partition": "test", "index": 962, "time": "2019-04-23 13:51:54"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/finite_discrete.py", "func_name": "_maybe_validate_args", "original_string": "def _maybe_validate_args(outcomes, logits, probs, validate_args):\n  \"\"\"Validate `outcomes`, `logits` and `probs`'s shapes.\"\"\"\n  assertions = []\n\n  def validate_equal_last_dim(tensor_a, tensor_b, message):\n    if tensor_a.shape.is_fully_defined() and tensor_b.shape.is_fully_defined():\n      if tensor_a.shape[-1] != tensor_b.shape[-1]:\n        raise ValueError(message)\n    elif validate_args:\n      assertions.append(\n          tf.compat.v1.assert_equal(\n              tf.shape(input=tensor_a)[-1],\n              tf.shape(input=tensor_b)[-1],\n              message=message))\n\n  if logits is not None:\n    validate_equal_last_dim(\n        outcomes,\n        logits,\n        message='Last dimension of outcomes and logits must be equal size.')\n  if probs is not None:\n    validate_equal_last_dim(\n        outcomes,\n        probs,\n        message='Last dimension of outcomes and probs must be equal size.')\n\n  message = 'Rank of outcomes must be 1.'\n  if outcomes.shape.ndims is not None:\n    if outcomes.shape.ndims != 1:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(tf.compat.v1.assert_rank(outcomes, 1, message=message))\n\n  message = 'Size of outcomes must be greater than 0.'\n  if outcomes.shape.num_elements() is not None:\n    if outcomes.shape.num_elements() == 0:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        tf.compat.v1.assert_greater(\n            tf.size(input=outcomes), 0, message=message))\n\n  if validate_args:\n    assertions.append(\n        tf.compat.v1.assert_equal(\n            tf.math.is_strictly_increasing(outcomes),\n            True,\n            message='outcomes is not strictly increasing.'))\n\n  return assertions", "language": "python", "code": "def _maybe_validate_args(outcomes, logits, probs, validate_args):\n  \"\"\"Validate `outcomes`, `logits` and `probs`'s shapes.\"\"\"\n  assertions = []\n\n  def validate_equal_last_dim(tensor_a, tensor_b, message):\n    if tensor_a.shape.is_fully_defined() and tensor_b.shape.is_fully_defined():\n      if tensor_a.shape[-1] != tensor_b.shape[-1]:\n        raise ValueError(message)\n    elif validate_args:\n      assertions.append(\n          tf.compat.v1.assert_equal(\n              tf.shape(input=tensor_a)[-1],\n              tf.shape(input=tensor_b)[-1],\n              message=message))\n\n  if logits is not None:\n    validate_equal_last_dim(\n        outcomes,\n        logits,\n        message='Last dimension of outcomes and logits must be equal size.')\n  if probs is not None:\n    validate_equal_last_dim(\n        outcomes,\n        probs,\n        message='Last dimension of outcomes and probs must be equal size.')\n\n  message = 'Rank of outcomes must be 1.'\n  if outcomes.shape.ndims is not None:\n    if outcomes.shape.ndims != 1:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(tf.compat.v1.assert_rank(outcomes, 1, message=message))\n\n  message = 'Size of outcomes must be greater than 0.'\n  if outcomes.shape.num_elements() is not None:\n    if outcomes.shape.num_elements() == 0:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        tf.compat.v1.assert_greater(\n            tf.size(input=outcomes), 0, message=message))\n\n  if validate_args:\n    assertions.append(\n        tf.compat.v1.assert_equal(\n            tf.math.is_strictly_increasing(outcomes),\n            True,\n            message='outcomes is not strictly increasing.'))\n\n  return assertions", "code_tokens": ["def", "_maybe_validate_args", "(", "outcomes", ",", "logits", ",", "probs", ",", "validate_args", ")", ":", "assertions", "=", "[", "]", "def", "validate_equal_last_dim", "(", "tensor_a", ",", "tensor_b", ",", "message", ")", ":", "if", "tensor_a", ".", "shape", ".", "is_fully_defined", "(", ")", "and", "tensor_b", ".", "shape", ".", "is_fully_defined", "(", ")", ":", "if", "tensor_a", ".", "shape", "[", "-", "1", "]", "!=", "tensor_b", ".", "shape", "[", "-", "1", "]", ":", "raise", "ValueError", "(", "message", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "tf", ".", "compat", ".", "v1", ".", "assert_equal", "(", "tf", ".", "shape", "(", "input", "=", "tensor_a", ")", "[", "-", "1", "]", ",", "tf", ".", "shape", "(", "input", "=", "tensor_b", ")", "[", "-", "1", "]", ",", "message", "=", "message", ")", ")", "if", "logits", "is", "not", "None", ":", "validate_equal_last_dim", "(", "outcomes", ",", "logits", ",", "message", "=", "'Last dimension of outcomes and logits must be equal size.'", ")", "if", "probs", "is", "not", "None", ":", "validate_equal_last_dim", "(", "outcomes", ",", "probs", ",", "message", "=", "'Last dimension of outcomes and probs must be equal size.'", ")", "message", "=", "'Rank of outcomes must be 1.'", "if", "outcomes", ".", "shape", ".", "ndims", "is", "not", "None", ":", "if", "outcomes", ".", "shape", ".", "ndims", "!=", "1", ":", "raise", "ValueError", "(", "message", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "tf", ".", "compat", ".", "v1", ".", "assert_rank", "(", "outcomes", ",", "1", ",", "message", "=", "message", ")", ")", "message", "=", "'Size of outcomes must be greater than 0.'", "if", "outcomes", ".", "shape", ".", "num_elements", "(", ")", "is", "not", "None", ":", "if", "outcomes", ".", "shape", ".", "num_elements", "(", ")", "==", "0", ":", "raise", "ValueError", "(", "message", ")", "elif", "validate_args", ":", "assertions", ".", "append", "(", "tf", ".", "compat", ".", "v1", ".", "assert_greater", "(", "tf", ".", "size", "(", "input", "=", "outcomes", ")", ",", "0", ",", "message", "=", "message", ")", ")", "if", "validate_args", ":", "assertions", ".", "append", "(", "tf", ".", "compat", ".", "v1", ".", "assert_equal", "(", "tf", ".", "math", ".", "is_strictly_increasing", "(", "outcomes", ")", ",", "True", ",", "message", "=", "'outcomes is not strictly increasing.'", ")", ")", "return", "assertions"], "docstring": "Validate `outcomes`, `logits` and `probs`'s shapes.", "docstring_tokens": ["Validate", "outcomes", "logits", "and", "probs", "s", "shapes", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/finite_discrete.py#L248-L297", "partition": "test", "index": 652, "time": "2019-04-25 11:46:02"}
