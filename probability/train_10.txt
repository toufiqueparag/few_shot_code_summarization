def _get_samples ( dist , z , n , seed ) : with tf . compat . v1 . name_scope ( 'get_samples' , values = [ z , n ] ) : if ( n is None ) == ( z is None ) : raise ValueError ( 'Must specify exactly one of arguments "n" and "z".  Found: ' 'n = %s, z = %s' % ( n , z ) ) if n is not None : return dist . sample ( n , seed = seed ) else : return tf . convert_to_tensor ( value = z , name = 'z' )	<s> Check args and return samples . </s>
def multivariate_normal_tril ( x , dims , layer_fn = tf . compat . v1 . layers . dense , loc_fn = lambda x : x , scale_fn = tril_with_diag_softplus_and_shift , name = None ) : with tf . compat . v1 . name_scope ( name , 'multivariate_normal_tril' , [ x , dims ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) x = layer_fn ( x , dims + dims * ( dims + 1 ) // 2 ) return tfd . MultivariateNormalTriL ( loc = loc_fn ( x [ ... , : dims ] ) , scale_tril = scale_fn ( x [ ... , dims : ] ) )	<s> Constructs a trainable tfd . MultivariateNormalTriL distribution . </s>
def bernoulli ( x , layer_fn = tf . compat . v1 . layers . dense , name = None ) : with tf . compat . v1 . name_scope ( name , 'bernoulli' , [ x ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) logits = tf . squeeze ( layer_fn ( x , 1 ) , axis = - 1 ) return tfd . Bernoulli ( logits = logits )	<s> Constructs a trainable tfd . Bernoulli distribution . </s>
def triangular ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , "triangular" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = "logu" ) return pearson ( logu ) / ( 1. + tf . exp ( logu ) )	<s> The Triangular Csiszar - function in log - space . </s>
def squared_hellinger ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , "squared_hellinger" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = "logu" ) return pearson ( 0.5 * logu )	<s> The Squared - Hellinger Csiszar - function in log - space . </s>
def jensen_shannon ( logu , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , "jensen_shannon" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = "logu" ) npdt = logu . dtype . as_numpy_dtype y = tf . nn . softplus ( logu ) if self_normalized : y -= np . log ( 2 ) . astype ( npdt ) return tf . exp ( logu ) * logu - ( 1. + tf . exp ( logu ) ) * y	<s> The Jensen - Shannon Csiszar - function in log - space . </s>
def t_power ( logu , t , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , "t_power" , [ logu , t ] ) : logu = tf . convert_to_tensor ( value = logu , name = "logu" ) t = tf . convert_to_tensor ( value = t , dtype = logu . dtype . base_dtype , name = "t" ) fu = tf . math . expm1 ( t * logu ) if self_normalized : fu -= t * tf . math . expm1 ( logu ) fu *= tf . where ( tf . logical_and ( 0. < t , t < 1. ) , - tf . ones_like ( t ) , tf . ones_like ( t ) ) return fu	<s> The T - Power Csiszar - function in log - space . </s>
def log1p_abs ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , "log1p_abs" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = "logu" ) return tf . math . expm1 ( tf . abs ( logu ) )	<s> The log1p - abs Csiszar - function in log - space . </s>
def jeffreys ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , "jeffreys" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = "logu" ) return 0.5 * tf . math . expm1 ( logu ) * logu	<s> The Jeffreys Csiszar - function in log - space . </s>
def modified_gan ( logu , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , "chi_square" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = "logu" ) y = tf . nn . softplus ( logu ) - logu if self_normalized : y += 0.5 * tf . math . expm1 ( logu ) return y	<s> The Modified - GAN Csiszar - function in log - space . </s>
