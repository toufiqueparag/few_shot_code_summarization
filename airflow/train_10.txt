def get_event_buffer ( self , dag_ids = None ) : cleared_events = dict ( ) if dag_ids is None : cleared_events = self . event_buffer self . event_buffer = dict ( ) else : for key in list ( self . event_buffer . keys ( ) ) : dag_id , _ , _ , _ = key if dag_id in dag_ids : cleared_events [ key ] = self . event_buffer . pop ( key ) return cleared_events	<s> Returns and flush the event buffer . In case dag_ids is specified it will only return and flush events for the given dag_ids . Otherwise it returns and flushes all </s>
def get_conn ( self ) : conn = self . get_connection ( self . mysql_conn_id ) conn_config = { "user" : conn . login , "passwd" : conn . password or '' , "host" : conn . host or 'localhost' , "db" : self . schema or conn . schema or '' } if not conn . port : conn_config [ "port" ] = 3306 else : conn_config [ "port" ] = int ( conn . port ) if conn . extra_dejson . get ( 'charset' , False ) : conn_config [ "charset" ] = conn . extra_dejson [ "charset" ] if ( conn_config [ "charset" ] ) . lower ( ) == 'utf8' or ( conn_config [ "charset" ] ) . lower ( ) == 'utf-8' : conn_config [ "use_unicode" ] = True if conn . extra_dejson . get ( 'cursor' , False ) : if ( conn . extra_dejson [ "cursor" ] ) . lower ( ) == 'sscursor' : conn_config [ "cursorclass" ] = MySQLdb . cursors . SSCursor elif ( conn . extra_dejson [ "cursor" ] ) . lower ( ) == 'dictcursor' : conn_config [ "cursorclass" ] = MySQLdb . cursors . DictCursor elif ( conn . extra_dejson [ "cursor" ] ) . lower ( ) == 'ssdictcursor' : conn_config [ "cursorclass" ] = MySQLdb . cursors . SSDictCursor local_infile = conn . extra_dejson . get ( 'local_infile' , False ) if conn . extra_dejson . get ( 'ssl' , False ) : # SSL parameter for MySQL has to be a dictionary and in case # of extra/dejson we can get string if extra is passed via # URL parameters dejson_ssl = conn . extra_dejson [ 'ssl' ] if isinstance ( dejson_ssl , six . string_types ) : dejson_ssl = json . loads ( dejson_ssl ) conn_config [ 'ssl' ] = dejson_ssl if conn . extra_dejson . get ( 'unix_socket' ) : conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] if local_infile : conn_config [ "local_infile" ] = 1 conn = MySQLdb . connect ( * * conn_config ) return conn	<s> Returns a mysql connection object </s>
def max_partition ( table , schema = "default" , field = None , filter_map = None , metastore_conn_id = 'metastore_default' ) : from airflow . hooks . hive_hooks import HiveMetastoreHook if '.' in table : schema , table = table . split ( '.' ) hh = HiveMetastoreHook ( metastore_conn_id = metastore_conn_id ) return hh . max_partition ( schema = schema , table_name = table , field = field , filter_map = filter_map )	<s> Gets the max partition for a table . </s>
def get_metastore_client ( self ) : import hmsclient from thrift . transport import TSocket , TTransport from thrift . protocol import TBinaryProtocol ms = self . metastore_conn auth_mechanism = ms . extra_dejson . get ( 'authMechanism' , 'NOSASL' ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth_mechanism = ms . extra_dejson . get ( 'authMechanism' , 'GSSAPI' ) kerberos_service_name = ms . extra_dejson . get ( 'kerberos_service_name' , 'hive' ) socket = TSocket . TSocket ( ms . host , ms . port ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' and auth_mechanism == 'GSSAPI' : try : import saslwrapper as sasl except ImportError : import sasl def sasl_factory ( ) : sasl_client = sasl . Client ( ) sasl_client . setAttr ( "host" , ms . host ) sasl_client . setAttr ( "service" , kerberos_service_name ) sasl_client . init ( ) return sasl_client from thrift_sasl import TSaslClientTransport transport = TSaslClientTransport ( sasl_factory , "GSSAPI" , socket ) else : transport = TTransport . TBufferedTransport ( socket ) protocol = TBinaryProtocol . TBinaryProtocol ( transport ) return hmsclient . HMSClient ( iprot = protocol )	<s> Returns a Hive thrift client . </s>
def _task_instances_for_dag_run ( self , dag_run , session = None ) : tasks_to_run = { } if dag_run is None : return tasks_to_run # check if we have orphaned tasks self . reset_state_for_orphaned_tasks ( filter_by_dag_run = dag_run , session = session ) # for some reason if we don't refresh the reference to run is lost dag_run . refresh_from_db ( ) make_transient ( dag_run ) # TODO(edgarRd): AIRFLOW-1464 change to batch query to improve perf for ti in dag_run . get_task_instances ( ) : # all tasks part of the backfill are scheduled to run if ti . state == State . NONE : ti . set_state ( State . SCHEDULED , session = session ) if ti . state != State . REMOVED : tasks_to_run [ ti . key ] = ti return tasks_to_run	<s> Returns a map of task instance key to task instance object for the tasks to run in the given dag run . </s>
def _get_dag_run ( self , run_date , session = None ) : run_id = BackfillJob . ID_FORMAT_PREFIX . format ( run_date . isoformat ( ) ) # consider max_active_runs but ignore when running subdags respect_dag_max_active_limit = ( True if ( self . dag . schedule_interval and not self . dag . is_subdag ) else False ) current_active_dag_count = self . dag . get_num_active_runs ( external_trigger = False ) # check if we are scheduling on top of a already existing dag_run # we could find a "scheduled" run instead of a "backfill" run = DagRun . find ( dag_id = self . dag . dag_id , execution_date = run_date , session = session ) if run is not None and len ( run ) > 0 : run = run [ 0 ] if run . state == State . RUNNING : respect_dag_max_active_limit = False else : run = None # enforce max_active_runs limit for dag, special cases already # handled by respect_dag_max_active_limit if ( respect_dag_max_active_limit and current_active_dag_count >= self . dag . max_active_runs ) : return None run = run or self . dag . create_dagrun ( run_id = run_id , execution_date = run_date , start_date = timezone . utcnow ( ) , state = State . RUNNING , external_trigger = False , session = session , conf = self . conf , ) # set required transient field run . dag = self . dag # explicitly mark as backfill and running run . state = State . RUNNING run . run_id = run_id run . verify_integrity ( session = session ) return run	<s> Returns a dag run for the given run date which will be matched to an existing dag run if available or create a new dag run otherwise . If the max_active_runs limit is reached this function will return None . </s>
def heartbeat ( self ) : try : with create_session ( ) as session : job = session . query ( BaseJob ) . filter_by ( id = self . id ) . one ( ) make_transient ( job ) session . commit ( ) if job . state == State . SHUTDOWN : self . kill ( ) is_unit_test = conf . getboolean ( 'core' , 'unit_test_mode' ) if not is_unit_test : # Figure out how long to sleep for sleep_for = 0 if job . latest_heartbeat : seconds_remaining = self . heartrate - ( timezone . utcnow ( ) - job . latest_heartbeat ) . total_seconds ( ) sleep_for = max ( 0 , seconds_remaining ) sleep ( sleep_for ) # Update last heartbeat time with create_session ( ) as session : job = session . query ( BaseJob ) . filter ( BaseJob . id == self . id ) . first ( ) job . latest_heartbeat = timezone . utcnow ( ) session . merge ( job ) session . commit ( ) self . heartbeat_callback ( session = session ) self . log . debug ( '[heartbeat]' ) except OperationalError as e : self . log . error ( "Scheduler heartbeat got an exception: %s" , str ( e ) )	<s> Heartbeats update the job s entry in the database with a timestamp for the latest_heartbeat and allows for the job to be killed externally . This allows at the system level to monitor what is actually active . </s>
def ds_add ( ds , days ) : ds = datetime . strptime ( ds , '%Y-%m-%d' ) if days : ds = ds + timedelta ( days ) return ds . isoformat ( ) [ : 10 ]	<s> Add or subtract days from a YYYY - MM - DD </s>
def make_cache_key ( * args , * * kwargs ) : path = request . path args = str ( hash ( frozenset ( request . args . items ( ) ) ) ) return ( path + args ) . encode ( 'ascii' , 'ignore' )	<s> Used by cache to get a unique key per URL </s>
def run ( self , hql , parameters = None ) : return super ( ) . run ( self . _strip_sql ( hql ) , parameters )	<s> Execute the statement against Presto . Can be used to create views . </s>
