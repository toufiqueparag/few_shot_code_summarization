{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/hidden_markov_model.py", "func_name": "_extract_log_probs", "original_string": "def _extract_log_probs(num_states, dist):\n  \"\"\"Tabulate log probabilities from a batch of distributions.\"\"\"\n\n  states = tf.reshape(tf.range(num_states),\n                      tf.concat([[num_states],\n                                 tf.ones_like(dist.batch_shape_tensor())],\n                                axis=0))\n  return distribution_util.move_dimension(dist.log_prob(states), 0, -1)", "language": "python", "code": "def _extract_log_probs(num_states, dist):\n  \"\"\"Tabulate log probabilities from a batch of distributions.\"\"\"\n\n  states = tf.reshape(tf.range(num_states),\n                      tf.concat([[num_states],\n                                 tf.ones_like(dist.batch_shape_tensor())],\n                                axis=0))\n  return distribution_util.move_dimension(dist.log_prob(states), 0, -1)", "code_tokens": ["def", "_extract_log_probs", "(", "num_states", ",", "dist", ")", ":", "states", "=", "tf", ".", "reshape", "(", "tf", ".", "range", "(", "num_states", ")", ",", "tf", ".", "concat", "(", "[", "[", "num_states", "]", ",", "tf", ".", "ones_like", "(", "dist", ".", "batch_shape_tensor", "(", ")", ")", "]", ",", "axis", "=", "0", ")", ")", "return", "distribution_util", ".", "move_dimension", "(", "dist", ".", "log_prob", "(", "states", ")", ",", "0", ",", "-", "1", ")"], "docstring": "Tabulate log probabilities from a batch of distributions.", "docstring_tokens": ["Tabulate", "log", "probabilities", "from", "a", "batch", "of", "distributions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/hidden_markov_model.py#L926-L933", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/nelder_mead.py", "func_name": "_expansion_fn", "original_string": "def _expansion_fn(objective_function,\n                  simplex,\n                  objective_values,\n                  worst_index,\n                  reflected,\n                  objective_at_reflected,\n                  face_centroid,\n                  expansion):\n  \"\"\"Creates the condition function pair for an expansion.\"\"\"\n  def _expand_and_maybe_replace():\n    \"\"\"Performs the expansion step.\"\"\"\n    expanded = face_centroid + expansion * (reflected - face_centroid)\n    expanded_objective_value = objective_function(expanded)\n    expanded_is_better = (expanded_objective_value <\n                          objective_at_reflected)\n    accept_expanded_fn = lambda: (expanded, expanded_objective_value)\n    accept_reflected_fn = lambda: (reflected, objective_at_reflected)\n    next_pt, next_objective_value = prefer_static.cond(\n        expanded_is_better, accept_expanded_fn, accept_reflected_fn)\n    next_simplex = _replace_at_index(simplex, worst_index, next_pt)\n    next_objective_at_simplex = _replace_at_index(objective_values,\n                                                  worst_index,\n                                                  next_objective_value)\n    return False, next_simplex, next_objective_at_simplex, 1\n  return _expand_and_maybe_replace", "language": "python", "code": "def _expansion_fn(objective_function,\n                  simplex,\n                  objective_values,\n                  worst_index,\n                  reflected,\n                  objective_at_reflected,\n                  face_centroid,\n                  expansion):\n  \"\"\"Creates the condition function pair for an expansion.\"\"\"\n  def _expand_and_maybe_replace():\n    \"\"\"Performs the expansion step.\"\"\"\n    expanded = face_centroid + expansion * (reflected - face_centroid)\n    expanded_objective_value = objective_function(expanded)\n    expanded_is_better = (expanded_objective_value <\n                          objective_at_reflected)\n    accept_expanded_fn = lambda: (expanded, expanded_objective_value)\n    accept_reflected_fn = lambda: (reflected, objective_at_reflected)\n    next_pt, next_objective_value = prefer_static.cond(\n        expanded_is_better, accept_expanded_fn, accept_reflected_fn)\n    next_simplex = _replace_at_index(simplex, worst_index, next_pt)\n    next_objective_at_simplex = _replace_at_index(objective_values,\n                                                  worst_index,\n                                                  next_objective_value)\n    return False, next_simplex, next_objective_at_simplex, 1\n  return _expand_and_maybe_replace", "code_tokens": ["def", "_expansion_fn", "(", "objective_function", ",", "simplex", ",", "objective_values", ",", "worst_index", ",", "reflected", ",", "objective_at_reflected", ",", "face_centroid", ",", "expansion", ")", ":", "def", "_expand_and_maybe_replace", "(", ")", ":", "\"\"\"Performs the expansion step.\"\"\"", "expanded", "=", "face_centroid", "+", "expansion", "*", "(", "reflected", "-", "face_centroid", ")", "expanded_objective_value", "=", "objective_function", "(", "expanded", ")", "expanded_is_better", "=", "(", "expanded_objective_value", "<", "objective_at_reflected", ")", "accept_expanded_fn", "=", "lambda", ":", "(", "expanded", ",", "expanded_objective_value", ")", "accept_reflected_fn", "=", "lambda", ":", "(", "reflected", ",", "objective_at_reflected", ")", "next_pt", ",", "next_objective_value", "=", "prefer_static", ".", "cond", "(", "expanded_is_better", ",", "accept_expanded_fn", ",", "accept_reflected_fn", ")", "next_simplex", "=", "_replace_at_index", "(", "simplex", ",", "worst_index", ",", "next_pt", ")", "next_objective_at_simplex", "=", "_replace_at_index", "(", "objective_values", ",", "worst_index", ",", "next_objective_value", ")", "return", "False", ",", "next_simplex", ",", "next_objective_at_simplex", ",", "1", "return", "_expand_and_maybe_replace"], "docstring": "Creates the condition function pair for an expansion.", "docstring_tokens": ["Creates", "the", "condition", "function", "pair", "for", "an", "expansion", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/nelder_mead.py#L473-L497", "partition": "test"}
{"repo": "connectordb/connectordb-python", "path": "connectordb/_datapointarray.py", "func_name": "DatapointArray.sum", "original_string": "def sum(self):\n        \"\"\"Gets the sum of the data portions of all datapoints within\"\"\"\n        raw = self.raw()\n        s = 0\n        for i in range(len(raw)):\n            s += raw[i][\"d\"]\n        return s", "language": "python", "code": "def sum(self):\n        \"\"\"Gets the sum of the data portions of all datapoints within\"\"\"\n        raw = self.raw()\n        s = 0\n        for i in range(len(raw)):\n            s += raw[i][\"d\"]\n        return s", "code_tokens": ["def", "sum", "(", "self", ")", ":", "raw", "=", "self", ".", "raw", "(", ")", "s", "=", "0", "for", "i", "in", "range", "(", "len", "(", "raw", ")", ")", ":", "s", "+=", "raw", "[", "i", "]", "[", "\"d\"", "]", "return", "s"], "docstring": "Gets the sum of the data portions of all datapoints within", "docstring_tokens": ["Gets", "the", "sum", "of", "the", "data", "portions", "of", "all", "datapoints", "within"], "sha": "2092b0cb30898139a247176bcf433d5a4abde7cb", "url": "https://github.com/connectordb/connectordb-python/blob/2092b0cb30898139a247176bcf433d5a4abde7cb/connectordb/_datapointarray.py#L124-L130", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/parallel/controller/dictdb.py", "func_name": "DictDB._extract_subdict", "original_string": "def _extract_subdict(self, rec, keys):\n        \"\"\"extract subdict of keys\"\"\"\n        d = {}\n        d['msg_id'] = rec['msg_id']\n        for key in keys:\n            d[key] = rec[key]\n        return copy(d)", "language": "python", "code": "def _extract_subdict(self, rec, keys):\n        \"\"\"extract subdict of keys\"\"\"\n        d = {}\n        d['msg_id'] = rec['msg_id']\n        for key in keys:\n            d[key] = rec[key]\n        return copy(d)", "code_tokens": ["def", "_extract_subdict", "(", "self", ",", "rec", ",", "keys", ")", ":", "d", "=", "{", "}", "d", "[", "'msg_id'", "]", "=", "rec", "[", "'msg_id'", "]", "for", "key", "in", "keys", ":", "d", "[", "key", "]", "=", "rec", "[", "key", "]", "return", "copy", "(", "d", ")"], "docstring": "extract subdict of keys", "docstring_tokens": ["extract", "subdict", "of", "keys"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/parallel/controller/dictdb.py#L126-L132", "partition": "test"}
{"repo": "mseclab/PyJFuzz", "path": "pyjfuzz/core/pjf_executor.py", "func_name": "PJFExecutor.close", "original_string": "def close(self):\n        \"\"\"\n        Terminate the newly created process\n        \"\"\"\n        try:\n            self.process.terminate()\n            self.return_code = self.process.returncode\n        except OSError:\n            pass\n        self.process.stdin.close()\n        self.process.stdout.close()\n        self.process.stderr.close()\n        self.logger.debug(\"[{0}] - PJFExecutor successfully completed\".format(time.strftime(\"%H:%M:%S\")))", "language": "python", "code": "def close(self):\n        \"\"\"\n        Terminate the newly created process\n        \"\"\"\n        try:\n            self.process.terminate()\n            self.return_code = self.process.returncode\n        except OSError:\n            pass\n        self.process.stdin.close()\n        self.process.stdout.close()\n        self.process.stderr.close()\n        self.logger.debug(\"[{0}] - PJFExecutor successfully completed\".format(time.strftime(\"%H:%M:%S\")))", "code_tokens": ["def", "close", "(", "self", ")", ":", "try", ":", "self", ".", "process", ".", "terminate", "(", ")", "self", ".", "return_code", "=", "self", ".", "process", ".", "returncode", "except", "OSError", ":", "pass", "self", ".", "process", ".", "stdin", ".", "close", "(", ")", "self", ".", "process", ".", "stdout", ".", "close", "(", ")", "self", ".", "process", ".", "stderr", ".", "close", "(", ")", "self", ".", "logger", ".", "debug", "(", "\"[{0}] - PJFExecutor successfully completed\"", ".", "format", "(", "time", ".", "strftime", "(", "\"%H:%M:%S\"", ")", ")", ")"], "docstring": "Terminate the newly created process", "docstring_tokens": ["Terminate", "the", "newly", "created", "process"], "sha": "f777067076f62c9ab74ffea6e90fd54402b7a1b4", "url": "https://github.com/mseclab/PyJFuzz/blob/f777067076f62c9ab74ffea6e90fd54402b7a1b4/pyjfuzz/core/pjf_executor.py#L106-L118", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/grid/grid_search.py", "func_name": "H2OGridSearch.show", "original_string": "def show(self):\n        \"\"\"Print models sorted by metric.\"\"\"\n        hyper_combos = itertools.product(*list(self.hyper_params.values()))\n        if not self.models:\n            c_values = [[idx + 1, list(val)] for idx, val in enumerate(hyper_combos)]\n            print(H2OTwoDimTable(\n                col_header=['Model', 'Hyperparameters: [' + ', '.join(list(self.hyper_params.keys())) + ']'],\n                table_header='Grid Search of Model ' + self.model.__class__.__name__, cell_values=c_values))\n        else:\n            print(self.sorted_metric_table())", "language": "python", "code": "def show(self):\n        \"\"\"Print models sorted by metric.\"\"\"\n        hyper_combos = itertools.product(*list(self.hyper_params.values()))\n        if not self.models:\n            c_values = [[idx + 1, list(val)] for idx, val in enumerate(hyper_combos)]\n            print(H2OTwoDimTable(\n                col_header=['Model', 'Hyperparameters: [' + ', '.join(list(self.hyper_params.keys())) + ']'],\n                table_header='Grid Search of Model ' + self.model.__class__.__name__, cell_values=c_values))\n        else:\n            print(self.sorted_metric_table())", "code_tokens": ["def", "show", "(", "self", ")", ":", "hyper_combos", "=", "itertools", ".", "product", "(", "*", "list", "(", "self", ".", "hyper_params", ".", "values", "(", ")", ")", ")", "if", "not", "self", ".", "models", ":", "c_values", "=", "[", "[", "idx", "+", "1", ",", "list", "(", "val", ")", "]", "for", "idx", ",", "val", "in", "enumerate", "(", "hyper_combos", ")", "]", "print", "(", "H2OTwoDimTable", "(", "col_header", "=", "[", "'Model'", ",", "'Hyperparameters: ['", "+", "', '", ".", "join", "(", "list", "(", "self", ".", "hyper_params", ".", "keys", "(", ")", ")", ")", "+", "']'", "]", ",", "table_header", "=", "'Grid Search of Model '", "+", "self", ".", "model", ".", "__class__", ".", "__name__", ",", "cell_values", "=", "c_values", ")", ")", "else", ":", "print", "(", "self", ".", "sorted_metric_table", "(", ")", ")"], "docstring": "Print models sorted by metric.", "docstring_tokens": ["Print", "models", "sorted", "by", "metric", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/grid/grid_search.py#L460-L469", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/nose/plugins/manager.py", "func_name": "PluginProxy.generate", "original_string": "def generate(self, *arg, **kw):\n        \"\"\"Call all plugins, yielding each item in each non-None result.\n        \"\"\"\n        for p, meth in self.plugins:\n            result = None\n            try:\n                result = meth(*arg, **kw)\n                if result is not None:\n                    for r in result:\n                        yield r\n            except (KeyboardInterrupt, SystemExit):\n                raise\n            except:\n                exc = sys.exc_info()\n                yield Failure(*exc)\n                continue", "language": "python", "code": "def generate(self, *arg, **kw):\n        \"\"\"Call all plugins, yielding each item in each non-None result.\n        \"\"\"\n        for p, meth in self.plugins:\n            result = None\n            try:\n                result = meth(*arg, **kw)\n                if result is not None:\n                    for r in result:\n                        yield r\n            except (KeyboardInterrupt, SystemExit):\n                raise\n            except:\n                exc = sys.exc_info()\n                yield Failure(*exc)\n                continue", "code_tokens": ["def", "generate", "(", "self", ",", "*", "arg", ",", "*", "*", "kw", ")", ":", "for", "p", ",", "meth", "in", "self", ".", "plugins", ":", "result", "=", "None", "try", ":", "result", "=", "meth", "(", "*", "arg", ",", "*", "*", "kw", ")", "if", "result", "is", "not", "None", ":", "for", "r", "in", "result", ":", "yield", "r", "except", "(", "KeyboardInterrupt", ",", "SystemExit", ")", ":", "raise", "except", ":", "exc", "=", "sys", ".", "exc_info", "(", ")", "yield", "Failure", "(", "*", "exc", ")", "continue"], "docstring": "Call all plugins, yielding each item in each non-None result.", "docstring_tokens": ["Call", "all", "plugins", "yielding", "each", "item", "in", "each", "non", "-", "None", "result", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/nose/plugins/manager.py#L146-L161", "partition": "test"}
{"repo": "funilrys/PyFunceble", "path": "PyFunceble/auto_continue.py", "func_name": "AutoContinue.backup", "original_string": "def backup(self):\n        \"\"\"\n        Backup the current execution state.\n        \"\"\"\n\n        if PyFunceble.CONFIGURATION[\"auto_continue\"]:\n            # The auto_continue subsystem is activated.\n\n            # We initiate the location where we are going to save the data to backup.\n            data_to_backup = {}\n            # We get the current counter states.\n            configuration_counter = PyFunceble.INTERN[\"counter\"][\"number\"]\n\n            # We initiate the data we have to backup.\n            data_to_backup[PyFunceble.INTERN[\"file_to_test\"]] = {\n                # We backup the number of tested.\n                \"tested\": configuration_counter[\"tested\"],\n                # We backup the number of up.\n                \"up\": configuration_counter[\"up\"],\n                # We backup the number of down.\n                \"down\": configuration_counter[\"down\"],\n                # We backup the number of invalid.\n                \"invalid\": configuration_counter[\"invalid\"],\n            }\n\n            # We initiate the final data we have to save.\n            # We initiate this variable instead of updating backup_content because\n            # we do not want to touch the backup_content.\n            to_save = {}\n\n            # We add the backup_content into to_save.\n            to_save.update(self.backup_content)\n            # And we overwrite with the newly data to backup.\n            to_save.update(data_to_backup)\n\n            # Finaly, we save our informations into the log file.\n            Dict(to_save).to_json(self.autocontinue_log_file)", "language": "python", "code": "def backup(self):\n        \"\"\"\n        Backup the current execution state.\n        \"\"\"\n\n        if PyFunceble.CONFIGURATION[\"auto_continue\"]:\n            # The auto_continue subsystem is activated.\n\n            # We initiate the location where we are going to save the data to backup.\n            data_to_backup = {}\n            # We get the current counter states.\n            configuration_counter = PyFunceble.INTERN[\"counter\"][\"number\"]\n\n            # We initiate the data we have to backup.\n            data_to_backup[PyFunceble.INTERN[\"file_to_test\"]] = {\n                # We backup the number of tested.\n                \"tested\": configuration_counter[\"tested\"],\n                # We backup the number of up.\n                \"up\": configuration_counter[\"up\"],\n                # We backup the number of down.\n                \"down\": configuration_counter[\"down\"],\n                # We backup the number of invalid.\n                \"invalid\": configuration_counter[\"invalid\"],\n            }\n\n            # We initiate the final data we have to save.\n            # We initiate this variable instead of updating backup_content because\n            # we do not want to touch the backup_content.\n            to_save = {}\n\n            # We add the backup_content into to_save.\n            to_save.update(self.backup_content)\n            # And we overwrite with the newly data to backup.\n            to_save.update(data_to_backup)\n\n            # Finaly, we save our informations into the log file.\n            Dict(to_save).to_json(self.autocontinue_log_file)", "code_tokens": ["def", "backup", "(", "self", ")", ":", "if", "PyFunceble", ".", "CONFIGURATION", "[", "\"auto_continue\"", "]", ":", "# The auto_continue subsystem is activated.", "# We initiate the location where we are going to save the data to backup.", "data_to_backup", "=", "{", "}", "# We get the current counter states.", "configuration_counter", "=", "PyFunceble", ".", "INTERN", "[", "\"counter\"", "]", "[", "\"number\"", "]", "# We initiate the data we have to backup.", "data_to_backup", "[", "PyFunceble", ".", "INTERN", "[", "\"file_to_test\"", "]", "]", "=", "{", "# We backup the number of tested.", "\"tested\"", ":", "configuration_counter", "[", "\"tested\"", "]", ",", "# We backup the number of up.", "\"up\"", ":", "configuration_counter", "[", "\"up\"", "]", ",", "# We backup the number of down.", "\"down\"", ":", "configuration_counter", "[", "\"down\"", "]", ",", "# We backup the number of invalid.", "\"invalid\"", ":", "configuration_counter", "[", "\"invalid\"", "]", ",", "}", "# We initiate the final data we have to save.", "# We initiate this variable instead of updating backup_content because", "# we do not want to touch the backup_content.", "to_save", "=", "{", "}", "# We add the backup_content into to_save.", "to_save", ".", "update", "(", "self", ".", "backup_content", ")", "# And we overwrite with the newly data to backup.", "to_save", ".", "update", "(", "data_to_backup", ")", "# Finaly, we save our informations into the log file.", "Dict", "(", "to_save", ")", ".", "to_json", "(", "self", ".", "autocontinue_log_file", ")"], "docstring": "Backup the current execution state.", "docstring_tokens": ["Backup", "the", "current", "execution", "state", "."], "sha": "cdf69cbde120199171f7158e1c33635753e6e2f5", "url": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/auto_continue.py#L105-L141", "partition": "test"}
{"repo": "Chilipp/sphinx-nbexamples", "path": "sphinx_nbexamples/__init__.py", "func_name": "NotebookProcessor.thumbnail_div", "original_string": "def thumbnail_div(self):\n        \"\"\"The string for creating the thumbnail of this example\"\"\"\n        return self.THUMBNAIL_TEMPLATE.format(\n            snippet=self.get_description()[1], thumbnail=self.thumb_file,\n            ref_name=self.reference)", "language": "python", "code": "def thumbnail_div(self):\n        \"\"\"The string for creating the thumbnail of this example\"\"\"\n        return self.THUMBNAIL_TEMPLATE.format(\n            snippet=self.get_description()[1], thumbnail=self.thumb_file,\n            ref_name=self.reference)", "code_tokens": ["def", "thumbnail_div", "(", "self", ")", ":", "return", "self", ".", "THUMBNAIL_TEMPLATE", ".", "format", "(", "snippet", "=", "self", ".", "get_description", "(", ")", "[", "1", "]", ",", "thumbnail", "=", "self", ".", "thumb_file", ",", "ref_name", "=", "self", ".", "reference", ")"], "docstring": "The string for creating the thumbnail of this example", "docstring_tokens": ["The", "string", "for", "creating", "the", "thumbnail", "of", "this", "example"], "sha": "08e0319ff3c70f8a931dfa8890caf48add4d0470", "url": "https://github.com/Chilipp/sphinx-nbexamples/blob/08e0319ff3c70f8a931dfa8890caf48add4d0470/sphinx_nbexamples/__init__.py#L201-L205", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/parse/variant/models.py", "func_name": "parse_genetic_models", "original_string": "def parse_genetic_models(models_info, case_id):\n    \"\"\"Parse the genetic models entry of a vcf\n\n    Args:\n        models_info(str): The raw vcf information\n        case_id(str)\n\n    Returns:\n        genetic_models(list)\n\n    \"\"\"\n    genetic_models = []\n    if models_info:\n        for family_info in models_info.split(','):\n            splitted_info = family_info.split(':')\n            if splitted_info[0] == case_id:\n                genetic_models = splitted_info[1].split('|')\n\n    return genetic_models", "language": "python", "code": "def parse_genetic_models(models_info, case_id):\n    \"\"\"Parse the genetic models entry of a vcf\n\n    Args:\n        models_info(str): The raw vcf information\n        case_id(str)\n\n    Returns:\n        genetic_models(list)\n\n    \"\"\"\n    genetic_models = []\n    if models_info:\n        for family_info in models_info.split(','):\n            splitted_info = family_info.split(':')\n            if splitted_info[0] == case_id:\n                genetic_models = splitted_info[1].split('|')\n\n    return genetic_models", "code_tokens": ["def", "parse_genetic_models", "(", "models_info", ",", "case_id", ")", ":", "genetic_models", "=", "[", "]", "if", "models_info", ":", "for", "family_info", "in", "models_info", ".", "split", "(", "','", ")", ":", "splitted_info", "=", "family_info", ".", "split", "(", "':'", ")", "if", "splitted_info", "[", "0", "]", "==", "case_id", ":", "genetic_models", "=", "splitted_info", "[", "1", "]", ".", "split", "(", "'|'", ")", "return", "genetic_models"], "docstring": "Parse the genetic models entry of a vcf\n\n    Args:\n        models_info(str): The raw vcf information\n        case_id(str)\n\n    Returns:\n        genetic_models(list)", "docstring_tokens": ["Parse", "the", "genetic", "models", "entry", "of", "a", "vcf"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/parse/variant/models.py#L2-L20", "partition": "test"}
{"repo": "dossier/dossier.store", "path": "dossier/store/store.py", "func_name": "Store.put", "original_string": "def put(self, items, indexes=True):\n        '''Add feature collections to the store.\n\n        Given an iterable of tuples of the form\n        ``(content_id, feature collection)``, add each to the store\n        and overwrite any that already exist.\n\n        This method optionally accepts a keyword argument `indexes`,\n        which by default is set to ``True``. When it is ``True``,\n        it will *create* new indexes for each content object for all\n        indexes defined on this store.\n\n        Note that this will not update existing indexes. (There is\n        currently no way to do this without running some sort of\n        garbage collection process.)\n\n        :param iterable items: iterable of\n                               ``(content_id, FeatureCollection)``.\n        :type fc: :class:`dossier.fc.FeatureCollection`\n        '''\n        # So why accept an iterable? Ideally, some day, `kvlayer.put` would\n        # accept an iterable, so we should too.\n        #\n        # But we have to transform it to a list in order to update indexes\n        # anyway. Well, if we don't have to update indexes, then we can avoid\n        # loading everything into memory, which seems like an optimization\n        # worth having even if it's only some of the time.\n        #\n        # N.B. If you're thinking, \"Just use itertools.tee\", then you should\n        # heed this warning from Python docs: \"This itertool may require\n        # significant auxiliary storage (depending on how much temporary data\n        # needs to be stored). In general, if one iterator uses most or all of\n        # the data before another iterator starts, it is faster to use list()\n        # instead of tee().\"\n        #\n        # i.e., `tee` has to store everything into memory because `kvlayer`\n        # will exhaust the first iterator before indexes get updated.\n        items = list(items)\n        self.kvl.put(self.TABLE,\n                     *imap(lambda (cid, fc): ((cid,), fc_dumps(fc)), items))\n        if indexes:\n            for idx_name in self._indexes:\n                self._index_put(idx_name, *items)", "language": "python", "code": "def put(self, items, indexes=True):\n        '''Add feature collections to the store.\n\n        Given an iterable of tuples of the form\n        ``(content_id, feature collection)``, add each to the store\n        and overwrite any that already exist.\n\n        This method optionally accepts a keyword argument `indexes`,\n        which by default is set to ``True``. When it is ``True``,\n        it will *create* new indexes for each content object for all\n        indexes defined on this store.\n\n        Note that this will not update existing indexes. (There is\n        currently no way to do this without running some sort of\n        garbage collection process.)\n\n        :param iterable items: iterable of\n                               ``(content_id, FeatureCollection)``.\n        :type fc: :class:`dossier.fc.FeatureCollection`\n        '''\n        # So why accept an iterable? Ideally, some day, `kvlayer.put` would\n        # accept an iterable, so we should too.\n        #\n        # But we have to transform it to a list in order to update indexes\n        # anyway. Well, if we don't have to update indexes, then we can avoid\n        # loading everything into memory, which seems like an optimization\n        # worth having even if it's only some of the time.\n        #\n        # N.B. If you're thinking, \"Just use itertools.tee\", then you should\n        # heed this warning from Python docs: \"This itertool may require\n        # significant auxiliary storage (depending on how much temporary data\n        # needs to be stored). In general, if one iterator uses most or all of\n        # the data before another iterator starts, it is faster to use list()\n        # instead of tee().\"\n        #\n        # i.e., `tee` has to store everything into memory because `kvlayer`\n        # will exhaust the first iterator before indexes get updated.\n        items = list(items)\n        self.kvl.put(self.TABLE,\n                     *imap(lambda (cid, fc): ((cid,), fc_dumps(fc)), items))\n        if indexes:\n            for idx_name in self._indexes:\n                self._index_put(idx_name, *items)", "code_tokens": ["def", "put", "(", "self", ",", "items", ",", "indexes", "=", "True", ")", ":", "# So why accept an iterable? Ideally, some day, `kvlayer.put` would", "# accept an iterable, so we should too.", "#", "# But we have to transform it to a list in order to update indexes", "# anyway. Well, if we don't have to update indexes, then we can avoid", "# loading everything into memory, which seems like an optimization", "# worth having even if it's only some of the time.", "#", "# N.B. If you're thinking, \"Just use itertools.tee\", then you should", "# heed this warning from Python docs: \"This itertool may require", "# significant auxiliary storage (depending on how much temporary data", "# needs to be stored). In general, if one iterator uses most or all of", "# the data before another iterator starts, it is faster to use list()", "# instead of tee().\"", "#", "# i.e., `tee` has to store everything into memory because `kvlayer`", "# will exhaust the first iterator before indexes get updated.", "items", "=", "list", "(", "items", ")", "self", ".", "kvl", ".", "put", "(", "self", ".", "TABLE", ",", "*", "imap", "(", "lambda", "(", "cid", ",", "fc", ")", ":", "(", "(", "cid", ",", ")", ",", "fc_dumps", "(", "fc", ")", ")", ",", "items", ")", ")", "if", "indexes", ":", "for", "idx_name", "in", "self", ".", "_indexes", ":", "self", ".", "_index_put", "(", "idx_name", ",", "*", "items", ")"], "docstring": "Add feature collections to the store.\n\n        Given an iterable of tuples of the form\n        ``(content_id, feature collection)``, add each to the store\n        and overwrite any that already exist.\n\n        This method optionally accepts a keyword argument `indexes`,\n        which by default is set to ``True``. When it is ``True``,\n        it will *create* new indexes for each content object for all\n        indexes defined on this store.\n\n        Note that this will not update existing indexes. (There is\n        currently no way to do this without running some sort of\n        garbage collection process.)\n\n        :param iterable items: iterable of\n                               ``(content_id, FeatureCollection)``.\n        :type fc: :class:`dossier.fc.FeatureCollection`", "docstring_tokens": ["Add", "feature", "collections", "to", "the", "store", "."], "sha": "b22ffe2470bba9fcc98a30cb55b437bfa1521e7f", "url": "https://github.com/dossier/dossier.store/blob/b22ffe2470bba9fcc98a30cb55b437bfa1521e7f/dossier/store/store.py#L169-L211", "partition": "test"}
{"repo": "Numergy/yoda", "path": "yoda/workspace.py", "func_name": "Workspace.repository_exists", "original_string": "def repository_exists(self, workspace, repo):\n        \"\"\"Return True if workspace contains repository name.\"\"\"\n        if not self.exists(workspace):\n            return False\n\n        workspaces = self.list()\n        return repo in workspaces[workspace][\"repositories\"]", "language": "python", "code": "def repository_exists(self, workspace, repo):\n        \"\"\"Return True if workspace contains repository name.\"\"\"\n        if not self.exists(workspace):\n            return False\n\n        workspaces = self.list()\n        return repo in workspaces[workspace][\"repositories\"]", "code_tokens": ["def", "repository_exists", "(", "self", ",", "workspace", ",", "repo", ")", ":", "if", "not", "self", ".", "exists", "(", "workspace", ")", ":", "return", "False", "workspaces", "=", "self", ".", "list", "(", ")", "return", "repo", "in", "workspaces", "[", "workspace", "]", "[", "\"repositories\"", "]"], "docstring": "Return True if workspace contains repository name.", "docstring_tokens": ["Return", "True", "if", "workspace", "contains", "repository", "name", "."], "sha": "109f0e9441130488b0155f05883ef6531cf46ee9", "url": "https://github.com/Numergy/yoda/blob/109f0e9441130488b0155f05883ef6531cf46ee9/yoda/workspace.py#L78-L84", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/interactiveshell.py", "func_name": "InteractiveShell.reset", "original_string": "def reset(self, new_session=True):\n        \"\"\"Clear all internal namespaces, and attempt to release references to\n        user objects.\n\n        If new_session is True, a new history session will be opened.\n        \"\"\"\n        # Clear histories\n        self.history_manager.reset(new_session)\n        # Reset counter used to index all histories\n        if new_session:\n            self.execution_count = 1\n\n        # Flush cached output items\n        if self.displayhook.do_full_cache:\n            self.displayhook.flush()\n\n        # The main execution namespaces must be cleared very carefully,\n        # skipping the deletion of the builtin-related keys, because doing so\n        # would cause errors in many object's __del__ methods.\n        if self.user_ns is not self.user_global_ns:\n            self.user_ns.clear()\n        ns = self.user_global_ns\n        drop_keys = set(ns.keys())\n        drop_keys.discard('__builtin__')\n        drop_keys.discard('__builtins__')\n        drop_keys.discard('__name__')\n        for k in drop_keys:\n            del ns[k]\n        \n        self.user_ns_hidden.clear()\n        \n        # Restore the user namespaces to minimal usability\n        self.init_user_ns()\n\n        # Restore the default and user aliases\n        self.alias_manager.clear_aliases()\n        self.alias_manager.init_aliases()\n\n        # Flush the private list of module references kept for script\n        # execution protection\n        self.clear_main_mod_cache()\n\n        # Clear out the namespace from the last %run\n        self.new_main_mod()", "language": "python", "code": "def reset(self, new_session=True):\n        \"\"\"Clear all internal namespaces, and attempt to release references to\n        user objects.\n\n        If new_session is True, a new history session will be opened.\n        \"\"\"\n        # Clear histories\n        self.history_manager.reset(new_session)\n        # Reset counter used to index all histories\n        if new_session:\n            self.execution_count = 1\n\n        # Flush cached output items\n        if self.displayhook.do_full_cache:\n            self.displayhook.flush()\n\n        # The main execution namespaces must be cleared very carefully,\n        # skipping the deletion of the builtin-related keys, because doing so\n        # would cause errors in many object's __del__ methods.\n        if self.user_ns is not self.user_global_ns:\n            self.user_ns.clear()\n        ns = self.user_global_ns\n        drop_keys = set(ns.keys())\n        drop_keys.discard('__builtin__')\n        drop_keys.discard('__builtins__')\n        drop_keys.discard('__name__')\n        for k in drop_keys:\n            del ns[k]\n        \n        self.user_ns_hidden.clear()\n        \n        # Restore the user namespaces to minimal usability\n        self.init_user_ns()\n\n        # Restore the default and user aliases\n        self.alias_manager.clear_aliases()\n        self.alias_manager.init_aliases()\n\n        # Flush the private list of module references kept for script\n        # execution protection\n        self.clear_main_mod_cache()\n\n        # Clear out the namespace from the last %run\n        self.new_main_mod()", "code_tokens": ["def", "reset", "(", "self", ",", "new_session", "=", "True", ")", ":", "# Clear histories", "self", ".", "history_manager", ".", "reset", "(", "new_session", ")", "# Reset counter used to index all histories", "if", "new_session", ":", "self", ".", "execution_count", "=", "1", "# Flush cached output items", "if", "self", ".", "displayhook", ".", "do_full_cache", ":", "self", ".", "displayhook", ".", "flush", "(", ")", "# The main execution namespaces must be cleared very carefully,", "# skipping the deletion of the builtin-related keys, because doing so", "# would cause errors in many object's __del__ methods.", "if", "self", ".", "user_ns", "is", "not", "self", ".", "user_global_ns", ":", "self", ".", "user_ns", ".", "clear", "(", ")", "ns", "=", "self", ".", "user_global_ns", "drop_keys", "=", "set", "(", "ns", ".", "keys", "(", ")", ")", "drop_keys", ".", "discard", "(", "'__builtin__'", ")", "drop_keys", ".", "discard", "(", "'__builtins__'", ")", "drop_keys", ".", "discard", "(", "'__name__'", ")", "for", "k", "in", "drop_keys", ":", "del", "ns", "[", "k", "]", "self", ".", "user_ns_hidden", ".", "clear", "(", ")", "# Restore the user namespaces to minimal usability", "self", ".", "init_user_ns", "(", ")", "# Restore the default and user aliases", "self", ".", "alias_manager", ".", "clear_aliases", "(", ")", "self", ".", "alias_manager", ".", "init_aliases", "(", ")", "# Flush the private list of module references kept for script", "# execution protection", "self", ".", "clear_main_mod_cache", "(", ")", "# Clear out the namespace from the last %run", "self", ".", "new_main_mod", "(", ")"], "docstring": "Clear all internal namespaces, and attempt to release references to\n        user objects.\n\n        If new_session is True, a new history session will be opened.", "docstring_tokens": ["Clear", "all", "internal", "namespaces", "and", "attempt", "to", "release", "references", "to", "user", "objects", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/interactiveshell.py#L1155-L1198", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/core/messages/payloads/cancel.py", "func_name": "CancelResponsePayload.read", "original_string": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the Cancel response payload and decode it into\n        its constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is missing from the\n                encoded payload.\n        \"\"\"\n        super(CancelResponsePayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(\n                enums.Tags.ASYNCHRONOUS_CORRELATION_VALUE,\n                local_stream\n        ):\n            self._asynchronous_correlation_value = primitives.ByteString(\n                tag=enums.Tags.ASYNCHRONOUS_CORRELATION_VALUE\n            )\n            self._asynchronous_correlation_value.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self.is_tag_next(enums.Tags.CANCELLATION_RESULT, local_stream):\n            self._cancellation_result = primitives.Enumeration(\n                enums.CancellationResult,\n                tag=enums.Tags.CANCELLATION_RESULT\n            )\n            self._cancellation_result.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.is_oversized(local_stream)", "language": "python", "code": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the Cancel response payload and decode it into\n        its constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is missing from the\n                encoded payload.\n        \"\"\"\n        super(CancelResponsePayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(\n                enums.Tags.ASYNCHRONOUS_CORRELATION_VALUE,\n                local_stream\n        ):\n            self._asynchronous_correlation_value = primitives.ByteString(\n                tag=enums.Tags.ASYNCHRONOUS_CORRELATION_VALUE\n            )\n            self._asynchronous_correlation_value.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self.is_tag_next(enums.Tags.CANCELLATION_RESULT, local_stream):\n            self._cancellation_result = primitives.Enumeration(\n                enums.CancellationResult,\n                tag=enums.Tags.CANCELLATION_RESULT\n            )\n            self._cancellation_result.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.is_oversized(local_stream)", "code_tokens": ["def", "read", "(", "self", ",", "input_stream", ",", "kmip_version", "=", "enums", ".", "KMIPVersion", ".", "KMIP_1_0", ")", ":", "super", "(", "CancelResponsePayload", ",", "self", ")", ".", "read", "(", "input_stream", ",", "kmip_version", "=", "kmip_version", ")", "local_stream", "=", "utils", ".", "BytearrayStream", "(", "input_stream", ".", "read", "(", "self", ".", "length", ")", ")", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "ASYNCHRONOUS_CORRELATION_VALUE", ",", "local_stream", ")", ":", "self", ".", "_asynchronous_correlation_value", "=", "primitives", ".", "ByteString", "(", "tag", "=", "enums", ".", "Tags", ".", "ASYNCHRONOUS_CORRELATION_VALUE", ")", "self", ".", "_asynchronous_correlation_value", ".", "read", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "CANCELLATION_RESULT", ",", "local_stream", ")", ":", "self", ".", "_cancellation_result", "=", "primitives", ".", "Enumeration", "(", "enums", ".", "CancellationResult", ",", "tag", "=", "enums", ".", "Tags", ".", "CANCELLATION_RESULT", ")", "self", ".", "_cancellation_result", ".", "read", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "self", ".", "is_oversized", "(", "local_stream", ")"], "docstring": "Read the data encoding the Cancel response payload and decode it into\n        its constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is missing from the\n                encoded payload.", "docstring_tokens": ["Read", "the", "data", "encoding", "the", "Cancel", "response", "payload", "and", "decode", "it", "into", "its", "constituent", "parts", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/core/messages/payloads/cancel.py#L237-L281", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/adapter/mongo/case.py", "func_name": "CaseHandler.replace_case", "original_string": "def replace_case(self, case_obj):\n        \"\"\"Replace a existing case with a new one\n\n        Keeps the object id\n\n        Args:\n            case_obj(dict)\n\n        Returns:\n            updated_case(dict)\n        \"\"\"\n        # Todo: Figure out and describe when this method destroys a case if invoked instead of\n        # update_case\n        LOG.info(\"Saving case %s\", case_obj['_id'])\n        # update updated_at of case to \"today\"\n\n        case_obj['updated_at'] = datetime.datetime.now(),\n\n        updated_case = self.case_collection.find_one_and_replace(\n            {'_id': case_obj['_id']},\n            case_obj,\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        return updated_case", "language": "python", "code": "def replace_case(self, case_obj):\n        \"\"\"Replace a existing case with a new one\n\n        Keeps the object id\n\n        Args:\n            case_obj(dict)\n\n        Returns:\n            updated_case(dict)\n        \"\"\"\n        # Todo: Figure out and describe when this method destroys a case if invoked instead of\n        # update_case\n        LOG.info(\"Saving case %s\", case_obj['_id'])\n        # update updated_at of case to \"today\"\n\n        case_obj['updated_at'] = datetime.datetime.now(),\n\n        updated_case = self.case_collection.find_one_and_replace(\n            {'_id': case_obj['_id']},\n            case_obj,\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        return updated_case", "code_tokens": ["def", "replace_case", "(", "self", ",", "case_obj", ")", ":", "# Todo: Figure out and describe when this method destroys a case if invoked instead of", "# update_case", "LOG", ".", "info", "(", "\"Saving case %s\"", ",", "case_obj", "[", "'_id'", "]", ")", "# update updated_at of case to \"today\"", "case_obj", "[", "'updated_at'", "]", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ",", "updated_case", "=", "self", ".", "case_collection", ".", "find_one_and_replace", "(", "{", "'_id'", ":", "case_obj", "[", "'_id'", "]", "}", ",", "case_obj", ",", "return_document", "=", "pymongo", ".", "ReturnDocument", ".", "AFTER", ")", "return", "updated_case"], "docstring": "Replace a existing case with a new one\n\n        Keeps the object id\n\n        Args:\n            case_obj(dict)\n\n        Returns:\n            updated_case(dict)", "docstring_tokens": ["Replace", "a", "existing", "case", "with", "a", "new", "one"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/adapter/mongo/case.py#L433-L457", "partition": "test"}
{"repo": "google/gin-config", "path": "gin/selector_map.py", "func_name": "SelectorMap.get_all_matches", "original_string": "def get_all_matches(self, partial_selector):\n    \"\"\"Returns all values matching `partial_selector` as a list.\"\"\"\n    matching_selectors = self.matching_selectors(partial_selector)\n    return [self._selector_map[selector] for selector in matching_selectors]", "language": "python", "code": "def get_all_matches(self, partial_selector):\n    \"\"\"Returns all values matching `partial_selector` as a list.\"\"\"\n    matching_selectors = self.matching_selectors(partial_selector)\n    return [self._selector_map[selector] for selector in matching_selectors]", "code_tokens": ["def", "get_all_matches", "(", "self", ",", "partial_selector", ")", ":", "matching_selectors", "=", "self", ".", "matching_selectors", "(", "partial_selector", ")", "return", "[", "self", ".", "_selector_map", "[", "selector", "]", "for", "selector", "in", "matching_selectors", "]"], "docstring": "Returns all values matching `partial_selector` as a list.", "docstring_tokens": ["Returns", "all", "values", "matching", "partial_selector", "as", "a", "list", "."], "sha": "17a170e0a6711005d1c78e67cf493dc44674d44f", "url": "https://github.com/google/gin-config/blob/17a170e0a6711005d1c78e67cf493dc44674d44f/gin/selector_map.py#L180-L183", "partition": "test"}
{"repo": "google/brotli", "path": "research/brotlidump.py", "func_name": "Code.showCode", "original_string": "def showCode(self, width=80):\n        \"\"\"Show all words of the code in a nice format.\n        \"\"\"\n        #make table of all symbols with binary strings\n        symbolStrings = [\n            (self.bitPattern(s.index), self.mnemonic(s.index))\n            for s in self\n            ]\n        #determine column widths the way Lisp programmers do it\n        leftColWidth, rightColWidth = map(max, map(\n            map,\n            repeat(len),\n            zip(*symbolStrings)\n            ))\n        colwidth = leftColWidth+rightColWidth\n        columns = 81//(colwidth+2)\n        rows = -(-len(symbolStrings)//columns)\n        def justify(bs):\n            b,s = bs\n            return b.rjust(leftColWidth)+':'+s.ljust(rightColWidth)\n        for i in range(rows):\n            print(' '.join(map(justify, symbolStrings[i::rows])).rstrip())", "language": "python", "code": "def showCode(self, width=80):\n        \"\"\"Show all words of the code in a nice format.\n        \"\"\"\n        #make table of all symbols with binary strings\n        symbolStrings = [\n            (self.bitPattern(s.index), self.mnemonic(s.index))\n            for s in self\n            ]\n        #determine column widths the way Lisp programmers do it\n        leftColWidth, rightColWidth = map(max, map(\n            map,\n            repeat(len),\n            zip(*symbolStrings)\n            ))\n        colwidth = leftColWidth+rightColWidth\n        columns = 81//(colwidth+2)\n        rows = -(-len(symbolStrings)//columns)\n        def justify(bs):\n            b,s = bs\n            return b.rjust(leftColWidth)+':'+s.ljust(rightColWidth)\n        for i in range(rows):\n            print(' '.join(map(justify, symbolStrings[i::rows])).rstrip())", "code_tokens": ["def", "showCode", "(", "self", ",", "width", "=", "80", ")", ":", "#make table of all symbols with binary strings", "symbolStrings", "=", "[", "(", "self", ".", "bitPattern", "(", "s", ".", "index", ")", ",", "self", ".", "mnemonic", "(", "s", ".", "index", ")", ")", "for", "s", "in", "self", "]", "#determine column widths the way Lisp programmers do it", "leftColWidth", ",", "rightColWidth", "=", "map", "(", "max", ",", "map", "(", "map", ",", "repeat", "(", "len", ")", ",", "zip", "(", "*", "symbolStrings", ")", ")", ")", "colwidth", "=", "leftColWidth", "+", "rightColWidth", "columns", "=", "81", "//", "(", "colwidth", "+", "2", ")", "rows", "=", "-", "(", "-", "len", "(", "symbolStrings", ")", "//", "columns", ")", "def", "justify", "(", "bs", ")", ":", "b", ",", "s", "=", "bs", "return", "b", ".", "rjust", "(", "leftColWidth", ")", "+", "':'", "+", "s", ".", "ljust", "(", "rightColWidth", ")", "for", "i", "in", "range", "(", "rows", ")", ":", "print", "(", "' '", ".", "join", "(", "map", "(", "justify", ",", "symbolStrings", "[", "i", ":", ":", "rows", "]", ")", ")", ".", "rstrip", "(", ")", ")"], "docstring": "Show all words of the code in a nice format.", "docstring_tokens": ["Show", "all", "words", "of", "the", "code", "in", "a", "nice", "format", "."], "sha": "4b2b2d4f83ffeaac7708e44409fe34896a01a278", "url": "https://github.com/google/brotli/blob/4b2b2d4f83ffeaac7708e44409fe34896a01a278/research/brotlidump.py#L412-L433", "partition": "test"}
{"repo": "streamlink/streamlink", "path": "src/streamlink_cli/utils/progress.py", "func_name": "get_cut_prefix", "original_string": "def get_cut_prefix(value, max_len):\n    \"\"\"Drops Characters by unicode not by bytes.\"\"\"\n    should_convert = isinstance(value, bytes)\n    if should_convert:\n        value = value.decode(\"utf8\", \"ignore\")\n    for i in range(len(value)):\n        if terminal_width(value[i:]) <= max_len:\n            break\n    return value[i:].encode(\"utf8\", \"ignore\") if should_convert else value[i:]", "language": "python", "code": "def get_cut_prefix(value, max_len):\n    \"\"\"Drops Characters by unicode not by bytes.\"\"\"\n    should_convert = isinstance(value, bytes)\n    if should_convert:\n        value = value.decode(\"utf8\", \"ignore\")\n    for i in range(len(value)):\n        if terminal_width(value[i:]) <= max_len:\n            break\n    return value[i:].encode(\"utf8\", \"ignore\") if should_convert else value[i:]", "code_tokens": ["def", "get_cut_prefix", "(", "value", ",", "max_len", ")", ":", "should_convert", "=", "isinstance", "(", "value", ",", "bytes", ")", "if", "should_convert", ":", "value", "=", "value", ".", "decode", "(", "\"utf8\"", ",", "\"ignore\"", ")", "for", "i", "in", "range", "(", "len", "(", "value", ")", ")", ":", "if", "terminal_width", "(", "value", "[", "i", ":", "]", ")", "<=", "max_len", ":", "break", "return", "value", "[", "i", ":", "]", ".", "encode", "(", "\"utf8\"", ",", "\"ignore\"", ")", "if", "should_convert", "else", "value", "[", "i", ":", "]"], "docstring": "Drops Characters by unicode not by bytes.", "docstring_tokens": ["Drops", "Characters", "by", "unicode", "not", "by", "bytes", "."], "sha": "c8ed1daff14ac03195870238b9b900c1109dd5c1", "url": "https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink_cli/utils/progress.py#L46-L54", "partition": "test"}
{"repo": "pmacosta/peng", "path": "peng/wave_functions.py", "func_name": "iffti", "original_string": "def iffti(wave, npoints=None, indep_min=None, indep_max=None):\n    r\"\"\"\n    Return the imaginary part of the inverse Fast Fourier Transform of a waveform.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param npoints: Number of points to use in the transform. If **npoints**\n                    is less than the size of the independent variable vector\n                    the waveform is truncated; if **npoints** is greater than\n                    the size of the independent variable vector, the waveform\n                    is zero-padded\n    :type  npoints: positive integer\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.iffti\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`npoints\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n     * RuntimeError (Non-uniform frequency spacing)\n\n    .. [[[end]]]\n    \"\"\"\n    return imag(ifft(wave, npoints, indep_min, indep_max))", "language": "python", "code": "def iffti(wave, npoints=None, indep_min=None, indep_max=None):\n    r\"\"\"\n    Return the imaginary part of the inverse Fast Fourier Transform of a waveform.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param npoints: Number of points to use in the transform. If **npoints**\n                    is less than the size of the independent variable vector\n                    the waveform is truncated; if **npoints** is greater than\n                    the size of the independent variable vector, the waveform\n                    is zero-padded\n    :type  npoints: positive integer\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.iffti\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`npoints\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n     * RuntimeError (Non-uniform frequency spacing)\n\n    .. [[[end]]]\n    \"\"\"\n    return imag(ifft(wave, npoints, indep_min, indep_max))", "code_tokens": ["def", "iffti", "(", "wave", ",", "npoints", "=", "None", ",", "indep_min", "=", "None", ",", "indep_max", "=", "None", ")", ":", "return", "imag", "(", "ifft", "(", "wave", ",", "npoints", ",", "indep_min", ",", "indep_max", ")", ")"], "docstring": "r\"\"\"\n    Return the imaginary part of the inverse Fast Fourier Transform of a waveform.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param npoints: Number of points to use in the transform. If **npoints**\n                    is less than the size of the independent variable vector\n                    the waveform is truncated; if **npoints** is greater than\n                    the size of the independent variable vector, the waveform\n                    is zero-padded\n    :type  npoints: positive integer\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.iffti\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`npoints\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n     * RuntimeError (Non-uniform frequency spacing)\n\n    .. [[[end]]]", "docstring_tokens": ["r", "Return", "the", "imaginary", "part", "of", "the", "inverse", "Fast", "Fourier", "Transform", "of", "a", "waveform", "."], "sha": "976935377adaa3de26fc5677aceb2cdfbd6f93a7", "url": "https://github.com/pmacosta/peng/blob/976935377adaa3de26fc5677aceb2cdfbd6f93a7/peng/wave_functions.py#L1115-L1157", "partition": "test"}
{"repo": "streamlink/streamlink", "path": "src/streamlink/plugin/api/http_session.py", "func_name": "HTTPSession.determine_json_encoding", "original_string": "def determine_json_encoding(cls, sample):\n        \"\"\"\n        Determine which Unicode encoding the JSON text sample is encoded with\n\n        RFC4627 (http://www.ietf.org/rfc/rfc4627.txt) suggests that the encoding of JSON text can be determined\n        by checking the pattern of NULL bytes in first 4 octets of the text.\n        :param sample: a sample of at least 4 bytes of the JSON text\n        :return: the most likely encoding of the JSON text\n        \"\"\"\n        nulls_at = [i for i, j in enumerate(bytearray(sample[:4])) if j == 0]\n        if nulls_at == [0, 1, 2]:\n            return \"UTF-32BE\"\n        elif nulls_at == [0, 2]:\n            return \"UTF-16BE\"\n        elif nulls_at == [1, 2, 3]:\n            return \"UTF-32LE\"\n        elif nulls_at == [1, 3]:\n            return \"UTF-16LE\"\n        else:\n            return \"UTF-8\"", "language": "python", "code": "def determine_json_encoding(cls, sample):\n        \"\"\"\n        Determine which Unicode encoding the JSON text sample is encoded with\n\n        RFC4627 (http://www.ietf.org/rfc/rfc4627.txt) suggests that the encoding of JSON text can be determined\n        by checking the pattern of NULL bytes in first 4 octets of the text.\n        :param sample: a sample of at least 4 bytes of the JSON text\n        :return: the most likely encoding of the JSON text\n        \"\"\"\n        nulls_at = [i for i, j in enumerate(bytearray(sample[:4])) if j == 0]\n        if nulls_at == [0, 1, 2]:\n            return \"UTF-32BE\"\n        elif nulls_at == [0, 2]:\n            return \"UTF-16BE\"\n        elif nulls_at == [1, 2, 3]:\n            return \"UTF-32LE\"\n        elif nulls_at == [1, 3]:\n            return \"UTF-16LE\"\n        else:\n            return \"UTF-8\"", "code_tokens": ["def", "determine_json_encoding", "(", "cls", ",", "sample", ")", ":", "nulls_at", "=", "[", "i", "for", "i", ",", "j", "in", "enumerate", "(", "bytearray", "(", "sample", "[", ":", "4", "]", ")", ")", "if", "j", "==", "0", "]", "if", "nulls_at", "==", "[", "0", ",", "1", ",", "2", "]", ":", "return", "\"UTF-32BE\"", "elif", "nulls_at", "==", "[", "0", ",", "2", "]", ":", "return", "\"UTF-16BE\"", "elif", "nulls_at", "==", "[", "1", ",", "2", ",", "3", "]", ":", "return", "\"UTF-32LE\"", "elif", "nulls_at", "==", "[", "1", ",", "3", "]", ":", "return", "\"UTF-16LE\"", "else", ":", "return", "\"UTF-8\""], "docstring": "Determine which Unicode encoding the JSON text sample is encoded with\n\n        RFC4627 (http://www.ietf.org/rfc/rfc4627.txt) suggests that the encoding of JSON text can be determined\n        by checking the pattern of NULL bytes in first 4 octets of the text.\n        :param sample: a sample of at least 4 bytes of the JSON text\n        :return: the most likely encoding of the JSON text", "docstring_tokens": ["Determine", "which", "Unicode", "encoding", "the", "JSON", "text", "sample", "is", "encoded", "with"], "sha": "c8ed1daff14ac03195870238b9b900c1109dd5c1", "url": "https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink/plugin/api/http_session.py#L76-L95", "partition": "test"}
{"repo": "Yelp/py_zipkin", "path": "py_zipkin/zipkin.py", "func_name": "create_http_headers_for_new_span", "original_string": "def create_http_headers_for_new_span(context_stack=None, tracer=None):\n    \"\"\"\n    Generate the headers for a new zipkin span.\n\n    .. note::\n\n        If the method is not called from within a zipkin_trace context,\n        empty dict will be returned back.\n\n    :returns: dict containing (X-B3-TraceId, X-B3-SpanId, X-B3-ParentSpanId,\n                X-B3-Flags and X-B3-Sampled) keys OR an empty dict.\n    \"\"\"\n    if tracer:\n        zipkin_attrs = tracer.get_zipkin_attrs()\n    elif context_stack:\n        zipkin_attrs = context_stack.get()\n    else:\n        zipkin_attrs = get_default_tracer().get_zipkin_attrs()\n\n    if not zipkin_attrs:\n        return {}\n\n    return {\n        'X-B3-TraceId': zipkin_attrs.trace_id,\n        'X-B3-SpanId': generate_random_64bit_string(),\n        'X-B3-ParentSpanId': zipkin_attrs.span_id,\n        'X-B3-Flags': '0',\n        'X-B3-Sampled': '1' if zipkin_attrs.is_sampled else '0',\n    }", "language": "python", "code": "def create_http_headers_for_new_span(context_stack=None, tracer=None):\n    \"\"\"\n    Generate the headers for a new zipkin span.\n\n    .. note::\n\n        If the method is not called from within a zipkin_trace context,\n        empty dict will be returned back.\n\n    :returns: dict containing (X-B3-TraceId, X-B3-SpanId, X-B3-ParentSpanId,\n                X-B3-Flags and X-B3-Sampled) keys OR an empty dict.\n    \"\"\"\n    if tracer:\n        zipkin_attrs = tracer.get_zipkin_attrs()\n    elif context_stack:\n        zipkin_attrs = context_stack.get()\n    else:\n        zipkin_attrs = get_default_tracer().get_zipkin_attrs()\n\n    if not zipkin_attrs:\n        return {}\n\n    return {\n        'X-B3-TraceId': zipkin_attrs.trace_id,\n        'X-B3-SpanId': generate_random_64bit_string(),\n        'X-B3-ParentSpanId': zipkin_attrs.span_id,\n        'X-B3-Flags': '0',\n        'X-B3-Sampled': '1' if zipkin_attrs.is_sampled else '0',\n    }", "code_tokens": ["def", "create_http_headers_for_new_span", "(", "context_stack", "=", "None", ",", "tracer", "=", "None", ")", ":", "if", "tracer", ":", "zipkin_attrs", "=", "tracer", ".", "get_zipkin_attrs", "(", ")", "elif", "context_stack", ":", "zipkin_attrs", "=", "context_stack", ".", "get", "(", ")", "else", ":", "zipkin_attrs", "=", "get_default_tracer", "(", ")", ".", "get_zipkin_attrs", "(", ")", "if", "not", "zipkin_attrs", ":", "return", "{", "}", "return", "{", "'X-B3-TraceId'", ":", "zipkin_attrs", ".", "trace_id", ",", "'X-B3-SpanId'", ":", "generate_random_64bit_string", "(", ")", ",", "'X-B3-ParentSpanId'", ":", "zipkin_attrs", ".", "span_id", ",", "'X-B3-Flags'", ":", "'0'", ",", "'X-B3-Sampled'", ":", "'1'", "if", "zipkin_attrs", ".", "is_sampled", "else", "'0'", ",", "}"], "docstring": "Generate the headers for a new zipkin span.\n\n    .. note::\n\n        If the method is not called from within a zipkin_trace context,\n        empty dict will be returned back.\n\n    :returns: dict containing (X-B3-TraceId, X-B3-SpanId, X-B3-ParentSpanId,\n                X-B3-Flags and X-B3-Sampled) keys OR an empty dict.", "docstring_tokens": ["Generate", "the", "headers", "for", "a", "new", "zipkin", "span", "."], "sha": "0944d9a3fb1f1798dbb276694aeed99f2b4283ba", "url": "https://github.com/Yelp/py_zipkin/blob/0944d9a3fb1f1798dbb276694aeed99f2b4283ba/py_zipkin/zipkin.py#L683-L711", "partition": "test"}
{"repo": "BD2KGenomics/toil-lib", "path": "src/toil_lib/abstractPipelineWrapper.py", "func_name": "AbstractPipelineWrapper._add_option", "original_string": "def _add_option(self, arg_parser, name, *args, **kwargs):\n        \"\"\"\n        Add an argument to the given arg_parser with the given name.\n\n        :param argparse.ArgumentParser arg_parser:\n        :param str name: The name of the option.\n        \"\"\"\n        arg_parser.add_argument('--' + name, *args, **kwargs)", "language": "python", "code": "def _add_option(self, arg_parser, name, *args, **kwargs):\n        \"\"\"\n        Add an argument to the given arg_parser with the given name.\n\n        :param argparse.ArgumentParser arg_parser:\n        :param str name: The name of the option.\n        \"\"\"\n        arg_parser.add_argument('--' + name, *args, **kwargs)", "code_tokens": ["def", "_add_option", "(", "self", ",", "arg_parser", ",", "name", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "arg_parser", ".", "add_argument", "(", "'--'", "+", "name", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Add an argument to the given arg_parser with the given name.\n\n        :param argparse.ArgumentParser arg_parser:\n        :param str name: The name of the option.", "docstring_tokens": ["Add", "an", "argument", "to", "the", "given", "arg_parser", "with", "the", "given", "name", "."], "sha": "022a615fc3dc98fc1aaa7bfd232409962ca44fbd", "url": "https://github.com/BD2KGenomics/toil-lib/blob/022a615fc3dc98fc1aaa7bfd232409962ca44fbd/src/toil_lib/abstractPipelineWrapper.py#L185-L192", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/parallel/client/asyncresult.py", "func_name": "AsyncResult.elapsed", "original_string": "def elapsed(self):\n        \"\"\"elapsed time since initial submission\"\"\"\n        if self.ready():\n            return self.wall_time\n        \n        now = submitted = datetime.now()\n        for msg_id in self.msg_ids:\n            if msg_id in self._client.metadata:\n                stamp = self._client.metadata[msg_id]['submitted']\n                if stamp and stamp < submitted:\n                    submitted = stamp\n        return _total_seconds(now-submitted)", "language": "python", "code": "def elapsed(self):\n        \"\"\"elapsed time since initial submission\"\"\"\n        if self.ready():\n            return self.wall_time\n        \n        now = submitted = datetime.now()\n        for msg_id in self.msg_ids:\n            if msg_id in self._client.metadata:\n                stamp = self._client.metadata[msg_id]['submitted']\n                if stamp and stamp < submitted:\n                    submitted = stamp\n        return _total_seconds(now-submitted)", "code_tokens": ["def", "elapsed", "(", "self", ")", ":", "if", "self", ".", "ready", "(", ")", ":", "return", "self", ".", "wall_time", "now", "=", "submitted", "=", "datetime", ".", "now", "(", ")", "for", "msg_id", "in", "self", ".", "msg_ids", ":", "if", "msg_id", "in", "self", ".", "_client", ".", "metadata", ":", "stamp", "=", "self", ".", "_client", ".", "metadata", "[", "msg_id", "]", "[", "'submitted'", "]", "if", "stamp", "and", "stamp", "<", "submitted", ":", "submitted", "=", "stamp", "return", "_total_seconds", "(", "now", "-", "submitted", ")"], "docstring": "elapsed time since initial submission", "docstring_tokens": ["elapsed", "time", "since", "initial", "submission"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/parallel/client/asyncresult.py#L334-L345", "partition": "test"}
{"repo": "uogbuji/amara3-xml", "path": "pylib/uxml/html5.py", "func_name": "markup_fragment", "original_string": "def markup_fragment(source, encoding=None):\n    '''\n    Parse a fragment if markup in HTML mode, and return a bindery node\n\n    Warning: if you pass a string, you must make sure it's a byte string, not a Unicode object.  You might also want to wrap it with amara.lib.inputsource.text if it's not obviously XML or HTML (for example it could be confused with a file name)\n\n    from amara.lib import inputsource\n    from amara.bindery import html\n    doc = html.markup_fragment(inputsource.text('XXX<html><body onload=\"\" color=\"white\"><p>Spam!<p>Eggs!</body></html>YYY'))\n\n    See also: http://wiki.xml3k.org/Amara2/Tagsoup\n    '''\n    doc = parse(source, encoding=encoding)\n    frag = doc.html.body\n    return frag", "language": "python", "code": "def markup_fragment(source, encoding=None):\n    '''\n    Parse a fragment if markup in HTML mode, and return a bindery node\n\n    Warning: if you pass a string, you must make sure it's a byte string, not a Unicode object.  You might also want to wrap it with amara.lib.inputsource.text if it's not obviously XML or HTML (for example it could be confused with a file name)\n\n    from amara.lib import inputsource\n    from amara.bindery import html\n    doc = html.markup_fragment(inputsource.text('XXX<html><body onload=\"\" color=\"white\"><p>Spam!<p>Eggs!</body></html>YYY'))\n\n    See also: http://wiki.xml3k.org/Amara2/Tagsoup\n    '''\n    doc = parse(source, encoding=encoding)\n    frag = doc.html.body\n    return frag", "code_tokens": ["def", "markup_fragment", "(", "source", ",", "encoding", "=", "None", ")", ":", "doc", "=", "parse", "(", "source", ",", "encoding", "=", "encoding", ")", "frag", "=", "doc", ".", "html", ".", "body", "return", "frag"], "docstring": "Parse a fragment if markup in HTML mode, and return a bindery node\n\n    Warning: if you pass a string, you must make sure it's a byte string, not a Unicode object.  You might also want to wrap it with amara.lib.inputsource.text if it's not obviously XML or HTML (for example it could be confused with a file name)\n\n    from amara.lib import inputsource\n    from amara.bindery import html\n    doc = html.markup_fragment(inputsource.text('XXX<html><body onload=\"\" color=\"white\"><p>Spam!<p>Eggs!</body></html>YYY'))\n\n    See also: http://wiki.xml3k.org/Amara2/Tagsoup", "docstring_tokens": ["Parse", "a", "fragment", "if", "markup", "in", "HTML", "mode", "and", "return", "a", "bindery", "node"], "sha": "88c18876418cffc89bb85b4a3193e5002b6b39a6", "url": "https://github.com/uogbuji/amara3-xml/blob/88c18876418cffc89bb85b4a3193e5002b6b39a6/pylib/uxml/html5.py#L253-L267", "partition": "test"}
{"repo": "trec-kba/streamcorpus-pipeline", "path": "streamcorpus_pipeline/stages.py", "func_name": "StageRegistry.init_stage", "original_string": "def init_stage(self, name, config):\n\n        '''Construct and configure a stage from known stages.\n\n        `name` must be the name of one of the stages in this.  `config`\n        is the configuration dictionary of the containing object, and its `name`\n        member will be passed into the stage constructor.\n\n        :param str name: name of the stage\n        :param dict config: parent object configuration\n        :return: callable stage\n        :raise exceptions.KeyError: if `name` is not a known stage\n\n        '''\n        subconfig = config.get(name, {})\n        ctor = self[name]\n        return ctor(subconfig)", "language": "python", "code": "def init_stage(self, name, config):\n\n        '''Construct and configure a stage from known stages.\n\n        `name` must be the name of one of the stages in this.  `config`\n        is the configuration dictionary of the containing object, and its `name`\n        member will be passed into the stage constructor.\n\n        :param str name: name of the stage\n        :param dict config: parent object configuration\n        :return: callable stage\n        :raise exceptions.KeyError: if `name` is not a known stage\n\n        '''\n        subconfig = config.get(name, {})\n        ctor = self[name]\n        return ctor(subconfig)", "code_tokens": ["def", "init_stage", "(", "self", ",", "name", ",", "config", ")", ":", "subconfig", "=", "config", ".", "get", "(", "name", ",", "{", "}", ")", "ctor", "=", "self", "[", "name", "]", "return", "ctor", "(", "subconfig", ")"], "docstring": "Construct and configure a stage from known stages.\n\n        `name` must be the name of one of the stages in this.  `config`\n        is the configuration dictionary of the containing object, and its `name`\n        member will be passed into the stage constructor.\n\n        :param str name: name of the stage\n        :param dict config: parent object configuration\n        :return: callable stage\n        :raise exceptions.KeyError: if `name` is not a known stage", "docstring_tokens": ["Construct", "and", "configure", "a", "stage", "from", "known", "stages", "."], "sha": "8bb82ea1beb83c6b40ed03fa1659df2897c2292a", "url": "https://github.com/trec-kba/streamcorpus-pipeline/blob/8bb82ea1beb83c6b40ed03fa1659df2897c2292a/streamcorpus_pipeline/stages.py#L235-L251", "partition": "test"}
{"repo": "mental32/spotify.py", "path": "spotify/models/user.py", "func_name": "User.get_devices", "original_string": "async def get_devices(self) -> List[Device]:\n        \"\"\"Get information about the users avaliable devices.\n\n        Returns\n        -------\n        devices : List[Device]\n            The devices the user has available.\n        \"\"\"\n        data = await self.http.available_devices()\n        return [Device(item) for item in data['devices']]", "language": "python", "code": "async def get_devices(self) -> List[Device]:\n        \"\"\"Get information about the users avaliable devices.\n\n        Returns\n        -------\n        devices : List[Device]\n            The devices the user has available.\n        \"\"\"\n        data = await self.http.available_devices()\n        return [Device(item) for item in data['devices']]", "code_tokens": ["async", "def", "get_devices", "(", "self", ")", "->", "List", "[", "Device", "]", ":", "data", "=", "await", "self", ".", "http", ".", "available_devices", "(", ")", "return", "[", "Device", "(", "item", ")", "for", "item", "in", "data", "[", "'devices'", "]", "]"], "docstring": "Get information about the users avaliable devices.\n\n        Returns\n        -------\n        devices : List[Device]\n            The devices the user has available.", "docstring_tokens": ["Get", "information", "about", "the", "users", "avaliable", "devices", "."], "sha": "bb296cac7c3dd289908906b7069bd80f43950515", "url": "https://github.com/mental32/spotify.py/blob/bb296cac7c3dd289908906b7069bd80f43950515/spotify/models/user.py#L186-L195", "partition": "test"}
{"repo": "pmacosta/peng", "path": "peng/wave_functions.py", "func_name": "nmax", "original_string": "def nmax(wave, indep_min=None, indep_max=None):\n    r\"\"\"\n    Return the maximum of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: float\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.nmax\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n    .. [[[end]]]\n    \"\"\"\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    return np.max(ret._dep_vector)", "language": "python", "code": "def nmax(wave, indep_min=None, indep_max=None):\n    r\"\"\"\n    Return the maximum of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: float\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.nmax\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n    .. [[[end]]]\n    \"\"\"\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    return np.max(ret._dep_vector)", "code_tokens": ["def", "nmax", "(", "wave", ",", "indep_min", "=", "None", ",", "indep_max", "=", "None", ")", ":", "ret", "=", "copy", ".", "copy", "(", "wave", ")", "_bound_waveform", "(", "ret", ",", "indep_min", ",", "indep_max", ")", "return", "np", ".", "max", "(", "ret", ".", "_dep_vector", ")"], "docstring": "r\"\"\"\n    Return the maximum of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: float\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.nmax\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n    .. [[[end]]]", "docstring_tokens": ["r", "Return", "the", "maximum", "of", "a", "waveform", "s", "dependent", "variable", "vector", "."], "sha": "976935377adaa3de26fc5677aceb2cdfbd6f93a7", "url": "https://github.com/pmacosta/peng/blob/976935377adaa3de26fc5677aceb2cdfbd6f93a7/peng/wave_functions.py#L1555-L1588", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/internal/missing_values_util.py", "func_name": "moments_of_masked_time_series", "original_string": "def moments_of_masked_time_series(time_series_tensor, broadcast_mask):\n  \"\"\"Compute mean and variance, accounting for a mask.\n\n  Args:\n    time_series_tensor: float `Tensor` time series of shape\n      `concat([batch_shape, [num_timesteps]])`.\n    broadcast_mask: bool `Tensor` of the same shape as `time_series`.\n  Returns:\n    mean: float `Tensor` of shape `batch_shape`.\n    variance: float `Tensor` of shape `batch_shape`.\n  \"\"\"\n  num_unmasked_entries = tf.cast(\n      tf.reduce_sum(input_tensor=tf.cast(~broadcast_mask, tf.int32), axis=-1),\n      time_series_tensor.dtype)\n\n  # Manually compute mean and variance, excluding masked entries.\n  mean = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      time_series_tensor), axis=-1) / num_unmasked_entries)\n  variance = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      (time_series_tensor - mean[..., tf.newaxis]) ** 2), axis=-1)\n              / num_unmasked_entries)\n  return mean, variance", "language": "python", "code": "def moments_of_masked_time_series(time_series_tensor, broadcast_mask):\n  \"\"\"Compute mean and variance, accounting for a mask.\n\n  Args:\n    time_series_tensor: float `Tensor` time series of shape\n      `concat([batch_shape, [num_timesteps]])`.\n    broadcast_mask: bool `Tensor` of the same shape as `time_series`.\n  Returns:\n    mean: float `Tensor` of shape `batch_shape`.\n    variance: float `Tensor` of shape `batch_shape`.\n  \"\"\"\n  num_unmasked_entries = tf.cast(\n      tf.reduce_sum(input_tensor=tf.cast(~broadcast_mask, tf.int32), axis=-1),\n      time_series_tensor.dtype)\n\n  # Manually compute mean and variance, excluding masked entries.\n  mean = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      time_series_tensor), axis=-1) / num_unmasked_entries)\n  variance = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      (time_series_tensor - mean[..., tf.newaxis]) ** 2), axis=-1)\n              / num_unmasked_entries)\n  return mean, variance", "code_tokens": ["def", "moments_of_masked_time_series", "(", "time_series_tensor", ",", "broadcast_mask", ")", ":", "num_unmasked_entries", "=", "tf", ".", "cast", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "cast", "(", "~", "broadcast_mask", ",", "tf", ".", "int32", ")", ",", "axis", "=", "-", "1", ")", ",", "time_series_tensor", ".", "dtype", ")", "# Manually compute mean and variance, excluding masked entries.", "mean", "=", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "where", "(", "broadcast_mask", ",", "tf", ".", "zeros_like", "(", "time_series_tensor", ")", ",", "time_series_tensor", ")", ",", "axis", "=", "-", "1", ")", "/", "num_unmasked_entries", ")", "variance", "=", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "where", "(", "broadcast_mask", ",", "tf", ".", "zeros_like", "(", "time_series_tensor", ")", ",", "(", "time_series_tensor", "-", "mean", "[", "...", ",", "tf", ".", "newaxis", "]", ")", "**", "2", ")", ",", "axis", "=", "-", "1", ")", "/", "num_unmasked_entries", ")", "return", "mean", ",", "variance"], "docstring": "Compute mean and variance, accounting for a mask.\n\n  Args:\n    time_series_tensor: float `Tensor` time series of shape\n      `concat([batch_shape, [num_timesteps]])`.\n    broadcast_mask: bool `Tensor` of the same shape as `time_series`.\n  Returns:\n    mean: float `Tensor` of shape `batch_shape`.\n    variance: float `Tensor` of shape `batch_shape`.", "docstring_tokens": ["Compute", "mean", "and", "variance", "accounting", "for", "a", "mask", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/internal/missing_values_util.py#L109-L134", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/meetup.py", "func_name": "MeetupClient.events", "original_string": "def events(self, group, from_date=DEFAULT_DATETIME):\n        \"\"\"Fetch the events pages of a given group.\"\"\"\n\n        date = datetime_to_utc(from_date)\n        date = date.strftime(\"since:%Y-%m-%dT%H:%M:%S.000Z\")\n\n        resource = urijoin(group, self.REVENTS)\n\n        # Hack required due to Metup API does not support list\n        # values with the format `?param=value1&param=value2`.\n        # It only works with `?param=value1,value2`.\n        # Morever, urrlib3 encodes comma characters when values\n        # are given using params dict, which it doesn't work\n        # with Meetup, either.\n        fixed_params = '?' + self.PFIELDS + '=' + ','.join(self.VEVENT_FIELDS)\n        fixed_params += '&' + self.PSTATUS + '=' + ','.join(self.VSTATUS)\n        resource += fixed_params\n\n        params = {\n            self.PORDER: self.VUPDATED,\n            self.PSCROLL: date,\n            self.PPAGE: self.max_items\n        }\n\n        try:\n            for page in self._fetch(resource, params):\n                yield page\n        except requests.exceptions.HTTPError as error:\n            if error.response.status_code == 410:\n                msg = \"Group is no longer accessible: {}\".format(error)\n                raise RepositoryError(cause=msg)\n            else:\n                raise error", "language": "python", "code": "def events(self, group, from_date=DEFAULT_DATETIME):\n        \"\"\"Fetch the events pages of a given group.\"\"\"\n\n        date = datetime_to_utc(from_date)\n        date = date.strftime(\"since:%Y-%m-%dT%H:%M:%S.000Z\")\n\n        resource = urijoin(group, self.REVENTS)\n\n        # Hack required due to Metup API does not support list\n        # values with the format `?param=value1&param=value2`.\n        # It only works with `?param=value1,value2`.\n        # Morever, urrlib3 encodes comma characters when values\n        # are given using params dict, which it doesn't work\n        # with Meetup, either.\n        fixed_params = '?' + self.PFIELDS + '=' + ','.join(self.VEVENT_FIELDS)\n        fixed_params += '&' + self.PSTATUS + '=' + ','.join(self.VSTATUS)\n        resource += fixed_params\n\n        params = {\n            self.PORDER: self.VUPDATED,\n            self.PSCROLL: date,\n            self.PPAGE: self.max_items\n        }\n\n        try:\n            for page in self._fetch(resource, params):\n                yield page\n        except requests.exceptions.HTTPError as error:\n            if error.response.status_code == 410:\n                msg = \"Group is no longer accessible: {}\".format(error)\n                raise RepositoryError(cause=msg)\n            else:\n                raise error", "code_tokens": ["def", "events", "(", "self", ",", "group", ",", "from_date", "=", "DEFAULT_DATETIME", ")", ":", "date", "=", "datetime_to_utc", "(", "from_date", ")", "date", "=", "date", ".", "strftime", "(", "\"since:%Y-%m-%dT%H:%M:%S.000Z\"", ")", "resource", "=", "urijoin", "(", "group", ",", "self", ".", "REVENTS", ")", "# Hack required due to Metup API does not support list", "# values with the format `?param=value1&param=value2`.", "# It only works with `?param=value1,value2`.", "# Morever, urrlib3 encodes comma characters when values", "# are given using params dict, which it doesn't work", "# with Meetup, either.", "fixed_params", "=", "'?'", "+", "self", ".", "PFIELDS", "+", "'='", "+", "','", ".", "join", "(", "self", ".", "VEVENT_FIELDS", ")", "fixed_params", "+=", "'&'", "+", "self", ".", "PSTATUS", "+", "'='", "+", "','", ".", "join", "(", "self", ".", "VSTATUS", ")", "resource", "+=", "fixed_params", "params", "=", "{", "self", ".", "PORDER", ":", "self", ".", "VUPDATED", ",", "self", ".", "PSCROLL", ":", "date", ",", "self", ".", "PPAGE", ":", "self", ".", "max_items", "}", "try", ":", "for", "page", "in", "self", ".", "_fetch", "(", "resource", ",", "params", ")", ":", "yield", "page", "except", "requests", ".", "exceptions", ".", "HTTPError", "as", "error", ":", "if", "error", ".", "response", ".", "status_code", "==", "410", ":", "msg", "=", "\"Group is no longer accessible: {}\"", ".", "format", "(", "error", ")", "raise", "RepositoryError", "(", "cause", "=", "msg", ")", "else", ":", "raise", "error"], "docstring": "Fetch the events pages of a given group.", "docstring_tokens": ["Fetch", "the", "events", "pages", "of", "a", "given", "group", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/meetup.py#L361-L393", "partition": "test"}
{"repo": "aholkner/bacon", "path": "native/Vendor/FreeType/src/tools/glnames.py", "func_name": "dump_encoding", "original_string": "def dump_encoding( file, encoding_name, encoding_list ):\n  \"\"\"dump a given encoding\"\"\"\n\n  write = file.write\n  write( \"  /* the following are indices into the SID name table */\\n\" )\n  write( \"  static const unsigned short  \" + encoding_name +\n         \"[\" + repr( len( encoding_list ) ) + \"] =\\n\" )\n  write( \"  {\\n\" )\n\n  line  = \"    \"\n  comma = \"\"\n  col   = 0\n  for value in encoding_list:\n    line += comma\n    line += \"%3d\" % value\n    comma = \",\"\n    col  += 1\n    if col == 16:\n      col = 0\n      comma = \",\\n    \"\n\n  write( line + \"\\n  };\\n\\n\\n\" )", "language": "python", "code": "def dump_encoding( file, encoding_name, encoding_list ):\n  \"\"\"dump a given encoding\"\"\"\n\n  write = file.write\n  write( \"  /* the following are indices into the SID name table */\\n\" )\n  write( \"  static const unsigned short  \" + encoding_name +\n         \"[\" + repr( len( encoding_list ) ) + \"] =\\n\" )\n  write( \"  {\\n\" )\n\n  line  = \"    \"\n  comma = \"\"\n  col   = 0\n  for value in encoding_list:\n    line += comma\n    line += \"%3d\" % value\n    comma = \",\"\n    col  += 1\n    if col == 16:\n      col = 0\n      comma = \",\\n    \"\n\n  write( line + \"\\n  };\\n\\n\\n\" )", "code_tokens": ["def", "dump_encoding", "(", "file", ",", "encoding_name", ",", "encoding_list", ")", ":", "write", "=", "file", ".", "write", "write", "(", "\"  /* the following are indices into the SID name table */\\n\"", ")", "write", "(", "\"  static const unsigned short  \"", "+", "encoding_name", "+", "\"[\"", "+", "repr", "(", "len", "(", "encoding_list", ")", ")", "+", "\"] =\\n\"", ")", "write", "(", "\"  {\\n\"", ")", "line", "=", "\"    \"", "comma", "=", "\"\"", "col", "=", "0", "for", "value", "in", "encoding_list", ":", "line", "+=", "comma", "line", "+=", "\"%3d\"", "%", "value", "comma", "=", "\",\"", "col", "+=", "1", "if", "col", "==", "16", ":", "col", "=", "0", "comma", "=", "\",\\n    \"", "write", "(", "line", "+", "\"\\n  };\\n\\n\\n\"", ")"], "docstring": "dump a given encoding", "docstring_tokens": ["dump", "a", "given", "encoding"], "sha": "edf3810dcb211942d392a8637945871399b0650d", "url": "https://github.com/aholkner/bacon/blob/edf3810dcb211942d392a8637945871399b0650d/native/Vendor/FreeType/src/tools/glnames.py#L5186-L5207", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/python3.py", "func_name": "_in_iterating_context", "original_string": "def _in_iterating_context(node):\n    \"\"\"Check if the node is being used as an iterator.\n\n    Definition is taken from lib2to3.fixer_util.in_special_context().\n    \"\"\"\n    parent = node.parent\n    # Since a call can't be the loop variant we only need to know if the node's\n    # parent is a 'for' loop to know it's being used as the iterator for the\n    # loop.\n    if isinstance(parent, astroid.For):\n        return True\n    # Need to make sure the use of the node is in the iterator part of the\n    # comprehension.\n    if isinstance(parent, astroid.Comprehension):\n        if parent.iter == node:\n            return True\n    # Various built-ins can take in an iterable or list and lead to the same\n    # value.\n    elif isinstance(parent, astroid.Call):\n        if isinstance(parent.func, astroid.Name):\n            parent_scope = parent.func.lookup(parent.func.name)[0]\n            if _is_builtin(parent_scope) and parent.func.name in _ACCEPTS_ITERATOR:\n                return True\n        elif isinstance(parent.func, astroid.Attribute):\n            if parent.func.attrname in ATTRIBUTES_ACCEPTS_ITERATOR:\n                return True\n\n        inferred = utils.safe_infer(parent.func)\n        if inferred:\n            if inferred.qname() in _BUILTIN_METHOD_ACCEPTS_ITERATOR:\n                return True\n            root = inferred.root()\n            if root and root.name == \"itertools\":\n                return True\n    # If the call is in an unpacking, there's no need to warn,\n    # since it can be considered iterating.\n    elif isinstance(parent, astroid.Assign) and isinstance(\n        parent.targets[0], (astroid.List, astroid.Tuple)\n    ):\n        if len(parent.targets[0].elts) > 1:\n            return True\n    # If the call is in a containment check, we consider that to\n    # be an iterating context\n    elif (\n        isinstance(parent, astroid.Compare)\n        and len(parent.ops) == 1\n        and parent.ops[0][0] == \"in\"\n    ):\n        return True\n    # Also if it's an `yield from`, that's fair\n    elif isinstance(parent, astroid.YieldFrom):\n        return True\n    if isinstance(parent, astroid.Starred):\n        return True\n    return False", "language": "python", "code": "def _in_iterating_context(node):\n    \"\"\"Check if the node is being used as an iterator.\n\n    Definition is taken from lib2to3.fixer_util.in_special_context().\n    \"\"\"\n    parent = node.parent\n    # Since a call can't be the loop variant we only need to know if the node's\n    # parent is a 'for' loop to know it's being used as the iterator for the\n    # loop.\n    if isinstance(parent, astroid.For):\n        return True\n    # Need to make sure the use of the node is in the iterator part of the\n    # comprehension.\n    if isinstance(parent, astroid.Comprehension):\n        if parent.iter == node:\n            return True\n    # Various built-ins can take in an iterable or list and lead to the same\n    # value.\n    elif isinstance(parent, astroid.Call):\n        if isinstance(parent.func, astroid.Name):\n            parent_scope = parent.func.lookup(parent.func.name)[0]\n            if _is_builtin(parent_scope) and parent.func.name in _ACCEPTS_ITERATOR:\n                return True\n        elif isinstance(parent.func, astroid.Attribute):\n            if parent.func.attrname in ATTRIBUTES_ACCEPTS_ITERATOR:\n                return True\n\n        inferred = utils.safe_infer(parent.func)\n        if inferred:\n            if inferred.qname() in _BUILTIN_METHOD_ACCEPTS_ITERATOR:\n                return True\n            root = inferred.root()\n            if root and root.name == \"itertools\":\n                return True\n    # If the call is in an unpacking, there's no need to warn,\n    # since it can be considered iterating.\n    elif isinstance(parent, astroid.Assign) and isinstance(\n        parent.targets[0], (astroid.List, astroid.Tuple)\n    ):\n        if len(parent.targets[0].elts) > 1:\n            return True\n    # If the call is in a containment check, we consider that to\n    # be an iterating context\n    elif (\n        isinstance(parent, astroid.Compare)\n        and len(parent.ops) == 1\n        and parent.ops[0][0] == \"in\"\n    ):\n        return True\n    # Also if it's an `yield from`, that's fair\n    elif isinstance(parent, astroid.YieldFrom):\n        return True\n    if isinstance(parent, astroid.Starred):\n        return True\n    return False", "code_tokens": ["def", "_in_iterating_context", "(", "node", ")", ":", "parent", "=", "node", ".", "parent", "# Since a call can't be the loop variant we only need to know if the node's", "# parent is a 'for' loop to know it's being used as the iterator for the", "# loop.", "if", "isinstance", "(", "parent", ",", "astroid", ".", "For", ")", ":", "return", "True", "# Need to make sure the use of the node is in the iterator part of the", "# comprehension.", "if", "isinstance", "(", "parent", ",", "astroid", ".", "Comprehension", ")", ":", "if", "parent", ".", "iter", "==", "node", ":", "return", "True", "# Various built-ins can take in an iterable or list and lead to the same", "# value.", "elif", "isinstance", "(", "parent", ",", "astroid", ".", "Call", ")", ":", "if", "isinstance", "(", "parent", ".", "func", ",", "astroid", ".", "Name", ")", ":", "parent_scope", "=", "parent", ".", "func", ".", "lookup", "(", "parent", ".", "func", ".", "name", ")", "[", "0", "]", "if", "_is_builtin", "(", "parent_scope", ")", "and", "parent", ".", "func", ".", "name", "in", "_ACCEPTS_ITERATOR", ":", "return", "True", "elif", "isinstance", "(", "parent", ".", "func", ",", "astroid", ".", "Attribute", ")", ":", "if", "parent", ".", "func", ".", "attrname", "in", "ATTRIBUTES_ACCEPTS_ITERATOR", ":", "return", "True", "inferred", "=", "utils", ".", "safe_infer", "(", "parent", ".", "func", ")", "if", "inferred", ":", "if", "inferred", ".", "qname", "(", ")", "in", "_BUILTIN_METHOD_ACCEPTS_ITERATOR", ":", "return", "True", "root", "=", "inferred", ".", "root", "(", ")", "if", "root", "and", "root", ".", "name", "==", "\"itertools\"", ":", "return", "True", "# If the call is in an unpacking, there's no need to warn,", "# since it can be considered iterating.", "elif", "isinstance", "(", "parent", ",", "astroid", ".", "Assign", ")", "and", "isinstance", "(", "parent", ".", "targets", "[", "0", "]", ",", "(", "astroid", ".", "List", ",", "astroid", ".", "Tuple", ")", ")", ":", "if", "len", "(", "parent", ".", "targets", "[", "0", "]", ".", "elts", ")", ">", "1", ":", "return", "True", "# If the call is in a containment check, we consider that to", "# be an iterating context", "elif", "(", "isinstance", "(", "parent", ",", "astroid", ".", "Compare", ")", "and", "len", "(", "parent", ".", "ops", ")", "==", "1", "and", "parent", ".", "ops", "[", "0", "]", "[", "0", "]", "==", "\"in\"", ")", ":", "return", "True", "# Also if it's an `yield from`, that's fair", "elif", "isinstance", "(", "parent", ",", "astroid", ".", "YieldFrom", ")", ":", "return", "True", "if", "isinstance", "(", "parent", ",", "astroid", ".", "Starred", ")", ":", "return", "True", "return", "False"], "docstring": "Check if the node is being used as an iterator.\n\n    Definition is taken from lib2to3.fixer_util.in_special_context().", "docstring_tokens": ["Check", "if", "the", "node", "is", "being", "used", "as", "an", "iterator", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/python3.py#L96-L150", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicemanagement-legacy/azure/servicemanagement/_http/winhttp.py", "func_name": "_WinHttpRequest.status_text", "original_string": "def status_text(self):\n        ''' Gets status text of response. '''\n\n        bstr_status_text = c_void_p()\n        _WinHttpRequest._StatusText(self, byref(bstr_status_text))\n        bstr_status_text = ctypes.cast(bstr_status_text, c_wchar_p)\n        status_text = bstr_status_text.value\n        _SysFreeString(bstr_status_text)\n        return status_text", "language": "python", "code": "def status_text(self):\n        ''' Gets status text of response. '''\n\n        bstr_status_text = c_void_p()\n        _WinHttpRequest._StatusText(self, byref(bstr_status_text))\n        bstr_status_text = ctypes.cast(bstr_status_text, c_wchar_p)\n        status_text = bstr_status_text.value\n        _SysFreeString(bstr_status_text)\n        return status_text", "code_tokens": ["def", "status_text", "(", "self", ")", ":", "bstr_status_text", "=", "c_void_p", "(", ")", "_WinHttpRequest", ".", "_StatusText", "(", "self", ",", "byref", "(", "bstr_status_text", ")", ")", "bstr_status_text", "=", "ctypes", ".", "cast", "(", "bstr_status_text", ",", "c_wchar_p", ")", "status_text", "=", "bstr_status_text", ".", "value", "_SysFreeString", "(", "bstr_status_text", ")", "return", "status_text"], "docstring": "Gets status text of response.", "docstring_tokens": ["Gets", "status", "text", "of", "response", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/_http/winhttp.py#L320-L328", "partition": "test"}
{"repo": "vaexio/vaex", "path": "packages/vaex-core/vaex/dataframe.py", "func_name": "DataFrameArrays.add_column", "original_string": "def add_column(self, name, data):\n        \"\"\"Add a column to the DataFrame\n\n        :param str name: name of column\n        :param data: numpy array with the data\n        \"\"\"\n        # assert _is_array_type_ok(data), \"dtype not supported: %r, %r\" % (data.dtype, data.dtype.type)\n        # self._length = len(data)\n        # if self._length_unfiltered is None:\n        #     self._length_unfiltered = len(data)\n        #     self._length_original = len(data)\n        #     self._index_end = self._length_unfiltered\n        super(DataFrameArrays, self).add_column(name, data)\n        self._length_unfiltered = int(round(self._length_original * self._active_fraction))", "language": "python", "code": "def add_column(self, name, data):\n        \"\"\"Add a column to the DataFrame\n\n        :param str name: name of column\n        :param data: numpy array with the data\n        \"\"\"\n        # assert _is_array_type_ok(data), \"dtype not supported: %r, %r\" % (data.dtype, data.dtype.type)\n        # self._length = len(data)\n        # if self._length_unfiltered is None:\n        #     self._length_unfiltered = len(data)\n        #     self._length_original = len(data)\n        #     self._index_end = self._length_unfiltered\n        super(DataFrameArrays, self).add_column(name, data)\n        self._length_unfiltered = int(round(self._length_original * self._active_fraction))", "code_tokens": ["def", "add_column", "(", "self", ",", "name", ",", "data", ")", ":", "# assert _is_array_type_ok(data), \"dtype not supported: %r, %r\" % (data.dtype, data.dtype.type)", "# self._length = len(data)", "# if self._length_unfiltered is None:", "#     self._length_unfiltered = len(data)", "#     self._length_original = len(data)", "#     self._index_end = self._length_unfiltered", "super", "(", "DataFrameArrays", ",", "self", ")", ".", "add_column", "(", "name", ",", "data", ")", "self", ".", "_length_unfiltered", "=", "int", "(", "round", "(", "self", ".", "_length_original", "*", "self", ".", "_active_fraction", ")", ")"], "docstring": "Add a column to the DataFrame\n\n        :param str name: name of column\n        :param data: numpy array with the data", "docstring_tokens": ["Add", "a", "column", "to", "the", "DataFrame"], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/dataframe.py#L5420-L5433", "partition": "test"}
{"repo": "albertodonato/prometheus-aioexporter", "path": "prometheus_aioexporter/metric.py", "func_name": "MetricsRegistry.get_metric", "original_string": "def get_metric(\n            self, name: str,\n            labels: Union[Dict[str, str], None] = None) -> Metric:\n        \"\"\"Return a metric, optionally configured with labels.\"\"\"\n        metric = self._metrics[name]\n        if labels:\n            return metric.labels(**labels)\n\n        return metric", "language": "python", "code": "def get_metric(\n            self, name: str,\n            labels: Union[Dict[str, str], None] = None) -> Metric:\n        \"\"\"Return a metric, optionally configured with labels.\"\"\"\n        metric = self._metrics[name]\n        if labels:\n            return metric.labels(**labels)\n\n        return metric", "code_tokens": ["def", "get_metric", "(", "self", ",", "name", ":", "str", ",", "labels", ":", "Union", "[", "Dict", "[", "str", ",", "str", "]", ",", "None", "]", "=", "None", ")", "->", "Metric", ":", "metric", "=", "self", ".", "_metrics", "[", "name", "]", "if", "labels", ":", "return", "metric", ".", "labels", "(", "*", "*", "labels", ")", "return", "metric"], "docstring": "Return a metric, optionally configured with labels.", "docstring_tokens": ["Return", "a", "metric", "optionally", "configured", "with", "labels", "."], "sha": "e1b85544ce72bfaae9182597709a2ecede8c8242", "url": "https://github.com/albertodonato/prometheus-aioexporter/blob/e1b85544ce72bfaae9182597709a2ecede8c8242/prometheus_aioexporter/metric.py#L90-L98", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/services/server/auth/utils.py", "func_name": "get_common_names_from_certificate", "original_string": "def get_common_names_from_certificate(certificate):\n    \"\"\"\n    Given an X.509 certificate, extract and return all common names.\n    \"\"\"\n\n    common_names = certificate.subject.get_attributes_for_oid(\n        x509.oid.NameOID.COMMON_NAME\n    )\n    return [common_name.value for common_name in common_names]", "language": "python", "code": "def get_common_names_from_certificate(certificate):\n    \"\"\"\n    Given an X.509 certificate, extract and return all common names.\n    \"\"\"\n\n    common_names = certificate.subject.get_attributes_for_oid(\n        x509.oid.NameOID.COMMON_NAME\n    )\n    return [common_name.value for common_name in common_names]", "code_tokens": ["def", "get_common_names_from_certificate", "(", "certificate", ")", ":", "common_names", "=", "certificate", ".", "subject", ".", "get_attributes_for_oid", "(", "x509", ".", "oid", ".", "NameOID", ".", "COMMON_NAME", ")", "return", "[", "common_name", ".", "value", "for", "common_name", "in", "common_names", "]"], "docstring": "Given an X.509 certificate, extract and return all common names.", "docstring_tokens": ["Given", "an", "X", ".", "509", "certificate", "extract", "and", "return", "all", "common", "names", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/services/server/auth/utils.py#L48-L56", "partition": "test"}
{"repo": "trp07/messages", "path": "messages/cli.py", "func_name": "trim_args", "original_string": "def trim_args(kwds):\n    \"\"\"Gets rid of args with value of None, as well as select keys.\"\"\"\n    reject_key = (\"type\", \"types\", \"configure\")\n    reject_val = (None, ())\n    kwargs = {\n        k: v for k, v in kwds.items() if k not in reject_key and v not in reject_val\n    }\n    for k, v in kwargs.items():\n        if k in (\"to\", \"cc\", \"bcc\", \"attachments\"):\n            kwargs[k] = list(kwargs[k])\n    return kwargs", "language": "python", "code": "def trim_args(kwds):\n    \"\"\"Gets rid of args with value of None, as well as select keys.\"\"\"\n    reject_key = (\"type\", \"types\", \"configure\")\n    reject_val = (None, ())\n    kwargs = {\n        k: v for k, v in kwds.items() if k not in reject_key and v not in reject_val\n    }\n    for k, v in kwargs.items():\n        if k in (\"to\", \"cc\", \"bcc\", \"attachments\"):\n            kwargs[k] = list(kwargs[k])\n    return kwargs", "code_tokens": ["def", "trim_args", "(", "kwds", ")", ":", "reject_key", "=", "(", "\"type\"", ",", "\"types\"", ",", "\"configure\"", ")", "reject_val", "=", "(", "None", ",", "(", ")", ")", "kwargs", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "kwds", ".", "items", "(", ")", "if", "k", "not", "in", "reject_key", "and", "v", "not", "in", "reject_val", "}", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "if", "k", "in", "(", "\"to\"", ",", "\"cc\"", ",", "\"bcc\"", ",", "\"attachments\"", ")", ":", "kwargs", "[", "k", "]", "=", "list", "(", "kwargs", "[", "k", "]", ")", "return", "kwargs"], "docstring": "Gets rid of args with value of None, as well as select keys.", "docstring_tokens": ["Gets", "rid", "of", "args", "with", "value", "of", "None", "as", "well", "as", "select", "keys", "."], "sha": "7789ebc960335a59ea5d319fceed3dd349023648", "url": "https://github.com/trp07/messages/blob/7789ebc960335a59ea5d319fceed3dd349023648/messages/cli.py#L27-L37", "partition": "test"}
{"repo": "wreckage/django-happenings", "path": "happenings/views.py", "func_name": "EventDayView.check_for_cancelled_events", "original_string": "def check_for_cancelled_events(self, d):\n        \"\"\"Check if any events are cancelled on the given date 'd'.\"\"\"\n        for event in self.events:\n            for cn in event.cancellations.all():\n                if cn.date == d:\n                    event.title += ' (CANCELLED)'", "language": "python", "code": "def check_for_cancelled_events(self, d):\n        \"\"\"Check if any events are cancelled on the given date 'd'.\"\"\"\n        for event in self.events:\n            for cn in event.cancellations.all():\n                if cn.date == d:\n                    event.title += ' (CANCELLED)'", "code_tokens": ["def", "check_for_cancelled_events", "(", "self", ",", "d", ")", ":", "for", "event", "in", "self", ".", "events", ":", "for", "cn", "in", "event", ".", "cancellations", ".", "all", "(", ")", ":", "if", "cn", ".", "date", "==", "d", ":", "event", ".", "title", "+=", "' (CANCELLED)'"], "docstring": "Check if any events are cancelled on the given date 'd'.", "docstring_tokens": ["Check", "if", "any", "events", "are", "cancelled", "on", "the", "given", "date", "d", "."], "sha": "7bca5576efa6cd4c4e87356bf9e5b8cd538ae91d", "url": "https://github.com/wreckage/django-happenings/blob/7bca5576efa6cd4c4e87356bf9e5b8cd538ae91d/happenings/views.py#L145-L150", "partition": "test"}
{"repo": "uploadcare/pyuploadcare", "path": "pyuploadcare/api_resources.py", "func_name": "FilesStorage.uuids", "original_string": "def uuids(self):\n        \"\"\" Extract uuid from each item of specified ``seq``.\n        \"\"\"\n        for f in self._seq:\n            if isinstance(f, File):\n                yield f.uuid\n            elif isinstance(f, six.string_types):\n                yield f\n            else:\n                raise ValueError(\n                    'Invalid type for sequence item: {0}'.format(type(f)))", "language": "python", "code": "def uuids(self):\n        \"\"\" Extract uuid from each item of specified ``seq``.\n        \"\"\"\n        for f in self._seq:\n            if isinstance(f, File):\n                yield f.uuid\n            elif isinstance(f, six.string_types):\n                yield f\n            else:\n                raise ValueError(\n                    'Invalid type for sequence item: {0}'.format(type(f)))", "code_tokens": ["def", "uuids", "(", "self", ")", ":", "for", "f", "in", "self", ".", "_seq", ":", "if", "isinstance", "(", "f", ",", "File", ")", ":", "yield", "f", ".", "uuid", "elif", "isinstance", "(", "f", ",", "six", ".", "string_types", ")", ":", "yield", "f", "else", ":", "raise", "ValueError", "(", "'Invalid type for sequence item: {0}'", ".", "format", "(", "type", "(", "f", ")", ")", ")"], "docstring": "Extract uuid from each item of specified ``seq``.", "docstring_tokens": ["Extract", "uuid", "from", "each", "item", "of", "specified", "seq", "."], "sha": "cefddc0306133a71e37b18e8700df5948ef49b37", "url": "https://github.com/uploadcare/pyuploadcare/blob/cefddc0306133a71e37b18e8700df5948ef49b37/pyuploadcare/api_resources.py#L929-L939", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/github.py", "func_name": "GitHubClient.pulls", "original_string": "def pulls(self, from_date=None):\n        \"\"\"Fetch the pull requests from the repository.\n\n        The method retrieves, from a GitHub repository, the pull requests\n        updated since the given date.\n\n        :param from_date: obtain pull requests updated since this date\n\n        :returns: a generator of pull requests\n        \"\"\"\n        issues_groups = self.issues(from_date=from_date)\n\n        for raw_issues in issues_groups:\n            issues = json.loads(raw_issues)\n            for issue in issues:\n\n                if \"pull_request\" not in issue:\n                    continue\n\n                pull_number = issue[\"number\"]\n                path = urijoin(self.base_url, 'repos', self.owner, self.repository, \"pulls\", pull_number)\n\n                r = self.fetch(path)\n                pull = r.text\n\n                yield pull", "language": "python", "code": "def pulls(self, from_date=None):\n        \"\"\"Fetch the pull requests from the repository.\n\n        The method retrieves, from a GitHub repository, the pull requests\n        updated since the given date.\n\n        :param from_date: obtain pull requests updated since this date\n\n        :returns: a generator of pull requests\n        \"\"\"\n        issues_groups = self.issues(from_date=from_date)\n\n        for raw_issues in issues_groups:\n            issues = json.loads(raw_issues)\n            for issue in issues:\n\n                if \"pull_request\" not in issue:\n                    continue\n\n                pull_number = issue[\"number\"]\n                path = urijoin(self.base_url, 'repos', self.owner, self.repository, \"pulls\", pull_number)\n\n                r = self.fetch(path)\n                pull = r.text\n\n                yield pull", "code_tokens": ["def", "pulls", "(", "self", ",", "from_date", "=", "None", ")", ":", "issues_groups", "=", "self", ".", "issues", "(", "from_date", "=", "from_date", ")", "for", "raw_issues", "in", "issues_groups", ":", "issues", "=", "json", ".", "loads", "(", "raw_issues", ")", "for", "issue", "in", "issues", ":", "if", "\"pull_request\"", "not", "in", "issue", ":", "continue", "pull_number", "=", "issue", "[", "\"number\"", "]", "path", "=", "urijoin", "(", "self", ".", "base_url", ",", "'repos'", ",", "self", ".", "owner", ",", "self", ".", "repository", ",", "\"pulls\"", ",", "pull_number", ")", "r", "=", "self", ".", "fetch", "(", "path", ")", "pull", "=", "r", ".", "text", "yield", "pull"], "docstring": "Fetch the pull requests from the repository.\n\n        The method retrieves, from a GitHub repository, the pull requests\n        updated since the given date.\n\n        :param from_date: obtain pull requests updated since this date\n\n        :returns: a generator of pull requests", "docstring_tokens": ["Fetch", "the", "pull", "requests", "from", "the", "repository", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/github.py#L600-L625", "partition": "test"}
{"repo": "streamlink/streamlink", "path": "src/streamlink/plugin/api/http_session.py", "func_name": "HTTPSession.json", "original_string": "def json(cls, res, *args, **kwargs):\n        \"\"\"Parses JSON from a response.\"\"\"\n        # if an encoding is already set then use the provided encoding\n        if res.encoding is None:\n            res.encoding = cls.determine_json_encoding(res.content[:4])\n        return parse_json(res.text, *args, **kwargs)", "language": "python", "code": "def json(cls, res, *args, **kwargs):\n        \"\"\"Parses JSON from a response.\"\"\"\n        # if an encoding is already set then use the provided encoding\n        if res.encoding is None:\n            res.encoding = cls.determine_json_encoding(res.content[:4])\n        return parse_json(res.text, *args, **kwargs)", "code_tokens": ["def", "json", "(", "cls", ",", "res", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "# if an encoding is already set then use the provided encoding", "if", "res", ".", "encoding", "is", "None", ":", "res", ".", "encoding", "=", "cls", ".", "determine_json_encoding", "(", "res", ".", "content", "[", ":", "4", "]", ")", "return", "parse_json", "(", "res", ".", "text", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Parses JSON from a response.", "docstring_tokens": ["Parses", "JSON", "from", "a", "response", "."], "sha": "c8ed1daff14ac03195870238b9b900c1109dd5c1", "url": "https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink/plugin/api/http_session.py#L98-L103", "partition": "test"}
{"repo": "deepmipt/DeepPavlov", "path": "deeppavlov/core/commands/infer.py", "func_name": "predict_on_stream", "original_string": "def predict_on_stream(config: Union[str, Path, dict], batch_size: int = 1, file_path: Optional[str] = None) -> None:\n    \"\"\"Make a prediction with the component described in corresponding configuration file.\"\"\"\n    if file_path is None or file_path == '-':\n        if sys.stdin.isatty():\n            raise RuntimeError('To process data from terminal please use interact mode')\n        f = sys.stdin\n    else:\n        f = open(file_path, encoding='utf8')\n\n    model: Chainer = build_model(config)\n\n    args_count = len(model.in_x)\n    while True:\n        batch = list((l.strip() for l in islice(f, batch_size * args_count)))\n\n        if not batch:\n            break\n\n        args = []\n        for i in range(args_count):\n            args.append(batch[i::args_count])\n\n        res = model(*args)\n        if len(model.out_params) == 1:\n            res = [res]\n        for res in zip(*res):\n            res = json.dumps(res, ensure_ascii=False)\n            print(res, flush=True)\n\n    if f is not sys.stdin:\n        f.close()", "language": "python", "code": "def predict_on_stream(config: Union[str, Path, dict], batch_size: int = 1, file_path: Optional[str] = None) -> None:\n    \"\"\"Make a prediction with the component described in corresponding configuration file.\"\"\"\n    if file_path is None or file_path == '-':\n        if sys.stdin.isatty():\n            raise RuntimeError('To process data from terminal please use interact mode')\n        f = sys.stdin\n    else:\n        f = open(file_path, encoding='utf8')\n\n    model: Chainer = build_model(config)\n\n    args_count = len(model.in_x)\n    while True:\n        batch = list((l.strip() for l in islice(f, batch_size * args_count)))\n\n        if not batch:\n            break\n\n        args = []\n        for i in range(args_count):\n            args.append(batch[i::args_count])\n\n        res = model(*args)\n        if len(model.out_params) == 1:\n            res = [res]\n        for res in zip(*res):\n            res = json.dumps(res, ensure_ascii=False)\n            print(res, flush=True)\n\n    if f is not sys.stdin:\n        f.close()", "code_tokens": ["def", "predict_on_stream", "(", "config", ":", "Union", "[", "str", ",", "Path", ",", "dict", "]", ",", "batch_size", ":", "int", "=", "1", ",", "file_path", ":", "Optional", "[", "str", "]", "=", "None", ")", "->", "None", ":", "if", "file_path", "is", "None", "or", "file_path", "==", "'-'", ":", "if", "sys", ".", "stdin", ".", "isatty", "(", ")", ":", "raise", "RuntimeError", "(", "'To process data from terminal please use interact mode'", ")", "f", "=", "sys", ".", "stdin", "else", ":", "f", "=", "open", "(", "file_path", ",", "encoding", "=", "'utf8'", ")", "model", ":", "Chainer", "=", "build_model", "(", "config", ")", "args_count", "=", "len", "(", "model", ".", "in_x", ")", "while", "True", ":", "batch", "=", "list", "(", "(", "l", ".", "strip", "(", ")", "for", "l", "in", "islice", "(", "f", ",", "batch_size", "*", "args_count", ")", ")", ")", "if", "not", "batch", ":", "break", "args", "=", "[", "]", "for", "i", "in", "range", "(", "args_count", ")", ":", "args", ".", "append", "(", "batch", "[", "i", ":", ":", "args_count", "]", ")", "res", "=", "model", "(", "*", "args", ")", "if", "len", "(", "model", ".", "out_params", ")", "==", "1", ":", "res", "=", "[", "res", "]", "for", "res", "in", "zip", "(", "*", "res", ")", ":", "res", "=", "json", ".", "dumps", "(", "res", ",", "ensure_ascii", "=", "False", ")", "print", "(", "res", ",", "flush", "=", "True", ")", "if", "f", "is", "not", "sys", ".", "stdin", ":", "f", ".", "close", "(", ")"], "docstring": "Make a prediction with the component described in corresponding configuration file.", "docstring_tokens": ["Make", "a", "prediction", "with", "the", "component", "described", "in", "corresponding", "configuration", "file", "."], "sha": "f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c", "url": "https://github.com/deepmipt/DeepPavlov/blob/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c/deeppavlov/core/commands/infer.py#L92-L122", "partition": "test"}
{"repo": "ioam/parambokeh", "path": "parambokeh/__init__.py", "func_name": "Widgets.widget", "original_string": "def widget(self, param_name):\n        \"\"\"Get widget for param_name\"\"\"\n        if param_name not in self._widgets:\n            self._widgets[param_name] = self._make_widget(param_name)\n        return self._widgets[param_name]", "language": "python", "code": "def widget(self, param_name):\n        \"\"\"Get widget for param_name\"\"\"\n        if param_name not in self._widgets:\n            self._widgets[param_name] = self._make_widget(param_name)\n        return self._widgets[param_name]", "code_tokens": ["def", "widget", "(", "self", ",", "param_name", ")", ":", "if", "param_name", "not", "in", "self", ".", "_widgets", ":", "self", ".", "_widgets", "[", "param_name", "]", "=", "self", ".", "_make_widget", "(", "param_name", ")", "return", "self", ".", "_widgets", "[", "param_name", "]"], "docstring": "Get widget for param_name", "docstring_tokens": ["Get", "widget", "for", "param_name"], "sha": "fb9744f216273c7b24e65d037b1d621c08d7fde6", "url": "https://github.com/ioam/parambokeh/blob/fb9744f216273c7b24e65d037b1d621c08d7fde6/parambokeh/__init__.py#L458-L462", "partition": "test"}
{"repo": "dwhswenson/autorelease", "path": "setup.py", "func_name": "get_git_version", "original_string": "def get_git_version():\n    \"\"\"\n    Return the git hash as a string.\n\n    Apparently someone got this from numpy's setup.py. It has since been\n    modified a few times.\n    \"\"\"\n    # Return the git revision as a string\n    # copied from numpy setup.py\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in ['SYSTEMROOT', 'PATH']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env['LANGUAGE'] = 'C'\n        env['LANG'] = 'C'\n        env['LC_ALL'] = 'C'\n        with open(os.devnull, 'w') as err_out:\n            out = subprocess.Popen(cmd,\n                                   stdout=subprocess.PIPE,\n                                   stderr=err_out, # maybe debug later?\n                                   env=env).communicate()[0]\n        return out\n\n    try:\n        git_dir = os.path.dirname(os.path.realpath(__file__))\n        out = _minimal_ext_cmd(['git', '-C', git_dir, 'rev-parse', 'HEAD'])\n        GIT_REVISION = out.strip().decode('ascii')\n    except OSError:\n        GIT_REVISION = 'Unknown'\n\n    return GIT_REVISION", "language": "python", "code": "def get_git_version():\n    \"\"\"\n    Return the git hash as a string.\n\n    Apparently someone got this from numpy's setup.py. It has since been\n    modified a few times.\n    \"\"\"\n    # Return the git revision as a string\n    # copied from numpy setup.py\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in ['SYSTEMROOT', 'PATH']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env['LANGUAGE'] = 'C'\n        env['LANG'] = 'C'\n        env['LC_ALL'] = 'C'\n        with open(os.devnull, 'w') as err_out:\n            out = subprocess.Popen(cmd,\n                                   stdout=subprocess.PIPE,\n                                   stderr=err_out, # maybe debug later?\n                                   env=env).communicate()[0]\n        return out\n\n    try:\n        git_dir = os.path.dirname(os.path.realpath(__file__))\n        out = _minimal_ext_cmd(['git', '-C', git_dir, 'rev-parse', 'HEAD'])\n        GIT_REVISION = out.strip().decode('ascii')\n    except OSError:\n        GIT_REVISION = 'Unknown'\n\n    return GIT_REVISION", "code_tokens": ["def", "get_git_version", "(", ")", ":", "# Return the git revision as a string", "# copied from numpy setup.py", "def", "_minimal_ext_cmd", "(", "cmd", ")", ":", "# construct minimal environment", "env", "=", "{", "}", "for", "k", "in", "[", "'SYSTEMROOT'", ",", "'PATH'", "]", ":", "v", "=", "os", ".", "environ", ".", "get", "(", "k", ")", "if", "v", "is", "not", "None", ":", "env", "[", "k", "]", "=", "v", "# LANGUAGE is used on win32", "env", "[", "'LANGUAGE'", "]", "=", "'C'", "env", "[", "'LANG'", "]", "=", "'C'", "env", "[", "'LC_ALL'", "]", "=", "'C'", "with", "open", "(", "os", ".", "devnull", ",", "'w'", ")", "as", "err_out", ":", "out", "=", "subprocess", ".", "Popen", "(", "cmd", ",", "stdout", "=", "subprocess", ".", "PIPE", ",", "stderr", "=", "err_out", ",", "# maybe debug later?", "env", "=", "env", ")", ".", "communicate", "(", ")", "[", "0", "]", "return", "out", "try", ":", "git_dir", "=", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "realpath", "(", "__file__", ")", ")", "out", "=", "_minimal_ext_cmd", "(", "[", "'git'", ",", "'-C'", ",", "git_dir", ",", "'rev-parse'", ",", "'HEAD'", "]", ")", "GIT_REVISION", "=", "out", ".", "strip", "(", ")", ".", "decode", "(", "'ascii'", ")", "except", "OSError", ":", "GIT_REVISION", "=", "'Unknown'", "return", "GIT_REVISION"], "docstring": "Return the git hash as a string.\n\n    Apparently someone got this from numpy's setup.py. It has since been\n    modified a few times.", "docstring_tokens": ["Return", "the", "git", "hash", "as", "a", "string", "."], "sha": "339c32c3934e4751857f35aaa2bfffaaaf3b39c4", "url": "https://github.com/dwhswenson/autorelease/blob/339c32c3934e4751857f35aaa2bfffaaaf3b39c4/setup.py#L70-L104", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/parameter.py", "func_name": "SparseParameter._build_names", "original_string": "def _build_names(self, name_idx, is_dia):\n        \"\"\"Formats a name for storage\n\n        :return: A tuple of names with the following format:\n\n            `xspm__spsp__XXXX__spsp__XXXXXXXX` where the first 'XXXX' refer to the property and\n            the latter 'XXXXXXX' to the sparse matrix index.\n\n        \"\"\"\n        name_list = self._get_name_list(is_dia)\n        return tuple(['explored%s.set_%05d.xspm_%s_%08d' % (SparseParameter.IDENTIFIER,\n                                                         name_idx // 200, name, name_idx)\n                                                                        for name in name_list])", "language": "python", "code": "def _build_names(self, name_idx, is_dia):\n        \"\"\"Formats a name for storage\n\n        :return: A tuple of names with the following format:\n\n            `xspm__spsp__XXXX__spsp__XXXXXXXX` where the first 'XXXX' refer to the property and\n            the latter 'XXXXXXX' to the sparse matrix index.\n\n        \"\"\"\n        name_list = self._get_name_list(is_dia)\n        return tuple(['explored%s.set_%05d.xspm_%s_%08d' % (SparseParameter.IDENTIFIER,\n                                                         name_idx // 200, name, name_idx)\n                                                                        for name in name_list])", "code_tokens": ["def", "_build_names", "(", "self", ",", "name_idx", ",", "is_dia", ")", ":", "name_list", "=", "self", ".", "_get_name_list", "(", "is_dia", ")", "return", "tuple", "(", "[", "'explored%s.set_%05d.xspm_%s_%08d'", "%", "(", "SparseParameter", ".", "IDENTIFIER", ",", "name_idx", "//", "200", ",", "name", ",", "name_idx", ")", "for", "name", "in", "name_list", "]", ")"], "docstring": "Formats a name for storage\n\n        :return: A tuple of names with the following format:\n\n            `xspm__spsp__XXXX__spsp__XXXXXXXX` where the first 'XXXX' refer to the property and\n            the latter 'XXXXXXX' to the sparse matrix index.", "docstring_tokens": ["Formats", "a", "name", "for", "storage"], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/parameter.py#L1566-L1578", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcs_hook.py", "func_name": "GoogleCloudStorageHook.get_size", "original_string": "def get_size(self, bucket_name, object_name):\n        \"\"\"\n        Gets the size of a file in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google\n            cloud storage bucket_name.\n        :type object_name: str\n\n        \"\"\"\n        self.log.info('Checking the file size of object: %s in bucket_name: %s',\n                      object_name,\n                      bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n        blob_size = blob.size\n        self.log.info('The file size of %s is %s bytes.', object_name, blob_size)\n        return blob_size", "language": "python", "code": "def get_size(self, bucket_name, object_name):\n        \"\"\"\n        Gets the size of a file in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google\n            cloud storage bucket_name.\n        :type object_name: str\n\n        \"\"\"\n        self.log.info('Checking the file size of object: %s in bucket_name: %s',\n                      object_name,\n                      bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n        blob_size = blob.size\n        self.log.info('The file size of %s is %s bytes.', object_name, blob_size)\n        return blob_size", "code_tokens": ["def", "get_size", "(", "self", ",", "bucket_name", ",", "object_name", ")", ":", "self", ".", "log", ".", "info", "(", "'Checking the file size of object: %s in bucket_name: %s'", ",", "object_name", ",", "bucket_name", ")", "client", "=", "self", ".", "get_conn", "(", ")", "bucket", "=", "client", ".", "get_bucket", "(", "bucket_name", "=", "bucket_name", ")", "blob", "=", "bucket", ".", "get_blob", "(", "blob_name", "=", "object_name", ")", "blob", ".", "reload", "(", ")", "blob_size", "=", "blob", ".", "size", "self", ".", "log", ".", "info", "(", "'The file size of %s is %s bytes.'", ",", "object_name", ",", "blob_size", ")", "return", "blob_size"], "docstring": "Gets the size of a file in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google\n            cloud storage bucket_name.\n        :type object_name: str", "docstring_tokens": ["Gets", "the", "size", "of", "a", "file", "in", "Google", "Cloud", "Storage", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L318-L338", "partition": "test"}
{"repo": "HydraChain/hydrachain", "path": "hydrachain/consensus/manager.py", "func_name": "HeightManager.last_valid_lockset", "original_string": "def last_valid_lockset(self):\n        \"highest valid lockset on height\"\n        for r in self.rounds:\n            ls = self.rounds[r].lockset\n            if ls.is_valid:\n                return ls\n        return None", "language": "python", "code": "def last_valid_lockset(self):\n        \"highest valid lockset on height\"\n        for r in self.rounds:\n            ls = self.rounds[r].lockset\n            if ls.is_valid:\n                return ls\n        return None", "code_tokens": ["def", "last_valid_lockset", "(", "self", ")", ":", "for", "r", "in", "self", ".", "rounds", ":", "ls", "=", "self", ".", "rounds", "[", "r", "]", ".", "lockset", "if", "ls", ".", "is_valid", ":", "return", "ls", "return", "None"], "docstring": "highest valid lockset on height", "docstring_tokens": ["highest", "valid", "lockset", "on", "height"], "sha": "6c0919b0575dc8aa481f3a8c703e1a7f0575ecc3", "url": "https://github.com/HydraChain/hydrachain/blob/6c0919b0575dc8aa481f3a8c703e1a7f0575ecc3/hydrachain/consensus/manager.py#L508-L514", "partition": "test"}
{"repo": "NLeSC/pycoeman", "path": "pycoeman/utils_execution.py", "func_name": "apply_argument_parser", "original_string": "def apply_argument_parser(argumentsParser, options=None):\n    \"\"\" Apply the argument parser. \"\"\"\n    if options is not None:\n        args = argumentsParser.parse_args(options)\n    else:\n        args = argumentsParser.parse_args()\n    return args", "language": "python", "code": "def apply_argument_parser(argumentsParser, options=None):\n    \"\"\" Apply the argument parser. \"\"\"\n    if options is not None:\n        args = argumentsParser.parse_args(options)\n    else:\n        args = argumentsParser.parse_args()\n    return args", "code_tokens": ["def", "apply_argument_parser", "(", "argumentsParser", ",", "options", "=", "None", ")", ":", "if", "options", "is", "not", "None", ":", "args", "=", "argumentsParser", ".", "parse_args", "(", "options", ")", "else", ":", "args", "=", "argumentsParser", ".", "parse_args", "(", ")", "return", "args"], "docstring": "Apply the argument parser.", "docstring_tokens": ["Apply", "the", "argument", "parser", "."], "sha": "246d517b55cb98eb46f69aae453492cf9dd9d5af", "url": "https://github.com/NLeSC/pycoeman/blob/246d517b55cb98eb46f69aae453492cf9dd9d5af/pycoeman/utils_execution.py#L51-L57", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/history.py", "func_name": "HistoryManager._get_range_session", "original_string": "def _get_range_session(self, start=1, stop=None, raw=True, output=False):\n        \"\"\"Get input and output history from the current session. Called by\n        get_range, and takes similar parameters.\"\"\"\n        input_hist = self.input_hist_raw if raw else self.input_hist_parsed\n            \n        n = len(input_hist)\n        if start < 0:\n            start += n\n        if not stop or (stop > n):\n            stop = n\n        elif stop < 0:\n            stop += n\n        \n        for i in range(start, stop):\n            if output:\n                line = (input_hist[i], self.output_hist_reprs.get(i))\n            else:\n                line = input_hist[i]\n            yield (0, i, line)", "language": "python", "code": "def _get_range_session(self, start=1, stop=None, raw=True, output=False):\n        \"\"\"Get input and output history from the current session. Called by\n        get_range, and takes similar parameters.\"\"\"\n        input_hist = self.input_hist_raw if raw else self.input_hist_parsed\n            \n        n = len(input_hist)\n        if start < 0:\n            start += n\n        if not stop or (stop > n):\n            stop = n\n        elif stop < 0:\n            stop += n\n        \n        for i in range(start, stop):\n            if output:\n                line = (input_hist[i], self.output_hist_reprs.get(i))\n            else:\n                line = input_hist[i]\n            yield (0, i, line)", "code_tokens": ["def", "_get_range_session", "(", "self", ",", "start", "=", "1", ",", "stop", "=", "None", ",", "raw", "=", "True", ",", "output", "=", "False", ")", ":", "input_hist", "=", "self", ".", "input_hist_raw", "if", "raw", "else", "self", ".", "input_hist_parsed", "n", "=", "len", "(", "input_hist", ")", "if", "start", "<", "0", ":", "start", "+=", "n", "if", "not", "stop", "or", "(", "stop", ">", "n", ")", ":", "stop", "=", "n", "elif", "stop", "<", "0", ":", "stop", "+=", "n", "for", "i", "in", "range", "(", "start", ",", "stop", ")", ":", "if", "output", ":", "line", "=", "(", "input_hist", "[", "i", "]", ",", "self", ".", "output_hist_reprs", ".", "get", "(", "i", ")", ")", "else", ":", "line", "=", "input_hist", "[", "i", "]", "yield", "(", "0", ",", "i", ",", "line", ")"], "docstring": "Get input and output history from the current session. Called by\n        get_range, and takes similar parameters.", "docstring_tokens": ["Get", "input", "and", "output", "history", "from", "the", "current", "session", ".", "Called", "by", "get_range", "and", "takes", "similar", "parameters", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/history.py#L462-L480", "partition": "test"}
{"repo": "spotify/pyschema", "path": "pyschema_extensions/avro_schema_parser.py", "func_name": "parse_schema_string", "original_string": "def parse_schema_string(schema_string):\n    \"\"\"\n    Load and return a PySchema class from an avsc string\n    \"\"\"\n    if isinstance(schema_string, str):\n        schema_string = schema_string.decode(\"utf8\")\n    schema_struct = json.loads(schema_string)\n\n    return AvroSchemaParser().parse_schema_struct(schema_struct)", "language": "python", "code": "def parse_schema_string(schema_string):\n    \"\"\"\n    Load and return a PySchema class from an avsc string\n    \"\"\"\n    if isinstance(schema_string, str):\n        schema_string = schema_string.decode(\"utf8\")\n    schema_struct = json.loads(schema_string)\n\n    return AvroSchemaParser().parse_schema_struct(schema_struct)", "code_tokens": ["def", "parse_schema_string", "(", "schema_string", ")", ":", "if", "isinstance", "(", "schema_string", ",", "str", ")", ":", "schema_string", "=", "schema_string", ".", "decode", "(", "\"utf8\"", ")", "schema_struct", "=", "json", ".", "loads", "(", "schema_string", ")", "return", "AvroSchemaParser", "(", ")", ".", "parse_schema_struct", "(", "schema_struct", ")"], "docstring": "Load and return a PySchema class from an avsc string", "docstring_tokens": ["Load", "and", "return", "a", "PySchema", "class", "from", "an", "avsc", "string"], "sha": "7e6c3934150bcb040c628d74ace6caf5fcf867df", "url": "https://github.com/spotify/pyschema/blob/7e6c3934150bcb040c628d74ace6caf5fcf867df/pyschema_extensions/avro_schema_parser.py#L49-L57", "partition": "test"}
{"repo": "edx/ecommerce-worker", "path": "ecommerce_worker/sailthru/v1/tasks.py", "func_name": "_update_unenrolled_list", "original_string": "def _update_unenrolled_list(sailthru_client, email, course_url, unenroll):\n    \"\"\"Maintain a list of courses the user has unenrolled from in the Sailthru user record\n\n    Arguments:\n        sailthru_client (object): SailthruClient\n        email (str): user's email address\n        course_url (str): LMS url for course info page.\n        unenroll (boolean): True if unenrolling, False if enrolling\n\n    Returns:\n        False if retryable error, else True\n    \"\"\"\n    try:\n        # get the user 'vars' values from sailthru\n        sailthru_response = sailthru_client.api_get(\"user\", {\"id\": email, \"fields\": {\"vars\": 1}})\n        if not sailthru_response.is_ok():\n            error = sailthru_response.get_error()\n            logger.error(\"Error attempting to read user record from Sailthru: %s\", error.get_message())\n            return not can_retry_sailthru_request(error)\n\n        response_json = sailthru_response.json\n\n        unenroll_list = []\n        if response_json and \"vars\" in response_json and response_json[\"vars\"] \\\n           and \"unenrolled\" in response_json[\"vars\"]:\n            unenroll_list = response_json[\"vars\"][\"unenrolled\"]\n\n        changed = False\n        # if unenrolling, add course to unenroll list\n        if unenroll:\n            if course_url not in unenroll_list:\n                unenroll_list.append(course_url)\n                changed = True\n\n        # if enrolling, remove course from unenroll list\n        elif course_url in unenroll_list:\n            unenroll_list.remove(course_url)\n            changed = True\n\n        if changed:\n            # write user record back\n            sailthru_response = sailthru_client.api_post(\n                'user', {'id': email, 'key': 'email', 'vars': {'unenrolled': unenroll_list}})\n\n            if not sailthru_response.is_ok():\n                error = sailthru_response.get_error()\n                logger.error(\"Error attempting to update user record in Sailthru: %s\", error.get_message())\n                return not can_retry_sailthru_request(error)\n\n        return True\n\n    except SailthruClientError as exc:\n        logger.exception(\"Exception attempting to update user record for %s in Sailthru - %s\", email, text_type(exc))\n        return False", "language": "python", "code": "def _update_unenrolled_list(sailthru_client, email, course_url, unenroll):\n    \"\"\"Maintain a list of courses the user has unenrolled from in the Sailthru user record\n\n    Arguments:\n        sailthru_client (object): SailthruClient\n        email (str): user's email address\n        course_url (str): LMS url for course info page.\n        unenroll (boolean): True if unenrolling, False if enrolling\n\n    Returns:\n        False if retryable error, else True\n    \"\"\"\n    try:\n        # get the user 'vars' values from sailthru\n        sailthru_response = sailthru_client.api_get(\"user\", {\"id\": email, \"fields\": {\"vars\": 1}})\n        if not sailthru_response.is_ok():\n            error = sailthru_response.get_error()\n            logger.error(\"Error attempting to read user record from Sailthru: %s\", error.get_message())\n            return not can_retry_sailthru_request(error)\n\n        response_json = sailthru_response.json\n\n        unenroll_list = []\n        if response_json and \"vars\" in response_json and response_json[\"vars\"] \\\n           and \"unenrolled\" in response_json[\"vars\"]:\n            unenroll_list = response_json[\"vars\"][\"unenrolled\"]\n\n        changed = False\n        # if unenrolling, add course to unenroll list\n        if unenroll:\n            if course_url not in unenroll_list:\n                unenroll_list.append(course_url)\n                changed = True\n\n        # if enrolling, remove course from unenroll list\n        elif course_url in unenroll_list:\n            unenroll_list.remove(course_url)\n            changed = True\n\n        if changed:\n            # write user record back\n            sailthru_response = sailthru_client.api_post(\n                'user', {'id': email, 'key': 'email', 'vars': {'unenrolled': unenroll_list}})\n\n            if not sailthru_response.is_ok():\n                error = sailthru_response.get_error()\n                logger.error(\"Error attempting to update user record in Sailthru: %s\", error.get_message())\n                return not can_retry_sailthru_request(error)\n\n        return True\n\n    except SailthruClientError as exc:\n        logger.exception(\"Exception attempting to update user record for %s in Sailthru - %s\", email, text_type(exc))\n        return False", "code_tokens": ["def", "_update_unenrolled_list", "(", "sailthru_client", ",", "email", ",", "course_url", ",", "unenroll", ")", ":", "try", ":", "# get the user 'vars' values from sailthru", "sailthru_response", "=", "sailthru_client", ".", "api_get", "(", "\"user\"", ",", "{", "\"id\"", ":", "email", ",", "\"fields\"", ":", "{", "\"vars\"", ":", "1", "}", "}", ")", "if", "not", "sailthru_response", ".", "is_ok", "(", ")", ":", "error", "=", "sailthru_response", ".", "get_error", "(", ")", "logger", ".", "error", "(", "\"Error attempting to read user record from Sailthru: %s\"", ",", "error", ".", "get_message", "(", ")", ")", "return", "not", "can_retry_sailthru_request", "(", "error", ")", "response_json", "=", "sailthru_response", ".", "json", "unenroll_list", "=", "[", "]", "if", "response_json", "and", "\"vars\"", "in", "response_json", "and", "response_json", "[", "\"vars\"", "]", "and", "\"unenrolled\"", "in", "response_json", "[", "\"vars\"", "]", ":", "unenroll_list", "=", "response_json", "[", "\"vars\"", "]", "[", "\"unenrolled\"", "]", "changed", "=", "False", "# if unenrolling, add course to unenroll list", "if", "unenroll", ":", "if", "course_url", "not", "in", "unenroll_list", ":", "unenroll_list", ".", "append", "(", "course_url", ")", "changed", "=", "True", "# if enrolling, remove course from unenroll list", "elif", "course_url", "in", "unenroll_list", ":", "unenroll_list", ".", "remove", "(", "course_url", ")", "changed", "=", "True", "if", "changed", ":", "# write user record back", "sailthru_response", "=", "sailthru_client", ".", "api_post", "(", "'user'", ",", "{", "'id'", ":", "email", ",", "'key'", ":", "'email'", ",", "'vars'", ":", "{", "'unenrolled'", ":", "unenroll_list", "}", "}", ")", "if", "not", "sailthru_response", ".", "is_ok", "(", ")", ":", "error", "=", "sailthru_response", ".", "get_error", "(", ")", "logger", ".", "error", "(", "\"Error attempting to update user record in Sailthru: %s\"", ",", "error", ".", "get_message", "(", ")", ")", "return", "not", "can_retry_sailthru_request", "(", "error", ")", "return", "True", "except", "SailthruClientError", "as", "exc", ":", "logger", ".", "exception", "(", "\"Exception attempting to update user record for %s in Sailthru - %s\"", ",", "email", ",", "text_type", "(", "exc", ")", ")", "return", "False"], "docstring": "Maintain a list of courses the user has unenrolled from in the Sailthru user record\n\n    Arguments:\n        sailthru_client (object): SailthruClient\n        email (str): user's email address\n        course_url (str): LMS url for course info page.\n        unenroll (boolean): True if unenrolling, False if enrolling\n\n    Returns:\n        False if retryable error, else True", "docstring_tokens": ["Maintain", "a", "list", "of", "courses", "the", "user", "has", "unenrolled", "from", "in", "the", "Sailthru", "user", "record"], "sha": "55246961d805b1f64d661a5c0bae0a216589401f", "url": "https://github.com/edx/ecommerce-worker/blob/55246961d805b1f64d661a5c0bae0a216589401f/ecommerce_worker/sailthru/v1/tasks.py#L157-L210", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/pip/locations.py", "func_name": "_get_build_prefix", "original_string": "def _get_build_prefix():\n    \"\"\" Returns a safe build_prefix \"\"\"\n    path = os.path.join(\n        tempfile.gettempdir(),\n        'pip_build_%s' % __get_username().replace(' ', '_')\n    )\n    if WINDOWS:\n        \"\"\" on windows(tested on 7) temp dirs are isolated \"\"\"\n        return path\n    try:\n        os.mkdir(path)\n        write_delete_marker_file(path)\n    except OSError:\n        file_uid = None\n        try:\n            # raises OSError for symlinks\n            # https://github.com/pypa/pip/pull/935#discussion_r5307003\n            file_uid = get_path_uid(path)\n        except OSError:\n            file_uid = None\n\n        if file_uid != os.geteuid():\n            msg = (\n                \"The temporary folder for building (%s) is either not owned by\"\n                \" you, or is a symlink.\" % path\n            )\n            print(msg)\n            print(\n                \"pip will not work until the temporary folder is either \"\n                \"deleted or is a real directory owned by your user account.\"\n            )\n            raise exceptions.InstallationError(msg)\n    return path", "language": "python", "code": "def _get_build_prefix():\n    \"\"\" Returns a safe build_prefix \"\"\"\n    path = os.path.join(\n        tempfile.gettempdir(),\n        'pip_build_%s' % __get_username().replace(' ', '_')\n    )\n    if WINDOWS:\n        \"\"\" on windows(tested on 7) temp dirs are isolated \"\"\"\n        return path\n    try:\n        os.mkdir(path)\n        write_delete_marker_file(path)\n    except OSError:\n        file_uid = None\n        try:\n            # raises OSError for symlinks\n            # https://github.com/pypa/pip/pull/935#discussion_r5307003\n            file_uid = get_path_uid(path)\n        except OSError:\n            file_uid = None\n\n        if file_uid != os.geteuid():\n            msg = (\n                \"The temporary folder for building (%s) is either not owned by\"\n                \" you, or is a symlink.\" % path\n            )\n            print(msg)\n            print(\n                \"pip will not work until the temporary folder is either \"\n                \"deleted or is a real directory owned by your user account.\"\n            )\n            raise exceptions.InstallationError(msg)\n    return path", "code_tokens": ["def", "_get_build_prefix", "(", ")", ":", "path", "=", "os", ".", "path", ".", "join", "(", "tempfile", ".", "gettempdir", "(", ")", ",", "'pip_build_%s'", "%", "__get_username", "(", ")", ".", "replace", "(", "' '", ",", "'_'", ")", ")", "if", "WINDOWS", ":", "\"\"\" on windows(tested on 7) temp dirs are isolated \"\"\"", "return", "path", "try", ":", "os", ".", "mkdir", "(", "path", ")", "write_delete_marker_file", "(", "path", ")", "except", "OSError", ":", "file_uid", "=", "None", "try", ":", "# raises OSError for symlinks", "# https://github.com/pypa/pip/pull/935#discussion_r5307003", "file_uid", "=", "get_path_uid", "(", "path", ")", "except", "OSError", ":", "file_uid", "=", "None", "if", "file_uid", "!=", "os", ".", "geteuid", "(", ")", ":", "msg", "=", "(", "\"The temporary folder for building (%s) is either not owned by\"", "\" you, or is a symlink.\"", "%", "path", ")", "print", "(", "msg", ")", "print", "(", "\"pip will not work until the temporary folder is either \"", "\"deleted or is a real directory owned by your user account.\"", ")", "raise", "exceptions", ".", "InstallationError", "(", "msg", ")", "return", "path"], "docstring": "Returns a safe build_prefix", "docstring_tokens": ["Returns", "a", "safe", "build_prefix"], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/pip/locations.py#L111-L143", "partition": "test"}
{"repo": "jaraco/svg.charts", "path": "svg/charts/graph.py", "func_name": "Graph.render_inline_styles", "original_string": "def render_inline_styles(self):\n\t\t\"Hard-code the styles into the SVG XML if style sheets are not used.\"\n\t\tif not self.css_inline:\n\t\t\t# do nothing\n\t\t\treturn\n\n\t\tstyles = self.parse_css()\n\t\tfor node in self.root.xpath('//*[@class]'):\n\t\t\tcl = '.' + node.attrib['class']\n\t\t\tif cl not in styles:\n\t\t\t\tcontinue\n\t\t\tstyle = styles[cl]\n\t\t\tif 'style' in node.attrib:\n\t\t\t\tstyle += node.attrib['style']\n\t\t\tnode.attrib['style'] = style", "language": "python", "code": "def render_inline_styles(self):\n\t\t\"Hard-code the styles into the SVG XML if style sheets are not used.\"\n\t\tif not self.css_inline:\n\t\t\t# do nothing\n\t\t\treturn\n\n\t\tstyles = self.parse_css()\n\t\tfor node in self.root.xpath('//*[@class]'):\n\t\t\tcl = '.' + node.attrib['class']\n\t\t\tif cl not in styles:\n\t\t\t\tcontinue\n\t\t\tstyle = styles[cl]\n\t\t\tif 'style' in node.attrib:\n\t\t\t\tstyle += node.attrib['style']\n\t\t\tnode.attrib['style'] = style", "code_tokens": ["def", "render_inline_styles", "(", "self", ")", ":", "if", "not", "self", ".", "css_inline", ":", "# do nothing", "return", "styles", "=", "self", ".", "parse_css", "(", ")", "for", "node", "in", "self", ".", "root", ".", "xpath", "(", "'//*[@class]'", ")", ":", "cl", "=", "'.'", "+", "node", ".", "attrib", "[", "'class'", "]", "if", "cl", "not", "in", "styles", ":", "continue", "style", "=", "styles", "[", "cl", "]", "if", "'style'", "in", "node", ".", "attrib", ":", "style", "+=", "node", ".", "attrib", "[", "'style'", "]", "node", ".", "attrib", "[", "'style'", "]", "=", "style"], "docstring": "Hard-code the styles into the SVG XML if style sheets are not used.", "docstring_tokens": ["Hard", "-", "code", "the", "styles", "into", "the", "SVG", "XML", "if", "style", "sheets", "are", "not", "used", "."], "sha": "23053497b3f1af4e760f355050107ae3bc05909d", "url": "https://github.com/jaraco/svg.charts/blob/23053497b3f1af4e760f355050107ae3bc05909d/svg/charts/graph.py#L592-L606", "partition": "test"}
{"repo": "ioam/parambokeh", "path": "parambokeh/view.py", "func_name": "render_function", "original_string": "def render_function(obj, view):\n    \"\"\"\n    The default Renderer function which handles HoloViews objects.\n    \"\"\"\n    try:\n        import holoviews as hv\n    except:\n        hv = None\n\n    if hv and isinstance(obj, hv.core.Dimensioned):\n        renderer = hv.renderer('bokeh')\n        if not view._notebook:\n            renderer = renderer.instance(mode='server')\n        plot = renderer.get_plot(obj, doc=view._document)\n        if view._notebook:\n            plot.comm = view._comm\n        plot.document = view._document\n        return plot.state\n    return obj", "language": "python", "code": "def render_function(obj, view):\n    \"\"\"\n    The default Renderer function which handles HoloViews objects.\n    \"\"\"\n    try:\n        import holoviews as hv\n    except:\n        hv = None\n\n    if hv and isinstance(obj, hv.core.Dimensioned):\n        renderer = hv.renderer('bokeh')\n        if not view._notebook:\n            renderer = renderer.instance(mode='server')\n        plot = renderer.get_plot(obj, doc=view._document)\n        if view._notebook:\n            plot.comm = view._comm\n        plot.document = view._document\n        return plot.state\n    return obj", "code_tokens": ["def", "render_function", "(", "obj", ",", "view", ")", ":", "try", ":", "import", "holoviews", "as", "hv", "except", ":", "hv", "=", "None", "if", "hv", "and", "isinstance", "(", "obj", ",", "hv", ".", "core", ".", "Dimensioned", ")", ":", "renderer", "=", "hv", ".", "renderer", "(", "'bokeh'", ")", "if", "not", "view", ".", "_notebook", ":", "renderer", "=", "renderer", ".", "instance", "(", "mode", "=", "'server'", ")", "plot", "=", "renderer", ".", "get_plot", "(", "obj", ",", "doc", "=", "view", ".", "_document", ")", "if", "view", ".", "_notebook", ":", "plot", ".", "comm", "=", "view", ".", "_comm", "plot", ".", "document", "=", "view", ".", "_document", "return", "plot", ".", "state", "return", "obj"], "docstring": "The default Renderer function which handles HoloViews objects.", "docstring_tokens": ["The", "default", "Renderer", "function", "which", "handles", "HoloViews", "objects", "."], "sha": "fb9744f216273c7b24e65d037b1d621c08d7fde6", "url": "https://github.com/ioam/parambokeh/blob/fb9744f216273c7b24e65d037b1d621c08d7fde6/parambokeh/view.py#L3-L21", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "examples/example_17_wrapping_an_existing_project/pypetwrap.py", "func_name": "make_filename", "original_string": "def make_filename(traj):\n    \"\"\" Function to create generic filenames based on what has been explored \"\"\"\n    explored_parameters = traj.f_get_explored_parameters()\n    filename = ''\n    for param in explored_parameters.values():\n        short_name = param.v_name\n        val = param.f_get()\n        filename += '%s_%s__' % (short_name, str(val))\n\n    return filename[:-2] + '.png'", "language": "python", "code": "def make_filename(traj):\n    \"\"\" Function to create generic filenames based on what has been explored \"\"\"\n    explored_parameters = traj.f_get_explored_parameters()\n    filename = ''\n    for param in explored_parameters.values():\n        short_name = param.v_name\n        val = param.f_get()\n        filename += '%s_%s__' % (short_name, str(val))\n\n    return filename[:-2] + '.png'", "code_tokens": ["def", "make_filename", "(", "traj", ")", ":", "explored_parameters", "=", "traj", ".", "f_get_explored_parameters", "(", ")", "filename", "=", "''", "for", "param", "in", "explored_parameters", ".", "values", "(", ")", ":", "short_name", "=", "param", ".", "v_name", "val", "=", "param", ".", "f_get", "(", ")", "filename", "+=", "'%s_%s__'", "%", "(", "short_name", ",", "str", "(", "val", ")", ")", "return", "filename", "[", ":", "-", "2", "]", "+", "'.png'"], "docstring": "Function to create generic filenames based on what has been explored", "docstring_tokens": ["Function", "to", "create", "generic", "filenames", "based", "on", "what", "has", "been", "explored"], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/examples/example_17_wrapping_an_existing_project/pypetwrap.py#L23-L32", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/task/task_runner/base_task_runner.py", "func_name": "BaseTaskRunner.run_command", "original_string": "def run_command(self, run_with=None, join_args=False):\n        \"\"\"\n        Run the task command.\n\n        :param run_with: list of tokens to run the task command with e.g. ``['bash', '-c']``\n        :type run_with: list\n        :param join_args: whether to concatenate the list of command tokens e.g. ``['airflow', 'run']`` vs\n            ``['airflow run']``\n        :param join_args: bool\n        :return: the process that was run\n        :rtype: subprocess.Popen\n        \"\"\"\n        run_with = run_with or []\n        cmd = [\" \".join(self._command)] if join_args else self._command\n        full_cmd = run_with + cmd\n\n        self.log.info('Running: %s', full_cmd)\n        proc = subprocess.Popen(\n            full_cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            universal_newlines=True,\n            close_fds=True,\n            env=os.environ.copy(),\n            preexec_fn=os.setsid\n        )\n\n        # Start daemon thread to read subprocess logging output\n        log_reader = threading.Thread(\n            target=self._read_task_logs,\n            args=(proc.stdout,),\n        )\n        log_reader.daemon = True\n        log_reader.start()\n        return proc", "language": "python", "code": "def run_command(self, run_with=None, join_args=False):\n        \"\"\"\n        Run the task command.\n\n        :param run_with: list of tokens to run the task command with e.g. ``['bash', '-c']``\n        :type run_with: list\n        :param join_args: whether to concatenate the list of command tokens e.g. ``['airflow', 'run']`` vs\n            ``['airflow run']``\n        :param join_args: bool\n        :return: the process that was run\n        :rtype: subprocess.Popen\n        \"\"\"\n        run_with = run_with or []\n        cmd = [\" \".join(self._command)] if join_args else self._command\n        full_cmd = run_with + cmd\n\n        self.log.info('Running: %s', full_cmd)\n        proc = subprocess.Popen(\n            full_cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            universal_newlines=True,\n            close_fds=True,\n            env=os.environ.copy(),\n            preexec_fn=os.setsid\n        )\n\n        # Start daemon thread to read subprocess logging output\n        log_reader = threading.Thread(\n            target=self._read_task_logs,\n            args=(proc.stdout,),\n        )\n        log_reader.daemon = True\n        log_reader.start()\n        return proc", "code_tokens": ["def", "run_command", "(", "self", ",", "run_with", "=", "None", ",", "join_args", "=", "False", ")", ":", "run_with", "=", "run_with", "or", "[", "]", "cmd", "=", "[", "\" \"", ".", "join", "(", "self", ".", "_command", ")", "]", "if", "join_args", "else", "self", ".", "_command", "full_cmd", "=", "run_with", "+", "cmd", "self", ".", "log", ".", "info", "(", "'Running: %s'", ",", "full_cmd", ")", "proc", "=", "subprocess", ".", "Popen", "(", "full_cmd", ",", "stdout", "=", "subprocess", ".", "PIPE", ",", "stderr", "=", "subprocess", ".", "STDOUT", ",", "universal_newlines", "=", "True", ",", "close_fds", "=", "True", ",", "env", "=", "os", ".", "environ", ".", "copy", "(", ")", ",", "preexec_fn", "=", "os", ".", "setsid", ")", "# Start daemon thread to read subprocess logging output", "log_reader", "=", "threading", ".", "Thread", "(", "target", "=", "self", ".", "_read_task_logs", ",", "args", "=", "(", "proc", ".", "stdout", ",", ")", ",", ")", "log_reader", ".", "daemon", "=", "True", "log_reader", ".", "start", "(", ")", "return", "proc"], "docstring": "Run the task command.\n\n        :param run_with: list of tokens to run the task command with e.g. ``['bash', '-c']``\n        :type run_with: list\n        :param join_args: whether to concatenate the list of command tokens e.g. ``['airflow', 'run']`` vs\n            ``['airflow run']``\n        :param join_args: bool\n        :return: the process that was run\n        :rtype: subprocess.Popen", "docstring_tokens": ["Run", "the", "task", "command", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/task/task_runner/base_task_runner.py#L101-L135", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/html/notebook/handlers.py", "func_name": "IOPubHandler.stop_hb", "original_string": "def stop_hb(self):\n        \"\"\"Stop the heartbeating and cancel all related callbacks.\"\"\"\n        if self._beating:\n            self._beating = False\n            self._hb_periodic_callback.stop()\n            if not self.hb_stream.closed():\n                self.hb_stream.on_recv(None)", "language": "python", "code": "def stop_hb(self):\n        \"\"\"Stop the heartbeating and cancel all related callbacks.\"\"\"\n        if self._beating:\n            self._beating = False\n            self._hb_periodic_callback.stop()\n            if not self.hb_stream.closed():\n                self.hb_stream.on_recv(None)", "code_tokens": ["def", "stop_hb", "(", "self", ")", ":", "if", "self", ".", "_beating", ":", "self", ".", "_beating", "=", "False", "self", ".", "_hb_periodic_callback", ".", "stop", "(", ")", "if", "not", "self", ".", "hb_stream", ".", "closed", "(", ")", ":", "self", ".", "hb_stream", ".", "on_recv", "(", "None", ")"], "docstring": "Stop the heartbeating and cancel all related callbacks.", "docstring_tokens": ["Stop", "the", "heartbeating", "and", "cancel", "all", "related", "callbacks", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/html/notebook/handlers.py#L530-L536", "partition": "test"}
{"repo": "agile-geoscience/striplog", "path": "striplog/striplog.py", "func_name": "Striplog.from_log", "original_string": "def from_log(cls, log,\n                 cutoff=None,\n                 components=None,\n                 legend=None,\n                 legend_field=None,\n                 field=None,\n                 right=False,\n                 basis=None,\n                 source='Log'):\n        \"\"\"\n        Turn a 1D array into a striplog, given a cutoff.\n\n        Args:\n            log (array-like): A 1D array or a list of integers.\n            cutoff (number or array-like): The log value(s) at which to bin\n                the log. Optional.\n            components (array-like): A list of components. Use this or\n                ``legend``.\n            legend (``Legend``): A legend object. Use this or ``components``.\n            legend_field ('str'): If you're not trying to match against\n                components, then you can match the log values to this field in\n                the Decors.\n            field (str): The field in the Interval's ``data`` to store the log\n                values as.\n            right (bool): Which side of the cutoff to send things that are\n                equal to, i.e. right on, the cutoff.\n            basis (array-like): A depth basis for the log, so striplog knows\n                where to put the boundaries.\n            source (str): The source of the data. Default 'Log'.\n\n        Returns:\n            Striplog: The ``striplog`` object.\n        \"\"\"\n        if (components is None) and (legend is None) and (field is None):\n            m = 'You must provide a list of components, and legend, or a field.'\n            raise StriplogError(m)\n\n        if (legend is not None) and (legend_field is None):\n            try:  # To treat it like a legend.\n                components = [deepcopy(decor.component) for decor in legend]\n            except AttributeError:  # It's just a list of components.\n                pass\n\n        if legend_field is not None:\n            field_values = [getattr(d, legend_field, 0) for d in legend]\n            components = [Component() for i in range(int(max(field_values)+1))]\n            for i, decor in enumerate(legend):\n                components[i] = deepcopy(decor.component)\n\n        if cutoff is not None:\n\n            # First make sure we have enough components.\n            try:\n                n = len(cutoff)\n            except TypeError:\n                n = 1\n            if len(components) < n+1:\n                m = 'For n cutoffs, you need to provide at least'\n                m += 'n+1 components.'\n                raise StriplogError(m)\n\n            # Digitize.\n            try:  # To use cutoff as a list.\n                a = np.digitize(log, cutoff, right)\n            except ValueError:  # It's just a number.\n                a = np.digitize(log, [cutoff], right)\n\n        else:\n            a = np.copy(log)\n\n        tops, values = utils.tops_from_loglike(a)\n\n        if basis is None:\n            m = 'You must provide a depth or elevation basis.'\n            raise StriplogError(m)\n\n        list_of_Intervals = cls.__intervals_from_tops(tops,\n                                                      values,\n                                                      basis,\n                                                      components,\n                                                      field=field\n                                                      )\n\n        return cls(list_of_Intervals, source=source)", "language": "python", "code": "def from_log(cls, log,\n                 cutoff=None,\n                 components=None,\n                 legend=None,\n                 legend_field=None,\n                 field=None,\n                 right=False,\n                 basis=None,\n                 source='Log'):\n        \"\"\"\n        Turn a 1D array into a striplog, given a cutoff.\n\n        Args:\n            log (array-like): A 1D array or a list of integers.\n            cutoff (number or array-like): The log value(s) at which to bin\n                the log. Optional.\n            components (array-like): A list of components. Use this or\n                ``legend``.\n            legend (``Legend``): A legend object. Use this or ``components``.\n            legend_field ('str'): If you're not trying to match against\n                components, then you can match the log values to this field in\n                the Decors.\n            field (str): The field in the Interval's ``data`` to store the log\n                values as.\n            right (bool): Which side of the cutoff to send things that are\n                equal to, i.e. right on, the cutoff.\n            basis (array-like): A depth basis for the log, so striplog knows\n                where to put the boundaries.\n            source (str): The source of the data. Default 'Log'.\n\n        Returns:\n            Striplog: The ``striplog`` object.\n        \"\"\"\n        if (components is None) and (legend is None) and (field is None):\n            m = 'You must provide a list of components, and legend, or a field.'\n            raise StriplogError(m)\n\n        if (legend is not None) and (legend_field is None):\n            try:  # To treat it like a legend.\n                components = [deepcopy(decor.component) for decor in legend]\n            except AttributeError:  # It's just a list of components.\n                pass\n\n        if legend_field is not None:\n            field_values = [getattr(d, legend_field, 0) for d in legend]\n            components = [Component() for i in range(int(max(field_values)+1))]\n            for i, decor in enumerate(legend):\n                components[i] = deepcopy(decor.component)\n\n        if cutoff is not None:\n\n            # First make sure we have enough components.\n            try:\n                n = len(cutoff)\n            except TypeError:\n                n = 1\n            if len(components) < n+1:\n                m = 'For n cutoffs, you need to provide at least'\n                m += 'n+1 components.'\n                raise StriplogError(m)\n\n            # Digitize.\n            try:  # To use cutoff as a list.\n                a = np.digitize(log, cutoff, right)\n            except ValueError:  # It's just a number.\n                a = np.digitize(log, [cutoff], right)\n\n        else:\n            a = np.copy(log)\n\n        tops, values = utils.tops_from_loglike(a)\n\n        if basis is None:\n            m = 'You must provide a depth or elevation basis.'\n            raise StriplogError(m)\n\n        list_of_Intervals = cls.__intervals_from_tops(tops,\n                                                      values,\n                                                      basis,\n                                                      components,\n                                                      field=field\n                                                      )\n\n        return cls(list_of_Intervals, source=source)", "code_tokens": ["def", "from_log", "(", "cls", ",", "log", ",", "cutoff", "=", "None", ",", "components", "=", "None", ",", "legend", "=", "None", ",", "legend_field", "=", "None", ",", "field", "=", "None", ",", "right", "=", "False", ",", "basis", "=", "None", ",", "source", "=", "'Log'", ")", ":", "if", "(", "components", "is", "None", ")", "and", "(", "legend", "is", "None", ")", "and", "(", "field", "is", "None", ")", ":", "m", "=", "'You must provide a list of components, and legend, or a field.'", "raise", "StriplogError", "(", "m", ")", "if", "(", "legend", "is", "not", "None", ")", "and", "(", "legend_field", "is", "None", ")", ":", "try", ":", "# To treat it like a legend.", "components", "=", "[", "deepcopy", "(", "decor", ".", "component", ")", "for", "decor", "in", "legend", "]", "except", "AttributeError", ":", "# It's just a list of components.", "pass", "if", "legend_field", "is", "not", "None", ":", "field_values", "=", "[", "getattr", "(", "d", ",", "legend_field", ",", "0", ")", "for", "d", "in", "legend", "]", "components", "=", "[", "Component", "(", ")", "for", "i", "in", "range", "(", "int", "(", "max", "(", "field_values", ")", "+", "1", ")", ")", "]", "for", "i", ",", "decor", "in", "enumerate", "(", "legend", ")", ":", "components", "[", "i", "]", "=", "deepcopy", "(", "decor", ".", "component", ")", "if", "cutoff", "is", "not", "None", ":", "# First make sure we have enough components.", "try", ":", "n", "=", "len", "(", "cutoff", ")", "except", "TypeError", ":", "n", "=", "1", "if", "len", "(", "components", ")", "<", "n", "+", "1", ":", "m", "=", "'For n cutoffs, you need to provide at least'", "m", "+=", "'n+1 components.'", "raise", "StriplogError", "(", "m", ")", "# Digitize.", "try", ":", "# To use cutoff as a list.", "a", "=", "np", ".", "digitize", "(", "log", ",", "cutoff", ",", "right", ")", "except", "ValueError", ":", "# It's just a number.", "a", "=", "np", ".", "digitize", "(", "log", ",", "[", "cutoff", "]", ",", "right", ")", "else", ":", "a", "=", "np", ".", "copy", "(", "log", ")", "tops", ",", "values", "=", "utils", ".", "tops_from_loglike", "(", "a", ")", "if", "basis", "is", "None", ":", "m", "=", "'You must provide a depth or elevation basis.'", "raise", "StriplogError", "(", "m", ")", "list_of_Intervals", "=", "cls", ".", "__intervals_from_tops", "(", "tops", ",", "values", ",", "basis", ",", "components", ",", "field", "=", "field", ")", "return", "cls", "(", "list_of_Intervals", ",", "source", "=", "source", ")"], "docstring": "Turn a 1D array into a striplog, given a cutoff.\n\n        Args:\n            log (array-like): A 1D array or a list of integers.\n            cutoff (number or array-like): The log value(s) at which to bin\n                the log. Optional.\n            components (array-like): A list of components. Use this or\n                ``legend``.\n            legend (``Legend``): A legend object. Use this or ``components``.\n            legend_field ('str'): If you're not trying to match against\n                components, then you can match the log values to this field in\n                the Decors.\n            field (str): The field in the Interval's ``data`` to store the log\n                values as.\n            right (bool): Which side of the cutoff to send things that are\n                equal to, i.e. right on, the cutoff.\n            basis (array-like): A depth basis for the log, so striplog knows\n                where to put the boundaries.\n            source (str): The source of the data. Default 'Log'.\n\n        Returns:\n            Striplog: The ``striplog`` object.", "docstring_tokens": ["Turn", "a", "1D", "array", "into", "a", "striplog", "given", "a", "cutoff", "."], "sha": "8033b673a151f96c29802b43763e863519a3124c", "url": "https://github.com/agile-geoscience/striplog/blob/8033b673a151f96c29802b43763e863519a3124c/striplog/striplog.py#L856-L939", "partition": "test"}
{"repo": "susam/ice", "path": "ice.py", "func_name": "Response.set_cookie", "original_string": "def set_cookie(self, name, value, attrs={}):\n        \"\"\"Add a Set-Cookie header to response object.\n\n        For a description about cookie attribute values, see\n        https://docs.python.org/3/library/http.cookies.html#http.cookies.Morsel.\n\n        Arguments:\n          name (str): Name of the cookie\n          value (str): Value of the cookie\n          attrs (dict): Dicitionary with cookie attribute keys and\n                        values.\n        \"\"\"\n        cookie = http.cookies.SimpleCookie()\n        cookie[name] = value\n        for key, value in attrs.items():\n            cookie[name][key] = value\n        self.add_header('Set-Cookie', cookie[name].OutputString())", "language": "python", "code": "def set_cookie(self, name, value, attrs={}):\n        \"\"\"Add a Set-Cookie header to response object.\n\n        For a description about cookie attribute values, see\n        https://docs.python.org/3/library/http.cookies.html#http.cookies.Morsel.\n\n        Arguments:\n          name (str): Name of the cookie\n          value (str): Value of the cookie\n          attrs (dict): Dicitionary with cookie attribute keys and\n                        values.\n        \"\"\"\n        cookie = http.cookies.SimpleCookie()\n        cookie[name] = value\n        for key, value in attrs.items():\n            cookie[name][key] = value\n        self.add_header('Set-Cookie', cookie[name].OutputString())", "code_tokens": ["def", "set_cookie", "(", "self", ",", "name", ",", "value", ",", "attrs", "=", "{", "}", ")", ":", "cookie", "=", "http", ".", "cookies", ".", "SimpleCookie", "(", ")", "cookie", "[", "name", "]", "=", "value", "for", "key", ",", "value", "in", "attrs", ".", "items", "(", ")", ":", "cookie", "[", "name", "]", "[", "key", "]", "=", "value", "self", ".", "add_header", "(", "'Set-Cookie'", ",", "cookie", "[", "name", "]", ".", "OutputString", "(", ")", ")"], "docstring": "Add a Set-Cookie header to response object.\n\n        For a description about cookie attribute values, see\n        https://docs.python.org/3/library/http.cookies.html#http.cookies.Morsel.\n\n        Arguments:\n          name (str): Name of the cookie\n          value (str): Value of the cookie\n          attrs (dict): Dicitionary with cookie attribute keys and\n                        values.", "docstring_tokens": ["Add", "a", "Set", "-", "Cookie", "header", "to", "response", "object", "."], "sha": "532e685c504ea96f9e42833594585159ac1d2068", "url": "https://github.com/susam/ice/blob/532e685c504ea96f9e42833594585159ac1d2068/ice.py#L794-L810", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/interceptor.py", "func_name": "tape", "original_string": "def tape():\n  \"\"\"Context manager for recording interceptable executions onto a tape.\n\n  Similar to `tf.GradientTape`, operations are recorded if they are executed\n  within this context manager. In addition, the operation must be registered\n  (wrapped) as `ed.interceptable`.\n\n  Yields:\n    tape: OrderedDict where operations are recorded in sequence. Keys are\n      the `name` keyword argument to the operation (typically, a random\n      variable's `name`) and values are the corresponding output of the\n      operation. If the operation has no name, it is not recorded.\n\n  #### Examples\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def probabilistic_matrix_factorization():\n    users = ed.Normal(0., 1., sample_shape=[5000, 128], name=\"users\")\n    items = ed.Normal(0., 1., sample_shape=[7500, 128], name=\"items\")\n    ratings = ed.Normal(loc=tf.matmul(users, items, transpose_b=True),\n                        scale=0.1,\n                        name=\"ratings\")\n    return ratings\n\n  with ed.tape() as model_tape:\n    ratings = probabilistic_matrix_factorization()\n\n  assert model_tape[\"users\"].shape == (5000, 128)\n  assert model_tape[\"items\"].shape == (7500, 128)\n  assert model_tape[\"ratings\"] == ratings\n  ```\n\n  \"\"\"\n  tape_data = collections.OrderedDict({})\n\n  def record(f, *args, **kwargs):\n    \"\"\"Records execution to a tape.\"\"\"\n    name = kwargs.get(\"name\")\n    output = interceptable(f)(*args, **kwargs)\n    if name:\n      tape_data[name] = output\n    return output\n\n  with interception(record):\n    yield tape_data", "language": "python", "code": "def tape():\n  \"\"\"Context manager for recording interceptable executions onto a tape.\n\n  Similar to `tf.GradientTape`, operations are recorded if they are executed\n  within this context manager. In addition, the operation must be registered\n  (wrapped) as `ed.interceptable`.\n\n  Yields:\n    tape: OrderedDict where operations are recorded in sequence. Keys are\n      the `name` keyword argument to the operation (typically, a random\n      variable's `name`) and values are the corresponding output of the\n      operation. If the operation has no name, it is not recorded.\n\n  #### Examples\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def probabilistic_matrix_factorization():\n    users = ed.Normal(0., 1., sample_shape=[5000, 128], name=\"users\")\n    items = ed.Normal(0., 1., sample_shape=[7500, 128], name=\"items\")\n    ratings = ed.Normal(loc=tf.matmul(users, items, transpose_b=True),\n                        scale=0.1,\n                        name=\"ratings\")\n    return ratings\n\n  with ed.tape() as model_tape:\n    ratings = probabilistic_matrix_factorization()\n\n  assert model_tape[\"users\"].shape == (5000, 128)\n  assert model_tape[\"items\"].shape == (7500, 128)\n  assert model_tape[\"ratings\"] == ratings\n  ```\n\n  \"\"\"\n  tape_data = collections.OrderedDict({})\n\n  def record(f, *args, **kwargs):\n    \"\"\"Records execution to a tape.\"\"\"\n    name = kwargs.get(\"name\")\n    output = interceptable(f)(*args, **kwargs)\n    if name:\n      tape_data[name] = output\n    return output\n\n  with interception(record):\n    yield tape_data", "code_tokens": ["def", "tape", "(", ")", ":", "tape_data", "=", "collections", ".", "OrderedDict", "(", "{", "}", ")", "def", "record", "(", "f", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "\"\"\"Records execution to a tape.\"\"\"", "name", "=", "kwargs", ".", "get", "(", "\"name\"", ")", "output", "=", "interceptable", "(", "f", ")", "(", "*", "args", ",", "*", "*", "kwargs", ")", "if", "name", ":", "tape_data", "[", "name", "]", "=", "output", "return", "output", "with", "interception", "(", "record", ")", ":", "yield", "tape_data"], "docstring": "Context manager for recording interceptable executions onto a tape.\n\n  Similar to `tf.GradientTape`, operations are recorded if they are executed\n  within this context manager. In addition, the operation must be registered\n  (wrapped) as `ed.interceptable`.\n\n  Yields:\n    tape: OrderedDict where operations are recorded in sequence. Keys are\n      the `name` keyword argument to the operation (typically, a random\n      variable's `name`) and values are the corresponding output of the\n      operation. If the operation has no name, it is not recorded.\n\n  #### Examples\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def probabilistic_matrix_factorization():\n    users = ed.Normal(0., 1., sample_shape=[5000, 128], name=\"users\")\n    items = ed.Normal(0., 1., sample_shape=[7500, 128], name=\"items\")\n    ratings = ed.Normal(loc=tf.matmul(users, items, transpose_b=True),\n                        scale=0.1,\n                        name=\"ratings\")\n    return ratings\n\n  with ed.tape() as model_tape:\n    ratings = probabilistic_matrix_factorization()\n\n  assert model_tape[\"users\"].shape == (5000, 128)\n  assert model_tape[\"items\"].shape == (7500, 128)\n  assert model_tape[\"ratings\"] == ratings\n  ```", "docstring_tokens": ["Context", "manager", "for", "recording", "interceptable", "executions", "onto", "a", "tape", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/interceptor.py#L199-L245", "partition": "test"}
{"repo": "alexprengere/currencyconverter", "path": "currency_converter/currency_converter.py", "func_name": "CurrencyConverter._compute_missing_rates", "original_string": "def _compute_missing_rates(self, currency):\n        \"\"\"Fill missing rates of a currency.\n\n        This is done by linear interpolation of the two closest available rates.\n\n        :param str currency: The currency to fill missing rates for.\n        \"\"\"\n        rates = self._rates[currency]\n\n        # tmp will store the closest rates forward and backward\n        tmp = defaultdict(lambda: [None, None])\n\n        for date in sorted(rates):\n            rate = rates[date]\n            if rate is not None:\n                closest_rate = rate\n                dist = 0\n            else:\n                dist += 1\n                tmp[date][0] = closest_rate, dist\n\n        for date in sorted(rates, reverse=True):\n            rate = rates[date]\n            if rate is not None:\n                closest_rate = rate\n                dist = 0\n            else:\n                dist += 1\n                tmp[date][1] = closest_rate, dist\n\n        for date in sorted(tmp):\n            (r0, d0), (r1, d1) = tmp[date]\n            rates[date] = (r0 * d1 + r1 * d0) / (d0 + d1)\n            if self.verbose:\n                print(('{0}: filling {1} missing rate using {2} ({3}d old) and '\n                       '{4} ({5}d later)').format(currency, date, r0, d0, r1, d1))", "language": "python", "code": "def _compute_missing_rates(self, currency):\n        \"\"\"Fill missing rates of a currency.\n\n        This is done by linear interpolation of the two closest available rates.\n\n        :param str currency: The currency to fill missing rates for.\n        \"\"\"\n        rates = self._rates[currency]\n\n        # tmp will store the closest rates forward and backward\n        tmp = defaultdict(lambda: [None, None])\n\n        for date in sorted(rates):\n            rate = rates[date]\n            if rate is not None:\n                closest_rate = rate\n                dist = 0\n            else:\n                dist += 1\n                tmp[date][0] = closest_rate, dist\n\n        for date in sorted(rates, reverse=True):\n            rate = rates[date]\n            if rate is not None:\n                closest_rate = rate\n                dist = 0\n            else:\n                dist += 1\n                tmp[date][1] = closest_rate, dist\n\n        for date in sorted(tmp):\n            (r0, d0), (r1, d1) = tmp[date]\n            rates[date] = (r0 * d1 + r1 * d0) / (d0 + d1)\n            if self.verbose:\n                print(('{0}: filling {1} missing rate using {2} ({3}d old) and '\n                       '{4} ({5}d later)').format(currency, date, r0, d0, r1, d1))", "code_tokens": ["def", "_compute_missing_rates", "(", "self", ",", "currency", ")", ":", "rates", "=", "self", ".", "_rates", "[", "currency", "]", "# tmp will store the closest rates forward and backward", "tmp", "=", "defaultdict", "(", "lambda", ":", "[", "None", ",", "None", "]", ")", "for", "date", "in", "sorted", "(", "rates", ")", ":", "rate", "=", "rates", "[", "date", "]", "if", "rate", "is", "not", "None", ":", "closest_rate", "=", "rate", "dist", "=", "0", "else", ":", "dist", "+=", "1", "tmp", "[", "date", "]", "[", "0", "]", "=", "closest_rate", ",", "dist", "for", "date", "in", "sorted", "(", "rates", ",", "reverse", "=", "True", ")", ":", "rate", "=", "rates", "[", "date", "]", "if", "rate", "is", "not", "None", ":", "closest_rate", "=", "rate", "dist", "=", "0", "else", ":", "dist", "+=", "1", "tmp", "[", "date", "]", "[", "1", "]", "=", "closest_rate", ",", "dist", "for", "date", "in", "sorted", "(", "tmp", ")", ":", "(", "r0", ",", "d0", ")", ",", "(", "r1", ",", "d1", ")", "=", "tmp", "[", "date", "]", "rates", "[", "date", "]", "=", "(", "r0", "*", "d1", "+", "r1", "*", "d0", ")", "/", "(", "d0", "+", "d1", ")", "if", "self", ".", "verbose", ":", "print", "(", "(", "'{0}: filling {1} missing rate using {2} ({3}d old) and '", "'{4} ({5}d later)'", ")", ".", "format", "(", "currency", ",", "date", ",", "r0", ",", "d0", ",", "r1", ",", "d1", ")", ")"], "docstring": "Fill missing rates of a currency.\n\n        This is done by linear interpolation of the two closest available rates.\n\n        :param str currency: The currency to fill missing rates for.", "docstring_tokens": ["Fill", "missing", "rates", "of", "a", "currency", "."], "sha": "e3cb0d693819c0c824214225b23a47e9380f71df", "url": "https://github.com/alexprengere/currencyconverter/blob/e3cb0d693819c0c824214225b23a47e9380f71df/currency_converter/currency_converter.py#L208-L243", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/parallel/controller/scheduler.py", "func_name": "TaskScheduler.submit_task", "original_string": "def submit_task(self, job, indices=None):\n        \"\"\"Submit a task to any of a subset of our targets.\"\"\"\n        if indices:\n            loads = [self.loads[i] for i in indices]\n        else:\n            loads = self.loads\n        idx = self.scheme(loads)\n        if indices:\n            idx = indices[idx]\n        target = self.targets[idx]\n        # print (target, map(str, msg[:3]))\n        # send job to the engine\n        self.engine_stream.send(target, flags=zmq.SNDMORE, copy=False)\n        self.engine_stream.send_multipart(job.raw_msg, copy=False)\n        # update load\n        self.add_job(idx)\n        self.pending[target][job.msg_id] = job\n        # notify Hub\n        content = dict(msg_id=job.msg_id, engine_id=target.decode('ascii'))\n        self.session.send(self.mon_stream, 'task_destination', content=content,\n                        ident=[b'tracktask',self.ident])", "language": "python", "code": "def submit_task(self, job, indices=None):\n        \"\"\"Submit a task to any of a subset of our targets.\"\"\"\n        if indices:\n            loads = [self.loads[i] for i in indices]\n        else:\n            loads = self.loads\n        idx = self.scheme(loads)\n        if indices:\n            idx = indices[idx]\n        target = self.targets[idx]\n        # print (target, map(str, msg[:3]))\n        # send job to the engine\n        self.engine_stream.send(target, flags=zmq.SNDMORE, copy=False)\n        self.engine_stream.send_multipart(job.raw_msg, copy=False)\n        # update load\n        self.add_job(idx)\n        self.pending[target][job.msg_id] = job\n        # notify Hub\n        content = dict(msg_id=job.msg_id, engine_id=target.decode('ascii'))\n        self.session.send(self.mon_stream, 'task_destination', content=content,\n                        ident=[b'tracktask',self.ident])", "code_tokens": ["def", "submit_task", "(", "self", ",", "job", ",", "indices", "=", "None", ")", ":", "if", "indices", ":", "loads", "=", "[", "self", ".", "loads", "[", "i", "]", "for", "i", "in", "indices", "]", "else", ":", "loads", "=", "self", ".", "loads", "idx", "=", "self", ".", "scheme", "(", "loads", ")", "if", "indices", ":", "idx", "=", "indices", "[", "idx", "]", "target", "=", "self", ".", "targets", "[", "idx", "]", "# print (target, map(str, msg[:3]))", "# send job to the engine", "self", ".", "engine_stream", ".", "send", "(", "target", ",", "flags", "=", "zmq", ".", "SNDMORE", ",", "copy", "=", "False", ")", "self", ".", "engine_stream", ".", "send_multipart", "(", "job", ".", "raw_msg", ",", "copy", "=", "False", ")", "# update load", "self", ".", "add_job", "(", "idx", ")", "self", ".", "pending", "[", "target", "]", "[", "job", ".", "msg_id", "]", "=", "job", "# notify Hub", "content", "=", "dict", "(", "msg_id", "=", "job", ".", "msg_id", ",", "engine_id", "=", "target", ".", "decode", "(", "'ascii'", ")", ")", "self", ".", "session", ".", "send", "(", "self", ".", "mon_stream", ",", "'task_destination'", ",", "content", "=", "content", ",", "ident", "=", "[", "b'tracktask'", ",", "self", ".", "ident", "]", ")"], "docstring": "Submit a task to any of a subset of our targets.", "docstring_tokens": ["Submit", "a", "task", "to", "any", "of", "a", "subset", "of", "our", "targets", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/parallel/controller/scheduler.py#L543-L563", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/message/message_store.py", "func_name": "MessagesStore.get_message_definitions", "original_string": "def get_message_definitions(self, msgid_or_symbol: str) -> list:\n        \"\"\"Returns the Message object for this message.\n\n        :param str msgid_or_symbol: msgid_or_symbol may be either a numeric or symbolic id.\n        :raises UnknownMessageError: if the message id is not defined.\n        :rtype: List of MessageDefinition\n        :return: A message definition corresponding to msgid_or_symbol\n        \"\"\"\n        if msgid_or_symbol[1:].isdigit():\n            msgid_or_symbol = msgid_or_symbol.upper()\n        for source in (self._alternative_names, self._messages_definitions):\n            try:\n                return [source[msgid_or_symbol]]\n            except KeyError:\n                pass\n        error_msg = \"No such message id or symbol '{msgid_or_symbol}'.\".format(\n            msgid_or_symbol=msgid_or_symbol\n        )\n        raise UnknownMessageError(error_msg)", "language": "python", "code": "def get_message_definitions(self, msgid_or_symbol: str) -> list:\n        \"\"\"Returns the Message object for this message.\n\n        :param str msgid_or_symbol: msgid_or_symbol may be either a numeric or symbolic id.\n        :raises UnknownMessageError: if the message id is not defined.\n        :rtype: List of MessageDefinition\n        :return: A message definition corresponding to msgid_or_symbol\n        \"\"\"\n        if msgid_or_symbol[1:].isdigit():\n            msgid_or_symbol = msgid_or_symbol.upper()\n        for source in (self._alternative_names, self._messages_definitions):\n            try:\n                return [source[msgid_or_symbol]]\n            except KeyError:\n                pass\n        error_msg = \"No such message id or symbol '{msgid_or_symbol}'.\".format(\n            msgid_or_symbol=msgid_or_symbol\n        )\n        raise UnknownMessageError(error_msg)", "code_tokens": ["def", "get_message_definitions", "(", "self", ",", "msgid_or_symbol", ":", "str", ")", "->", "list", ":", "if", "msgid_or_symbol", "[", "1", ":", "]", ".", "isdigit", "(", ")", ":", "msgid_or_symbol", "=", "msgid_or_symbol", ".", "upper", "(", ")", "for", "source", "in", "(", "self", ".", "_alternative_names", ",", "self", ".", "_messages_definitions", ")", ":", "try", ":", "return", "[", "source", "[", "msgid_or_symbol", "]", "]", "except", "KeyError", ":", "pass", "error_msg", "=", "\"No such message id or symbol '{msgid_or_symbol}'.\"", ".", "format", "(", "msgid_or_symbol", "=", "msgid_or_symbol", ")", "raise", "UnknownMessageError", "(", "error_msg", ")"], "docstring": "Returns the Message object for this message.\n\n        :param str msgid_or_symbol: msgid_or_symbol may be either a numeric or symbolic id.\n        :raises UnknownMessageError: if the message id is not defined.\n        :rtype: List of MessageDefinition\n        :return: A message definition corresponding to msgid_or_symbol", "docstring_tokens": ["Returns", "the", "Message", "object", "for", "this", "message", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/message/message_store.py#L162-L180", "partition": "test"}
{"repo": "neurosynth/neurosynth", "path": "neurosynth/utils.py", "func_name": "deprecated", "original_string": "def deprecated(*args):\n    \"\"\" Deprecation warning decorator. Takes optional deprecation message,\n    otherwise will use a generic warning. \"\"\"\n    def wrap(func):\n        def wrapped_func(*args, **kwargs):\n            warnings.warn(msg, category=DeprecationWarning)\n            return func(*args, **kwargs)\n        return wrapped_func\n\n    if len(args) == 1 and callable(args[0]):\n        msg = \"Function '%s' will be deprecated in future versions of \" \\\n            \"Neurosynth.\" % args[0].__name__\n        return wrap(args[0])\n    else:\n        msg = args[0]\n        return wrap", "language": "python", "code": "def deprecated(*args):\n    \"\"\" Deprecation warning decorator. Takes optional deprecation message,\n    otherwise will use a generic warning. \"\"\"\n    def wrap(func):\n        def wrapped_func(*args, **kwargs):\n            warnings.warn(msg, category=DeprecationWarning)\n            return func(*args, **kwargs)\n        return wrapped_func\n\n    if len(args) == 1 and callable(args[0]):\n        msg = \"Function '%s' will be deprecated in future versions of \" \\\n            \"Neurosynth.\" % args[0].__name__\n        return wrap(args[0])\n    else:\n        msg = args[0]\n        return wrap", "code_tokens": ["def", "deprecated", "(", "*", "args", ")", ":", "def", "wrap", "(", "func", ")", ":", "def", "wrapped_func", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "warnings", ".", "warn", "(", "msg", ",", "category", "=", "DeprecationWarning", ")", "return", "func", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "wrapped_func", "if", "len", "(", "args", ")", "==", "1", "and", "callable", "(", "args", "[", "0", "]", ")", ":", "msg", "=", "\"Function '%s' will be deprecated in future versions of \"", "\"Neurosynth.\"", "%", "args", "[", "0", "]", ".", "__name__", "return", "wrap", "(", "args", "[", "0", "]", ")", "else", ":", "msg", "=", "args", "[", "0", "]", "return", "wrap"], "docstring": "Deprecation warning decorator. Takes optional deprecation message,\n    otherwise will use a generic warning.", "docstring_tokens": ["Deprecation", "warning", "decorator", ".", "Takes", "optional", "deprecation", "message", "otherwise", "will", "use", "a", "generic", "warning", "."], "sha": "948ce7edce15d7df693446e76834e0c23bfe8f11", "url": "https://github.com/neurosynth/neurosynth/blob/948ce7edce15d7df693446e76834e0c23bfe8f11/neurosynth/utils.py#L3-L18", "partition": "test"}
{"repo": "vaexio/vaex", "path": "packages/vaex-core/vaex/functions.py", "func_name": "dt_minute", "original_string": "def dt_minute(x):\n    \"\"\"Extracts the minute out of a datetime samples.\n\n    :returns: an expression containing the minute extracted from a datetime column.\n\n    Example:\n\n    >>> import vaex\n    >>> import numpy as np\n    >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)\n    >>> df = vaex.from_arrays(date=date)\n    >>> df\n      #  date\n      0  2009-10-12 03:31:00\n      1  2016-02-11 10:17:34\n      2  2015-11-12 11:34:22\n\n    >>> df.date.dt.minute\n    Expression = dt_minute(date)\n    Length: 3 dtype: int64 (expression)\n    -----------------------------------\n    0  31\n    1  17\n    2  34\n    \"\"\"\n    import pandas as pd\n    return pd.Series(x).dt.minute.values", "language": "python", "code": "def dt_minute(x):\n    \"\"\"Extracts the minute out of a datetime samples.\n\n    :returns: an expression containing the minute extracted from a datetime column.\n\n    Example:\n\n    >>> import vaex\n    >>> import numpy as np\n    >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)\n    >>> df = vaex.from_arrays(date=date)\n    >>> df\n      #  date\n      0  2009-10-12 03:31:00\n      1  2016-02-11 10:17:34\n      2  2015-11-12 11:34:22\n\n    >>> df.date.dt.minute\n    Expression = dt_minute(date)\n    Length: 3 dtype: int64 (expression)\n    -----------------------------------\n    0  31\n    1  17\n    2  34\n    \"\"\"\n    import pandas as pd\n    return pd.Series(x).dt.minute.values", "code_tokens": ["def", "dt_minute", "(", "x", ")", ":", "import", "pandas", "as", "pd", "return", "pd", ".", "Series", "(", "x", ")", ".", "dt", ".", "minute", ".", "values"], "docstring": "Extracts the minute out of a datetime samples.\n\n    :returns: an expression containing the minute extracted from a datetime column.\n\n    Example:\n\n    >>> import vaex\n    >>> import numpy as np\n    >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)\n    >>> df = vaex.from_arrays(date=date)\n    >>> df\n      #  date\n      0  2009-10-12 03:31:00\n      1  2016-02-11 10:17:34\n      2  2015-11-12 11:34:22\n\n    >>> df.date.dt.minute\n    Expression = dt_minute(date)\n    Length: 3 dtype: int64 (expression)\n    -----------------------------------\n    0  31\n    1  17\n    2  34", "docstring_tokens": ["Extracts", "the", "minute", "out", "of", "a", "datetime", "samples", "."], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/functions.py#L450-L476", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/base.py", "func_name": "BasicChecker._check_not_in_finally", "original_string": "def _check_not_in_finally(self, node, node_name, breaker_classes=()):\n        \"\"\"check that a node is not inside a finally clause of a\n        try...finally statement.\n        If we found before a try...finally bloc a parent which its type is\n        in breaker_classes, we skip the whole check.\"\"\"\n        # if self._tryfinallys is empty, we're not an in try...finally block\n        if not self._tryfinallys:\n            return\n        # the node could be a grand-grand...-children of the try...finally\n        _parent = node.parent\n        _node = node\n        while _parent and not isinstance(_parent, breaker_classes):\n            if hasattr(_parent, \"finalbody\") and _node in _parent.finalbody:\n                self.add_message(\"lost-exception\", node=node, args=node_name)\n                return\n            _node = _parent\n            _parent = _node.parent", "language": "python", "code": "def _check_not_in_finally(self, node, node_name, breaker_classes=()):\n        \"\"\"check that a node is not inside a finally clause of a\n        try...finally statement.\n        If we found before a try...finally bloc a parent which its type is\n        in breaker_classes, we skip the whole check.\"\"\"\n        # if self._tryfinallys is empty, we're not an in try...finally block\n        if not self._tryfinallys:\n            return\n        # the node could be a grand-grand...-children of the try...finally\n        _parent = node.parent\n        _node = node\n        while _parent and not isinstance(_parent, breaker_classes):\n            if hasattr(_parent, \"finalbody\") and _node in _parent.finalbody:\n                self.add_message(\"lost-exception\", node=node, args=node_name)\n                return\n            _node = _parent\n            _parent = _node.parent", "code_tokens": ["def", "_check_not_in_finally", "(", "self", ",", "node", ",", "node_name", ",", "breaker_classes", "=", "(", ")", ")", ":", "# if self._tryfinallys is empty, we're not an in try...finally block", "if", "not", "self", ".", "_tryfinallys", ":", "return", "# the node could be a grand-grand...-children of the try...finally", "_parent", "=", "node", ".", "parent", "_node", "=", "node", "while", "_parent", "and", "not", "isinstance", "(", "_parent", ",", "breaker_classes", ")", ":", "if", "hasattr", "(", "_parent", ",", "\"finalbody\"", ")", "and", "_node", "in", "_parent", ".", "finalbody", ":", "self", ".", "add_message", "(", "\"lost-exception\"", ",", "node", "=", "node", ",", "args", "=", "node_name", ")", "return", "_node", "=", "_parent", "_parent", "=", "_node", ".", "parent"], "docstring": "check that a node is not inside a finally clause of a\n        try...finally statement.\n        If we found before a try...finally bloc a parent which its type is\n        in breaker_classes, we skip the whole check.", "docstring_tokens": ["check", "that", "a", "node", "is", "not", "inside", "a", "finally", "clause", "of", "a", "try", "...", "finally", "statement", ".", "If", "we", "found", "before", "a", "try", "...", "finally", "bloc", "a", "parent", "which", "its", "type", "is", "in", "breaker_classes", "we", "skip", "the", "whole", "check", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/base.py#L1379-L1395", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/vq_vae.py", "func_name": "visualize_training", "original_string": "def visualize_training(images_val,\n                       reconstructed_images_val,\n                       random_images_val,\n                       log_dir, prefix, viz_n=10):\n  \"\"\"Helper method to save images visualizing model reconstructions.\n\n  Args:\n    images_val: Numpy array containing a batch of input images.\n    reconstructed_images_val: Numpy array giving the expected output\n      (mean) of the decoder.\n    random_images_val: Optionally, a Numpy array giving the expected output\n      (mean) of decoding samples from the prior, or `None`.\n    log_dir: The directory to write images (Python `str`).\n    prefix: A specific label for the saved visualizations, which\n      determines their filenames (Python `str`).\n    viz_n: The number of images from each batch to visualize (Python `int`).\n  \"\"\"\n  save_imgs(images_val[:viz_n],\n            os.path.join(log_dir, \"{}_inputs.png\".format(prefix)))\n  save_imgs(reconstructed_images_val[:viz_n],\n            os.path.join(log_dir,\n                         \"{}_reconstructions.png\".format(prefix)))\n\n  if random_images_val is not None:\n    save_imgs(random_images_val[:viz_n],\n              os.path.join(log_dir,\n                           \"{}_prior_samples.png\".format(prefix)))", "language": "python", "code": "def visualize_training(images_val,\n                       reconstructed_images_val,\n                       random_images_val,\n                       log_dir, prefix, viz_n=10):\n  \"\"\"Helper method to save images visualizing model reconstructions.\n\n  Args:\n    images_val: Numpy array containing a batch of input images.\n    reconstructed_images_val: Numpy array giving the expected output\n      (mean) of the decoder.\n    random_images_val: Optionally, a Numpy array giving the expected output\n      (mean) of decoding samples from the prior, or `None`.\n    log_dir: The directory to write images (Python `str`).\n    prefix: A specific label for the saved visualizations, which\n      determines their filenames (Python `str`).\n    viz_n: The number of images from each batch to visualize (Python `int`).\n  \"\"\"\n  save_imgs(images_val[:viz_n],\n            os.path.join(log_dir, \"{}_inputs.png\".format(prefix)))\n  save_imgs(reconstructed_images_val[:viz_n],\n            os.path.join(log_dir,\n                         \"{}_reconstructions.png\".format(prefix)))\n\n  if random_images_val is not None:\n    save_imgs(random_images_val[:viz_n],\n              os.path.join(log_dir,\n                           \"{}_prior_samples.png\".format(prefix)))", "code_tokens": ["def", "visualize_training", "(", "images_val", ",", "reconstructed_images_val", ",", "random_images_val", ",", "log_dir", ",", "prefix", ",", "viz_n", "=", "10", ")", ":", "save_imgs", "(", "images_val", "[", ":", "viz_n", "]", ",", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"{}_inputs.png\"", ".", "format", "(", "prefix", ")", ")", ")", "save_imgs", "(", "reconstructed_images_val", "[", ":", "viz_n", "]", ",", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"{}_reconstructions.png\"", ".", "format", "(", "prefix", ")", ")", ")", "if", "random_images_val", "is", "not", "None", ":", "save_imgs", "(", "random_images_val", "[", ":", "viz_n", "]", ",", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"{}_prior_samples.png\"", ".", "format", "(", "prefix", ")", ")", ")"], "docstring": "Helper method to save images visualizing model reconstructions.\n\n  Args:\n    images_val: Numpy array containing a batch of input images.\n    reconstructed_images_val: Numpy array giving the expected output\n      (mean) of the decoder.\n    random_images_val: Optionally, a Numpy array giving the expected output\n      (mean) of decoding samples from the prior, or `None`.\n    log_dir: The directory to write images (Python `str`).\n    prefix: A specific label for the saved visualizations, which\n      determines their filenames (Python `str`).\n    viz_n: The number of images from each batch to visualize (Python `int`).", "docstring_tokens": ["Helper", "method", "to", "save", "images", "visualizing", "model", "reconstructions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/vq_vae.py#L324-L350", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/flask/app.py", "func_name": "Flask.auto_find_instance_path", "original_string": "def auto_find_instance_path(self):\n        \"\"\"Tries to locate the instance path if it was not provided to the\n        constructor of the application class.  It will basically calculate\n        the path to a folder named ``instance`` next to your main file or\n        the package.\n\n        .. versionadded:: 0.8\n        \"\"\"\n        prefix, package_path = find_package(self.import_name)\n        if prefix is None:\n            return os.path.join(package_path, 'instance')\n        return os.path.join(prefix, 'var', self.name + '-instance')", "language": "python", "code": "def auto_find_instance_path(self):\n        \"\"\"Tries to locate the instance path if it was not provided to the\n        constructor of the application class.  It will basically calculate\n        the path to a folder named ``instance`` next to your main file or\n        the package.\n\n        .. versionadded:: 0.8\n        \"\"\"\n        prefix, package_path = find_package(self.import_name)\n        if prefix is None:\n            return os.path.join(package_path, 'instance')\n        return os.path.join(prefix, 'var', self.name + '-instance')", "code_tokens": ["def", "auto_find_instance_path", "(", "self", ")", ":", "prefix", ",", "package_path", "=", "find_package", "(", "self", ".", "import_name", ")", "if", "prefix", "is", "None", ":", "return", "os", ".", "path", ".", "join", "(", "package_path", ",", "'instance'", ")", "return", "os", ".", "path", ".", "join", "(", "prefix", ",", "'var'", ",", "self", ".", "name", "+", "'-instance'", ")"], "docstring": "Tries to locate the instance path if it was not provided to the\n        constructor of the application class.  It will basically calculate\n        the path to a folder named ``instance`` next to your main file or\n        the package.\n\n        .. versionadded:: 0.8", "docstring_tokens": ["Tries", "to", "locate", "the", "instance", "path", "if", "it", "was", "not", "provided", "to", "the", "constructor", "of", "the", "application", "class", ".", "It", "will", "basically", "calculate", "the", "path", "to", "a", "folder", "named", "instance", "next", "to", "your", "main", "file", "or", "the", "package", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/flask/app.py#L614-L625", "partition": "test"}
{"repo": "singularityhub/sregistry-cli", "path": "sregistry/main/google_build/delete.py", "func_name": "delete", "original_string": "def delete(self, name):\n    '''delete an image from Google Storage.\n\n       Parameters\n       ==========\n       name: the name of the file (or image) to delete\n\n    '''\n\n    bot.debug(\"DELETE %s\" % name)\n\n    for file_object in files:\n        if isinstance(file_object, dict):\n            if \"kind\" in file_object:\n                if file_object['kind'] == \"storage#object\":\n                    object_name = \"/\".join(file_object['id'].split('/')[:-1])\n                    object_name = re.sub('%s/' %self._bucket['name'],'', object_name,1)\n\n                    delete_object(service=self._bucket_service,\n                                  bucket_name=bucket['name'],\n                                  object_name=object_name)", "language": "python", "code": "def delete(self, name):\n    '''delete an image from Google Storage.\n\n       Parameters\n       ==========\n       name: the name of the file (or image) to delete\n\n    '''\n\n    bot.debug(\"DELETE %s\" % name)\n\n    for file_object in files:\n        if isinstance(file_object, dict):\n            if \"kind\" in file_object:\n                if file_object['kind'] == \"storage#object\":\n                    object_name = \"/\".join(file_object['id'].split('/')[:-1])\n                    object_name = re.sub('%s/' %self._bucket['name'],'', object_name,1)\n\n                    delete_object(service=self._bucket_service,\n                                  bucket_name=bucket['name'],\n                                  object_name=object_name)", "code_tokens": ["def", "delete", "(", "self", ",", "name", ")", ":", "bot", ".", "debug", "(", "\"DELETE %s\"", "%", "name", ")", "for", "file_object", "in", "files", ":", "if", "isinstance", "(", "file_object", ",", "dict", ")", ":", "if", "\"kind\"", "in", "file_object", ":", "if", "file_object", "[", "'kind'", "]", "==", "\"storage#object\"", ":", "object_name", "=", "\"/\"", ".", "join", "(", "file_object", "[", "'id'", "]", ".", "split", "(", "'/'", ")", "[", ":", "-", "1", "]", ")", "object_name", "=", "re", ".", "sub", "(", "'%s/'", "%", "self", ".", "_bucket", "[", "'name'", "]", ",", "''", ",", "object_name", ",", "1", ")", "delete_object", "(", "service", "=", "self", ".", "_bucket_service", ",", "bucket_name", "=", "bucket", "[", "'name'", "]", ",", "object_name", "=", "object_name", ")"], "docstring": "delete an image from Google Storage.\n\n       Parameters\n       ==========\n       name: the name of the file (or image) to delete", "docstring_tokens": ["delete", "an", "image", "from", "Google", "Storage", "."], "sha": "abc96140a1d15b5e96d83432e1e0e1f4f8f36331", "url": "https://github.com/singularityhub/sregistry-cli/blob/abc96140a1d15b5e96d83432e1e0e1f4f8f36331/sregistry/main/google_build/delete.py#L42-L62", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/operators/slack_operator.py", "func_name": "SlackAPIOperator.execute", "original_string": "def execute(self, **kwargs):\n        \"\"\"\n        SlackAPIOperator calls will not fail even if the call is not unsuccessful.\n        It should not prevent a DAG from completing in success\n        \"\"\"\n        if not self.api_params:\n            self.construct_api_call_params()\n        slack = SlackHook(token=self.token, slack_conn_id=self.slack_conn_id)\n        slack.call(self.method, self.api_params)", "language": "python", "code": "def execute(self, **kwargs):\n        \"\"\"\n        SlackAPIOperator calls will not fail even if the call is not unsuccessful.\n        It should not prevent a DAG from completing in success\n        \"\"\"\n        if not self.api_params:\n            self.construct_api_call_params()\n        slack = SlackHook(token=self.token, slack_conn_id=self.slack_conn_id)\n        slack.call(self.method, self.api_params)", "code_tokens": ["def", "execute", "(", "self", ",", "*", "*", "kwargs", ")", ":", "if", "not", "self", ".", "api_params", ":", "self", ".", "construct_api_call_params", "(", ")", "slack", "=", "SlackHook", "(", "token", "=", "self", ".", "token", ",", "slack_conn_id", "=", "self", ".", "slack_conn_id", ")", "slack", ".", "call", "(", "self", ".", "method", ",", "self", ".", "api_params", ")"], "docstring": "SlackAPIOperator calls will not fail even if the call is not unsuccessful.\n        It should not prevent a DAG from completing in success", "docstring_tokens": ["SlackAPIOperator", "calls", "will", "not", "fail", "even", "if", "the", "call", "is", "not", "unsuccessful", ".", "It", "should", "not", "prevent", "a", "DAG", "from", "completing", "in", "success"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/slack_operator.py#L79-L87", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/obj.py", "func_name": "map_lrepr", "original_string": "def map_lrepr(\n    entries: Callable[[], Iterable[Tuple[Any, Any]]],\n    start: str,\n    end: str,\n    meta=None,\n    **kwargs,\n) -> str:\n    \"\"\"Produce a Lisp representation of an associative collection, bookended\n    with the start and end string supplied. The entries argument must be a\n    callable which will produce tuples of key-value pairs.\n\n    The keyword arguments will be passed along to lrepr for the sequence\n    elements.\"\"\"\n    print_level = kwargs[\"print_level\"]\n    if isinstance(print_level, int) and print_level < 1:\n        return SURPASSED_PRINT_LEVEL\n\n    kwargs = _process_kwargs(**kwargs)\n\n    def entry_reprs():\n        for k, v in entries():\n            yield \"{k} {v}\".format(k=lrepr(k, **kwargs), v=lrepr(v, **kwargs))\n\n    trailer = []\n    print_dup = kwargs[\"print_dup\"]\n    print_length = kwargs[\"print_length\"]\n    if not print_dup and isinstance(print_length, int):\n        items = seq(entry_reprs()).take(print_length + 1).to_list()\n        if len(items) > print_length:\n            items.pop()\n            trailer.append(SURPASSED_PRINT_LENGTH)\n    else:\n        items = list(entry_reprs())\n\n    seq_lrepr = PRINT_SEPARATOR.join(items + trailer)\n\n    print_meta = kwargs[\"print_meta\"]\n    if print_meta and meta:\n        return f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\"\n\n    return f\"{start}{seq_lrepr}{end}\"", "language": "python", "code": "def map_lrepr(\n    entries: Callable[[], Iterable[Tuple[Any, Any]]],\n    start: str,\n    end: str,\n    meta=None,\n    **kwargs,\n) -> str:\n    \"\"\"Produce a Lisp representation of an associative collection, bookended\n    with the start and end string supplied. The entries argument must be a\n    callable which will produce tuples of key-value pairs.\n\n    The keyword arguments will be passed along to lrepr for the sequence\n    elements.\"\"\"\n    print_level = kwargs[\"print_level\"]\n    if isinstance(print_level, int) and print_level < 1:\n        return SURPASSED_PRINT_LEVEL\n\n    kwargs = _process_kwargs(**kwargs)\n\n    def entry_reprs():\n        for k, v in entries():\n            yield \"{k} {v}\".format(k=lrepr(k, **kwargs), v=lrepr(v, **kwargs))\n\n    trailer = []\n    print_dup = kwargs[\"print_dup\"]\n    print_length = kwargs[\"print_length\"]\n    if not print_dup and isinstance(print_length, int):\n        items = seq(entry_reprs()).take(print_length + 1).to_list()\n        if len(items) > print_length:\n            items.pop()\n            trailer.append(SURPASSED_PRINT_LENGTH)\n    else:\n        items = list(entry_reprs())\n\n    seq_lrepr = PRINT_SEPARATOR.join(items + trailer)\n\n    print_meta = kwargs[\"print_meta\"]\n    if print_meta and meta:\n        return f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\"\n\n    return f\"{start}{seq_lrepr}{end}\"", "code_tokens": ["def", "map_lrepr", "(", "entries", ":", "Callable", "[", "[", "]", ",", "Iterable", "[", "Tuple", "[", "Any", ",", "Any", "]", "]", "]", ",", "start", ":", "str", ",", "end", ":", "str", ",", "meta", "=", "None", ",", "*", "*", "kwargs", ",", ")", "->", "str", ":", "print_level", "=", "kwargs", "[", "\"print_level\"", "]", "if", "isinstance", "(", "print_level", ",", "int", ")", "and", "print_level", "<", "1", ":", "return", "SURPASSED_PRINT_LEVEL", "kwargs", "=", "_process_kwargs", "(", "*", "*", "kwargs", ")", "def", "entry_reprs", "(", ")", ":", "for", "k", ",", "v", "in", "entries", "(", ")", ":", "yield", "\"{k} {v}\"", ".", "format", "(", "k", "=", "lrepr", "(", "k", ",", "*", "*", "kwargs", ")", ",", "v", "=", "lrepr", "(", "v", ",", "*", "*", "kwargs", ")", ")", "trailer", "=", "[", "]", "print_dup", "=", "kwargs", "[", "\"print_dup\"", "]", "print_length", "=", "kwargs", "[", "\"print_length\"", "]", "if", "not", "print_dup", "and", "isinstance", "(", "print_length", ",", "int", ")", ":", "items", "=", "seq", "(", "entry_reprs", "(", ")", ")", ".", "take", "(", "print_length", "+", "1", ")", ".", "to_list", "(", ")", "if", "len", "(", "items", ")", ">", "print_length", ":", "items", ".", "pop", "(", ")", "trailer", ".", "append", "(", "SURPASSED_PRINT_LENGTH", ")", "else", ":", "items", "=", "list", "(", "entry_reprs", "(", ")", ")", "seq_lrepr", "=", "PRINT_SEPARATOR", ".", "join", "(", "items", "+", "trailer", ")", "print_meta", "=", "kwargs", "[", "\"print_meta\"", "]", "if", "print_meta", "and", "meta", ":", "return", "f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\"", "return", "f\"{start}{seq_lrepr}{end}\""], "docstring": "Produce a Lisp representation of an associative collection, bookended\n    with the start and end string supplied. The entries argument must be a\n    callable which will produce tuples of key-value pairs.\n\n    The keyword arguments will be passed along to lrepr for the sequence\n    elements.", "docstring_tokens": ["Produce", "a", "Lisp", "representation", "of", "an", "associative", "collection", "bookended", "with", "the", "start", "and", "end", "string", "supplied", ".", "The", "entries", "argument", "must", "be", "a", "callable", "which", "will", "produce", "tuples", "of", "key", "-", "value", "pairs", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/obj.py#L67-L107", "partition": "test"}
{"repo": "nprapps/copydoc", "path": "copydoc.py", "func_name": "CopyDoc._parse_attr", "original_string": "def _parse_attr(self, tagname, attr, value):\n        \"\"\"\n        Parse attribute. Delegate to href parser for hrefs, otherwise return\n        value.\n        \"\"\"\n        if tagname == 'a' and attr == 'href':\n            return self._parse_href(value)\n        else:\n            return value", "language": "python", "code": "def _parse_attr(self, tagname, attr, value):\n        \"\"\"\n        Parse attribute. Delegate to href parser for hrefs, otherwise return\n        value.\n        \"\"\"\n        if tagname == 'a' and attr == 'href':\n            return self._parse_href(value)\n        else:\n            return value", "code_tokens": ["def", "_parse_attr", "(", "self", ",", "tagname", ",", "attr", ",", "value", ")", ":", "if", "tagname", "==", "'a'", "and", "attr", "==", "'href'", ":", "return", "self", ".", "_parse_href", "(", "value", ")", "else", ":", "return", "value"], "docstring": "Parse attribute. Delegate to href parser for hrefs, otherwise return\n        value.", "docstring_tokens": ["Parse", "attribute", ".", "Delegate", "to", "href", "parser", "for", "hrefs", "otherwise", "return", "value", "."], "sha": "e1ab09b287beb0439748c319cf165cbc06c66624", "url": "https://github.com/nprapps/copydoc/blob/e1ab09b287beb0439748c319cf165cbc06c66624/copydoc.py#L197-L205", "partition": "test"}
{"repo": "bolt-project/bolt", "path": "bolt/spark/construct.py", "func_name": "ConstructSpark.ones", "original_string": "def ones(shape, context=None, axis=(0,), dtype=float64, npartitions=None):\n        \"\"\"\n        Create a spark bolt array of ones.\n\n        Parameters\n        ----------\n        shape : tuple\n            The desired shape of the array.\n\n        context : SparkContext\n            A context running Spark. (see pyspark)\n\n        axis : tuple, optional, default=(0,)\n            Which axes to distribute the array along. The resulting\n            distributed object will use keys to represent these axes,\n            with the remaining axes represented by values.\n\n        dtype : data-type, optional, default=float64\n            The desired data-type for the array. If None, will\n            be determined from the data. (see numpy)\n\n        npartitions : int\n            Number of partitions for parallization.\n\n        Returns\n        -------\n        BoltArraySpark\n        \"\"\"\n        from numpy import ones\n        return ConstructSpark._wrap(ones, shape, context, axis, dtype, npartitions)", "language": "python", "code": "def ones(shape, context=None, axis=(0,), dtype=float64, npartitions=None):\n        \"\"\"\n        Create a spark bolt array of ones.\n\n        Parameters\n        ----------\n        shape : tuple\n            The desired shape of the array.\n\n        context : SparkContext\n            A context running Spark. (see pyspark)\n\n        axis : tuple, optional, default=(0,)\n            Which axes to distribute the array along. The resulting\n            distributed object will use keys to represent these axes,\n            with the remaining axes represented by values.\n\n        dtype : data-type, optional, default=float64\n            The desired data-type for the array. If None, will\n            be determined from the data. (see numpy)\n\n        npartitions : int\n            Number of partitions for parallization.\n\n        Returns\n        -------\n        BoltArraySpark\n        \"\"\"\n        from numpy import ones\n        return ConstructSpark._wrap(ones, shape, context, axis, dtype, npartitions)", "code_tokens": ["def", "ones", "(", "shape", ",", "context", "=", "None", ",", "axis", "=", "(", "0", ",", ")", ",", "dtype", "=", "float64", ",", "npartitions", "=", "None", ")", ":", "from", "numpy", "import", "ones", "return", "ConstructSpark", ".", "_wrap", "(", "ones", ",", "shape", ",", "context", ",", "axis", ",", "dtype", ",", "npartitions", ")"], "docstring": "Create a spark bolt array of ones.\n\n        Parameters\n        ----------\n        shape : tuple\n            The desired shape of the array.\n\n        context : SparkContext\n            A context running Spark. (see pyspark)\n\n        axis : tuple, optional, default=(0,)\n            Which axes to distribute the array along. The resulting\n            distributed object will use keys to represent these axes,\n            with the remaining axes represented by values.\n\n        dtype : data-type, optional, default=float64\n            The desired data-type for the array. If None, will\n            be determined from the data. (see numpy)\n\n        npartitions : int\n            Number of partitions for parallization.\n\n        Returns\n        -------\n        BoltArraySpark", "docstring_tokens": ["Create", "a", "spark", "bolt", "array", "of", "ones", "."], "sha": "9cd7104aa085498da3097b72696184b9d3651c51", "url": "https://github.com/bolt-project/bolt/blob/9cd7104aa085498da3097b72696184b9d3651c51/bolt/spark/construct.py#L73-L102", "partition": "test"}
{"repo": "pyblish/pyblish-maya", "path": "pyblish_maya/vendor/Qt.py", "func_name": "convert", "original_string": "def convert(lines):\n    \"\"\"Convert compiled .ui file from PySide2 to Qt.py\n\n    Arguments:\n        lines (list): Each line of of .ui file\n\n    Usage:\n        >> with open(\"myui.py\") as f:\n        ..   lines = convert(f.readlines())\n\n    \"\"\"\n\n    def parse(line):\n        line = line.replace(\"from PySide2 import\", \"from Qt import\")\n        line = line.replace(\"QtWidgets.QApplication.translate\",\n                            \"Qt.QtCompat.translate\")\n        return line\n\n    parsed = list()\n    for line in lines:\n        line = parse(line)\n        parsed.append(line)\n\n    return parsed", "language": "python", "code": "def convert(lines):\n    \"\"\"Convert compiled .ui file from PySide2 to Qt.py\n\n    Arguments:\n        lines (list): Each line of of .ui file\n\n    Usage:\n        >> with open(\"myui.py\") as f:\n        ..   lines = convert(f.readlines())\n\n    \"\"\"\n\n    def parse(line):\n        line = line.replace(\"from PySide2 import\", \"from Qt import\")\n        line = line.replace(\"QtWidgets.QApplication.translate\",\n                            \"Qt.QtCompat.translate\")\n        return line\n\n    parsed = list()\n    for line in lines:\n        line = parse(line)\n        parsed.append(line)\n\n    return parsed", "code_tokens": ["def", "convert", "(", "lines", ")", ":", "def", "parse", "(", "line", ")", ":", "line", "=", "line", ".", "replace", "(", "\"from PySide2 import\"", ",", "\"from Qt import\"", ")", "line", "=", "line", ".", "replace", "(", "\"QtWidgets.QApplication.translate\"", ",", "\"Qt.QtCompat.translate\"", ")", "return", "line", "parsed", "=", "list", "(", ")", "for", "line", "in", "lines", ":", "line", "=", "parse", "(", "line", ")", "parsed", ".", "append", "(", "line", ")", "return", "parsed"], "docstring": "Convert compiled .ui file from PySide2 to Qt.py\n\n    Arguments:\n        lines (list): Each line of of .ui file\n\n    Usage:\n        >> with open(\"myui.py\") as f:\n        ..   lines = convert(f.readlines())", "docstring_tokens": ["Convert", "compiled", ".", "ui", "file", "from", "PySide2", "to", "Qt", ".", "py"], "sha": "75db8b5d8de9d53ae95e74195a788b5f6db2cb5f", "url": "https://github.com/pyblish/pyblish-maya/blob/75db8b5d8de9d53ae95e74195a788b5f6db2cb5f/pyblish_maya/vendor/Qt.py#L87-L110", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/vi/csiszar_divergence.py", "func_name": "symmetrized_csiszar_function", "original_string": "def symmetrized_csiszar_function(logu, csiszar_function, name=None):\n  \"\"\"Symmetrizes a Csiszar-function in log-space.\n\n  A Csiszar-function is a member of,\n\n  ```none\n  F = { f:R_+ to R : f convex }.\n  ```\n\n  The symmetrized Csiszar-function is defined as:\n\n  ```none\n  f_g(u) = 0.5 g(u) + 0.5 u g (1 / u)\n  ```\n\n  where `g` is some other Csiszar-function.\n\n  We say the function is \"symmetrized\" because:\n\n  ```none\n  D_{f_g}[p, q] = D_{f_g}[q, p]\n  ```\n\n  for all `p << >> q` (i.e., `support(p) = support(q)`).\n\n  There exists alternatives for symmetrizing a Csiszar-function. For example,\n\n  ```none\n  f_g(u) = max(f(u), f^*(u)),\n  ```\n\n  where `f^*` is the dual Csiszar-function, also implies a symmetric\n  f-Divergence.\n\n  Example:\n\n  When either of the following functions are symmetrized, we obtain the\n  Jensen-Shannon Csiszar-function, i.e.,\n\n  ```none\n  g(u) = -log(u) - (1 + u) log((1 + u) / 2) + u - 1\n  h(u) = log(4) + 2 u log(u / (1 + u))\n  ```\n\n  implies,\n\n  ```none\n  f_g(u) = f_h(u) = u log(u) - (1 + u) log((1 + u) / 2)\n         = jensen_shannon(log(u)).\n  ```\n\n  Warning: this function makes non-log-space calculations and may therefore be\n  numerically unstable for `|logu| >> 0`.\n\n  Args:\n    logu: `float`-like `Tensor` representing `log(u)` from above.\n    csiszar_function: Python `callable` representing a Csiszar-function over\n      log-domain.\n    name: Python `str` name prefixed to Ops created by this function.\n\n  Returns:\n    symmetrized_g_of_u: `float`-like `Tensor` of the result of applying the\n      symmetrization of `g` evaluated at `u = exp(logu)`.\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, \"symmetrized_csiszar_function\", [logu]):\n    logu = tf.convert_to_tensor(value=logu, name=\"logu\")\n    return 0.5 * (csiszar_function(logu)\n                  + dual_csiszar_function(logu, csiszar_function))", "language": "python", "code": "def symmetrized_csiszar_function(logu, csiszar_function, name=None):\n  \"\"\"Symmetrizes a Csiszar-function in log-space.\n\n  A Csiszar-function is a member of,\n\n  ```none\n  F = { f:R_+ to R : f convex }.\n  ```\n\n  The symmetrized Csiszar-function is defined as:\n\n  ```none\n  f_g(u) = 0.5 g(u) + 0.5 u g (1 / u)\n  ```\n\n  where `g` is some other Csiszar-function.\n\n  We say the function is \"symmetrized\" because:\n\n  ```none\n  D_{f_g}[p, q] = D_{f_g}[q, p]\n  ```\n\n  for all `p << >> q` (i.e., `support(p) = support(q)`).\n\n  There exists alternatives for symmetrizing a Csiszar-function. For example,\n\n  ```none\n  f_g(u) = max(f(u), f^*(u)),\n  ```\n\n  where `f^*` is the dual Csiszar-function, also implies a symmetric\n  f-Divergence.\n\n  Example:\n\n  When either of the following functions are symmetrized, we obtain the\n  Jensen-Shannon Csiszar-function, i.e.,\n\n  ```none\n  g(u) = -log(u) - (1 + u) log((1 + u) / 2) + u - 1\n  h(u) = log(4) + 2 u log(u / (1 + u))\n  ```\n\n  implies,\n\n  ```none\n  f_g(u) = f_h(u) = u log(u) - (1 + u) log((1 + u) / 2)\n         = jensen_shannon(log(u)).\n  ```\n\n  Warning: this function makes non-log-space calculations and may therefore be\n  numerically unstable for `|logu| >> 0`.\n\n  Args:\n    logu: `float`-like `Tensor` representing `log(u)` from above.\n    csiszar_function: Python `callable` representing a Csiszar-function over\n      log-domain.\n    name: Python `str` name prefixed to Ops created by this function.\n\n  Returns:\n    symmetrized_g_of_u: `float`-like `Tensor` of the result of applying the\n      symmetrization of `g` evaluated at `u = exp(logu)`.\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, \"symmetrized_csiszar_function\", [logu]):\n    logu = tf.convert_to_tensor(value=logu, name=\"logu\")\n    return 0.5 * (csiszar_function(logu)\n                  + dual_csiszar_function(logu, csiszar_function))", "code_tokens": ["def", "symmetrized_csiszar_function", "(", "logu", ",", "csiszar_function", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "\"symmetrized_csiszar_function\"", ",", "[", "logu", "]", ")", ":", "logu", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "logu", ",", "name", "=", "\"logu\"", ")", "return", "0.5", "*", "(", "csiszar_function", "(", "logu", ")", "+", "dual_csiszar_function", "(", "logu", ",", "csiszar_function", ")", ")"], "docstring": "Symmetrizes a Csiszar-function in log-space.\n\n  A Csiszar-function is a member of,\n\n  ```none\n  F = { f:R_+ to R : f convex }.\n  ```\n\n  The symmetrized Csiszar-function is defined as:\n\n  ```none\n  f_g(u) = 0.5 g(u) + 0.5 u g (1 / u)\n  ```\n\n  where `g` is some other Csiszar-function.\n\n  We say the function is \"symmetrized\" because:\n\n  ```none\n  D_{f_g}[p, q] = D_{f_g}[q, p]\n  ```\n\n  for all `p << >> q` (i.e., `support(p) = support(q)`).\n\n  There exists alternatives for symmetrizing a Csiszar-function. For example,\n\n  ```none\n  f_g(u) = max(f(u), f^*(u)),\n  ```\n\n  where `f^*` is the dual Csiszar-function, also implies a symmetric\n  f-Divergence.\n\n  Example:\n\n  When either of the following functions are symmetrized, we obtain the\n  Jensen-Shannon Csiszar-function, i.e.,\n\n  ```none\n  g(u) = -log(u) - (1 + u) log((1 + u) / 2) + u - 1\n  h(u) = log(4) + 2 u log(u / (1 + u))\n  ```\n\n  implies,\n\n  ```none\n  f_g(u) = f_h(u) = u log(u) - (1 + u) log((1 + u) / 2)\n         = jensen_shannon(log(u)).\n  ```\n\n  Warning: this function makes non-log-space calculations and may therefore be\n  numerically unstable for `|logu| >> 0`.\n\n  Args:\n    logu: `float`-like `Tensor` representing `log(u)` from above.\n    csiszar_function: Python `callable` representing a Csiszar-function over\n      log-domain.\n    name: Python `str` name prefixed to Ops created by this function.\n\n  Returns:\n    symmetrized_g_of_u: `float`-like `Tensor` of the result of applying the\n      symmetrization of `g` evaluated at `u = exp(logu)`.", "docstring_tokens": ["Symmetrizes", "a", "Csiszar", "-", "function", "in", "log", "-", "space", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/vi/csiszar_divergence.py#L712-L780", "partition": "test"}
{"repo": "ojarva/python-sshpubkeys", "path": "sshpubkeys/keys.py", "func_name": "SSHKey.hash_sha256", "original_string": "def hash_sha256(self):\n        \"\"\"Calculate sha256 fingerprint.\"\"\"\n        fp_plain = hashlib.sha256(self._decoded_key).digest()\n        return (b\"SHA256:\" + base64.b64encode(fp_plain).replace(b\"=\", b\"\")).decode(\"utf-8\")", "language": "python", "code": "def hash_sha256(self):\n        \"\"\"Calculate sha256 fingerprint.\"\"\"\n        fp_plain = hashlib.sha256(self._decoded_key).digest()\n        return (b\"SHA256:\" + base64.b64encode(fp_plain).replace(b\"=\", b\"\")).decode(\"utf-8\")", "code_tokens": ["def", "hash_sha256", "(", "self", ")", ":", "fp_plain", "=", "hashlib", ".", "sha256", "(", "self", ".", "_decoded_key", ")", ".", "digest", "(", ")", "return", "(", "b\"SHA256:\"", "+", "base64", ".", "b64encode", "(", "fp_plain", ")", ".", "replace", "(", "b\"=\"", ",", "b\"\"", ")", ")", ".", "decode", "(", "\"utf-8\"", ")"], "docstring": "Calculate sha256 fingerprint.", "docstring_tokens": ["Calculate", "sha256", "fingerprint", "."], "sha": "86dc1ab27ce82dcc091ce127416cc3ee219e9bec", "url": "https://github.com/ojarva/python-sshpubkeys/blob/86dc1ab27ce82dcc091ce127416cc3ee219e9bec/sshpubkeys/keys.py#L158-L161", "partition": "test"}
{"repo": "UDST/pandana", "path": "pandana/loaders/osm.py", "func_name": "make_osm_query", "original_string": "def make_osm_query(query):\n    \"\"\"\n    Make a request to OSM and return the parsed JSON.\n\n    Parameters\n    ----------\n    query : str\n        A string in the Overpass QL format.\n\n    Returns\n    -------\n    data : dict\n\n    \"\"\"\n    osm_url = 'http://www.overpass-api.de/api/interpreter'\n    req = requests.get(osm_url, params={'data': query})\n    req.raise_for_status()\n\n    return req.json()", "language": "python", "code": "def make_osm_query(query):\n    \"\"\"\n    Make a request to OSM and return the parsed JSON.\n\n    Parameters\n    ----------\n    query : str\n        A string in the Overpass QL format.\n\n    Returns\n    -------\n    data : dict\n\n    \"\"\"\n    osm_url = 'http://www.overpass-api.de/api/interpreter'\n    req = requests.get(osm_url, params={'data': query})\n    req.raise_for_status()\n\n    return req.json()", "code_tokens": ["def", "make_osm_query", "(", "query", ")", ":", "osm_url", "=", "'http://www.overpass-api.de/api/interpreter'", "req", "=", "requests", ".", "get", "(", "osm_url", ",", "params", "=", "{", "'data'", ":", "query", "}", ")", "req", ".", "raise_for_status", "(", ")", "return", "req", ".", "json", "(", ")"], "docstring": "Make a request to OSM and return the parsed JSON.\n\n    Parameters\n    ----------\n    query : str\n        A string in the Overpass QL format.\n\n    Returns\n    -------\n    data : dict", "docstring_tokens": ["Make", "a", "request", "to", "OSM", "and", "return", "the", "parsed", "JSON", "."], "sha": "961a7ef8d3b0144b190cb60bbd61845fca6fb314", "url": "https://github.com/UDST/pandana/blob/961a7ef8d3b0144b190cb60bbd61845fca6fb314/pandana/loaders/osm.py#L99-L117", "partition": "test"}
{"repo": "dbcli/cli_helpers", "path": "cli_helpers/config.py", "func_name": "Config.additional_files", "original_string": "def additional_files(self):\n        \"\"\"Get a list of absolute paths to the additional config files.\"\"\"\n        return [os.path.join(f, self.filename) for f in self.additional_dirs]", "language": "python", "code": "def additional_files(self):\n        \"\"\"Get a list of absolute paths to the additional config files.\"\"\"\n        return [os.path.join(f, self.filename) for f in self.additional_dirs]", "code_tokens": ["def", "additional_files", "(", "self", ")", ":", "return", "[", "os", ".", "path", ".", "join", "(", "f", ",", "self", ".", "filename", ")", "for", "f", "in", "self", ".", "additional_dirs", "]"], "docstring": "Get a list of absolute paths to the additional config files.", "docstring_tokens": ["Get", "a", "list", "of", "absolute", "paths", "to", "the", "additional", "config", "files", "."], "sha": "3ebd891ac0c02bad061182dbcb54a47fb21980ae", "url": "https://github.com/dbcli/cli_helpers/blob/3ebd891ac0c02bad061182dbcb54a47fb21980ae/cli_helpers/config.py#L124-L126", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/interactiveshell.py", "func_name": "InteractiveShell._showtraceback", "original_string": "def _showtraceback(self, etype, evalue, stb):\n        \"\"\"Actually show a traceback.\n\n        Subclasses may override this method to put the traceback on a different\n        place, like a side channel.\n        \"\"\"\n        print >> io.stdout, self.InteractiveTB.stb2text(stb)", "language": "python", "code": "def _showtraceback(self, etype, evalue, stb):\n        \"\"\"Actually show a traceback.\n\n        Subclasses may override this method to put the traceback on a different\n        place, like a side channel.\n        \"\"\"\n        print >> io.stdout, self.InteractiveTB.stb2text(stb)", "code_tokens": ["def", "_showtraceback", "(", "self", ",", "etype", ",", "evalue", ",", "stb", ")", ":", "print", ">>", "io", ".", "stdout", ",", "self", ".", "InteractiveTB", ".", "stb2text", "(", "stb", ")"], "docstring": "Actually show a traceback.\n\n        Subclasses may override this method to put the traceback on a different\n        place, like a side channel.", "docstring_tokens": ["Actually", "show", "a", "traceback", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/interactiveshell.py#L1732-L1738", "partition": "test"}
{"repo": "pytorch/vision", "path": "torchvision/datasets/utils.py", "func_name": "download_url", "original_string": "def download_url(url, root, filename=None, md5=None):\n    \"\"\"Download a file from a url and place it in root.\n\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    \"\"\"\n    from six.moves import urllib\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    # downloads file\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print('Using downloaded and verified file: ' + fpath)\n    else:\n        try:\n            print('Downloading ' + url + ' to ' + fpath)\n            urllib.request.urlretrieve(\n                url, fpath,\n                reporthook=gen_bar_updater()\n            )\n        except OSError:\n            if url[:5] == 'https':\n                url = url.replace('https:', 'http:')\n                print('Failed download. Trying https -> http instead.'\n                      ' Downloading ' + url + ' to ' + fpath)\n                urllib.request.urlretrieve(\n                    url, fpath,\n                    reporthook=gen_bar_updater()\n                )", "language": "python", "code": "def download_url(url, root, filename=None, md5=None):\n    \"\"\"Download a file from a url and place it in root.\n\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    \"\"\"\n    from six.moves import urllib\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    # downloads file\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print('Using downloaded and verified file: ' + fpath)\n    else:\n        try:\n            print('Downloading ' + url + ' to ' + fpath)\n            urllib.request.urlretrieve(\n                url, fpath,\n                reporthook=gen_bar_updater()\n            )\n        except OSError:\n            if url[:5] == 'https':\n                url = url.replace('https:', 'http:')\n                print('Failed download. Trying https -> http instead.'\n                      ' Downloading ' + url + ' to ' + fpath)\n                urllib.request.urlretrieve(\n                    url, fpath,\n                    reporthook=gen_bar_updater()\n                )", "code_tokens": ["def", "download_url", "(", "url", ",", "root", ",", "filename", "=", "None", ",", "md5", "=", "None", ")", ":", "from", "six", ".", "moves", "import", "urllib", "root", "=", "os", ".", "path", ".", "expanduser", "(", "root", ")", "if", "not", "filename", ":", "filename", "=", "os", ".", "path", ".", "basename", "(", "url", ")", "fpath", "=", "os", ".", "path", ".", "join", "(", "root", ",", "filename", ")", "makedir_exist_ok", "(", "root", ")", "# downloads file", "if", "os", ".", "path", ".", "isfile", "(", "fpath", ")", "and", "check_integrity", "(", "fpath", ",", "md5", ")", ":", "print", "(", "'Using downloaded and verified file: '", "+", "fpath", ")", "else", ":", "try", ":", "print", "(", "'Downloading '", "+", "url", "+", "' to '", "+", "fpath", ")", "urllib", ".", "request", ".", "urlretrieve", "(", "url", ",", "fpath", ",", "reporthook", "=", "gen_bar_updater", "(", ")", ")", "except", "OSError", ":", "if", "url", "[", ":", "5", "]", "==", "'https'", ":", "url", "=", "url", ".", "replace", "(", "'https:'", ",", "'http:'", ")", "print", "(", "'Failed download. Trying https -> http instead.'", "' Downloading '", "+", "url", "+", "' to '", "+", "fpath", ")", "urllib", ".", "request", ".", "urlretrieve", "(", "url", ",", "fpath", ",", "reporthook", "=", "gen_bar_updater", "(", ")", ")"], "docstring": "Download a file from a url and place it in root.\n\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n        md5 (str, optional): MD5 checksum of the download. If None, do not check", "docstring_tokens": ["Download", "a", "file", "from", "a", "url", "and", "place", "it", "in", "root", "."], "sha": "3afcf3cd49661c466c75ea536b0b2a7ff57f9a05", "url": "https://github.com/pytorch/vision/blob/3afcf3cd49661c466c75ea536b0b2a7ff57f9a05/torchvision/datasets/utils.py#L54-L90", "partition": "test"}
{"repo": "bakwc/PySyncObj", "path": "pysyncobj/transport.py", "func_name": "TCPTransport._onOutgoingMessageReceived", "original_string": "def _onOutgoingMessageReceived(self, conn, message):\n        \"\"\"\n        Callback for receiving a message on a new outgoing connection. Used only if encryption is enabled to exchange the random keys.\n        Once the key exchange is done, this triggers the onNodeConnected callback, and further messages are deferred to the onMessageReceived callback.\n\n        :param conn: connection object\n        :type conn: TcpConnection\n        :param message: received message\n        :type message: any\n        \"\"\"\n\n        if not conn.sendRandKey:\n            conn.sendRandKey = message\n            conn.send(self._selfNode.address)\n\n        node = self._connToNode(conn)\n        conn.setOnMessageReceivedCallback(functools.partial(self._onMessageReceived, node))\n        self._onNodeConnected(node)", "language": "python", "code": "def _onOutgoingMessageReceived(self, conn, message):\n        \"\"\"\n        Callback for receiving a message on a new outgoing connection. Used only if encryption is enabled to exchange the random keys.\n        Once the key exchange is done, this triggers the onNodeConnected callback, and further messages are deferred to the onMessageReceived callback.\n\n        :param conn: connection object\n        :type conn: TcpConnection\n        :param message: received message\n        :type message: any\n        \"\"\"\n\n        if not conn.sendRandKey:\n            conn.sendRandKey = message\n            conn.send(self._selfNode.address)\n\n        node = self._connToNode(conn)\n        conn.setOnMessageReceivedCallback(functools.partial(self._onMessageReceived, node))\n        self._onNodeConnected(node)", "code_tokens": ["def", "_onOutgoingMessageReceived", "(", "self", ",", "conn", ",", "message", ")", ":", "if", "not", "conn", ".", "sendRandKey", ":", "conn", ".", "sendRandKey", "=", "message", "conn", ".", "send", "(", "self", ".", "_selfNode", ".", "address", ")", "node", "=", "self", ".", "_connToNode", "(", "conn", ")", "conn", ".", "setOnMessageReceivedCallback", "(", "functools", ".", "partial", "(", "self", ".", "_onMessageReceived", ",", "node", ")", ")", "self", ".", "_onNodeConnected", "(", "node", ")"], "docstring": "Callback for receiving a message on a new outgoing connection. Used only if encryption is enabled to exchange the random keys.\n        Once the key exchange is done, this triggers the onNodeConnected callback, and further messages are deferred to the onMessageReceived callback.\n\n        :param conn: connection object\n        :type conn: TcpConnection\n        :param message: received message\n        :type message: any", "docstring_tokens": ["Callback", "for", "receiving", "a", "message", "on", "a", "new", "outgoing", "connection", ".", "Used", "only", "if", "encryption", "is", "enabled", "to", "exchange", "the", "random", "keys", ".", "Once", "the", "key", "exchange", "is", "done", "this", "triggers", "the", "onNodeConnected", "callback", "and", "further", "messages", "are", "deferred", "to", "the", "onMessageReceived", "callback", "."], "sha": "be3b0aaa932d5156f5df140c23c962430f51b7b8", "url": "https://github.com/bakwc/PySyncObj/blob/be3b0aaa932d5156f5df140c23c962430f51b7b8/pysyncobj/transport.py#L441-L458", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/rich_ipython_widget.py", "func_name": "RichIPythonWidget._append_jpg", "original_string": "def _append_jpg(self, jpg, before_prompt=False):\n        \"\"\" Append raw JPG data to the widget.\"\"\"\n        self._append_custom(self._insert_jpg, jpg, before_prompt)", "language": "python", "code": "def _append_jpg(self, jpg, before_prompt=False):\n        \"\"\" Append raw JPG data to the widget.\"\"\"\n        self._append_custom(self._insert_jpg, jpg, before_prompt)", "code_tokens": ["def", "_append_jpg", "(", "self", ",", "jpg", ",", "before_prompt", "=", "False", ")", ":", "self", ".", "_append_custom", "(", "self", ".", "_insert_jpg", ",", "jpg", ",", "before_prompt", ")"], "docstring": "Append raw JPG data to the widget.", "docstring_tokens": ["Append", "raw", "JPG", "data", "to", "the", "widget", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/rich_ipython_widget.py#L167-L169", "partition": "test"}
{"repo": "elliterate/capybara.py", "path": "capybara/selector/selector.py", "func_name": "SelectorFactory.expression_filter", "original_string": "def expression_filter(self, name, **kwargs):\n        \"\"\"\n        Returns a decorator function for adding an expression filter.\n\n        Args:\n            name (str): The name of the filter.\n            **kwargs: Variable keyword arguments for the filter.\n\n        Returns:\n            Callable[[Callable[[AbstractExpression, Any], AbstractExpression]]]: A decorator\n                function for adding an expression filter.\n        \"\"\"\n\n        def decorator(func):\n            self.filters[name] = ExpressionFilter(name, func, **kwargs)\n\n        return decorator", "language": "python", "code": "def expression_filter(self, name, **kwargs):\n        \"\"\"\n        Returns a decorator function for adding an expression filter.\n\n        Args:\n            name (str): The name of the filter.\n            **kwargs: Variable keyword arguments for the filter.\n\n        Returns:\n            Callable[[Callable[[AbstractExpression, Any], AbstractExpression]]]: A decorator\n                function for adding an expression filter.\n        \"\"\"\n\n        def decorator(func):\n            self.filters[name] = ExpressionFilter(name, func, **kwargs)\n\n        return decorator", "code_tokens": ["def", "expression_filter", "(", "self", ",", "name", ",", "*", "*", "kwargs", ")", ":", "def", "decorator", "(", "func", ")", ":", "self", ".", "filters", "[", "name", "]", "=", "ExpressionFilter", "(", "name", ",", "func", ",", "*", "*", "kwargs", ")", "return", "decorator"], "docstring": "Returns a decorator function for adding an expression filter.\n\n        Args:\n            name (str): The name of the filter.\n            **kwargs: Variable keyword arguments for the filter.\n\n        Returns:\n            Callable[[Callable[[AbstractExpression, Any], AbstractExpression]]]: A decorator\n                function for adding an expression filter.", "docstring_tokens": ["Returns", "a", "decorator", "function", "for", "adding", "an", "expression", "filter", "."], "sha": "0c6ae449cc37e4445ec3cd6af95674533beedc6c", "url": "https://github.com/elliterate/capybara.py/blob/0c6ae449cc37e4445ec3cd6af95674533beedc6c/capybara/selector/selector.py#L123-L139", "partition": "test"}
{"repo": "respondcreate/django-versatileimagefield", "path": "versatileimagefield/fields.py", "func_name": "VersatileImageField.process_placeholder_image", "original_string": "def process_placeholder_image(self):\n        \"\"\"\n        Process the field's placeholder image.\n\n        Ensures the placeholder image has been saved to the same storage class\n        as the field in a top level folder with a name specified by\n        settings.VERSATILEIMAGEFIELD_SETTINGS['placeholder_directory_name']\n\n        This should be called by the VersatileImageFileDescriptor __get__.\n        If self.placeholder_image_name is already set it just returns right away.\n        \"\"\"\n        if self.placeholder_image_name:\n            return\n\n        placeholder_image_name = None\n        placeholder_image = self.placeholder_image\n        if placeholder_image:\n            if isinstance(placeholder_image, OnStoragePlaceholderImage):\n                name = placeholder_image.path\n            else:\n                name = placeholder_image.image_data.name\n            placeholder_image_name = os.path.join(\n                VERSATILEIMAGEFIELD_PLACEHOLDER_DIRNAME, name\n            )\n            if not self.storage.exists(placeholder_image_name):\n                self.storage.save(\n                    placeholder_image_name,\n                    placeholder_image.image_data\n                )\n        self.placeholder_image_name = placeholder_image_name", "language": "python", "code": "def process_placeholder_image(self):\n        \"\"\"\n        Process the field's placeholder image.\n\n        Ensures the placeholder image has been saved to the same storage class\n        as the field in a top level folder with a name specified by\n        settings.VERSATILEIMAGEFIELD_SETTINGS['placeholder_directory_name']\n\n        This should be called by the VersatileImageFileDescriptor __get__.\n        If self.placeholder_image_name is already set it just returns right away.\n        \"\"\"\n        if self.placeholder_image_name:\n            return\n\n        placeholder_image_name = None\n        placeholder_image = self.placeholder_image\n        if placeholder_image:\n            if isinstance(placeholder_image, OnStoragePlaceholderImage):\n                name = placeholder_image.path\n            else:\n                name = placeholder_image.image_data.name\n            placeholder_image_name = os.path.join(\n                VERSATILEIMAGEFIELD_PLACEHOLDER_DIRNAME, name\n            )\n            if not self.storage.exists(placeholder_image_name):\n                self.storage.save(\n                    placeholder_image_name,\n                    placeholder_image.image_data\n                )\n        self.placeholder_image_name = placeholder_image_name", "code_tokens": ["def", "process_placeholder_image", "(", "self", ")", ":", "if", "self", ".", "placeholder_image_name", ":", "return", "placeholder_image_name", "=", "None", "placeholder_image", "=", "self", ".", "placeholder_image", "if", "placeholder_image", ":", "if", "isinstance", "(", "placeholder_image", ",", "OnStoragePlaceholderImage", ")", ":", "name", "=", "placeholder_image", ".", "path", "else", ":", "name", "=", "placeholder_image", ".", "image_data", ".", "name", "placeholder_image_name", "=", "os", ".", "path", ".", "join", "(", "VERSATILEIMAGEFIELD_PLACEHOLDER_DIRNAME", ",", "name", ")", "if", "not", "self", ".", "storage", ".", "exists", "(", "placeholder_image_name", ")", ":", "self", ".", "storage", ".", "save", "(", "placeholder_image_name", ",", "placeholder_image", ".", "image_data", ")", "self", ".", "placeholder_image_name", "=", "placeholder_image_name"], "docstring": "Process the field's placeholder image.\n\n        Ensures the placeholder image has been saved to the same storage class\n        as the field in a top level folder with a name specified by\n        settings.VERSATILEIMAGEFIELD_SETTINGS['placeholder_directory_name']\n\n        This should be called by the VersatileImageFileDescriptor __get__.\n        If self.placeholder_image_name is already set it just returns right away.", "docstring_tokens": ["Process", "the", "field", "s", "placeholder", "image", "."], "sha": "d41e279c39cccffafbe876c67596184704ae8877", "url": "https://github.com/respondcreate/django-versatileimagefield/blob/d41e279c39cccffafbe876c67596184704ae8877/versatileimagefield/fields.py#L50-L79", "partition": "test"}
{"repo": "ekzhu/datasketch", "path": "datasketch/hyperloglog.py", "func_name": "HyperLogLog.clear", "original_string": "def clear(self):\n        '''\n        Reset the current HyperLogLog to empty.\n        '''\n        self.reg = np.zeros((self.m,), dtype=np.int8)", "language": "python", "code": "def clear(self):\n        '''\n        Reset the current HyperLogLog to empty.\n        '''\n        self.reg = np.zeros((self.m,), dtype=np.int8)", "code_tokens": ["def", "clear", "(", "self", ")", ":", "self", ".", "reg", "=", "np", ".", "zeros", "(", "(", "self", ".", "m", ",", ")", ",", "dtype", "=", "np", ".", "int8", ")"], "docstring": "Reset the current HyperLogLog to empty.", "docstring_tokens": ["Reset", "the", "current", "HyperLogLog", "to", "empty", "."], "sha": "b3e4129987890a2beb04f2c0b6dc618ae35f2e14", "url": "https://github.com/ekzhu/datasketch/blob/b3e4129987890a2beb04f2c0b6dc618ae35f2e14/datasketch/hyperloglog.py#L184-L188", "partition": "test"}
{"repo": "deepmipt/DeepPavlov", "path": "deeppavlov/core/layers/tf_layers.py", "func_name": "cudnn_compatible_lstm", "original_string": "def cudnn_compatible_lstm(units, n_hidden, n_layers=1, trainable_initial_states=None, seq_lengths=None, initial_h=None,\n                          initial_c=None, name='cudnn_lstm', reuse=False):\n    \"\"\" CuDNN Compatible LSTM implementation.\n        It should be used to load models saved with CudnnLSTMCell to run on CPU.\n\n        Args:\n            units: tf.Tensor with dimensions [B x T x F], where\n                B - batch size\n                T - number of tokens\n                F - features\n            n_hidden: dimensionality of hidden state\n            n_layers: number of layers\n            trainable_initial_states: whether to create a special trainable variable\n                to initialize the hidden states of the network or use just zeros\n            seq_lengths: tensor of sequence lengths with dimension [B]\n            initial_h: optional initial hidden state, masks trainable_initial_states\n                if provided\n            initial_c: optional initial cell state, masks trainable_initial_states\n                if provided\n            name: name of the variable scope to use\n            reuse:whether to reuse already initialized variable\n\n\n        Returns:\n            h - all hidden states along T dimension,\n                tf.Tensor with dimensionality [B x T x F]\n            h_last - last hidden state, tf.Tensor with dimensionality [B x H]\n                where H - number of hidden units\n            c_last - last cell state, tf.Tensor with dimensionality [B x H]\n                where H - number of hidden units\n        \"\"\"\n\n    with tf.variable_scope(name, reuse=reuse):\n        if trainable_initial_states:\n            init_h = tf.get_variable('init_h', [n_layers, 1, n_hidden])\n            init_h = tf.tile(init_h, (1, tf.shape(units)[0], 1))\n            init_c = tf.get_variable('init_c', [n_layers, 1, n_hidden])\n            init_c = tf.tile(init_c, (1, tf.shape(units)[0], 1))\n        else:\n            init_h = init_c = tf.zeros([n_layers, tf.shape(units)[0], n_hidden])\n\n        initial_h = initial_h or init_h\n        initial_c = initial_c or init_c\n\n        with tf.variable_scope('cudnn_lstm', reuse=reuse):\n            def single_cell(): return tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(n_hidden)\n\n            cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(n_layers)])\n\n            units = tf.transpose(units, (1, 0, 2))\n\n            init = tuple([tf.nn.rnn_cell.LSTMStateTuple(ic, ih) for ih, ic in\n                          zip(tf.unstack(initial_h, axis=0), tf.unstack(initial_c, axis=0))])\n\n            h, state = tf.nn.dynamic_rnn(cell=cell, inputs=units, time_major=True, initial_state=init)\n\n            h = tf.transpose(h, (1, 0, 2))\n            h_last = state[-1].h\n            c_last = state[-1].c\n\n            # Extract last states if they are provided\n            if seq_lengths is not None:\n                indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths-1], axis=1)\n                h_last = tf.gather_nd(h, indices)\n\n            return h, (h_last, c_last)", "language": "python", "code": "def cudnn_compatible_lstm(units, n_hidden, n_layers=1, trainable_initial_states=None, seq_lengths=None, initial_h=None,\n                          initial_c=None, name='cudnn_lstm', reuse=False):\n    \"\"\" CuDNN Compatible LSTM implementation.\n        It should be used to load models saved with CudnnLSTMCell to run on CPU.\n\n        Args:\n            units: tf.Tensor with dimensions [B x T x F], where\n                B - batch size\n                T - number of tokens\n                F - features\n            n_hidden: dimensionality of hidden state\n            n_layers: number of layers\n            trainable_initial_states: whether to create a special trainable variable\n                to initialize the hidden states of the network or use just zeros\n            seq_lengths: tensor of sequence lengths with dimension [B]\n            initial_h: optional initial hidden state, masks trainable_initial_states\n                if provided\n            initial_c: optional initial cell state, masks trainable_initial_states\n                if provided\n            name: name of the variable scope to use\n            reuse:whether to reuse already initialized variable\n\n\n        Returns:\n            h - all hidden states along T dimension,\n                tf.Tensor with dimensionality [B x T x F]\n            h_last - last hidden state, tf.Tensor with dimensionality [B x H]\n                where H - number of hidden units\n            c_last - last cell state, tf.Tensor with dimensionality [B x H]\n                where H - number of hidden units\n        \"\"\"\n\n    with tf.variable_scope(name, reuse=reuse):\n        if trainable_initial_states:\n            init_h = tf.get_variable('init_h', [n_layers, 1, n_hidden])\n            init_h = tf.tile(init_h, (1, tf.shape(units)[0], 1))\n            init_c = tf.get_variable('init_c', [n_layers, 1, n_hidden])\n            init_c = tf.tile(init_c, (1, tf.shape(units)[0], 1))\n        else:\n            init_h = init_c = tf.zeros([n_layers, tf.shape(units)[0], n_hidden])\n\n        initial_h = initial_h or init_h\n        initial_c = initial_c or init_c\n\n        with tf.variable_scope('cudnn_lstm', reuse=reuse):\n            def single_cell(): return tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(n_hidden)\n\n            cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(n_layers)])\n\n            units = tf.transpose(units, (1, 0, 2))\n\n            init = tuple([tf.nn.rnn_cell.LSTMStateTuple(ic, ih) for ih, ic in\n                          zip(tf.unstack(initial_h, axis=0), tf.unstack(initial_c, axis=0))])\n\n            h, state = tf.nn.dynamic_rnn(cell=cell, inputs=units, time_major=True, initial_state=init)\n\n            h = tf.transpose(h, (1, 0, 2))\n            h_last = state[-1].h\n            c_last = state[-1].c\n\n            # Extract last states if they are provided\n            if seq_lengths is not None:\n                indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths-1], axis=1)\n                h_last = tf.gather_nd(h, indices)\n\n            return h, (h_last, c_last)", "code_tokens": ["def", "cudnn_compatible_lstm", "(", "units", ",", "n_hidden", ",", "n_layers", "=", "1", ",", "trainable_initial_states", "=", "None", ",", "seq_lengths", "=", "None", ",", "initial_h", "=", "None", ",", "initial_c", "=", "None", ",", "name", "=", "'cudnn_lstm'", ",", "reuse", "=", "False", ")", ":", "with", "tf", ".", "variable_scope", "(", "name", ",", "reuse", "=", "reuse", ")", ":", "if", "trainable_initial_states", ":", "init_h", "=", "tf", ".", "get_variable", "(", "'init_h'", ",", "[", "n_layers", ",", "1", ",", "n_hidden", "]", ")", "init_h", "=", "tf", ".", "tile", "(", "init_h", ",", "(", "1", ",", "tf", ".", "shape", "(", "units", ")", "[", "0", "]", ",", "1", ")", ")", "init_c", "=", "tf", ".", "get_variable", "(", "'init_c'", ",", "[", "n_layers", ",", "1", ",", "n_hidden", "]", ")", "init_c", "=", "tf", ".", "tile", "(", "init_c", ",", "(", "1", ",", "tf", ".", "shape", "(", "units", ")", "[", "0", "]", ",", "1", ")", ")", "else", ":", "init_h", "=", "init_c", "=", "tf", ".", "zeros", "(", "[", "n_layers", ",", "tf", ".", "shape", "(", "units", ")", "[", "0", "]", ",", "n_hidden", "]", ")", "initial_h", "=", "initial_h", "or", "init_h", "initial_c", "=", "initial_c", "or", "init_c", "with", "tf", ".", "variable_scope", "(", "'cudnn_lstm'", ",", "reuse", "=", "reuse", ")", ":", "def", "single_cell", "(", ")", ":", "return", "tf", ".", "contrib", ".", "cudnn_rnn", ".", "CudnnCompatibleLSTMCell", "(", "n_hidden", ")", "cell", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "MultiRNNCell", "(", "[", "single_cell", "(", ")", "for", "_", "in", "range", "(", "n_layers", ")", "]", ")", "units", "=", "tf", ".", "transpose", "(", "units", ",", "(", "1", ",", "0", ",", "2", ")", ")", "init", "=", "tuple", "(", "[", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMStateTuple", "(", "ic", ",", "ih", ")", "for", "ih", ",", "ic", "in", "zip", "(", "tf", ".", "unstack", "(", "initial_h", ",", "axis", "=", "0", ")", ",", "tf", ".", "unstack", "(", "initial_c", ",", "axis", "=", "0", ")", ")", "]", ")", "h", ",", "state", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell", "=", "cell", ",", "inputs", "=", "units", ",", "time_major", "=", "True", ",", "initial_state", "=", "init", ")", "h", "=", "tf", ".", "transpose", "(", "h", ",", "(", "1", ",", "0", ",", "2", ")", ")", "h_last", "=", "state", "[", "-", "1", "]", ".", "h", "c_last", "=", "state", "[", "-", "1", "]", ".", "c", "# Extract last states if they are provided", "if", "seq_lengths", "is", "not", "None", ":", "indices", "=", "tf", ".", "stack", "(", "[", "tf", ".", "range", "(", "tf", ".", "shape", "(", "h", ")", "[", "0", "]", ")", ",", "seq_lengths", "-", "1", "]", ",", "axis", "=", "1", ")", "h_last", "=", "tf", ".", "gather_nd", "(", "h", ",", "indices", ")", "return", "h", ",", "(", "h_last", ",", "c_last", ")"], "docstring": "CuDNN Compatible LSTM implementation.\n        It should be used to load models saved with CudnnLSTMCell to run on CPU.\n\n        Args:\n            units: tf.Tensor with dimensions [B x T x F], where\n                B - batch size\n                T - number of tokens\n                F - features\n            n_hidden: dimensionality of hidden state\n            n_layers: number of layers\n            trainable_initial_states: whether to create a special trainable variable\n                to initialize the hidden states of the network or use just zeros\n            seq_lengths: tensor of sequence lengths with dimension [B]\n            initial_h: optional initial hidden state, masks trainable_initial_states\n                if provided\n            initial_c: optional initial cell state, masks trainable_initial_states\n                if provided\n            name: name of the variable scope to use\n            reuse:whether to reuse already initialized variable\n\n\n        Returns:\n            h - all hidden states along T dimension,\n                tf.Tensor with dimensionality [B x T x F]\n            h_last - last hidden state, tf.Tensor with dimensionality [B x H]\n                where H - number of hidden units\n            c_last - last cell state, tf.Tensor with dimensionality [B x H]\n                where H - number of hidden units", "docstring_tokens": ["CuDNN", "Compatible", "LSTM", "implementation", ".", "It", "should", "be", "used", "to", "load", "models", "saved", "with", "CudnnLSTMCell", "to", "run", "on", "CPU", "."], "sha": "f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c", "url": "https://github.com/deepmipt/DeepPavlov/blob/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c/deeppavlov/core/layers/tf_layers.py#L681-L746", "partition": "test"}
{"repo": "juga0/dhcpcanon", "path": "dhcpcanon/timers.py", "func_name": "gen_renewing_time", "original_string": "def gen_renewing_time(lease_time, elapsed=0):\n    \"\"\"Generate RENEWING time.\n\n    [:rfc:`2131#section-4.4.5`]::\n\n        T1\n        defaults to (0.5 * duration_of_lease).  T2 defaults to (0.875 *\n        duration_of_lease).  Times T1 and T2 SHOULD be chosen with some\n        random \"fuzz\" around a fixed value, to avoid synchronization of\n        client reacquisition.\n\n    \"\"\"\n    renewing_time = int(lease_time) * RENEW_PERC - elapsed\n    # FIXME:80 [:rfc:`2131#section-4.4.5`]: the chosen \"fuzz\" could fingerprint\n    # the implementation\n    # NOTE: here using same \"fuzz\" as systemd?\n    range_fuzz = int(lease_time) * REBIND_PERC - renewing_time\n    logger.debug('rebinding fuzz range %s', range_fuzz)\n    fuzz = random.uniform(-(range_fuzz),\n                          +(range_fuzz))\n    renewing_time += fuzz\n    logger.debug('Renewing time %s.', renewing_time)\n    return renewing_time", "language": "python", "code": "def gen_renewing_time(lease_time, elapsed=0):\n    \"\"\"Generate RENEWING time.\n\n    [:rfc:`2131#section-4.4.5`]::\n\n        T1\n        defaults to (0.5 * duration_of_lease).  T2 defaults to (0.875 *\n        duration_of_lease).  Times T1 and T2 SHOULD be chosen with some\n        random \"fuzz\" around a fixed value, to avoid synchronization of\n        client reacquisition.\n\n    \"\"\"\n    renewing_time = int(lease_time) * RENEW_PERC - elapsed\n    # FIXME:80 [:rfc:`2131#section-4.4.5`]: the chosen \"fuzz\" could fingerprint\n    # the implementation\n    # NOTE: here using same \"fuzz\" as systemd?\n    range_fuzz = int(lease_time) * REBIND_PERC - renewing_time\n    logger.debug('rebinding fuzz range %s', range_fuzz)\n    fuzz = random.uniform(-(range_fuzz),\n                          +(range_fuzz))\n    renewing_time += fuzz\n    logger.debug('Renewing time %s.', renewing_time)\n    return renewing_time", "code_tokens": ["def", "gen_renewing_time", "(", "lease_time", ",", "elapsed", "=", "0", ")", ":", "renewing_time", "=", "int", "(", "lease_time", ")", "*", "RENEW_PERC", "-", "elapsed", "# FIXME:80 [:rfc:`2131#section-4.4.5`]: the chosen \"fuzz\" could fingerprint", "# the implementation", "# NOTE: here using same \"fuzz\" as systemd?", "range_fuzz", "=", "int", "(", "lease_time", ")", "*", "REBIND_PERC", "-", "renewing_time", "logger", ".", "debug", "(", "'rebinding fuzz range %s'", ",", "range_fuzz", ")", "fuzz", "=", "random", ".", "uniform", "(", "-", "(", "range_fuzz", ")", ",", "+", "(", "range_fuzz", ")", ")", "renewing_time", "+=", "fuzz", "logger", ".", "debug", "(", "'Renewing time %s.'", ",", "renewing_time", ")", "return", "renewing_time"], "docstring": "Generate RENEWING time.\n\n    [:rfc:`2131#section-4.4.5`]::\n\n        T1\n        defaults to (0.5 * duration_of_lease).  T2 defaults to (0.875 *\n        duration_of_lease).  Times T1 and T2 SHOULD be chosen with some\n        random \"fuzz\" around a fixed value, to avoid synchronization of\n        client reacquisition.", "docstring_tokens": ["Generate", "RENEWING", "time", "."], "sha": "9f51a29e57fe93dc93fb22bb0ed12fcfe9557e59", "url": "https://github.com/juga0/dhcpcanon/blob/9f51a29e57fe93dc93fb22bb0ed12fcfe9557e59/dhcpcanon/timers.py#L110-L132", "partition": "test"}
{"repo": "kgiusti/pyngus", "path": "examples/recv.py", "func_name": "ReceiverEventHandler.receiver_remote_closed", "original_string": "def receiver_remote_closed(self, receiver_link, pn_condition):\n        \"\"\"Peer has closed its end of the link.\"\"\"\n        LOG.debug(\"receiver_remote_closed condition=%s\", pn_condition)\n        receiver_link.close()\n        self.done = True", "language": "python", "code": "def receiver_remote_closed(self, receiver_link, pn_condition):\n        \"\"\"Peer has closed its end of the link.\"\"\"\n        LOG.debug(\"receiver_remote_closed condition=%s\", pn_condition)\n        receiver_link.close()\n        self.done = True", "code_tokens": ["def", "receiver_remote_closed", "(", "self", ",", "receiver_link", ",", "pn_condition", ")", ":", "LOG", ".", "debug", "(", "\"receiver_remote_closed condition=%s\"", ",", "pn_condition", ")", "receiver_link", ".", "close", "(", ")", "self", ".", "done", "=", "True"], "docstring": "Peer has closed its end of the link.", "docstring_tokens": ["Peer", "has", "closed", "its", "end", "of", "the", "link", "."], "sha": "5392392046989f1bb84ba938c30e4d48311075f1", "url": "https://github.com/kgiusti/pyngus/blob/5392392046989f1bb84ba938c30e4d48311075f1/examples/recv.py#L54-L58", "partition": "test"}
{"repo": "jazzband/django-ddp", "path": "dddp/main.py", "func_name": "ddpp_sockjs_info", "original_string": "def ddpp_sockjs_info(environ, start_response):\n    \"\"\"Inform client that WebSocket service is available.\"\"\"\n    import random\n    import ejson\n\n    start_response(\n        '200 OK',\n        [\n            ('Content-Type', 'application/json; charset=UTF-8'),\n        ] + common_headers(environ),\n    )\n    yield ejson.dumps(collections.OrderedDict([\n        ('websocket', True),\n        ('origins', [\n            '*:*',\n        ]),\n        ('cookie_needed', False),\n        ('entropy', random.getrandbits(32)),\n    ]))", "language": "python", "code": "def ddpp_sockjs_info(environ, start_response):\n    \"\"\"Inform client that WebSocket service is available.\"\"\"\n    import random\n    import ejson\n\n    start_response(\n        '200 OK',\n        [\n            ('Content-Type', 'application/json; charset=UTF-8'),\n        ] + common_headers(environ),\n    )\n    yield ejson.dumps(collections.OrderedDict([\n        ('websocket', True),\n        ('origins', [\n            '*:*',\n        ]),\n        ('cookie_needed', False),\n        ('entropy', random.getrandbits(32)),\n    ]))", "code_tokens": ["def", "ddpp_sockjs_info", "(", "environ", ",", "start_response", ")", ":", "import", "random", "import", "ejson", "start_response", "(", "'200 OK'", ",", "[", "(", "'Content-Type'", ",", "'application/json; charset=UTF-8'", ")", ",", "]", "+", "common_headers", "(", "environ", ")", ",", ")", "yield", "ejson", ".", "dumps", "(", "collections", ".", "OrderedDict", "(", "[", "(", "'websocket'", ",", "True", ")", ",", "(", "'origins'", ",", "[", "'*:*'", ",", "]", ")", ",", "(", "'cookie_needed'", ",", "False", ")", ",", "(", "'entropy'", ",", "random", ".", "getrandbits", "(", "32", ")", ")", ",", "]", ")", ")"], "docstring": "Inform client that WebSocket service is available.", "docstring_tokens": ["Inform", "client", "that", "WebSocket", "service", "is", "available", "."], "sha": "1e1954b06fe140346acea43582515991685e4e01", "url": "https://github.com/jazzband/django-ddp/blob/1e1954b06fe140346acea43582515991685e4e01/dddp/main.py#L48-L66", "partition": "test"}
{"repo": "mseclab/PyJFuzz", "path": "pyjfuzz/core/pjf_configuration.py", "func_name": "PJFConfiguration.start", "original_string": "def start(self):\n        \"\"\"\n        Parse the command line and start PyJFuzz\n        \"\"\"\n        from .pjf_worker import PJFWorker\n        worker = PJFWorker(self)\n        if self.update_pjf:\n            worker.update_library()\n        elif self.browser_auto:\n            worker.browser_autopwn()\n        elif self.fuzz_web:\n            worker.web_fuzzer()\n        elif self.json:\n            if not self.web_server and not self.ext_fuzz and not self.cmd_fuzz:\n                worker.fuzz()\n            elif self.ext_fuzz:\n                if self.stdin:\n                    worker.fuzz_stdin()\n                else:\n                    worker.fuzz_command_line()\n            elif self.cmd_fuzz:\n                if self.stdin:\n                    worker.fuzz_external(True)\n                else:\n                    worker.fuzz_external()\n            else:\n                worker.start_http_server()\n        elif self.json_file:\n            worker.start_file_fuzz()\n        elif self.process_to_monitor:\n            worker.start_process_monitor()", "language": "python", "code": "def start(self):\n        \"\"\"\n        Parse the command line and start PyJFuzz\n        \"\"\"\n        from .pjf_worker import PJFWorker\n        worker = PJFWorker(self)\n        if self.update_pjf:\n            worker.update_library()\n        elif self.browser_auto:\n            worker.browser_autopwn()\n        elif self.fuzz_web:\n            worker.web_fuzzer()\n        elif self.json:\n            if not self.web_server and not self.ext_fuzz and not self.cmd_fuzz:\n                worker.fuzz()\n            elif self.ext_fuzz:\n                if self.stdin:\n                    worker.fuzz_stdin()\n                else:\n                    worker.fuzz_command_line()\n            elif self.cmd_fuzz:\n                if self.stdin:\n                    worker.fuzz_external(True)\n                else:\n                    worker.fuzz_external()\n            else:\n                worker.start_http_server()\n        elif self.json_file:\n            worker.start_file_fuzz()\n        elif self.process_to_monitor:\n            worker.start_process_monitor()", "code_tokens": ["def", "start", "(", "self", ")", ":", "from", ".", "pjf_worker", "import", "PJFWorker", "worker", "=", "PJFWorker", "(", "self", ")", "if", "self", ".", "update_pjf", ":", "worker", ".", "update_library", "(", ")", "elif", "self", ".", "browser_auto", ":", "worker", ".", "browser_autopwn", "(", ")", "elif", "self", ".", "fuzz_web", ":", "worker", ".", "web_fuzzer", "(", ")", "elif", "self", ".", "json", ":", "if", "not", "self", ".", "web_server", "and", "not", "self", ".", "ext_fuzz", "and", "not", "self", ".", "cmd_fuzz", ":", "worker", ".", "fuzz", "(", ")", "elif", "self", ".", "ext_fuzz", ":", "if", "self", ".", "stdin", ":", "worker", ".", "fuzz_stdin", "(", ")", "else", ":", "worker", ".", "fuzz_command_line", "(", ")", "elif", "self", ".", "cmd_fuzz", ":", "if", "self", ".", "stdin", ":", "worker", ".", "fuzz_external", "(", "True", ")", "else", ":", "worker", ".", "fuzz_external", "(", ")", "else", ":", "worker", ".", "start_http_server", "(", ")", "elif", "self", ".", "json_file", ":", "worker", ".", "start_file_fuzz", "(", ")", "elif", "self", ".", "process_to_monitor", ":", "worker", ".", "start_process_monitor", "(", ")"], "docstring": "Parse the command line and start PyJFuzz", "docstring_tokens": ["Parse", "the", "command", "line", "and", "start", "PyJFuzz"], "sha": "f777067076f62c9ab74ffea6e90fd54402b7a1b4", "url": "https://github.com/mseclab/PyJFuzz/blob/f777067076f62c9ab74ffea6e90fd54402b7a1b4/pyjfuzz/core/pjf_configuration.py#L125-L155", "partition": "test"}
{"repo": "xtuml/pyxtuml", "path": "xtuml/persist.py", "func_name": "serialize_value", "original_string": "def serialize_value(value, ty):\n    '''\n    Serialize a value from an xtuml metamodel instance.\n    '''\n    ty = ty.upper()\n    \n    null_value = {\n        'BOOLEAN'   : False,\n        'INTEGER'   : 0,\n        'REAL'      : 0.0,\n        'STRING'    : '',\n        'UNIQUE_ID' : 0\n    }\n    \n    transfer_fn = {\n        'BOOLEAN'     : lambda v: '%d' % int(v),\n        'INTEGER'     : lambda v: '%d' % v,\n        'REAL'        : lambda v: '%f' % v,\n        'STRING'      : lambda v: \"'%s'\" % v.replace(\"'\", \"''\"),\n        'UNIQUE_ID'   : lambda v: '\"%s\"' % uuid.UUID(int=v)\n    }\n\n    if value is None:\n        value = null_value[ty]\n    \n    return transfer_fn[ty](value)", "language": "python", "code": "def serialize_value(value, ty):\n    '''\n    Serialize a value from an xtuml metamodel instance.\n    '''\n    ty = ty.upper()\n    \n    null_value = {\n        'BOOLEAN'   : False,\n        'INTEGER'   : 0,\n        'REAL'      : 0.0,\n        'STRING'    : '',\n        'UNIQUE_ID' : 0\n    }\n    \n    transfer_fn = {\n        'BOOLEAN'     : lambda v: '%d' % int(v),\n        'INTEGER'     : lambda v: '%d' % v,\n        'REAL'        : lambda v: '%f' % v,\n        'STRING'      : lambda v: \"'%s'\" % v.replace(\"'\", \"''\"),\n        'UNIQUE_ID'   : lambda v: '\"%s\"' % uuid.UUID(int=v)\n    }\n\n    if value is None:\n        value = null_value[ty]\n    \n    return transfer_fn[ty](value)", "code_tokens": ["def", "serialize_value", "(", "value", ",", "ty", ")", ":", "ty", "=", "ty", ".", "upper", "(", ")", "null_value", "=", "{", "'BOOLEAN'", ":", "False", ",", "'INTEGER'", ":", "0", ",", "'REAL'", ":", "0.0", ",", "'STRING'", ":", "''", ",", "'UNIQUE_ID'", ":", "0", "}", "transfer_fn", "=", "{", "'BOOLEAN'", ":", "lambda", "v", ":", "'%d'", "%", "int", "(", "v", ")", ",", "'INTEGER'", ":", "lambda", "v", ":", "'%d'", "%", "v", ",", "'REAL'", ":", "lambda", "v", ":", "'%f'", "%", "v", ",", "'STRING'", ":", "lambda", "v", ":", "\"'%s'\"", "%", "v", ".", "replace", "(", "\"'\"", ",", "\"''\"", ")", ",", "'UNIQUE_ID'", ":", "lambda", "v", ":", "'\"%s\"'", "%", "uuid", ".", "UUID", "(", "int", "=", "v", ")", "}", "if", "value", "is", "None", ":", "value", "=", "null_value", "[", "ty", "]", "return", "transfer_fn", "[", "ty", "]", "(", "value", ")"], "docstring": "Serialize a value from an xtuml metamodel instance.", "docstring_tokens": ["Serialize", "a", "value", "from", "an", "xtuml", "metamodel", "instance", "."], "sha": "7dd9343b9a0191d1db1887ab9288d0a026608d9a", "url": "https://github.com/xtuml/pyxtuml/blob/7dd9343b9a0191d1db1887ab9288d0a026608d9a/xtuml/persist.py#L32-L57", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/utils/frame.py", "func_name": "debugx", "original_string": "def debugx(expr,pre_msg=''):\n    \"\"\"Print the value of an expression from the caller's frame.\n\n    Takes an expression, evaluates it in the caller's frame and prints both\n    the given expression and the resulting value (as well as a debug mark\n    indicating the name of the calling function.  The input must be of a form\n    suitable for eval().\n\n    An optional message can be passed, which will be prepended to the printed\n    expr->value pair.\"\"\"\n\n    cf = sys._getframe(1)\n    print '[DBG:%s] %s%s -> %r' % (cf.f_code.co_name,pre_msg,expr,\n                                   eval(expr,cf.f_globals,cf.f_locals))", "language": "python", "code": "def debugx(expr,pre_msg=''):\n    \"\"\"Print the value of an expression from the caller's frame.\n\n    Takes an expression, evaluates it in the caller's frame and prints both\n    the given expression and the resulting value (as well as a debug mark\n    indicating the name of the calling function.  The input must be of a form\n    suitable for eval().\n\n    An optional message can be passed, which will be prepended to the printed\n    expr->value pair.\"\"\"\n\n    cf = sys._getframe(1)\n    print '[DBG:%s] %s%s -> %r' % (cf.f_code.co_name,pre_msg,expr,\n                                   eval(expr,cf.f_globals,cf.f_locals))", "code_tokens": ["def", "debugx", "(", "expr", ",", "pre_msg", "=", "''", ")", ":", "cf", "=", "sys", ".", "_getframe", "(", "1", ")", "print", "'[DBG:%s] %s%s -> %r'", "%", "(", "cf", ".", "f_code", ".", "co_name", ",", "pre_msg", ",", "expr", ",", "eval", "(", "expr", ",", "cf", ".", "f_globals", ",", "cf", ".", "f_locals", ")", ")"], "docstring": "Print the value of an expression from the caller's frame.\n\n    Takes an expression, evaluates it in the caller's frame and prints both\n    the given expression and the resulting value (as well as a debug mark\n    indicating the name of the calling function.  The input must be of a form\n    suitable for eval().\n\n    An optional message can be passed, which will be prepended to the printed\n    expr->value pair.", "docstring_tokens": ["Print", "the", "value", "of", "an", "expression", "from", "the", "caller", "s", "frame", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/utils/frame.py#L69-L82", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/utils/sqlalchemy.py", "func_name": "UtcDateTime.process_result_value", "original_string": "def process_result_value(self, value, dialect):\n        \"\"\"\n        Processes DateTimes from the DB making sure it is always\n        returning UTC. Not using timezone.convert_to_utc as that\n        converts to configured TIMEZONE while the DB might be\n        running with some other setting. We assume UTC datetimes\n        in the database.\n        \"\"\"\n        if value is not None:\n            if value.tzinfo is None:\n                value = value.replace(tzinfo=utc)\n            else:\n                value = value.astimezone(utc)\n\n        return value", "language": "python", "code": "def process_result_value(self, value, dialect):\n        \"\"\"\n        Processes DateTimes from the DB making sure it is always\n        returning UTC. Not using timezone.convert_to_utc as that\n        converts to configured TIMEZONE while the DB might be\n        running with some other setting. We assume UTC datetimes\n        in the database.\n        \"\"\"\n        if value is not None:\n            if value.tzinfo is None:\n                value = value.replace(tzinfo=utc)\n            else:\n                value = value.astimezone(utc)\n\n        return value", "code_tokens": ["def", "process_result_value", "(", "self", ",", "value", ",", "dialect", ")", ":", "if", "value", "is", "not", "None", ":", "if", "value", ".", "tzinfo", "is", "None", ":", "value", "=", "value", ".", "replace", "(", "tzinfo", "=", "utc", ")", "else", ":", "value", "=", "value", ".", "astimezone", "(", "utc", ")", "return", "value"], "docstring": "Processes DateTimes from the DB making sure it is always\n        returning UTC. Not using timezone.convert_to_utc as that\n        converts to configured TIMEZONE while the DB might be\n        running with some other setting. We assume UTC datetimes\n        in the database.", "docstring_tokens": ["Processes", "DateTimes", "from", "the", "DB", "making", "sure", "it", "is", "always", "returning", "UTC", ".", "Not", "using", "timezone", ".", "convert_to_utc", "as", "that", "converts", "to", "configured", "TIMEZONE", "while", "the", "DB", "might", "be", "running", "with", "some", "other", "setting", ".", "We", "assume", "UTC", "datetimes", "in", "the", "database", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/sqlalchemy.py#L156-L170", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/terminal/console/interactiveshell.py", "func_name": "ZMQTerminalInteractiveShell.wait_for_kernel", "original_string": "def wait_for_kernel(self, timeout=None):\n        \"\"\"method to wait for a kernel to be ready\"\"\"\n        tic = time.time()\n        self.km.hb_channel.unpause()\n        while True:\n            self.run_cell('1', False)\n            if self.km.hb_channel.is_beating():\n                # heart failure was not the reason this returned\n                break\n            else:\n                # heart failed\n                if timeout is not None and (time.time() - tic) > timeout:\n                    return False\n        return True", "language": "python", "code": "def wait_for_kernel(self, timeout=None):\n        \"\"\"method to wait for a kernel to be ready\"\"\"\n        tic = time.time()\n        self.km.hb_channel.unpause()\n        while True:\n            self.run_cell('1', False)\n            if self.km.hb_channel.is_beating():\n                # heart failure was not the reason this returned\n                break\n            else:\n                # heart failed\n                if timeout is not None and (time.time() - tic) > timeout:\n                    return False\n        return True", "code_tokens": ["def", "wait_for_kernel", "(", "self", ",", "timeout", "=", "None", ")", ":", "tic", "=", "time", ".", "time", "(", ")", "self", ".", "km", ".", "hb_channel", ".", "unpause", "(", ")", "while", "True", ":", "self", ".", "run_cell", "(", "'1'", ",", "False", ")", "if", "self", ".", "km", ".", "hb_channel", ".", "is_beating", "(", ")", ":", "# heart failure was not the reason this returned", "break", "else", ":", "# heart failed", "if", "timeout", "is", "not", "None", "and", "(", "time", ".", "time", "(", ")", "-", "tic", ")", ">", "timeout", ":", "return", "False", "return", "True"], "docstring": "method to wait for a kernel to be ready", "docstring_tokens": ["method", "to", "wait", "for", "a", "kernel", "to", "be", "ready"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/terminal/console/interactiveshell.py#L220-L233", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/groupsio.py", "func_name": "GroupsioClient.__fetch", "original_string": "def __fetch(self, url, payload):\n        \"\"\"Fetch requests from groupsio API\"\"\"\n\n        r = requests.get(url, params=payload, auth=self.auth, verify=self.verify)\n        try:\n            r.raise_for_status()\n        except requests.exceptions.HTTPError as e:\n            raise e\n\n        return r", "language": "python", "code": "def __fetch(self, url, payload):\n        \"\"\"Fetch requests from groupsio API\"\"\"\n\n        r = requests.get(url, params=payload, auth=self.auth, verify=self.verify)\n        try:\n            r.raise_for_status()\n        except requests.exceptions.HTTPError as e:\n            raise e\n\n        return r", "code_tokens": ["def", "__fetch", "(", "self", ",", "url", ",", "payload", ")", ":", "r", "=", "requests", ".", "get", "(", "url", ",", "params", "=", "payload", ",", "auth", "=", "self", ".", "auth", ",", "verify", "=", "self", ".", "verify", ")", "try", ":", "r", ".", "raise_for_status", "(", ")", "except", "requests", ".", "exceptions", ".", "HTTPError", "as", "e", ":", "raise", "e", "return", "r"], "docstring": "Fetch requests from groupsio API", "docstring_tokens": ["Fetch", "requests", "from", "groupsio", "API"], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/groupsio.py#L242-L251", "partition": "test"}
{"repo": "jazzband/django-ddp", "path": "dddp/postgres.py", "func_name": "PostgresGreenlet.poll", "original_string": "def poll(self, conn):\n        \"\"\"Poll DB socket and process async tasks.\"\"\"\n        while 1:\n            state = conn.poll()\n            if state == psycopg2.extensions.POLL_OK:\n                while conn.notifies:\n                    notify = conn.notifies.pop()\n                    self.logger.info(\n                        \"Got NOTIFY (pid=%d, payload=%r)\",\n                        notify.pid, notify.payload,\n                    )\n\n                    # read the header and check seq/fin.\n                    hdr, chunk = notify.payload.split('|', 1)\n                    # print('RECEIVE: %s' % hdr)\n                    header = ejson.loads(hdr)\n                    uuid = header['uuid']\n                    size, chunks = self.chunks.setdefault(uuid, [0, {}])\n                    if header['fin']:\n                        size = self.chunks[uuid][0] = header['seq']\n\n                    # stash the chunk\n                    chunks[header['seq']] = chunk\n\n                    if len(chunks) != size:\n                        # haven't got all the chunks yet\n                        continue  # process next NOTIFY in loop\n\n                    # got the last chunk -> process it.\n                    data = ''.join(\n                        chunk for _, chunk in sorted(chunks.items())\n                    )\n                    del self.chunks[uuid]  # don't forget to cleanup!\n                    data = ejson.loads(data)\n                    sender = data.pop('_sender', None)\n                    tx_id = data.pop('_tx_id', None)\n                    for connection_id in data.pop('_connection_ids'):\n                        try:\n                            websocket = self.connections[connection_id]\n                        except KeyError:\n                            continue  # connection not in this process\n                        if connection_id == sender:\n                            websocket.send(data, tx_id=tx_id)\n                        else:\n                            websocket.send(data)\n                break\n            elif state == psycopg2.extensions.POLL_WRITE:\n                gevent.select.select([], [conn.fileno()], [])\n            elif state == psycopg2.extensions.POLL_READ:\n                gevent.select.select([conn.fileno()], [], [])\n            else:\n                self.logger.warn('POLL_ERR: %s', state)", "language": "python", "code": "def poll(self, conn):\n        \"\"\"Poll DB socket and process async tasks.\"\"\"\n        while 1:\n            state = conn.poll()\n            if state == psycopg2.extensions.POLL_OK:\n                while conn.notifies:\n                    notify = conn.notifies.pop()\n                    self.logger.info(\n                        \"Got NOTIFY (pid=%d, payload=%r)\",\n                        notify.pid, notify.payload,\n                    )\n\n                    # read the header and check seq/fin.\n                    hdr, chunk = notify.payload.split('|', 1)\n                    # print('RECEIVE: %s' % hdr)\n                    header = ejson.loads(hdr)\n                    uuid = header['uuid']\n                    size, chunks = self.chunks.setdefault(uuid, [0, {}])\n                    if header['fin']:\n                        size = self.chunks[uuid][0] = header['seq']\n\n                    # stash the chunk\n                    chunks[header['seq']] = chunk\n\n                    if len(chunks) != size:\n                        # haven't got all the chunks yet\n                        continue  # process next NOTIFY in loop\n\n                    # got the last chunk -> process it.\n                    data = ''.join(\n                        chunk for _, chunk in sorted(chunks.items())\n                    )\n                    del self.chunks[uuid]  # don't forget to cleanup!\n                    data = ejson.loads(data)\n                    sender = data.pop('_sender', None)\n                    tx_id = data.pop('_tx_id', None)\n                    for connection_id in data.pop('_connection_ids'):\n                        try:\n                            websocket = self.connections[connection_id]\n                        except KeyError:\n                            continue  # connection not in this process\n                        if connection_id == sender:\n                            websocket.send(data, tx_id=tx_id)\n                        else:\n                            websocket.send(data)\n                break\n            elif state == psycopg2.extensions.POLL_WRITE:\n                gevent.select.select([], [conn.fileno()], [])\n            elif state == psycopg2.extensions.POLL_READ:\n                gevent.select.select([conn.fileno()], [], [])\n            else:\n                self.logger.warn('POLL_ERR: %s', state)", "code_tokens": ["def", "poll", "(", "self", ",", "conn", ")", ":", "while", "1", ":", "state", "=", "conn", ".", "poll", "(", ")", "if", "state", "==", "psycopg2", ".", "extensions", ".", "POLL_OK", ":", "while", "conn", ".", "notifies", ":", "notify", "=", "conn", ".", "notifies", ".", "pop", "(", ")", "self", ".", "logger", ".", "info", "(", "\"Got NOTIFY (pid=%d, payload=%r)\"", ",", "notify", ".", "pid", ",", "notify", ".", "payload", ",", ")", "# read the header and check seq/fin.", "hdr", ",", "chunk", "=", "notify", ".", "payload", ".", "split", "(", "'|'", ",", "1", ")", "# print('RECEIVE: %s' % hdr)", "header", "=", "ejson", ".", "loads", "(", "hdr", ")", "uuid", "=", "header", "[", "'uuid'", "]", "size", ",", "chunks", "=", "self", ".", "chunks", ".", "setdefault", "(", "uuid", ",", "[", "0", ",", "{", "}", "]", ")", "if", "header", "[", "'fin'", "]", ":", "size", "=", "self", ".", "chunks", "[", "uuid", "]", "[", "0", "]", "=", "header", "[", "'seq'", "]", "# stash the chunk", "chunks", "[", "header", "[", "'seq'", "]", "]", "=", "chunk", "if", "len", "(", "chunks", ")", "!=", "size", ":", "# haven't got all the chunks yet", "continue", "# process next NOTIFY in loop", "# got the last chunk -> process it.", "data", "=", "''", ".", "join", "(", "chunk", "for", "_", ",", "chunk", "in", "sorted", "(", "chunks", ".", "items", "(", ")", ")", ")", "del", "self", ".", "chunks", "[", "uuid", "]", "# don't forget to cleanup!", "data", "=", "ejson", ".", "loads", "(", "data", ")", "sender", "=", "data", ".", "pop", "(", "'_sender'", ",", "None", ")", "tx_id", "=", "data", ".", "pop", "(", "'_tx_id'", ",", "None", ")", "for", "connection_id", "in", "data", ".", "pop", "(", "'_connection_ids'", ")", ":", "try", ":", "websocket", "=", "self", ".", "connections", "[", "connection_id", "]", "except", "KeyError", ":", "continue", "# connection not in this process", "if", "connection_id", "==", "sender", ":", "websocket", ".", "send", "(", "data", ",", "tx_id", "=", "tx_id", ")", "else", ":", "websocket", ".", "send", "(", "data", ")", "break", "elif", "state", "==", "psycopg2", ".", "extensions", ".", "POLL_WRITE", ":", "gevent", ".", "select", ".", "select", "(", "[", "]", ",", "[", "conn", ".", "fileno", "(", ")", "]", ",", "[", "]", ")", "elif", "state", "==", "psycopg2", ".", "extensions", ".", "POLL_READ", ":", "gevent", ".", "select", ".", "select", "(", "[", "conn", ".", "fileno", "(", ")", "]", ",", "[", "]", ",", "[", "]", ")", "else", ":", "self", ".", "logger", ".", "warn", "(", "'POLL_ERR: %s'", ",", "state", ")"], "docstring": "Poll DB socket and process async tasks.", "docstring_tokens": ["Poll", "DB", "socket", "and", "process", "async", "tasks", "."], "sha": "1e1954b06fe140346acea43582515991685e4e01", "url": "https://github.com/jazzband/django-ddp/blob/1e1954b06fe140346acea43582515991685e4e01/dddp/postgres.py#L99-L150", "partition": "test"}
{"repo": "connectordb/connectordb-python", "path": "connectordb/_websocket.py", "func_name": "WebsocketHandler.__on_open", "original_string": "def __on_open(self, ws):\n        \"\"\"Called when the websocket is opened\"\"\"\n        logging.debug(\"ConnectorDB: Websocket opened\")\n\n        # Connection success - decrease the wait time for next connection\n        self.reconnect_time /= self.reconnect_time_backoff_multiplier\n\n        self.status = \"connected\"\n\n        self.lastpingtime = time.time()\n        self.__ensure_ping()\n\n        self.connected_time = time.time()\n\n        # Release the lock that connect called\n        self.ws_openlock.release()", "language": "python", "code": "def __on_open(self, ws):\n        \"\"\"Called when the websocket is opened\"\"\"\n        logging.debug(\"ConnectorDB: Websocket opened\")\n\n        # Connection success - decrease the wait time for next connection\n        self.reconnect_time /= self.reconnect_time_backoff_multiplier\n\n        self.status = \"connected\"\n\n        self.lastpingtime = time.time()\n        self.__ensure_ping()\n\n        self.connected_time = time.time()\n\n        # Release the lock that connect called\n        self.ws_openlock.release()", "code_tokens": ["def", "__on_open", "(", "self", ",", "ws", ")", ":", "logging", ".", "debug", "(", "\"ConnectorDB: Websocket opened\"", ")", "# Connection success - decrease the wait time for next connection", "self", ".", "reconnect_time", "/=", "self", ".", "reconnect_time_backoff_multiplier", "self", ".", "status", "=", "\"connected\"", "self", ".", "lastpingtime", "=", "time", ".", "time", "(", ")", "self", ".", "__ensure_ping", "(", ")", "self", ".", "connected_time", "=", "time", ".", "time", "(", ")", "# Release the lock that connect called", "self", ".", "ws_openlock", ".", "release", "(", ")"], "docstring": "Called when the websocket is opened", "docstring_tokens": ["Called", "when", "the", "websocket", "is", "opened"], "sha": "2092b0cb30898139a247176bcf433d5a4abde7cb", "url": "https://github.com/connectordb/connectordb-python/blob/2092b0cb30898139a247176bcf433d5a4abde7cb/connectordb/_websocket.py#L232-L247", "partition": "test"}
{"repo": "5j9/wikitextparser", "path": "wikitextparser/_wikitext.py", "func_name": "SubWikiText.ancestors", "original_string": "def ancestors(self, type_: Optional[str] = None) -> List['WikiText']:\n        \"\"\"Return the ancestors of the current node.\n\n        :param type_: the type of the desired ancestors as a string.\n            Currently the following types are supported: {Template,\n            ParserFunction, WikiLink, Comment, Parameter, ExtensionTag}.\n            The default is None and means all the ancestors of any type above.\n        \"\"\"\n        if type_ is None:\n            types = SPAN_PARSER_TYPES\n        else:\n            types = type_,\n        lststr = self._lststr\n        type_to_spans = self._type_to_spans\n        ss, se = self._span\n        ancestors = []\n        ancestors_append = ancestors.append\n        for type_ in types:\n            cls = globals()[type_]\n            spans = type_to_spans[type_]\n            for span in spans[:bisect(spans, [ss])]:\n                if se < span[1]:\n                    ancestors_append(cls(lststr, type_to_spans, span, type_))\n        return sorted(ancestors, key=lambda i: ss - i._span[0])", "language": "python", "code": "def ancestors(self, type_: Optional[str] = None) -> List['WikiText']:\n        \"\"\"Return the ancestors of the current node.\n\n        :param type_: the type of the desired ancestors as a string.\n            Currently the following types are supported: {Template,\n            ParserFunction, WikiLink, Comment, Parameter, ExtensionTag}.\n            The default is None and means all the ancestors of any type above.\n        \"\"\"\n        if type_ is None:\n            types = SPAN_PARSER_TYPES\n        else:\n            types = type_,\n        lststr = self._lststr\n        type_to_spans = self._type_to_spans\n        ss, se = self._span\n        ancestors = []\n        ancestors_append = ancestors.append\n        for type_ in types:\n            cls = globals()[type_]\n            spans = type_to_spans[type_]\n            for span in spans[:bisect(spans, [ss])]:\n                if se < span[1]:\n                    ancestors_append(cls(lststr, type_to_spans, span, type_))\n        return sorted(ancestors, key=lambda i: ss - i._span[0])", "code_tokens": ["def", "ancestors", "(", "self", ",", "type_", ":", "Optional", "[", "str", "]", "=", "None", ")", "->", "List", "[", "'WikiText'", "]", ":", "if", "type_", "is", "None", ":", "types", "=", "SPAN_PARSER_TYPES", "else", ":", "types", "=", "type_", ",", "lststr", "=", "self", ".", "_lststr", "type_to_spans", "=", "self", ".", "_type_to_spans", "ss", ",", "se", "=", "self", ".", "_span", "ancestors", "=", "[", "]", "ancestors_append", "=", "ancestors", ".", "append", "for", "type_", "in", "types", ":", "cls", "=", "globals", "(", ")", "[", "type_", "]", "spans", "=", "type_to_spans", "[", "type_", "]", "for", "span", "in", "spans", "[", ":", "bisect", "(", "spans", ",", "[", "ss", "]", ")", "]", ":", "if", "se", "<", "span", "[", "1", "]", ":", "ancestors_append", "(", "cls", "(", "lststr", ",", "type_to_spans", ",", "span", ",", "type_", ")", ")", "return", "sorted", "(", "ancestors", ",", "key", "=", "lambda", "i", ":", "ss", "-", "i", ".", "_span", "[", "0", "]", ")"], "docstring": "Return the ancestors of the current node.\n\n        :param type_: the type of the desired ancestors as a string.\n            Currently the following types are supported: {Template,\n            ParserFunction, WikiLink, Comment, Parameter, ExtensionTag}.\n            The default is None and means all the ancestors of any type above.", "docstring_tokens": ["Return", "the", "ancestors", "of", "the", "current", "node", "."], "sha": "1347425814361d7955342c53212edbb27f0ff4b5", "url": "https://github.com/5j9/wikitextparser/blob/1347425814361d7955342c53212edbb27f0ff4b5/wikitextparser/_wikitext.py#L1065-L1088", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/core/constantq.py", "func_name": "__early_downsample_count", "original_string": "def __early_downsample_count(nyquist, filter_cutoff, hop_length, n_octaves):\n    '''Compute the number of early downsampling operations'''\n\n    downsample_count1 = max(0, int(np.ceil(np.log2(audio.BW_FASTEST * nyquist /\n                                                   filter_cutoff)) - 1) - 1)\n\n    num_twos = __num_two_factors(hop_length)\n    downsample_count2 = max(0, num_twos - n_octaves + 1)\n\n    return min(downsample_count1, downsample_count2)", "language": "python", "code": "def __early_downsample_count(nyquist, filter_cutoff, hop_length, n_octaves):\n    '''Compute the number of early downsampling operations'''\n\n    downsample_count1 = max(0, int(np.ceil(np.log2(audio.BW_FASTEST * nyquist /\n                                                   filter_cutoff)) - 1) - 1)\n\n    num_twos = __num_two_factors(hop_length)\n    downsample_count2 = max(0, num_twos - n_octaves + 1)\n\n    return min(downsample_count1, downsample_count2)", "code_tokens": ["def", "__early_downsample_count", "(", "nyquist", ",", "filter_cutoff", ",", "hop_length", ",", "n_octaves", ")", ":", "downsample_count1", "=", "max", "(", "0", ",", "int", "(", "np", ".", "ceil", "(", "np", ".", "log2", "(", "audio", ".", "BW_FASTEST", "*", "nyquist", "/", "filter_cutoff", ")", ")", "-", "1", ")", "-", "1", ")", "num_twos", "=", "__num_two_factors", "(", "hop_length", ")", "downsample_count2", "=", "max", "(", "0", ",", "num_twos", "-", "n_octaves", "+", "1", ")", "return", "min", "(", "downsample_count1", ",", "downsample_count2", ")"], "docstring": "Compute the number of early downsampling operations", "docstring_tokens": ["Compute", "the", "number", "of", "early", "downsampling", "operations"], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/core/constantq.py#L768-L777", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/git.py", "func_name": "GitRepository.show", "original_string": "def show(self, commits=None, encoding='utf-8'):\n        \"\"\"Show the data of a set of commits.\n\n        The method returns the output of Git show command for a\n        set of commits using the following options:\n\n            git show --raw --numstat --pretty=fuller --decorate=full\n                --parents -M -C -c [<commit>...<commit>]\n\n        When the list of commits is empty, the command will return\n        data about the last commit, like the default behaviour of\n        `git show`.\n\n        :param commits: list of commits to show data\n        :param encoding: encode the output using this format\n\n        :returns: a generator where each item is a line from the show output\n\n        :raises EmptyRepositoryError: when the repository is empty and\n            the action cannot be performed\n        :raises RepositoryError: when an error occurs fetching the show output\n        \"\"\"\n        if self.is_empty():\n            logger.warning(\"Git %s repository is empty; unable to run show\",\n                           self.uri)\n            raise EmptyRepositoryError(repository=self.uri)\n\n        if commits is None:\n            commits = []\n\n        cmd_show = ['git', 'show']\n        cmd_show.extend(self.GIT_PRETTY_OUTPUT_OPTS)\n        cmd_show.extend(commits)\n\n        for line in self._exec_nb(cmd_show, cwd=self.dirpath, env=self.gitenv):\n            yield line\n\n        logger.debug(\"Git show fetched from %s repository (%s)\",\n                     self.uri, self.dirpath)", "language": "python", "code": "def show(self, commits=None, encoding='utf-8'):\n        \"\"\"Show the data of a set of commits.\n\n        The method returns the output of Git show command for a\n        set of commits using the following options:\n\n            git show --raw --numstat --pretty=fuller --decorate=full\n                --parents -M -C -c [<commit>...<commit>]\n\n        When the list of commits is empty, the command will return\n        data about the last commit, like the default behaviour of\n        `git show`.\n\n        :param commits: list of commits to show data\n        :param encoding: encode the output using this format\n\n        :returns: a generator where each item is a line from the show output\n\n        :raises EmptyRepositoryError: when the repository is empty and\n            the action cannot be performed\n        :raises RepositoryError: when an error occurs fetching the show output\n        \"\"\"\n        if self.is_empty():\n            logger.warning(\"Git %s repository is empty; unable to run show\",\n                           self.uri)\n            raise EmptyRepositoryError(repository=self.uri)\n\n        if commits is None:\n            commits = []\n\n        cmd_show = ['git', 'show']\n        cmd_show.extend(self.GIT_PRETTY_OUTPUT_OPTS)\n        cmd_show.extend(commits)\n\n        for line in self._exec_nb(cmd_show, cwd=self.dirpath, env=self.gitenv):\n            yield line\n\n        logger.debug(\"Git show fetched from %s repository (%s)\",\n                     self.uri, self.dirpath)", "code_tokens": ["def", "show", "(", "self", ",", "commits", "=", "None", ",", "encoding", "=", "'utf-8'", ")", ":", "if", "self", ".", "is_empty", "(", ")", ":", "logger", ".", "warning", "(", "\"Git %s repository is empty; unable to run show\"", ",", "self", ".", "uri", ")", "raise", "EmptyRepositoryError", "(", "repository", "=", "self", ".", "uri", ")", "if", "commits", "is", "None", ":", "commits", "=", "[", "]", "cmd_show", "=", "[", "'git'", ",", "'show'", "]", "cmd_show", ".", "extend", "(", "self", ".", "GIT_PRETTY_OUTPUT_OPTS", ")", "cmd_show", ".", "extend", "(", "commits", ")", "for", "line", "in", "self", ".", "_exec_nb", "(", "cmd_show", ",", "cwd", "=", "self", ".", "dirpath", ",", "env", "=", "self", ".", "gitenv", ")", ":", "yield", "line", "logger", ".", "debug", "(", "\"Git show fetched from %s repository (%s)\"", ",", "self", ".", "uri", ",", "self", ".", "dirpath", ")"], "docstring": "Show the data of a set of commits.\n\n        The method returns the output of Git show command for a\n        set of commits using the following options:\n\n            git show --raw --numstat --pretty=fuller --decorate=full\n                --parents -M -C -c [<commit>...<commit>]\n\n        When the list of commits is empty, the command will return\n        data about the last commit, like the default behaviour of\n        `git show`.\n\n        :param commits: list of commits to show data\n        :param encoding: encode the output using this format\n\n        :returns: a generator where each item is a line from the show output\n\n        :raises EmptyRepositoryError: when the repository is empty and\n            the action cannot be performed\n        :raises RepositoryError: when an error occurs fetching the show output", "docstring_tokens": ["Show", "the", "data", "of", "a", "set", "of", "commits", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/git.py#L1043-L1081", "partition": "test"}
{"repo": "Azure/Azure-MachineLearning-ClientLibrary-Python", "path": "azureml/services.py", "func_name": "_Serializer.find_globals", "original_string": "def find_globals(code):\n        \"\"\"walks the byte code to find the variables which are actually globals\"\"\"\n        cur_byte = 0\n        byte_code = code.co_code\n        \n        names = set()\n        while cur_byte < len(byte_code):\n            op = ord(byte_code[cur_byte])\n\n            if op >= dis.HAVE_ARGUMENT:\n                if op == _LOAD_GLOBAL:\n                    oparg = ord(byte_code[cur_byte + 1]) + (ord(byte_code[cur_byte + 2]) << 8)\n                    name = code.co_names[oparg]\n                    names.add(name)\n\n                cur_byte += 2\n            cur_byte += 1\n        \n        return names", "language": "python", "code": "def find_globals(code):\n        \"\"\"walks the byte code to find the variables which are actually globals\"\"\"\n        cur_byte = 0\n        byte_code = code.co_code\n        \n        names = set()\n        while cur_byte < len(byte_code):\n            op = ord(byte_code[cur_byte])\n\n            if op >= dis.HAVE_ARGUMENT:\n                if op == _LOAD_GLOBAL:\n                    oparg = ord(byte_code[cur_byte + 1]) + (ord(byte_code[cur_byte + 2]) << 8)\n                    name = code.co_names[oparg]\n                    names.add(name)\n\n                cur_byte += 2\n            cur_byte += 1\n        \n        return names", "code_tokens": ["def", "find_globals", "(", "code", ")", ":", "cur_byte", "=", "0", "byte_code", "=", "code", ".", "co_code", "names", "=", "set", "(", ")", "while", "cur_byte", "<", "len", "(", "byte_code", ")", ":", "op", "=", "ord", "(", "byte_code", "[", "cur_byte", "]", ")", "if", "op", ">=", "dis", ".", "HAVE_ARGUMENT", ":", "if", "op", "==", "_LOAD_GLOBAL", ":", "oparg", "=", "ord", "(", "byte_code", "[", "cur_byte", "+", "1", "]", ")", "+", "(", "ord", "(", "byte_code", "[", "cur_byte", "+", "2", "]", ")", "<<", "8", ")", "name", "=", "code", ".", "co_names", "[", "oparg", "]", "names", ".", "add", "(", "name", ")", "cur_byte", "+=", "2", "cur_byte", "+=", "1", "return", "names"], "docstring": "walks the byte code to find the variables which are actually globals", "docstring_tokens": ["walks", "the", "byte", "code", "to", "find", "the", "variables", "which", "are", "actually", "globals"], "sha": "d1211b289747671898eb063013e0dc53d3c80acd", "url": "https://github.com/Azure/Azure-MachineLearning-ClientLibrary-Python/blob/d1211b289747671898eb063013e0dc53d3c80acd/azureml/services.py#L394-L412", "partition": "test"}
{"repo": "ToucanToco/toucan-data-sdk", "path": "toucan_data_sdk/utils/decorators.py", "func_name": "log_message", "original_string": "def log_message(logger, message=\"\"):\n    \"\"\"\n    Decorator to log a message before executing a function\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            _log_message(logger, func.__name__, message)\n            result = func(*args, **kwargs)\n            return result\n        return wrapper\n    return decorator", "language": "python", "code": "def log_message(logger, message=\"\"):\n    \"\"\"\n    Decorator to log a message before executing a function\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            _log_message(logger, func.__name__, message)\n            result = func(*args, **kwargs)\n            return result\n        return wrapper\n    return decorator", "code_tokens": ["def", "log_message", "(", "logger", ",", "message", "=", "\"\"", ")", ":", "def", "decorator", "(", "func", ")", ":", "@", "wraps", "(", "func", ")", "def", "wrapper", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "_log_message", "(", "logger", ",", "func", ".", "__name__", ",", "message", ")", "result", "=", "func", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "result", "return", "wrapper", "return", "decorator"], "docstring": "Decorator to log a message before executing a function", "docstring_tokens": ["Decorator", "to", "log", "a", "message", "before", "executing", "a", "function"], "sha": "c3ca874e1b64f4bdcc2edda750a72d45d1561d8a", "url": "https://github.com/ToucanToco/toucan-data-sdk/blob/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a/toucan_data_sdk/utils/decorators.py#L109-L120", "partition": "test"}
{"repo": "singularityhub/sregistry-cli", "path": "sregistry/utils/terminal.py", "func_name": "get_singularity_version", "original_string": "def get_singularity_version(singularity_version=None):\n    '''get_singularity_version will determine the singularity version for a\n       build first, an environmental variable is looked at, followed by \n       using the system version.\n\n       Parameters\n       ==========\n       singularity_version: if not defined, look for in environment. If still\n       not find, try finding via executing --version to Singularity. Only return\n       None if not set in environment or installed.\n    '''\n\n    if singularity_version is None:        \n        singularity_version = os.environ.get(\"SINGULARITY_VERSION\")\n        \n    if singularity_version is None:\n        try:\n            cmd = ['singularity','--version']\n            output = run_command(cmd)\n\n            if isinstance(output['message'],bytes):\n                output['message'] = output['message'].decode('utf-8')\n            singularity_version = output['message'].strip('\\n')\n            bot.info(\"Singularity %s being used.\" % singularity_version)\n            \n        except:\n            singularity_version = None\n            bot.warning(\"Singularity version not found, so it's likely not installed.\")\n\n    return singularity_version", "language": "python", "code": "def get_singularity_version(singularity_version=None):\n    '''get_singularity_version will determine the singularity version for a\n       build first, an environmental variable is looked at, followed by \n       using the system version.\n\n       Parameters\n       ==========\n       singularity_version: if not defined, look for in environment. If still\n       not find, try finding via executing --version to Singularity. Only return\n       None if not set in environment or installed.\n    '''\n\n    if singularity_version is None:        \n        singularity_version = os.environ.get(\"SINGULARITY_VERSION\")\n        \n    if singularity_version is None:\n        try:\n            cmd = ['singularity','--version']\n            output = run_command(cmd)\n\n            if isinstance(output['message'],bytes):\n                output['message'] = output['message'].decode('utf-8')\n            singularity_version = output['message'].strip('\\n')\n            bot.info(\"Singularity %s being used.\" % singularity_version)\n            \n        except:\n            singularity_version = None\n            bot.warning(\"Singularity version not found, so it's likely not installed.\")\n\n    return singularity_version", "code_tokens": ["def", "get_singularity_version", "(", "singularity_version", "=", "None", ")", ":", "if", "singularity_version", "is", "None", ":", "singularity_version", "=", "os", ".", "environ", ".", "get", "(", "\"SINGULARITY_VERSION\"", ")", "if", "singularity_version", "is", "None", ":", "try", ":", "cmd", "=", "[", "'singularity'", ",", "'--version'", "]", "output", "=", "run_command", "(", "cmd", ")", "if", "isinstance", "(", "output", "[", "'message'", "]", ",", "bytes", ")", ":", "output", "[", "'message'", "]", "=", "output", "[", "'message'", "]", ".", "decode", "(", "'utf-8'", ")", "singularity_version", "=", "output", "[", "'message'", "]", ".", "strip", "(", "'\\n'", ")", "bot", ".", "info", "(", "\"Singularity %s being used.\"", "%", "singularity_version", ")", "except", ":", "singularity_version", "=", "None", "bot", ".", "warning", "(", "\"Singularity version not found, so it's likely not installed.\"", ")", "return", "singularity_version"], "docstring": "get_singularity_version will determine the singularity version for a\n       build first, an environmental variable is looked at, followed by \n       using the system version.\n\n       Parameters\n       ==========\n       singularity_version: if not defined, look for in environment. If still\n       not find, try finding via executing --version to Singularity. Only return\n       None if not set in environment or installed.", "docstring_tokens": ["get_singularity_version", "will", "determine", "the", "singularity", "version", "for", "a", "build", "first", "an", "environmental", "variable", "is", "looked", "at", "followed", "by", "using", "the", "system", "version", "."], "sha": "abc96140a1d15b5e96d83432e1e0e1f4f8f36331", "url": "https://github.com/singularityhub/sregistry-cli/blob/abc96140a1d15b5e96d83432e1e0e1f4f8f36331/sregistry/utils/terminal.py#L25-L54", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/storageservice.py", "func_name": "HDF5StorageService._trj_fill_run_table", "original_string": "def _trj_fill_run_table(self, traj, start, stop):\n        \"\"\"Fills the `run` overview table with information.\n\n        Will also update new information.\n\n        \"\"\"\n\n        def _make_row(info_dict):\n            row = (info_dict['idx'],\n                   info_dict['name'],\n                   info_dict['time'],\n                   info_dict['timestamp'],\n                   info_dict['finish_timestamp'],\n                   info_dict['runtime'],\n                   info_dict['parameter_summary'],\n                   info_dict['short_environment_hexsha'],\n                   info_dict['completed'])\n            return row\n\n        runtable = getattr(self._overview_group, 'runs')\n\n        rows = []\n        updated_run_information = traj._updated_run_information\n        for idx in range(start, stop):\n            info_dict = traj._run_information[traj._single_run_ids[idx]]\n            rows.append(_make_row(info_dict))\n            updated_run_information.discard(idx)\n\n        if rows:\n            runtable.append(rows)\n            runtable.flush()\n\n        # Store all runs that are updated and that have not been stored yet\n        rows = []\n        indices = []\n        for idx in updated_run_information:\n            info_dict = traj.f_get_run_information(idx, copy=False)\n            rows.append(_make_row(info_dict))\n            indices.append(idx)\n\n        if rows:\n            runtable.modify_coordinates(indices, rows)\n\n        traj._updated_run_information = set()", "language": "python", "code": "def _trj_fill_run_table(self, traj, start, stop):\n        \"\"\"Fills the `run` overview table with information.\n\n        Will also update new information.\n\n        \"\"\"\n\n        def _make_row(info_dict):\n            row = (info_dict['idx'],\n                   info_dict['name'],\n                   info_dict['time'],\n                   info_dict['timestamp'],\n                   info_dict['finish_timestamp'],\n                   info_dict['runtime'],\n                   info_dict['parameter_summary'],\n                   info_dict['short_environment_hexsha'],\n                   info_dict['completed'])\n            return row\n\n        runtable = getattr(self._overview_group, 'runs')\n\n        rows = []\n        updated_run_information = traj._updated_run_information\n        for idx in range(start, stop):\n            info_dict = traj._run_information[traj._single_run_ids[idx]]\n            rows.append(_make_row(info_dict))\n            updated_run_information.discard(idx)\n\n        if rows:\n            runtable.append(rows)\n            runtable.flush()\n\n        # Store all runs that are updated and that have not been stored yet\n        rows = []\n        indices = []\n        for idx in updated_run_information:\n            info_dict = traj.f_get_run_information(idx, copy=False)\n            rows.append(_make_row(info_dict))\n            indices.append(idx)\n\n        if rows:\n            runtable.modify_coordinates(indices, rows)\n\n        traj._updated_run_information = set()", "code_tokens": ["def", "_trj_fill_run_table", "(", "self", ",", "traj", ",", "start", ",", "stop", ")", ":", "def", "_make_row", "(", "info_dict", ")", ":", "row", "=", "(", "info_dict", "[", "'idx'", "]", ",", "info_dict", "[", "'name'", "]", ",", "info_dict", "[", "'time'", "]", ",", "info_dict", "[", "'timestamp'", "]", ",", "info_dict", "[", "'finish_timestamp'", "]", ",", "info_dict", "[", "'runtime'", "]", ",", "info_dict", "[", "'parameter_summary'", "]", ",", "info_dict", "[", "'short_environment_hexsha'", "]", ",", "info_dict", "[", "'completed'", "]", ")", "return", "row", "runtable", "=", "getattr", "(", "self", ".", "_overview_group", ",", "'runs'", ")", "rows", "=", "[", "]", "updated_run_information", "=", "traj", ".", "_updated_run_information", "for", "idx", "in", "range", "(", "start", ",", "stop", ")", ":", "info_dict", "=", "traj", ".", "_run_information", "[", "traj", ".", "_single_run_ids", "[", "idx", "]", "]", "rows", ".", "append", "(", "_make_row", "(", "info_dict", ")", ")", "updated_run_information", ".", "discard", "(", "idx", ")", "if", "rows", ":", "runtable", ".", "append", "(", "rows", ")", "runtable", ".", "flush", "(", ")", "# Store all runs that are updated and that have not been stored yet", "rows", "=", "[", "]", "indices", "=", "[", "]", "for", "idx", "in", "updated_run_information", ":", "info_dict", "=", "traj", ".", "f_get_run_information", "(", "idx", ",", "copy", "=", "False", ")", "rows", ".", "append", "(", "_make_row", "(", "info_dict", ")", ")", "indices", ".", "append", "(", "idx", ")", "if", "rows", ":", "runtable", ".", "modify_coordinates", "(", "indices", ",", "rows", ")", "traj", ".", "_updated_run_information", "=", "set", "(", ")"], "docstring": "Fills the `run` overview table with information.\n\n        Will also update new information.", "docstring_tokens": ["Fills", "the", "run", "overview", "table", "with", "information", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/storageservice.py#L2177-L2220", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/transpiler/passes/unroller.py", "func_name": "Unroller.run", "original_string": "def run(self, dag):\n        \"\"\"Expand all op nodes to the given basis.\n\n        Args:\n            dag(DAGCircuit): input dag\n\n        Raises:\n            QiskitError: if unable to unroll given the basis due to undefined\n            decomposition rules (such as a bad basis) or excessive recursion.\n\n        Returns:\n            DAGCircuit: output unrolled dag\n        \"\"\"\n        # Walk through the DAG and expand each non-basis node\n        for node in dag.op_nodes():\n            basic_insts = ['measure', 'reset', 'barrier', 'snapshot']\n            if node.name in basic_insts:\n                # TODO: this is legacy behavior.Basis_insts should be removed that these\n                #  instructions should be part of the device-reported basis. Currently, no\n                #  backend reports \"measure\", for example.\n                continue\n            if node.name in self.basis:  # If already a base, ignore.\n                continue\n\n            # TODO: allow choosing other possible decompositions\n            rule = node.op.definition\n            if not rule:\n                raise QiskitError(\"Cannot unroll the circuit to the given basis, %s. \"\n                                  \"No rule to expand instruction %s.\" %\n                                  (str(self.basis), node.op.name))\n\n            # hacky way to build a dag on the same register as the rule is defined\n            # TODO: need anonymous rules to address wires by index\n            decomposition = DAGCircuit()\n            decomposition.add_qreg(rule[0][1][0][0])\n            for inst in rule:\n                decomposition.apply_operation_back(*inst)\n\n            unrolled_dag = self.run(decomposition)  # recursively unroll ops\n            dag.substitute_node_with_dag(node, unrolled_dag)\n        return dag", "language": "python", "code": "def run(self, dag):\n        \"\"\"Expand all op nodes to the given basis.\n\n        Args:\n            dag(DAGCircuit): input dag\n\n        Raises:\n            QiskitError: if unable to unroll given the basis due to undefined\n            decomposition rules (such as a bad basis) or excessive recursion.\n\n        Returns:\n            DAGCircuit: output unrolled dag\n        \"\"\"\n        # Walk through the DAG and expand each non-basis node\n        for node in dag.op_nodes():\n            basic_insts = ['measure', 'reset', 'barrier', 'snapshot']\n            if node.name in basic_insts:\n                # TODO: this is legacy behavior.Basis_insts should be removed that these\n                #  instructions should be part of the device-reported basis. Currently, no\n                #  backend reports \"measure\", for example.\n                continue\n            if node.name in self.basis:  # If already a base, ignore.\n                continue\n\n            # TODO: allow choosing other possible decompositions\n            rule = node.op.definition\n            if not rule:\n                raise QiskitError(\"Cannot unroll the circuit to the given basis, %s. \"\n                                  \"No rule to expand instruction %s.\" %\n                                  (str(self.basis), node.op.name))\n\n            # hacky way to build a dag on the same register as the rule is defined\n            # TODO: need anonymous rules to address wires by index\n            decomposition = DAGCircuit()\n            decomposition.add_qreg(rule[0][1][0][0])\n            for inst in rule:\n                decomposition.apply_operation_back(*inst)\n\n            unrolled_dag = self.run(decomposition)  # recursively unroll ops\n            dag.substitute_node_with_dag(node, unrolled_dag)\n        return dag", "code_tokens": ["def", "run", "(", "self", ",", "dag", ")", ":", "# Walk through the DAG and expand each non-basis node", "for", "node", "in", "dag", ".", "op_nodes", "(", ")", ":", "basic_insts", "=", "[", "'measure'", ",", "'reset'", ",", "'barrier'", ",", "'snapshot'", "]", "if", "node", ".", "name", "in", "basic_insts", ":", "# TODO: this is legacy behavior.Basis_insts should be removed that these", "#  instructions should be part of the device-reported basis. Currently, no", "#  backend reports \"measure\", for example.", "continue", "if", "node", ".", "name", "in", "self", ".", "basis", ":", "# If already a base, ignore.", "continue", "# TODO: allow choosing other possible decompositions", "rule", "=", "node", ".", "op", ".", "definition", "if", "not", "rule", ":", "raise", "QiskitError", "(", "\"Cannot unroll the circuit to the given basis, %s. \"", "\"No rule to expand instruction %s.\"", "%", "(", "str", "(", "self", ".", "basis", ")", ",", "node", ".", "op", ".", "name", ")", ")", "# hacky way to build a dag on the same register as the rule is defined", "# TODO: need anonymous rules to address wires by index", "decomposition", "=", "DAGCircuit", "(", ")", "decomposition", ".", "add_qreg", "(", "rule", "[", "0", "]", "[", "1", "]", "[", "0", "]", "[", "0", "]", ")", "for", "inst", "in", "rule", ":", "decomposition", ".", "apply_operation_back", "(", "*", "inst", ")", "unrolled_dag", "=", "self", ".", "run", "(", "decomposition", ")", "# recursively unroll ops", "dag", ".", "substitute_node_with_dag", "(", "node", ",", "unrolled_dag", ")", "return", "dag"], "docstring": "Expand all op nodes to the given basis.\n\n        Args:\n            dag(DAGCircuit): input dag\n\n        Raises:\n            QiskitError: if unable to unroll given the basis due to undefined\n            decomposition rules (such as a bad basis) or excessive recursion.\n\n        Returns:\n            DAGCircuit: output unrolled dag", "docstring_tokens": ["Expand", "all", "op", "nodes", "to", "the", "given", "basis", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/transpiler/passes/unroller.py#L29-L69", "partition": "test"}
{"repo": "sublee/zeronimo", "path": "zeronimo/core.py", "func_name": "Worker.accept", "original_string": "def accept(self, reply_socket, channel):\n        \"\"\"Sends ACCEPT reply.\"\"\"\n        info = self.info or b''\n        self.send_raw(reply_socket, ACCEPT, info, *channel)", "language": "python", "code": "def accept(self, reply_socket, channel):\n        \"\"\"Sends ACCEPT reply.\"\"\"\n        info = self.info or b''\n        self.send_raw(reply_socket, ACCEPT, info, *channel)", "code_tokens": ["def", "accept", "(", "self", ",", "reply_socket", ",", "channel", ")", ":", "info", "=", "self", ".", "info", "or", "b''", "self", ".", "send_raw", "(", "reply_socket", ",", "ACCEPT", ",", "info", ",", "*", "channel", ")"], "docstring": "Sends ACCEPT reply.", "docstring_tokens": ["Sends", "ACCEPT", "reply", "."], "sha": "b216638232932718d2cbc5eabd870c8f5b5e83fb", "url": "https://github.com/sublee/zeronimo/blob/b216638232932718d2cbc5eabd870c8f5b5e83fb/zeronimo/core.py#L331-L334", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/pulse/schedule.py", "func_name": "Schedule.ch_duration", "original_string": "def ch_duration(self, *channels: List[Channel]) -> int:\n        \"\"\"Return duration of supplied channels.\n\n        Args:\n            *channels: Supplied channels\n        \"\"\"\n        return self.timeslots.ch_duration(*channels)", "language": "python", "code": "def ch_duration(self, *channels: List[Channel]) -> int:\n        \"\"\"Return duration of supplied channels.\n\n        Args:\n            *channels: Supplied channels\n        \"\"\"\n        return self.timeslots.ch_duration(*channels)", "code_tokens": ["def", "ch_duration", "(", "self", ",", "*", "channels", ":", "List", "[", "Channel", "]", ")", "->", "int", ":", "return", "self", ".", "timeslots", ".", "ch_duration", "(", "*", "channels", ")"], "docstring": "Return duration of supplied channels.\n\n        Args:\n            *channels: Supplied channels", "docstring_tokens": ["Return", "duration", "of", "supplied", "channels", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/pulse/schedule.py#L100-L106", "partition": "test"}
{"repo": "oscarbranson/latools", "path": "latools/filtering/filt_obj.py", "func_name": "filt.remove", "original_string": "def remove(self, name=None, setn=None):\n        \"\"\"\n        Remove filter.\n\n        Parameters\n        ----------\n        name : str\n            name of the filter to remove\n        setn : int or True\n            int: number of set to remove\n            True: remove all filters in set that 'name' belongs to\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if isinstance(name, int):\n            name = self.index[name]\n\n        if setn is not None:\n            name = self.sets[setn]\n            del self.sets[setn]\n        elif isinstance(name, (int, str)):\n            name = [name]\n\n        if setn is True:\n            for n in name:\n                for k, v in self.sets.items():\n                    if n in v:\n                        name.append([m for m in v if m != n])\n\n        for n in name:\n            for k, v in self.sets.items():\n                if n in v:\n                    self.sets[k] = [m for m in v if n != m]\n            del self.components[n]\n            del self.info[n]\n            del self.params[n]\n            del self.keys[n]\n            for a in self.analytes:\n                del self.switches[a][n]\n            return", "language": "python", "code": "def remove(self, name=None, setn=None):\n        \"\"\"\n        Remove filter.\n\n        Parameters\n        ----------\n        name : str\n            name of the filter to remove\n        setn : int or True\n            int: number of set to remove\n            True: remove all filters in set that 'name' belongs to\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if isinstance(name, int):\n            name = self.index[name]\n\n        if setn is not None:\n            name = self.sets[setn]\n            del self.sets[setn]\n        elif isinstance(name, (int, str)):\n            name = [name]\n\n        if setn is True:\n            for n in name:\n                for k, v in self.sets.items():\n                    if n in v:\n                        name.append([m for m in v if m != n])\n\n        for n in name:\n            for k, v in self.sets.items():\n                if n in v:\n                    self.sets[k] = [m for m in v if n != m]\n            del self.components[n]\n            del self.info[n]\n            del self.params[n]\n            del self.keys[n]\n            for a in self.analytes:\n                del self.switches[a][n]\n            return", "code_tokens": ["def", "remove", "(", "self", ",", "name", "=", "None", ",", "setn", "=", "None", ")", ":", "if", "isinstance", "(", "name", ",", "int", ")", ":", "name", "=", "self", ".", "index", "[", "name", "]", "if", "setn", "is", "not", "None", ":", "name", "=", "self", ".", "sets", "[", "setn", "]", "del", "self", ".", "sets", "[", "setn", "]", "elif", "isinstance", "(", "name", ",", "(", "int", ",", "str", ")", ")", ":", "name", "=", "[", "name", "]", "if", "setn", "is", "True", ":", "for", "n", "in", "name", ":", "for", "k", ",", "v", "in", "self", ".", "sets", ".", "items", "(", ")", ":", "if", "n", "in", "v", ":", "name", ".", "append", "(", "[", "m", "for", "m", "in", "v", "if", "m", "!=", "n", "]", ")", "for", "n", "in", "name", ":", "for", "k", ",", "v", "in", "self", ".", "sets", ".", "items", "(", ")", ":", "if", "n", "in", "v", ":", "self", ".", "sets", "[", "k", "]", "=", "[", "m", "for", "m", "in", "v", "if", "n", "!=", "m", "]", "del", "self", ".", "components", "[", "n", "]", "del", "self", ".", "info", "[", "n", "]", "del", "self", ".", "params", "[", "n", "]", "del", "self", ".", "keys", "[", "n", "]", "for", "a", "in", "self", ".", "analytes", ":", "del", "self", ".", "switches", "[", "a", "]", "[", "n", "]", "return"], "docstring": "Remove filter.\n\n        Parameters\n        ----------\n        name : str\n            name of the filter to remove\n        setn : int or True\n            int: number of set to remove\n            True: remove all filters in set that 'name' belongs to\n\n        Returns\n        -------\n        None", "docstring_tokens": ["Remove", "filter", "."], "sha": "cd25a650cfee318152f234d992708511f7047fbe", "url": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/filtering/filt_obj.py#L131-L172", "partition": "test"}
{"repo": "neherlab/treetime", "path": "treetime/treeregression.py", "func_name": "base_regression", "original_string": "def base_regression(Q, slope=None):\n    \"\"\"\n    this function calculates the regression coefficients for a\n    given vector containing the averages of tip and branch\n    quantities.\n\n    Parameters\n    ----------\n    Q : numpy.array\n        vector with\n    slope : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\n    if slope is None:\n        slope = (Q[dtavgii] - Q[tavgii]*Q[davgii]/Q[sii]) \\\n                /(Q[tsqii] - Q[tavgii]**2/Q[sii])\n        only_intercept=False\n    else:\n        only_intercept=True\n\n    intercept = (Q[davgii] - Q[tavgii]*slope)/Q[sii]\n\n    if only_intercept:\n        return {'slope':slope, 'intercept':intercept,\n                'chisq': 0.5*(Q[dsqii]/Q[sii] - Q[davgii]**2/Q[sii]**2)}\n\n    chisq = 0.5*(Q[dsqii] - Q[davgii]**2/Q[sii]\n                - (Q[dtavgii] - Q[davgii]*Q[tavgii]/Q[sii])**2/(Q[tsqii]\n                - Q[tavgii]**2/Q[sii]))\n\n    estimator_hessian = np.array([[Q[tsqii], Q[tavgii]], [Q[tavgii], Q[sii]]])\n\n    return {'slope':slope, 'intercept':intercept,\n            'chisq':chisq, 'hessian':estimator_hessian,\n            'cov':np.linalg.inv(estimator_hessian)}", "language": "python", "code": "def base_regression(Q, slope=None):\n    \"\"\"\n    this function calculates the regression coefficients for a\n    given vector containing the averages of tip and branch\n    quantities.\n\n    Parameters\n    ----------\n    Q : numpy.array\n        vector with\n    slope : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\n    if slope is None:\n        slope = (Q[dtavgii] - Q[tavgii]*Q[davgii]/Q[sii]) \\\n                /(Q[tsqii] - Q[tavgii]**2/Q[sii])\n        only_intercept=False\n    else:\n        only_intercept=True\n\n    intercept = (Q[davgii] - Q[tavgii]*slope)/Q[sii]\n\n    if only_intercept:\n        return {'slope':slope, 'intercept':intercept,\n                'chisq': 0.5*(Q[dsqii]/Q[sii] - Q[davgii]**2/Q[sii]**2)}\n\n    chisq = 0.5*(Q[dsqii] - Q[davgii]**2/Q[sii]\n                - (Q[dtavgii] - Q[davgii]*Q[tavgii]/Q[sii])**2/(Q[tsqii]\n                - Q[tavgii]**2/Q[sii]))\n\n    estimator_hessian = np.array([[Q[tsqii], Q[tavgii]], [Q[tavgii], Q[sii]]])\n\n    return {'slope':slope, 'intercept':intercept,\n            'chisq':chisq, 'hessian':estimator_hessian,\n            'cov':np.linalg.inv(estimator_hessian)}", "code_tokens": ["def", "base_regression", "(", "Q", ",", "slope", "=", "None", ")", ":", "if", "slope", "is", "None", ":", "slope", "=", "(", "Q", "[", "dtavgii", "]", "-", "Q", "[", "tavgii", "]", "*", "Q", "[", "davgii", "]", "/", "Q", "[", "sii", "]", ")", "/", "(", "Q", "[", "tsqii", "]", "-", "Q", "[", "tavgii", "]", "**", "2", "/", "Q", "[", "sii", "]", ")", "only_intercept", "=", "False", "else", ":", "only_intercept", "=", "True", "intercept", "=", "(", "Q", "[", "davgii", "]", "-", "Q", "[", "tavgii", "]", "*", "slope", ")", "/", "Q", "[", "sii", "]", "if", "only_intercept", ":", "return", "{", "'slope'", ":", "slope", ",", "'intercept'", ":", "intercept", ",", "'chisq'", ":", "0.5", "*", "(", "Q", "[", "dsqii", "]", "/", "Q", "[", "sii", "]", "-", "Q", "[", "davgii", "]", "**", "2", "/", "Q", "[", "sii", "]", "**", "2", ")", "}", "chisq", "=", "0.5", "*", "(", "Q", "[", "dsqii", "]", "-", "Q", "[", "davgii", "]", "**", "2", "/", "Q", "[", "sii", "]", "-", "(", "Q", "[", "dtavgii", "]", "-", "Q", "[", "davgii", "]", "*", "Q", "[", "tavgii", "]", "/", "Q", "[", "sii", "]", ")", "**", "2", "/", "(", "Q", "[", "tsqii", "]", "-", "Q", "[", "tavgii", "]", "**", "2", "/", "Q", "[", "sii", "]", ")", ")", "estimator_hessian", "=", "np", ".", "array", "(", "[", "[", "Q", "[", "tsqii", "]", ",", "Q", "[", "tavgii", "]", "]", ",", "[", "Q", "[", "tavgii", "]", ",", "Q", "[", "sii", "]", "]", "]", ")", "return", "{", "'slope'", ":", "slope", ",", "'intercept'", ":", "intercept", ",", "'chisq'", ":", "chisq", ",", "'hessian'", ":", "estimator_hessian", ",", "'cov'", ":", "np", ".", "linalg", ".", "inv", "(", "estimator_hessian", ")", "}"], "docstring": "this function calculates the regression coefficients for a\n    given vector containing the averages of tip and branch\n    quantities.\n\n    Parameters\n    ----------\n    Q : numpy.array\n        vector with\n    slope : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description", "docstring_tokens": ["this", "function", "calculates", "the", "regression", "coefficients", "for", "a", "given", "vector", "containing", "the", "averages", "of", "tip", "and", "branch", "quantities", "."], "sha": "f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0", "url": "https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/treeregression.py#L6-L45", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/dockerhub.py", "func_name": "DockerHubClient.repository", "original_string": "def repository(self, owner, repository):\n        \"\"\"Fetch information about a repository.\"\"\"\n\n        url = urijoin(self.base_url, self.RREPOSITORY, owner, repository)\n\n        logger.debug(\"DockerHub client requests: %s\", url)\n\n        response = self.fetch(url)\n\n        return response.text", "language": "python", "code": "def repository(self, owner, repository):\n        \"\"\"Fetch information about a repository.\"\"\"\n\n        url = urijoin(self.base_url, self.RREPOSITORY, owner, repository)\n\n        logger.debug(\"DockerHub client requests: %s\", url)\n\n        response = self.fetch(url)\n\n        return response.text", "code_tokens": ["def", "repository", "(", "self", ",", "owner", ",", "repository", ")", ":", "url", "=", "urijoin", "(", "self", ".", "base_url", ",", "self", ".", "RREPOSITORY", ",", "owner", ",", "repository", ")", "logger", ".", "debug", "(", "\"DockerHub client requests: %s\"", ",", "url", ")", "response", "=", "self", ".", "fetch", "(", "url", ")", "return", "response", ".", "text"], "docstring": "Fetch information about a repository.", "docstring_tokens": ["Fetch", "information", "about", "a", "repository", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/dockerhub.py#L191-L200", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs_utils.py", "func_name": "_update_position", "original_string": "def _update_position(state,\n                     position_delta,\n                     next_objective,\n                     next_gradient,\n                     grad_tolerance,\n                     f_relative_tolerance,\n                     x_tolerance):\n  \"\"\"Updates the state advancing its position by a given position_delta.\"\"\"\n  failed = state.failed | ~tf.math.is_finite(next_objective) | ~tf.reduce_all(\n      input_tensor=tf.math.is_finite(next_gradient), axis=-1)\n\n  next_position = state.position + position_delta\n  converged = ~failed & _check_convergence(state.position,\n                                           next_position,\n                                           state.objective_value,\n                                           next_objective,\n                                           next_gradient,\n                                           grad_tolerance,\n                                           f_relative_tolerance,\n                                           x_tolerance)\n  return update_fields(\n      state,\n      converged=state.converged | converged,\n      failed=failed,\n      position=next_position,\n      objective_value=next_objective,\n      objective_gradient=next_gradient)", "language": "python", "code": "def _update_position(state,\n                     position_delta,\n                     next_objective,\n                     next_gradient,\n                     grad_tolerance,\n                     f_relative_tolerance,\n                     x_tolerance):\n  \"\"\"Updates the state advancing its position by a given position_delta.\"\"\"\n  failed = state.failed | ~tf.math.is_finite(next_objective) | ~tf.reduce_all(\n      input_tensor=tf.math.is_finite(next_gradient), axis=-1)\n\n  next_position = state.position + position_delta\n  converged = ~failed & _check_convergence(state.position,\n                                           next_position,\n                                           state.objective_value,\n                                           next_objective,\n                                           next_gradient,\n                                           grad_tolerance,\n                                           f_relative_tolerance,\n                                           x_tolerance)\n  return update_fields(\n      state,\n      converged=state.converged | converged,\n      failed=failed,\n      position=next_position,\n      objective_value=next_objective,\n      objective_gradient=next_gradient)", "code_tokens": ["def", "_update_position", "(", "state", ",", "position_delta", ",", "next_objective", ",", "next_gradient", ",", "grad_tolerance", ",", "f_relative_tolerance", ",", "x_tolerance", ")", ":", "failed", "=", "state", ".", "failed", "|", "~", "tf", ".", "math", ".", "is_finite", "(", "next_objective", ")", "|", "~", "tf", ".", "reduce_all", "(", "input_tensor", "=", "tf", ".", "math", ".", "is_finite", "(", "next_gradient", ")", ",", "axis", "=", "-", "1", ")", "next_position", "=", "state", ".", "position", "+", "position_delta", "converged", "=", "~", "failed", "&", "_check_convergence", "(", "state", ".", "position", ",", "next_position", ",", "state", ".", "objective_value", ",", "next_objective", ",", "next_gradient", ",", "grad_tolerance", ",", "f_relative_tolerance", ",", "x_tolerance", ")", "return", "update_fields", "(", "state", ",", "converged", "=", "state", ".", "converged", "|", "converged", ",", "failed", "=", "failed", ",", "position", "=", "next_position", ",", "objective_value", "=", "next_objective", ",", "objective_gradient", "=", "next_gradient", ")"], "docstring": "Updates the state advancing its position by a given position_delta.", "docstring_tokens": ["Updates", "the", "state", "advancing", "its", "position", "by", "a", "given", "position_delta", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs_utils.py#L258-L284", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "examples/example_24_large_scale_brian2_simulation/clusternet.py", "func_name": "CNConnections.build", "original_string": "def build(self, traj, brian_list, network_dict):\n        \"\"\"Builds the connections.\n\n        Build is only performed if connections have not\n        been pre-build.\n\n        :param traj: Trajectory container\n\n        :param brian_list:\n\n            List of objects passed to BRIAN network constructor.\n\n            Adds:\n\n            Connections, amount depends on clustering\n\n        :param network_dict:\n\n            Dictionary of elements shared among the components\n\n            Expects:\n\n            'neurons_i': Inhibitory neuron group\n\n            'neurons_e': Excitatory neuron group\n\n            Adds:\n\n            Connections, amount depends on clustering\n\n        \"\"\"\n        if not hasattr(self, '_pre_build') or not self._pre_build:\n            self._build_connections(traj, brian_list, network_dict)", "language": "python", "code": "def build(self, traj, brian_list, network_dict):\n        \"\"\"Builds the connections.\n\n        Build is only performed if connections have not\n        been pre-build.\n\n        :param traj: Trajectory container\n\n        :param brian_list:\n\n            List of objects passed to BRIAN network constructor.\n\n            Adds:\n\n            Connections, amount depends on clustering\n\n        :param network_dict:\n\n            Dictionary of elements shared among the components\n\n            Expects:\n\n            'neurons_i': Inhibitory neuron group\n\n            'neurons_e': Excitatory neuron group\n\n            Adds:\n\n            Connections, amount depends on clustering\n\n        \"\"\"\n        if not hasattr(self, '_pre_build') or not self._pre_build:\n            self._build_connections(traj, brian_list, network_dict)", "code_tokens": ["def", "build", "(", "self", ",", "traj", ",", "brian_list", ",", "network_dict", ")", ":", "if", "not", "hasattr", "(", "self", ",", "'_pre_build'", ")", "or", "not", "self", ".", "_pre_build", ":", "self", ".", "_build_connections", "(", "traj", ",", "brian_list", ",", "network_dict", ")"], "docstring": "Builds the connections.\n\n        Build is only performed if connections have not\n        been pre-build.\n\n        :param traj: Trajectory container\n\n        :param brian_list:\n\n            List of objects passed to BRIAN network constructor.\n\n            Adds:\n\n            Connections, amount depends on clustering\n\n        :param network_dict:\n\n            Dictionary of elements shared among the components\n\n            Expects:\n\n            'neurons_i': Inhibitory neuron group\n\n            'neurons_e': Excitatory neuron group\n\n            Adds:\n\n            Connections, amount depends on clustering", "docstring_tokens": ["Builds", "the", "connections", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/examples/example_24_large_scale_brian2_simulation/clusternet.py#L328-L360", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/lib/clipboard.py", "func_name": "tkinter_clipboard_get", "original_string": "def tkinter_clipboard_get():\n    \"\"\" Get the clipboard's text using Tkinter.\n\n    This is the default on systems that are not Windows or OS X. It may\n    interfere with other UI toolkits and should be replaced with an\n    implementation that uses that toolkit.\n    \"\"\"\n    try:\n        import Tkinter\n    except ImportError:\n        raise TryNext(\"Getting text from the clipboard on this platform \"\n                      \"requires Tkinter.\")\n    root = Tkinter.Tk()\n    root.withdraw()\n    text = root.clipboard_get()\n    root.destroy()\n    return text", "language": "python", "code": "def tkinter_clipboard_get():\n    \"\"\" Get the clipboard's text using Tkinter.\n\n    This is the default on systems that are not Windows or OS X. It may\n    interfere with other UI toolkits and should be replaced with an\n    implementation that uses that toolkit.\n    \"\"\"\n    try:\n        import Tkinter\n    except ImportError:\n        raise TryNext(\"Getting text from the clipboard on this platform \"\n                      \"requires Tkinter.\")\n    root = Tkinter.Tk()\n    root.withdraw()\n    text = root.clipboard_get()\n    root.destroy()\n    return text", "code_tokens": ["def", "tkinter_clipboard_get", "(", ")", ":", "try", ":", "import", "Tkinter", "except", "ImportError", ":", "raise", "TryNext", "(", "\"Getting text from the clipboard on this platform \"", "\"requires Tkinter.\"", ")", "root", "=", "Tkinter", ".", "Tk", "(", ")", "root", ".", "withdraw", "(", ")", "text", "=", "root", ".", "clipboard_get", "(", ")", "root", ".", "destroy", "(", ")", "return", "text"], "docstring": "Get the clipboard's text using Tkinter.\n\n    This is the default on systems that are not Windows or OS X. It may\n    interfere with other UI toolkits and should be replaced with an\n    implementation that uses that toolkit.", "docstring_tokens": ["Get", "the", "clipboard", "s", "text", "using", "Tkinter", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/lib/clipboard.py#L36-L52", "partition": "test"}
{"repo": "trec-kba/streamcorpus-pipeline", "path": "streamcorpus_pipeline/_kvlayer.py", "func_name": "streamitem_to_key_data", "original_string": "def streamitem_to_key_data(si):\n    '''\n    extract the parts of a StreamItem that go into a kvlayer key,\n    convert StreamItem to blob for storage.\n\n    return (kvlayer key tuple), data blob\n    '''\n    key = key_for_stream_item(si)\n    data = streamcorpus.serialize(si)\n    errors, data = streamcorpus.compress_and_encrypt(data)\n    assert not errors, errors\n    return key, data", "language": "python", "code": "def streamitem_to_key_data(si):\n    '''\n    extract the parts of a StreamItem that go into a kvlayer key,\n    convert StreamItem to blob for storage.\n\n    return (kvlayer key tuple), data blob\n    '''\n    key = key_for_stream_item(si)\n    data = streamcorpus.serialize(si)\n    errors, data = streamcorpus.compress_and_encrypt(data)\n    assert not errors, errors\n    return key, data", "code_tokens": ["def", "streamitem_to_key_data", "(", "si", ")", ":", "key", "=", "key_for_stream_item", "(", "si", ")", "data", "=", "streamcorpus", ".", "serialize", "(", "si", ")", "errors", ",", "data", "=", "streamcorpus", ".", "compress_and_encrypt", "(", "data", ")", "assert", "not", "errors", ",", "errors", "return", "key", ",", "data"], "docstring": "extract the parts of a StreamItem that go into a kvlayer key,\n    convert StreamItem to blob for storage.\n\n    return (kvlayer key tuple), data blob", "docstring_tokens": ["extract", "the", "parts", "of", "a", "StreamItem", "that", "go", "into", "a", "kvlayer", "key", "convert", "StreamItem", "to", "blob", "for", "storage", "."], "sha": "8bb82ea1beb83c6b40ed03fa1659df2897c2292a", "url": "https://github.com/trec-kba/streamcorpus-pipeline/blob/8bb82ea1beb83c6b40ed03fa1659df2897c2292a/streamcorpus_pipeline/_kvlayer.py#L411-L422", "partition": "test"}
{"repo": "Toblerity/rtree", "path": "rtree/index.py", "func_name": "Index.count", "original_string": "def count(self, coordinates):\n        \"\"\"Return number of objects that intersect the given coordinates.\n\n        :param coordinates: sequence or array\n            This may be an object that satisfies the numpy array\n            protocol, providing the index's dimension * 2 coordinate\n            pairs representing the `mink` and `maxk` coordinates in\n            each dimension defining the bounds of the query window.\n\n        The following example queries the index for any objects any objects\n        that were stored in the index intersect the bounds given in the\n        coordinates::\n\n            >>> from rtree import index\n            >>> idx = index.Index()\n            >>> idx.insert(4321,\n            ...            (34.3776829412, 26.7375853734, 49.3776829412,\n            ...             41.7375853734),\n            ...            obj=42)\n\n            >>> print(idx.count((0, 0, 60, 60)))\n            1\n\n        \"\"\"\n        p_mins, p_maxs = self.get_coordinate_pointers(coordinates)\n\n        p_num_results = ctypes.c_uint64(0)\n\n        core.rt.Index_Intersects_count(self.handle,\n                                       p_mins,\n                                       p_maxs,\n                                       self.properties.dimension,\n                                       ctypes.byref(p_num_results))\n\n        return p_num_results.value", "language": "python", "code": "def count(self, coordinates):\n        \"\"\"Return number of objects that intersect the given coordinates.\n\n        :param coordinates: sequence or array\n            This may be an object that satisfies the numpy array\n            protocol, providing the index's dimension * 2 coordinate\n            pairs representing the `mink` and `maxk` coordinates in\n            each dimension defining the bounds of the query window.\n\n        The following example queries the index for any objects any objects\n        that were stored in the index intersect the bounds given in the\n        coordinates::\n\n            >>> from rtree import index\n            >>> idx = index.Index()\n            >>> idx.insert(4321,\n            ...            (34.3776829412, 26.7375853734, 49.3776829412,\n            ...             41.7375853734),\n            ...            obj=42)\n\n            >>> print(idx.count((0, 0, 60, 60)))\n            1\n\n        \"\"\"\n        p_mins, p_maxs = self.get_coordinate_pointers(coordinates)\n\n        p_num_results = ctypes.c_uint64(0)\n\n        core.rt.Index_Intersects_count(self.handle,\n                                       p_mins,\n                                       p_maxs,\n                                       self.properties.dimension,\n                                       ctypes.byref(p_num_results))\n\n        return p_num_results.value", "code_tokens": ["def", "count", "(", "self", ",", "coordinates", ")", ":", "p_mins", ",", "p_maxs", "=", "self", ".", "get_coordinate_pointers", "(", "coordinates", ")", "p_num_results", "=", "ctypes", ".", "c_uint64", "(", "0", ")", "core", ".", "rt", ".", "Index_Intersects_count", "(", "self", ".", "handle", ",", "p_mins", ",", "p_maxs", ",", "self", ".", "properties", ".", "dimension", ",", "ctypes", ".", "byref", "(", "p_num_results", ")", ")", "return", "p_num_results", ".", "value"], "docstring": "Return number of objects that intersect the given coordinates.\n\n        :param coordinates: sequence or array\n            This may be an object that satisfies the numpy array\n            protocol, providing the index's dimension * 2 coordinate\n            pairs representing the `mink` and `maxk` coordinates in\n            each dimension defining the bounds of the query window.\n\n        The following example queries the index for any objects any objects\n        that were stored in the index intersect the bounds given in the\n        coordinates::\n\n            >>> from rtree import index\n            >>> idx = index.Index()\n            >>> idx.insert(4321,\n            ...            (34.3776829412, 26.7375853734, 49.3776829412,\n            ...             41.7375853734),\n            ...            obj=42)\n\n            >>> print(idx.count((0, 0, 60, 60)))\n            1", "docstring_tokens": ["Return", "number", "of", "objects", "that", "intersect", "the", "given", "coordinates", "."], "sha": "5d33357c8e88f1a8344415dc15a7d2440211b281", "url": "https://github.com/Toblerity/rtree/blob/5d33357c8e88f1a8344415dc15a7d2440211b281/rtree/index.py#L396-L430", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/completion_html.py", "func_name": "CompletionHtml.select_down", "original_string": "def select_down(self):\n        \"\"\"move cursor down\"\"\"\n        r, c = self._index\n        self._select_index(r+1, c)", "language": "python", "code": "def select_down(self):\n        \"\"\"move cursor down\"\"\"\n        r, c = self._index\n        self._select_index(r+1, c)", "code_tokens": ["def", "select_down", "(", "self", ")", ":", "r", ",", "c", "=", "self", ".", "_index", "self", ".", "_select_index", "(", "r", "+", "1", ",", "c", ")"], "docstring": "move cursor down", "docstring_tokens": ["move", "cursor", "down"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/completion_html.py#L287-L290", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/_vendor/nvd3/NVD3Chart.py", "func_name": "_main", "original_string": "def _main():\n    \"\"\"\n    Parse options and process commands\n    \"\"\"\n    # Parse arguments\n    usage = \"usage: nvd3.py [options]\"\n    parser = OptionParser(usage=usage,\n                          version=(\"python-nvd3 - Charts generator with \"\n                                   \"nvd3.js and d3.js\"))\n    parser.add_option(\"-q\", \"--quiet\",\n                      action=\"store_false\", dest=\"verbose\", default=True,\n                      help=\"don't print messages to stdout\")\n\n    (options, args) = parser.parse_args()", "language": "python", "code": "def _main():\n    \"\"\"\n    Parse options and process commands\n    \"\"\"\n    # Parse arguments\n    usage = \"usage: nvd3.py [options]\"\n    parser = OptionParser(usage=usage,\n                          version=(\"python-nvd3 - Charts generator with \"\n                                   \"nvd3.js and d3.js\"))\n    parser.add_option(\"-q\", \"--quiet\",\n                      action=\"store_false\", dest=\"verbose\", default=True,\n                      help=\"don't print messages to stdout\")\n\n    (options, args) = parser.parse_args()", "code_tokens": ["def", "_main", "(", ")", ":", "# Parse arguments", "usage", "=", "\"usage: nvd3.py [options]\"", "parser", "=", "OptionParser", "(", "usage", "=", "usage", ",", "version", "=", "(", "\"python-nvd3 - Charts generator with \"", "\"nvd3.js and d3.js\"", ")", ")", "parser", ".", "add_option", "(", "\"-q\"", ",", "\"--quiet\"", ",", "action", "=", "\"store_false\"", ",", "dest", "=", "\"verbose\"", ",", "default", "=", "True", ",", "help", "=", "\"don't print messages to stdout\"", ")", "(", "options", ",", "args", ")", "=", "parser", ".", "parse_args", "(", ")"], "docstring": "Parse options and process commands", "docstring_tokens": ["Parse", "options", "and", "process", "commands"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/_vendor/nvd3/NVD3Chart.py#L488-L501", "partition": "test"}
{"repo": "funilrys/PyFunceble", "path": "PyFunceble/core.py", "func_name": "Core._file_decision", "original_string": "def _file_decision(self, current, last, status=None):\n        \"\"\"\n        Manage the database, autosave and autocontinue systems for the case that we are reading\n        a file.\n\n        :param current: The currently tested element.\n        :type current: str\n\n        :param last: The last element of the list.\n        :type last: str\n\n        :param status: The status of the currently tested element.\n        :type status: str\n        \"\"\"\n\n        if (\n            status\n            and not PyFunceble.CONFIGURATION[\"simple\"]\n            and PyFunceble.INTERN[\"file_to_test\"]\n        ):\n            # * The status is given.\n            # and\n            # * The simple mode is deactivated.\n            # and\n            # * A file to test is set.\n\n            # We run the mining logic.\n            self.mining.process()\n\n            # We delete the currently tested element from the mining\n            # database.\n            # Indeed, as it is tested, it is already in our\n            # testing process which means that we don't need it into\n            # the mining database.\n            self.mining.remove()\n\n            if (\n                status.lower() in PyFunceble.STATUS[\"list\"][\"up\"]\n                or status.lower() in PyFunceble.STATUS[\"list\"][\"valid\"]\n            ):\n                # The status is in the list of up status.\n\n                if self.inactive_database.is_present():\n                    # The currently tested element is in the database.\n\n                    # We generate the suspicious file(s).\n                    Generate(PyFunceble.STATUS[\"official\"][\"up\"]).analytic_file(\n                        \"suspicious\"\n                    )\n\n                    # We remove the currently tested element from the\n                    # database.\n                    self.inactive_database.remove()\n\n            else:\n                # The status is not in the list of up status.\n\n                # We add the currently tested element to the\n                # database.\n                self.inactive_database.add()\n\n            # We backup the current state of the file reading\n            # for the case that we need to continue later.\n            self.auto_continue.backup()\n\n            if current != last:\n                # The current element is not the last one.\n\n                # We run the autosave logic.\n                AutoSave()\n            else:\n                # The current element is the last one.\n\n                # We stop and log the execution time.\n                ExecutionTime(\"stop\", last=True)\n\n                # We show/log the percentage.\n                self.percentage.log()\n\n                # We reset the counters as we end the process.\n                self.reset_counters()\n\n                # We backup the current state of the file reading\n                # for the case that we need to continue later.\n                self.auto_continue.backup()\n\n                # We show the colored logo.\n                self.colorify_logo()\n\n                # We save and stop the script if we are under\n                # Travis CI.\n                AutoSave(True)\n\n        for index in [\"http_code\", \"referer\"]:\n            # We loop through some configuration index we have to empty.\n\n            if index in PyFunceble.INTERN:\n                # The index is in the configuration.\n\n                # We empty the configuration index.\n                PyFunceble.INTERN[index] = \"\"", "language": "python", "code": "def _file_decision(self, current, last, status=None):\n        \"\"\"\n        Manage the database, autosave and autocontinue systems for the case that we are reading\n        a file.\n\n        :param current: The currently tested element.\n        :type current: str\n\n        :param last: The last element of the list.\n        :type last: str\n\n        :param status: The status of the currently tested element.\n        :type status: str\n        \"\"\"\n\n        if (\n            status\n            and not PyFunceble.CONFIGURATION[\"simple\"]\n            and PyFunceble.INTERN[\"file_to_test\"]\n        ):\n            # * The status is given.\n            # and\n            # * The simple mode is deactivated.\n            # and\n            # * A file to test is set.\n\n            # We run the mining logic.\n            self.mining.process()\n\n            # We delete the currently tested element from the mining\n            # database.\n            # Indeed, as it is tested, it is already in our\n            # testing process which means that we don't need it into\n            # the mining database.\n            self.mining.remove()\n\n            if (\n                status.lower() in PyFunceble.STATUS[\"list\"][\"up\"]\n                or status.lower() in PyFunceble.STATUS[\"list\"][\"valid\"]\n            ):\n                # The status is in the list of up status.\n\n                if self.inactive_database.is_present():\n                    # The currently tested element is in the database.\n\n                    # We generate the suspicious file(s).\n                    Generate(PyFunceble.STATUS[\"official\"][\"up\"]).analytic_file(\n                        \"suspicious\"\n                    )\n\n                    # We remove the currently tested element from the\n                    # database.\n                    self.inactive_database.remove()\n\n            else:\n                # The status is not in the list of up status.\n\n                # We add the currently tested element to the\n                # database.\n                self.inactive_database.add()\n\n            # We backup the current state of the file reading\n            # for the case that we need to continue later.\n            self.auto_continue.backup()\n\n            if current != last:\n                # The current element is not the last one.\n\n                # We run the autosave logic.\n                AutoSave()\n            else:\n                # The current element is the last one.\n\n                # We stop and log the execution time.\n                ExecutionTime(\"stop\", last=True)\n\n                # We show/log the percentage.\n                self.percentage.log()\n\n                # We reset the counters as we end the process.\n                self.reset_counters()\n\n                # We backup the current state of the file reading\n                # for the case that we need to continue later.\n                self.auto_continue.backup()\n\n                # We show the colored logo.\n                self.colorify_logo()\n\n                # We save and stop the script if we are under\n                # Travis CI.\n                AutoSave(True)\n\n        for index in [\"http_code\", \"referer\"]:\n            # We loop through some configuration index we have to empty.\n\n            if index in PyFunceble.INTERN:\n                # The index is in the configuration.\n\n                # We empty the configuration index.\n                PyFunceble.INTERN[index] = \"\"", "code_tokens": ["def", "_file_decision", "(", "self", ",", "current", ",", "last", ",", "status", "=", "None", ")", ":", "if", "(", "status", "and", "not", "PyFunceble", ".", "CONFIGURATION", "[", "\"simple\"", "]", "and", "PyFunceble", ".", "INTERN", "[", "\"file_to_test\"", "]", ")", ":", "# * The status is given.", "# and", "# * The simple mode is deactivated.", "# and", "# * A file to test is set.", "# We run the mining logic.", "self", ".", "mining", ".", "process", "(", ")", "# We delete the currently tested element from the mining", "# database.", "# Indeed, as it is tested, it is already in our", "# testing process which means that we don't need it into", "# the mining database.", "self", ".", "mining", ".", "remove", "(", ")", "if", "(", "status", ".", "lower", "(", ")", "in", "PyFunceble", ".", "STATUS", "[", "\"list\"", "]", "[", "\"up\"", "]", "or", "status", ".", "lower", "(", ")", "in", "PyFunceble", ".", "STATUS", "[", "\"list\"", "]", "[", "\"valid\"", "]", ")", ":", "# The status is in the list of up status.", "if", "self", ".", "inactive_database", ".", "is_present", "(", ")", ":", "# The currently tested element is in the database.", "# We generate the suspicious file(s).", "Generate", "(", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"up\"", "]", ")", ".", "analytic_file", "(", "\"suspicious\"", ")", "# We remove the currently tested element from the", "# database.", "self", ".", "inactive_database", ".", "remove", "(", ")", "else", ":", "# The status is not in the list of up status.", "# We add the currently tested element to the", "# database.", "self", ".", "inactive_database", ".", "add", "(", ")", "# We backup the current state of the file reading", "# for the case that we need to continue later.", "self", ".", "auto_continue", ".", "backup", "(", ")", "if", "current", "!=", "last", ":", "# The current element is not the last one.", "# We run the autosave logic.", "AutoSave", "(", ")", "else", ":", "# The current element is the last one.", "# We stop and log the execution time.", "ExecutionTime", "(", "\"stop\"", ",", "last", "=", "True", ")", "# We show/log the percentage.", "self", ".", "percentage", ".", "log", "(", ")", "# We reset the counters as we end the process.", "self", ".", "reset_counters", "(", ")", "# We backup the current state of the file reading", "# for the case that we need to continue later.", "self", ".", "auto_continue", ".", "backup", "(", ")", "# We show the colored logo.", "self", ".", "colorify_logo", "(", ")", "# We save and stop the script if we are under", "# Travis CI.", "AutoSave", "(", "True", ")", "for", "index", "in", "[", "\"http_code\"", ",", "\"referer\"", "]", ":", "# We loop through some configuration index we have to empty.", "if", "index", "in", "PyFunceble", ".", "INTERN", ":", "# The index is in the configuration.", "# We empty the configuration index.", "PyFunceble", ".", "INTERN", "[", "index", "]", "=", "\"\""], "docstring": "Manage the database, autosave and autocontinue systems for the case that we are reading\n        a file.\n\n        :param current: The currently tested element.\n        :type current: str\n\n        :param last: The last element of the list.\n        :type last: str\n\n        :param status: The status of the currently tested element.\n        :type status: str", "docstring_tokens": ["Manage", "the", "database", "autosave", "and", "autocontinue", "systems", "for", "the", "case", "that", "we", "are", "reading", "a", "file", "."], "sha": "cdf69cbde120199171f7158e1c33635753e6e2f5", "url": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/core.py#L617-L717", "partition": "test"}
{"repo": "pytorch/vision", "path": "torchvision/datasets/utils.py", "func_name": "download_file_from_google_drive", "original_string": "def download_file_from_google_drive(file_id, root, filename=None, md5=None):\n    \"\"\"Download a Google Drive file from  and place it in root.\n\n    Args:\n        file_id (str): id of file to be downloaded\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the id of the file.\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    \"\"\"\n    # Based on https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n    import requests\n    url = \"https://docs.google.com/uc?export=download\"\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = file_id\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print('Using downloaded and verified file: ' + fpath)\n    else:\n        session = requests.Session()\n\n        response = session.get(url, params={'id': file_id}, stream=True)\n        token = _get_confirm_token(response)\n\n        if token:\n            params = {'id': file_id, 'confirm': token}\n            response = session.get(url, params=params, stream=True)\n\n        _save_response_content(response, fpath)", "language": "python", "code": "def download_file_from_google_drive(file_id, root, filename=None, md5=None):\n    \"\"\"Download a Google Drive file from  and place it in root.\n\n    Args:\n        file_id (str): id of file to be downloaded\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the id of the file.\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    \"\"\"\n    # Based on https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n    import requests\n    url = \"https://docs.google.com/uc?export=download\"\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = file_id\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print('Using downloaded and verified file: ' + fpath)\n    else:\n        session = requests.Session()\n\n        response = session.get(url, params={'id': file_id}, stream=True)\n        token = _get_confirm_token(response)\n\n        if token:\n            params = {'id': file_id, 'confirm': token}\n            response = session.get(url, params=params, stream=True)\n\n        _save_response_content(response, fpath)", "code_tokens": ["def", "download_file_from_google_drive", "(", "file_id", ",", "root", ",", "filename", "=", "None", ",", "md5", "=", "None", ")", ":", "# Based on https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url", "import", "requests", "url", "=", "\"https://docs.google.com/uc?export=download\"", "root", "=", "os", ".", "path", ".", "expanduser", "(", "root", ")", "if", "not", "filename", ":", "filename", "=", "file_id", "fpath", "=", "os", ".", "path", ".", "join", "(", "root", ",", "filename", ")", "makedir_exist_ok", "(", "root", ")", "if", "os", ".", "path", ".", "isfile", "(", "fpath", ")", "and", "check_integrity", "(", "fpath", ",", "md5", ")", ":", "print", "(", "'Using downloaded and verified file: '", "+", "fpath", ")", "else", ":", "session", "=", "requests", ".", "Session", "(", ")", "response", "=", "session", ".", "get", "(", "url", ",", "params", "=", "{", "'id'", ":", "file_id", "}", ",", "stream", "=", "True", ")", "token", "=", "_get_confirm_token", "(", "response", ")", "if", "token", ":", "params", "=", "{", "'id'", ":", "file_id", ",", "'confirm'", ":", "token", "}", "response", "=", "session", ".", "get", "(", "url", ",", "params", "=", "params", ",", "stream", "=", "True", ")", "_save_response_content", "(", "response", ",", "fpath", ")"], "docstring": "Download a Google Drive file from  and place it in root.\n\n    Args:\n        file_id (str): id of file to be downloaded\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the id of the file.\n        md5 (str, optional): MD5 checksum of the download. If None, do not check", "docstring_tokens": ["Download", "a", "Google", "Drive", "file", "from", "and", "place", "it", "in", "root", "."], "sha": "3afcf3cd49661c466c75ea536b0b2a7ff57f9a05", "url": "https://github.com/pytorch/vision/blob/3afcf3cd49661c466c75ea536b0b2a7ff57f9a05/torchvision/datasets/utils.py#L139-L171", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/core/messages/payloads/create_key_pair.py", "func_name": "CreateKeyPairRequestPayload.read", "original_string": "def read(self, input_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the CreateKeyPair request payload and decode it\n        into its constituent parts.\n\n        Args:\n            input_buffer (stream): A data buffer containing encoded object\n                data, supporting a read method.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"\n        super(CreateKeyPairRequestPayload, self).read(\n            input_buffer,\n            kmip_version=kmip_version\n        )\n        local_buffer = utils.BytearrayStream(input_buffer.read(self.length))\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            if self.is_tag_next(\n                    enums.Tags.COMMON_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._common_template_attribute = objects.TemplateAttribute(\n                    tag=enums.Tags.COMMON_TEMPLATE_ATTRIBUTE\n                )\n                self._common_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n        else:\n            if self.is_tag_next(enums.Tags.COMMON_ATTRIBUTES, local_buffer):\n                attributes = objects.Attributes(\n                    tag=enums.Tags.COMMON_ATTRIBUTES\n                )\n                attributes.read(local_buffer, kmip_version=kmip_version)\n                self._common_template_attribute = \\\n                    objects.convert_attributes_to_template_attribute(\n                        attributes\n                    )\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            if self.is_tag_next(\n                    enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._private_key_template_attribute = \\\n                    objects.TemplateAttribute(\n                        tag=enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE\n                    )\n                self._private_key_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n        else:\n            if self.is_tag_next(\n                    enums.Tags.PRIVATE_KEY_ATTRIBUTES,\n                    local_buffer\n            ):\n                attributes = objects.Attributes(\n                    tag=enums.Tags.PRIVATE_KEY_ATTRIBUTES\n                )\n                attributes.read(local_buffer, kmip_version=kmip_version)\n                self._private_key_template_attribute = \\\n                    objects.convert_attributes_to_template_attribute(\n                        attributes\n                    )\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            if self.is_tag_next(\n                    enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._public_key_template_attribute = \\\n                    objects.TemplateAttribute(\n                        tag=enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE\n                    )\n                self._public_key_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n        else:\n            if self.is_tag_next(\n                    enums.Tags.PUBLIC_KEY_ATTRIBUTES,\n                    local_buffer\n            ):\n                attributes = objects.Attributes(\n                    tag=enums.Tags.PUBLIC_KEY_ATTRIBUTES\n                )\n                attributes.read(local_buffer, kmip_version=kmip_version)\n                self._public_key_template_attribute = \\\n                    objects.convert_attributes_to_template_attribute(\n                        attributes\n                    )\n\n        self.is_oversized(local_buffer)", "language": "python", "code": "def read(self, input_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the CreateKeyPair request payload and decode it\n        into its constituent parts.\n\n        Args:\n            input_buffer (stream): A data buffer containing encoded object\n                data, supporting a read method.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"\n        super(CreateKeyPairRequestPayload, self).read(\n            input_buffer,\n            kmip_version=kmip_version\n        )\n        local_buffer = utils.BytearrayStream(input_buffer.read(self.length))\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            if self.is_tag_next(\n                    enums.Tags.COMMON_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._common_template_attribute = objects.TemplateAttribute(\n                    tag=enums.Tags.COMMON_TEMPLATE_ATTRIBUTE\n                )\n                self._common_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n        else:\n            if self.is_tag_next(enums.Tags.COMMON_ATTRIBUTES, local_buffer):\n                attributes = objects.Attributes(\n                    tag=enums.Tags.COMMON_ATTRIBUTES\n                )\n                attributes.read(local_buffer, kmip_version=kmip_version)\n                self._common_template_attribute = \\\n                    objects.convert_attributes_to_template_attribute(\n                        attributes\n                    )\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            if self.is_tag_next(\n                    enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._private_key_template_attribute = \\\n                    objects.TemplateAttribute(\n                        tag=enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE\n                    )\n                self._private_key_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n        else:\n            if self.is_tag_next(\n                    enums.Tags.PRIVATE_KEY_ATTRIBUTES,\n                    local_buffer\n            ):\n                attributes = objects.Attributes(\n                    tag=enums.Tags.PRIVATE_KEY_ATTRIBUTES\n                )\n                attributes.read(local_buffer, kmip_version=kmip_version)\n                self._private_key_template_attribute = \\\n                    objects.convert_attributes_to_template_attribute(\n                        attributes\n                    )\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            if self.is_tag_next(\n                    enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._public_key_template_attribute = \\\n                    objects.TemplateAttribute(\n                        tag=enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE\n                    )\n                self._public_key_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n        else:\n            if self.is_tag_next(\n                    enums.Tags.PUBLIC_KEY_ATTRIBUTES,\n                    local_buffer\n            ):\n                attributes = objects.Attributes(\n                    tag=enums.Tags.PUBLIC_KEY_ATTRIBUTES\n                )\n                attributes.read(local_buffer, kmip_version=kmip_version)\n                self._public_key_template_attribute = \\\n                    objects.convert_attributes_to_template_attribute(\n                        attributes\n                    )\n\n        self.is_oversized(local_buffer)", "code_tokens": ["def", "read", "(", "self", ",", "input_buffer", ",", "kmip_version", "=", "enums", ".", "KMIPVersion", ".", "KMIP_1_0", ")", ":", "super", "(", "CreateKeyPairRequestPayload", ",", "self", ")", ".", "read", "(", "input_buffer", ",", "kmip_version", "=", "kmip_version", ")", "local_buffer", "=", "utils", ".", "BytearrayStream", "(", "input_buffer", ".", "read", "(", "self", ".", "length", ")", ")", "if", "kmip_version", "<", "enums", ".", "KMIPVersion", ".", "KMIP_2_0", ":", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "COMMON_TEMPLATE_ATTRIBUTE", ",", "local_buffer", ")", ":", "self", ".", "_common_template_attribute", "=", "objects", ".", "TemplateAttribute", "(", "tag", "=", "enums", ".", "Tags", ".", "COMMON_TEMPLATE_ATTRIBUTE", ")", "self", ".", "_common_template_attribute", ".", "read", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "else", ":", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "COMMON_ATTRIBUTES", ",", "local_buffer", ")", ":", "attributes", "=", "objects", ".", "Attributes", "(", "tag", "=", "enums", ".", "Tags", ".", "COMMON_ATTRIBUTES", ")", "attributes", ".", "read", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "self", ".", "_common_template_attribute", "=", "objects", ".", "convert_attributes_to_template_attribute", "(", "attributes", ")", "if", "kmip_version", "<", "enums", ".", "KMIPVersion", ".", "KMIP_2_0", ":", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "PRIVATE_KEY_TEMPLATE_ATTRIBUTE", ",", "local_buffer", ")", ":", "self", ".", "_private_key_template_attribute", "=", "objects", ".", "TemplateAttribute", "(", "tag", "=", "enums", ".", "Tags", ".", "PRIVATE_KEY_TEMPLATE_ATTRIBUTE", ")", "self", ".", "_private_key_template_attribute", ".", "read", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "else", ":", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "PRIVATE_KEY_ATTRIBUTES", ",", "local_buffer", ")", ":", "attributes", "=", "objects", ".", "Attributes", "(", "tag", "=", "enums", ".", "Tags", ".", "PRIVATE_KEY_ATTRIBUTES", ")", "attributes", ".", "read", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "self", ".", "_private_key_template_attribute", "=", "objects", ".", "convert_attributes_to_template_attribute", "(", "attributes", ")", "if", "kmip_version", "<", "enums", ".", "KMIPVersion", ".", "KMIP_2_0", ":", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "PUBLIC_KEY_TEMPLATE_ATTRIBUTE", ",", "local_buffer", ")", ":", "self", ".", "_public_key_template_attribute", "=", "objects", ".", "TemplateAttribute", "(", "tag", "=", "enums", ".", "Tags", ".", "PUBLIC_KEY_TEMPLATE_ATTRIBUTE", ")", "self", ".", "_public_key_template_attribute", ".", "read", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "else", ":", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "PUBLIC_KEY_ATTRIBUTES", ",", "local_buffer", ")", ":", "attributes", "=", "objects", ".", "Attributes", "(", "tag", "=", "enums", ".", "Tags", ".", "PUBLIC_KEY_ATTRIBUTES", ")", "attributes", ".", "read", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "self", ".", "_public_key_template_attribute", "=", "objects", ".", "convert_attributes_to_template_attribute", "(", "attributes", ")", "self", ".", "is_oversized", "(", "local_buffer", ")"], "docstring": "Read the data encoding the CreateKeyPair request payload and decode it\n        into its constituent parts.\n\n        Args:\n            input_buffer (stream): A data buffer containing encoded object\n                data, supporting a read method.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.", "docstring_tokens": ["Read", "the", "data", "encoding", "the", "CreateKeyPair", "request", "payload", "and", "decode", "it", "into", "its", "constituent", "parts", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/core/messages/payloads/create_key_pair.py#L139-L234", "partition": "test"}
{"repo": "LionelAuroux/pyrser", "path": "pyrser/dsl.py", "func_name": "add_alt", "original_string": "def add_alt(self, alternatives, alt) -> bool:\n    \"\"\"Create a tree.Alt\"\"\"\n    if not hasattr(alternatives, 'parser_tree'):\n        # forward sublevel of alt as is\n        if hasattr(alt, 'parser_tree'):\n            alternatives.parser_tree = alt.parser_tree\n        else:\n            alternatives.parser_tree = alt\n    else:\n        oldnode = alternatives\n        if isinstance(oldnode.parser_tree, parsing.Alt):\n            oldpt = list(oldnode.parser_tree.ptlist)\n        else:\n            oldpt = [oldnode.parser_tree]\n        oldpt.append(alt.parser_tree)\n        alternatives.parser_tree = parsing.Alt(*tuple(oldpt))\n    return True", "language": "python", "code": "def add_alt(self, alternatives, alt) -> bool:\n    \"\"\"Create a tree.Alt\"\"\"\n    if not hasattr(alternatives, 'parser_tree'):\n        # forward sublevel of alt as is\n        if hasattr(alt, 'parser_tree'):\n            alternatives.parser_tree = alt.parser_tree\n        else:\n            alternatives.parser_tree = alt\n    else:\n        oldnode = alternatives\n        if isinstance(oldnode.parser_tree, parsing.Alt):\n            oldpt = list(oldnode.parser_tree.ptlist)\n        else:\n            oldpt = [oldnode.parser_tree]\n        oldpt.append(alt.parser_tree)\n        alternatives.parser_tree = parsing.Alt(*tuple(oldpt))\n    return True", "code_tokens": ["def", "add_alt", "(", "self", ",", "alternatives", ",", "alt", ")", "->", "bool", ":", "if", "not", "hasattr", "(", "alternatives", ",", "'parser_tree'", ")", ":", "# forward sublevel of alt as is", "if", "hasattr", "(", "alt", ",", "'parser_tree'", ")", ":", "alternatives", ".", "parser_tree", "=", "alt", ".", "parser_tree", "else", ":", "alternatives", ".", "parser_tree", "=", "alt", "else", ":", "oldnode", "=", "alternatives", "if", "isinstance", "(", "oldnode", ".", "parser_tree", ",", "parsing", ".", "Alt", ")", ":", "oldpt", "=", "list", "(", "oldnode", ".", "parser_tree", ".", "ptlist", ")", "else", ":", "oldpt", "=", "[", "oldnode", ".", "parser_tree", "]", "oldpt", ".", "append", "(", "alt", ".", "parser_tree", ")", "alternatives", ".", "parser_tree", "=", "parsing", ".", "Alt", "(", "*", "tuple", "(", "oldpt", ")", ")", "return", "True"], "docstring": "Create a tree.Alt", "docstring_tokens": ["Create", "a", "tree", ".", "Alt"], "sha": "f153a97ef2b6bf915a1ed468c0252a9a59b754d5", "url": "https://github.com/LionelAuroux/pyrser/blob/f153a97ef2b6bf915a1ed468c0252a9a59b754d5/pyrser/dsl.py#L549-L565", "partition": "test"}
{"repo": "katerina7479/pypdflite", "path": "pypdflite/font_loader.py", "func_name": "FontLoader.get_ttf", "original_string": "def get_ttf(self):\n        \"\"\" Given a search path, find file with requested extension \"\"\"\n        font_dict = {}\n        families = []\n        rootdirlist = string.split(self.search_path, os.pathsep)\n\n        #for rootdir in rootdirlist:\n        #    rootdir = os.path.expanduser(rootdir)\n        for dirName, subdirList, filelist in itertools.chain.from_iterable(os.walk(path) for path in rootdirlist):\n            for item in filelist:\n                root, ext = os.path.splitext(item)\n                if ext == '.ttf':\n                    if root[0].lower() in english:\n                        source = os.path.join(dirName, item)\n                        name = root.lower().replace('_', ' ')\n                        if ' bold' in name:\n                            name = name.replace(' bold', '_bold')\n                            if ' italic' in name:\n                                name = name.replace(' italic', '_italic')\n                        elif 'bold' in name:\n                            name = name.replace('bold', '_bold')\n                            if 'italic' in name:\n                                name = name.replace('italic', '_italic')\n                        elif ' italic' in name:\n                            name = name.replace(' italic', '_italic')\n                        elif 'italic' in name:\n                            name = name.replace('italic', '_italic')\n                        elif 'oblique' in name:\n                            name = name.replace('oblique', '_italic')\n                        else:\n                            families.append(name)\n                        font_dict[name] = source\n                    else:\n                        source = os.path.join(dirName, item)\n                        name = root.lower().replace('_', ' ')\n                        font_dict[name] = source\n                        families.append(name)\n        self.font_dict = font_dict\n        self.families = families", "language": "python", "code": "def get_ttf(self):\n        \"\"\" Given a search path, find file with requested extension \"\"\"\n        font_dict = {}\n        families = []\n        rootdirlist = string.split(self.search_path, os.pathsep)\n\n        #for rootdir in rootdirlist:\n        #    rootdir = os.path.expanduser(rootdir)\n        for dirName, subdirList, filelist in itertools.chain.from_iterable(os.walk(path) for path in rootdirlist):\n            for item in filelist:\n                root, ext = os.path.splitext(item)\n                if ext == '.ttf':\n                    if root[0].lower() in english:\n                        source = os.path.join(dirName, item)\n                        name = root.lower().replace('_', ' ')\n                        if ' bold' in name:\n                            name = name.replace(' bold', '_bold')\n                            if ' italic' in name:\n                                name = name.replace(' italic', '_italic')\n                        elif 'bold' in name:\n                            name = name.replace('bold', '_bold')\n                            if 'italic' in name:\n                                name = name.replace('italic', '_italic')\n                        elif ' italic' in name:\n                            name = name.replace(' italic', '_italic')\n                        elif 'italic' in name:\n                            name = name.replace('italic', '_italic')\n                        elif 'oblique' in name:\n                            name = name.replace('oblique', '_italic')\n                        else:\n                            families.append(name)\n                        font_dict[name] = source\n                    else:\n                        source = os.path.join(dirName, item)\n                        name = root.lower().replace('_', ' ')\n                        font_dict[name] = source\n                        families.append(name)\n        self.font_dict = font_dict\n        self.families = families", "code_tokens": ["def", "get_ttf", "(", "self", ")", ":", "font_dict", "=", "{", "}", "families", "=", "[", "]", "rootdirlist", "=", "string", ".", "split", "(", "self", ".", "search_path", ",", "os", ".", "pathsep", ")", "#for rootdir in rootdirlist:", "#    rootdir = os.path.expanduser(rootdir)", "for", "dirName", ",", "subdirList", ",", "filelist", "in", "itertools", ".", "chain", ".", "from_iterable", "(", "os", ".", "walk", "(", "path", ")", "for", "path", "in", "rootdirlist", ")", ":", "for", "item", "in", "filelist", ":", "root", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "item", ")", "if", "ext", "==", "'.ttf'", ":", "if", "root", "[", "0", "]", ".", "lower", "(", ")", "in", "english", ":", "source", "=", "os", ".", "path", ".", "join", "(", "dirName", ",", "item", ")", "name", "=", "root", ".", "lower", "(", ")", ".", "replace", "(", "'_'", ",", "' '", ")", "if", "' bold'", "in", "name", ":", "name", "=", "name", ".", "replace", "(", "' bold'", ",", "'_bold'", ")", "if", "' italic'", "in", "name", ":", "name", "=", "name", ".", "replace", "(", "' italic'", ",", "'_italic'", ")", "elif", "'bold'", "in", "name", ":", "name", "=", "name", ".", "replace", "(", "'bold'", ",", "'_bold'", ")", "if", "'italic'", "in", "name", ":", "name", "=", "name", ".", "replace", "(", "'italic'", ",", "'_italic'", ")", "elif", "' italic'", "in", "name", ":", "name", "=", "name", ".", "replace", "(", "' italic'", ",", "'_italic'", ")", "elif", "'italic'", "in", "name", ":", "name", "=", "name", ".", "replace", "(", "'italic'", ",", "'_italic'", ")", "elif", "'oblique'", "in", "name", ":", "name", "=", "name", ".", "replace", "(", "'oblique'", ",", "'_italic'", ")", "else", ":", "families", ".", "append", "(", "name", ")", "font_dict", "[", "name", "]", "=", "source", "else", ":", "source", "=", "os", ".", "path", ".", "join", "(", "dirName", ",", "item", ")", "name", "=", "root", ".", "lower", "(", ")", ".", "replace", "(", "'_'", ",", "' '", ")", "font_dict", "[", "name", "]", "=", "source", "families", ".", "append", "(", "name", ")", "self", ".", "font_dict", "=", "font_dict", "self", ".", "families", "=", "families"], "docstring": "Given a search path, find file with requested extension", "docstring_tokens": ["Given", "a", "search", "path", "find", "file", "with", "requested", "extension"], "sha": "ac2501f30d6619eae9dea5644717575ca9263d0a", "url": "https://github.com/katerina7479/pypdflite/blob/ac2501f30d6619eae9dea5644717575ca9263d0a/pypdflite/font_loader.py#L30-L68", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/converters/ast_to_dag.py", "func_name": "AstInterpreter._process_custom_unitary", "original_string": "def _process_custom_unitary(self, node):\n        \"\"\"Process a custom unitary node.\"\"\"\n        name = node.name\n        if node.arguments is not None:\n            args = self._process_node(node.arguments)\n        else:\n            args = []\n        bits = [self._process_bit_id(node_element)\n                for node_element in node.bitlist.children]\n        if name in self.gates:\n            gargs = self.gates[name][\"args\"]\n            gbits = self.gates[name][\"bits\"]\n            # Loop over register arguments, if any.\n            maxidx = max(map(len, bits))\n            for idx in range(maxidx):\n                self.arg_stack.append({gargs[j]: args[j]\n                                       for j in range(len(gargs))})\n                # Only index into register arguments.\n                element = [idx*x for x in\n                           [len(bits[j]) > 1 for j in range(len(bits))]]\n                self.bit_stack.append({gbits[j]: bits[j][element[j]]\n                                       for j in range(len(gbits))})\n                self._create_dag_op(name,\n                                    [self.arg_stack[-1][s].sym() for s in gargs],\n                                    [self.bit_stack[-1][s] for s in gbits])\n                self.arg_stack.pop()\n                self.bit_stack.pop()\n        else:\n            raise QiskitError(\"internal error undefined gate:\",\n                              \"line=%s\" % node.line, \"file=%s\" % node.file)", "language": "python", "code": "def _process_custom_unitary(self, node):\n        \"\"\"Process a custom unitary node.\"\"\"\n        name = node.name\n        if node.arguments is not None:\n            args = self._process_node(node.arguments)\n        else:\n            args = []\n        bits = [self._process_bit_id(node_element)\n                for node_element in node.bitlist.children]\n        if name in self.gates:\n            gargs = self.gates[name][\"args\"]\n            gbits = self.gates[name][\"bits\"]\n            # Loop over register arguments, if any.\n            maxidx = max(map(len, bits))\n            for idx in range(maxidx):\n                self.arg_stack.append({gargs[j]: args[j]\n                                       for j in range(len(gargs))})\n                # Only index into register arguments.\n                element = [idx*x for x in\n                           [len(bits[j]) > 1 for j in range(len(bits))]]\n                self.bit_stack.append({gbits[j]: bits[j][element[j]]\n                                       for j in range(len(gbits))})\n                self._create_dag_op(name,\n                                    [self.arg_stack[-1][s].sym() for s in gargs],\n                                    [self.bit_stack[-1][s] for s in gbits])\n                self.arg_stack.pop()\n                self.bit_stack.pop()\n        else:\n            raise QiskitError(\"internal error undefined gate:\",\n                              \"line=%s\" % node.line, \"file=%s\" % node.file)", "code_tokens": ["def", "_process_custom_unitary", "(", "self", ",", "node", ")", ":", "name", "=", "node", ".", "name", "if", "node", ".", "arguments", "is", "not", "None", ":", "args", "=", "self", ".", "_process_node", "(", "node", ".", "arguments", ")", "else", ":", "args", "=", "[", "]", "bits", "=", "[", "self", ".", "_process_bit_id", "(", "node_element", ")", "for", "node_element", "in", "node", ".", "bitlist", ".", "children", "]", "if", "name", "in", "self", ".", "gates", ":", "gargs", "=", "self", ".", "gates", "[", "name", "]", "[", "\"args\"", "]", "gbits", "=", "self", ".", "gates", "[", "name", "]", "[", "\"bits\"", "]", "# Loop over register arguments, if any.", "maxidx", "=", "max", "(", "map", "(", "len", ",", "bits", ")", ")", "for", "idx", "in", "range", "(", "maxidx", ")", ":", "self", ".", "arg_stack", ".", "append", "(", "{", "gargs", "[", "j", "]", ":", "args", "[", "j", "]", "for", "j", "in", "range", "(", "len", "(", "gargs", ")", ")", "}", ")", "# Only index into register arguments.", "element", "=", "[", "idx", "*", "x", "for", "x", "in", "[", "len", "(", "bits", "[", "j", "]", ")", ">", "1", "for", "j", "in", "range", "(", "len", "(", "bits", ")", ")", "]", "]", "self", ".", "bit_stack", ".", "append", "(", "{", "gbits", "[", "j", "]", ":", "bits", "[", "j", "]", "[", "element", "[", "j", "]", "]", "for", "j", "in", "range", "(", "len", "(", "gbits", ")", ")", "}", ")", "self", ".", "_create_dag_op", "(", "name", ",", "[", "self", ".", "arg_stack", "[", "-", "1", "]", "[", "s", "]", ".", "sym", "(", ")", "for", "s", "in", "gargs", "]", ",", "[", "self", ".", "bit_stack", "[", "-", "1", "]", "[", "s", "]", "for", "s", "in", "gbits", "]", ")", "self", ".", "arg_stack", ".", "pop", "(", ")", "self", ".", "bit_stack", ".", "pop", "(", ")", "else", ":", "raise", "QiskitError", "(", "\"internal error undefined gate:\"", ",", "\"line=%s\"", "%", "node", ".", "line", ",", "\"file=%s\"", "%", "node", ".", "file", ")"], "docstring": "Process a custom unitary node.", "docstring_tokens": ["Process", "a", "custom", "unitary", "node", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/converters/ast_to_dag.py#L122-L151", "partition": "test"}
{"repo": "reingart/gui2py", "path": "gui/controls/gridview.py", "func_name": "GridTable.SortColumn", "original_string": "def SortColumn(self, col):\r\n        \"col -> sort the data based on the column indexed by col\"\r\n        name = self.columns[col].name\r\n        _data = []\r\n\r\n        for row in self.data:\r\n            rowname, entry = row\r\n            _data.append((entry.get(name, None), row))\r\n\r\n        _data.sort()\r\n        self.data = []\r\n\r\n        for sortvalue, row in _data:\r\n            self.data.append(row)", "language": "python", "code": "def SortColumn(self, col):\r\n        \"col -> sort the data based on the column indexed by col\"\r\n        name = self.columns[col].name\r\n        _data = []\r\n\r\n        for row in self.data:\r\n            rowname, entry = row\r\n            _data.append((entry.get(name, None), row))\r\n\r\n        _data.sort()\r\n        self.data = []\r\n\r\n        for sortvalue, row in _data:\r\n            self.data.append(row)", "code_tokens": ["def", "SortColumn", "(", "self", ",", "col", ")", ":", "name", "=", "self", ".", "columns", "[", "col", "]", ".", "name", "_data", "=", "[", "]", "for", "row", "in", "self", ".", "data", ":", "rowname", ",", "entry", "=", "row", "_data", ".", "append", "(", "(", "entry", ".", "get", "(", "name", ",", "None", ")", ",", "row", ")", ")", "_data", ".", "sort", "(", ")", "self", ".", "data", "=", "[", "]", "for", "sortvalue", ",", "row", "in", "_data", ":", "self", ".", "data", ".", "append", "(", "row", ")"], "docstring": "col -> sort the data based on the column indexed by col", "docstring_tokens": ["col", "-", ">", "sort", "the", "data", "based", "on", "the", "column", "indexed", "by", "col"], "sha": "aca0a05f6fcde55c94ad7cc058671a06608b01a4", "url": "https://github.com/reingart/gui2py/blob/aca0a05f6fcde55c94ad7cc058671a06608b01a4/gui/controls/gridview.py#L313-L326", "partition": "test"}
{"repo": "celiao/tmdbsimple", "path": "tmdbsimple/movies.py", "func_name": "Movies.recommendations", "original_string": "def recommendations(self, **kwargs):\n        \"\"\"\n        Get a list of recommended movies for a movie.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n            page: (optional) Minimum value of 1.  Expected value is an integer.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('recommendations')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response", "language": "python", "code": "def recommendations(self, **kwargs):\n        \"\"\"\n        Get a list of recommended movies for a movie.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n            page: (optional) Minimum value of 1.  Expected value is an integer.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('recommendations')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response", "code_tokens": ["def", "recommendations", "(", "self", ",", "*", "*", "kwargs", ")", ":", "path", "=", "self", ".", "_get_id_path", "(", "'recommendations'", ")", "response", "=", "self", ".", "_GET", "(", "path", ",", "kwargs", ")", "self", ".", "_set_attrs_to_values", "(", "response", ")", "return", "response"], "docstring": "Get a list of recommended movies for a movie.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n            page: (optional) Minimum value of 1.  Expected value is an integer.\n\n        Returns:\n            A dict representation of the JSON returned from the API.", "docstring_tokens": ["Get", "a", "list", "of", "recommended", "movies", "for", "a", "movie", "."], "sha": "ff17893110c99771d6398a62c35d36dd9735f4b9", "url": "https://github.com/celiao/tmdbsimple/blob/ff17893110c99771d6398a62c35d36dd9735f4b9/tmdbsimple/movies.py#L152-L167", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/qobj/converters/pulse_instruction.py", "func_name": "PulseQobjConverter.convert_drive", "original_string": "def convert_drive(self, shift, instruction):\n        \"\"\"Return converted `PulseInstruction`.\n\n        Args:\n            shift(int): Offset time.\n            instruction (PulseInstruction): drive instruction.\n        Returns:\n            dict: Dictionary of required parameters.\n        \"\"\"\n        command_dict = {\n            'name': instruction.command.name,\n            't0': shift+instruction.start_time,\n            'ch': instruction.channels[0].name\n        }\n        return self._qobj_model(**command_dict)", "language": "python", "code": "def convert_drive(self, shift, instruction):\n        \"\"\"Return converted `PulseInstruction`.\n\n        Args:\n            shift(int): Offset time.\n            instruction (PulseInstruction): drive instruction.\n        Returns:\n            dict: Dictionary of required parameters.\n        \"\"\"\n        command_dict = {\n            'name': instruction.command.name,\n            't0': shift+instruction.start_time,\n            'ch': instruction.channels[0].name\n        }\n        return self._qobj_model(**command_dict)", "code_tokens": ["def", "convert_drive", "(", "self", ",", "shift", ",", "instruction", ")", ":", "command_dict", "=", "{", "'name'", ":", "instruction", ".", "command", ".", "name", ",", "'t0'", ":", "shift", "+", "instruction", ".", "start_time", ",", "'ch'", ":", "instruction", ".", "channels", "[", "0", "]", ".", "name", "}", "return", "self", ".", "_qobj_model", "(", "*", "*", "command_dict", ")"], "docstring": "Return converted `PulseInstruction`.\n\n        Args:\n            shift(int): Offset time.\n            instruction (PulseInstruction): drive instruction.\n        Returns:\n            dict: Dictionary of required parameters.", "docstring_tokens": ["Return", "converted", "PulseInstruction", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/qobj/converters/pulse_instruction.py#L196-L210", "partition": "test"}
{"repo": "andreikop/python-ws-discovery", "path": "wsdiscovery/cmdline.py", "func_name": "discover", "original_string": "def discover(scope, loglevel, capture):\n    \"Discover systems using WS-Discovery\"\n\n    if loglevel:\n        level = getattr(logging, loglevel, None)\n        if not level:\n           print(\"Invalid log level '%s'\" % loglevel)\n           return\n        logger.setLevel(level)\n\n    run(scope=scope, capture=capture)", "language": "python", "code": "def discover(scope, loglevel, capture):\n    \"Discover systems using WS-Discovery\"\n\n    if loglevel:\n        level = getattr(logging, loglevel, None)\n        if not level:\n           print(\"Invalid log level '%s'\" % loglevel)\n           return\n        logger.setLevel(level)\n\n    run(scope=scope, capture=capture)", "code_tokens": ["def", "discover", "(", "scope", ",", "loglevel", ",", "capture", ")", ":", "if", "loglevel", ":", "level", "=", "getattr", "(", "logging", ",", "loglevel", ",", "None", ")", "if", "not", "level", ":", "print", "(", "\"Invalid log level '%s'\"", "%", "loglevel", ")", "return", "logger", ".", "setLevel", "(", "level", ")", "run", "(", "scope", "=", "scope", ",", "capture", "=", "capture", ")"], "docstring": "Discover systems using WS-Discovery", "docstring_tokens": ["Discover", "systems", "using", "WS", "-", "Discovery"], "sha": "a7b852cf43115c6f986e509b1870d6963e76687f", "url": "https://github.com/andreikop/python-ws-discovery/blob/a7b852cf43115c6f986e509b1870d6963e76687f/wsdiscovery/cmdline.py#L64-L74", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/similar.py", "func_name": "Run", "original_string": "def Run(argv=None):\n    \"\"\"standalone command line access point\"\"\"\n    if argv is None:\n        argv = sys.argv[1:]\n    from getopt import getopt\n\n    s_opts = \"hdi\"\n    l_opts = (\n        \"help\",\n        \"duplicates=\",\n        \"ignore-comments\",\n        \"ignore-imports\",\n        \"ignore-docstrings\",\n    )\n    min_lines = 4\n    ignore_comments = False\n    ignore_docstrings = False\n    ignore_imports = False\n    opts, args = getopt(argv, s_opts, l_opts)\n    for opt, val in opts:\n        if opt in (\"-d\", \"--duplicates\"):\n            min_lines = int(val)\n        elif opt in (\"-h\", \"--help\"):\n            usage()\n        elif opt in (\"-i\", \"--ignore-comments\"):\n            ignore_comments = True\n        elif opt in (\"--ignore-docstrings\",):\n            ignore_docstrings = True\n        elif opt in (\"--ignore-imports\",):\n            ignore_imports = True\n    if not args:\n        usage(1)\n    sim = Similar(min_lines, ignore_comments, ignore_docstrings, ignore_imports)\n    for filename in args:\n        with open(filename) as stream:\n            sim.append_stream(filename, stream)\n    sim.run()\n    sys.exit(0)", "language": "python", "code": "def Run(argv=None):\n    \"\"\"standalone command line access point\"\"\"\n    if argv is None:\n        argv = sys.argv[1:]\n    from getopt import getopt\n\n    s_opts = \"hdi\"\n    l_opts = (\n        \"help\",\n        \"duplicates=\",\n        \"ignore-comments\",\n        \"ignore-imports\",\n        \"ignore-docstrings\",\n    )\n    min_lines = 4\n    ignore_comments = False\n    ignore_docstrings = False\n    ignore_imports = False\n    opts, args = getopt(argv, s_opts, l_opts)\n    for opt, val in opts:\n        if opt in (\"-d\", \"--duplicates\"):\n            min_lines = int(val)\n        elif opt in (\"-h\", \"--help\"):\n            usage()\n        elif opt in (\"-i\", \"--ignore-comments\"):\n            ignore_comments = True\n        elif opt in (\"--ignore-docstrings\",):\n            ignore_docstrings = True\n        elif opt in (\"--ignore-imports\",):\n            ignore_imports = True\n    if not args:\n        usage(1)\n    sim = Similar(min_lines, ignore_comments, ignore_docstrings, ignore_imports)\n    for filename in args:\n        with open(filename) as stream:\n            sim.append_stream(filename, stream)\n    sim.run()\n    sys.exit(0)", "code_tokens": ["def", "Run", "(", "argv", "=", "None", ")", ":", "if", "argv", "is", "None", ":", "argv", "=", "sys", ".", "argv", "[", "1", ":", "]", "from", "getopt", "import", "getopt", "s_opts", "=", "\"hdi\"", "l_opts", "=", "(", "\"help\"", ",", "\"duplicates=\"", ",", "\"ignore-comments\"", ",", "\"ignore-imports\"", ",", "\"ignore-docstrings\"", ",", ")", "min_lines", "=", "4", "ignore_comments", "=", "False", "ignore_docstrings", "=", "False", "ignore_imports", "=", "False", "opts", ",", "args", "=", "getopt", "(", "argv", ",", "s_opts", ",", "l_opts", ")", "for", "opt", ",", "val", "in", "opts", ":", "if", "opt", "in", "(", "\"-d\"", ",", "\"--duplicates\"", ")", ":", "min_lines", "=", "int", "(", "val", ")", "elif", "opt", "in", "(", "\"-h\"", ",", "\"--help\"", ")", ":", "usage", "(", ")", "elif", "opt", "in", "(", "\"-i\"", ",", "\"--ignore-comments\"", ")", ":", "ignore_comments", "=", "True", "elif", "opt", "in", "(", "\"--ignore-docstrings\"", ",", ")", ":", "ignore_docstrings", "=", "True", "elif", "opt", "in", "(", "\"--ignore-imports\"", ",", ")", ":", "ignore_imports", "=", "True", "if", "not", "args", ":", "usage", "(", "1", ")", "sim", "=", "Similar", "(", "min_lines", ",", "ignore_comments", ",", "ignore_docstrings", ",", "ignore_imports", ")", "for", "filename", "in", "args", ":", "with", "open", "(", "filename", ")", "as", "stream", ":", "sim", ".", "append_stream", "(", "filename", ",", "stream", ")", "sim", ".", "run", "(", ")", "sys", ".", "exit", "(", "0", ")"], "docstring": "standalone command line access point", "docstring_tokens": ["standalone", "command", "line", "access", "point"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/similar.py#L411-L448", "partition": "test"}
{"repo": "jazzband/django-ddp", "path": "dddp/accounts/ddp.py", "func_name": "Auth.create_user", "original_string": "def create_user(self, params):\n        \"\"\"Register a new user account.\"\"\"\n        receivers = create_user.send(\n            sender=__name__,\n            request=this.request,\n            params=params,\n        )\n        if len(receivers) == 0:\n            raise NotImplementedError(\n                'Handler for `create_user` not registered.'\n            )\n        user = receivers[0][1]\n        user = auth.authenticate(\n            username=user.get_username(), password=params['password'],\n        )\n        self.do_login(user)\n        return get_user_token(\n            user=user, purpose=HashPurpose.RESUME_LOGIN,\n            minutes_valid=HASH_MINUTES_VALID[HashPurpose.RESUME_LOGIN],\n        )", "language": "python", "code": "def create_user(self, params):\n        \"\"\"Register a new user account.\"\"\"\n        receivers = create_user.send(\n            sender=__name__,\n            request=this.request,\n            params=params,\n        )\n        if len(receivers) == 0:\n            raise NotImplementedError(\n                'Handler for `create_user` not registered.'\n            )\n        user = receivers[0][1]\n        user = auth.authenticate(\n            username=user.get_username(), password=params['password'],\n        )\n        self.do_login(user)\n        return get_user_token(\n            user=user, purpose=HashPurpose.RESUME_LOGIN,\n            minutes_valid=HASH_MINUTES_VALID[HashPurpose.RESUME_LOGIN],\n        )", "code_tokens": ["def", "create_user", "(", "self", ",", "params", ")", ":", "receivers", "=", "create_user", ".", "send", "(", "sender", "=", "__name__", ",", "request", "=", "this", ".", "request", ",", "params", "=", "params", ",", ")", "if", "len", "(", "receivers", ")", "==", "0", ":", "raise", "NotImplementedError", "(", "'Handler for `create_user` not registered.'", ")", "user", "=", "receivers", "[", "0", "]", "[", "1", "]", "user", "=", "auth", ".", "authenticate", "(", "username", "=", "user", ".", "get_username", "(", ")", ",", "password", "=", "params", "[", "'password'", "]", ",", ")", "self", ".", "do_login", "(", "user", ")", "return", "get_user_token", "(", "user", "=", "user", ",", "purpose", "=", "HashPurpose", ".", "RESUME_LOGIN", ",", "minutes_valid", "=", "HASH_MINUTES_VALID", "[", "HashPurpose", ".", "RESUME_LOGIN", "]", ",", ")"], "docstring": "Register a new user account.", "docstring_tokens": ["Register", "a", "new", "user", "account", "."], "sha": "1e1954b06fe140346acea43582515991685e4e01", "url": "https://github.com/jazzband/django-ddp/blob/1e1954b06fe140346acea43582515991685e4e01/dddp/accounts/ddp.py#L405-L424", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "scripts/addjavamessage2ignore.py", "func_name": "print_dict", "original_string": "def print_dict():\n    \"\"\"\n    Write the java ignored messages in g_ok_java_messages into a text file for humans to read.\n\n    :return: none\n    \"\"\"\n    global g_ok_java_messages\n    global g_java_messages_to_ignore_text_filename\n\n    allKeys = sorted(g_ok_java_messages.keys())\n\n    with open(g_java_messages_to_ignore_text_filename,'w') as ofile:\n        for key in allKeys:\n\n            for mess in g_ok_java_messages[key]:\n                ofile.write('KeyName: '+key+'\\n')\n                ofile.write('IgnoredMessage: '+mess+'\\n')\n\n            print('KeyName: ',key)\n            print('IgnoredMessage: ',g_ok_java_messages[key])\n            print('\\n')", "language": "python", "code": "def print_dict():\n    \"\"\"\n    Write the java ignored messages in g_ok_java_messages into a text file for humans to read.\n\n    :return: none\n    \"\"\"\n    global g_ok_java_messages\n    global g_java_messages_to_ignore_text_filename\n\n    allKeys = sorted(g_ok_java_messages.keys())\n\n    with open(g_java_messages_to_ignore_text_filename,'w') as ofile:\n        for key in allKeys:\n\n            for mess in g_ok_java_messages[key]:\n                ofile.write('KeyName: '+key+'\\n')\n                ofile.write('IgnoredMessage: '+mess+'\\n')\n\n            print('KeyName: ',key)\n            print('IgnoredMessage: ',g_ok_java_messages[key])\n            print('\\n')", "code_tokens": ["def", "print_dict", "(", ")", ":", "global", "g_ok_java_messages", "global", "g_java_messages_to_ignore_text_filename", "allKeys", "=", "sorted", "(", "g_ok_java_messages", ".", "keys", "(", ")", ")", "with", "open", "(", "g_java_messages_to_ignore_text_filename", ",", "'w'", ")", "as", "ofile", ":", "for", "key", "in", "allKeys", ":", "for", "mess", "in", "g_ok_java_messages", "[", "key", "]", ":", "ofile", ".", "write", "(", "'KeyName: '", "+", "key", "+", "'\\n'", ")", "ofile", ".", "write", "(", "'IgnoredMessage: '", "+", "mess", "+", "'\\n'", ")", "print", "(", "'KeyName: '", ",", "key", ")", "print", "(", "'IgnoredMessage: '", ",", "g_ok_java_messages", "[", "key", "]", ")", "print", "(", "'\\n'", ")"], "docstring": "Write the java ignored messages in g_ok_java_messages into a text file for humans to read.\n\n    :return: none", "docstring_tokens": ["Write", "the", "java", "ignored", "messages", "in", "g_ok_java_messages", "into", "a", "text", "file", "for", "humans", "to", "read", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/scripts/addjavamessage2ignore.py#L287-L307", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/core/primitives.py", "func_name": "BigInteger.validate", "original_string": "def validate(self):\n        \"\"\"\n        Verify that the value of the BigInteger is valid.\n\n        Raises:\n            TypeError: if the value is not of type int or long\n        \"\"\"\n        if self.value is not None:\n            if not isinstance(self.value, six.integer_types):\n                raise TypeError('expected (one of): {0}, observed: {1}'.format(\n                    six.integer_types, type(self.value)))", "language": "python", "code": "def validate(self):\n        \"\"\"\n        Verify that the value of the BigInteger is valid.\n\n        Raises:\n            TypeError: if the value is not of type int or long\n        \"\"\"\n        if self.value is not None:\n            if not isinstance(self.value, six.integer_types):\n                raise TypeError('expected (one of): {0}, observed: {1}'.format(\n                    six.integer_types, type(self.value)))", "code_tokens": ["def", "validate", "(", "self", ")", ":", "if", "self", ".", "value", "is", "not", "None", ":", "if", "not", "isinstance", "(", "self", ".", "value", ",", "six", ".", "integer_types", ")", ":", "raise", "TypeError", "(", "'expected (one of): {0}, observed: {1}'", ".", "format", "(", "six", ".", "integer_types", ",", "type", "(", "self", ".", "value", ")", ")", ")"], "docstring": "Verify that the value of the BigInteger is valid.\n\n        Raises:\n            TypeError: if the value is not of type int or long", "docstring_tokens": ["Verify", "that", "the", "value", "of", "the", "BigInteger", "is", "valid", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/core/primitives.py#L515-L525", "partition": "test"}
{"repo": "Julian/Ivoire", "path": "ivoire/run.py", "func_name": "parse", "original_string": "def parse(argv=None):\n    \"\"\"\n    Parse some arguments using the parser.\n\n    \"\"\"\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    # Evade http://bugs.python.org/issue9253\n    if not argv or argv[0] not in {\"run\", \"transform\"}:\n        argv = [\"run\"] + argv\n\n    arguments = _clean(_parser.parse_args(argv))\n    return arguments", "language": "python", "code": "def parse(argv=None):\n    \"\"\"\n    Parse some arguments using the parser.\n\n    \"\"\"\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    # Evade http://bugs.python.org/issue9253\n    if not argv or argv[0] not in {\"run\", \"transform\"}:\n        argv = [\"run\"] + argv\n\n    arguments = _clean(_parser.parse_args(argv))\n    return arguments", "code_tokens": ["def", "parse", "(", "argv", "=", "None", ")", ":", "if", "argv", "is", "None", ":", "argv", "=", "sys", ".", "argv", "[", "1", ":", "]", "# Evade http://bugs.python.org/issue9253", "if", "not", "argv", "or", "argv", "[", "0", "]", "not", "in", "{", "\"run\"", ",", "\"transform\"", "}", ":", "argv", "=", "[", "\"run\"", "]", "+", "argv", "arguments", "=", "_clean", "(", "_parser", ".", "parse_args", "(", "argv", ")", ")", "return", "arguments"], "docstring": "Parse some arguments using the parser.", "docstring_tokens": ["Parse", "some", "arguments", "using", "the", "parser", "."], "sha": "5b8218cffa409ed733cf850a6fde16fafb8fc2af", "url": "https://github.com/Julian/Ivoire/blob/5b8218cffa409ed733cf850a6fde16fafb8fc2af/ivoire/run.py#L45-L59", "partition": "test"}
{"repo": "urinieto/msaf", "path": "msaf/input_output.py", "func_name": "read_hier_references", "original_string": "def read_hier_references(jams_file, annotation_id=0, exclude_levels=[]):\n    \"\"\"Reads hierarchical references from a jams file.\n\n    Parameters\n    ----------\n    jams_file : str\n        Path to the jams file.\n    annotation_id : int > 0\n        Identifier of the annotator to read from.\n    exclude_levels: list\n        List of levels to exclude. Empty list to include all levels.\n\n    Returns\n    -------\n    hier_bounds : list\n        List of the segment boundary times in seconds for each level.\n    hier_labels : list\n        List of the segment labels for each level.\n    hier_levels : list\n        List of strings for the level identifiers.\n    \"\"\"\n    hier_bounds = []\n    hier_labels = []\n    hier_levels = []\n    jam = jams.load(jams_file)\n    namespaces = [\"segment_salami_upper\", \"segment_salami_function\",\n                  \"segment_open\", \"segment_tut\", \"segment_salami_lower\"]\n\n    # Remove levels if needed\n    for exclude in exclude_levels:\n        if exclude in namespaces:\n            namespaces.remove(exclude)\n\n    # Build hierarchy references\n    for ns in namespaces:\n        ann = jam.search(namespace=ns)\n        if not ann:\n            continue\n        ref_inters, ref_labels = ann[annotation_id].to_interval_values()\n        hier_bounds.append(utils.intervals_to_times(ref_inters))\n        hier_labels.append(ref_labels)\n        hier_levels.append(ns)\n\n    return hier_bounds, hier_labels, hier_levels", "language": "python", "code": "def read_hier_references(jams_file, annotation_id=0, exclude_levels=[]):\n    \"\"\"Reads hierarchical references from a jams file.\n\n    Parameters\n    ----------\n    jams_file : str\n        Path to the jams file.\n    annotation_id : int > 0\n        Identifier of the annotator to read from.\n    exclude_levels: list\n        List of levels to exclude. Empty list to include all levels.\n\n    Returns\n    -------\n    hier_bounds : list\n        List of the segment boundary times in seconds for each level.\n    hier_labels : list\n        List of the segment labels for each level.\n    hier_levels : list\n        List of strings for the level identifiers.\n    \"\"\"\n    hier_bounds = []\n    hier_labels = []\n    hier_levels = []\n    jam = jams.load(jams_file)\n    namespaces = [\"segment_salami_upper\", \"segment_salami_function\",\n                  \"segment_open\", \"segment_tut\", \"segment_salami_lower\"]\n\n    # Remove levels if needed\n    for exclude in exclude_levels:\n        if exclude in namespaces:\n            namespaces.remove(exclude)\n\n    # Build hierarchy references\n    for ns in namespaces:\n        ann = jam.search(namespace=ns)\n        if not ann:\n            continue\n        ref_inters, ref_labels = ann[annotation_id].to_interval_values()\n        hier_bounds.append(utils.intervals_to_times(ref_inters))\n        hier_labels.append(ref_labels)\n        hier_levels.append(ns)\n\n    return hier_bounds, hier_labels, hier_levels", "code_tokens": ["def", "read_hier_references", "(", "jams_file", ",", "annotation_id", "=", "0", ",", "exclude_levels", "=", "[", "]", ")", ":", "hier_bounds", "=", "[", "]", "hier_labels", "=", "[", "]", "hier_levels", "=", "[", "]", "jam", "=", "jams", ".", "load", "(", "jams_file", ")", "namespaces", "=", "[", "\"segment_salami_upper\"", ",", "\"segment_salami_function\"", ",", "\"segment_open\"", ",", "\"segment_tut\"", ",", "\"segment_salami_lower\"", "]", "# Remove levels if needed", "for", "exclude", "in", "exclude_levels", ":", "if", "exclude", "in", "namespaces", ":", "namespaces", ".", "remove", "(", "exclude", ")", "# Build hierarchy references", "for", "ns", "in", "namespaces", ":", "ann", "=", "jam", ".", "search", "(", "namespace", "=", "ns", ")", "if", "not", "ann", ":", "continue", "ref_inters", ",", "ref_labels", "=", "ann", "[", "annotation_id", "]", ".", "to_interval_values", "(", ")", "hier_bounds", ".", "append", "(", "utils", ".", "intervals_to_times", "(", "ref_inters", ")", ")", "hier_labels", ".", "append", "(", "ref_labels", ")", "hier_levels", ".", "append", "(", "ns", ")", "return", "hier_bounds", ",", "hier_labels", ",", "hier_levels"], "docstring": "Reads hierarchical references from a jams file.\n\n    Parameters\n    ----------\n    jams_file : str\n        Path to the jams file.\n    annotation_id : int > 0\n        Identifier of the annotator to read from.\n    exclude_levels: list\n        List of levels to exclude. Empty list to include all levels.\n\n    Returns\n    -------\n    hier_bounds : list\n        List of the segment boundary times in seconds for each level.\n    hier_labels : list\n        List of the segment labels for each level.\n    hier_levels : list\n        List of strings for the level identifiers.", "docstring_tokens": ["Reads", "hierarchical", "references", "from", "a", "jams", "file", "."], "sha": "9dbb57d77a1310465a65cc40f1641d083ca74385", "url": "https://github.com/urinieto/msaf/blob/9dbb57d77a1310465a65cc40f1641d083ca74385/msaf/input_output.py#L391-L434", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py", "func_name": "correlation_matrix_volume_rejection_samples", "original_string": "def correlation_matrix_volume_rejection_samples(\n    det_bounds, dim, sample_shape, dtype, seed):\n  \"\"\"Returns rejection samples from trying to get good correlation matrices.\n\n  The proposal being rejected from is the uniform distribution on\n  \"correlation-like\" matrices.  We say a matrix is \"correlation-like\"\n  if it is a symmetric square matrix with all entries between -1 and 1\n  (inclusive) and 1s on the main diagonal.  Of these, the ones that\n  are positive semi-definite are exactly the correlation matrices.\n\n  The rejection algorithm, then, is to sample a `Tensor` of\n  `sample_shape` correlation-like matrices of dimensions `dim` by\n  `dim`, and check each one for (i) being a correlation matrix (i.e.,\n  PSD), and (ii) having determinant at least the corresponding entry\n  of `det_bounds`.\n\n  Args:\n    det_bounds: A `Tensor` of lower bounds on the determinants of\n      acceptable matrices.  The shape must broadcast with `sample_shape`.\n    dim: A Python `int` dimension of correlation matrices to sample.\n    sample_shape: Python `tuple` of `int` shape of the samples to\n      compute, excluding the two matrix dimensions.\n    dtype: The `dtype` in which to do the computation.\n    seed: Random seed.\n\n  Returns:\n    weights: A `Tensor` of shape `sample_shape`.  Each entry is 0 if the\n      corresponding matrix was not a correlation matrix, or had too\n      small of a determinant.  Otherwise, the entry is the\n      multiplicative inverse of the density of proposing that matrix\n      uniformly, i.e., the volume of the set of `dim` by `dim`\n      correlation-like matrices.\n    volume: The volume of the set of `dim` by `dim` correlation-like\n      matrices.\n  \"\"\"\n  with tf.compat.v1.name_scope(\"rejection_sampler\"):\n    rej_proposals = _uniform_correlation_like_matrix(\n        dim, sample_shape, dtype, seed=seed)\n    rej_proposal_volume = 2. ** (dim * (dim - 1) / 2.)\n    # The density of proposing any given point is 1 / rej_proposal_volume;\n    # The weight of that point should be scaled by\n    # 1 / density = rej_proposal_volume.\n    rej_weights = rej_proposal_volume * _psd_mask(\n        rej_proposals) * _det_large_enough_mask(rej_proposals, det_bounds)\n    return rej_weights, rej_proposal_volume", "language": "python", "code": "def correlation_matrix_volume_rejection_samples(\n    det_bounds, dim, sample_shape, dtype, seed):\n  \"\"\"Returns rejection samples from trying to get good correlation matrices.\n\n  The proposal being rejected from is the uniform distribution on\n  \"correlation-like\" matrices.  We say a matrix is \"correlation-like\"\n  if it is a symmetric square matrix with all entries between -1 and 1\n  (inclusive) and 1s on the main diagonal.  Of these, the ones that\n  are positive semi-definite are exactly the correlation matrices.\n\n  The rejection algorithm, then, is to sample a `Tensor` of\n  `sample_shape` correlation-like matrices of dimensions `dim` by\n  `dim`, and check each one for (i) being a correlation matrix (i.e.,\n  PSD), and (ii) having determinant at least the corresponding entry\n  of `det_bounds`.\n\n  Args:\n    det_bounds: A `Tensor` of lower bounds on the determinants of\n      acceptable matrices.  The shape must broadcast with `sample_shape`.\n    dim: A Python `int` dimension of correlation matrices to sample.\n    sample_shape: Python `tuple` of `int` shape of the samples to\n      compute, excluding the two matrix dimensions.\n    dtype: The `dtype` in which to do the computation.\n    seed: Random seed.\n\n  Returns:\n    weights: A `Tensor` of shape `sample_shape`.  Each entry is 0 if the\n      corresponding matrix was not a correlation matrix, or had too\n      small of a determinant.  Otherwise, the entry is the\n      multiplicative inverse of the density of proposing that matrix\n      uniformly, i.e., the volume of the set of `dim` by `dim`\n      correlation-like matrices.\n    volume: The volume of the set of `dim` by `dim` correlation-like\n      matrices.\n  \"\"\"\n  with tf.compat.v1.name_scope(\"rejection_sampler\"):\n    rej_proposals = _uniform_correlation_like_matrix(\n        dim, sample_shape, dtype, seed=seed)\n    rej_proposal_volume = 2. ** (dim * (dim - 1) / 2.)\n    # The density of proposing any given point is 1 / rej_proposal_volume;\n    # The weight of that point should be scaled by\n    # 1 / density = rej_proposal_volume.\n    rej_weights = rej_proposal_volume * _psd_mask(\n        rej_proposals) * _det_large_enough_mask(rej_proposals, det_bounds)\n    return rej_weights, rej_proposal_volume", "code_tokens": ["def", "correlation_matrix_volume_rejection_samples", "(", "det_bounds", ",", "dim", ",", "sample_shape", ",", "dtype", ",", "seed", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "\"rejection_sampler\"", ")", ":", "rej_proposals", "=", "_uniform_correlation_like_matrix", "(", "dim", ",", "sample_shape", ",", "dtype", ",", "seed", "=", "seed", ")", "rej_proposal_volume", "=", "2.", "**", "(", "dim", "*", "(", "dim", "-", "1", ")", "/", "2.", ")", "# The density of proposing any given point is 1 / rej_proposal_volume;", "# The weight of that point should be scaled by", "# 1 / density = rej_proposal_volume.", "rej_weights", "=", "rej_proposal_volume", "*", "_psd_mask", "(", "rej_proposals", ")", "*", "_det_large_enough_mask", "(", "rej_proposals", ",", "det_bounds", ")", "return", "rej_weights", ",", "rej_proposal_volume"], "docstring": "Returns rejection samples from trying to get good correlation matrices.\n\n  The proposal being rejected from is the uniform distribution on\n  \"correlation-like\" matrices.  We say a matrix is \"correlation-like\"\n  if it is a symmetric square matrix with all entries between -1 and 1\n  (inclusive) and 1s on the main diagonal.  Of these, the ones that\n  are positive semi-definite are exactly the correlation matrices.\n\n  The rejection algorithm, then, is to sample a `Tensor` of\n  `sample_shape` correlation-like matrices of dimensions `dim` by\n  `dim`, and check each one for (i) being a correlation matrix (i.e.,\n  PSD), and (ii) having determinant at least the corresponding entry\n  of `det_bounds`.\n\n  Args:\n    det_bounds: A `Tensor` of lower bounds on the determinants of\n      acceptable matrices.  The shape must broadcast with `sample_shape`.\n    dim: A Python `int` dimension of correlation matrices to sample.\n    sample_shape: Python `tuple` of `int` shape of the samples to\n      compute, excluding the two matrix dimensions.\n    dtype: The `dtype` in which to do the computation.\n    seed: Random seed.\n\n  Returns:\n    weights: A `Tensor` of shape `sample_shape`.  Each entry is 0 if the\n      corresponding matrix was not a correlation matrix, or had too\n      small of a determinant.  Otherwise, the entry is the\n      multiplicative inverse of the density of proposing that matrix\n      uniformly, i.e., the volume of the set of `dim` by `dim`\n      correlation-like matrices.\n    volume: The volume of the set of `dim` by `dim` correlation-like\n      matrices.", "docstring_tokens": ["Returns", "rejection", "samples", "from", "trying", "to", "get", "good", "correlation", "matrices", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/correlation_matrix_volumes_lib.py#L172-L216", "partition": "test"}
{"repo": "pyca/pyopenssl", "path": "src/OpenSSL/crypto.py", "func_name": "get_elliptic_curve", "original_string": "def get_elliptic_curve(name):\n    \"\"\"\n    Return a single curve object selected by name.\n\n    See :py:func:`get_elliptic_curves` for information about curve objects.\n\n    :param name: The OpenSSL short name identifying the curve object to\n        retrieve.\n    :type name: :py:class:`unicode`\n\n    If the named curve is not supported then :py:class:`ValueError` is raised.\n    \"\"\"\n    for curve in get_elliptic_curves():\n        if curve.name == name:\n            return curve\n    raise ValueError(\"unknown curve name\", name)", "language": "python", "code": "def get_elliptic_curve(name):\n    \"\"\"\n    Return a single curve object selected by name.\n\n    See :py:func:`get_elliptic_curves` for information about curve objects.\n\n    :param name: The OpenSSL short name identifying the curve object to\n        retrieve.\n    :type name: :py:class:`unicode`\n\n    If the named curve is not supported then :py:class:`ValueError` is raised.\n    \"\"\"\n    for curve in get_elliptic_curves():\n        if curve.name == name:\n            return curve\n    raise ValueError(\"unknown curve name\", name)", "code_tokens": ["def", "get_elliptic_curve", "(", "name", ")", ":", "for", "curve", "in", "get_elliptic_curves", "(", ")", ":", "if", "curve", ".", "name", "==", "name", ":", "return", "curve", "raise", "ValueError", "(", "\"unknown curve name\"", ",", "name", ")"], "docstring": "Return a single curve object selected by name.\n\n    See :py:func:`get_elliptic_curves` for information about curve objects.\n\n    :param name: The OpenSSL short name identifying the curve object to\n        retrieve.\n    :type name: :py:class:`unicode`\n\n    If the named curve is not supported then :py:class:`ValueError` is raised.", "docstring_tokens": ["Return", "a", "single", "curve", "object", "selected", "by", "name", "."], "sha": "1fbe064c50fd030948141d7d630673761525b0d0", "url": "https://github.com/pyca/pyopenssl/blob/1fbe064c50fd030948141d7d630673761525b0d0/src/OpenSSL/crypto.py#L490-L505", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/utils/helpful_functions.py", "func_name": "get_matching_kwargs", "original_string": "def get_matching_kwargs(func, kwargs):\n    \"\"\"Takes a function and keyword arguments and returns the ones that can be passed.\"\"\"\n    args, uses_startstar = _get_argspec(func)\n    if uses_startstar:\n        return kwargs.copy()\n    else:\n        matching_kwargs = dict((k, kwargs[k]) for k in args if k in kwargs)\n        return matching_kwargs", "language": "python", "code": "def get_matching_kwargs(func, kwargs):\n    \"\"\"Takes a function and keyword arguments and returns the ones that can be passed.\"\"\"\n    args, uses_startstar = _get_argspec(func)\n    if uses_startstar:\n        return kwargs.copy()\n    else:\n        matching_kwargs = dict((k, kwargs[k]) for k in args if k in kwargs)\n        return matching_kwargs", "code_tokens": ["def", "get_matching_kwargs", "(", "func", ",", "kwargs", ")", ":", "args", ",", "uses_startstar", "=", "_get_argspec", "(", "func", ")", "if", "uses_startstar", ":", "return", "kwargs", ".", "copy", "(", ")", "else", ":", "matching_kwargs", "=", "dict", "(", "(", "k", ",", "kwargs", "[", "k", "]", ")", "for", "k", "in", "args", "if", "k", "in", "kwargs", ")", "return", "matching_kwargs"], "docstring": "Takes a function and keyword arguments and returns the ones that can be passed.", "docstring_tokens": ["Takes", "a", "function", "and", "keyword", "arguments", "and", "returns", "the", "ones", "that", "can", "be", "passed", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/utils/helpful_functions.py#L278-L285", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/setuptools/command/easy_install.py", "func_name": "easy_install.install_site_py", "original_string": "def install_site_py(self):\n        \"\"\"Make sure there's a site.py in the target dir, if needed\"\"\"\n\n        if self.sitepy_installed:\n            return  # already did it, or don't need to\n\n        sitepy = os.path.join(self.install_dir, \"site.py\")\n        source = resource_string(\"setuptools\", \"site-patch.py\")\n        current = \"\"\n\n        if os.path.exists(sitepy):\n            log.debug(\"Checking existing site.py in %s\", self.install_dir)\n            f = open(sitepy, 'rb')\n            current = f.read()\n            # we want str, not bytes\n            if PY3:\n                current = current.decode()\n\n            f.close()\n            if not current.startswith('def __boot():'):\n                raise DistutilsError(\n                    \"%s is not a setuptools-generated site.py; please\"\n                    \" remove it.\" % sitepy\n                )\n\n        if current != source:\n            log.info(\"Creating %s\", sitepy)\n            if not self.dry_run:\n                ensure_directory(sitepy)\n                f = open(sitepy, 'wb')\n                f.write(source)\n                f.close()\n            self.byte_compile([sitepy])\n\n        self.sitepy_installed = True", "language": "python", "code": "def install_site_py(self):\n        \"\"\"Make sure there's a site.py in the target dir, if needed\"\"\"\n\n        if self.sitepy_installed:\n            return  # already did it, or don't need to\n\n        sitepy = os.path.join(self.install_dir, \"site.py\")\n        source = resource_string(\"setuptools\", \"site-patch.py\")\n        current = \"\"\n\n        if os.path.exists(sitepy):\n            log.debug(\"Checking existing site.py in %s\", self.install_dir)\n            f = open(sitepy, 'rb')\n            current = f.read()\n            # we want str, not bytes\n            if PY3:\n                current = current.decode()\n\n            f.close()\n            if not current.startswith('def __boot():'):\n                raise DistutilsError(\n                    \"%s is not a setuptools-generated site.py; please\"\n                    \" remove it.\" % sitepy\n                )\n\n        if current != source:\n            log.info(\"Creating %s\", sitepy)\n            if not self.dry_run:\n                ensure_directory(sitepy)\n                f = open(sitepy, 'wb')\n                f.write(source)\n                f.close()\n            self.byte_compile([sitepy])\n\n        self.sitepy_installed = True", "code_tokens": ["def", "install_site_py", "(", "self", ")", ":", "if", "self", ".", "sitepy_installed", ":", "return", "# already did it, or don't need to", "sitepy", "=", "os", ".", "path", ".", "join", "(", "self", ".", "install_dir", ",", "\"site.py\"", ")", "source", "=", "resource_string", "(", "\"setuptools\"", ",", "\"site-patch.py\"", ")", "current", "=", "\"\"", "if", "os", ".", "path", ".", "exists", "(", "sitepy", ")", ":", "log", ".", "debug", "(", "\"Checking existing site.py in %s\"", ",", "self", ".", "install_dir", ")", "f", "=", "open", "(", "sitepy", ",", "'rb'", ")", "current", "=", "f", ".", "read", "(", ")", "# we want str, not bytes", "if", "PY3", ":", "current", "=", "current", ".", "decode", "(", ")", "f", ".", "close", "(", ")", "if", "not", "current", ".", "startswith", "(", "'def __boot():'", ")", ":", "raise", "DistutilsError", "(", "\"%s is not a setuptools-generated site.py; please\"", "\" remove it.\"", "%", "sitepy", ")", "if", "current", "!=", "source", ":", "log", ".", "info", "(", "\"Creating %s\"", ",", "sitepy", ")", "if", "not", "self", ".", "dry_run", ":", "ensure_directory", "(", "sitepy", ")", "f", "=", "open", "(", "sitepy", ",", "'wb'", ")", "f", ".", "write", "(", "source", ")", "f", ".", "close", "(", ")", "self", ".", "byte_compile", "(", "[", "sitepy", "]", ")", "self", ".", "sitepy_installed", "=", "True"], "docstring": "Make sure there's a site.py in the target dir, if needed", "docstring_tokens": ["Make", "sure", "there", "s", "a", "site", ".", "py", "in", "the", "target", "dir", "if", "needed"], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/setuptools/command/easy_install.py#L1223-L1257", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/parse/variant/frequency.py", "func_name": "parse_frequency", "original_string": "def parse_frequency(variant, info_key):\n    \"\"\"Parse any frequency from the info dict\n\n    Args:\n        variant(cyvcf2.Variant)\n        info_key(str)\n\n    Returns:\n        frequency(float): or None if frequency does not exist\n    \"\"\"\n    raw_annotation = variant.INFO.get(info_key)\n    raw_annotation = None if raw_annotation == '.' else raw_annotation\n    frequency = float(raw_annotation) if raw_annotation else None\n    return frequency", "language": "python", "code": "def parse_frequency(variant, info_key):\n    \"\"\"Parse any frequency from the info dict\n\n    Args:\n        variant(cyvcf2.Variant)\n        info_key(str)\n\n    Returns:\n        frequency(float): or None if frequency does not exist\n    \"\"\"\n    raw_annotation = variant.INFO.get(info_key)\n    raw_annotation = None if raw_annotation == '.' else raw_annotation\n    frequency = float(raw_annotation) if raw_annotation else None\n    return frequency", "code_tokens": ["def", "parse_frequency", "(", "variant", ",", "info_key", ")", ":", "raw_annotation", "=", "variant", ".", "INFO", ".", "get", "(", "info_key", ")", "raw_annotation", "=", "None", "if", "raw_annotation", "==", "'.'", "else", "raw_annotation", "frequency", "=", "float", "(", "raw_annotation", ")", "if", "raw_annotation", "else", "None", "return", "frequency"], "docstring": "Parse any frequency from the info dict\n\n    Args:\n        variant(cyvcf2.Variant)\n        info_key(str)\n\n    Returns:\n        frequency(float): or None if frequency does not exist", "docstring_tokens": ["Parse", "any", "frequency", "from", "the", "info", "dict"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/parse/variant/frequency.py#L98-L111", "partition": "test"}
{"repo": "lepture/flask-oauthlib", "path": "flask_oauthlib/provider/oauth1.py", "func_name": "OAuth1RequestValidator.save_access_token", "original_string": "def save_access_token(self, token, request):\n        \"\"\"Save access token to database.\n\n        A tokensetter is required, which accepts a token and request\n        parameters::\n\n            def tokensetter(token, request):\n                access_token = Token(\n                    client=request.client,\n                    user=request.user,\n                    token=token['oauth_token'],\n                    secret=token['oauth_token_secret'],\n                    realms=token['oauth_authorized_realms'],\n                )\n                return access_token.save()\n        \"\"\"\n        log.debug('Save access token %r', token)\n        self._tokensetter(token, request)", "language": "python", "code": "def save_access_token(self, token, request):\n        \"\"\"Save access token to database.\n\n        A tokensetter is required, which accepts a token and request\n        parameters::\n\n            def tokensetter(token, request):\n                access_token = Token(\n                    client=request.client,\n                    user=request.user,\n                    token=token['oauth_token'],\n                    secret=token['oauth_token_secret'],\n                    realms=token['oauth_authorized_realms'],\n                )\n                return access_token.save()\n        \"\"\"\n        log.debug('Save access token %r', token)\n        self._tokensetter(token, request)", "code_tokens": ["def", "save_access_token", "(", "self", ",", "token", ",", "request", ")", ":", "log", ".", "debug", "(", "'Save access token %r'", ",", "token", ")", "self", ".", "_tokensetter", "(", "token", ",", "request", ")"], "docstring": "Save access token to database.\n\n        A tokensetter is required, which accepts a token and request\n        parameters::\n\n            def tokensetter(token, request):\n                access_token = Token(\n                    client=request.client,\n                    user=request.user,\n                    token=token['oauth_token'],\n                    secret=token['oauth_token_secret'],\n                    realms=token['oauth_authorized_realms'],\n                )\n                return access_token.save()", "docstring_tokens": ["Save", "access", "token", "to", "database", "."], "sha": "9e6f152a5bb360e7496210da21561c3e6d41b0e1", "url": "https://github.com/lepture/flask-oauthlib/blob/9e6f152a5bb360e7496210da21561c3e6d41b0e1/flask_oauthlib/provider/oauth1.py#L841-L858", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/core/messages/contents.py", "func_name": "Authentication.write", "original_string": "def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Write the data encoding the Authentication struct to a stream.\n\n        Args:\n            output_stream (stream): A data stream in which to encode object\n                data, supporting a write method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"\n        local_stream = utils.BytearrayStream()\n\n        if len(self._credentials) == 0:\n            raise ValueError(\"Authentication struct missing credentials.\")\n        for credential in self._credentials:\n            credential.write(local_stream, kmip_version=kmip_version)\n\n        self.length = local_stream.length()\n        super(Authentication, self).write(\n            output_stream,\n            kmip_version=kmip_version\n        )\n        output_stream.write(local_stream.buffer)", "language": "python", "code": "def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Write the data encoding the Authentication struct to a stream.\n\n        Args:\n            output_stream (stream): A data stream in which to encode object\n                data, supporting a write method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"\n        local_stream = utils.BytearrayStream()\n\n        if len(self._credentials) == 0:\n            raise ValueError(\"Authentication struct missing credentials.\")\n        for credential in self._credentials:\n            credential.write(local_stream, kmip_version=kmip_version)\n\n        self.length = local_stream.length()\n        super(Authentication, self).write(\n            output_stream,\n            kmip_version=kmip_version\n        )\n        output_stream.write(local_stream.buffer)", "code_tokens": ["def", "write", "(", "self", ",", "output_stream", ",", "kmip_version", "=", "enums", ".", "KMIPVersion", ".", "KMIP_1_0", ")", ":", "local_stream", "=", "utils", ".", "BytearrayStream", "(", ")", "if", "len", "(", "self", ".", "_credentials", ")", "==", "0", ":", "raise", "ValueError", "(", "\"Authentication struct missing credentials.\"", ")", "for", "credential", "in", "self", ".", "_credentials", ":", "credential", ".", "write", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "self", ".", "length", "=", "local_stream", ".", "length", "(", ")", "super", "(", "Authentication", ",", "self", ")", ".", "write", "(", "output_stream", ",", "kmip_version", "=", "kmip_version", ")", "output_stream", ".", "write", "(", "local_stream", ".", "buffer", ")"], "docstring": "Write the data encoding the Authentication struct to a stream.\n\n        Args:\n            output_stream (stream): A data stream in which to encode object\n                data, supporting a write method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.", "docstring_tokens": ["Write", "the", "data", "encoding", "the", "Authentication", "struct", "to", "a", "stream", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/core/messages/contents.py#L388-L412", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "examples/example_22_saga_python/the_task.py", "func_name": "get_batch", "original_string": "def get_batch():\n    \"\"\"Function that parses the batch id from the command line arguments\"\"\"\n    optlist, args = getopt.getopt(sys.argv[1:], '', longopts='batch=')\n    batch = 0\n    for o, a in optlist:\n        if o == '--batch':\n            batch = int(a)\n            print('Found batch %d' % batch)\n\n    return batch", "language": "python", "code": "def get_batch():\n    \"\"\"Function that parses the batch id from the command line arguments\"\"\"\n    optlist, args = getopt.getopt(sys.argv[1:], '', longopts='batch=')\n    batch = 0\n    for o, a in optlist:\n        if o == '--batch':\n            batch = int(a)\n            print('Found batch %d' % batch)\n\n    return batch", "code_tokens": ["def", "get_batch", "(", ")", ":", "optlist", ",", "args", "=", "getopt", ".", "getopt", "(", "sys", ".", "argv", "[", "1", ":", "]", ",", "''", ",", "longopts", "=", "'batch='", ")", "batch", "=", "0", "for", "o", ",", "a", "in", "optlist", ":", "if", "o", "==", "'--batch'", ":", "batch", "=", "int", "(", "a", ")", "print", "(", "'Found batch %d'", "%", "batch", ")", "return", "batch"], "docstring": "Function that parses the batch id from the command line arguments", "docstring_tokens": ["Function", "that", "parses", "the", "batch", "id", "from", "the", "command", "line", "arguments"], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/examples/example_22_saga_python/the_task.py#L98-L107", "partition": "test"}
{"repo": "inveniosoftware/invenio-migrator", "path": "invenio_migrator/legacy/clients.py", "func_name": "dump", "original_string": "def dump(obj, from_date, with_json=True, latest_only=False, **kwargs):\n    \"\"\"Dump the oauth2server Client.\"\"\"\n    return dict(name=obj.name,\n                description=obj.description,\n                website=obj.website,\n                user_id=obj.user_id,\n                client_id=obj.client_id,\n                client_secret=obj.client_secret,\n                is_confidential=obj.is_confidential,\n                is_internal=obj.is_internal,\n                _redirect_uris=obj._redirect_uris,\n                _default_scopes=obj._default_scopes)", "language": "python", "code": "def dump(obj, from_date, with_json=True, latest_only=False, **kwargs):\n    \"\"\"Dump the oauth2server Client.\"\"\"\n    return dict(name=obj.name,\n                description=obj.description,\n                website=obj.website,\n                user_id=obj.user_id,\n                client_id=obj.client_id,\n                client_secret=obj.client_secret,\n                is_confidential=obj.is_confidential,\n                is_internal=obj.is_internal,\n                _redirect_uris=obj._redirect_uris,\n                _default_scopes=obj._default_scopes)", "code_tokens": ["def", "dump", "(", "obj", ",", "from_date", ",", "with_json", "=", "True", ",", "latest_only", "=", "False", ",", "*", "*", "kwargs", ")", ":", "return", "dict", "(", "name", "=", "obj", ".", "name", ",", "description", "=", "obj", ".", "description", ",", "website", "=", "obj", ".", "website", ",", "user_id", "=", "obj", ".", "user_id", ",", "client_id", "=", "obj", ".", "client_id", ",", "client_secret", "=", "obj", ".", "client_secret", ",", "is_confidential", "=", "obj", ".", "is_confidential", ",", "is_internal", "=", "obj", ".", "is_internal", ",", "_redirect_uris", "=", "obj", ".", "_redirect_uris", ",", "_default_scopes", "=", "obj", ".", "_default_scopes", ")"], "docstring": "Dump the oauth2server Client.", "docstring_tokens": ["Dump", "the", "oauth2server", "Client", "."], "sha": "6902c6968a39b747d15e32363f43b7dffe2622c2", "url": "https://github.com/inveniosoftware/invenio-migrator/blob/6902c6968a39b747d15e32363f43b7dffe2622c2/invenio_migrator/legacy/clients.py#L37-L48", "partition": "test"}
{"repo": "ellmetha/neojsonrpc", "path": "neojsonrpc/client.py", "func_name": "Client.validate_address", "original_string": "def validate_address(self, addr, **kwargs):\n        \"\"\" Validates if the considered string is a valid NEO address.\n\n        :param hex: string containing a potential NEO address\n        :type hex: str\n        :return: dictionary containing the result of the verification\n        :rtype: dictionary\n\n        \"\"\"\n        return self._call(JSONRPCMethods.VALIDATE_ADDRESS.value, [addr, ], **kwargs)", "language": "python", "code": "def validate_address(self, addr, **kwargs):\n        \"\"\" Validates if the considered string is a valid NEO address.\n\n        :param hex: string containing a potential NEO address\n        :type hex: str\n        :return: dictionary containing the result of the verification\n        :rtype: dictionary\n\n        \"\"\"\n        return self._call(JSONRPCMethods.VALIDATE_ADDRESS.value, [addr, ], **kwargs)", "code_tokens": ["def", "validate_address", "(", "self", ",", "addr", ",", "*", "*", "kwargs", ")", ":", "return", "self", ".", "_call", "(", "JSONRPCMethods", ".", "VALIDATE_ADDRESS", ".", "value", ",", "[", "addr", ",", "]", ",", "*", "*", "kwargs", ")"], "docstring": "Validates if the considered string is a valid NEO address.\n\n        :param hex: string containing a potential NEO address\n        :type hex: str\n        :return: dictionary containing the result of the verification\n        :rtype: dictionary", "docstring_tokens": ["Validates", "if", "the", "considered", "string", "is", "a", "valid", "NEO", "address", "."], "sha": "e369b633a727482d5f9e310f0c3337ae5f7265db", "url": "https://github.com/ellmetha/neojsonrpc/blob/e369b633a727482d5f9e310f0c3337ae5f7265db/neojsonrpc/client.py#L319-L328", "partition": "test"}
{"repo": "tensorflow/probability", "path": "experimental/no_u_turn_sampler/logistic_regression.py", "func_name": "covertype", "original_string": "def covertype():\n  \"\"\"Builds the Covertype data set.\"\"\"\n  import sklearn.datasets  # pylint: disable=g-import-not-at-top\n  data = sklearn.datasets.covtype.fetch_covtype()\n  features = data.data\n  labels = data.target\n\n  # Normalize features and append a column of ones for the intercept.\n  features -= features.mean(0)\n  features /= features.std(0)\n  features = np.hstack([features, np.ones([features.shape[0], 1])])\n  features = tf.cast(features, dtype=tf.float32)\n\n  # Binarize outcomes on whether it is a specific category.\n  _, counts = np.unique(labels, return_counts=True)\n  specific_category = np.argmax(counts)\n  labels = (labels == specific_category)\n  labels = tf.cast(labels, dtype=tf.int32)\n  return features, labels", "language": "python", "code": "def covertype():\n  \"\"\"Builds the Covertype data set.\"\"\"\n  import sklearn.datasets  # pylint: disable=g-import-not-at-top\n  data = sklearn.datasets.covtype.fetch_covtype()\n  features = data.data\n  labels = data.target\n\n  # Normalize features and append a column of ones for the intercept.\n  features -= features.mean(0)\n  features /= features.std(0)\n  features = np.hstack([features, np.ones([features.shape[0], 1])])\n  features = tf.cast(features, dtype=tf.float32)\n\n  # Binarize outcomes on whether it is a specific category.\n  _, counts = np.unique(labels, return_counts=True)\n  specific_category = np.argmax(counts)\n  labels = (labels == specific_category)\n  labels = tf.cast(labels, dtype=tf.int32)\n  return features, labels", "code_tokens": ["def", "covertype", "(", ")", ":", "import", "sklearn", ".", "datasets", "# pylint: disable=g-import-not-at-top", "data", "=", "sklearn", ".", "datasets", ".", "covtype", ".", "fetch_covtype", "(", ")", "features", "=", "data", ".", "data", "labels", "=", "data", ".", "target", "# Normalize features and append a column of ones for the intercept.", "features", "-=", "features", ".", "mean", "(", "0", ")", "features", "/=", "features", ".", "std", "(", "0", ")", "features", "=", "np", ".", "hstack", "(", "[", "features", ",", "np", ".", "ones", "(", "[", "features", ".", "shape", "[", "0", "]", ",", "1", "]", ")", "]", ")", "features", "=", "tf", ".", "cast", "(", "features", ",", "dtype", "=", "tf", ".", "float32", ")", "# Binarize outcomes on whether it is a specific category.", "_", ",", "counts", "=", "np", ".", "unique", "(", "labels", ",", "return_counts", "=", "True", ")", "specific_category", "=", "np", ".", "argmax", "(", "counts", ")", "labels", "=", "(", "labels", "==", "specific_category", ")", "labels", "=", "tf", ".", "cast", "(", "labels", ",", "dtype", "=", "tf", ".", "int32", ")", "return", "features", ",", "labels"], "docstring": "Builds the Covertype data set.", "docstring_tokens": ["Builds", "the", "Covertype", "data", "set", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/no_u_turn_sampler/logistic_regression.py#L67-L85", "partition": "test"}
{"repo": "inveniosoftware/invenio-migrator", "path": "invenio_migrator/legacy/userexts.py", "func_name": "get", "original_string": "def get(*args, **kwargs):\n    \"\"\"Get UserEXT objects.\"\"\"\n    try:\n        from invenio.modules.accounts.models import UserEXT\n    except ImportError:\n        from invenio_accounts.models import UserEXT\n    q = UserEXT.query\n    return q.count(), q.all()", "language": "python", "code": "def get(*args, **kwargs):\n    \"\"\"Get UserEXT objects.\"\"\"\n    try:\n        from invenio.modules.accounts.models import UserEXT\n    except ImportError:\n        from invenio_accounts.models import UserEXT\n    q = UserEXT.query\n    return q.count(), q.all()", "code_tokens": ["def", "get", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "try", ":", "from", "invenio", ".", "modules", ".", "accounts", ".", "models", "import", "UserEXT", "except", "ImportError", ":", "from", "invenio_accounts", ".", "models", "import", "UserEXT", "q", "=", "UserEXT", ".", "query", "return", "q", ".", "count", "(", ")", ",", "q", ".", "all", "(", ")"], "docstring": "Get UserEXT objects.", "docstring_tokens": ["Get", "UserEXT", "objects", "."], "sha": "6902c6968a39b747d15e32363f43b7dffe2622c2", "url": "https://github.com/inveniosoftware/invenio-migrator/blob/6902c6968a39b747d15e32363f43b7dffe2622c2/invenio_migrator/legacy/userexts.py#L30-L37", "partition": "test"}
{"repo": "ekmmetering/ekmmeters", "path": "ekmmeters.py", "func_name": "Meter.readSchedules", "original_string": "def readSchedules(self, tableset):\n        \"\"\" Serial call to read schedule tariffs buffer\n\n        Args:\n            tableset (int): :class:`~ekmmeters.ReadSchedules` buffer to return.\n\n        Returns:\n            bool: True on completion and ACK.\n        \"\"\"\n        self.setContext(\"readSchedules\")\n        try:\n            req_table = binascii.hexlify(str(tableset).zfill(1))\n            req_str = \"01523102303037\" + req_table + \"282903\"\n\n            self.request(False)\n            req_crc = self.calc_crc16(req_str[2:].decode(\"hex\"))\n            req_str += req_crc\n            self.m_serial_port.write(req_str.decode(\"hex\"))\n            raw_ret = self.m_serial_port.getResponse(self.getContext())\n            self.serialPostEnd()\n            return_crc = self.calc_crc16(raw_ret[1:-2])\n\n            if tableset == ReadSchedules.Schedules_1_To_4:\n                unpacked_read = self.unpackStruct(raw_ret, self.m_schd_1_to_4)\n                self.convertData(unpacked_read, self.m_schd_1_to_4, self.m_kwh_precision)\n                if str(return_crc) == str(self.m_schd_1_to_4[\"crc16\"][MeterData.StringValue]):\n                    ekm_log(\"Schedules 1 to 4 CRC success (06 return\")\n                    self.setContext(\"\")\n                    return True\n\n            elif tableset == ReadSchedules.Schedules_5_To_6:\n                unpacked_read = self.unpackStruct(raw_ret, self.m_schd_5_to_6)\n                self.convertData(unpacked_read, self.m_schd_5_to_6, self.m_kwh_precision)\n                if str(return_crc) == str(self.m_schd_5_to_6[\"crc16\"][MeterData.StringValue]):\n                    ekm_log(\"Schedules 5 to 8 CRC success (06 return)\")\n                    self.setContext(\"\")\n                    return True\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        self.setContext(\"\")\n        return False", "language": "python", "code": "def readSchedules(self, tableset):\n        \"\"\" Serial call to read schedule tariffs buffer\n\n        Args:\n            tableset (int): :class:`~ekmmeters.ReadSchedules` buffer to return.\n\n        Returns:\n            bool: True on completion and ACK.\n        \"\"\"\n        self.setContext(\"readSchedules\")\n        try:\n            req_table = binascii.hexlify(str(tableset).zfill(1))\n            req_str = \"01523102303037\" + req_table + \"282903\"\n\n            self.request(False)\n            req_crc = self.calc_crc16(req_str[2:].decode(\"hex\"))\n            req_str += req_crc\n            self.m_serial_port.write(req_str.decode(\"hex\"))\n            raw_ret = self.m_serial_port.getResponse(self.getContext())\n            self.serialPostEnd()\n            return_crc = self.calc_crc16(raw_ret[1:-2])\n\n            if tableset == ReadSchedules.Schedules_1_To_4:\n                unpacked_read = self.unpackStruct(raw_ret, self.m_schd_1_to_4)\n                self.convertData(unpacked_read, self.m_schd_1_to_4, self.m_kwh_precision)\n                if str(return_crc) == str(self.m_schd_1_to_4[\"crc16\"][MeterData.StringValue]):\n                    ekm_log(\"Schedules 1 to 4 CRC success (06 return\")\n                    self.setContext(\"\")\n                    return True\n\n            elif tableset == ReadSchedules.Schedules_5_To_6:\n                unpacked_read = self.unpackStruct(raw_ret, self.m_schd_5_to_6)\n                self.convertData(unpacked_read, self.m_schd_5_to_6, self.m_kwh_precision)\n                if str(return_crc) == str(self.m_schd_5_to_6[\"crc16\"][MeterData.StringValue]):\n                    ekm_log(\"Schedules 5 to 8 CRC success (06 return)\")\n                    self.setContext(\"\")\n                    return True\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        self.setContext(\"\")\n        return False", "code_tokens": ["def", "readSchedules", "(", "self", ",", "tableset", ")", ":", "self", ".", "setContext", "(", "\"readSchedules\"", ")", "try", ":", "req_table", "=", "binascii", ".", "hexlify", "(", "str", "(", "tableset", ")", ".", "zfill", "(", "1", ")", ")", "req_str", "=", "\"01523102303037\"", "+", "req_table", "+", "\"282903\"", "self", ".", "request", "(", "False", ")", "req_crc", "=", "self", ".", "calc_crc16", "(", "req_str", "[", "2", ":", "]", ".", "decode", "(", "\"hex\"", ")", ")", "req_str", "+=", "req_crc", "self", ".", "m_serial_port", ".", "write", "(", "req_str", ".", "decode", "(", "\"hex\"", ")", ")", "raw_ret", "=", "self", ".", "m_serial_port", ".", "getResponse", "(", "self", ".", "getContext", "(", ")", ")", "self", ".", "serialPostEnd", "(", ")", "return_crc", "=", "self", ".", "calc_crc16", "(", "raw_ret", "[", "1", ":", "-", "2", "]", ")", "if", "tableset", "==", "ReadSchedules", ".", "Schedules_1_To_4", ":", "unpacked_read", "=", "self", ".", "unpackStruct", "(", "raw_ret", ",", "self", ".", "m_schd_1_to_4", ")", "self", ".", "convertData", "(", "unpacked_read", ",", "self", ".", "m_schd_1_to_4", ",", "self", ".", "m_kwh_precision", ")", "if", "str", "(", "return_crc", ")", "==", "str", "(", "self", ".", "m_schd_1_to_4", "[", "\"crc16\"", "]", "[", "MeterData", ".", "StringValue", "]", ")", ":", "ekm_log", "(", "\"Schedules 1 to 4 CRC success (06 return\"", ")", "self", ".", "setContext", "(", "\"\"", ")", "return", "True", "elif", "tableset", "==", "ReadSchedules", ".", "Schedules_5_To_6", ":", "unpacked_read", "=", "self", ".", "unpackStruct", "(", "raw_ret", ",", "self", ".", "m_schd_5_to_6", ")", "self", ".", "convertData", "(", "unpacked_read", ",", "self", ".", "m_schd_5_to_6", ",", "self", ".", "m_kwh_precision", ")", "if", "str", "(", "return_crc", ")", "==", "str", "(", "self", ".", "m_schd_5_to_6", "[", "\"crc16\"", "]", "[", "MeterData", ".", "StringValue", "]", ")", ":", "ekm_log", "(", "\"Schedules 5 to 8 CRC success (06 return)\"", ")", "self", ".", "setContext", "(", "\"\"", ")", "return", "True", "except", ":", "ekm_log", "(", "traceback", ".", "format_exc", "(", "sys", ".", "exc_info", "(", ")", ")", ")", "self", ".", "setContext", "(", "\"\"", ")", "return", "False"], "docstring": "Serial call to read schedule tariffs buffer\n\n        Args:\n            tableset (int): :class:`~ekmmeters.ReadSchedules` buffer to return.\n\n        Returns:\n            bool: True on completion and ACK.", "docstring_tokens": ["Serial", "call", "to", "read", "schedule", "tariffs", "buffer"], "sha": "b3748bdf30263bfa46ea40157bdf8df2522e1904", "url": "https://github.com/ekmmetering/ekmmeters/blob/b3748bdf30263bfa46ea40157bdf8df2522e1904/ekmmeters.py#L2690-L2731", "partition": "test"}
{"repo": "bjoernricks/python-quilt", "path": "quilt/db.py", "func_name": "PatchSeries.patches_after", "original_string": "def patches_after(self, patch):\n        \"\"\" Returns a list of patches after patch from the patches list \"\"\"\n        return [line.get_patch() for line in self._patchlines_after(patch) if\n                line.get_patch()]", "language": "python", "code": "def patches_after(self, patch):\n        \"\"\" Returns a list of patches after patch from the patches list \"\"\"\n        return [line.get_patch() for line in self._patchlines_after(patch) if\n                line.get_patch()]", "code_tokens": ["def", "patches_after", "(", "self", ",", "patch", ")", ":", "return", "[", "line", ".", "get_patch", "(", ")", "for", "line", "in", "self", ".", "_patchlines_after", "(", "patch", ")", "if", "line", ".", "get_patch", "(", ")", "]"], "docstring": "Returns a list of patches after patch from the patches list", "docstring_tokens": ["Returns", "a", "list", "of", "patches", "after", "patch", "from", "the", "patches", "list"], "sha": "fae88237f601848cc34d073584d9dcb409f01777", "url": "https://github.com/bjoernricks/python-quilt/blob/fae88237f601848cc34d073584d9dcb409f01777/quilt/db.py#L219-L222", "partition": "test"}
{"repo": "josiahcarlson/rom", "path": "rom/model.py", "func_name": "Model.get", "original_string": "def get(cls, ids):\n        '''\n        Will fetch one or more entities of this type from the session or\n        Redis.\n\n        Used like::\n\n            MyModel.get(5)\n            MyModel.get([1, 6, 2, 4])\n\n        Passing a list or a tuple will return multiple entities, in the same\n        order that the ids were passed.\n        '''\n        conn = _connect(cls)\n        # prepare the ids\n        single = not isinstance(ids, (list, tuple, set, frozenset))\n        if single:\n            ids = [ids]\n        pks = ['%s:%s'%(cls._namespace, id) for id in map(int, ids)]\n        # get from the session, if possible\n        out = list(map(session.get, pks))\n        # if we couldn't get an instance from the session, load from Redis\n        if None in out:\n            pipe = conn.pipeline(True)\n            idxs = []\n            # Fetch missing data\n            for i, data in enumerate(out):\n                if data is None:\n                    idxs.append(i)\n                    pipe.hgetall(pks[i])\n            # Update output list\n            for i, data in zip(idxs, pipe.execute()):\n                if data:\n                    if six.PY3:\n                        data = dict((k.decode(), v.decode()) for k, v in data.items())\n                    out[i] = cls(_loading=True, **data)\n            # Get rid of missing models\n            out = [x for x in out if x]\n        if single:\n            return out[0] if out else None\n        return out", "language": "python", "code": "def get(cls, ids):\n        '''\n        Will fetch one or more entities of this type from the session or\n        Redis.\n\n        Used like::\n\n            MyModel.get(5)\n            MyModel.get([1, 6, 2, 4])\n\n        Passing a list or a tuple will return multiple entities, in the same\n        order that the ids were passed.\n        '''\n        conn = _connect(cls)\n        # prepare the ids\n        single = not isinstance(ids, (list, tuple, set, frozenset))\n        if single:\n            ids = [ids]\n        pks = ['%s:%s'%(cls._namespace, id) for id in map(int, ids)]\n        # get from the session, if possible\n        out = list(map(session.get, pks))\n        # if we couldn't get an instance from the session, load from Redis\n        if None in out:\n            pipe = conn.pipeline(True)\n            idxs = []\n            # Fetch missing data\n            for i, data in enumerate(out):\n                if data is None:\n                    idxs.append(i)\n                    pipe.hgetall(pks[i])\n            # Update output list\n            for i, data in zip(idxs, pipe.execute()):\n                if data:\n                    if six.PY3:\n                        data = dict((k.decode(), v.decode()) for k, v in data.items())\n                    out[i] = cls(_loading=True, **data)\n            # Get rid of missing models\n            out = [x for x in out if x]\n        if single:\n            return out[0] if out else None\n        return out", "code_tokens": ["def", "get", "(", "cls", ",", "ids", ")", ":", "conn", "=", "_connect", "(", "cls", ")", "# prepare the ids", "single", "=", "not", "isinstance", "(", "ids", ",", "(", "list", ",", "tuple", ",", "set", ",", "frozenset", ")", ")", "if", "single", ":", "ids", "=", "[", "ids", "]", "pks", "=", "[", "'%s:%s'", "%", "(", "cls", ".", "_namespace", ",", "id", ")", "for", "id", "in", "map", "(", "int", ",", "ids", ")", "]", "# get from the session, if possible", "out", "=", "list", "(", "map", "(", "session", ".", "get", ",", "pks", ")", ")", "# if we couldn't get an instance from the session, load from Redis", "if", "None", "in", "out", ":", "pipe", "=", "conn", ".", "pipeline", "(", "True", ")", "idxs", "=", "[", "]", "# Fetch missing data", "for", "i", ",", "data", "in", "enumerate", "(", "out", ")", ":", "if", "data", "is", "None", ":", "idxs", ".", "append", "(", "i", ")", "pipe", ".", "hgetall", "(", "pks", "[", "i", "]", ")", "# Update output list", "for", "i", ",", "data", "in", "zip", "(", "idxs", ",", "pipe", ".", "execute", "(", ")", ")", ":", "if", "data", ":", "if", "six", ".", "PY3", ":", "data", "=", "dict", "(", "(", "k", ".", "decode", "(", ")", ",", "v", ".", "decode", "(", ")", ")", "for", "k", ",", "v", "in", "data", ".", "items", "(", ")", ")", "out", "[", "i", "]", "=", "cls", "(", "_loading", "=", "True", ",", "*", "*", "data", ")", "# Get rid of missing models", "out", "=", "[", "x", "for", "x", "in", "out", "if", "x", "]", "if", "single", ":", "return", "out", "[", "0", "]", "if", "out", "else", "None", "return", "out"], "docstring": "Will fetch one or more entities of this type from the session or\n        Redis.\n\n        Used like::\n\n            MyModel.get(5)\n            MyModel.get([1, 6, 2, 4])\n\n        Passing a list or a tuple will return multiple entities, in the same\n        order that the ids were passed.", "docstring_tokens": ["Will", "fetch", "one", "or", "more", "entities", "of", "this", "type", "from", "the", "session", "or", "Redis", "."], "sha": "8b5607a856341df85df33422accc30ba9294dbdb", "url": "https://github.com/josiahcarlson/rom/blob/8b5607a856341df85df33422accc30ba9294dbdb/rom/model.py#L533-L573", "partition": "test"}
{"repo": "mohan3d/PyOpenload", "path": "openload/openload.py", "func_name": "OpenLoad.remote_upload_status", "original_string": "def remote_upload_status(self, limit=None, remote_upload_id=None):\n        \"\"\"Checks a remote file upload to status.\n\n        Args:\n            limit (:obj:`int`, optional): Maximum number of results (Default: 5, Maximum: 100).\n            remote_upload_id (:obj:`str`, optional): Remote Upload ID.\n\n        Returns:\n            dict: dictionary containing all remote uploads, each dictionary element is a dictionary. ::\n\n                {\n                    \"24\": {\n                      \"id\": \"24\",\n                      \"remoteurl\": \"http://proof.ovh.net/files/100Mio.dat\",\n                      \"status\": \"new\",\n                      \"folderid\": \"4248\",\n                      \"added\": \"2015-02-21 09:20:26\",\n                      \"last_update\": \"2015-02-21 09:20:26\",\n                      \"extid\": False,\n                      \"url\": False\n                    },\n                    \"22\": {\n                      \"id\": \"22\",\n                      \"remoteurl\": \"http://proof.ovh.net/files/1Gio.dat\",\n                      \"status\": \"downloading\",\n                      \"bytes_loaded\": \"823997062\",\n                      \"bytes_total\": \"1073741824\",\n                      \"folderid\": \"4248\",\n                      \"added\": \"2015-02-21 09:20:26\",\n                      \"last_update\": \"2015-02-21 09:21:56\",\n                      \"extid\": False,\n                      \"url\": False\n                    },\n                    ...\n                }\n\n        \"\"\"\n\n        kwargs = {'limit': limit, 'id': remote_upload_id}\n        params = {key: value for key, value in kwargs.items() if value}\n\n        return self._get('remotedl/status', params=params)", "language": "python", "code": "def remote_upload_status(self, limit=None, remote_upload_id=None):\n        \"\"\"Checks a remote file upload to status.\n\n        Args:\n            limit (:obj:`int`, optional): Maximum number of results (Default: 5, Maximum: 100).\n            remote_upload_id (:obj:`str`, optional): Remote Upload ID.\n\n        Returns:\n            dict: dictionary containing all remote uploads, each dictionary element is a dictionary. ::\n\n                {\n                    \"24\": {\n                      \"id\": \"24\",\n                      \"remoteurl\": \"http://proof.ovh.net/files/100Mio.dat\",\n                      \"status\": \"new\",\n                      \"folderid\": \"4248\",\n                      \"added\": \"2015-02-21 09:20:26\",\n                      \"last_update\": \"2015-02-21 09:20:26\",\n                      \"extid\": False,\n                      \"url\": False\n                    },\n                    \"22\": {\n                      \"id\": \"22\",\n                      \"remoteurl\": \"http://proof.ovh.net/files/1Gio.dat\",\n                      \"status\": \"downloading\",\n                      \"bytes_loaded\": \"823997062\",\n                      \"bytes_total\": \"1073741824\",\n                      \"folderid\": \"4248\",\n                      \"added\": \"2015-02-21 09:20:26\",\n                      \"last_update\": \"2015-02-21 09:21:56\",\n                      \"extid\": False,\n                      \"url\": False\n                    },\n                    ...\n                }\n\n        \"\"\"\n\n        kwargs = {'limit': limit, 'id': remote_upload_id}\n        params = {key: value for key, value in kwargs.items() if value}\n\n        return self._get('remotedl/status', params=params)", "code_tokens": ["def", "remote_upload_status", "(", "self", ",", "limit", "=", "None", ",", "remote_upload_id", "=", "None", ")", ":", "kwargs", "=", "{", "'limit'", ":", "limit", ",", "'id'", ":", "remote_upload_id", "}", "params", "=", "{", "key", ":", "value", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", "if", "value", "}", "return", "self", ".", "_get", "(", "'remotedl/status'", ",", "params", "=", "params", ")"], "docstring": "Checks a remote file upload to status.\n\n        Args:\n            limit (:obj:`int`, optional): Maximum number of results (Default: 5, Maximum: 100).\n            remote_upload_id (:obj:`str`, optional): Remote Upload ID.\n\n        Returns:\n            dict: dictionary containing all remote uploads, each dictionary element is a dictionary. ::\n\n                {\n                    \"24\": {\n                      \"id\": \"24\",\n                      \"remoteurl\": \"http://proof.ovh.net/files/100Mio.dat\",\n                      \"status\": \"new\",\n                      \"folderid\": \"4248\",\n                      \"added\": \"2015-02-21 09:20:26\",\n                      \"last_update\": \"2015-02-21 09:20:26\",\n                      \"extid\": False,\n                      \"url\": False\n                    },\n                    \"22\": {\n                      \"id\": \"22\",\n                      \"remoteurl\": \"http://proof.ovh.net/files/1Gio.dat\",\n                      \"status\": \"downloading\",\n                      \"bytes_loaded\": \"823997062\",\n                      \"bytes_total\": \"1073741824\",\n                      \"folderid\": \"4248\",\n                      \"added\": \"2015-02-21 09:20:26\",\n                      \"last_update\": \"2015-02-21 09:21:56\",\n                      \"extid\": False,\n                      \"url\": False\n                    },\n                    ...\n                }", "docstring_tokens": ["Checks", "a", "remote", "file", "upload", "to", "status", "."], "sha": "7f9353915ca5546926ef07be9395c6de60e761b1", "url": "https://github.com/mohan3d/PyOpenload/blob/7f9353915ca5546926ef07be9395c6de60e761b1/openload/openload.py#L290-L331", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/control.py", "func_name": "coverage._should_trace", "original_string": "def _should_trace(self, filename, frame):\n        \"\"\"Decide whether to trace execution in `filename`.\n\n        Calls `_should_trace_with_reason`, and returns just the decision.\n\n        \"\"\"\n        canonical, reason = self._should_trace_with_reason(filename, frame)\n        if self.debug.should('trace'):\n            if not canonical:\n                msg = \"Not tracing %r: %s\" % (filename, reason)\n            else:\n                msg = \"Tracing %r\" % (filename,)\n            self.debug.write(msg)\n        return canonical", "language": "python", "code": "def _should_trace(self, filename, frame):\n        \"\"\"Decide whether to trace execution in `filename`.\n\n        Calls `_should_trace_with_reason`, and returns just the decision.\n\n        \"\"\"\n        canonical, reason = self._should_trace_with_reason(filename, frame)\n        if self.debug.should('trace'):\n            if not canonical:\n                msg = \"Not tracing %r: %s\" % (filename, reason)\n            else:\n                msg = \"Tracing %r\" % (filename,)\n            self.debug.write(msg)\n        return canonical", "code_tokens": ["def", "_should_trace", "(", "self", ",", "filename", ",", "frame", ")", ":", "canonical", ",", "reason", "=", "self", ".", "_should_trace_with_reason", "(", "filename", ",", "frame", ")", "if", "self", ".", "debug", ".", "should", "(", "'trace'", ")", ":", "if", "not", "canonical", ":", "msg", "=", "\"Not tracing %r: %s\"", "%", "(", "filename", ",", "reason", ")", "else", ":", "msg", "=", "\"Tracing %r\"", "%", "(", "filename", ",", ")", "self", ".", "debug", ".", "write", "(", "msg", ")", "return", "canonical"], "docstring": "Decide whether to trace execution in `filename`.\n\n        Calls `_should_trace_with_reason`, and returns just the decision.", "docstring_tokens": ["Decide", "whether", "to", "trace", "execution", "in", "filename", "."], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/control.py#L290-L303", "partition": "test"}
{"repo": "gtaylor/python-route53", "path": "route53/hosted_zone.py", "func_name": "HostedZone.create_cname_record", "original_string": "def create_cname_record(self, name, values, ttl=60, weight=None, region=None,\n                           set_identifier=None):\n        \"\"\"\n        Creates a CNAME record attached to this hosted zone.\n\n        :param str name: The fully qualified name of the record to add.\n        :param list values: A list of value strings for the record.\n        :keyword int ttl: The time-to-live of the record (in seconds).\n        :keyword int weight: *For weighted record sets only*. Among resource record\n            sets that have the same combination of DNS name and type, a value\n            that determines what portion of traffic for the current resource\n            record set is routed to the associated location. Ranges from 0-255.\n        :keyword str region: *For latency-based record sets*. The Amazon EC2 region\n            where the resource that is specified in this resource record set\n            resides.\n        :keyword str set_identifier: *For weighted and latency resource record\n            sets only*. An identifier that differentiates among multiple\n            resource record sets that have the same combination of DNS name\n            and type. 1-128 chars.\n        :rtype: tuple\n        :returns: A tuple in the form of ``(rrset, change_info)``, where\n            ``rrset`` is the newly created CNAMEResourceRecordSet instance.\n        \"\"\"\n\n        self._halt_if_already_deleted()\n\n        # Grab the params/kwargs here for brevity's sake.\n        values = locals()\n        del values['self']\n\n        return self._add_record(CNAMEResourceRecordSet, **values)", "language": "python", "code": "def create_cname_record(self, name, values, ttl=60, weight=None, region=None,\n                           set_identifier=None):\n        \"\"\"\n        Creates a CNAME record attached to this hosted zone.\n\n        :param str name: The fully qualified name of the record to add.\n        :param list values: A list of value strings for the record.\n        :keyword int ttl: The time-to-live of the record (in seconds).\n        :keyword int weight: *For weighted record sets only*. Among resource record\n            sets that have the same combination of DNS name and type, a value\n            that determines what portion of traffic for the current resource\n            record set is routed to the associated location. Ranges from 0-255.\n        :keyword str region: *For latency-based record sets*. The Amazon EC2 region\n            where the resource that is specified in this resource record set\n            resides.\n        :keyword str set_identifier: *For weighted and latency resource record\n            sets only*. An identifier that differentiates among multiple\n            resource record sets that have the same combination of DNS name\n            and type. 1-128 chars.\n        :rtype: tuple\n        :returns: A tuple in the form of ``(rrset, change_info)``, where\n            ``rrset`` is the newly created CNAMEResourceRecordSet instance.\n        \"\"\"\n\n        self._halt_if_already_deleted()\n\n        # Grab the params/kwargs here for brevity's sake.\n        values = locals()\n        del values['self']\n\n        return self._add_record(CNAMEResourceRecordSet, **values)", "code_tokens": ["def", "create_cname_record", "(", "self", ",", "name", ",", "values", ",", "ttl", "=", "60", ",", "weight", "=", "None", ",", "region", "=", "None", ",", "set_identifier", "=", "None", ")", ":", "self", ".", "_halt_if_already_deleted", "(", ")", "# Grab the params/kwargs here for brevity's sake.", "values", "=", "locals", "(", ")", "del", "values", "[", "'self'", "]", "return", "self", ".", "_add_record", "(", "CNAMEResourceRecordSet", ",", "*", "*", "values", ")"], "docstring": "Creates a CNAME record attached to this hosted zone.\n\n        :param str name: The fully qualified name of the record to add.\n        :param list values: A list of value strings for the record.\n        :keyword int ttl: The time-to-live of the record (in seconds).\n        :keyword int weight: *For weighted record sets only*. Among resource record\n            sets that have the same combination of DNS name and type, a value\n            that determines what portion of traffic for the current resource\n            record set is routed to the associated location. Ranges from 0-255.\n        :keyword str region: *For latency-based record sets*. The Amazon EC2 region\n            where the resource that is specified in this resource record set\n            resides.\n        :keyword str set_identifier: *For weighted and latency resource record\n            sets only*. An identifier that differentiates among multiple\n            resource record sets that have the same combination of DNS name\n            and type. 1-128 chars.\n        :rtype: tuple\n        :returns: A tuple in the form of ``(rrset, change_info)``, where\n            ``rrset`` is the newly created CNAMEResourceRecordSet instance.", "docstring_tokens": ["Creates", "a", "CNAME", "record", "attached", "to", "this", "hosted", "zone", "."], "sha": "b9fc7e258a79551c9ed61e4a71668b7f06f9e774", "url": "https://github.com/gtaylor/python-route53/blob/b9fc7e258a79551c9ed61e4a71668b7f06f9e774/route53/hosted_zone.py#L252-L282", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicebus/azure/servicebus/control_client/servicebusservice.py", "func_name": "ServiceBusService.get_subscription", "original_string": "def get_subscription(self, topic_name, subscription_name):\n        '''\n        Gets an existing subscription.\n\n        topic_name:\n            Name of the topic.\n        subscription_name:\n            Name of the subscription.\n        '''\n        _validate_not_none('topic_name', topic_name)\n        _validate_not_none('subscription_name', subscription_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '/' + \\\n            _str(topic_name) + '/subscriptions/' + _str(subscription_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_subscription(response)", "language": "python", "code": "def get_subscription(self, topic_name, subscription_name):\n        '''\n        Gets an existing subscription.\n\n        topic_name:\n            Name of the topic.\n        subscription_name:\n            Name of the subscription.\n        '''\n        _validate_not_none('topic_name', topic_name)\n        _validate_not_none('subscription_name', subscription_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '/' + \\\n            _str(topic_name) + '/subscriptions/' + _str(subscription_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_subscription(response)", "code_tokens": ["def", "get_subscription", "(", "self", ",", "topic_name", ",", "subscription_name", ")", ":", "_validate_not_none", "(", "'topic_name'", ",", "topic_name", ")", "_validate_not_none", "(", "'subscription_name'", ",", "subscription_name", ")", "request", "=", "HTTPRequest", "(", ")", "request", ".", "method", "=", "'GET'", "request", ".", "host", "=", "self", ".", "_get_host", "(", ")", "request", ".", "path", "=", "'/'", "+", "_str", "(", "topic_name", ")", "+", "'/subscriptions/'", "+", "_str", "(", "subscription_name", ")", "+", "''", "request", ".", "path", ",", "request", ".", "query", "=", "self", ".", "_httpclient", ".", "_update_request_uri_query", "(", "request", ")", "# pylint: disable=protected-access", "request", ".", "headers", "=", "self", ".", "_update_service_bus_header", "(", "request", ")", "response", "=", "self", ".", "_perform_request", "(", "request", ")", "return", "_convert_response_to_subscription", "(", "response", ")"], "docstring": "Gets an existing subscription.\n\n        topic_name:\n            Name of the topic.\n        subscription_name:\n            Name of the subscription.", "docstring_tokens": ["Gets", "an", "existing", "subscription", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicebus/azure/servicebus/control_client/servicebusservice.py#L603-L623", "partition": "test"}
{"repo": "funilrys/PyFunceble", "path": "PyFunceble/adblock.py", "func_name": "AdBlock._is_to_ignore", "original_string": "def _is_to_ignore(cls, line):\n        \"\"\"\n        Check if we have to ignore the given line.\n\n        :param line: The line from the file.\n        :type line: str\n        \"\"\"\n\n        # We set the list of regex to match to be\n        # considered as ignored.\n        to_ignore = [r\"(^!|^@@|^\\/|^\\[|^\\.|^-|^_|^\\?|^&)\"]  # , r\"(\\$|,)(image)\"]\n\n        for element in to_ignore:\n            # We loop through the list of regex.\n\n            if Regex(line, element, return_data=False).match():\n                # The currently read line match the currently read\n                # regex.\n\n                # We return true, it has to be ignored.\n                return True\n\n        # Wer return False, it does not has to be ignored.\n        return False", "language": "python", "code": "def _is_to_ignore(cls, line):\n        \"\"\"\n        Check if we have to ignore the given line.\n\n        :param line: The line from the file.\n        :type line: str\n        \"\"\"\n\n        # We set the list of regex to match to be\n        # considered as ignored.\n        to_ignore = [r\"(^!|^@@|^\\/|^\\[|^\\.|^-|^_|^\\?|^&)\"]  # , r\"(\\$|,)(image)\"]\n\n        for element in to_ignore:\n            # We loop through the list of regex.\n\n            if Regex(line, element, return_data=False).match():\n                # The currently read line match the currently read\n                # regex.\n\n                # We return true, it has to be ignored.\n                return True\n\n        # Wer return False, it does not has to be ignored.\n        return False", "code_tokens": ["def", "_is_to_ignore", "(", "cls", ",", "line", ")", ":", "# We set the list of regex to match to be", "# considered as ignored.", "to_ignore", "=", "[", "r\"(^!|^@@|^\\/|^\\[|^\\.|^-|^_|^\\?|^&)\"", "]", "# , r\"(\\$|,)(image)\"]", "for", "element", "in", "to_ignore", ":", "# We loop through the list of regex.", "if", "Regex", "(", "line", ",", "element", ",", "return_data", "=", "False", ")", ".", "match", "(", ")", ":", "# The currently read line match the currently read", "# regex.", "# We return true, it has to be ignored.", "return", "True", "# Wer return False, it does not has to be ignored.", "return", "False"], "docstring": "Check if we have to ignore the given line.\n\n        :param line: The line from the file.\n        :type line: str", "docstring_tokens": ["Check", "if", "we", "have", "to", "ignore", "the", "given", "line", "."], "sha": "cdf69cbde120199171f7158e1c33635753e6e2f5", "url": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/adblock.py#L106-L129", "partition": "test"}
{"repo": "authomatic/authomatic", "path": "authomatic/providers/__init__.py", "func_name": "BaseProvider._session_set", "original_string": "def _session_set(self, key, value):\n        \"\"\"\n        Saves a value to session.\n        \"\"\"\n\n        self.session[self._session_key(key)] = value", "language": "python", "code": "def _session_set(self, key, value):\n        \"\"\"\n        Saves a value to session.\n        \"\"\"\n\n        self.session[self._session_key(key)] = value", "code_tokens": ["def", "_session_set", "(", "self", ",", "key", ",", "value", ")", ":", "self", ".", "session", "[", "self", ".", "_session_key", "(", "key", ")", "]", "=", "value"], "docstring": "Saves a value to session.", "docstring_tokens": ["Saves", "a", "value", "to", "session", "."], "sha": "90a9ce60cc405ae8a2bf5c3713acd5d78579a04e", "url": "https://github.com/authomatic/authomatic/blob/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e/authomatic/providers/__init__.py#L300-L305", "partition": "test"}
{"repo": "treycucco/pyebnf", "path": "pyebnf/compiler.py", "func_name": "Compiler._ast_special_handling_to_code", "original_string": "def _ast_special_handling_to_code(self, special_handling, **kwargs):\n    \"\"\"Convert an AST sepcial handling to python source code.\"\"\"\n    ident = special_handling.value.svalue\n    if ident in PB_SPECIAL_HANDLING:\n      return [\"PB.{0}\".format(ident)]\n    else:\n      return [\"self.{0}\".format(ident)]", "language": "python", "code": "def _ast_special_handling_to_code(self, special_handling, **kwargs):\n    \"\"\"Convert an AST sepcial handling to python source code.\"\"\"\n    ident = special_handling.value.svalue\n    if ident in PB_SPECIAL_HANDLING:\n      return [\"PB.{0}\".format(ident)]\n    else:\n      return [\"self.{0}\".format(ident)]", "code_tokens": ["def", "_ast_special_handling_to_code", "(", "self", ",", "special_handling", ",", "*", "*", "kwargs", ")", ":", "ident", "=", "special_handling", ".", "value", ".", "svalue", "if", "ident", "in", "PB_SPECIAL_HANDLING", ":", "return", "[", "\"PB.{0}\"", ".", "format", "(", "ident", ")", "]", "else", ":", "return", "[", "\"self.{0}\"", ".", "format", "(", "ident", ")", "]"], "docstring": "Convert an AST sepcial handling to python source code.", "docstring_tokens": ["Convert", "an", "AST", "sepcial", "handling", "to", "python", "source", "code", "."], "sha": "3634ddabbe5d73508bcc20f4a591f86a46634e1d", "url": "https://github.com/treycucco/pyebnf/blob/3634ddabbe5d73508bcc20f4a591f86a46634e1d/pyebnf/compiler.py#L376-L382", "partition": "test"}
{"repo": "neithere/argh", "path": "argh/compat.py", "func_name": "getargspec_permissive", "original_string": "def getargspec_permissive(func):\n    \"\"\"\n    An `inspect.getargspec` with a relaxed sanity check to support Cython.\n\n    Motivation:\n\n        A Cython-compiled function is *not* an instance of Python's\n        types.FunctionType.  That is the sanity check the standard Py2\n        library uses in `inspect.getargspec()`.  So, an exception is raised\n        when calling `argh.dispatch_command(cythonCompiledFunc)`.  However,\n        the CyFunctions do have perfectly usable `.func_code` and\n        `.func_defaults` which is all `inspect.getargspec` needs.\n\n        This function just copies `inspect.getargspec()` from the standard\n        library but relaxes the test to a more duck-typing one of having\n        both `.func_code` and `.func_defaults` attributes.\n    \"\"\"\n    if inspect.ismethod(func):\n        func = func.im_func\n\n    # Py2 Stdlib uses isfunction(func) which is too strict for Cython-compiled\n    # functions though such have perfectly usable func_code, func_defaults.\n    if not (hasattr(func, \"func_code\") and hasattr(func, \"func_defaults\")):\n        raise TypeError('{!r} missing func_code or func_defaults'.format(func))\n\n    args, varargs, varkw = inspect.getargs(func.func_code)\n    return inspect.ArgSpec(args, varargs, varkw, func.func_defaults)", "language": "python", "code": "def getargspec_permissive(func):\n    \"\"\"\n    An `inspect.getargspec` with a relaxed sanity check to support Cython.\n\n    Motivation:\n\n        A Cython-compiled function is *not* an instance of Python's\n        types.FunctionType.  That is the sanity check the standard Py2\n        library uses in `inspect.getargspec()`.  So, an exception is raised\n        when calling `argh.dispatch_command(cythonCompiledFunc)`.  However,\n        the CyFunctions do have perfectly usable `.func_code` and\n        `.func_defaults` which is all `inspect.getargspec` needs.\n\n        This function just copies `inspect.getargspec()` from the standard\n        library but relaxes the test to a more duck-typing one of having\n        both `.func_code` and `.func_defaults` attributes.\n    \"\"\"\n    if inspect.ismethod(func):\n        func = func.im_func\n\n    # Py2 Stdlib uses isfunction(func) which is too strict for Cython-compiled\n    # functions though such have perfectly usable func_code, func_defaults.\n    if not (hasattr(func, \"func_code\") and hasattr(func, \"func_defaults\")):\n        raise TypeError('{!r} missing func_code or func_defaults'.format(func))\n\n    args, varargs, varkw = inspect.getargs(func.func_code)\n    return inspect.ArgSpec(args, varargs, varkw, func.func_defaults)", "code_tokens": ["def", "getargspec_permissive", "(", "func", ")", ":", "if", "inspect", ".", "ismethod", "(", "func", ")", ":", "func", "=", "func", ".", "im_func", "# Py2 Stdlib uses isfunction(func) which is too strict for Cython-compiled", "# functions though such have perfectly usable func_code, func_defaults.", "if", "not", "(", "hasattr", "(", "func", ",", "\"func_code\"", ")", "and", "hasattr", "(", "func", ",", "\"func_defaults\"", ")", ")", ":", "raise", "TypeError", "(", "'{!r} missing func_code or func_defaults'", ".", "format", "(", "func", ")", ")", "args", ",", "varargs", ",", "varkw", "=", "inspect", ".", "getargs", "(", "func", ".", "func_code", ")", "return", "inspect", ".", "ArgSpec", "(", "args", ",", "varargs", ",", "varkw", ",", "func", ".", "func_defaults", ")"], "docstring": "An `inspect.getargspec` with a relaxed sanity check to support Cython.\n\n    Motivation:\n\n        A Cython-compiled function is *not* an instance of Python's\n        types.FunctionType.  That is the sanity check the standard Py2\n        library uses in `inspect.getargspec()`.  So, an exception is raised\n        when calling `argh.dispatch_command(cythonCompiledFunc)`.  However,\n        the CyFunctions do have perfectly usable `.func_code` and\n        `.func_defaults` which is all `inspect.getargspec` needs.\n\n        This function just copies `inspect.getargspec()` from the standard\n        library but relaxes the test to a more duck-typing one of having\n        both `.func_code` and `.func_defaults` attributes.", "docstring_tokens": ["An", "inspect", ".", "getargspec", "with", "a", "relaxed", "sanity", "check", "to", "support", "Cython", "."], "sha": "dcd3253f2994400a6a58a700c118c53765bc50a4", "url": "https://github.com/neithere/argh/blob/dcd3253f2994400a6a58a700c118c53765bc50a4/argh/compat.py#L22-L48", "partition": "test"}
{"repo": "HydraChain/hydrachain", "path": "hydrachain/consensus/base.py", "func_name": "Signed.sign", "original_string": "def sign(self, privkey):\n        \"\"\"Sign this with a private key\"\"\"\n        if self.v:\n            raise InvalidSignature(\"already signed\")\n\n        if privkey in (0, '', '\\x00' * 32):\n            raise InvalidSignature(\"Zero privkey cannot sign\")\n        rawhash = sha3(rlp.encode(self, self.__class__.exclude(['v', 'r', 's'])))\n\n        if len(privkey) == 64:\n            privkey = encode_privkey(privkey, 'bin')\n\n        pk = PrivateKey(privkey, raw=True)\n        signature = pk.ecdsa_recoverable_serialize(pk.ecdsa_sign_recoverable(rawhash, raw=True))\n\n        signature = signature[0] + chr(signature[1])\n\n        self.v = ord(signature[64]) + 27\n        self.r = big_endian_to_int(signature[0:32])\n        self.s = big_endian_to_int(signature[32:64])\n\n        self._sender = None\n        return self", "language": "python", "code": "def sign(self, privkey):\n        \"\"\"Sign this with a private key\"\"\"\n        if self.v:\n            raise InvalidSignature(\"already signed\")\n\n        if privkey in (0, '', '\\x00' * 32):\n            raise InvalidSignature(\"Zero privkey cannot sign\")\n        rawhash = sha3(rlp.encode(self, self.__class__.exclude(['v', 'r', 's'])))\n\n        if len(privkey) == 64:\n            privkey = encode_privkey(privkey, 'bin')\n\n        pk = PrivateKey(privkey, raw=True)\n        signature = pk.ecdsa_recoverable_serialize(pk.ecdsa_sign_recoverable(rawhash, raw=True))\n\n        signature = signature[0] + chr(signature[1])\n\n        self.v = ord(signature[64]) + 27\n        self.r = big_endian_to_int(signature[0:32])\n        self.s = big_endian_to_int(signature[32:64])\n\n        self._sender = None\n        return self", "code_tokens": ["def", "sign", "(", "self", ",", "privkey", ")", ":", "if", "self", ".", "v", ":", "raise", "InvalidSignature", "(", "\"already signed\"", ")", "if", "privkey", "in", "(", "0", ",", "''", ",", "'\\x00'", "*", "32", ")", ":", "raise", "InvalidSignature", "(", "\"Zero privkey cannot sign\"", ")", "rawhash", "=", "sha3", "(", "rlp", ".", "encode", "(", "self", ",", "self", ".", "__class__", ".", "exclude", "(", "[", "'v'", ",", "'r'", ",", "'s'", "]", ")", ")", ")", "if", "len", "(", "privkey", ")", "==", "64", ":", "privkey", "=", "encode_privkey", "(", "privkey", ",", "'bin'", ")", "pk", "=", "PrivateKey", "(", "privkey", ",", "raw", "=", "True", ")", "signature", "=", "pk", ".", "ecdsa_recoverable_serialize", "(", "pk", ".", "ecdsa_sign_recoverable", "(", "rawhash", ",", "raw", "=", "True", ")", ")", "signature", "=", "signature", "[", "0", "]", "+", "chr", "(", "signature", "[", "1", "]", ")", "self", ".", "v", "=", "ord", "(", "signature", "[", "64", "]", ")", "+", "27", "self", ".", "r", "=", "big_endian_to_int", "(", "signature", "[", "0", ":", "32", "]", ")", "self", ".", "s", "=", "big_endian_to_int", "(", "signature", "[", "32", ":", "64", "]", ")", "self", ".", "_sender", "=", "None", "return", "self"], "docstring": "Sign this with a private key", "docstring_tokens": ["Sign", "this", "with", "a", "private", "key"], "sha": "6c0919b0575dc8aa481f3a8c703e1a7f0575ecc3", "url": "https://github.com/HydraChain/hydrachain/blob/6c0919b0575dc8aa481f3a8c703e1a7f0575ecc3/hydrachain/consensus/base.py#L71-L93", "partition": "test"}
{"repo": "intel-analytics/BigDL", "path": "pyspark/bigdl/models/local_lenet/local_lenet.py", "func_name": "get_mnist", "original_string": "def get_mnist(data_type=\"train\", location=\"/tmp/mnist\"):\n    \"\"\"\n    Get mnist dataset with features and label as ndarray.\n    Data would be downloaded automatically if it doesn't present at the specific location.\n\n    :param data_type: \"train\" for training data and \"test\" for testing data.\n    :param location: Location to store mnist dataset.\n    :return: (features: ndarray, label: ndarray)\n    \"\"\"\n    X, Y = mnist.read_data_sets(location, data_type)\n    return X, Y + 1", "language": "python", "code": "def get_mnist(data_type=\"train\", location=\"/tmp/mnist\"):\n    \"\"\"\n    Get mnist dataset with features and label as ndarray.\n    Data would be downloaded automatically if it doesn't present at the specific location.\n\n    :param data_type: \"train\" for training data and \"test\" for testing data.\n    :param location: Location to store mnist dataset.\n    :return: (features: ndarray, label: ndarray)\n    \"\"\"\n    X, Y = mnist.read_data_sets(location, data_type)\n    return X, Y + 1", "code_tokens": ["def", "get_mnist", "(", "data_type", "=", "\"train\"", ",", "location", "=", "\"/tmp/mnist\"", ")", ":", "X", ",", "Y", "=", "mnist", ".", "read_data_sets", "(", "location", ",", "data_type", ")", "return", "X", ",", "Y", "+", "1"], "docstring": "Get mnist dataset with features and label as ndarray.\n    Data would be downloaded automatically if it doesn't present at the specific location.\n\n    :param data_type: \"train\" for training data and \"test\" for testing data.\n    :param location: Location to store mnist dataset.\n    :return: (features: ndarray, label: ndarray)", "docstring_tokens": ["Get", "mnist", "dataset", "with", "features", "and", "label", "as", "ndarray", ".", "Data", "would", "be", "downloaded", "automatically", "if", "it", "doesn", "t", "present", "at", "the", "specific", "location", "."], "sha": "e9c19788285986ab789a2e2998f9a85d7524779f", "url": "https://github.com/intel-analytics/BigDL/blob/e9c19788285986ab789a2e2998f9a85d7524779f/pyspark/bigdl/models/local_lenet/local_lenet.py#L25-L35", "partition": "test"}
{"repo": "rwl/godot", "path": "godot/util.py", "func_name": "move_to_origin", "original_string": "def move_to_origin(components):\n    \"\"\" Components are positioned relative to their container. Use this\n        method to position the bottom-left corner of the components at\n        the origin.\n    \"\"\"\n    for component in components:\n        if isinstance(component, Ellipse):\n            component.x_origin = component.e_width\n            component.y_origin = component.e_height\n\n        elif isinstance(component, (Polygon, BSpline)):\n            min_x = min( [t[0] for t in component.points] )\n            min_y = min( [t[1] for t in component.points] )\n\n            component.points = [\n                ( p[0]-min_x, p[1]-min_y ) for p in component.points\n            ]\n\n        elif isinstance(component, Text):\n            font = str_to_font( str(component.pen.font) )\n            component.text_x = 0#-( component.text_w / 2 )\n            component.text_y = 0", "language": "python", "code": "def move_to_origin(components):\n    \"\"\" Components are positioned relative to their container. Use this\n        method to position the bottom-left corner of the components at\n        the origin.\n    \"\"\"\n    for component in components:\n        if isinstance(component, Ellipse):\n            component.x_origin = component.e_width\n            component.y_origin = component.e_height\n\n        elif isinstance(component, (Polygon, BSpline)):\n            min_x = min( [t[0] for t in component.points] )\n            min_y = min( [t[1] for t in component.points] )\n\n            component.points = [\n                ( p[0]-min_x, p[1]-min_y ) for p in component.points\n            ]\n\n        elif isinstance(component, Text):\n            font = str_to_font( str(component.pen.font) )\n            component.text_x = 0#-( component.text_w / 2 )\n            component.text_y = 0", "code_tokens": ["def", "move_to_origin", "(", "components", ")", ":", "for", "component", "in", "components", ":", "if", "isinstance", "(", "component", ",", "Ellipse", ")", ":", "component", ".", "x_origin", "=", "component", ".", "e_width", "component", ".", "y_origin", "=", "component", ".", "e_height", "elif", "isinstance", "(", "component", ",", "(", "Polygon", ",", "BSpline", ")", ")", ":", "min_x", "=", "min", "(", "[", "t", "[", "0", "]", "for", "t", "in", "component", ".", "points", "]", ")", "min_y", "=", "min", "(", "[", "t", "[", "1", "]", "for", "t", "in", "component", ".", "points", "]", ")", "component", ".", "points", "=", "[", "(", "p", "[", "0", "]", "-", "min_x", ",", "p", "[", "1", "]", "-", "min_y", ")", "for", "p", "in", "component", ".", "points", "]", "elif", "isinstance", "(", "component", ",", "Text", ")", ":", "font", "=", "str_to_font", "(", "str", "(", "component", ".", "pen", ".", "font", ")", ")", "component", ".", "text_x", "=", "0", "#-( component.text_w / 2 )", "component", ".", "text_y", "=", "0"], "docstring": "Components are positioned relative to their container. Use this\n        method to position the bottom-left corner of the components at\n        the origin.", "docstring_tokens": ["Components", "are", "positioned", "relative", "to", "their", "container", ".", "Use", "this", "method", "to", "position", "the", "bottom", "-", "left", "corner", "of", "the", "components", "at", "the", "origin", "."], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/util.py#L139-L160", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/message/message_store.py", "func_name": "MessagesStore.add_renamed_message", "original_string": "def add_renamed_message(self, old_id, old_symbol, new_symbol):\n        \"\"\"Register the old ID and symbol for a warning that was renamed.\n\n        This allows users to keep using the old ID/symbol in suppressions.\n        \"\"\"\n        message_definition = self.get_message_definitions(new_symbol)[0]\n        message_definition.old_names.append((old_id, old_symbol))\n        self._register_alternative_name(message_definition, old_id, old_symbol)", "language": "python", "code": "def add_renamed_message(self, old_id, old_symbol, new_symbol):\n        \"\"\"Register the old ID and symbol for a warning that was renamed.\n\n        This allows users to keep using the old ID/symbol in suppressions.\n        \"\"\"\n        message_definition = self.get_message_definitions(new_symbol)[0]\n        message_definition.old_names.append((old_id, old_symbol))\n        self._register_alternative_name(message_definition, old_id, old_symbol)", "code_tokens": ["def", "add_renamed_message", "(", "self", ",", "old_id", ",", "old_symbol", ",", "new_symbol", ")", ":", "message_definition", "=", "self", ".", "get_message_definitions", "(", "new_symbol", ")", "[", "0", "]", "message_definition", ".", "old_names", ".", "append", "(", "(", "old_id", ",", "old_symbol", ")", ")", "self", ".", "_register_alternative_name", "(", "message_definition", ",", "old_id", ",", "old_symbol", ")"], "docstring": "Register the old ID and symbol for a warning that was renamed.\n\n        This allows users to keep using the old ID/symbol in suppressions.", "docstring_tokens": ["Register", "the", "old", "ID", "and", "symbol", "for", "a", "warning", "that", "was", "renamed", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/message/message_store.py#L37-L44", "partition": "test"}
{"repo": "un33k/django-toolware", "path": "toolware/templatetags/generic.py", "func_name": "splitBy", "original_string": "def splitBy(data, num):\n    \"\"\" Turn a list to list of list \"\"\"\n    return [data[i:i + num] for i in range(0, len(data), num)]", "language": "python", "code": "def splitBy(data, num):\n    \"\"\" Turn a list to list of list \"\"\"\n    return [data[i:i + num] for i in range(0, len(data), num)]", "code_tokens": ["def", "splitBy", "(", "data", ",", "num", ")", ":", "return", "[", "data", "[", "i", ":", "i", "+", "num", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "data", ")", ",", "num", ")", "]"], "docstring": "Turn a list to list of list", "docstring_tokens": ["Turn", "a", "list", "to", "list", "of", "list"], "sha": "973f3e003dc38b812897dab88455bee37dcaf931", "url": "https://github.com/un33k/django-toolware/blob/973f3e003dc38b812897dab88455bee37dcaf931/toolware/templatetags/generic.py#L18-L20", "partition": "test"}
{"repo": "3DLIRIOUS/MeshLabXML", "path": "meshlabxml/compute.py", "func_name": "parse_hausdorff", "original_string": "def parse_hausdorff(ml_log, log=None, print_output=False):\n    \"\"\"Parse the ml_log file generated by the hausdorff_distance function.\n\n    Args:\n        ml_log (str): MeshLab log file to parse\n        log (str): filename to log output\n\n    Returns:\n        dict: dictionary with the following keys:\n            number_points (int): number of points in mesh\n            min_distance (float): minimum hausdorff distance\n            max_distance (float): maximum hausdorff distance\n            mean_distance (float): mean hausdorff distance\n            rms_distance (float): root mean square distance\n\n    \"\"\"\n    hausdorff_distance = {\"min_distance\": 0.0,\n                          \"max_distance\": 0.0,\n                          \"mean_distance\": 0.0,\n                          \"rms_distance\": 0.0,\n                          \"number_points\": 0}\n    with open(ml_log) as fread:\n        result = fread.readlines()\n        data = \"\"\n\n        for idx, line in enumerate(result):\n            m = re.match(r\"\\s*Sampled (\\d+) pts.*\", line)\n            if m is not None:\n                hausdorff_distance[\"number_points\"] = int(m.group(1))\n            if 'Hausdorff Distance computed' in line:\n                data = result[idx + 2]\n\n        m = re.match(r\"\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\", data)\n        hausdorff_distance[\"min_distance\"] = float(m.group(1))\n        hausdorff_distance[\"max_distance\"] = float(m.group(2))\n        hausdorff_distance[\"mean_distance\"] = float(m.group(3))\n        hausdorff_distance[\"rms_distance\"] = float(m.group(4))\n        for key, value in hausdorff_distance.items():\n            if log is not None:\n                log_file = open(log, 'a')\n                log_file.write('{:16} = {}\\n'.format(key, value))\n                log_file.close()\n            elif print_output:\n                print('{:16} = {}'.format(key, value))\n        return hausdorff_distance", "language": "python", "code": "def parse_hausdorff(ml_log, log=None, print_output=False):\n    \"\"\"Parse the ml_log file generated by the hausdorff_distance function.\n\n    Args:\n        ml_log (str): MeshLab log file to parse\n        log (str): filename to log output\n\n    Returns:\n        dict: dictionary with the following keys:\n            number_points (int): number of points in mesh\n            min_distance (float): minimum hausdorff distance\n            max_distance (float): maximum hausdorff distance\n            mean_distance (float): mean hausdorff distance\n            rms_distance (float): root mean square distance\n\n    \"\"\"\n    hausdorff_distance = {\"min_distance\": 0.0,\n                          \"max_distance\": 0.0,\n                          \"mean_distance\": 0.0,\n                          \"rms_distance\": 0.0,\n                          \"number_points\": 0}\n    with open(ml_log) as fread:\n        result = fread.readlines()\n        data = \"\"\n\n        for idx, line in enumerate(result):\n            m = re.match(r\"\\s*Sampled (\\d+) pts.*\", line)\n            if m is not None:\n                hausdorff_distance[\"number_points\"] = int(m.group(1))\n            if 'Hausdorff Distance computed' in line:\n                data = result[idx + 2]\n\n        m = re.match(r\"\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\", data)\n        hausdorff_distance[\"min_distance\"] = float(m.group(1))\n        hausdorff_distance[\"max_distance\"] = float(m.group(2))\n        hausdorff_distance[\"mean_distance\"] = float(m.group(3))\n        hausdorff_distance[\"rms_distance\"] = float(m.group(4))\n        for key, value in hausdorff_distance.items():\n            if log is not None:\n                log_file = open(log, 'a')\n                log_file.write('{:16} = {}\\n'.format(key, value))\n                log_file.close()\n            elif print_output:\n                print('{:16} = {}'.format(key, value))\n        return hausdorff_distance", "code_tokens": ["def", "parse_hausdorff", "(", "ml_log", ",", "log", "=", "None", ",", "print_output", "=", "False", ")", ":", "hausdorff_distance", "=", "{", "\"min_distance\"", ":", "0.0", ",", "\"max_distance\"", ":", "0.0", ",", "\"mean_distance\"", ":", "0.0", ",", "\"rms_distance\"", ":", "0.0", ",", "\"number_points\"", ":", "0", "}", "with", "open", "(", "ml_log", ")", "as", "fread", ":", "result", "=", "fread", ".", "readlines", "(", ")", "data", "=", "\"\"", "for", "idx", ",", "line", "in", "enumerate", "(", "result", ")", ":", "m", "=", "re", ".", "match", "(", "r\"\\s*Sampled (\\d+) pts.*\"", ",", "line", ")", "if", "m", "is", "not", "None", ":", "hausdorff_distance", "[", "\"number_points\"", "]", "=", "int", "(", "m", ".", "group", "(", "1", ")", ")", "if", "'Hausdorff Distance computed'", "in", "line", ":", "data", "=", "result", "[", "idx", "+", "2", "]", "m", "=", "re", ".", "match", "(", "r\"\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\"", ",", "data", ")", "hausdorff_distance", "[", "\"min_distance\"", "]", "=", "float", "(", "m", ".", "group", "(", "1", ")", ")", "hausdorff_distance", "[", "\"max_distance\"", "]", "=", "float", "(", "m", ".", "group", "(", "2", ")", ")", "hausdorff_distance", "[", "\"mean_distance\"", "]", "=", "float", "(", "m", ".", "group", "(", "3", ")", ")", "hausdorff_distance", "[", "\"rms_distance\"", "]", "=", "float", "(", "m", ".", "group", "(", "4", ")", ")", "for", "key", ",", "value", "in", "hausdorff_distance", ".", "items", "(", ")", ":", "if", "log", "is", "not", "None", ":", "log_file", "=", "open", "(", "log", ",", "'a'", ")", "log_file", ".", "write", "(", "'{:16} = {}\\n'", ".", "format", "(", "key", ",", "value", ")", ")", "log_file", ".", "close", "(", ")", "elif", "print_output", ":", "print", "(", "'{:16} = {}'", ".", "format", "(", "key", ",", "value", ")", ")", "return", "hausdorff_distance"], "docstring": "Parse the ml_log file generated by the hausdorff_distance function.\n\n    Args:\n        ml_log (str): MeshLab log file to parse\n        log (str): filename to log output\n\n    Returns:\n        dict: dictionary with the following keys:\n            number_points (int): number of points in mesh\n            min_distance (float): minimum hausdorff distance\n            max_distance (float): maximum hausdorff distance\n            mean_distance (float): mean hausdorff distance\n            rms_distance (float): root mean square distance", "docstring_tokens": ["Parse", "the", "ml_log", "file", "generated", "by", "the", "hausdorff_distance", "function", "."], "sha": "177cce21e92baca500f56a932d66bd9a33257af8", "url": "https://github.com/3DLIRIOUS/MeshLabXML/blob/177cce21e92baca500f56a932d66bd9a33257af8/meshlabxml/compute.py#L311-L355", "partition": "test"}
{"repo": "scheibler/khard", "path": "khard/address_book.py", "func_name": "AddressBook._compare_uids", "original_string": "def _compare_uids(uid1, uid2):\n        \"\"\"Calculate the minimum length of initial substrings of uid1 and uid2\n        for them to be different.\n\n        :param uid1: first uid to compare\n        :type uid1: str\n        :param uid2: second uid to compare\n        :type uid2: str\n        :returns: the length of the shortes unequal initial substrings\n        :rtype: int\n        \"\"\"\n        sum = 0\n        for char1, char2 in zip(uid1, uid2):\n            if char1 == char2:\n                sum += 1\n            else:\n                break\n        return sum", "language": "python", "code": "def _compare_uids(uid1, uid2):\n        \"\"\"Calculate the minimum length of initial substrings of uid1 and uid2\n        for them to be different.\n\n        :param uid1: first uid to compare\n        :type uid1: str\n        :param uid2: second uid to compare\n        :type uid2: str\n        :returns: the length of the shortes unequal initial substrings\n        :rtype: int\n        \"\"\"\n        sum = 0\n        for char1, char2 in zip(uid1, uid2):\n            if char1 == char2:\n                sum += 1\n            else:\n                break\n        return sum", "code_tokens": ["def", "_compare_uids", "(", "uid1", ",", "uid2", ")", ":", "sum", "=", "0", "for", "char1", ",", "char2", "in", "zip", "(", "uid1", ",", "uid2", ")", ":", "if", "char1", "==", "char2", ":", "sum", "+=", "1", "else", ":", "break", "return", "sum"], "docstring": "Calculate the minimum length of initial substrings of uid1 and uid2\n        for them to be different.\n\n        :param uid1: first uid to compare\n        :type uid1: str\n        :param uid2: second uid to compare\n        :type uid2: str\n        :returns: the length of the shortes unequal initial substrings\n        :rtype: int", "docstring_tokens": ["Calculate", "the", "minimum", "length", "of", "initial", "substrings", "of", "uid1", "and", "uid2", "for", "them", "to", "be", "different", "."], "sha": "0f69430c2680f1ff5f073a977a3c5b753b96cc17", "url": "https://github.com/scheibler/khard/blob/0f69430c2680f1ff5f073a977a3c5b753b96cc17/khard/address_book.py#L59-L76", "partition": "test"}
{"repo": "vaexio/vaex", "path": "packages/vaex-core/vaex/dataframe.py", "func_name": "DataFrame.select_non_missing", "original_string": "def select_non_missing(self, drop_nan=True, drop_masked=True, column_names=None, mode=\"replace\", name=\"default\"):\n        \"\"\"Create a selection that selects rows having non missing values for all columns in column_names.\n\n        The name reflect Panda's, no rows are really dropped, but a mask is kept to keep track of the selection\n\n        :param drop_nan: drop rows when there is a NaN in any of the columns (will only affect float values)\n        :param drop_masked: drop rows when there is a masked value in any of the columns\n        :param column_names: The columns to consider, default: all (real, non-virtual) columns\n        :param str mode: Possible boolean operator: replace/and/or/xor/subtract\n        :param str name: history tree or selection 'slot' to use\n        :return:\n        \"\"\"\n        column_names = column_names or self.get_column_names(virtual=False)\n\n        def create(current):\n            return selections.SelectionDropNa(drop_nan, drop_masked, column_names, current, mode)\n        self._selection(create, name)", "language": "python", "code": "def select_non_missing(self, drop_nan=True, drop_masked=True, column_names=None, mode=\"replace\", name=\"default\"):\n        \"\"\"Create a selection that selects rows having non missing values for all columns in column_names.\n\n        The name reflect Panda's, no rows are really dropped, but a mask is kept to keep track of the selection\n\n        :param drop_nan: drop rows when there is a NaN in any of the columns (will only affect float values)\n        :param drop_masked: drop rows when there is a masked value in any of the columns\n        :param column_names: The columns to consider, default: all (real, non-virtual) columns\n        :param str mode: Possible boolean operator: replace/and/or/xor/subtract\n        :param str name: history tree or selection 'slot' to use\n        :return:\n        \"\"\"\n        column_names = column_names or self.get_column_names(virtual=False)\n\n        def create(current):\n            return selections.SelectionDropNa(drop_nan, drop_masked, column_names, current, mode)\n        self._selection(create, name)", "code_tokens": ["def", "select_non_missing", "(", "self", ",", "drop_nan", "=", "True", ",", "drop_masked", "=", "True", ",", "column_names", "=", "None", ",", "mode", "=", "\"replace\"", ",", "name", "=", "\"default\"", ")", ":", "column_names", "=", "column_names", "or", "self", ".", "get_column_names", "(", "virtual", "=", "False", ")", "def", "create", "(", "current", ")", ":", "return", "selections", ".", "SelectionDropNa", "(", "drop_nan", ",", "drop_masked", ",", "column_names", ",", "current", ",", "mode", ")", "self", ".", "_selection", "(", "create", ",", "name", ")"], "docstring": "Create a selection that selects rows having non missing values for all columns in column_names.\n\n        The name reflect Panda's, no rows are really dropped, but a mask is kept to keep track of the selection\n\n        :param drop_nan: drop rows when there is a NaN in any of the columns (will only affect float values)\n        :param drop_masked: drop rows when there is a masked value in any of the columns\n        :param column_names: The columns to consider, default: all (real, non-virtual) columns\n        :param str mode: Possible boolean operator: replace/and/or/xor/subtract\n        :param str name: history tree or selection 'slot' to use\n        :return:", "docstring_tokens": ["Create", "a", "selection", "that", "selects", "rows", "having", "non", "missing", "values", "for", "all", "columns", "in", "column_names", "."], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/dataframe.py#L4132-L4148", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/jenkins.py", "func_name": "JenkinsClient.get_jobs", "original_string": "def get_jobs(self):\n        \"\"\" Retrieve all jobs\"\"\"\n\n        url_jenkins = urijoin(self.base_url, \"api\", \"json\")\n\n        response = self.fetch(url_jenkins)\n        return response.text", "language": "python", "code": "def get_jobs(self):\n        \"\"\" Retrieve all jobs\"\"\"\n\n        url_jenkins = urijoin(self.base_url, \"api\", \"json\")\n\n        response = self.fetch(url_jenkins)\n        return response.text", "code_tokens": ["def", "get_jobs", "(", "self", ")", ":", "url_jenkins", "=", "urijoin", "(", "self", ".", "base_url", ",", "\"api\"", ",", "\"json\"", ")", "response", "=", "self", ".", "fetch", "(", "url_jenkins", ")", "return", "response", ".", "text"], "docstring": "Retrieve all jobs", "docstring_tokens": ["Retrieve", "all", "jobs"], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/jenkins.py#L222-L228", "partition": "test"}
{"repo": "LordSputnik/mutagen", "path": "mutagen/apev2.py", "func_name": "APEValue", "original_string": "def APEValue(value, kind):\n    \"\"\"APEv2 tag value factory.\n\n    Use this if you need to specify the value's type manually.  Binary\n    and text data are automatically detected by APEv2.__setitem__.\n    \"\"\"\n\n    if kind in (TEXT, EXTERNAL):\n        if not isinstance(value, text_type):\n            # stricter with py3\n            if PY3:\n                raise TypeError(\"str only for text/external values\")\n        else:\n            value = value.encode(\"utf-8\")\n\n    if kind == TEXT:\n        return APETextValue(value, kind)\n    elif kind == BINARY:\n        return APEBinaryValue(value, kind)\n    elif kind == EXTERNAL:\n        return APEExtValue(value, kind)\n    else:\n        raise ValueError(\"kind must be TEXT, BINARY, or EXTERNAL\")", "language": "python", "code": "def APEValue(value, kind):\n    \"\"\"APEv2 tag value factory.\n\n    Use this if you need to specify the value's type manually.  Binary\n    and text data are automatically detected by APEv2.__setitem__.\n    \"\"\"\n\n    if kind in (TEXT, EXTERNAL):\n        if not isinstance(value, text_type):\n            # stricter with py3\n            if PY3:\n                raise TypeError(\"str only for text/external values\")\n        else:\n            value = value.encode(\"utf-8\")\n\n    if kind == TEXT:\n        return APETextValue(value, kind)\n    elif kind == BINARY:\n        return APEBinaryValue(value, kind)\n    elif kind == EXTERNAL:\n        return APEExtValue(value, kind)\n    else:\n        raise ValueError(\"kind must be TEXT, BINARY, or EXTERNAL\")", "code_tokens": ["def", "APEValue", "(", "value", ",", "kind", ")", ":", "if", "kind", "in", "(", "TEXT", ",", "EXTERNAL", ")", ":", "if", "not", "isinstance", "(", "value", ",", "text_type", ")", ":", "# stricter with py3", "if", "PY3", ":", "raise", "TypeError", "(", "\"str only for text/external values\"", ")", "else", ":", "value", "=", "value", ".", "encode", "(", "\"utf-8\"", ")", "if", "kind", "==", "TEXT", ":", "return", "APETextValue", "(", "value", ",", "kind", ")", "elif", "kind", "==", "BINARY", ":", "return", "APEBinaryValue", "(", "value", ",", "kind", ")", "elif", "kind", "==", "EXTERNAL", ":", "return", "APEExtValue", "(", "value", ",", "kind", ")", "else", ":", "raise", "ValueError", "(", "\"kind must be TEXT, BINARY, or EXTERNAL\"", ")"], "docstring": "APEv2 tag value factory.\n\n    Use this if you need to specify the value's type manually.  Binary\n    and text data are automatically detected by APEv2.__setitem__.", "docstring_tokens": ["APEv2", "tag", "value", "factory", "."], "sha": "38e62c8dc35c72b16554f5dbe7c0fde91acc3411", "url": "https://github.com/LordSputnik/mutagen/blob/38e62c8dc35c72b16554f5dbe7c0fde91acc3411/mutagen/apev2.py#L460-L482", "partition": "test"}
{"repo": "jupyter-widgets/jupyterlab-sidecar", "path": "setupbase.py", "func_name": "ensure_python", "original_string": "def ensure_python(specs):\n    \"\"\"Given a list of range specifiers for python, ensure compatibility.\n    \"\"\"\n    if not isinstance(specs, (list, tuple)):\n        specs = [specs]\n    v = sys.version_info\n    part = '%s.%s' % (v.major, v.minor)\n    for spec in specs:\n        if part == spec:\n            return\n        try:\n            if eval(part + spec):\n                return\n        except SyntaxError:\n            pass\n    raise ValueError('Python version %s unsupported' % part)", "language": "python", "code": "def ensure_python(specs):\n    \"\"\"Given a list of range specifiers for python, ensure compatibility.\n    \"\"\"\n    if not isinstance(specs, (list, tuple)):\n        specs = [specs]\n    v = sys.version_info\n    part = '%s.%s' % (v.major, v.minor)\n    for spec in specs:\n        if part == spec:\n            return\n        try:\n            if eval(part + spec):\n                return\n        except SyntaxError:\n            pass\n    raise ValueError('Python version %s unsupported' % part)", "code_tokens": ["def", "ensure_python", "(", "specs", ")", ":", "if", "not", "isinstance", "(", "specs", ",", "(", "list", ",", "tuple", ")", ")", ":", "specs", "=", "[", "specs", "]", "v", "=", "sys", ".", "version_info", "part", "=", "'%s.%s'", "%", "(", "v", ".", "major", ",", "v", ".", "minor", ")", "for", "spec", "in", "specs", ":", "if", "part", "==", "spec", ":", "return", "try", ":", "if", "eval", "(", "part", "+", "spec", ")", ":", "return", "except", "SyntaxError", ":", "pass", "raise", "ValueError", "(", "'Python version %s unsupported'", "%", "part", ")"], "docstring": "Given a list of range specifiers for python, ensure compatibility.", "docstring_tokens": ["Given", "a", "list", "of", "range", "specifiers", "for", "python", "ensure", "compatibility", "."], "sha": "8889d09f1a0933e2cbee06d4874f720b075b29e8", "url": "https://github.com/jupyter-widgets/jupyterlab-sidecar/blob/8889d09f1a0933e2cbee06d4874f720b075b29e8/setupbase.py#L89-L104", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/compiler/generator.py", "func_name": "_chain_py_ast", "original_string": "def _chain_py_ast(*genned: GeneratedPyAST,) -> Tuple[PyASTStream, PyASTStream]:\n    \"\"\"Chain a sequence of generated Python ASTs into a tuple of dependency nodes\"\"\"\n    deps = chain.from_iterable(map(lambda n: n.dependencies, genned))\n    nodes = map(lambda n: n.node, genned)\n    return deps, nodes", "language": "python", "code": "def _chain_py_ast(*genned: GeneratedPyAST,) -> Tuple[PyASTStream, PyASTStream]:\n    \"\"\"Chain a sequence of generated Python ASTs into a tuple of dependency nodes\"\"\"\n    deps = chain.from_iterable(map(lambda n: n.dependencies, genned))\n    nodes = map(lambda n: n.node, genned)\n    return deps, nodes", "code_tokens": ["def", "_chain_py_ast", "(", "*", "genned", ":", "GeneratedPyAST", ",", ")", "->", "Tuple", "[", "PyASTStream", ",", "PyASTStream", "]", ":", "deps", "=", "chain", ".", "from_iterable", "(", "map", "(", "lambda", "n", ":", "n", ".", "dependencies", ",", "genned", ")", ")", "nodes", "=", "map", "(", "lambda", "n", ":", "n", ".", "node", ",", "genned", ")", "return", "deps", ",", "nodes"], "docstring": "Chain a sequence of generated Python ASTs into a tuple of dependency nodes", "docstring_tokens": ["Chain", "a", "sequence", "of", "generated", "Python", "ASTs", "into", "a", "tuple", "of", "dependency", "nodes"], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/compiler/generator.py#L294-L298", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlDatabaseHook.retrieve_connection", "original_string": "def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            return connections[0]\n        return None", "language": "python", "code": "def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            return connections[0]\n        return None", "code_tokens": ["def", "retrieve_connection", "(", "self", ",", "session", "=", "None", ")", ":", "self", ".", "log", ".", "info", "(", "\"Retrieving connection %s\"", ",", "self", ".", "db_conn_id", ")", "connections", "=", "session", ".", "query", "(", "Connection", ")", ".", "filter", "(", "Connection", ".", "conn_id", "==", "self", ".", "db_conn_id", ")", "if", "connections", ".", "count", "(", ")", ":", "return", "connections", "[", "0", "]", "return", "None"], "docstring": "Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).", "docstring_tokens": ["Retrieves", "the", "dynamically", "created", "connection", "from", "the", "Connection", "table", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L911-L923", "partition": "test"}
{"repo": "farzadghanei/statsd-metrics", "path": "statsdmetrics/client/__init__.py", "func_name": "AutoClosingSharedSocket.remove_client", "original_string": "def remove_client(self, client):\n        # type: (object) -> None\n        \"\"\"Remove the client from the users of the socket.\n\n        If there are no more clients for the socket, it\n        will close automatically.\n        \"\"\"\n\n        try:\n            self._clients.remove(id(client))\n        except ValueError:\n            pass\n\n        if len(self._clients) < 1:\n            self.close()", "language": "python", "code": "def remove_client(self, client):\n        # type: (object) -> None\n        \"\"\"Remove the client from the users of the socket.\n\n        If there are no more clients for the socket, it\n        will close automatically.\n        \"\"\"\n\n        try:\n            self._clients.remove(id(client))\n        except ValueError:\n            pass\n\n        if len(self._clients) < 1:\n            self.close()", "code_tokens": ["def", "remove_client", "(", "self", ",", "client", ")", ":", "# type: (object) -> None", "try", ":", "self", ".", "_clients", ".", "remove", "(", "id", "(", "client", ")", ")", "except", "ValueError", ":", "pass", "if", "len", "(", "self", ".", "_clients", ")", "<", "1", ":", "self", ".", "close", "(", ")"], "docstring": "Remove the client from the users of the socket.\n\n        If there are no more clients for the socket, it\n        will close automatically.", "docstring_tokens": ["Remove", "the", "client", "from", "the", "users", "of", "the", "socket", "."], "sha": "153ff37b79777f208e49bb9d3fb737ba52b99f98", "url": "https://github.com/farzadghanei/statsd-metrics/blob/153ff37b79777f208e49bb9d3fb737ba52b99f98/statsdmetrics/client/__init__.py#L74-L88", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/hooks/S3_hook.py", "func_name": "S3Hook.list_keys", "original_string": "def list_keys(self, bucket_name, prefix='', delimiter='',\n                  page_size=None, max_items=None):\n        \"\"\"\n        Lists keys in a bucket under prefix and not containing delimiter\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int\n        \"\"\"\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('list_objects_v2')\n        response = paginator.paginate(Bucket=bucket_name,\n                                      Prefix=prefix,\n                                      Delimiter=delimiter,\n                                      PaginationConfig=config)\n\n        has_results = False\n        keys = []\n        for page in response:\n            if 'Contents' in page:\n                has_results = True\n                for k in page['Contents']:\n                    keys.append(k['Key'])\n\n        if has_results:\n            return keys", "language": "python", "code": "def list_keys(self, bucket_name, prefix='', delimiter='',\n                  page_size=None, max_items=None):\n        \"\"\"\n        Lists keys in a bucket under prefix and not containing delimiter\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int\n        \"\"\"\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('list_objects_v2')\n        response = paginator.paginate(Bucket=bucket_name,\n                                      Prefix=prefix,\n                                      Delimiter=delimiter,\n                                      PaginationConfig=config)\n\n        has_results = False\n        keys = []\n        for page in response:\n            if 'Contents' in page:\n                has_results = True\n                for k in page['Contents']:\n                    keys.append(k['Key'])\n\n        if has_results:\n            return keys", "code_tokens": ["def", "list_keys", "(", "self", ",", "bucket_name", ",", "prefix", "=", "''", ",", "delimiter", "=", "''", ",", "page_size", "=", "None", ",", "max_items", "=", "None", ")", ":", "config", "=", "{", "'PageSize'", ":", "page_size", ",", "'MaxItems'", ":", "max_items", ",", "}", "paginator", "=", "self", ".", "get_conn", "(", ")", ".", "get_paginator", "(", "'list_objects_v2'", ")", "response", "=", "paginator", ".", "paginate", "(", "Bucket", "=", "bucket_name", ",", "Prefix", "=", "prefix", ",", "Delimiter", "=", "delimiter", ",", "PaginationConfig", "=", "config", ")", "has_results", "=", "False", "keys", "=", "[", "]", "for", "page", "in", "response", ":", "if", "'Contents'", "in", "page", ":", "has_results", "=", "True", "for", "k", "in", "page", "[", "'Contents'", "]", ":", "keys", ".", "append", "(", "k", "[", "'Key'", "]", ")", "if", "has_results", ":", "return", "keys"], "docstring": "Lists keys in a bucket under prefix and not containing delimiter\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int", "docstring_tokens": ["Lists", "keys", "in", "a", "bucket", "under", "prefix", "and", "not", "containing", "delimiter"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L147-L183", "partition": "test"}
{"repo": "rocky/python3-trepan", "path": "trepan/processor/command/info_subcmd/pc.py", "func_name": "InfoPC.run", "original_string": "def run(self, args):\n        \"\"\"Program counter.\"\"\"\n        mainfile = self.core.filename(None)\n        if self.core.is_running():\n            curframe = self.proc.curframe\n            if curframe:\n                line_no = inspect.getlineno(curframe)\n                offset  = curframe.f_lasti\n                self.msg(\"PC offset is %d.\" % offset)\n                offset = max(offset, 0)\n                code = curframe.f_code\n                co_code = code.co_code\n                disassemble_bytes(self.msg, self.msg_nocr,\n                                  co_code, offset, line_no, line_no-1, line_no+1,\n                                  constants=code.co_consts, cells=code.co_cellvars,\n                                  varnames=code.co_varnames, freevars=code.co_freevars,\n                                  linestarts=dict(findlinestarts(code)),\n                                  end_offset=offset+10)\n                pass\n            pass\n        else:\n            if mainfile:\n                part1 = \"Python program '%s'\" % mainfile\n                msg   = \"is not currently running. \"\n                self.msg(Mmisc.wrapped_lines(part1, msg,\n                                             self.settings['width']))\n            else:\n                self.msg('No Python program is currently running.')\n                pass\n            self.msg(self.core.execution_status)\n            pass\n        return False", "language": "python", "code": "def run(self, args):\n        \"\"\"Program counter.\"\"\"\n        mainfile = self.core.filename(None)\n        if self.core.is_running():\n            curframe = self.proc.curframe\n            if curframe:\n                line_no = inspect.getlineno(curframe)\n                offset  = curframe.f_lasti\n                self.msg(\"PC offset is %d.\" % offset)\n                offset = max(offset, 0)\n                code = curframe.f_code\n                co_code = code.co_code\n                disassemble_bytes(self.msg, self.msg_nocr,\n                                  co_code, offset, line_no, line_no-1, line_no+1,\n                                  constants=code.co_consts, cells=code.co_cellvars,\n                                  varnames=code.co_varnames, freevars=code.co_freevars,\n                                  linestarts=dict(findlinestarts(code)),\n                                  end_offset=offset+10)\n                pass\n            pass\n        else:\n            if mainfile:\n                part1 = \"Python program '%s'\" % mainfile\n                msg   = \"is not currently running. \"\n                self.msg(Mmisc.wrapped_lines(part1, msg,\n                                             self.settings['width']))\n            else:\n                self.msg('No Python program is currently running.')\n                pass\n            self.msg(self.core.execution_status)\n            pass\n        return False", "code_tokens": ["def", "run", "(", "self", ",", "args", ")", ":", "mainfile", "=", "self", ".", "core", ".", "filename", "(", "None", ")", "if", "self", ".", "core", ".", "is_running", "(", ")", ":", "curframe", "=", "self", ".", "proc", ".", "curframe", "if", "curframe", ":", "line_no", "=", "inspect", ".", "getlineno", "(", "curframe", ")", "offset", "=", "curframe", ".", "f_lasti", "self", ".", "msg", "(", "\"PC offset is %d.\"", "%", "offset", ")", "offset", "=", "max", "(", "offset", ",", "0", ")", "code", "=", "curframe", ".", "f_code", "co_code", "=", "code", ".", "co_code", "disassemble_bytes", "(", "self", ".", "msg", ",", "self", ".", "msg_nocr", ",", "co_code", ",", "offset", ",", "line_no", ",", "line_no", "-", "1", ",", "line_no", "+", "1", ",", "constants", "=", "code", ".", "co_consts", ",", "cells", "=", "code", ".", "co_cellvars", ",", "varnames", "=", "code", ".", "co_varnames", ",", "freevars", "=", "code", ".", "co_freevars", ",", "linestarts", "=", "dict", "(", "findlinestarts", "(", "code", ")", ")", ",", "end_offset", "=", "offset", "+", "10", ")", "pass", "pass", "else", ":", "if", "mainfile", ":", "part1", "=", "\"Python program '%s'\"", "%", "mainfile", "msg", "=", "\"is not currently running. \"", "self", ".", "msg", "(", "Mmisc", ".", "wrapped_lines", "(", "part1", ",", "msg", ",", "self", ".", "settings", "[", "'width'", "]", ")", ")", "else", ":", "self", ".", "msg", "(", "'No Python program is currently running.'", ")", "pass", "self", ".", "msg", "(", "self", ".", "core", ".", "execution_status", ")", "pass", "return", "False"], "docstring": "Program counter.", "docstring_tokens": ["Program", "counter", "."], "sha": "14e91bc0acce090d67be145b1ac040cab92ac5f3", "url": "https://github.com/rocky/python3-trepan/blob/14e91bc0acce090d67be145b1ac040cab92ac5f3/trepan/processor/command/info_subcmd/pc.py#L42-L73", "partition": "test"}
{"repo": "neurodata/ndio", "path": "ndio/convert/hdf5.py", "func_name": "save", "original_string": "def save(hdf5_filename, array):\n    \"\"\"\n    Export a numpy array to a HDF5 file.\n\n    Arguments:\n        hdf5_filename (str): A filename to which to save the HDF5 data\n        array (numpy.ndarray): The numpy array to save to HDF5\n\n    Returns:\n        String. The expanded filename that now holds the HDF5 data\n    \"\"\"\n    # Expand filename to be absolute\n    hdf5_filename = os.path.expanduser(hdf5_filename)\n\n    try:\n        h = h5py.File(hdf5_filename, \"w\")\n        h.create_dataset('CUTOUT', data=array)\n        h.close()\n    except Exception as e:\n        raise ValueError(\"Could not save HDF5 file {0}.\".format(hdf5_filename))\n\n    return hdf5_filename", "language": "python", "code": "def save(hdf5_filename, array):\n    \"\"\"\n    Export a numpy array to a HDF5 file.\n\n    Arguments:\n        hdf5_filename (str): A filename to which to save the HDF5 data\n        array (numpy.ndarray): The numpy array to save to HDF5\n\n    Returns:\n        String. The expanded filename that now holds the HDF5 data\n    \"\"\"\n    # Expand filename to be absolute\n    hdf5_filename = os.path.expanduser(hdf5_filename)\n\n    try:\n        h = h5py.File(hdf5_filename, \"w\")\n        h.create_dataset('CUTOUT', data=array)\n        h.close()\n    except Exception as e:\n        raise ValueError(\"Could not save HDF5 file {0}.\".format(hdf5_filename))\n\n    return hdf5_filename", "code_tokens": ["def", "save", "(", "hdf5_filename", ",", "array", ")", ":", "# Expand filename to be absolute", "hdf5_filename", "=", "os", ".", "path", ".", "expanduser", "(", "hdf5_filename", ")", "try", ":", "h", "=", "h5py", ".", "File", "(", "hdf5_filename", ",", "\"w\"", ")", "h", ".", "create_dataset", "(", "'CUTOUT'", ",", "data", "=", "array", ")", "h", ".", "close", "(", ")", "except", "Exception", "as", "e", ":", "raise", "ValueError", "(", "\"Could not save HDF5 file {0}.\"", ".", "format", "(", "hdf5_filename", ")", ")", "return", "hdf5_filename"], "docstring": "Export a numpy array to a HDF5 file.\n\n    Arguments:\n        hdf5_filename (str): A filename to which to save the HDF5 data\n        array (numpy.ndarray): The numpy array to save to HDF5\n\n    Returns:\n        String. The expanded filename that now holds the HDF5 data", "docstring_tokens": ["Export", "a", "numpy", "array", "to", "a", "HDF5", "file", "."], "sha": "792dd5816bc770b05a3db2f4327da42ff6253531", "url": "https://github.com/neurodata/ndio/blob/792dd5816bc770b05a3db2f4327da42ff6253531/ndio/convert/hdf5.py#L32-L53", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/tools/qi/qi.py", "func_name": "choi_to_rauli", "original_string": "def choi_to_rauli(choi, order=1):\n    \"\"\"\n    Convert a Choi-matrix to a Pauli-basis superoperator.\n\n    Note that this function assumes that the Choi-matrix\n    is defined in the standard column-stacking convention\n    and is normalized to have trace 1. For a channel E this\n    is defined as: choi = (I \\\\otimes E)(bell_state).\n\n    The resulting 'rauli' R acts on input states as\n    |rho_out>_p = R.|rho_in>_p\n    where |rho> = vectorize(rho, method='pauli') for order=1\n    and |rho> = vectorize(rho, method='pauli_weights') for order=0.\n\n    Args:\n        choi (matrix): the input Choi-matrix.\n        order (int): ordering of the Pauli group vector.\n            order=1 (default) is standard lexicographic ordering.\n                Eg: [II, IX, IY, IZ, XI, XX, XY,...]\n            order=0 is ordered by weights.\n                Eg. [II, IX, IY, IZ, XI, XY, XZ, XX, XY,...]\n\n    Returns:\n        np.array: A superoperator in the Pauli basis.\n    \"\"\"\n    if order == 0:\n        order = 'weight'\n    elif order == 1:\n        order = 'tensor'\n\n    # get number of qubits'\n    num_qubits = int(np.log2(np.sqrt(len(choi))))\n    pgp = pauli_group(num_qubits, case=order)\n    rauli = []\n    for i in pgp:\n        for j in pgp:\n            pauliop = np.kron(j.to_matrix().T, i.to_matrix())\n            rauli += [np.trace(np.dot(choi, pauliop))]\n    return np.array(rauli).reshape(4 ** num_qubits, 4 ** num_qubits)", "language": "python", "code": "def choi_to_rauli(choi, order=1):\n    \"\"\"\n    Convert a Choi-matrix to a Pauli-basis superoperator.\n\n    Note that this function assumes that the Choi-matrix\n    is defined in the standard column-stacking convention\n    and is normalized to have trace 1. For a channel E this\n    is defined as: choi = (I \\\\otimes E)(bell_state).\n\n    The resulting 'rauli' R acts on input states as\n    |rho_out>_p = R.|rho_in>_p\n    where |rho> = vectorize(rho, method='pauli') for order=1\n    and |rho> = vectorize(rho, method='pauli_weights') for order=0.\n\n    Args:\n        choi (matrix): the input Choi-matrix.\n        order (int): ordering of the Pauli group vector.\n            order=1 (default) is standard lexicographic ordering.\n                Eg: [II, IX, IY, IZ, XI, XX, XY,...]\n            order=0 is ordered by weights.\n                Eg. [II, IX, IY, IZ, XI, XY, XZ, XX, XY,...]\n\n    Returns:\n        np.array: A superoperator in the Pauli basis.\n    \"\"\"\n    if order == 0:\n        order = 'weight'\n    elif order == 1:\n        order = 'tensor'\n\n    # get number of qubits'\n    num_qubits = int(np.log2(np.sqrt(len(choi))))\n    pgp = pauli_group(num_qubits, case=order)\n    rauli = []\n    for i in pgp:\n        for j in pgp:\n            pauliop = np.kron(j.to_matrix().T, i.to_matrix())\n            rauli += [np.trace(np.dot(choi, pauliop))]\n    return np.array(rauli).reshape(4 ** num_qubits, 4 ** num_qubits)", "code_tokens": ["def", "choi_to_rauli", "(", "choi", ",", "order", "=", "1", ")", ":", "if", "order", "==", "0", ":", "order", "=", "'weight'", "elif", "order", "==", "1", ":", "order", "=", "'tensor'", "# get number of qubits'", "num_qubits", "=", "int", "(", "np", ".", "log2", "(", "np", ".", "sqrt", "(", "len", "(", "choi", ")", ")", ")", ")", "pgp", "=", "pauli_group", "(", "num_qubits", ",", "case", "=", "order", ")", "rauli", "=", "[", "]", "for", "i", "in", "pgp", ":", "for", "j", "in", "pgp", ":", "pauliop", "=", "np", ".", "kron", "(", "j", ".", "to_matrix", "(", ")", ".", "T", ",", "i", ".", "to_matrix", "(", ")", ")", "rauli", "+=", "[", "np", ".", "trace", "(", "np", ".", "dot", "(", "choi", ",", "pauliop", ")", ")", "]", "return", "np", ".", "array", "(", "rauli", ")", ".", "reshape", "(", "4", "**", "num_qubits", ",", "4", "**", "num_qubits", ")"], "docstring": "Convert a Choi-matrix to a Pauli-basis superoperator.\n\n    Note that this function assumes that the Choi-matrix\n    is defined in the standard column-stacking convention\n    and is normalized to have trace 1. For a channel E this\n    is defined as: choi = (I \\\\otimes E)(bell_state).\n\n    The resulting 'rauli' R acts on input states as\n    |rho_out>_p = R.|rho_in>_p\n    where |rho> = vectorize(rho, method='pauli') for order=1\n    and |rho> = vectorize(rho, method='pauli_weights') for order=0.\n\n    Args:\n        choi (matrix): the input Choi-matrix.\n        order (int): ordering of the Pauli group vector.\n            order=1 (default) is standard lexicographic ordering.\n                Eg: [II, IX, IY, IZ, XI, XX, XY,...]\n            order=0 is ordered by weights.\n                Eg. [II, IX, IY, IZ, XI, XY, XZ, XX, XY,...]\n\n    Returns:\n        np.array: A superoperator in the Pauli basis.", "docstring_tokens": ["Convert", "a", "Choi", "-", "matrix", "to", "a", "Pauli", "-", "basis", "superoperator", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/tools/qi/qi.py#L249-L287", "partition": "test"}
{"repo": "mozilla-iot/webthing-python", "path": "webthing/thing.py", "func_name": "Thing.property_notify", "original_string": "def property_notify(self, property_):\n        \"\"\"\n        Notify all subscribers of a property change.\n\n        property_ -- the property that changed\n        \"\"\"\n        message = json.dumps({\n            'messageType': 'propertyStatus',\n            'data': {\n                property_.name: property_.get_value(),\n            }\n        })\n\n        for subscriber in list(self.subscribers):\n            try:\n                subscriber.write_message(message)\n            except tornado.websocket.WebSocketClosedError:\n                pass", "language": "python", "code": "def property_notify(self, property_):\n        \"\"\"\n        Notify all subscribers of a property change.\n\n        property_ -- the property that changed\n        \"\"\"\n        message = json.dumps({\n            'messageType': 'propertyStatus',\n            'data': {\n                property_.name: property_.get_value(),\n            }\n        })\n\n        for subscriber in list(self.subscribers):\n            try:\n                subscriber.write_message(message)\n            except tornado.websocket.WebSocketClosedError:\n                pass", "code_tokens": ["def", "property_notify", "(", "self", ",", "property_", ")", ":", "message", "=", "json", ".", "dumps", "(", "{", "'messageType'", ":", "'propertyStatus'", ",", "'data'", ":", "{", "property_", ".", "name", ":", "property_", ".", "get_value", "(", ")", ",", "}", "}", ")", "for", "subscriber", "in", "list", "(", "self", ".", "subscribers", ")", ":", "try", ":", "subscriber", ".", "write_message", "(", "message", ")", "except", "tornado", ".", "websocket", ".", "WebSocketClosedError", ":", "pass"], "docstring": "Notify all subscribers of a property change.\n\n        property_ -- the property that changed", "docstring_tokens": ["Notify", "all", "subscribers", "of", "a", "property", "change", "."], "sha": "65d467c89ed79d0bbc42b8b3c8f9e5a320edd237", "url": "https://github.com/mozilla-iot/webthing-python/blob/65d467c89ed79d0bbc42b8b3c8f9e5a320edd237/webthing/thing.py#L423-L440", "partition": "test"}
{"repo": "jupyter-widgets/jupyterlab-sidecar", "path": "setupbase.py", "func_name": "ensure_targets", "original_string": "def ensure_targets(targets):\n    \"\"\"Return a Command that checks that certain files exist.\n\n    Raises a ValueError if any of the files are missing.\n\n    Note: The check is skipped if the `--skip-npm` flag is used.\n    \"\"\"\n\n    class TargetsCheck(BaseCommand):\n        def run(self):\n            if skip_npm:\n                log.info('Skipping target checks')\n                return\n            missing = [t for t in targets if not os.path.exists(t)]\n            if missing:\n                raise ValueError(('missing files: %s' % missing))\n\n    return TargetsCheck", "language": "python", "code": "def ensure_targets(targets):\n    \"\"\"Return a Command that checks that certain files exist.\n\n    Raises a ValueError if any of the files are missing.\n\n    Note: The check is skipped if the `--skip-npm` flag is used.\n    \"\"\"\n\n    class TargetsCheck(BaseCommand):\n        def run(self):\n            if skip_npm:\n                log.info('Skipping target checks')\n                return\n            missing = [t for t in targets if not os.path.exists(t)]\n            if missing:\n                raise ValueError(('missing files: %s' % missing))\n\n    return TargetsCheck", "code_tokens": ["def", "ensure_targets", "(", "targets", ")", ":", "class", "TargetsCheck", "(", "BaseCommand", ")", ":", "def", "run", "(", "self", ")", ":", "if", "skip_npm", ":", "log", ".", "info", "(", "'Skipping target checks'", ")", "return", "missing", "=", "[", "t", "for", "t", "in", "targets", "if", "not", "os", ".", "path", ".", "exists", "(", "t", ")", "]", "if", "missing", ":", "raise", "ValueError", "(", "(", "'missing files: %s'", "%", "missing", ")", ")", "return", "TargetsCheck"], "docstring": "Return a Command that checks that certain files exist.\n\n    Raises a ValueError if any of the files are missing.\n\n    Note: The check is skipped if the `--skip-npm` flag is used.", "docstring_tokens": ["Return", "a", "Command", "that", "checks", "that", "certain", "files", "exist", "."], "sha": "8889d09f1a0933e2cbee06d4874f720b075b29e8", "url": "https://github.com/jupyter-widgets/jupyterlab-sidecar/blob/8889d09f1a0933e2cbee06d4874f720b075b29e8/setupbase.py#L375-L392", "partition": "test"}
{"repo": "mcs07/MolVS", "path": "molvs/standardize.py", "func_name": "Standardizer.charge_parent", "original_string": "def charge_parent(self, mol, skip_standardize=False):\n        \"\"\"Return the charge parent of a given molecule.\n\n        The charge parent is the uncharged version of the fragment parent.\n\n        :param mol: The input molecule.\n        :type mol: rdkit.Chem.rdchem.Mol\n        :param bool skip_standardize: Set to True if mol has already been standardized.\n        :returns: The charge parent molecule.\n        :rtype: rdkit.Chem.rdchem.Mol\n        \"\"\"\n        # TODO: All ionized acids and bases should be neutralised.\n        if not skip_standardize:\n            mol = self.standardize(mol)\n        fragment = self.fragment_parent(mol, skip_standardize=True)\n        if fragment:\n            uncharged = self.uncharge(fragment)\n            # During final standardization, the Reionizer ensures any remaining charges are in the right places\n            uncharged = self.standardize(uncharged)\n            return uncharged", "language": "python", "code": "def charge_parent(self, mol, skip_standardize=False):\n        \"\"\"Return the charge parent of a given molecule.\n\n        The charge parent is the uncharged version of the fragment parent.\n\n        :param mol: The input molecule.\n        :type mol: rdkit.Chem.rdchem.Mol\n        :param bool skip_standardize: Set to True if mol has already been standardized.\n        :returns: The charge parent molecule.\n        :rtype: rdkit.Chem.rdchem.Mol\n        \"\"\"\n        # TODO: All ionized acids and bases should be neutralised.\n        if not skip_standardize:\n            mol = self.standardize(mol)\n        fragment = self.fragment_parent(mol, skip_standardize=True)\n        if fragment:\n            uncharged = self.uncharge(fragment)\n            # During final standardization, the Reionizer ensures any remaining charges are in the right places\n            uncharged = self.standardize(uncharged)\n            return uncharged", "code_tokens": ["def", "charge_parent", "(", "self", ",", "mol", ",", "skip_standardize", "=", "False", ")", ":", "# TODO: All ionized acids and bases should be neutralised.", "if", "not", "skip_standardize", ":", "mol", "=", "self", ".", "standardize", "(", "mol", ")", "fragment", "=", "self", ".", "fragment_parent", "(", "mol", ",", "skip_standardize", "=", "True", ")", "if", "fragment", ":", "uncharged", "=", "self", ".", "uncharge", "(", "fragment", ")", "# During final standardization, the Reionizer ensures any remaining charges are in the right places", "uncharged", "=", "self", ".", "standardize", "(", "uncharged", ")", "return", "uncharged"], "docstring": "Return the charge parent of a given molecule.\n\n        The charge parent is the uncharged version of the fragment parent.\n\n        :param mol: The input molecule.\n        :type mol: rdkit.Chem.rdchem.Mol\n        :param bool skip_standardize: Set to True if mol has already been standardized.\n        :returns: The charge parent molecule.\n        :rtype: rdkit.Chem.rdchem.Mol", "docstring_tokens": ["Return", "the", "charge", "parent", "of", "a", "given", "molecule", "."], "sha": "d815fe52d160abcecbcbf117e6437bf727dbd8ad", "url": "https://github.com/mcs07/MolVS/blob/d815fe52d160abcecbcbf117e6437bf727dbd8ad/molvs/standardize.py#L171-L190", "partition": "test"}
{"repo": "oscarbranson/latools", "path": "latools/filtering/classifier_obj.py", "func_name": "classifier.fit_kmeans", "original_string": "def fit_kmeans(self, data, n_clusters, **kwargs):\n        \"\"\"\n        Fit KMeans clustering algorithm to data.\n\n        Parameters\n        ----------\n        data : array-like\n            A dataset formatted by `classifier.fitting_data`.\n        n_clusters : int\n            The number of clusters in the data.\n        **kwargs\n            passed to `sklearn.cluster.KMeans`.\n\n        Returns\n        -------\n        Fitted `sklearn.cluster.KMeans` object.\n        \"\"\"\n        km = cl.KMeans(n_clusters=n_clusters, **kwargs)\n        km.fit(data)\n        return km", "language": "python", "code": "def fit_kmeans(self, data, n_clusters, **kwargs):\n        \"\"\"\n        Fit KMeans clustering algorithm to data.\n\n        Parameters\n        ----------\n        data : array-like\n            A dataset formatted by `classifier.fitting_data`.\n        n_clusters : int\n            The number of clusters in the data.\n        **kwargs\n            passed to `sklearn.cluster.KMeans`.\n\n        Returns\n        -------\n        Fitted `sklearn.cluster.KMeans` object.\n        \"\"\"\n        km = cl.KMeans(n_clusters=n_clusters, **kwargs)\n        km.fit(data)\n        return km", "code_tokens": ["def", "fit_kmeans", "(", "self", ",", "data", ",", "n_clusters", ",", "*", "*", "kwargs", ")", ":", "km", "=", "cl", ".", "KMeans", "(", "n_clusters", "=", "n_clusters", ",", "*", "*", "kwargs", ")", "km", ".", "fit", "(", "data", ")", "return", "km"], "docstring": "Fit KMeans clustering algorithm to data.\n\n        Parameters\n        ----------\n        data : array-like\n            A dataset formatted by `classifier.fitting_data`.\n        n_clusters : int\n            The number of clusters in the data.\n        **kwargs\n            passed to `sklearn.cluster.KMeans`.\n\n        Returns\n        -------\n        Fitted `sklearn.cluster.KMeans` object.", "docstring_tokens": ["Fit", "KMeans", "clustering", "algorithm", "to", "data", "."], "sha": "cd25a650cfee318152f234d992708511f7047fbe", "url": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/filtering/classifier_obj.py#L89-L108", "partition": "test"}
{"repo": "hdima/erlport", "path": "priv/python3/erlport/erlterms.py", "func_name": "decode", "original_string": "def decode(string):\n    \"\"\"Decode Erlang external term.\"\"\"\n    if not string:\n        raise IncompleteData(string)\n    if string[0] != 131:\n        raise ValueError(\"unknown protocol version: %r\" % string[0])\n    if string[1:2] == b'P':\n        # compressed term\n        if len(string) < 16:\n            raise IncompleteData(string)\n        d = decompressobj()\n        term_string = d.decompress(string[6:]) + d.flush()\n        uncompressed_size, = _int4_unpack(string[2:6])\n        if len(term_string) != uncompressed_size:\n            raise ValueError(\n                \"invalid compressed tag, \"\n                \"%d bytes but got %d\" % (uncompressed_size, len(term_string)))\n        # tail data returned by decode_term() can be simple ignored\n        term, _tail = decode_term(term_string)\n        return term, d.unused_data\n    return decode_term(string[1:])", "language": "python", "code": "def decode(string):\n    \"\"\"Decode Erlang external term.\"\"\"\n    if not string:\n        raise IncompleteData(string)\n    if string[0] != 131:\n        raise ValueError(\"unknown protocol version: %r\" % string[0])\n    if string[1:2] == b'P':\n        # compressed term\n        if len(string) < 16:\n            raise IncompleteData(string)\n        d = decompressobj()\n        term_string = d.decompress(string[6:]) + d.flush()\n        uncompressed_size, = _int4_unpack(string[2:6])\n        if len(term_string) != uncompressed_size:\n            raise ValueError(\n                \"invalid compressed tag, \"\n                \"%d bytes but got %d\" % (uncompressed_size, len(term_string)))\n        # tail data returned by decode_term() can be simple ignored\n        term, _tail = decode_term(term_string)\n        return term, d.unused_data\n    return decode_term(string[1:])", "code_tokens": ["def", "decode", "(", "string", ")", ":", "if", "not", "string", ":", "raise", "IncompleteData", "(", "string", ")", "if", "string", "[", "0", "]", "!=", "131", ":", "raise", "ValueError", "(", "\"unknown protocol version: %r\"", "%", "string", "[", "0", "]", ")", "if", "string", "[", "1", ":", "2", "]", "==", "b'P'", ":", "# compressed term", "if", "len", "(", "string", ")", "<", "16", ":", "raise", "IncompleteData", "(", "string", ")", "d", "=", "decompressobj", "(", ")", "term_string", "=", "d", ".", "decompress", "(", "string", "[", "6", ":", "]", ")", "+", "d", ".", "flush", "(", ")", "uncompressed_size", ",", "=", "_int4_unpack", "(", "string", "[", "2", ":", "6", "]", ")", "if", "len", "(", "term_string", ")", "!=", "uncompressed_size", ":", "raise", "ValueError", "(", "\"invalid compressed tag, \"", "\"%d bytes but got %d\"", "%", "(", "uncompressed_size", ",", "len", "(", "term_string", ")", ")", ")", "# tail data returned by decode_term() can be simple ignored", "term", ",", "_tail", "=", "decode_term", "(", "term_string", ")", "return", "term", ",", "d", ".", "unused_data", "return", "decode_term", "(", "string", "[", "1", ":", "]", ")"], "docstring": "Decode Erlang external term.", "docstring_tokens": ["Decode", "Erlang", "external", "term", "."], "sha": "246b7722d62b87b48be66d9a871509a537728962", "url": "https://github.com/hdima/erlport/blob/246b7722d62b87b48be66d9a871509a537728962/priv/python3/erlport/erlterms.py#L168-L188", "partition": "test"}
{"repo": "tmontaigu/pylas", "path": "pylas/headers/rawheader.py", "func_name": "RawHeader1_1.date", "original_string": "def date(self, date):\n        \"\"\" Returns the date of file creation as a python date object\n        \"\"\"\n        self.creation_year = date.year\n        self.creation_day_of_year = date.timetuple().tm_yday", "language": "python", "code": "def date(self, date):\n        \"\"\" Returns the date of file creation as a python date object\n        \"\"\"\n        self.creation_year = date.year\n        self.creation_day_of_year = date.timetuple().tm_yday", "code_tokens": ["def", "date", "(", "self", ",", "date", ")", ":", "self", ".", "creation_year", "=", "date", ".", "year", "self", ".", "creation_day_of_year", "=", "date", ".", "timetuple", "(", ")", ".", "tm_yday"], "docstring": "Returns the date of file creation as a python date object", "docstring_tokens": ["Returns", "the", "date", "of", "file", "creation", "as", "a", "python", "date", "object"], "sha": "8335a1a7d7677f0e4bc391bb6fa3c75b42ed5b06", "url": "https://github.com/tmontaigu/pylas/blob/8335a1a7d7677f0e4bc391bb6fa3c75b42ed5b06/pylas/headers/rawheader.py#L159-L163", "partition": "test"}
{"repo": "howie6879/ruia", "path": "ruia/spider.py", "func_name": "Spider.request", "original_string": "def request(self,\n                url: str,\n                method: str = 'GET',\n                *,\n                callback=None,\n                encoding: typing.Optional[str] = None,\n                headers: dict = None,\n                metadata: dict = None,\n                request_config: dict = None,\n                request_session=None,\n                **kwargs):\n        \"\"\"Init a Request class for crawling html\"\"\"\n        headers = headers or {}\n        metadata = metadata or {}\n        request_config = request_config or {}\n        request_session = request_session or self.request_session\n\n        headers.update(self.headers.copy())\n        request_config.update(self.request_config.copy())\n        kwargs.update(self.kwargs.copy())\n\n        return Request(\n            url=url,\n            method=method,\n            callback=callback,\n            encoding=encoding,\n            headers=headers,\n            metadata=metadata,\n            request_config=request_config,\n            request_session=request_session,\n            **kwargs)", "language": "python", "code": "def request(self,\n                url: str,\n                method: str = 'GET',\n                *,\n                callback=None,\n                encoding: typing.Optional[str] = None,\n                headers: dict = None,\n                metadata: dict = None,\n                request_config: dict = None,\n                request_session=None,\n                **kwargs):\n        \"\"\"Init a Request class for crawling html\"\"\"\n        headers = headers or {}\n        metadata = metadata or {}\n        request_config = request_config or {}\n        request_session = request_session or self.request_session\n\n        headers.update(self.headers.copy())\n        request_config.update(self.request_config.copy())\n        kwargs.update(self.kwargs.copy())\n\n        return Request(\n            url=url,\n            method=method,\n            callback=callback,\n            encoding=encoding,\n            headers=headers,\n            metadata=metadata,\n            request_config=request_config,\n            request_session=request_session,\n            **kwargs)", "code_tokens": ["def", "request", "(", "self", ",", "url", ":", "str", ",", "method", ":", "str", "=", "'GET'", ",", "*", ",", "callback", "=", "None", ",", "encoding", ":", "typing", ".", "Optional", "[", "str", "]", "=", "None", ",", "headers", ":", "dict", "=", "None", ",", "metadata", ":", "dict", "=", "None", ",", "request_config", ":", "dict", "=", "None", ",", "request_session", "=", "None", ",", "*", "*", "kwargs", ")", ":", "headers", "=", "headers", "or", "{", "}", "metadata", "=", "metadata", "or", "{", "}", "request_config", "=", "request_config", "or", "{", "}", "request_session", "=", "request_session", "or", "self", ".", "request_session", "headers", ".", "update", "(", "self", ".", "headers", ".", "copy", "(", ")", ")", "request_config", ".", "update", "(", "self", ".", "request_config", ".", "copy", "(", ")", ")", "kwargs", ".", "update", "(", "self", ".", "kwargs", ".", "copy", "(", ")", ")", "return", "Request", "(", "url", "=", "url", ",", "method", "=", "method", ",", "callback", "=", "callback", ",", "encoding", "=", "encoding", ",", "headers", "=", "headers", ",", "metadata", "=", "metadata", ",", "request_config", "=", "request_config", ",", "request_session", "=", "request_session", ",", "*", "*", "kwargs", ")"], "docstring": "Init a Request class for crawling html", "docstring_tokens": ["Init", "a", "Request", "class", "for", "crawling", "html"], "sha": "2dc5262fc9c3e902a8faa7d5fa2f046f9d9ee1fa", "url": "https://github.com/howie6879/ruia/blob/2dc5262fc9c3e902a8faa7d5fa2f046f9d9ee1fa/ruia/spider.py#L378-L408", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/hmc.py", "func_name": "make_simple_step_size_update_policy", "original_string": "def make_simple_step_size_update_policy(num_adaptation_steps,\n                                        target_rate=0.75,\n                                        decrement_multiplier=0.01,\n                                        increment_multiplier=0.01,\n                                        step_counter=None):\n  \"\"\"Create a function implementing a step-size update policy.\n\n  The simple policy increases or decreases the `step_size_var` based on the\n  average of `exp(minimum(0., log_accept_ratio))`. It is based on\n  [Section 4.2 of Andrieu and Thoms (2008)](\n  https://people.eecs.berkeley.edu/~jordan/sail/readings/andrieu-thoms.pdf).\n\n  The `num_adaptation_steps` argument is set independently of any burnin\n  for the overall chain. In general, adaptation prevents the chain from\n  reaching a stationary distribution, so obtaining consistent samples requires\n  `num_adaptation_steps` be set to a value [somewhat smaller](\n  http://andrewgelman.com/2017/12/15/burn-vs-warm-iterative-simulation-algorithms/#comment-627745)\n  than the number of burnin steps. However, it may sometimes be helpful to set\n  `num_adaptation_steps` to a larger value during development in order to\n  inspect the behavior of the chain during adaptation.\n\n  Args:\n    num_adaptation_steps: Scalar `int` `Tensor` number of initial steps to\n      during which to adjust the step size. This may be greater, less than, or\n      equal to the number of burnin steps. If `None`, the step size is adapted\n      on every step (note this breaks stationarity of the chain!).\n    target_rate: Scalar `Tensor` representing desired `accept_ratio`.\n      Default value: `0.75` (i.e., [center of asymptotically optimal\n      rate](https://arxiv.org/abs/1411.6669)).\n    decrement_multiplier: `Tensor` representing amount to downscale current\n      `step_size`.\n      Default value: `0.01`.\n    increment_multiplier: `Tensor` representing amount to upscale current\n      `step_size`.\n      Default value: `0.01`.\n    step_counter: Scalar `int` `Variable` specifying the current step. The step\n      size is adapted iff `step_counter < num_adaptation_steps`.\n      Default value: if `None`, an internal variable\n        `step_size_adaptation_step_counter` is created and initialized to `-1`.\n\n  Returns:\n    step_size_simple_update_fn: Callable that takes args\n      `step_size_var, kernel_results` and returns updated step size(s).\n  \"\"\"\n  if step_counter is None and num_adaptation_steps is not None:\n    step_counter = tf.compat.v1.get_variable(\n        name='step_size_adaptation_step_counter',\n        initializer=np.array(-1, dtype=np.int32),\n        # Specify the dtype for variable sharing to work correctly\n        # (b/120599991).\n        dtype=tf.int32,\n        trainable=False,\n        use_resource=True)\n\n  def step_size_simple_update_fn(step_size_var, kernel_results):\n    \"\"\"Updates (list of) `step_size` using a standard adaptive MCMC procedure.\n\n    Args:\n      step_size_var: (List of) `tf.Variable`s representing the per `state_part`\n        HMC `step_size`.\n      kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from most recent call to `one_step`.\n\n    Returns:\n      step_size_assign: (List of) `Tensor`(s) representing updated\n        `step_size_var`(s).\n    \"\"\"\n\n    if kernel_results is None:\n      if mcmc_util.is_list_like(step_size_var):\n        return [tf.identity(ss) for ss in step_size_var]\n      return tf.identity(step_size_var)\n    log_n = tf.math.log(\n        tf.cast(\n            tf.size(input=kernel_results.log_accept_ratio),\n            kernel_results.log_accept_ratio.dtype))\n    log_mean_accept_ratio = tf.reduce_logsumexp(\n        input_tensor=tf.minimum(kernel_results.log_accept_ratio, 0.)) - log_n\n    adjustment = tf.where(\n        log_mean_accept_ratio < tf.cast(\n            tf.math.log(target_rate), log_mean_accept_ratio.dtype),\n        -decrement_multiplier / (1. + decrement_multiplier),\n        increment_multiplier)\n\n    def build_assign_op():\n      if mcmc_util.is_list_like(step_size_var):\n        return [\n            ss.assign_add(ss * tf.cast(adjustment, ss.dtype))\n            for ss in step_size_var\n        ]\n      return step_size_var.assign_add(\n          step_size_var * tf.cast(adjustment, step_size_var.dtype))\n\n    if num_adaptation_steps is None:\n      return build_assign_op()\n    else:\n      with tf.control_dependencies([step_counter.assign_add(1)]):\n        return tf.cond(\n            pred=step_counter < num_adaptation_steps,\n            true_fn=build_assign_op,\n            false_fn=lambda: step_size_var)\n\n  return step_size_simple_update_fn", "language": "python", "code": "def make_simple_step_size_update_policy(num_adaptation_steps,\n                                        target_rate=0.75,\n                                        decrement_multiplier=0.01,\n                                        increment_multiplier=0.01,\n                                        step_counter=None):\n  \"\"\"Create a function implementing a step-size update policy.\n\n  The simple policy increases or decreases the `step_size_var` based on the\n  average of `exp(minimum(0., log_accept_ratio))`. It is based on\n  [Section 4.2 of Andrieu and Thoms (2008)](\n  https://people.eecs.berkeley.edu/~jordan/sail/readings/andrieu-thoms.pdf).\n\n  The `num_adaptation_steps` argument is set independently of any burnin\n  for the overall chain. In general, adaptation prevents the chain from\n  reaching a stationary distribution, so obtaining consistent samples requires\n  `num_adaptation_steps` be set to a value [somewhat smaller](\n  http://andrewgelman.com/2017/12/15/burn-vs-warm-iterative-simulation-algorithms/#comment-627745)\n  than the number of burnin steps. However, it may sometimes be helpful to set\n  `num_adaptation_steps` to a larger value during development in order to\n  inspect the behavior of the chain during adaptation.\n\n  Args:\n    num_adaptation_steps: Scalar `int` `Tensor` number of initial steps to\n      during which to adjust the step size. This may be greater, less than, or\n      equal to the number of burnin steps. If `None`, the step size is adapted\n      on every step (note this breaks stationarity of the chain!).\n    target_rate: Scalar `Tensor` representing desired `accept_ratio`.\n      Default value: `0.75` (i.e., [center of asymptotically optimal\n      rate](https://arxiv.org/abs/1411.6669)).\n    decrement_multiplier: `Tensor` representing amount to downscale current\n      `step_size`.\n      Default value: `0.01`.\n    increment_multiplier: `Tensor` representing amount to upscale current\n      `step_size`.\n      Default value: `0.01`.\n    step_counter: Scalar `int` `Variable` specifying the current step. The step\n      size is adapted iff `step_counter < num_adaptation_steps`.\n      Default value: if `None`, an internal variable\n        `step_size_adaptation_step_counter` is created and initialized to `-1`.\n\n  Returns:\n    step_size_simple_update_fn: Callable that takes args\n      `step_size_var, kernel_results` and returns updated step size(s).\n  \"\"\"\n  if step_counter is None and num_adaptation_steps is not None:\n    step_counter = tf.compat.v1.get_variable(\n        name='step_size_adaptation_step_counter',\n        initializer=np.array(-1, dtype=np.int32),\n        # Specify the dtype for variable sharing to work correctly\n        # (b/120599991).\n        dtype=tf.int32,\n        trainable=False,\n        use_resource=True)\n\n  def step_size_simple_update_fn(step_size_var, kernel_results):\n    \"\"\"Updates (list of) `step_size` using a standard adaptive MCMC procedure.\n\n    Args:\n      step_size_var: (List of) `tf.Variable`s representing the per `state_part`\n        HMC `step_size`.\n      kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from most recent call to `one_step`.\n\n    Returns:\n      step_size_assign: (List of) `Tensor`(s) representing updated\n        `step_size_var`(s).\n    \"\"\"\n\n    if kernel_results is None:\n      if mcmc_util.is_list_like(step_size_var):\n        return [tf.identity(ss) for ss in step_size_var]\n      return tf.identity(step_size_var)\n    log_n = tf.math.log(\n        tf.cast(\n            tf.size(input=kernel_results.log_accept_ratio),\n            kernel_results.log_accept_ratio.dtype))\n    log_mean_accept_ratio = tf.reduce_logsumexp(\n        input_tensor=tf.minimum(kernel_results.log_accept_ratio, 0.)) - log_n\n    adjustment = tf.where(\n        log_mean_accept_ratio < tf.cast(\n            tf.math.log(target_rate), log_mean_accept_ratio.dtype),\n        -decrement_multiplier / (1. + decrement_multiplier),\n        increment_multiplier)\n\n    def build_assign_op():\n      if mcmc_util.is_list_like(step_size_var):\n        return [\n            ss.assign_add(ss * tf.cast(adjustment, ss.dtype))\n            for ss in step_size_var\n        ]\n      return step_size_var.assign_add(\n          step_size_var * tf.cast(adjustment, step_size_var.dtype))\n\n    if num_adaptation_steps is None:\n      return build_assign_op()\n    else:\n      with tf.control_dependencies([step_counter.assign_add(1)]):\n        return tf.cond(\n            pred=step_counter < num_adaptation_steps,\n            true_fn=build_assign_op,\n            false_fn=lambda: step_size_var)\n\n  return step_size_simple_update_fn", "code_tokens": ["def", "make_simple_step_size_update_policy", "(", "num_adaptation_steps", ",", "target_rate", "=", "0.75", ",", "decrement_multiplier", "=", "0.01", ",", "increment_multiplier", "=", "0.01", ",", "step_counter", "=", "None", ")", ":", "if", "step_counter", "is", "None", "and", "num_adaptation_steps", "is", "not", "None", ":", "step_counter", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "name", "=", "'step_size_adaptation_step_counter'", ",", "initializer", "=", "np", ".", "array", "(", "-", "1", ",", "dtype", "=", "np", ".", "int32", ")", ",", "# Specify the dtype for variable sharing to work correctly", "# (b/120599991).", "dtype", "=", "tf", ".", "int32", ",", "trainable", "=", "False", ",", "use_resource", "=", "True", ")", "def", "step_size_simple_update_fn", "(", "step_size_var", ",", "kernel_results", ")", ":", "\"\"\"Updates (list of) `step_size` using a standard adaptive MCMC procedure.\n\n    Args:\n      step_size_var: (List of) `tf.Variable`s representing the per `state_part`\n        HMC `step_size`.\n      kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from most recent call to `one_step`.\n\n    Returns:\n      step_size_assign: (List of) `Tensor`(s) representing updated\n        `step_size_var`(s).\n    \"\"\"", "if", "kernel_results", "is", "None", ":", "if", "mcmc_util", ".", "is_list_like", "(", "step_size_var", ")", ":", "return", "[", "tf", ".", "identity", "(", "ss", ")", "for", "ss", "in", "step_size_var", "]", "return", "tf", ".", "identity", "(", "step_size_var", ")", "log_n", "=", "tf", ".", "math", ".", "log", "(", "tf", ".", "cast", "(", "tf", ".", "size", "(", "input", "=", "kernel_results", ".", "log_accept_ratio", ")", ",", "kernel_results", ".", "log_accept_ratio", ".", "dtype", ")", ")", "log_mean_accept_ratio", "=", "tf", ".", "reduce_logsumexp", "(", "input_tensor", "=", "tf", ".", "minimum", "(", "kernel_results", ".", "log_accept_ratio", ",", "0.", ")", ")", "-", "log_n", "adjustment", "=", "tf", ".", "where", "(", "log_mean_accept_ratio", "<", "tf", ".", "cast", "(", "tf", ".", "math", ".", "log", "(", "target_rate", ")", ",", "log_mean_accept_ratio", ".", "dtype", ")", ",", "-", "decrement_multiplier", "/", "(", "1.", "+", "decrement_multiplier", ")", ",", "increment_multiplier", ")", "def", "build_assign_op", "(", ")", ":", "if", "mcmc_util", ".", "is_list_like", "(", "step_size_var", ")", ":", "return", "[", "ss", ".", "assign_add", "(", "ss", "*", "tf", ".", "cast", "(", "adjustment", ",", "ss", ".", "dtype", ")", ")", "for", "ss", "in", "step_size_var", "]", "return", "step_size_var", ".", "assign_add", "(", "step_size_var", "*", "tf", ".", "cast", "(", "adjustment", ",", "step_size_var", ".", "dtype", ")", ")", "if", "num_adaptation_steps", "is", "None", ":", "return", "build_assign_op", "(", ")", "else", ":", "with", "tf", ".", "control_dependencies", "(", "[", "step_counter", ".", "assign_add", "(", "1", ")", "]", ")", ":", "return", "tf", ".", "cond", "(", "pred", "=", "step_counter", "<", "num_adaptation_steps", ",", "true_fn", "=", "build_assign_op", ",", "false_fn", "=", "lambda", ":", "step_size_var", ")", "return", "step_size_simple_update_fn"], "docstring": "Create a function implementing a step-size update policy.\n\n  The simple policy increases or decreases the `step_size_var` based on the\n  average of `exp(minimum(0., log_accept_ratio))`. It is based on\n  [Section 4.2 of Andrieu and Thoms (2008)](\n  https://people.eecs.berkeley.edu/~jordan/sail/readings/andrieu-thoms.pdf).\n\n  The `num_adaptation_steps` argument is set independently of any burnin\n  for the overall chain. In general, adaptation prevents the chain from\n  reaching a stationary distribution, so obtaining consistent samples requires\n  `num_adaptation_steps` be set to a value [somewhat smaller](\n  http://andrewgelman.com/2017/12/15/burn-vs-warm-iterative-simulation-algorithms/#comment-627745)\n  than the number of burnin steps. However, it may sometimes be helpful to set\n  `num_adaptation_steps` to a larger value during development in order to\n  inspect the behavior of the chain during adaptation.\n\n  Args:\n    num_adaptation_steps: Scalar `int` `Tensor` number of initial steps to\n      during which to adjust the step size. This may be greater, less than, or\n      equal to the number of burnin steps. If `None`, the step size is adapted\n      on every step (note this breaks stationarity of the chain!).\n    target_rate: Scalar `Tensor` representing desired `accept_ratio`.\n      Default value: `0.75` (i.e., [center of asymptotically optimal\n      rate](https://arxiv.org/abs/1411.6669)).\n    decrement_multiplier: `Tensor` representing amount to downscale current\n      `step_size`.\n      Default value: `0.01`.\n    increment_multiplier: `Tensor` representing amount to upscale current\n      `step_size`.\n      Default value: `0.01`.\n    step_counter: Scalar `int` `Variable` specifying the current step. The step\n      size is adapted iff `step_counter < num_adaptation_steps`.\n      Default value: if `None`, an internal variable\n        `step_size_adaptation_step_counter` is created and initialized to `-1`.\n\n  Returns:\n    step_size_simple_update_fn: Callable that takes args\n      `step_size_var, kernel_results` and returns updated step size(s).", "docstring_tokens": ["Create", "a", "function", "implementing", "a", "step", "-", "size", "update", "policy", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/hmc.py#L60-L162", "partition": "test"}
{"repo": "bkg/greenwich", "path": "greenwich/raster.py", "func_name": "Raster.masked_array", "original_string": "def masked_array(self, geometry=None):\n        \"\"\"Returns a MaskedArray using nodata values.\n\n        Keyword args:\n        geometry -- any geometry, envelope, or coordinate extent tuple\n        \"\"\"\n        if geometry is None:\n            return self._masked_array()\n        geom = transform(geometry, self.sref)\n        env = Envelope.from_geom(geom).intersect(self.envelope)\n        arr = self._masked_array(env)\n        if geom.GetGeometryType() != ogr.wkbPoint:\n            dims = self.get_offset(env)[2:]\n            affine = AffineTransform(*tuple(self.affine))\n            affine.origin = env.ul\n            mask = ~np.ma.make_mask(geom_to_array(geom, dims, affine))\n            arr.mask = arr.mask | mask\n        return arr", "language": "python", "code": "def masked_array(self, geometry=None):\n        \"\"\"Returns a MaskedArray using nodata values.\n\n        Keyword args:\n        geometry -- any geometry, envelope, or coordinate extent tuple\n        \"\"\"\n        if geometry is None:\n            return self._masked_array()\n        geom = transform(geometry, self.sref)\n        env = Envelope.from_geom(geom).intersect(self.envelope)\n        arr = self._masked_array(env)\n        if geom.GetGeometryType() != ogr.wkbPoint:\n            dims = self.get_offset(env)[2:]\n            affine = AffineTransform(*tuple(self.affine))\n            affine.origin = env.ul\n            mask = ~np.ma.make_mask(geom_to_array(geom, dims, affine))\n            arr.mask = arr.mask | mask\n        return arr", "code_tokens": ["def", "masked_array", "(", "self", ",", "geometry", "=", "None", ")", ":", "if", "geometry", "is", "None", ":", "return", "self", ".", "_masked_array", "(", ")", "geom", "=", "transform", "(", "geometry", ",", "self", ".", "sref", ")", "env", "=", "Envelope", ".", "from_geom", "(", "geom", ")", ".", "intersect", "(", "self", ".", "envelope", ")", "arr", "=", "self", ".", "_masked_array", "(", "env", ")", "if", "geom", ".", "GetGeometryType", "(", ")", "!=", "ogr", ".", "wkbPoint", ":", "dims", "=", "self", ".", "get_offset", "(", "env", ")", "[", "2", ":", "]", "affine", "=", "AffineTransform", "(", "*", "tuple", "(", "self", ".", "affine", ")", ")", "affine", ".", "origin", "=", "env", ".", "ul", "mask", "=", "~", "np", ".", "ma", ".", "make_mask", "(", "geom_to_array", "(", "geom", ",", "dims", ",", "affine", ")", ")", "arr", ".", "mask", "=", "arr", ".", "mask", "|", "mask", "return", "arr"], "docstring": "Returns a MaskedArray using nodata values.\n\n        Keyword args:\n        geometry -- any geometry, envelope, or coordinate extent tuple", "docstring_tokens": ["Returns", "a", "MaskedArray", "using", "nodata", "values", "."], "sha": "57ec644dadfe43ce0ecf2cfd32a2de71e0c8c141", "url": "https://github.com/bkg/greenwich/blob/57ec644dadfe43ce0ecf2cfd32a2de71e0c8c141/greenwich/raster.py#L544-L561", "partition": "test"}
{"repo": "RRZE-HPC/kerncraft", "path": "kerncraft/cacheprediction.py", "func_name": "CacheSimulationPredictor.get_evicts", "original_string": "def get_evicts(self):\n        \"\"\"Return a list with number of evicted cache lines per memory hierarchy level.\"\"\"\n        return [self.stats[cache_level]['EVICT_count']/self.first_dim_factor\n                for cache_level in range(len(self.machine['memory hierarchy']))]", "language": "python", "code": "def get_evicts(self):\n        \"\"\"Return a list with number of evicted cache lines per memory hierarchy level.\"\"\"\n        return [self.stats[cache_level]['EVICT_count']/self.first_dim_factor\n                for cache_level in range(len(self.machine['memory hierarchy']))]", "code_tokens": ["def", "get_evicts", "(", "self", ")", ":", "return", "[", "self", ".", "stats", "[", "cache_level", "]", "[", "'EVICT_count'", "]", "/", "self", ".", "first_dim_factor", "for", "cache_level", "in", "range", "(", "len", "(", "self", ".", "machine", "[", "'memory hierarchy'", "]", ")", ")", "]"], "docstring": "Return a list with number of evicted cache lines per memory hierarchy level.", "docstring_tokens": ["Return", "a", "list", "with", "number", "of", "evicted", "cache", "lines", "per", "memory", "hierarchy", "level", "."], "sha": "c60baf8043e4da8d8d66da7575021c2f4c6c78af", "url": "https://github.com/RRZE-HPC/kerncraft/blob/c60baf8043e4da8d8d66da7575021c2f4c6c78af/kerncraft/cacheprediction.py#L506-L509", "partition": "test"}
{"repo": "google/gin-config", "path": "gin/config.py", "func_name": "constants_from_enum", "original_string": "def constants_from_enum(cls, module=None):\n  \"\"\"Decorator for an enum class that generates Gin constants from values.\n\n  Generated constants have format `module.ClassName.ENUM_VALUE`. The module\n  name is optional when using the constant.\n\n  Args:\n    cls: Class type.\n    module: The module to associate with the constants, to help handle naming\n      collisions. If `None`, `cls.__module__` will be used.\n\n  Returns:\n    Class type (identity function).\n\n  Raises:\n    TypeError: When applied to a non-enum class.\n  \"\"\"\n  if not issubclass(cls, enum.Enum):\n    raise TypeError(\"Class '{}' is not subclass of enum.\".format(cls.__name__))\n\n  if module is None:\n    module = cls.__module__\n  for value in cls:\n    constant('{}.{}'.format(module, str(value)), value)\n  return cls", "language": "python", "code": "def constants_from_enum(cls, module=None):\n  \"\"\"Decorator for an enum class that generates Gin constants from values.\n\n  Generated constants have format `module.ClassName.ENUM_VALUE`. The module\n  name is optional when using the constant.\n\n  Args:\n    cls: Class type.\n    module: The module to associate with the constants, to help handle naming\n      collisions. If `None`, `cls.__module__` will be used.\n\n  Returns:\n    Class type (identity function).\n\n  Raises:\n    TypeError: When applied to a non-enum class.\n  \"\"\"\n  if not issubclass(cls, enum.Enum):\n    raise TypeError(\"Class '{}' is not subclass of enum.\".format(cls.__name__))\n\n  if module is None:\n    module = cls.__module__\n  for value in cls:\n    constant('{}.{}'.format(module, str(value)), value)\n  return cls", "code_tokens": ["def", "constants_from_enum", "(", "cls", ",", "module", "=", "None", ")", ":", "if", "not", "issubclass", "(", "cls", ",", "enum", ".", "Enum", ")", ":", "raise", "TypeError", "(", "\"Class '{}' is not subclass of enum.\"", ".", "format", "(", "cls", ".", "__name__", ")", ")", "if", "module", "is", "None", ":", "module", "=", "cls", ".", "__module__", "for", "value", "in", "cls", ":", "constant", "(", "'{}.{}'", ".", "format", "(", "module", ",", "str", "(", "value", ")", ")", ",", "value", ")", "return", "cls"], "docstring": "Decorator for an enum class that generates Gin constants from values.\n\n  Generated constants have format `module.ClassName.ENUM_VALUE`. The module\n  name is optional when using the constant.\n\n  Args:\n    cls: Class type.\n    module: The module to associate with the constants, to help handle naming\n      collisions. If `None`, `cls.__module__` will be used.\n\n  Returns:\n    Class type (identity function).\n\n  Raises:\n    TypeError: When applied to a non-enum class.", "docstring_tokens": ["Decorator", "for", "an", "enum", "class", "that", "generates", "Gin", "constants", "from", "values", "."], "sha": "17a170e0a6711005d1c78e67cf493dc44674d44f", "url": "https://github.com/google/gin-config/blob/17a170e0a6711005d1c78e67cf493dc44674d44f/gin/config.py#L1703-L1727", "partition": "test"}
{"repo": "zenodo/zenodo-accessrequests", "path": "zenodo_accessrequests/models.py", "func_name": "AccessRequest.accept", "original_string": "def accept(self, message=None, expires_at=None):\n        \"\"\"Accept request.\"\"\"\n        with db.session.begin_nested():\n            if self.status != RequestStatus.PENDING:\n                raise InvalidRequestStateError(RequestStatus.PENDING)\n            self.status = RequestStatus.ACCEPTED\n        request_accepted.send(self, message=message, expires_at=expires_at)", "language": "python", "code": "def accept(self, message=None, expires_at=None):\n        \"\"\"Accept request.\"\"\"\n        with db.session.begin_nested():\n            if self.status != RequestStatus.PENDING:\n                raise InvalidRequestStateError(RequestStatus.PENDING)\n            self.status = RequestStatus.ACCEPTED\n        request_accepted.send(self, message=message, expires_at=expires_at)", "code_tokens": ["def", "accept", "(", "self", ",", "message", "=", "None", ",", "expires_at", "=", "None", ")", ":", "with", "db", ".", "session", ".", "begin_nested", "(", ")", ":", "if", "self", ".", "status", "!=", "RequestStatus", ".", "PENDING", ":", "raise", "InvalidRequestStateError", "(", "RequestStatus", ".", "PENDING", ")", "self", ".", "status", "=", "RequestStatus", ".", "ACCEPTED", "request_accepted", ".", "send", "(", "self", ",", "message", "=", "message", ",", "expires_at", "=", "expires_at", ")"], "docstring": "Accept request.", "docstring_tokens": ["Accept", "request", "."], "sha": "ce2cf3f1425d02ba4f3ad3202cfca43a1892558a", "url": "https://github.com/zenodo/zenodo-accessrequests/blob/ce2cf3f1425d02ba4f3ad3202cfca43a1892558a/zenodo_accessrequests/models.py#L344-L350", "partition": "test"}
{"repo": "Loudr/asana-hub", "path": "asana_hub/transport.py", "func_name": "format_task_numbers_with_links", "original_string": "def format_task_numbers_with_links(tasks):\n    \"\"\"Returns formatting for the tasks section of asana.\"\"\"\n\n    project_id = data.get('asana-project', None)\n\n    def _task_format(task_id):\n        if project_id:\n            asana_url = tool.ToolApp.make_asana_url(project_id, task_id)\n            return \"[#%d](%s)\" % (task_id, asana_url)\n        else:\n            return \"#%d\" % task_id\n\n    return \"\\n\".join([_task_format(tid) for tid in tasks])", "language": "python", "code": "def format_task_numbers_with_links(tasks):\n    \"\"\"Returns formatting for the tasks section of asana.\"\"\"\n\n    project_id = data.get('asana-project', None)\n\n    def _task_format(task_id):\n        if project_id:\n            asana_url = tool.ToolApp.make_asana_url(project_id, task_id)\n            return \"[#%d](%s)\" % (task_id, asana_url)\n        else:\n            return \"#%d\" % task_id\n\n    return \"\\n\".join([_task_format(tid) for tid in tasks])", "code_tokens": ["def", "format_task_numbers_with_links", "(", "tasks", ")", ":", "project_id", "=", "data", ".", "get", "(", "'asana-project'", ",", "None", ")", "def", "_task_format", "(", "task_id", ")", ":", "if", "project_id", ":", "asana_url", "=", "tool", ".", "ToolApp", ".", "make_asana_url", "(", "project_id", ",", "task_id", ")", "return", "\"[#%d](%s)\"", "%", "(", "task_id", ",", "asana_url", ")", "else", ":", "return", "\"#%d\"", "%", "task_id", "return", "\"\\n\"", ".", "join", "(", "[", "_task_format", "(", "tid", ")", "for", "tid", "in", "tasks", "]", ")"], "docstring": "Returns formatting for the tasks section of asana.", "docstring_tokens": ["Returns", "formatting", "for", "the", "tasks", "section", "of", "asana", "."], "sha": "af996ce890ed23d8ede5bf68dcd318e3438829cb", "url": "https://github.com/Loudr/asana-hub/blob/af996ce890ed23d8ede5bf68dcd318e3438829cb/asana_hub/transport.py#L320-L332", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/utils/jsonutil.py", "func_name": "rekey", "original_string": "def rekey(dikt):\n    \"\"\"Rekey a dict that has been forced to use str keys where there should be\n    ints by json.\"\"\"\n    for k in dikt.iterkeys():\n        if isinstance(k, basestring):\n            ik=fk=None\n            try:\n                ik = int(k)\n            except ValueError:\n                try:\n                    fk = float(k)\n                except ValueError:\n                    continue\n            if ik is not None:\n                nk = ik\n            else:\n                nk = fk\n            if nk in dikt:\n                raise KeyError(\"already have key %r\"%nk)\n            dikt[nk] = dikt.pop(k)\n    return dikt", "language": "python", "code": "def rekey(dikt):\n    \"\"\"Rekey a dict that has been forced to use str keys where there should be\n    ints by json.\"\"\"\n    for k in dikt.iterkeys():\n        if isinstance(k, basestring):\n            ik=fk=None\n            try:\n                ik = int(k)\n            except ValueError:\n                try:\n                    fk = float(k)\n                except ValueError:\n                    continue\n            if ik is not None:\n                nk = ik\n            else:\n                nk = fk\n            if nk in dikt:\n                raise KeyError(\"already have key %r\"%nk)\n            dikt[nk] = dikt.pop(k)\n    return dikt", "code_tokens": ["def", "rekey", "(", "dikt", ")", ":", "for", "k", "in", "dikt", ".", "iterkeys", "(", ")", ":", "if", "isinstance", "(", "k", ",", "basestring", ")", ":", "ik", "=", "fk", "=", "None", "try", ":", "ik", "=", "int", "(", "k", ")", "except", "ValueError", ":", "try", ":", "fk", "=", "float", "(", "k", ")", "except", "ValueError", ":", "continue", "if", "ik", "is", "not", "None", ":", "nk", "=", "ik", "else", ":", "nk", "=", "fk", "if", "nk", "in", "dikt", ":", "raise", "KeyError", "(", "\"already have key %r\"", "%", "nk", ")", "dikt", "[", "nk", "]", "=", "dikt", ".", "pop", "(", "k", ")", "return", "dikt"], "docstring": "Rekey a dict that has been forced to use str keys where there should be\n    ints by json.", "docstring_tokens": ["Rekey", "a", "dict", "that", "has", "been", "forced", "to", "use", "str", "keys", "where", "there", "should", "be", "ints", "by", "json", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/utils/jsonutil.py#L38-L58", "partition": "test"}
{"repo": "gnosis/gnosis-py", "path": "gnosis/safe/safe_service.py", "func_name": "SafeService.estimate_tx_gas", "original_string": "def estimate_tx_gas(self, safe_address: str, to: str, value: int, data: bytes, operation: int) -> int:\n        \"\"\"\n        Estimate tx gas. Use the max of calculation using safe method and web3 if operation == CALL or\n        use just the safe calculation otherwise\n        \"\"\"\n        # Costs to route through the proxy and nested calls\n        proxy_gas = 1000\n        # https://github.com/ethereum/solidity/blob/dfe3193c7382c80f1814247a162663a97c3f5e67/libsolidity/codegen/ExpressionCompiler.cpp#L1764\n        # This was `false` before solc 0.4.21 -> `m_context.evmVersion().canOverchargeGasForCall()`\n        # So gas needed by caller will be around 35k\n        old_call_gas = 35000\n        safe_gas_estimation = (self.estimate_tx_gas_with_safe(safe_address, to, value, data, operation)\n                               + proxy_gas + old_call_gas)\n        # We cannot estimate DELEGATECALL (different storage)\n        if SafeOperation(operation) == SafeOperation.CALL:\n            try:\n                web3_gas_estimation = (self.estimate_tx_gas_with_web3(safe_address, to, value, data)\n                                       + proxy_gas + old_call_gas)\n            except ValueError:\n                web3_gas_estimation = 0\n            return max(safe_gas_estimation, web3_gas_estimation)\n\n        else:\n            return safe_gas_estimation", "language": "python", "code": "def estimate_tx_gas(self, safe_address: str, to: str, value: int, data: bytes, operation: int) -> int:\n        \"\"\"\n        Estimate tx gas. Use the max of calculation using safe method and web3 if operation == CALL or\n        use just the safe calculation otherwise\n        \"\"\"\n        # Costs to route through the proxy and nested calls\n        proxy_gas = 1000\n        # https://github.com/ethereum/solidity/blob/dfe3193c7382c80f1814247a162663a97c3f5e67/libsolidity/codegen/ExpressionCompiler.cpp#L1764\n        # This was `false` before solc 0.4.21 -> `m_context.evmVersion().canOverchargeGasForCall()`\n        # So gas needed by caller will be around 35k\n        old_call_gas = 35000\n        safe_gas_estimation = (self.estimate_tx_gas_with_safe(safe_address, to, value, data, operation)\n                               + proxy_gas + old_call_gas)\n        # We cannot estimate DELEGATECALL (different storage)\n        if SafeOperation(operation) == SafeOperation.CALL:\n            try:\n                web3_gas_estimation = (self.estimate_tx_gas_with_web3(safe_address, to, value, data)\n                                       + proxy_gas + old_call_gas)\n            except ValueError:\n                web3_gas_estimation = 0\n            return max(safe_gas_estimation, web3_gas_estimation)\n\n        else:\n            return safe_gas_estimation", "code_tokens": ["def", "estimate_tx_gas", "(", "self", ",", "safe_address", ":", "str", ",", "to", ":", "str", ",", "value", ":", "int", ",", "data", ":", "bytes", ",", "operation", ":", "int", ")", "->", "int", ":", "# Costs to route through the proxy and nested calls", "proxy_gas", "=", "1000", "# https://github.com/ethereum/solidity/blob/dfe3193c7382c80f1814247a162663a97c3f5e67/libsolidity/codegen/ExpressionCompiler.cpp#L1764", "# This was `false` before solc 0.4.21 -> `m_context.evmVersion().canOverchargeGasForCall()`", "# So gas needed by caller will be around 35k", "old_call_gas", "=", "35000", "safe_gas_estimation", "=", "(", "self", ".", "estimate_tx_gas_with_safe", "(", "safe_address", ",", "to", ",", "value", ",", "data", ",", "operation", ")", "+", "proxy_gas", "+", "old_call_gas", ")", "# We cannot estimate DELEGATECALL (different storage)", "if", "SafeOperation", "(", "operation", ")", "==", "SafeOperation", ".", "CALL", ":", "try", ":", "web3_gas_estimation", "=", "(", "self", ".", "estimate_tx_gas_with_web3", "(", "safe_address", ",", "to", ",", "value", ",", "data", ")", "+", "proxy_gas", "+", "old_call_gas", ")", "except", "ValueError", ":", "web3_gas_estimation", "=", "0", "return", "max", "(", "safe_gas_estimation", ",", "web3_gas_estimation", ")", "else", ":", "return", "safe_gas_estimation"], "docstring": "Estimate tx gas. Use the max of calculation using safe method and web3 if operation == CALL or\n        use just the safe calculation otherwise", "docstring_tokens": ["Estimate", "tx", "gas", ".", "Use", "the", "max", "of", "calculation", "using", "safe", "method", "and", "web3", "if", "operation", "==", "CALL", "or", "use", "just", "the", "safe", "calculation", "otherwise"], "sha": "2a9a5d75a375fc9813ac04df133e6910c82f9d49", "url": "https://github.com/gnosis/gnosis-py/blob/2a9a5d75a375fc9813ac04df133e6910c82f9d49/gnosis/safe/safe_service.py#L550-L573", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/h2o.py", "func_name": "import_sql_table", "original_string": "def import_sql_table(connection_url, table, username, password, columns=None, optimize=True, fetch_mode=None):\n    \"\"\"\n    Import SQL table to H2OFrame in memory.\n\n    Assumes that the SQL table is not being updated and is stable.\n    Runs multiple SELECT SQL queries concurrently for parallel ingestion.\n    Be sure to start the h2o.jar in the terminal with your downloaded JDBC driver in the classpath::\n\n        java -cp <path_to_h2o_jar>:<path_to_jdbc_driver_jar> water.H2OApp\n\n    Also see :func:`import_sql_select`.\n    Currently supported SQL databases are MySQL, PostgreSQL, MariaDB, Hive, Oracle and Microsoft SQL.\n\n    :param connection_url: URL of the SQL database connection as specified by the Java Database Connectivity (JDBC)\n        Driver. For example, \"jdbc:mysql://localhost:3306/menagerie?&useSSL=false\"\n    :param table: name of SQL table\n    :param columns: a list of column names to import from SQL table. Default is to import all columns.\n    :param username: username for SQL server\n    :param password: password for SQL server\n    :param optimize: DEPRECATED. Ignored - use fetch_mode instead. Optimize import of SQL table for faster imports.\n    :param fetch_mode: Set to DISTRIBUTED to enable distributed import. Set to SINGLE to force a sequential read by a single node\n        from the database.\n\n    :returns: an :class:`H2OFrame` containing data of the specified SQL table.\n\n    :examples:\n        >>> conn_url = \"jdbc:mysql://172.16.2.178:3306/ingestSQL?&useSSL=false\"\n        >>> table = \"citibike20k\"\n        >>> username = \"root\"\n        >>> password = \"abc123\"\n        >>> my_citibike_data = h2o.import_sql_table(conn_url, table, username, password)\n    \"\"\"\n    assert_is_type(connection_url, str)\n    assert_is_type(table, str)\n    assert_is_type(username, str)\n    assert_is_type(password, str)\n    assert_is_type(columns, [str], None)\n    assert_is_type(optimize, bool)\n    assert_is_type(fetch_mode, str, None)\n    p = {\"connection_url\": connection_url, \"table\": table, \"username\": username, \"password\": password,\n         \"fetch_mode\": fetch_mode}\n    if columns:\n        p[\"columns\"] = \", \".join(columns)\n    j = H2OJob(api(\"POST /99/ImportSQLTable\", data=p), \"Import SQL Table\").poll()\n    return get_frame(j.dest_key)", "language": "python", "code": "def import_sql_table(connection_url, table, username, password, columns=None, optimize=True, fetch_mode=None):\n    \"\"\"\n    Import SQL table to H2OFrame in memory.\n\n    Assumes that the SQL table is not being updated and is stable.\n    Runs multiple SELECT SQL queries concurrently for parallel ingestion.\n    Be sure to start the h2o.jar in the terminal with your downloaded JDBC driver in the classpath::\n\n        java -cp <path_to_h2o_jar>:<path_to_jdbc_driver_jar> water.H2OApp\n\n    Also see :func:`import_sql_select`.\n    Currently supported SQL databases are MySQL, PostgreSQL, MariaDB, Hive, Oracle and Microsoft SQL.\n\n    :param connection_url: URL of the SQL database connection as specified by the Java Database Connectivity (JDBC)\n        Driver. For example, \"jdbc:mysql://localhost:3306/menagerie?&useSSL=false\"\n    :param table: name of SQL table\n    :param columns: a list of column names to import from SQL table. Default is to import all columns.\n    :param username: username for SQL server\n    :param password: password for SQL server\n    :param optimize: DEPRECATED. Ignored - use fetch_mode instead. Optimize import of SQL table for faster imports.\n    :param fetch_mode: Set to DISTRIBUTED to enable distributed import. Set to SINGLE to force a sequential read by a single node\n        from the database.\n\n    :returns: an :class:`H2OFrame` containing data of the specified SQL table.\n\n    :examples:\n        >>> conn_url = \"jdbc:mysql://172.16.2.178:3306/ingestSQL?&useSSL=false\"\n        >>> table = \"citibike20k\"\n        >>> username = \"root\"\n        >>> password = \"abc123\"\n        >>> my_citibike_data = h2o.import_sql_table(conn_url, table, username, password)\n    \"\"\"\n    assert_is_type(connection_url, str)\n    assert_is_type(table, str)\n    assert_is_type(username, str)\n    assert_is_type(password, str)\n    assert_is_type(columns, [str], None)\n    assert_is_type(optimize, bool)\n    assert_is_type(fetch_mode, str, None)\n    p = {\"connection_url\": connection_url, \"table\": table, \"username\": username, \"password\": password,\n         \"fetch_mode\": fetch_mode}\n    if columns:\n        p[\"columns\"] = \", \".join(columns)\n    j = H2OJob(api(\"POST /99/ImportSQLTable\", data=p), \"Import SQL Table\").poll()\n    return get_frame(j.dest_key)", "code_tokens": ["def", "import_sql_table", "(", "connection_url", ",", "table", ",", "username", ",", "password", ",", "columns", "=", "None", ",", "optimize", "=", "True", ",", "fetch_mode", "=", "None", ")", ":", "assert_is_type", "(", "connection_url", ",", "str", ")", "assert_is_type", "(", "table", ",", "str", ")", "assert_is_type", "(", "username", ",", "str", ")", "assert_is_type", "(", "password", ",", "str", ")", "assert_is_type", "(", "columns", ",", "[", "str", "]", ",", "None", ")", "assert_is_type", "(", "optimize", ",", "bool", ")", "assert_is_type", "(", "fetch_mode", ",", "str", ",", "None", ")", "p", "=", "{", "\"connection_url\"", ":", "connection_url", ",", "\"table\"", ":", "table", ",", "\"username\"", ":", "username", ",", "\"password\"", ":", "password", ",", "\"fetch_mode\"", ":", "fetch_mode", "}", "if", "columns", ":", "p", "[", "\"columns\"", "]", "=", "\", \"", ".", "join", "(", "columns", ")", "j", "=", "H2OJob", "(", "api", "(", "\"POST /99/ImportSQLTable\"", ",", "data", "=", "p", ")", ",", "\"Import SQL Table\"", ")", ".", "poll", "(", ")", "return", "get_frame", "(", "j", ".", "dest_key", ")"], "docstring": "Import SQL table to H2OFrame in memory.\n\n    Assumes that the SQL table is not being updated and is stable.\n    Runs multiple SELECT SQL queries concurrently for parallel ingestion.\n    Be sure to start the h2o.jar in the terminal with your downloaded JDBC driver in the classpath::\n\n        java -cp <path_to_h2o_jar>:<path_to_jdbc_driver_jar> water.H2OApp\n\n    Also see :func:`import_sql_select`.\n    Currently supported SQL databases are MySQL, PostgreSQL, MariaDB, Hive, Oracle and Microsoft SQL.\n\n    :param connection_url: URL of the SQL database connection as specified by the Java Database Connectivity (JDBC)\n        Driver. For example, \"jdbc:mysql://localhost:3306/menagerie?&useSSL=false\"\n    :param table: name of SQL table\n    :param columns: a list of column names to import from SQL table. Default is to import all columns.\n    :param username: username for SQL server\n    :param password: password for SQL server\n    :param optimize: DEPRECATED. Ignored - use fetch_mode instead. Optimize import of SQL table for faster imports.\n    :param fetch_mode: Set to DISTRIBUTED to enable distributed import. Set to SINGLE to force a sequential read by a single node\n        from the database.\n\n    :returns: an :class:`H2OFrame` containing data of the specified SQL table.\n\n    :examples:\n        >>> conn_url = \"jdbc:mysql://172.16.2.178:3306/ingestSQL?&useSSL=false\"\n        >>> table = \"citibike20k\"\n        >>> username = \"root\"\n        >>> password = \"abc123\"\n        >>> my_citibike_data = h2o.import_sql_table(conn_url, table, username, password)", "docstring_tokens": ["Import", "SQL", "table", "to", "H2OFrame", "in", "memory", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/h2o.py#L464-L508", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/model/model_base.py", "func_name": "ModelBase.show", "original_string": "def show(self):\n        \"\"\"Print innards of model, without regards to type.\"\"\"\n        if self._future:\n            self._job.poll_once()\n            return\n        if self._model_json is None:\n            print(\"No model trained yet\")\n            return\n        if self.model_id is None:\n            print(\"This H2OEstimator has been removed.\")\n            return\n        model = self._model_json[\"output\"]\n        print(\"Model Details\")\n        print(\"=============\")\n\n        print(self.__class__.__name__, \": \", self._model_json[\"algo_full_name\"])\n        print(\"Model Key: \", self._id)\n\n        self.summary()\n\n        print()\n        # training metrics\n        tm = model[\"training_metrics\"]\n        if tm: tm.show()\n        vm = model[\"validation_metrics\"]\n        if vm: vm.show()\n        xm = model[\"cross_validation_metrics\"]\n        if xm: xm.show()\n        xms = model[\"cross_validation_metrics_summary\"]\n        if xms: xms.show()\n\n        if \"scoring_history\" in model and model[\"scoring_history\"]:\n            model[\"scoring_history\"].show()\n        if \"variable_importances\" in model and model[\"variable_importances\"]:\n            model[\"variable_importances\"].show()", "language": "python", "code": "def show(self):\n        \"\"\"Print innards of model, without regards to type.\"\"\"\n        if self._future:\n            self._job.poll_once()\n            return\n        if self._model_json is None:\n            print(\"No model trained yet\")\n            return\n        if self.model_id is None:\n            print(\"This H2OEstimator has been removed.\")\n            return\n        model = self._model_json[\"output\"]\n        print(\"Model Details\")\n        print(\"=============\")\n\n        print(self.__class__.__name__, \": \", self._model_json[\"algo_full_name\"])\n        print(\"Model Key: \", self._id)\n\n        self.summary()\n\n        print()\n        # training metrics\n        tm = model[\"training_metrics\"]\n        if tm: tm.show()\n        vm = model[\"validation_metrics\"]\n        if vm: vm.show()\n        xm = model[\"cross_validation_metrics\"]\n        if xm: xm.show()\n        xms = model[\"cross_validation_metrics_summary\"]\n        if xms: xms.show()\n\n        if \"scoring_history\" in model and model[\"scoring_history\"]:\n            model[\"scoring_history\"].show()\n        if \"variable_importances\" in model and model[\"variable_importances\"]:\n            model[\"variable_importances\"].show()", "code_tokens": ["def", "show", "(", "self", ")", ":", "if", "self", ".", "_future", ":", "self", ".", "_job", ".", "poll_once", "(", ")", "return", "if", "self", ".", "_model_json", "is", "None", ":", "print", "(", "\"No model trained yet\"", ")", "return", "if", "self", ".", "model_id", "is", "None", ":", "print", "(", "\"This H2OEstimator has been removed.\"", ")", "return", "model", "=", "self", ".", "_model_json", "[", "\"output\"", "]", "print", "(", "\"Model Details\"", ")", "print", "(", "\"=============\"", ")", "print", "(", "self", ".", "__class__", ".", "__name__", ",", "\": \"", ",", "self", ".", "_model_json", "[", "\"algo_full_name\"", "]", ")", "print", "(", "\"Model Key: \"", ",", "self", ".", "_id", ")", "self", ".", "summary", "(", ")", "print", "(", ")", "# training metrics", "tm", "=", "model", "[", "\"training_metrics\"", "]", "if", "tm", ":", "tm", ".", "show", "(", ")", "vm", "=", "model", "[", "\"validation_metrics\"", "]", "if", "vm", ":", "vm", ".", "show", "(", ")", "xm", "=", "model", "[", "\"cross_validation_metrics\"", "]", "if", "xm", ":", "xm", ".", "show", "(", ")", "xms", "=", "model", "[", "\"cross_validation_metrics_summary\"", "]", "if", "xms", ":", "xms", ".", "show", "(", ")", "if", "\"scoring_history\"", "in", "model", "and", "model", "[", "\"scoring_history\"", "]", ":", "model", "[", "\"scoring_history\"", "]", ".", "show", "(", ")", "if", "\"variable_importances\"", "in", "model", "and", "model", "[", "\"variable_importances\"", "]", ":", "model", "[", "\"variable_importances\"", "]", ".", "show", "(", ")"], "docstring": "Print innards of model, without regards to type.", "docstring_tokens": ["Print", "innards", "of", "model", "without", "regards", "to", "type", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/model/model_base.py#L389-L423", "partition": "test"}
{"repo": "uw-it-aca/uw-restclients-canvas", "path": "uw_canvas/sections.py", "func_name": "Sections.get_section", "original_string": "def get_section(self, section_id, params={}):\n        \"\"\"\n        Return section resource for given canvas section id.\n\n        https://canvas.instructure.com/doc/api/sections.html#method.sections.show\n        \"\"\"\n        url = SECTIONS_API.format(section_id)\n        return CanvasSection(data=self._get_resource(url, params=params))", "language": "python", "code": "def get_section(self, section_id, params={}):\n        \"\"\"\n        Return section resource for given canvas section id.\n\n        https://canvas.instructure.com/doc/api/sections.html#method.sections.show\n        \"\"\"\n        url = SECTIONS_API.format(section_id)\n        return CanvasSection(data=self._get_resource(url, params=params))", "code_tokens": ["def", "get_section", "(", "self", ",", "section_id", ",", "params", "=", "{", "}", ")", ":", "url", "=", "SECTIONS_API", ".", "format", "(", "section_id", ")", "return", "CanvasSection", "(", "data", "=", "self", ".", "_get_resource", "(", "url", ",", "params", "=", "params", ")", ")"], "docstring": "Return section resource for given canvas section id.\n\n        https://canvas.instructure.com/doc/api/sections.html#method.sections.show", "docstring_tokens": ["Return", "section", "resource", "for", "given", "canvas", "section", "id", "."], "sha": "9845faf33d49a8f06908efc22640c001116d6ea2", "url": "https://github.com/uw-it-aca/uw-restclients-canvas/blob/9845faf33d49a8f06908efc22640c001116d6ea2/uw_canvas/sections.py#L9-L16", "partition": "test"}
{"repo": "dossier/dossier.store", "path": "dossier/store/elastic.py", "func_name": "ElasticStore._fc_index_disjunction_from_query", "original_string": "def _fc_index_disjunction_from_query(self, query_fc, fname):\n        'Creates a disjunction for keyword scan queries.'\n        if len(query_fc.get(fname, [])) == 0:\n            return []\n        terms = query_fc[fname].keys()\n\n        disj = []\n        for fname in self.indexes[fname]['feature_names']:\n            disj.append({'terms': {fname_to_idx_name(fname): terms}})\n        return disj", "language": "python", "code": "def _fc_index_disjunction_from_query(self, query_fc, fname):\n        'Creates a disjunction for keyword scan queries.'\n        if len(query_fc.get(fname, [])) == 0:\n            return []\n        terms = query_fc[fname].keys()\n\n        disj = []\n        for fname in self.indexes[fname]['feature_names']:\n            disj.append({'terms': {fname_to_idx_name(fname): terms}})\n        return disj", "code_tokens": ["def", "_fc_index_disjunction_from_query", "(", "self", ",", "query_fc", ",", "fname", ")", ":", "if", "len", "(", "query_fc", ".", "get", "(", "fname", ",", "[", "]", ")", ")", "==", "0", ":", "return", "[", "]", "terms", "=", "query_fc", "[", "fname", "]", ".", "keys", "(", ")", "disj", "=", "[", "]", "for", "fname", "in", "self", ".", "indexes", "[", "fname", "]", "[", "'feature_names'", "]", ":", "disj", ".", "append", "(", "{", "'terms'", ":", "{", "fname_to_idx_name", "(", "fname", ")", ":", "terms", "}", "}", ")", "return", "disj"], "docstring": "Creates a disjunction for keyword scan queries.", "docstring_tokens": ["Creates", "a", "disjunction", "for", "keyword", "scan", "queries", "."], "sha": "b22ffe2470bba9fcc98a30cb55b437bfa1521e7f", "url": "https://github.com/dossier/dossier.store/blob/b22ffe2470bba9fcc98a30cb55b437bfa1521e7f/dossier/store/elastic.py#L782-L791", "partition": "test"}
{"repo": "neurodata/ndio", "path": "ndio/convert/tiff.py", "func_name": "save", "original_string": "def save(tiff_filename, numpy_data):\n    \"\"\"\n    Export a numpy array to a TIFF file.\n\n    Arguments:\n        tiff_filename:  A filename to which to save the TIFF data\n        numpy_data:     The numpy array to save to TIFF\n\n    Returns:\n        String. The expanded filename that now holds the TIFF data\n    \"\"\"\n    # Expand filename to be absolute\n    tiff_filename = os.path.expanduser(tiff_filename)\n\n    if type(numpy_data) is str:\n        fp = open(png_filename, \"wb\")\n        fp.write(numpy_data)\n        fp.close()\n        return png_filename\n\n    try:\n        img = tiff.imsave(tiff_filename, numpy_data)\n    except Exception as e:\n        raise ValueError(\"Could not save TIFF file {0}.\".format(tiff_filename))\n\n    return tiff_filename", "language": "python", "code": "def save(tiff_filename, numpy_data):\n    \"\"\"\n    Export a numpy array to a TIFF file.\n\n    Arguments:\n        tiff_filename:  A filename to which to save the TIFF data\n        numpy_data:     The numpy array to save to TIFF\n\n    Returns:\n        String. The expanded filename that now holds the TIFF data\n    \"\"\"\n    # Expand filename to be absolute\n    tiff_filename = os.path.expanduser(tiff_filename)\n\n    if type(numpy_data) is str:\n        fp = open(png_filename, \"wb\")\n        fp.write(numpy_data)\n        fp.close()\n        return png_filename\n\n    try:\n        img = tiff.imsave(tiff_filename, numpy_data)\n    except Exception as e:\n        raise ValueError(\"Could not save TIFF file {0}.\".format(tiff_filename))\n\n    return tiff_filename", "code_tokens": ["def", "save", "(", "tiff_filename", ",", "numpy_data", ")", ":", "# Expand filename to be absolute", "tiff_filename", "=", "os", ".", "path", ".", "expanduser", "(", "tiff_filename", ")", "if", "type", "(", "numpy_data", ")", "is", "str", ":", "fp", "=", "open", "(", "png_filename", ",", "\"wb\"", ")", "fp", ".", "write", "(", "numpy_data", ")", "fp", ".", "close", "(", ")", "return", "png_filename", "try", ":", "img", "=", "tiff", ".", "imsave", "(", "tiff_filename", ",", "numpy_data", ")", "except", "Exception", "as", "e", ":", "raise", "ValueError", "(", "\"Could not save TIFF file {0}.\"", ".", "format", "(", "tiff_filename", ")", ")", "return", "tiff_filename"], "docstring": "Export a numpy array to a TIFF file.\n\n    Arguments:\n        tiff_filename:  A filename to which to save the TIFF data\n        numpy_data:     The numpy array to save to TIFF\n\n    Returns:\n        String. The expanded filename that now holds the TIFF data", "docstring_tokens": ["Export", "a", "numpy", "array", "to", "a", "TIFF", "file", "."], "sha": "792dd5816bc770b05a3db2f4327da42ff6253531", "url": "https://github.com/neurodata/ndio/blob/792dd5816bc770b05a3db2f4327da42ff6253531/ndio/convert/tiff.py#L31-L56", "partition": "test"}
{"repo": "pyca/pyopenssl", "path": "src/OpenSSL/crypto.py", "func_name": "X509Req.set_pubkey", "original_string": "def set_pubkey(self, pkey):\n        \"\"\"\n        Set the public key of the certificate signing request.\n\n        :param pkey: The public key to use.\n        :type pkey: :py:class:`PKey`\n\n        :return: ``None``\n        \"\"\"\n        set_result = _lib.X509_REQ_set_pubkey(self._req, pkey._pkey)\n        _openssl_assert(set_result == 1)", "language": "python", "code": "def set_pubkey(self, pkey):\n        \"\"\"\n        Set the public key of the certificate signing request.\n\n        :param pkey: The public key to use.\n        :type pkey: :py:class:`PKey`\n\n        :return: ``None``\n        \"\"\"\n        set_result = _lib.X509_REQ_set_pubkey(self._req, pkey._pkey)\n        _openssl_assert(set_result == 1)", "code_tokens": ["def", "set_pubkey", "(", "self", ",", "pkey", ")", ":", "set_result", "=", "_lib", ".", "X509_REQ_set_pubkey", "(", "self", ".", "_req", ",", "pkey", ".", "_pkey", ")", "_openssl_assert", "(", "set_result", "==", "1", ")"], "docstring": "Set the public key of the certificate signing request.\n\n        :param pkey: The public key to use.\n        :type pkey: :py:class:`PKey`\n\n        :return: ``None``", "docstring_tokens": ["Set", "the", "public", "key", "of", "the", "certificate", "signing", "request", "."], "sha": "1fbe064c50fd030948141d7d630673761525b0d0", "url": "https://github.com/pyca/pyopenssl/blob/1fbe064c50fd030948141d7d630673761525b0d0/src/OpenSSL/crypto.py#L896-L906", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/git.py", "func_name": "GitRepository.count_objects", "original_string": "def count_objects(self):\n        \"\"\"Count the objects of a repository.\n\n        The method returns the total number of objects (packed and unpacked)\n        available on the repository.\n\n        :raises RepositoryError: when an error occurs counting the objects\n            of a repository\n        \"\"\"\n        cmd_count = ['git', 'count-objects', '-v']\n\n        outs = self._exec(cmd_count, cwd=self.dirpath, env=self.gitenv)\n        outs = outs.decode('utf-8', errors='surrogateescape').rstrip()\n\n        try:\n            cobjs = {k: v for k, v in (x.split(': ') for x in outs.split('\\n'))}\n            nobjs = int(cobjs['count']) + int(cobjs['in-pack'])\n        except KeyError as e:\n            error = \"unable to parse 'count-objects' output; reason: '%s' entry not found\" \\\n                % e.args[0]\n            raise RepositoryError(cause=error)\n        except ValueError as e:\n            error = \"unable to parse 'count-objects' output; reason: %s\" % str(e)\n            raise RepositoryError(cause=error)\n\n        logger.debug(\"Git %s repository has %s objects\",\n                     self.uri, str(nobjs))\n\n        return nobjs", "language": "python", "code": "def count_objects(self):\n        \"\"\"Count the objects of a repository.\n\n        The method returns the total number of objects (packed and unpacked)\n        available on the repository.\n\n        :raises RepositoryError: when an error occurs counting the objects\n            of a repository\n        \"\"\"\n        cmd_count = ['git', 'count-objects', '-v']\n\n        outs = self._exec(cmd_count, cwd=self.dirpath, env=self.gitenv)\n        outs = outs.decode('utf-8', errors='surrogateescape').rstrip()\n\n        try:\n            cobjs = {k: v for k, v in (x.split(': ') for x in outs.split('\\n'))}\n            nobjs = int(cobjs['count']) + int(cobjs['in-pack'])\n        except KeyError as e:\n            error = \"unable to parse 'count-objects' output; reason: '%s' entry not found\" \\\n                % e.args[0]\n            raise RepositoryError(cause=error)\n        except ValueError as e:\n            error = \"unable to parse 'count-objects' output; reason: %s\" % str(e)\n            raise RepositoryError(cause=error)\n\n        logger.debug(\"Git %s repository has %s objects\",\n                     self.uri, str(nobjs))\n\n        return nobjs", "code_tokens": ["def", "count_objects", "(", "self", ")", ":", "cmd_count", "=", "[", "'git'", ",", "'count-objects'", ",", "'-v'", "]", "outs", "=", "self", ".", "_exec", "(", "cmd_count", ",", "cwd", "=", "self", ".", "dirpath", ",", "env", "=", "self", ".", "gitenv", ")", "outs", "=", "outs", ".", "decode", "(", "'utf-8'", ",", "errors", "=", "'surrogateescape'", ")", ".", "rstrip", "(", ")", "try", ":", "cobjs", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "(", "x", ".", "split", "(", "': '", ")", "for", "x", "in", "outs", ".", "split", "(", "'\\n'", ")", ")", "}", "nobjs", "=", "int", "(", "cobjs", "[", "'count'", "]", ")", "+", "int", "(", "cobjs", "[", "'in-pack'", "]", ")", "except", "KeyError", "as", "e", ":", "error", "=", "\"unable to parse 'count-objects' output; reason: '%s' entry not found\"", "%", "e", ".", "args", "[", "0", "]", "raise", "RepositoryError", "(", "cause", "=", "error", ")", "except", "ValueError", "as", "e", ":", "error", "=", "\"unable to parse 'count-objects' output; reason: %s\"", "%", "str", "(", "e", ")", "raise", "RepositoryError", "(", "cause", "=", "error", ")", "logger", ".", "debug", "(", "\"Git %s repository has %s objects\"", ",", "self", ".", "uri", ",", "str", "(", "nobjs", ")", ")", "return", "nobjs"], "docstring": "Count the objects of a repository.\n\n        The method returns the total number of objects (packed and unpacked)\n        available on the repository.\n\n        :raises RepositoryError: when an error occurs counting the objects\n            of a repository", "docstring_tokens": ["Count", "the", "objects", "of", "a", "repository", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/git.py#L831-L859", "partition": "test"}
{"repo": "PythonSanSebastian/docstamp", "path": "docstamp/template.py", "func_name": "SVGDocument.render", "original_string": "def render(self, file_path, **kwargs):\n        \"\"\" Save the content of the .svg file in the chosen rendered format.\n\n        Parameters\n        ----------\n        file_path: str\n            Path to the output file.\n\n        Kwargs\n        ------\n        file_type: str\n            Choices: 'png', 'pdf', 'svg'\n            Default: 'pdf'\n\n        dpi: int\n            Dots-per-inch for the png and pdf.\n            Default: 150\n\n        support_unicode: bool\n            Whether to allow unicode to be encoded in the PDF.\n            Default: False\n        \"\"\"\n        temp = get_tempfile(suffix='.svg')\n        self.save_content(temp.name)\n\n        file_type = kwargs.get('file_type', 'pdf')\n        dpi = kwargs.get('dpi', 150)\n        support_unicode = kwargs.get('support_unicode', False)\n        try:\n            if file_type == 'svg':\n                shutil.copyfile(temp.name, file_path)\n            elif file_type == 'png':\n                svg2png(temp.name, file_path, dpi=dpi)\n            elif file_type == 'pdf':\n                svg2pdf(temp.name, file_path, dpi=dpi, support_unicode=support_unicode)\n        except:\n            log.exception(\n                'Error exporting file {} to {}'.format(file_path, file_type)\n            )\n            raise", "language": "python", "code": "def render(self, file_path, **kwargs):\n        \"\"\" Save the content of the .svg file in the chosen rendered format.\n\n        Parameters\n        ----------\n        file_path: str\n            Path to the output file.\n\n        Kwargs\n        ------\n        file_type: str\n            Choices: 'png', 'pdf', 'svg'\n            Default: 'pdf'\n\n        dpi: int\n            Dots-per-inch for the png and pdf.\n            Default: 150\n\n        support_unicode: bool\n            Whether to allow unicode to be encoded in the PDF.\n            Default: False\n        \"\"\"\n        temp = get_tempfile(suffix='.svg')\n        self.save_content(temp.name)\n\n        file_type = kwargs.get('file_type', 'pdf')\n        dpi = kwargs.get('dpi', 150)\n        support_unicode = kwargs.get('support_unicode', False)\n        try:\n            if file_type == 'svg':\n                shutil.copyfile(temp.name, file_path)\n            elif file_type == 'png':\n                svg2png(temp.name, file_path, dpi=dpi)\n            elif file_type == 'pdf':\n                svg2pdf(temp.name, file_path, dpi=dpi, support_unicode=support_unicode)\n        except:\n            log.exception(\n                'Error exporting file {} to {}'.format(file_path, file_type)\n            )\n            raise", "code_tokens": ["def", "render", "(", "self", ",", "file_path", ",", "*", "*", "kwargs", ")", ":", "temp", "=", "get_tempfile", "(", "suffix", "=", "'.svg'", ")", "self", ".", "save_content", "(", "temp", ".", "name", ")", "file_type", "=", "kwargs", ".", "get", "(", "'file_type'", ",", "'pdf'", ")", "dpi", "=", "kwargs", ".", "get", "(", "'dpi'", ",", "150", ")", "support_unicode", "=", "kwargs", ".", "get", "(", "'support_unicode'", ",", "False", ")", "try", ":", "if", "file_type", "==", "'svg'", ":", "shutil", ".", "copyfile", "(", "temp", ".", "name", ",", "file_path", ")", "elif", "file_type", "==", "'png'", ":", "svg2png", "(", "temp", ".", "name", ",", "file_path", ",", "dpi", "=", "dpi", ")", "elif", "file_type", "==", "'pdf'", ":", "svg2pdf", "(", "temp", ".", "name", ",", "file_path", ",", "dpi", "=", "dpi", ",", "support_unicode", "=", "support_unicode", ")", "except", ":", "log", ".", "exception", "(", "'Error exporting file {} to {}'", ".", "format", "(", "file_path", ",", "file_type", ")", ")", "raise"], "docstring": "Save the content of the .svg file in the chosen rendered format.\n\n        Parameters\n        ----------\n        file_path: str\n            Path to the output file.\n\n        Kwargs\n        ------\n        file_type: str\n            Choices: 'png', 'pdf', 'svg'\n            Default: 'pdf'\n\n        dpi: int\n            Dots-per-inch for the png and pdf.\n            Default: 150\n\n        support_unicode: bool\n            Whether to allow unicode to be encoded in the PDF.\n            Default: False", "docstring_tokens": ["Save", "the", "content", "of", "the", ".", "svg", "file", "in", "the", "chosen", "rendered", "format", "."], "sha": "b43808f2e15351b0b2f0b7eade9c7ef319c9e646", "url": "https://github.com/PythonSanSebastian/docstamp/blob/b43808f2e15351b0b2f0b7eade9c7ef319c9e646/docstamp/template.py#L225-L264", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/core/messages/contents.py", "func_name": "ProtocolVersion.read", "original_string": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the ProtocolVersion struct and decode it into\n        its constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if either the major or minor protocol versions\n                are missing from the encoding.\n        \"\"\"\n        super(ProtocolVersion, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(enums.Tags.PROTOCOL_VERSION_MAJOR, local_stream):\n            self._major = primitives.Integer(\n                tag=enums.Tags.PROTOCOL_VERSION_MAJOR\n            )\n            self._major.read(local_stream, kmip_version=kmip_version)\n        else:\n            raise ValueError(\n                \"Invalid encoding missing the major protocol version number.\"\n            )\n\n        if self.is_tag_next(enums.Tags.PROTOCOL_VERSION_MINOR, local_stream):\n            self._minor = primitives.Integer(\n                tag=enums.Tags.PROTOCOL_VERSION_MINOR\n            )\n            self._minor.read(local_stream, kmip_version=kmip_version)\n        else:\n            raise ValueError(\n                \"Invalid encoding missing the minor protocol version number.\"\n            )\n\n        self.is_oversized(local_stream)", "language": "python", "code": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the ProtocolVersion struct and decode it into\n        its constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if either the major or minor protocol versions\n                are missing from the encoding.\n        \"\"\"\n        super(ProtocolVersion, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(enums.Tags.PROTOCOL_VERSION_MAJOR, local_stream):\n            self._major = primitives.Integer(\n                tag=enums.Tags.PROTOCOL_VERSION_MAJOR\n            )\n            self._major.read(local_stream, kmip_version=kmip_version)\n        else:\n            raise ValueError(\n                \"Invalid encoding missing the major protocol version number.\"\n            )\n\n        if self.is_tag_next(enums.Tags.PROTOCOL_VERSION_MINOR, local_stream):\n            self._minor = primitives.Integer(\n                tag=enums.Tags.PROTOCOL_VERSION_MINOR\n            )\n            self._minor.read(local_stream, kmip_version=kmip_version)\n        else:\n            raise ValueError(\n                \"Invalid encoding missing the minor protocol version number.\"\n            )\n\n        self.is_oversized(local_stream)", "code_tokens": ["def", "read", "(", "self", ",", "input_stream", ",", "kmip_version", "=", "enums", ".", "KMIPVersion", ".", "KMIP_1_0", ")", ":", "super", "(", "ProtocolVersion", ",", "self", ")", ".", "read", "(", "input_stream", ",", "kmip_version", "=", "kmip_version", ")", "local_stream", "=", "utils", ".", "BytearrayStream", "(", "input_stream", ".", "read", "(", "self", ".", "length", ")", ")", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "PROTOCOL_VERSION_MAJOR", ",", "local_stream", ")", ":", "self", ".", "_major", "=", "primitives", ".", "Integer", "(", "tag", "=", "enums", ".", "Tags", ".", "PROTOCOL_VERSION_MAJOR", ")", "self", ".", "_major", ".", "read", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "else", ":", "raise", "ValueError", "(", "\"Invalid encoding missing the major protocol version number.\"", ")", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "PROTOCOL_VERSION_MINOR", ",", "local_stream", ")", ":", "self", ".", "_minor", "=", "primitives", ".", "Integer", "(", "tag", "=", "enums", ".", "Tags", ".", "PROTOCOL_VERSION_MINOR", ")", "self", ".", "_minor", ".", "read", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "else", ":", "raise", "ValueError", "(", "\"Invalid encoding missing the minor protocol version number.\"", ")", "self", ".", "is_oversized", "(", "local_stream", ")"], "docstring": "Read the data encoding the ProtocolVersion struct and decode it into\n        its constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if either the major or minor protocol versions\n                are missing from the encoding.", "docstring_tokens": ["Read", "the", "data", "encoding", "the", "ProtocolVersion", "struct", "and", "decode", "it", "into", "its", "constituent", "parts", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/core/messages/contents.py#L101-L144", "partition": "test"}
{"repo": "rwl/godot", "path": "godot/util.py", "func_name": "Serializable.load_from_file", "original_string": "def load_from_file(cls, filename, format=None):\n        \"\"\" Return an instance of the class that is saved in the file with the\n            given filename in the specified format.\n        \"\"\"\n        if format is None:\n            # try to derive protocol from file extension\n            format = format_from_extension(filename)\n        with file(filename,'rbU') as fp:\n            obj = cls.load_from_file_like(fp, format)\n            obj.filename = filename\n            return obj", "language": "python", "code": "def load_from_file(cls, filename, format=None):\n        \"\"\" Return an instance of the class that is saved in the file with the\n            given filename in the specified format.\n        \"\"\"\n        if format is None:\n            # try to derive protocol from file extension\n            format = format_from_extension(filename)\n        with file(filename,'rbU') as fp:\n            obj = cls.load_from_file_like(fp, format)\n            obj.filename = filename\n            return obj", "code_tokens": ["def", "load_from_file", "(", "cls", ",", "filename", ",", "format", "=", "None", ")", ":", "if", "format", "is", "None", ":", "# try to derive protocol from file extension", "format", "=", "format_from_extension", "(", "filename", ")", "with", "file", "(", "filename", ",", "'rbU'", ")", "as", "fp", ":", "obj", "=", "cls", ".", "load_from_file_like", "(", "fp", ",", "format", ")", "obj", ".", "filename", "=", "filename", "return", "obj"], "docstring": "Return an instance of the class that is saved in the file with the\n            given filename in the specified format.", "docstring_tokens": ["Return", "an", "instance", "of", "the", "class", "that", "is", "saved", "in", "the", "file", "with", "the", "given", "filename", "in", "the", "specified", "format", "."], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/util.py#L98-L108", "partition": "test"}
{"repo": "shaunduncan/giphypop", "path": "giphypop.py", "func_name": "Giphy.translate", "original_string": "def translate(self, term=None, phrase=None, strict=False, rating=None):\n        \"\"\"\n        Retrieve a single image that represents a transalation of a term or\n        phrase into an animated gif. Punctuation is ignored. By default, this\n        will perform a `term` translation. If you want to translate by phrase,\n        use the `phrase` keyword argument.\n\n        :param term: Search term or terms\n        :type term: string\n        :param phrase: Search phrase\n        :type phrase: string\n        :param strict: Whether an exception should be raised when no results\n        :type strict: boolean\n        :param rating: limit results to those rated (y,g, pg, pg-13 or r).\n        :type rating: string\n        \"\"\"\n        assert any((term, phrase)), 'You must supply a term or phrase to search'\n\n        # Phrases should have dashes and not spaces\n        if phrase:\n            phrase = phrase.replace(' ', '-')\n\n        params = {'s': (term or phrase)}\n        if rating:\n            params.update({'rating': rating})\n        resp = self._fetch('translate', **params)\n        if resp['data']:\n            return GiphyImage(resp['data'])\n        elif strict or self.strict:\n            raise GiphyApiException(\n                \"Term/Phrase '%s' could not be translated into a GIF\" %\n                (term or phrase))", "language": "python", "code": "def translate(self, term=None, phrase=None, strict=False, rating=None):\n        \"\"\"\n        Retrieve a single image that represents a transalation of a term or\n        phrase into an animated gif. Punctuation is ignored. By default, this\n        will perform a `term` translation. If you want to translate by phrase,\n        use the `phrase` keyword argument.\n\n        :param term: Search term or terms\n        :type term: string\n        :param phrase: Search phrase\n        :type phrase: string\n        :param strict: Whether an exception should be raised when no results\n        :type strict: boolean\n        :param rating: limit results to those rated (y,g, pg, pg-13 or r).\n        :type rating: string\n        \"\"\"\n        assert any((term, phrase)), 'You must supply a term or phrase to search'\n\n        # Phrases should have dashes and not spaces\n        if phrase:\n            phrase = phrase.replace(' ', '-')\n\n        params = {'s': (term or phrase)}\n        if rating:\n            params.update({'rating': rating})\n        resp = self._fetch('translate', **params)\n        if resp['data']:\n            return GiphyImage(resp['data'])\n        elif strict or self.strict:\n            raise GiphyApiException(\n                \"Term/Phrase '%s' could not be translated into a GIF\" %\n                (term or phrase))", "code_tokens": ["def", "translate", "(", "self", ",", "term", "=", "None", ",", "phrase", "=", "None", ",", "strict", "=", "False", ",", "rating", "=", "None", ")", ":", "assert", "any", "(", "(", "term", ",", "phrase", ")", ")", ",", "'You must supply a term or phrase to search'", "# Phrases should have dashes and not spaces", "if", "phrase", ":", "phrase", "=", "phrase", ".", "replace", "(", "' '", ",", "'-'", ")", "params", "=", "{", "'s'", ":", "(", "term", "or", "phrase", ")", "}", "if", "rating", ":", "params", ".", "update", "(", "{", "'rating'", ":", "rating", "}", ")", "resp", "=", "self", ".", "_fetch", "(", "'translate'", ",", "*", "*", "params", ")", "if", "resp", "[", "'data'", "]", ":", "return", "GiphyImage", "(", "resp", "[", "'data'", "]", ")", "elif", "strict", "or", "self", ".", "strict", ":", "raise", "GiphyApiException", "(", "\"Term/Phrase '%s' could not be translated into a GIF\"", "%", "(", "term", "or", "phrase", ")", ")"], "docstring": "Retrieve a single image that represents a transalation of a term or\n        phrase into an animated gif. Punctuation is ignored. By default, this\n        will perform a `term` translation. If you want to translate by phrase,\n        use the `phrase` keyword argument.\n\n        :param term: Search term or terms\n        :type term: string\n        :param phrase: Search phrase\n        :type phrase: string\n        :param strict: Whether an exception should be raised when no results\n        :type strict: boolean\n        :param rating: limit results to those rated (y,g, pg, pg-13 or r).\n        :type rating: string", "docstring_tokens": ["Retrieve", "a", "single", "image", "that", "represents", "a", "transalation", "of", "a", "term", "or", "phrase", "into", "an", "animated", "gif", ".", "Punctuation", "is", "ignored", ".", "By", "default", "this", "will", "perform", "a", "term", "translation", ".", "If", "you", "want", "to", "translate", "by", "phrase", "use", "the", "phrase", "keyword", "argument", "."], "sha": "21e7f51c4f000ae24be3805b7eeec52bcce3d390", "url": "https://github.com/shaunduncan/giphypop/blob/21e7f51c4f000ae24be3805b7eeec52bcce3d390/giphypop.py#L340-L371", "partition": "test"}
{"repo": "Jasily/jasily-python", "path": "jasily/lang/with_any.py", "func_name": "with_objattrs", "original_string": "def with_objattrs(*names):\n    '''\n    like `with_objattr` but enter context one by one.\n    '''\n\n    def _wrap(func):\n\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            with contextlib.ExitStack() as stack:\n                for name in names:\n                    stack.enter_context(getattr(self, name))\n                return func(self, *args, **kwargs)\n\n        return wrapper\n\n    return _wrap", "language": "python", "code": "def with_objattrs(*names):\n    '''\n    like `with_objattr` but enter context one by one.\n    '''\n\n    def _wrap(func):\n\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            with contextlib.ExitStack() as stack:\n                for name in names:\n                    stack.enter_context(getattr(self, name))\n                return func(self, *args, **kwargs)\n\n        return wrapper\n\n    return _wrap", "code_tokens": ["def", "with_objattrs", "(", "*", "names", ")", ":", "def", "_wrap", "(", "func", ")", ":", "@", "functools", ".", "wraps", "(", "func", ")", "def", "wrapper", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "with", "contextlib", ".", "ExitStack", "(", ")", "as", "stack", ":", "for", "name", "in", "names", ":", "stack", ".", "enter_context", "(", "getattr", "(", "self", ",", "name", ")", ")", "return", "func", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "wrapper", "return", "_wrap"], "docstring": "like `with_objattr` but enter context one by one.", "docstring_tokens": ["like", "with_objattr", "but", "enter", "context", "one", "by", "one", "."], "sha": "1c821a120ebbbbc3c5761f5f1e8a73588059242a", "url": "https://github.com/Jasily/jasily-python/blob/1c821a120ebbbbc3c5761f5f1e8a73588059242a/jasily/lang/with_any.py#L63-L79", "partition": "test"}
{"repo": "h2non/pook", "path": "pook/mock.py", "func_name": "Mock.url", "original_string": "def url(self, url):\n        \"\"\"\n        Defines the mock URL to match.\n        It can be a full URL with path and query params.\n\n        Protocol schema is optional, defaults to ``http://``.\n\n        Arguments:\n            url (str): mock URL to match. E.g: ``server.com/api``.\n\n        Returns:\n            self: current Mock instance.\n        \"\"\"\n        self._request.url = url\n        self.add_matcher(matcher('URLMatcher', url))", "language": "python", "code": "def url(self, url):\n        \"\"\"\n        Defines the mock URL to match.\n        It can be a full URL with path and query params.\n\n        Protocol schema is optional, defaults to ``http://``.\n\n        Arguments:\n            url (str): mock URL to match. E.g: ``server.com/api``.\n\n        Returns:\n            self: current Mock instance.\n        \"\"\"\n        self._request.url = url\n        self.add_matcher(matcher('URLMatcher', url))", "code_tokens": ["def", "url", "(", "self", ",", "url", ")", ":", "self", ".", "_request", ".", "url", "=", "url", "self", ".", "add_matcher", "(", "matcher", "(", "'URLMatcher'", ",", "url", ")", ")"], "docstring": "Defines the mock URL to match.\n        It can be a full URL with path and query params.\n\n        Protocol schema is optional, defaults to ``http://``.\n\n        Arguments:\n            url (str): mock URL to match. E.g: ``server.com/api``.\n\n        Returns:\n            self: current Mock instance.", "docstring_tokens": ["Defines", "the", "mock", "URL", "to", "match", ".", "It", "can", "be", "a", "full", "URL", "with", "path", "and", "query", "params", "."], "sha": "e64094e41e4d89d98d2d29af7608ef27dc50cf19", "url": "https://github.com/h2non/pook/blob/e64094e41e4d89d98d2d29af7608ef27dc50cf19/pook/mock.py#L138-L152", "partition": "test"}
{"repo": "pltrdy/rouge", "path": "rouge/rouge_score.py", "func_name": "multi_rouge_n", "original_string": "def multi_rouge_n(sequences, scores_ids, n=2):\n    \"\"\"\n    Efficient way to compute highly repetitive scoring\n    i.e. sequences are involved multiple time\n\n    Args:\n        sequences(list[str]): list of sequences (either hyp or ref)\n        scores_ids(list[tuple(int)]): list of pairs (hyp_id, ref_id)\n            ie. scores[i] = rouge_n(scores_ids[i][0],\n                                    scores_ids[i][1])\n\n    Returns:\n        scores: list of length `len(scores_ids)` containing rouge `n`\n                scores as a dict with 'f', 'r', 'p'\n    Raises:\n        KeyError: if there's a value of i in scores_ids that is not in\n                  [0, len(sequences)[\n    \"\"\"\n    ngrams = [_get_word_ngrams(n, sequence) for sequence in sequences]\n    counts = [len(ngram) for ngram in ngrams]\n\n    scores = []\n    for hyp_id, ref_id in scores_ids:\n        evaluated_ngrams = ngrams[hyp_id]\n        evaluated_count = counts[hyp_id]\n\n        reference_ngrams = ngrams[ref_id]\n        reference_count = counts[ref_id]\n\n        overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n        overlapping_count = len(overlapping_ngrams)\n\n        scores += [f_r_p_rouge_n(evaluated_count,\n                                 reference_count, overlapping_count)]\n    return scores", "language": "python", "code": "def multi_rouge_n(sequences, scores_ids, n=2):\n    \"\"\"\n    Efficient way to compute highly repetitive scoring\n    i.e. sequences are involved multiple time\n\n    Args:\n        sequences(list[str]): list of sequences (either hyp or ref)\n        scores_ids(list[tuple(int)]): list of pairs (hyp_id, ref_id)\n            ie. scores[i] = rouge_n(scores_ids[i][0],\n                                    scores_ids[i][1])\n\n    Returns:\n        scores: list of length `len(scores_ids)` containing rouge `n`\n                scores as a dict with 'f', 'r', 'p'\n    Raises:\n        KeyError: if there's a value of i in scores_ids that is not in\n                  [0, len(sequences)[\n    \"\"\"\n    ngrams = [_get_word_ngrams(n, sequence) for sequence in sequences]\n    counts = [len(ngram) for ngram in ngrams]\n\n    scores = []\n    for hyp_id, ref_id in scores_ids:\n        evaluated_ngrams = ngrams[hyp_id]\n        evaluated_count = counts[hyp_id]\n\n        reference_ngrams = ngrams[ref_id]\n        reference_count = counts[ref_id]\n\n        overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n        overlapping_count = len(overlapping_ngrams)\n\n        scores += [f_r_p_rouge_n(evaluated_count,\n                                 reference_count, overlapping_count)]\n    return scores", "code_tokens": ["def", "multi_rouge_n", "(", "sequences", ",", "scores_ids", ",", "n", "=", "2", ")", ":", "ngrams", "=", "[", "_get_word_ngrams", "(", "n", ",", "sequence", ")", "for", "sequence", "in", "sequences", "]", "counts", "=", "[", "len", "(", "ngram", ")", "for", "ngram", "in", "ngrams", "]", "scores", "=", "[", "]", "for", "hyp_id", ",", "ref_id", "in", "scores_ids", ":", "evaluated_ngrams", "=", "ngrams", "[", "hyp_id", "]", "evaluated_count", "=", "counts", "[", "hyp_id", "]", "reference_ngrams", "=", "ngrams", "[", "ref_id", "]", "reference_count", "=", "counts", "[", "ref_id", "]", "overlapping_ngrams", "=", "evaluated_ngrams", ".", "intersection", "(", "reference_ngrams", ")", "overlapping_count", "=", "len", "(", "overlapping_ngrams", ")", "scores", "+=", "[", "f_r_p_rouge_n", "(", "evaluated_count", ",", "reference_count", ",", "overlapping_count", ")", "]", "return", "scores"], "docstring": "Efficient way to compute highly repetitive scoring\n    i.e. sequences are involved multiple time\n\n    Args:\n        sequences(list[str]): list of sequences (either hyp or ref)\n        scores_ids(list[tuple(int)]): list of pairs (hyp_id, ref_id)\n            ie. scores[i] = rouge_n(scores_ids[i][0],\n                                    scores_ids[i][1])\n\n    Returns:\n        scores: list of length `len(scores_ids)` containing rouge `n`\n                scores as a dict with 'f', 'r', 'p'\n    Raises:\n        KeyError: if there's a value of i in scores_ids that is not in\n                  [0, len(sequences)[", "docstring_tokens": ["Efficient", "way", "to", "compute", "highly", "repetitive", "scoring", "i", ".", "e", ".", "sequences", "are", "involved", "multiple", "time"], "sha": "7bf8a83af5ca5c1677b93620b4e1f85ffd63b377", "url": "https://github.com/pltrdy/rouge/blob/7bf8a83af5ca5c1677b93620b4e1f85ffd63b377/rouge/rouge_score.py#L140-L174", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/transpiler/passes/unroll_3q_or_more.py", "func_name": "Unroll3qOrMore.run", "original_string": "def run(self, dag):\n        \"\"\"Expand 3+ qubit gates using their decomposition rules.\n\n        Args:\n            dag(DAGCircuit): input dag\n        Returns:\n            DAGCircuit: output dag with maximum node degrees of 2\n        Raises:\n            QiskitError: if a 3q+ gate is not decomposable\n        \"\"\"\n        for node in dag.threeQ_or_more_gates():\n            # TODO: allow choosing other possible decompositions\n            rule = node.op.definition\n            if not rule:\n                raise QiskitError(\"Cannot unroll all 3q or more gates. \"\n                                  \"No rule to expand instruction %s.\" %\n                                  node.op.name)\n\n            # hacky way to build a dag on the same register as the rule is defined\n            # TODO: need anonymous rules to address wires by index\n            decomposition = DAGCircuit()\n            decomposition.add_qreg(rule[0][1][0][0])\n            for inst in rule:\n                decomposition.apply_operation_back(*inst)\n            decomposition = self.run(decomposition)  # recursively unroll\n            dag.substitute_node_with_dag(node, decomposition)\n        return dag", "language": "python", "code": "def run(self, dag):\n        \"\"\"Expand 3+ qubit gates using their decomposition rules.\n\n        Args:\n            dag(DAGCircuit): input dag\n        Returns:\n            DAGCircuit: output dag with maximum node degrees of 2\n        Raises:\n            QiskitError: if a 3q+ gate is not decomposable\n        \"\"\"\n        for node in dag.threeQ_or_more_gates():\n            # TODO: allow choosing other possible decompositions\n            rule = node.op.definition\n            if not rule:\n                raise QiskitError(\"Cannot unroll all 3q or more gates. \"\n                                  \"No rule to expand instruction %s.\" %\n                                  node.op.name)\n\n            # hacky way to build a dag on the same register as the rule is defined\n            # TODO: need anonymous rules to address wires by index\n            decomposition = DAGCircuit()\n            decomposition.add_qreg(rule[0][1][0][0])\n            for inst in rule:\n                decomposition.apply_operation_back(*inst)\n            decomposition = self.run(decomposition)  # recursively unroll\n            dag.substitute_node_with_dag(node, decomposition)\n        return dag", "code_tokens": ["def", "run", "(", "self", ",", "dag", ")", ":", "for", "node", "in", "dag", ".", "threeQ_or_more_gates", "(", ")", ":", "# TODO: allow choosing other possible decompositions", "rule", "=", "node", ".", "op", ".", "definition", "if", "not", "rule", ":", "raise", "QiskitError", "(", "\"Cannot unroll all 3q or more gates. \"", "\"No rule to expand instruction %s.\"", "%", "node", ".", "op", ".", "name", ")", "# hacky way to build a dag on the same register as the rule is defined", "# TODO: need anonymous rules to address wires by index", "decomposition", "=", "DAGCircuit", "(", ")", "decomposition", ".", "add_qreg", "(", "rule", "[", "0", "]", "[", "1", "]", "[", "0", "]", "[", "0", "]", ")", "for", "inst", "in", "rule", ":", "decomposition", ".", "apply_operation_back", "(", "*", "inst", ")", "decomposition", "=", "self", ".", "run", "(", "decomposition", ")", "# recursively unroll", "dag", ".", "substitute_node_with_dag", "(", "node", ",", "decomposition", ")", "return", "dag"], "docstring": "Expand 3+ qubit gates using their decomposition rules.\n\n        Args:\n            dag(DAGCircuit): input dag\n        Returns:\n            DAGCircuit: output dag with maximum node degrees of 2\n        Raises:\n            QiskitError: if a 3q+ gate is not decomposable", "docstring_tokens": ["Expand", "3", "+", "qubit", "gates", "using", "their", "decomposition", "rules", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/transpiler/passes/unroll_3q_or_more.py#L21-L47", "partition": "test"}
{"repo": "assemblerflow/flowcraft", "path": "flowcraft/templates/integrity_coverage.py", "func_name": "guess_file_compression", "original_string": "def guess_file_compression(file_path, magic_dict=None):\n    \"\"\"Guesses the compression of an input file.\n\n    This function guesses the compression of a given file by checking for\n    a binary signature at the beginning of the file. These signatures are\n    stored in the :py:data:`MAGIC_DICT` dictionary. The supported compression\n    formats are gzip, bzip2 and zip. If none of the signatures in this\n    dictionary are found at the beginning of the file, it returns ``None``.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to input file.\n    magic_dict : dict, optional\n        Dictionary containing the signatures of the compression types. The\n        key should be the binary signature and the value should be the\n        compression format. If left ``None``, it falls back to\n        :py:data:`MAGIC_DICT`.\n\n    Returns\n    -------\n    file_type : str or None\n        If a compression type is detected, returns a string with the format.\n        If not, returns ``None``.\n    \"\"\"\n\n    if not magic_dict:\n        magic_dict = MAGIC_DICT\n\n    max_len = max(len(x) for x in magic_dict)\n\n    with open(file_path, \"rb\") as f:\n        file_start = f.read(max_len)\n\n    logger.debug(\"Binary signature start: {}\".format(file_start))\n\n    for magic, file_type in magic_dict.items():\n        if file_start.startswith(magic):\n            return file_type\n\n    return None", "language": "python", "code": "def guess_file_compression(file_path, magic_dict=None):\n    \"\"\"Guesses the compression of an input file.\n\n    This function guesses the compression of a given file by checking for\n    a binary signature at the beginning of the file. These signatures are\n    stored in the :py:data:`MAGIC_DICT` dictionary. The supported compression\n    formats are gzip, bzip2 and zip. If none of the signatures in this\n    dictionary are found at the beginning of the file, it returns ``None``.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to input file.\n    magic_dict : dict, optional\n        Dictionary containing the signatures of the compression types. The\n        key should be the binary signature and the value should be the\n        compression format. If left ``None``, it falls back to\n        :py:data:`MAGIC_DICT`.\n\n    Returns\n    -------\n    file_type : str or None\n        If a compression type is detected, returns a string with the format.\n        If not, returns ``None``.\n    \"\"\"\n\n    if not magic_dict:\n        magic_dict = MAGIC_DICT\n\n    max_len = max(len(x) for x in magic_dict)\n\n    with open(file_path, \"rb\") as f:\n        file_start = f.read(max_len)\n\n    logger.debug(\"Binary signature start: {}\".format(file_start))\n\n    for magic, file_type in magic_dict.items():\n        if file_start.startswith(magic):\n            return file_type\n\n    return None", "code_tokens": ["def", "guess_file_compression", "(", "file_path", ",", "magic_dict", "=", "None", ")", ":", "if", "not", "magic_dict", ":", "magic_dict", "=", "MAGIC_DICT", "max_len", "=", "max", "(", "len", "(", "x", ")", "for", "x", "in", "magic_dict", ")", "with", "open", "(", "file_path", ",", "\"rb\"", ")", "as", "f", ":", "file_start", "=", "f", ".", "read", "(", "max_len", ")", "logger", ".", "debug", "(", "\"Binary signature start: {}\"", ".", "format", "(", "file_start", ")", ")", "for", "magic", ",", "file_type", "in", "magic_dict", ".", "items", "(", ")", ":", "if", "file_start", ".", "startswith", "(", "magic", ")", ":", "return", "file_type", "return", "None"], "docstring": "Guesses the compression of an input file.\n\n    This function guesses the compression of a given file by checking for\n    a binary signature at the beginning of the file. These signatures are\n    stored in the :py:data:`MAGIC_DICT` dictionary. The supported compression\n    formats are gzip, bzip2 and zip. If none of the signatures in this\n    dictionary are found at the beginning of the file, it returns ``None``.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to input file.\n    magic_dict : dict, optional\n        Dictionary containing the signatures of the compression types. The\n        key should be the binary signature and the value should be the\n        compression format. If left ``None``, it falls back to\n        :py:data:`MAGIC_DICT`.\n\n    Returns\n    -------\n    file_type : str or None\n        If a compression type is detected, returns a string with the format.\n        If not, returns ``None``.", "docstring_tokens": ["Guesses", "the", "compression", "of", "an", "input", "file", "."], "sha": "fc3f4bddded1efc76006600016dc71a06dd908c0", "url": "https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/templates/integrity_coverage.py#L140-L180", "partition": "test"}
{"repo": "urinieto/msaf", "path": "msaf/run.py", "func_name": "process", "original_string": "def process(in_path, annot_beats=False, feature=\"pcp\", framesync=False,\n            boundaries_id=msaf.config.default_bound_id,\n            labels_id=msaf.config.default_label_id, hier=False,\n            sonify_bounds=False, plot=False, n_jobs=4, annotator_id=0,\n            config=None, out_bounds=\"out_bounds.wav\", out_sr=22050):\n    \"\"\"Main process to segment a file or a collection of files.\n\n    Parameters\n    ----------\n    in_path: str\n        Input path. If a directory, MSAF will function in collection mode.\n        If audio file, MSAF will be in single file mode.\n    annot_beats: bool\n        Whether to use annotated beats or not.\n    feature: str\n        String representing the feature to be used (e.g. pcp, mfcc, tonnetz)\n    framesync: str\n        Whether to use framesync features or not (default: False -> beatsync)\n    boundaries_id: str\n        Identifier of the boundaries algorithm (use \"gt\" for groundtruth)\n    labels_id: str\n        Identifier of the labels algorithm (use None to not compute labels)\n    hier : bool\n        Whether to compute a hierarchical or flat segmentation.\n    sonify_bounds: bool\n        Whether to write an output audio file with the annotated boundaries\n        or not (only available in Single File Mode).\n    plot: bool\n        Whether to plot the boundaries and labels against the ground truth.\n    n_jobs: int\n        Number of processes to run in parallel. Only available in collection\n        mode.\n    annotator_id: int\n        Annotator identificator in the ground truth.\n    config: dict\n        Dictionary containing custom configuration parameters for the\n        algorithms.  If None, the default parameters are used.\n    out_bounds: str\n        Path to the output for the sonified boundaries (only in single file\n        mode, when sonify_bounds is True.\n    out_sr : int\n        Sampling rate for the sonified bounds.\n\n    Returns\n    -------\n    results : list\n        List containing tuples of (est_times, est_labels) of estimated\n        boundary times and estimated labels.\n        If labels_id is None, est_labels will be a list of -1.\n    \"\"\"\n    # Seed random to reproduce results\n    np.random.seed(123)\n\n    # Set up configuration based on algorithms parameters\n    if config is None:\n        config = io.get_configuration(feature, annot_beats, framesync,\n                                      boundaries_id, labels_id)\n        config[\"features\"] = None\n\n    # Save multi-segment (hierarchical) configuration\n    config[\"hier\"] = hier\n    if not os.path.exists(in_path):\n        raise NoAudioFileError(\"File or directory does not exists, %s\" %\n                               in_path)\n    if os.path.isfile(in_path):\n        # Single file mode\n        # Get (if they exitst) or compute features\n        file_struct = msaf.io.FileStruct(in_path)\n\n        # Use temporary file in single mode\n        file_struct.features_file = msaf.config.features_tmp_file\n\n        # Get features\n        config[\"features\"] = Features.select_features(\n            feature, file_struct, annot_beats, framesync)\n\n        # And run the algorithms\n        est_times, est_labels = run_algorithms(file_struct, boundaries_id,\n                                               labels_id, config,\n                                               annotator_id=annotator_id)\n\n        if sonify_bounds:\n            logging.info(\"Sonifying boundaries in %s...\" % out_bounds)\n            audio_hq, sr = librosa.load(in_path, sr=out_sr)\n            utils.sonify_clicks(audio_hq, est_times, out_bounds, out_sr)\n\n        if plot:\n            plotting.plot_one_track(file_struct, est_times, est_labels,\n                                    boundaries_id, labels_id)\n\n        # TODO: Only save if needed\n        # Save estimations\n        msaf.utils.ensure_dir(os.path.dirname(file_struct.est_file))\n        io.save_estimations(file_struct, est_times, est_labels,\n                            boundaries_id, labels_id, **config)\n\n        return est_times, est_labels\n    else:\n        # Collection mode\n        file_structs = io.get_dataset_files(in_path)\n\n        return Parallel(n_jobs=n_jobs)(delayed(process_track)(\n            file_struct, boundaries_id, labels_id, config,\n            annotator_id=annotator_id) for file_struct in file_structs[:])", "language": "python", "code": "def process(in_path, annot_beats=False, feature=\"pcp\", framesync=False,\n            boundaries_id=msaf.config.default_bound_id,\n            labels_id=msaf.config.default_label_id, hier=False,\n            sonify_bounds=False, plot=False, n_jobs=4, annotator_id=0,\n            config=None, out_bounds=\"out_bounds.wav\", out_sr=22050):\n    \"\"\"Main process to segment a file or a collection of files.\n\n    Parameters\n    ----------\n    in_path: str\n        Input path. If a directory, MSAF will function in collection mode.\n        If audio file, MSAF will be in single file mode.\n    annot_beats: bool\n        Whether to use annotated beats or not.\n    feature: str\n        String representing the feature to be used (e.g. pcp, mfcc, tonnetz)\n    framesync: str\n        Whether to use framesync features or not (default: False -> beatsync)\n    boundaries_id: str\n        Identifier of the boundaries algorithm (use \"gt\" for groundtruth)\n    labels_id: str\n        Identifier of the labels algorithm (use None to not compute labels)\n    hier : bool\n        Whether to compute a hierarchical or flat segmentation.\n    sonify_bounds: bool\n        Whether to write an output audio file with the annotated boundaries\n        or not (only available in Single File Mode).\n    plot: bool\n        Whether to plot the boundaries and labels against the ground truth.\n    n_jobs: int\n        Number of processes to run in parallel. Only available in collection\n        mode.\n    annotator_id: int\n        Annotator identificator in the ground truth.\n    config: dict\n        Dictionary containing custom configuration parameters for the\n        algorithms.  If None, the default parameters are used.\n    out_bounds: str\n        Path to the output for the sonified boundaries (only in single file\n        mode, when sonify_bounds is True.\n    out_sr : int\n        Sampling rate for the sonified bounds.\n\n    Returns\n    -------\n    results : list\n        List containing tuples of (est_times, est_labels) of estimated\n        boundary times and estimated labels.\n        If labels_id is None, est_labels will be a list of -1.\n    \"\"\"\n    # Seed random to reproduce results\n    np.random.seed(123)\n\n    # Set up configuration based on algorithms parameters\n    if config is None:\n        config = io.get_configuration(feature, annot_beats, framesync,\n                                      boundaries_id, labels_id)\n        config[\"features\"] = None\n\n    # Save multi-segment (hierarchical) configuration\n    config[\"hier\"] = hier\n    if not os.path.exists(in_path):\n        raise NoAudioFileError(\"File or directory does not exists, %s\" %\n                               in_path)\n    if os.path.isfile(in_path):\n        # Single file mode\n        # Get (if they exitst) or compute features\n        file_struct = msaf.io.FileStruct(in_path)\n\n        # Use temporary file in single mode\n        file_struct.features_file = msaf.config.features_tmp_file\n\n        # Get features\n        config[\"features\"] = Features.select_features(\n            feature, file_struct, annot_beats, framesync)\n\n        # And run the algorithms\n        est_times, est_labels = run_algorithms(file_struct, boundaries_id,\n                                               labels_id, config,\n                                               annotator_id=annotator_id)\n\n        if sonify_bounds:\n            logging.info(\"Sonifying boundaries in %s...\" % out_bounds)\n            audio_hq, sr = librosa.load(in_path, sr=out_sr)\n            utils.sonify_clicks(audio_hq, est_times, out_bounds, out_sr)\n\n        if plot:\n            plotting.plot_one_track(file_struct, est_times, est_labels,\n                                    boundaries_id, labels_id)\n\n        # TODO: Only save if needed\n        # Save estimations\n        msaf.utils.ensure_dir(os.path.dirname(file_struct.est_file))\n        io.save_estimations(file_struct, est_times, est_labels,\n                            boundaries_id, labels_id, **config)\n\n        return est_times, est_labels\n    else:\n        # Collection mode\n        file_structs = io.get_dataset_files(in_path)\n\n        return Parallel(n_jobs=n_jobs)(delayed(process_track)(\n            file_struct, boundaries_id, labels_id, config,\n            annotator_id=annotator_id) for file_struct in file_structs[:])", "code_tokens": ["def", "process", "(", "in_path", ",", "annot_beats", "=", "False", ",", "feature", "=", "\"pcp\"", ",", "framesync", "=", "False", ",", "boundaries_id", "=", "msaf", ".", "config", ".", "default_bound_id", ",", "labels_id", "=", "msaf", ".", "config", ".", "default_label_id", ",", "hier", "=", "False", ",", "sonify_bounds", "=", "False", ",", "plot", "=", "False", ",", "n_jobs", "=", "4", ",", "annotator_id", "=", "0", ",", "config", "=", "None", ",", "out_bounds", "=", "\"out_bounds.wav\"", ",", "out_sr", "=", "22050", ")", ":", "# Seed random to reproduce results", "np", ".", "random", ".", "seed", "(", "123", ")", "# Set up configuration based on algorithms parameters", "if", "config", "is", "None", ":", "config", "=", "io", ".", "get_configuration", "(", "feature", ",", "annot_beats", ",", "framesync", ",", "boundaries_id", ",", "labels_id", ")", "config", "[", "\"features\"", "]", "=", "None", "# Save multi-segment (hierarchical) configuration", "config", "[", "\"hier\"", "]", "=", "hier", "if", "not", "os", ".", "path", ".", "exists", "(", "in_path", ")", ":", "raise", "NoAudioFileError", "(", "\"File or directory does not exists, %s\"", "%", "in_path", ")", "if", "os", ".", "path", ".", "isfile", "(", "in_path", ")", ":", "# Single file mode", "# Get (if they exitst) or compute features", "file_struct", "=", "msaf", ".", "io", ".", "FileStruct", "(", "in_path", ")", "# Use temporary file in single mode", "file_struct", ".", "features_file", "=", "msaf", ".", "config", ".", "features_tmp_file", "# Get features", "config", "[", "\"features\"", "]", "=", "Features", ".", "select_features", "(", "feature", ",", "file_struct", ",", "annot_beats", ",", "framesync", ")", "# And run the algorithms", "est_times", ",", "est_labels", "=", "run_algorithms", "(", "file_struct", ",", "boundaries_id", ",", "labels_id", ",", "config", ",", "annotator_id", "=", "annotator_id", ")", "if", "sonify_bounds", ":", "logging", ".", "info", "(", "\"Sonifying boundaries in %s...\"", "%", "out_bounds", ")", "audio_hq", ",", "sr", "=", "librosa", ".", "load", "(", "in_path", ",", "sr", "=", "out_sr", ")", "utils", ".", "sonify_clicks", "(", "audio_hq", ",", "est_times", ",", "out_bounds", ",", "out_sr", ")", "if", "plot", ":", "plotting", ".", "plot_one_track", "(", "file_struct", ",", "est_times", ",", "est_labels", ",", "boundaries_id", ",", "labels_id", ")", "# TODO: Only save if needed", "# Save estimations", "msaf", ".", "utils", ".", "ensure_dir", "(", "os", ".", "path", ".", "dirname", "(", "file_struct", ".", "est_file", ")", ")", "io", ".", "save_estimations", "(", "file_struct", ",", "est_times", ",", "est_labels", ",", "boundaries_id", ",", "labels_id", ",", "*", "*", "config", ")", "return", "est_times", ",", "est_labels", "else", ":", "# Collection mode", "file_structs", "=", "io", ".", "get_dataset_files", "(", "in_path", ")", "return", "Parallel", "(", "n_jobs", "=", "n_jobs", ")", "(", "delayed", "(", "process_track", ")", "(", "file_struct", ",", "boundaries_id", ",", "labels_id", ",", "config", ",", "annotator_id", "=", "annotator_id", ")", "for", "file_struct", "in", "file_structs", "[", ":", "]", ")"], "docstring": "Main process to segment a file or a collection of files.\n\n    Parameters\n    ----------\n    in_path: str\n        Input path. If a directory, MSAF will function in collection mode.\n        If audio file, MSAF will be in single file mode.\n    annot_beats: bool\n        Whether to use annotated beats or not.\n    feature: str\n        String representing the feature to be used (e.g. pcp, mfcc, tonnetz)\n    framesync: str\n        Whether to use framesync features or not (default: False -> beatsync)\n    boundaries_id: str\n        Identifier of the boundaries algorithm (use \"gt\" for groundtruth)\n    labels_id: str\n        Identifier of the labels algorithm (use None to not compute labels)\n    hier : bool\n        Whether to compute a hierarchical or flat segmentation.\n    sonify_bounds: bool\n        Whether to write an output audio file with the annotated boundaries\n        or not (only available in Single File Mode).\n    plot: bool\n        Whether to plot the boundaries and labels against the ground truth.\n    n_jobs: int\n        Number of processes to run in parallel. Only available in collection\n        mode.\n    annotator_id: int\n        Annotator identificator in the ground truth.\n    config: dict\n        Dictionary containing custom configuration parameters for the\n        algorithms.  If None, the default parameters are used.\n    out_bounds: str\n        Path to the output for the sonified boundaries (only in single file\n        mode, when sonify_bounds is True.\n    out_sr : int\n        Sampling rate for the sonified bounds.\n\n    Returns\n    -------\n    results : list\n        List containing tuples of (est_times, est_labels) of estimated\n        boundary times and estimated labels.\n        If labels_id is None, est_labels will be a list of -1.", "docstring_tokens": ["Main", "process", "to", "segment", "a", "file", "or", "a", "collection", "of", "files", "."], "sha": "9dbb57d77a1310465a65cc40f1641d083ca74385", "url": "https://github.com/urinieto/msaf/blob/9dbb57d77a1310465a65cc40f1641d083ca74385/msaf/run.py#L265-L368", "partition": "test"}
{"repo": "intel-analytics/BigDL", "path": "pyspark/bigdl/transform/vision/image.py", "func_name": "LocalImageFrame.get_image", "original_string": "def get_image(self, float_key=\"floats\", to_chw=True):\n        \"\"\"\n        get image list from ImageFrame\n        \"\"\"\n        tensors = callBigDlFunc(self.bigdl_type,\n                                   \"localImageFrameToImageTensor\", self.value, float_key, to_chw)\n        return map(lambda tensor: tensor.to_ndarray(), tensors)", "language": "python", "code": "def get_image(self, float_key=\"floats\", to_chw=True):\n        \"\"\"\n        get image list from ImageFrame\n        \"\"\"\n        tensors = callBigDlFunc(self.bigdl_type,\n                                   \"localImageFrameToImageTensor\", self.value, float_key, to_chw)\n        return map(lambda tensor: tensor.to_ndarray(), tensors)", "code_tokens": ["def", "get_image", "(", "self", ",", "float_key", "=", "\"floats\"", ",", "to_chw", "=", "True", ")", ":", "tensors", "=", "callBigDlFunc", "(", "self", ".", "bigdl_type", ",", "\"localImageFrameToImageTensor\"", ",", "self", ".", "value", ",", "float_key", ",", "to_chw", ")", "return", "map", "(", "lambda", "tensor", ":", "tensor", ".", "to_ndarray", "(", ")", ",", "tensors", ")"], "docstring": "get image list from ImageFrame", "docstring_tokens": ["get", "image", "list", "from", "ImageFrame"], "sha": "e9c19788285986ab789a2e2998f9a85d7524779f", "url": "https://github.com/intel-analytics/BigDL/blob/e9c19788285986ab789a2e2998f9a85d7524779f/pyspark/bigdl/transform/vision/image.py#L226-L232", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/circuit/compositegate.py", "func_name": "CompositeGate.instruction_list", "original_string": "def instruction_list(self):\n        \"\"\"Return a list of instructions for this CompositeGate.\n\n        If the CompositeGate itself contains composites, call\n        this method recursively.\n        \"\"\"\n        instruction_list = []\n        for instruction in self.data:\n            if isinstance(instruction, CompositeGate):\n                instruction_list.extend(instruction.instruction_list())\n            else:\n                instruction_list.append(instruction)\n        return instruction_list", "language": "python", "code": "def instruction_list(self):\n        \"\"\"Return a list of instructions for this CompositeGate.\n\n        If the CompositeGate itself contains composites, call\n        this method recursively.\n        \"\"\"\n        instruction_list = []\n        for instruction in self.data:\n            if isinstance(instruction, CompositeGate):\n                instruction_list.extend(instruction.instruction_list())\n            else:\n                instruction_list.append(instruction)\n        return instruction_list", "code_tokens": ["def", "instruction_list", "(", "self", ")", ":", "instruction_list", "=", "[", "]", "for", "instruction", "in", "self", ".", "data", ":", "if", "isinstance", "(", "instruction", ",", "CompositeGate", ")", ":", "instruction_list", ".", "extend", "(", "instruction", ".", "instruction_list", "(", ")", ")", "else", ":", "instruction_list", ".", "append", "(", "instruction", ")", "return", "instruction_list"], "docstring": "Return a list of instructions for this CompositeGate.\n\n        If the CompositeGate itself contains composites, call\n        this method recursively.", "docstring_tokens": ["Return", "a", "list", "of", "instructions", "for", "this", "CompositeGate", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/circuit/compositegate.py#L34-L46", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/core/messages/payloads/derive_key.py", "func_name": "DeriveKeyRequestPayload.read", "original_string": "def read(self, input_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the DeriveKey request payload and decode it\n        into its constituent parts.\n\n        Args:\n            input_buffer (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is missing from the\n                encoded payload.\n        \"\"\"\n        super(DeriveKeyRequestPayload, self).read(\n            input_buffer,\n            kmip_version=kmip_version\n        )\n        local_buffer = utils.BytearrayStream(input_buffer.read(self.length))\n\n        if self.is_tag_next(enums.Tags.OBJECT_TYPE, local_buffer):\n            self._object_type = primitives.Enumeration(\n                enums.ObjectType,\n                tag=enums.Tags.OBJECT_TYPE\n            )\n            self._object_type.read(local_buffer, kmip_version=kmip_version)\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The DeriveKey request payload encoding is missing the object \"\n                \"type.\"\n            )\n\n        unique_identifiers = []\n        while self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_buffer):\n            unique_identifier = primitives.TextString(\n                tag=enums.Tags.UNIQUE_IDENTIFIER\n            )\n            unique_identifier.read(local_buffer, kmip_version=kmip_version)\n            unique_identifiers.append(unique_identifier)\n        if not unique_identifiers:\n            raise exceptions.InvalidKmipEncoding(\n                \"The DeriveKey request payload encoding is missing the unique \"\n                \"identifiers.\"\n            )\n        else:\n            self._unique_identifiers = unique_identifiers\n\n        if self.is_tag_next(enums.Tags.DERIVATION_METHOD, local_buffer):\n            self._derivation_method = primitives.Enumeration(\n                enums.DerivationMethod,\n                tag=enums.Tags.DERIVATION_METHOD\n            )\n            self._derivation_method.read(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The DeriveKey request payload encoding is missing the \"\n                \"derivation method.\"\n            )\n\n        if self.is_tag_next(enums.Tags.DERIVATION_PARAMETERS, local_buffer):\n            self._derivation_parameters = attributes.DerivationParameters()\n            self._derivation_parameters.read(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The DeriveKey request payload encoding is missing the \"\n                \"derivation parameters.\"\n            )\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            if self.is_tag_next(enums.Tags.TEMPLATE_ATTRIBUTE, local_buffer):\n                self._template_attribute = objects.TemplateAttribute()\n                self._template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n            else:\n                raise exceptions.InvalidKmipEncoding(\n                    \"The DeriveKey request payload encoding is missing the \"\n                    \"template attribute.\"\n                )\n        else:\n            if self.is_tag_next(enums.Tags.ATTRIBUTES, local_buffer):\n                attrs = objects.Attributes()\n                attrs.read(local_buffer, kmip_version=kmip_version)\n                value = objects.convert_attributes_to_template_attribute(\n                    attrs\n                )\n                self._template_attribute = value\n            else:\n                raise exceptions.InvalidKmipEncoding(\n                    \"The DeriveKey request payload encoding is missing the \"\n                    \"attributes structure.\"\n                )\n\n        self.is_oversized(local_buffer)", "language": "python", "code": "def read(self, input_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the DeriveKey request payload and decode it\n        into its constituent parts.\n\n        Args:\n            input_buffer (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is missing from the\n                encoded payload.\n        \"\"\"\n        super(DeriveKeyRequestPayload, self).read(\n            input_buffer,\n            kmip_version=kmip_version\n        )\n        local_buffer = utils.BytearrayStream(input_buffer.read(self.length))\n\n        if self.is_tag_next(enums.Tags.OBJECT_TYPE, local_buffer):\n            self._object_type = primitives.Enumeration(\n                enums.ObjectType,\n                tag=enums.Tags.OBJECT_TYPE\n            )\n            self._object_type.read(local_buffer, kmip_version=kmip_version)\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The DeriveKey request payload encoding is missing the object \"\n                \"type.\"\n            )\n\n        unique_identifiers = []\n        while self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_buffer):\n            unique_identifier = primitives.TextString(\n                tag=enums.Tags.UNIQUE_IDENTIFIER\n            )\n            unique_identifier.read(local_buffer, kmip_version=kmip_version)\n            unique_identifiers.append(unique_identifier)\n        if not unique_identifiers:\n            raise exceptions.InvalidKmipEncoding(\n                \"The DeriveKey request payload encoding is missing the unique \"\n                \"identifiers.\"\n            )\n        else:\n            self._unique_identifiers = unique_identifiers\n\n        if self.is_tag_next(enums.Tags.DERIVATION_METHOD, local_buffer):\n            self._derivation_method = primitives.Enumeration(\n                enums.DerivationMethod,\n                tag=enums.Tags.DERIVATION_METHOD\n            )\n            self._derivation_method.read(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The DeriveKey request payload encoding is missing the \"\n                \"derivation method.\"\n            )\n\n        if self.is_tag_next(enums.Tags.DERIVATION_PARAMETERS, local_buffer):\n            self._derivation_parameters = attributes.DerivationParameters()\n            self._derivation_parameters.read(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The DeriveKey request payload encoding is missing the \"\n                \"derivation parameters.\"\n            )\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            if self.is_tag_next(enums.Tags.TEMPLATE_ATTRIBUTE, local_buffer):\n                self._template_attribute = objects.TemplateAttribute()\n                self._template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n            else:\n                raise exceptions.InvalidKmipEncoding(\n                    \"The DeriveKey request payload encoding is missing the \"\n                    \"template attribute.\"\n                )\n        else:\n            if self.is_tag_next(enums.Tags.ATTRIBUTES, local_buffer):\n                attrs = objects.Attributes()\n                attrs.read(local_buffer, kmip_version=kmip_version)\n                value = objects.convert_attributes_to_template_attribute(\n                    attrs\n                )\n                self._template_attribute = value\n            else:\n                raise exceptions.InvalidKmipEncoding(\n                    \"The DeriveKey request payload encoding is missing the \"\n                    \"attributes structure.\"\n                )\n\n        self.is_oversized(local_buffer)", "code_tokens": ["def", "read", "(", "self", ",", "input_buffer", ",", "kmip_version", "=", "enums", ".", "KMIPVersion", ".", "KMIP_1_0", ")", ":", "super", "(", "DeriveKeyRequestPayload", ",", "self", ")", ".", "read", "(", "input_buffer", ",", "kmip_version", "=", "kmip_version", ")", "local_buffer", "=", "utils", ".", "BytearrayStream", "(", "input_buffer", ".", "read", "(", "self", ".", "length", ")", ")", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "OBJECT_TYPE", ",", "local_buffer", ")", ":", "self", ".", "_object_type", "=", "primitives", ".", "Enumeration", "(", "enums", ".", "ObjectType", ",", "tag", "=", "enums", ".", "Tags", ".", "OBJECT_TYPE", ")", "self", ".", "_object_type", ".", "read", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "else", ":", "raise", "exceptions", ".", "InvalidKmipEncoding", "(", "\"The DeriveKey request payload encoding is missing the object \"", "\"type.\"", ")", "unique_identifiers", "=", "[", "]", "while", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "UNIQUE_IDENTIFIER", ",", "local_buffer", ")", ":", "unique_identifier", "=", "primitives", ".", "TextString", "(", "tag", "=", "enums", ".", "Tags", ".", "UNIQUE_IDENTIFIER", ")", "unique_identifier", ".", "read", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "unique_identifiers", ".", "append", "(", "unique_identifier", ")", "if", "not", "unique_identifiers", ":", "raise", "exceptions", ".", "InvalidKmipEncoding", "(", "\"The DeriveKey request payload encoding is missing the unique \"", "\"identifiers.\"", ")", "else", ":", "self", ".", "_unique_identifiers", "=", "unique_identifiers", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "DERIVATION_METHOD", ",", "local_buffer", ")", ":", "self", ".", "_derivation_method", "=", "primitives", ".", "Enumeration", "(", "enums", ".", "DerivationMethod", ",", "tag", "=", "enums", ".", "Tags", ".", "DERIVATION_METHOD", ")", "self", ".", "_derivation_method", ".", "read", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "else", ":", "raise", "exceptions", ".", "InvalidKmipEncoding", "(", "\"The DeriveKey request payload encoding is missing the \"", "\"derivation method.\"", ")", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "DERIVATION_PARAMETERS", ",", "local_buffer", ")", ":", "self", ".", "_derivation_parameters", "=", "attributes", ".", "DerivationParameters", "(", ")", "self", ".", "_derivation_parameters", ".", "read", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "else", ":", "raise", "exceptions", ".", "InvalidKmipEncoding", "(", "\"The DeriveKey request payload encoding is missing the \"", "\"derivation parameters.\"", ")", "if", "kmip_version", "<", "enums", ".", "KMIPVersion", ".", "KMIP_2_0", ":", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "TEMPLATE_ATTRIBUTE", ",", "local_buffer", ")", ":", "self", ".", "_template_attribute", "=", "objects", ".", "TemplateAttribute", "(", ")", "self", ".", "_template_attribute", ".", "read", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "else", ":", "raise", "exceptions", ".", "InvalidKmipEncoding", "(", "\"The DeriveKey request payload encoding is missing the \"", "\"template attribute.\"", ")", "else", ":", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "ATTRIBUTES", ",", "local_buffer", ")", ":", "attrs", "=", "objects", ".", "Attributes", "(", ")", "attrs", ".", "read", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "value", "=", "objects", ".", "convert_attributes_to_template_attribute", "(", "attrs", ")", "self", ".", "_template_attribute", "=", "value", "else", ":", "raise", "exceptions", ".", "InvalidKmipEncoding", "(", "\"The DeriveKey request payload encoding is missing the \"", "\"attributes structure.\"", ")", "self", ".", "is_oversized", "(", "local_buffer", ")"], "docstring": "Read the data encoding the DeriveKey request payload and decode it\n        into its constituent parts.\n\n        Args:\n            input_buffer (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is missing from the\n                encoded payload.", "docstring_tokens": ["Read", "the", "data", "encoding", "the", "DeriveKey", "request", "payload", "and", "decode", "it", "into", "its", "constituent", "parts", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/core/messages/payloads/derive_key.py#L198-L301", "partition": "test"}
{"repo": "neherlab/treetime", "path": "treetime/clock_tree.py", "func_name": "ClockTree.date_uncertainty_due_to_rate", "original_string": "def date_uncertainty_due_to_rate(self, node, interval=(0.05, 0.095)):\n        \"\"\"use previously calculated variation of the rate to estimate\n        the uncertainty in a particular numdate due to rate variation.\n\n        Parameters\n        ----------\n        node : PhyloTree.Clade\n            node for which the confidence interval is to be calculated\n        interval : tuple, optional\n            Array of length two, or tuple, defining the bounds of the confidence interval\n\n        \"\"\"\n        if hasattr(node, \"numdate_rate_variation\"):\n            from scipy.special import erfinv\n            nsig = [np.sqrt(2.0)*erfinv(-1.0 + 2.0*x) if x*(1.0-x) else 0\n                    for x in interval]\n            l,c,u = [x[1] for x in node.numdate_rate_variation]\n            return np.array([c + x*np.abs(y-c) for x,y in zip(nsig, (l,u))])\n\n        else:\n            return None", "language": "python", "code": "def date_uncertainty_due_to_rate(self, node, interval=(0.05, 0.095)):\n        \"\"\"use previously calculated variation of the rate to estimate\n        the uncertainty in a particular numdate due to rate variation.\n\n        Parameters\n        ----------\n        node : PhyloTree.Clade\n            node for which the confidence interval is to be calculated\n        interval : tuple, optional\n            Array of length two, or tuple, defining the bounds of the confidence interval\n\n        \"\"\"\n        if hasattr(node, \"numdate_rate_variation\"):\n            from scipy.special import erfinv\n            nsig = [np.sqrt(2.0)*erfinv(-1.0 + 2.0*x) if x*(1.0-x) else 0\n                    for x in interval]\n            l,c,u = [x[1] for x in node.numdate_rate_variation]\n            return np.array([c + x*np.abs(y-c) for x,y in zip(nsig, (l,u))])\n\n        else:\n            return None", "code_tokens": ["def", "date_uncertainty_due_to_rate", "(", "self", ",", "node", ",", "interval", "=", "(", "0.05", ",", "0.095", ")", ")", ":", "if", "hasattr", "(", "node", ",", "\"numdate_rate_variation\"", ")", ":", "from", "scipy", ".", "special", "import", "erfinv", "nsig", "=", "[", "np", ".", "sqrt", "(", "2.0", ")", "*", "erfinv", "(", "-", "1.0", "+", "2.0", "*", "x", ")", "if", "x", "*", "(", "1.0", "-", "x", ")", "else", "0", "for", "x", "in", "interval", "]", "l", ",", "c", ",", "u", "=", "[", "x", "[", "1", "]", "for", "x", "in", "node", ".", "numdate_rate_variation", "]", "return", "np", ".", "array", "(", "[", "c", "+", "x", "*", "np", ".", "abs", "(", "y", "-", "c", ")", "for", "x", ",", "y", "in", "zip", "(", "nsig", ",", "(", "l", ",", "u", ")", ")", "]", ")", "else", ":", "return", "None"], "docstring": "use previously calculated variation of the rate to estimate\n        the uncertainty in a particular numdate due to rate variation.\n\n        Parameters\n        ----------\n        node : PhyloTree.Clade\n            node for which the confidence interval is to be calculated\n        interval : tuple, optional\n            Array of length two, or tuple, defining the bounds of the confidence interval", "docstring_tokens": ["use", "previously", "calculated", "variation", "of", "the", "rate", "to", "estimate", "the", "uncertainty", "in", "a", "particular", "numdate", "due", "to", "rate", "variation", "."], "sha": "f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0", "url": "https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/clock_tree.py#L760-L780", "partition": "test"}
{"repo": "mental32/spotify.py", "path": "spotify/models/artist.py", "func_name": "Artist.related_artists", "original_string": "async def related_artists(self) -> List[Artist]:\n        \"\"\"Get Spotify catalog information about artists similar to a given artist.\n\n        Similarity is based on analysis of the Spotify community\u2019s listening history.\n\n        Returns\n        -------\n        artists : List[Artits]\n            The artists deemed similar.\n        \"\"\"\n        related = await self.__client.http.artist_related_artists(self.id)\n        return list(Artist(self.__client, item) for item in related['artists'])", "language": "python", "code": "async def related_artists(self) -> List[Artist]:\n        \"\"\"Get Spotify catalog information about artists similar to a given artist.\n\n        Similarity is based on analysis of the Spotify community\u2019s listening history.\n\n        Returns\n        -------\n        artists : List[Artits]\n            The artists deemed similar.\n        \"\"\"\n        related = await self.__client.http.artist_related_artists(self.id)\n        return list(Artist(self.__client, item) for item in related['artists'])", "code_tokens": ["async", "def", "related_artists", "(", "self", ")", "->", "List", "[", "Artist", "]", ":", "related", "=", "await", "self", ".", "__client", ".", "http", ".", "artist_related_artists", "(", "self", ".", "id", ")", "return", "list", "(", "Artist", "(", "self", ".", "__client", ",", "item", ")", "for", "item", "in", "related", "[", "'artists'", "]", ")"], "docstring": "Get Spotify catalog information about artists similar to a given artist.\n\n        Similarity is based on analysis of the Spotify community\u2019s listening history.\n\n        Returns\n        -------\n        artists : List[Artits]\n            The artists deemed similar.", "docstring_tokens": ["Get", "Spotify", "catalog", "information", "about", "artists", "similar", "to", "a", "given", "artist", "."], "sha": "bb296cac7c3dd289908906b7069bd80f43950515", "url": "https://github.com/mental32/spotify.py/blob/bb296cac7c3dd289908906b7069bd80f43950515/spotify/models/artist.py#L137-L148", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/synthesizer/interfaceLevel/propDeclrCollector.py", "func_name": "PropDeclrCollector._updateParamsFrom", "original_string": "def _updateParamsFrom(self, otherObj:\"PropDeclrCollector\", updater, exclude:set, prefix:str) -> None:\n        \"\"\"\n        Update all parameters which are defined on self from otherObj\n\n        :param otherObj: other object which Param instances should be updated\n        :param updater: updater function(self, myParameter, onOtherParameterName, otherParameter)\n        :param exclude: iterable of parameter on otherObj object which should be excluded\n        :param prefix: prefix which should be added to name of paramters of this object before matching\n            parameter name on parent\n        \"\"\"\n        excluded = set()\n        if exclude is not None:\n            exclude = set(exclude)\n\n        for myP in self._params:\n            pPName = prefix + myP._scopes[self][1]\n            try:\n                otherP = getattr(otherObj, pPName)\n                if not isinstance(otherP, Param):\n                    continue\n            except AttributeError:\n                continue\n\n            if exclude and otherP in exclude:\n                excluded.add(otherP)\n                continue\n            updater(self, myP, otherP)\n        \n        if exclude is not None:\n            # assert that what should be excluded really exists\n            assert excluded == exclude", "language": "python", "code": "def _updateParamsFrom(self, otherObj:\"PropDeclrCollector\", updater, exclude:set, prefix:str) -> None:\n        \"\"\"\n        Update all parameters which are defined on self from otherObj\n\n        :param otherObj: other object which Param instances should be updated\n        :param updater: updater function(self, myParameter, onOtherParameterName, otherParameter)\n        :param exclude: iterable of parameter on otherObj object which should be excluded\n        :param prefix: prefix which should be added to name of paramters of this object before matching\n            parameter name on parent\n        \"\"\"\n        excluded = set()\n        if exclude is not None:\n            exclude = set(exclude)\n\n        for myP in self._params:\n            pPName = prefix + myP._scopes[self][1]\n            try:\n                otherP = getattr(otherObj, pPName)\n                if not isinstance(otherP, Param):\n                    continue\n            except AttributeError:\n                continue\n\n            if exclude and otherP in exclude:\n                excluded.add(otherP)\n                continue\n            updater(self, myP, otherP)\n        \n        if exclude is not None:\n            # assert that what should be excluded really exists\n            assert excluded == exclude", "code_tokens": ["def", "_updateParamsFrom", "(", "self", ",", "otherObj", ":", "\"PropDeclrCollector\"", ",", "updater", ",", "exclude", ":", "set", ",", "prefix", ":", "str", ")", "->", "None", ":", "excluded", "=", "set", "(", ")", "if", "exclude", "is", "not", "None", ":", "exclude", "=", "set", "(", "exclude", ")", "for", "myP", "in", "self", ".", "_params", ":", "pPName", "=", "prefix", "+", "myP", ".", "_scopes", "[", "self", "]", "[", "1", "]", "try", ":", "otherP", "=", "getattr", "(", "otherObj", ",", "pPName", ")", "if", "not", "isinstance", "(", "otherP", ",", "Param", ")", ":", "continue", "except", "AttributeError", ":", "continue", "if", "exclude", "and", "otherP", "in", "exclude", ":", "excluded", ".", "add", "(", "otherP", ")", "continue", "updater", "(", "self", ",", "myP", ",", "otherP", ")", "if", "exclude", "is", "not", "None", ":", "# assert that what should be excluded really exists", "assert", "excluded", "==", "exclude"], "docstring": "Update all parameters which are defined on self from otherObj\n\n        :param otherObj: other object which Param instances should be updated\n        :param updater: updater function(self, myParameter, onOtherParameterName, otherParameter)\n        :param exclude: iterable of parameter on otherObj object which should be excluded\n        :param prefix: prefix which should be added to name of paramters of this object before matching\n            parameter name on parent", "docstring_tokens": ["Update", "all", "parameters", "which", "are", "defined", "on", "self", "from", "otherObj"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/synthesizer/interfaceLevel/propDeclrCollector.py#L213-L243", "partition": "test"}
{"repo": "oscarbranson/latools", "path": "latools/filtering/signal_optimiser.py", "func_name": "bayes_scale", "original_string": "def bayes_scale(s):\n    \"\"\"\n    Remove mean and divide by standard deviation, using bayes_kvm statistics.\n    \"\"\"\n    if sum(~np.isnan(s)) > 1:\n        bm, bv, bs = bayes_mvs(s[~np.isnan(s)])\n        return (s - bm.statistic) / bs.statistic\n    else:\n        return np.full(s.shape, np.nan)", "language": "python", "code": "def bayes_scale(s):\n    \"\"\"\n    Remove mean and divide by standard deviation, using bayes_kvm statistics.\n    \"\"\"\n    if sum(~np.isnan(s)) > 1:\n        bm, bv, bs = bayes_mvs(s[~np.isnan(s)])\n        return (s - bm.statistic) / bs.statistic\n    else:\n        return np.full(s.shape, np.nan)", "code_tokens": ["def", "bayes_scale", "(", "s", ")", ":", "if", "sum", "(", "~", "np", ".", "isnan", "(", "s", ")", ")", ">", "1", ":", "bm", ",", "bv", ",", "bs", "=", "bayes_mvs", "(", "s", "[", "~", "np", ".", "isnan", "(", "s", ")", "]", ")", "return", "(", "s", "-", "bm", ".", "statistic", ")", "/", "bs", ".", "statistic", "else", ":", "return", "np", ".", "full", "(", "s", ".", "shape", ",", "np", ".", "nan", ")"], "docstring": "Remove mean and divide by standard deviation, using bayes_kvm statistics.", "docstring_tokens": ["Remove", "mean", "and", "divide", "by", "standard", "deviation", "using", "bayes_kvm", "statistics", "."], "sha": "cd25a650cfee318152f234d992708511f7047fbe", "url": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/filtering/signal_optimiser.py#L65-L73", "partition": "test"}
{"repo": "wreckage/django-happenings", "path": "happenings/managers.py", "func_name": "EventManager.live", "original_string": "def live(self, now):\n        \"\"\"\n        Returns a queryset of events that will occur again after 'now'.\n        Used to help generate a list of upcoming events.\n        \"\"\"\n        return self.model.objects.filter(\n            Q(end_repeat=None) | Q(end_repeat__gte=now) |\n            Q(start_date__gte=now) | Q(end_date__gte=now)\n        ).exclude(  # exclude single day events that won't occur again\n            start_date__lt=now, end_date__lt=now,\n            repeat=\"NEVER\", end_repeat=None,\n        ).prefetch_related('cancellations')", "language": "python", "code": "def live(self, now):\n        \"\"\"\n        Returns a queryset of events that will occur again after 'now'.\n        Used to help generate a list of upcoming events.\n        \"\"\"\n        return self.model.objects.filter(\n            Q(end_repeat=None) | Q(end_repeat__gte=now) |\n            Q(start_date__gte=now) | Q(end_date__gte=now)\n        ).exclude(  # exclude single day events that won't occur again\n            start_date__lt=now, end_date__lt=now,\n            repeat=\"NEVER\", end_repeat=None,\n        ).prefetch_related('cancellations')", "code_tokens": ["def", "live", "(", "self", ",", "now", ")", ":", "return", "self", ".", "model", ".", "objects", ".", "filter", "(", "Q", "(", "end_repeat", "=", "None", ")", "|", "Q", "(", "end_repeat__gte", "=", "now", ")", "|", "Q", "(", "start_date__gte", "=", "now", ")", "|", "Q", "(", "end_date__gte", "=", "now", ")", ")", ".", "exclude", "(", "# exclude single day events that won't occur again", "start_date__lt", "=", "now", ",", "end_date__lt", "=", "now", ",", "repeat", "=", "\"NEVER\"", ",", "end_repeat", "=", "None", ",", ")", ".", "prefetch_related", "(", "'cancellations'", ")"], "docstring": "Returns a queryset of events that will occur again after 'now'.\n        Used to help generate a list of upcoming events.", "docstring_tokens": ["Returns", "a", "queryset", "of", "events", "that", "will", "occur", "again", "after", "now", ".", "Used", "to", "help", "generate", "a", "list", "of", "upcoming", "events", "."], "sha": "7bca5576efa6cd4c4e87356bf9e5b8cd538ae91d", "url": "https://github.com/wreckage/django-happenings/blob/7bca5576efa6cd4c4e87356bf9e5b8cd538ae91d/happenings/managers.py#L88-L99", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/pip/wheel.py", "func_name": "uninstallation_paths", "original_string": "def uninstallation_paths(dist):\n    \"\"\"\n    Yield all the uninstallation paths for dist based on RECORD-without-.pyc\n\n    Yield paths to all the files in RECORD. For each .py file in RECORD, add\n    the .pyc in the same directory.\n\n    UninstallPathSet.add() takes care of the __pycache__ .pyc.\n    \"\"\"\n    from pip.utils import FakeFile  # circular import\n    r = csv.reader(FakeFile(dist.get_metadata_lines('RECORD')))\n    for row in r:\n        path = os.path.join(dist.location, row[0])\n        yield path\n        if path.endswith('.py'):\n            dn, fn = os.path.split(path)\n            base = fn[:-3]\n            path = os.path.join(dn, base + '.pyc')\n            yield path", "language": "python", "code": "def uninstallation_paths(dist):\n    \"\"\"\n    Yield all the uninstallation paths for dist based on RECORD-without-.pyc\n\n    Yield paths to all the files in RECORD. For each .py file in RECORD, add\n    the .pyc in the same directory.\n\n    UninstallPathSet.add() takes care of the __pycache__ .pyc.\n    \"\"\"\n    from pip.utils import FakeFile  # circular import\n    r = csv.reader(FakeFile(dist.get_metadata_lines('RECORD')))\n    for row in r:\n        path = os.path.join(dist.location, row[0])\n        yield path\n        if path.endswith('.py'):\n            dn, fn = os.path.split(path)\n            base = fn[:-3]\n            path = os.path.join(dn, base + '.pyc')\n            yield path", "code_tokens": ["def", "uninstallation_paths", "(", "dist", ")", ":", "from", "pip", ".", "utils", "import", "FakeFile", "# circular import", "r", "=", "csv", ".", "reader", "(", "FakeFile", "(", "dist", ".", "get_metadata_lines", "(", "'RECORD'", ")", ")", ")", "for", "row", "in", "r", ":", "path", "=", "os", ".", "path", ".", "join", "(", "dist", ".", "location", ",", "row", "[", "0", "]", ")", "yield", "path", "if", "path", ".", "endswith", "(", "'.py'", ")", ":", "dn", ",", "fn", "=", "os", ".", "path", ".", "split", "(", "path", ")", "base", "=", "fn", "[", ":", "-", "3", "]", "path", "=", "os", ".", "path", ".", "join", "(", "dn", ",", "base", "+", "'.pyc'", ")", "yield", "path"], "docstring": "Yield all the uninstallation paths for dist based on RECORD-without-.pyc\n\n    Yield paths to all the files in RECORD. For each .py file in RECORD, add\n    the .pyc in the same directory.\n\n    UninstallPathSet.add() takes care of the __pycache__ .pyc.", "docstring_tokens": ["Yield", "all", "the", "uninstallation", "paths", "for", "dist", "based", "on", "RECORD", "-", "without", "-", ".", "pyc"], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/pip/wheel.py#L521-L539", "partition": "test"}
{"repo": "getgauge/gauge-python", "path": "getgauge/parser_redbaron.py", "func_name": "RedbaronPythonFile._iter_step_func_decorators", "original_string": "def _iter_step_func_decorators(self):\n        \"\"\"Find functions with step decorator in parsed file.\"\"\"  \n        for node in self.py_tree.find_all('def'):\n            for decorator in node.decorators:\n                if decorator.name.value == 'step':\n                    yield node, decorator\n                    break", "language": "python", "code": "def _iter_step_func_decorators(self):\n        \"\"\"Find functions with step decorator in parsed file.\"\"\"  \n        for node in self.py_tree.find_all('def'):\n            for decorator in node.decorators:\n                if decorator.name.value == 'step':\n                    yield node, decorator\n                    break", "code_tokens": ["def", "_iter_step_func_decorators", "(", "self", ")", ":", "for", "node", "in", "self", ".", "py_tree", ".", "find_all", "(", "'def'", ")", ":", "for", "decorator", "in", "node", ".", "decorators", ":", "if", "decorator", ".", "name", ".", "value", "==", "'step'", ":", "yield", "node", ",", "decorator", "break"], "docstring": "Find functions with step decorator in parsed file.", "docstring_tokens": ["Find", "functions", "with", "step", "decorator", "in", "parsed", "file", "."], "sha": "90f3547dcfd2d16d51f116cdd4e53527eeab1a57", "url": "https://github.com/getgauge/gauge-python/blob/90f3547dcfd2d16d51f116cdd4e53527eeab1a57/getgauge/parser_redbaron.py#L54-L60", "partition": "test"}
{"repo": "streamlink/streamlink", "path": "src/streamlink/stream/dash_manifest.py", "func_name": "SegmentTemplate.segment_numbers", "original_string": "def segment_numbers(self):\n        \"\"\"\n        yield the segment number and when it will be available\n        There are two cases for segment number generation, static and dynamic.\n\n        In the case of static stream, the segment number starts at the startNumber and counts\n        up to the number of segments that are represented by the periods duration.\n\n        In the case of dynamic streams, the segments should appear at the specified time\n        in the simplest case the segment number is based on the time since the availabilityStartTime\n        :return:\n        \"\"\"\n        log.debug(\"Generating segment numbers for {0} playlist (id={1})\".format(self.root.type, self.parent.id))\n        if self.root.type == u\"static\":\n            available_iter = repeat(epoch_start)\n            duration = self.period.duration.seconds or self.root.mediaPresentationDuration.seconds\n            if duration:\n                number_iter = range(self.startNumber, int(duration / self.duration_seconds) + 1)\n            else:\n                number_iter = count(self.startNumber)\n        else:\n            now = datetime.datetime.now(utc)\n            if self.presentationTimeOffset:\n                since_start = (now - self.presentationTimeOffset) - self.root.availabilityStartTime\n                available_start_date = self.root.availabilityStartTime + self.presentationTimeOffset + since_start\n                available_start = available_start_date\n            else:\n                since_start = now - self.root.availabilityStartTime\n                available_start = now\n\n            # if there is no delay, use a delay of 3 seconds\n            suggested_delay = datetime.timedelta(seconds=(self.root.suggestedPresentationDelay.total_seconds()\n                                                          if self.root.suggestedPresentationDelay\n                                                          else 3))\n\n            # the number of the segment that is available at NOW - SUGGESTED_DELAY - BUFFER_TIME\n            number_iter = count(self.startNumber +\n                                int((since_start - suggested_delay - self.root.minBufferTime).total_seconds() /\n                                    self.duration_seconds))\n\n            # the time the segment number is available at NOW\n            available_iter = count_dt(available_start,\n                                      step=datetime.timedelta(seconds=self.duration_seconds))\n\n        for number, available_at in izip(number_iter, available_iter):\n            yield number, available_at", "language": "python", "code": "def segment_numbers(self):\n        \"\"\"\n        yield the segment number and when it will be available\n        There are two cases for segment number generation, static and dynamic.\n\n        In the case of static stream, the segment number starts at the startNumber and counts\n        up to the number of segments that are represented by the periods duration.\n\n        In the case of dynamic streams, the segments should appear at the specified time\n        in the simplest case the segment number is based on the time since the availabilityStartTime\n        :return:\n        \"\"\"\n        log.debug(\"Generating segment numbers for {0} playlist (id={1})\".format(self.root.type, self.parent.id))\n        if self.root.type == u\"static\":\n            available_iter = repeat(epoch_start)\n            duration = self.period.duration.seconds or self.root.mediaPresentationDuration.seconds\n            if duration:\n                number_iter = range(self.startNumber, int(duration / self.duration_seconds) + 1)\n            else:\n                number_iter = count(self.startNumber)\n        else:\n            now = datetime.datetime.now(utc)\n            if self.presentationTimeOffset:\n                since_start = (now - self.presentationTimeOffset) - self.root.availabilityStartTime\n                available_start_date = self.root.availabilityStartTime + self.presentationTimeOffset + since_start\n                available_start = available_start_date\n            else:\n                since_start = now - self.root.availabilityStartTime\n                available_start = now\n\n            # if there is no delay, use a delay of 3 seconds\n            suggested_delay = datetime.timedelta(seconds=(self.root.suggestedPresentationDelay.total_seconds()\n                                                          if self.root.suggestedPresentationDelay\n                                                          else 3))\n\n            # the number of the segment that is available at NOW - SUGGESTED_DELAY - BUFFER_TIME\n            number_iter = count(self.startNumber +\n                                int((since_start - suggested_delay - self.root.minBufferTime).total_seconds() /\n                                    self.duration_seconds))\n\n            # the time the segment number is available at NOW\n            available_iter = count_dt(available_start,\n                                      step=datetime.timedelta(seconds=self.duration_seconds))\n\n        for number, available_at in izip(number_iter, available_iter):\n            yield number, available_at", "code_tokens": ["def", "segment_numbers", "(", "self", ")", ":", "log", ".", "debug", "(", "\"Generating segment numbers for {0} playlist (id={1})\"", ".", "format", "(", "self", ".", "root", ".", "type", ",", "self", ".", "parent", ".", "id", ")", ")", "if", "self", ".", "root", ".", "type", "==", "u\"static\"", ":", "available_iter", "=", "repeat", "(", "epoch_start", ")", "duration", "=", "self", ".", "period", ".", "duration", ".", "seconds", "or", "self", ".", "root", ".", "mediaPresentationDuration", ".", "seconds", "if", "duration", ":", "number_iter", "=", "range", "(", "self", ".", "startNumber", ",", "int", "(", "duration", "/", "self", ".", "duration_seconds", ")", "+", "1", ")", "else", ":", "number_iter", "=", "count", "(", "self", ".", "startNumber", ")", "else", ":", "now", "=", "datetime", ".", "datetime", ".", "now", "(", "utc", ")", "if", "self", ".", "presentationTimeOffset", ":", "since_start", "=", "(", "now", "-", "self", ".", "presentationTimeOffset", ")", "-", "self", ".", "root", ".", "availabilityStartTime", "available_start_date", "=", "self", ".", "root", ".", "availabilityStartTime", "+", "self", ".", "presentationTimeOffset", "+", "since_start", "available_start", "=", "available_start_date", "else", ":", "since_start", "=", "now", "-", "self", ".", "root", ".", "availabilityStartTime", "available_start", "=", "now", "# if there is no delay, use a delay of 3 seconds", "suggested_delay", "=", "datetime", ".", "timedelta", "(", "seconds", "=", "(", "self", ".", "root", ".", "suggestedPresentationDelay", ".", "total_seconds", "(", ")", "if", "self", ".", "root", ".", "suggestedPresentationDelay", "else", "3", ")", ")", "# the number of the segment that is available at NOW - SUGGESTED_DELAY - BUFFER_TIME", "number_iter", "=", "count", "(", "self", ".", "startNumber", "+", "int", "(", "(", "since_start", "-", "suggested_delay", "-", "self", ".", "root", ".", "minBufferTime", ")", ".", "total_seconds", "(", ")", "/", "self", ".", "duration_seconds", ")", ")", "# the time the segment number is available at NOW", "available_iter", "=", "count_dt", "(", "available_start", ",", "step", "=", "datetime", ".", "timedelta", "(", "seconds", "=", "self", ".", "duration_seconds", ")", ")", "for", "number", ",", "available_at", "in", "izip", "(", "number_iter", ",", "available_iter", ")", ":", "yield", "number", ",", "available_at"], "docstring": "yield the segment number and when it will be available\n        There are two cases for segment number generation, static and dynamic.\n\n        In the case of static stream, the segment number starts at the startNumber and counts\n        up to the number of segments that are represented by the periods duration.\n\n        In the case of dynamic streams, the segments should appear at the specified time\n        in the simplest case the segment number is based on the time since the availabilityStartTime\n        :return:", "docstring_tokens": ["yield", "the", "segment", "number", "and", "when", "it", "will", "be", "available", "There", "are", "two", "cases", "for", "segment", "number", "generation", "static", "and", "dynamic", "."], "sha": "c8ed1daff14ac03195870238b9b900c1109dd5c1", "url": "https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink/stream/dash_manifest.py#L463-L508", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/bijector.py", "func_name": "_Mapping.merge", "original_string": "def merge(self, x=None, y=None, ildj=None, kwargs=None, mapping=None):\n    \"\"\"Returns new _Mapping with args merged with self.\n\n    Args:\n      x: `Tensor` or None. Input to forward; output of inverse.\n      y: `Tensor` or None. Input to inverse; output of forward.\n      ildj: `Tensor`. This is the (un-reduce_sum'ed) inverse log det jacobian.\n      kwargs: Python dictionary. Extra args supplied to forward/inverse/etc\n        functions.\n      mapping: Instance of _Mapping to merge. Can only be specified if no other\n        arg is specified.\n\n    Returns:\n      mapping: New instance of `_Mapping` which has inputs merged with self.\n\n    Raises:\n      ValueError: if mapping and any other arg is not `None`.\n    \"\"\"\n    if mapping is None:\n      mapping = _Mapping(x=x, y=y, ildj=ildj, kwargs=kwargs)\n    elif any(arg is not None for arg in [x, y, ildj, kwargs]):\n      raise ValueError(\"Cannot simultaneously specify mapping and individual \"\n                       \"arguments.\")\n\n    return _Mapping(\n        x=self._merge(self.x, mapping.x),\n        y=self._merge(self.y, mapping.y),\n        ildj=self._merge(self.ildj, mapping.ildj),\n        kwargs=self._merge(self.kwargs, mapping.kwargs, use_equals=True))", "language": "python", "code": "def merge(self, x=None, y=None, ildj=None, kwargs=None, mapping=None):\n    \"\"\"Returns new _Mapping with args merged with self.\n\n    Args:\n      x: `Tensor` or None. Input to forward; output of inverse.\n      y: `Tensor` or None. Input to inverse; output of forward.\n      ildj: `Tensor`. This is the (un-reduce_sum'ed) inverse log det jacobian.\n      kwargs: Python dictionary. Extra args supplied to forward/inverse/etc\n        functions.\n      mapping: Instance of _Mapping to merge. Can only be specified if no other\n        arg is specified.\n\n    Returns:\n      mapping: New instance of `_Mapping` which has inputs merged with self.\n\n    Raises:\n      ValueError: if mapping and any other arg is not `None`.\n    \"\"\"\n    if mapping is None:\n      mapping = _Mapping(x=x, y=y, ildj=ildj, kwargs=kwargs)\n    elif any(arg is not None for arg in [x, y, ildj, kwargs]):\n      raise ValueError(\"Cannot simultaneously specify mapping and individual \"\n                       \"arguments.\")\n\n    return _Mapping(\n        x=self._merge(self.x, mapping.x),\n        y=self._merge(self.y, mapping.y),\n        ildj=self._merge(self.ildj, mapping.ildj),\n        kwargs=self._merge(self.kwargs, mapping.kwargs, use_equals=True))", "code_tokens": ["def", "merge", "(", "self", ",", "x", "=", "None", ",", "y", "=", "None", ",", "ildj", "=", "None", ",", "kwargs", "=", "None", ",", "mapping", "=", "None", ")", ":", "if", "mapping", "is", "None", ":", "mapping", "=", "_Mapping", "(", "x", "=", "x", ",", "y", "=", "y", ",", "ildj", "=", "ildj", ",", "kwargs", "=", "kwargs", ")", "elif", "any", "(", "arg", "is", "not", "None", "for", "arg", "in", "[", "x", ",", "y", ",", "ildj", ",", "kwargs", "]", ")", ":", "raise", "ValueError", "(", "\"Cannot simultaneously specify mapping and individual \"", "\"arguments.\"", ")", "return", "_Mapping", "(", "x", "=", "self", ".", "_merge", "(", "self", ".", "x", ",", "mapping", ".", "x", ")", ",", "y", "=", "self", ".", "_merge", "(", "self", ".", "y", ",", "mapping", ".", "y", ")", ",", "ildj", "=", "self", ".", "_merge", "(", "self", ".", "ildj", ",", "mapping", ".", "ildj", ")", ",", "kwargs", "=", "self", ".", "_merge", "(", "self", ".", "kwargs", ",", "mapping", ".", "kwargs", ",", "use_equals", "=", "True", ")", ")"], "docstring": "Returns new _Mapping with args merged with self.\n\n    Args:\n      x: `Tensor` or None. Input to forward; output of inverse.\n      y: `Tensor` or None. Input to inverse; output of forward.\n      ildj: `Tensor`. This is the (un-reduce_sum'ed) inverse log det jacobian.\n      kwargs: Python dictionary. Extra args supplied to forward/inverse/etc\n        functions.\n      mapping: Instance of _Mapping to merge. Can only be specified if no other\n        arg is specified.\n\n    Returns:\n      mapping: New instance of `_Mapping` which has inputs merged with self.\n\n    Raises:\n      ValueError: if mapping and any other arg is not `None`.", "docstring_tokens": ["Returns", "new", "_Mapping", "with", "args", "merged", "with", "self", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/bijector.py#L74-L102", "partition": "test"}
{"repo": "jshiv/turntable", "path": "turntable/press.py", "func_name": "RecordSetter.set_attributes", "original_string": "def set_attributes(self, kwargs):\n        '''\n        Initalizes the given argument structure as properties of the class\n        to be used by name in specific method execution.\n\n        Parameters\n        ----------\n        kwargs : dictionary\n            Dictionary of extra attributes,\n            where keys are attributes names and values attributes values.\n\n        '''\n\n        for key, value in kwargs.items():\n            setattr(self, key, value)", "language": "python", "code": "def set_attributes(self, kwargs):\n        '''\n        Initalizes the given argument structure as properties of the class\n        to be used by name in specific method execution.\n\n        Parameters\n        ----------\n        kwargs : dictionary\n            Dictionary of extra attributes,\n            where keys are attributes names and values attributes values.\n\n        '''\n\n        for key, value in kwargs.items():\n            setattr(self, key, value)", "code_tokens": ["def", "set_attributes", "(", "self", ",", "kwargs", ")", ":", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", ":", "setattr", "(", "self", ",", "key", ",", "value", ")"], "docstring": "Initalizes the given argument structure as properties of the class\n        to be used by name in specific method execution.\n\n        Parameters\n        ----------\n        kwargs : dictionary\n            Dictionary of extra attributes,\n            where keys are attributes names and values attributes values.", "docstring_tokens": ["Initalizes", "the", "given", "argument", "structure", "as", "properties", "of", "the", "class", "to", "be", "used", "by", "name", "in", "specific", "method", "execution", "."], "sha": "c095a93df14d672ba54db164a7ab7373444d1829", "url": "https://github.com/jshiv/turntable/blob/c095a93df14d672ba54db164a7ab7373444d1829/turntable/press.py#L149-L163", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/utils.py", "func_name": "get_all_elements", "original_string": "def get_all_elements(\n    node: astroid.node_classes.NodeNG\n) -> Iterable[astroid.node_classes.NodeNG]:\n    \"\"\"Recursively returns all atoms in nested lists and tuples.\"\"\"\n    if isinstance(node, (astroid.Tuple, astroid.List)):\n        for child in node.elts:\n            for e in get_all_elements(child):\n                yield e\n    else:\n        yield node", "language": "python", "code": "def get_all_elements(\n    node: astroid.node_classes.NodeNG\n) -> Iterable[astroid.node_classes.NodeNG]:\n    \"\"\"Recursively returns all atoms in nested lists and tuples.\"\"\"\n    if isinstance(node, (astroid.Tuple, astroid.List)):\n        for child in node.elts:\n            for e in get_all_elements(child):\n                yield e\n    else:\n        yield node", "code_tokens": ["def", "get_all_elements", "(", "node", ":", "astroid", ".", "node_classes", ".", "NodeNG", ")", "->", "Iterable", "[", "astroid", ".", "node_classes", ".", "NodeNG", "]", ":", "if", "isinstance", "(", "node", ",", "(", "astroid", ".", "Tuple", ",", "astroid", ".", "List", ")", ")", ":", "for", "child", "in", "node", ".", "elts", ":", "for", "e", "in", "get_all_elements", "(", "child", ")", ":", "yield", "e", "else", ":", "yield", "node"], "docstring": "Recursively returns all atoms in nested lists and tuples.", "docstring_tokens": ["Recursively", "returns", "all", "atoms", "in", "nested", "lists", "and", "tuples", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/utils.py#L228-L237", "partition": "test"}
{"repo": "vaexio/vaex", "path": "packages/vaex-core/vaex/functions.py", "func_name": "str_capitalize", "original_string": "def str_capitalize(x):\n    \"\"\"Capitalize the first letter of a string sample.\n\n    :returns: an expression containing the capitalized strings.\n\n    Example:\n\n    >>> import vaex\n    >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']\n    >>> df = vaex.from_arrays(text=text)\n    >>> df\n      #  text\n      0  Something\n      1  very pretty\n      2  is coming\n      3  our\n      4  way.\n\n    >>> df.text.str.capitalize()\n    Expression = str_capitalize(text)\n    Length: 5 dtype: str (expression)\n    ---------------------------------\n    0    Something\n    1  Very pretty\n    2    Is coming\n    3          Our\n    4         Way.\n    \"\"\"\n    sl = _to_string_sequence(x).capitalize()\n    return column.ColumnStringArrow(sl.bytes, sl.indices, sl.length, sl.offset, string_sequence=sl)", "language": "python", "code": "def str_capitalize(x):\n    \"\"\"Capitalize the first letter of a string sample.\n\n    :returns: an expression containing the capitalized strings.\n\n    Example:\n\n    >>> import vaex\n    >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']\n    >>> df = vaex.from_arrays(text=text)\n    >>> df\n      #  text\n      0  Something\n      1  very pretty\n      2  is coming\n      3  our\n      4  way.\n\n    >>> df.text.str.capitalize()\n    Expression = str_capitalize(text)\n    Length: 5 dtype: str (expression)\n    ---------------------------------\n    0    Something\n    1  Very pretty\n    2    Is coming\n    3          Our\n    4         Way.\n    \"\"\"\n    sl = _to_string_sequence(x).capitalize()\n    return column.ColumnStringArrow(sl.bytes, sl.indices, sl.length, sl.offset, string_sequence=sl)", "code_tokens": ["def", "str_capitalize", "(", "x", ")", ":", "sl", "=", "_to_string_sequence", "(", "x", ")", ".", "capitalize", "(", ")", "return", "column", ".", "ColumnStringArrow", "(", "sl", ".", "bytes", ",", "sl", ".", "indices", ",", "sl", ".", "length", ",", "sl", ".", "offset", ",", "string_sequence", "=", "sl", ")"], "docstring": "Capitalize the first letter of a string sample.\n\n    :returns: an expression containing the capitalized strings.\n\n    Example:\n\n    >>> import vaex\n    >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']\n    >>> df = vaex.from_arrays(text=text)\n    >>> df\n      #  text\n      0  Something\n      1  very pretty\n      2  is coming\n      3  our\n      4  way.\n\n    >>> df.text.str.capitalize()\n    Expression = str_capitalize(text)\n    Length: 5 dtype: str (expression)\n    ---------------------------------\n    0    Something\n    1  Very pretty\n    2    Is coming\n    3          Our\n    4         Way.", "docstring_tokens": ["Capitalize", "the", "first", "letter", "of", "a", "string", "sample", "."], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/functions.py#L511-L540", "partition": "test"}
{"repo": "rocky/python3-trepan", "path": "trepan/lib/core.py", "func_name": "TrepanCore.filename", "original_string": "def filename(self, filename=None):\n        \"\"\"Return filename or the basename of that depending on the\n        basename setting\"\"\"\n        if filename is None:\n            if self.debugger.mainpyfile:\n                filename = self.debugger.mainpyfile\n            else:\n                return None\n        if self.debugger.settings['basename']:\n            return(os.path.basename(filename))\n        return filename", "language": "python", "code": "def filename(self, filename=None):\n        \"\"\"Return filename or the basename of that depending on the\n        basename setting\"\"\"\n        if filename is None:\n            if self.debugger.mainpyfile:\n                filename = self.debugger.mainpyfile\n            else:\n                return None\n        if self.debugger.settings['basename']:\n            return(os.path.basename(filename))\n        return filename", "code_tokens": ["def", "filename", "(", "self", ",", "filename", "=", "None", ")", ":", "if", "filename", "is", "None", ":", "if", "self", ".", "debugger", ".", "mainpyfile", ":", "filename", "=", "self", ".", "debugger", ".", "mainpyfile", "else", ":", "return", "None", "if", "self", ".", "debugger", ".", "settings", "[", "'basename'", "]", ":", "return", "(", "os", ".", "path", ".", "basename", "(", "filename", ")", ")", "return", "filename"], "docstring": "Return filename or the basename of that depending on the\n        basename setting", "docstring_tokens": ["Return", "filename", "or", "the", "basename", "of", "that", "depending", "on", "the", "basename", "setting"], "sha": "14e91bc0acce090d67be145b1ac040cab92ac5f3", "url": "https://github.com/rocky/python3-trepan/blob/14e91bc0acce090d67be145b1ac040cab92ac5f3/trepan/lib/core.py#L184-L194", "partition": "test"}
{"repo": "jupyter-widgets/jupyterlab-sidecar", "path": "setupbase.py", "func_name": "_translate_glob", "original_string": "def _translate_glob(pat):\n    \"\"\"Translate a glob PATTERN to a regular expression.\"\"\"\n    translated_parts = []\n    for part in _iexplode_path(pat):\n        translated_parts.append(_translate_glob_part(part))\n    os_sep_class = '[%s]' % re.escape(SEPARATORS)\n    res = _join_translated(translated_parts, os_sep_class)\n    return '{res}\\\\Z(?ms)'.format(res=res)", "language": "python", "code": "def _translate_glob(pat):\n    \"\"\"Translate a glob PATTERN to a regular expression.\"\"\"\n    translated_parts = []\n    for part in _iexplode_path(pat):\n        translated_parts.append(_translate_glob_part(part))\n    os_sep_class = '[%s]' % re.escape(SEPARATORS)\n    res = _join_translated(translated_parts, os_sep_class)\n    return '{res}\\\\Z(?ms)'.format(res=res)", "code_tokens": ["def", "_translate_glob", "(", "pat", ")", ":", "translated_parts", "=", "[", "]", "for", "part", "in", "_iexplode_path", "(", "pat", ")", ":", "translated_parts", ".", "append", "(", "_translate_glob_part", "(", "part", ")", ")", "os_sep_class", "=", "'[%s]'", "%", "re", ".", "escape", "(", "SEPARATORS", ")", "res", "=", "_join_translated", "(", "translated_parts", ",", "os_sep_class", ")", "return", "'{res}\\\\Z(?ms)'", ".", "format", "(", "res", "=", "res", ")"], "docstring": "Translate a glob PATTERN to a regular expression.", "docstring_tokens": ["Translate", "a", "glob", "PATTERN", "to", "a", "regular", "expression", "."], "sha": "8889d09f1a0933e2cbee06d4874f720b075b29e8", "url": "https://github.com/jupyter-widgets/jupyterlab-sidecar/blob/8889d09f1a0933e2cbee06d4874f720b075b29e8/setupbase.py#L633-L640", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/python3.py", "func_name": "Python3Checker.visit_raise", "original_string": "def visit_raise(self, node):\n        \"\"\"Visit a raise statement and check for raising\n        strings or old-raise-syntax.\n        \"\"\"\n\n        # Ignore empty raise.\n        if node.exc is None:\n            return\n        expr = node.exc\n        if self._check_raise_value(node, expr):\n            return\n        try:\n            value = next(astroid.unpack_infer(expr))\n        except astroid.InferenceError:\n            return\n        self._check_raise_value(node, value)", "language": "python", "code": "def visit_raise(self, node):\n        \"\"\"Visit a raise statement and check for raising\n        strings or old-raise-syntax.\n        \"\"\"\n\n        # Ignore empty raise.\n        if node.exc is None:\n            return\n        expr = node.exc\n        if self._check_raise_value(node, expr):\n            return\n        try:\n            value = next(astroid.unpack_infer(expr))\n        except astroid.InferenceError:\n            return\n        self._check_raise_value(node, value)", "code_tokens": ["def", "visit_raise", "(", "self", ",", "node", ")", ":", "# Ignore empty raise.", "if", "node", ".", "exc", "is", "None", ":", "return", "expr", "=", "node", ".", "exc", "if", "self", ".", "_check_raise_value", "(", "node", ",", "expr", ")", ":", "return", "try", ":", "value", "=", "next", "(", "astroid", ".", "unpack_infer", "(", "expr", ")", ")", "except", "astroid", ".", "InferenceError", ":", "return", "self", ".", "_check_raise_value", "(", "node", ",", "value", ")"], "docstring": "Visit a raise statement and check for raising\n        strings or old-raise-syntax.", "docstring_tokens": ["Visit", "a", "raise", "statement", "and", "check", "for", "raising", "strings", "or", "old", "-", "raise", "-", "syntax", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/python3.py#L1323-L1338", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/frontend_widget.py", "func_name": "FrontendWidget._call_tip", "original_string": "def _call_tip(self):\n        \"\"\" Shows a call tip, if appropriate, at the current cursor location.\n        \"\"\"\n        # Decide if it makes sense to show a call tip\n        if not self.enable_calltips:\n            return False\n        cursor = self._get_cursor()\n        cursor.movePosition(QtGui.QTextCursor.Left)\n        if cursor.document().characterAt(cursor.position()) != '(':\n            return False\n        context = self._get_context(cursor)\n        if not context:\n            return False\n\n        # Send the metadata request to the kernel\n        name = '.'.join(context)\n        msg_id = self.kernel_manager.shell_channel.object_info(name)\n        pos = self._get_cursor().position()\n        self._request_info['call_tip'] = self._CallTipRequest(msg_id, pos)\n        return True", "language": "python", "code": "def _call_tip(self):\n        \"\"\" Shows a call tip, if appropriate, at the current cursor location.\n        \"\"\"\n        # Decide if it makes sense to show a call tip\n        if not self.enable_calltips:\n            return False\n        cursor = self._get_cursor()\n        cursor.movePosition(QtGui.QTextCursor.Left)\n        if cursor.document().characterAt(cursor.position()) != '(':\n            return False\n        context = self._get_context(cursor)\n        if not context:\n            return False\n\n        # Send the metadata request to the kernel\n        name = '.'.join(context)\n        msg_id = self.kernel_manager.shell_channel.object_info(name)\n        pos = self._get_cursor().position()\n        self._request_info['call_tip'] = self._CallTipRequest(msg_id, pos)\n        return True", "code_tokens": ["def", "_call_tip", "(", "self", ")", ":", "# Decide if it makes sense to show a call tip", "if", "not", "self", ".", "enable_calltips", ":", "return", "False", "cursor", "=", "self", ".", "_get_cursor", "(", ")", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "Left", ")", "if", "cursor", ".", "document", "(", ")", ".", "characterAt", "(", "cursor", ".", "position", "(", ")", ")", "!=", "'('", ":", "return", "False", "context", "=", "self", ".", "_get_context", "(", "cursor", ")", "if", "not", "context", ":", "return", "False", "# Send the metadata request to the kernel", "name", "=", "'.'", ".", "join", "(", "context", ")", "msg_id", "=", "self", ".", "kernel_manager", ".", "shell_channel", ".", "object_info", "(", "name", ")", "pos", "=", "self", ".", "_get_cursor", "(", ")", ".", "position", "(", ")", "self", ".", "_request_info", "[", "'call_tip'", "]", "=", "self", ".", "_CallTipRequest", "(", "msg_id", ",", "pos", ")", "return", "True"], "docstring": "Shows a call tip, if appropriate, at the current cursor location.", "docstring_tokens": ["Shows", "a", "call", "tip", "if", "appropriate", "at", "the", "current", "cursor", "location", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/frontend_widget.py#L653-L672", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/pulse/timeslots.py", "func_name": "TimeslotCollection.shift", "original_string": "def shift(self, time: int) -> 'TimeslotCollection':\n        \"\"\"Return a new TimeslotCollection shifted by `time`.\n\n        Args:\n            time: time to be shifted by\n        \"\"\"\n        slots = [Timeslot(slot.interval.shift(time), slot.channel) for slot in self.timeslots]\n        return TimeslotCollection(*slots)", "language": "python", "code": "def shift(self, time: int) -> 'TimeslotCollection':\n        \"\"\"Return a new TimeslotCollection shifted by `time`.\n\n        Args:\n            time: time to be shifted by\n        \"\"\"\n        slots = [Timeslot(slot.interval.shift(time), slot.channel) for slot in self.timeslots]\n        return TimeslotCollection(*slots)", "code_tokens": ["def", "shift", "(", "self", ",", "time", ":", "int", ")", "->", "'TimeslotCollection'", ":", "slots", "=", "[", "Timeslot", "(", "slot", ".", "interval", ".", "shift", "(", "time", ")", ",", "slot", ".", "channel", ")", "for", "slot", "in", "self", ".", "timeslots", "]", "return", "TimeslotCollection", "(", "*", "slots", ")"], "docstring": "Return a new TimeslotCollection shifted by `time`.\n\n        Args:\n            time: time to be shifted by", "docstring_tokens": ["Return", "a", "new", "TimeslotCollection", "shifted", "by", "time", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/pulse/timeslots.py#L233-L240", "partition": "test"}
{"repo": "ioam/parambokeh", "path": "parambokeh/__init__.py", "func_name": "notebook_show", "original_string": "def notebook_show(obj, doc, comm):\n    \"\"\"\n    Displays bokeh output inside a notebook.\n    \"\"\"\n    target = obj.ref['id']\n    load_mime = 'application/vnd.holoviews_load.v0+json'\n    exec_mime = 'application/vnd.holoviews_exec.v0+json'\n\n    # Publish plot HTML\n    bokeh_script, bokeh_div, _ = bokeh.embed.notebook.notebook_content(obj, comm.id)\n    publish_display_data(data={'text/html': encode_utf8(bokeh_div)})\n\n    # Publish comm manager\n    JS = '\\n'.join([PYVIZ_PROXY, JupyterCommManager.js_manager])\n    publish_display_data(data={load_mime: JS, 'application/javascript': JS})\n\n    # Publish bokeh plot JS\n    msg_handler = bokeh_msg_handler.format(plot_id=target)\n    comm_js = comm.js_template.format(plot_id=target, comm_id=comm.id, msg_handler=msg_handler)\n    bokeh_js = '\\n'.join([comm_js, bokeh_script])\n\n    # Note: extension should be altered so text/html is not required\n    publish_display_data(data={exec_mime: '', 'text/html': '',\n                               'application/javascript': bokeh_js},\n                         metadata={exec_mime: {'id': target}})", "language": "python", "code": "def notebook_show(obj, doc, comm):\n    \"\"\"\n    Displays bokeh output inside a notebook.\n    \"\"\"\n    target = obj.ref['id']\n    load_mime = 'application/vnd.holoviews_load.v0+json'\n    exec_mime = 'application/vnd.holoviews_exec.v0+json'\n\n    # Publish plot HTML\n    bokeh_script, bokeh_div, _ = bokeh.embed.notebook.notebook_content(obj, comm.id)\n    publish_display_data(data={'text/html': encode_utf8(bokeh_div)})\n\n    # Publish comm manager\n    JS = '\\n'.join([PYVIZ_PROXY, JupyterCommManager.js_manager])\n    publish_display_data(data={load_mime: JS, 'application/javascript': JS})\n\n    # Publish bokeh plot JS\n    msg_handler = bokeh_msg_handler.format(plot_id=target)\n    comm_js = comm.js_template.format(plot_id=target, comm_id=comm.id, msg_handler=msg_handler)\n    bokeh_js = '\\n'.join([comm_js, bokeh_script])\n\n    # Note: extension should be altered so text/html is not required\n    publish_display_data(data={exec_mime: '', 'text/html': '',\n                               'application/javascript': bokeh_js},\n                         metadata={exec_mime: {'id': target}})", "code_tokens": ["def", "notebook_show", "(", "obj", ",", "doc", ",", "comm", ")", ":", "target", "=", "obj", ".", "ref", "[", "'id'", "]", "load_mime", "=", "'application/vnd.holoviews_load.v0+json'", "exec_mime", "=", "'application/vnd.holoviews_exec.v0+json'", "# Publish plot HTML", "bokeh_script", ",", "bokeh_div", ",", "_", "=", "bokeh", ".", "embed", ".", "notebook", ".", "notebook_content", "(", "obj", ",", "comm", ".", "id", ")", "publish_display_data", "(", "data", "=", "{", "'text/html'", ":", "encode_utf8", "(", "bokeh_div", ")", "}", ")", "# Publish comm manager", "JS", "=", "'\\n'", ".", "join", "(", "[", "PYVIZ_PROXY", ",", "JupyterCommManager", ".", "js_manager", "]", ")", "publish_display_data", "(", "data", "=", "{", "load_mime", ":", "JS", ",", "'application/javascript'", ":", "JS", "}", ")", "# Publish bokeh plot JS", "msg_handler", "=", "bokeh_msg_handler", ".", "format", "(", "plot_id", "=", "target", ")", "comm_js", "=", "comm", ".", "js_template", ".", "format", "(", "plot_id", "=", "target", ",", "comm_id", "=", "comm", ".", "id", ",", "msg_handler", "=", "msg_handler", ")", "bokeh_js", "=", "'\\n'", ".", "join", "(", "[", "comm_js", ",", "bokeh_script", "]", ")", "# Note: extension should be altered so text/html is not required", "publish_display_data", "(", "data", "=", "{", "exec_mime", ":", "''", ",", "'text/html'", ":", "''", ",", "'application/javascript'", ":", "bokeh_js", "}", ",", "metadata", "=", "{", "exec_mime", ":", "{", "'id'", ":", "target", "}", "}", ")"], "docstring": "Displays bokeh output inside a notebook.", "docstring_tokens": ["Displays", "bokeh", "output", "inside", "a", "notebook", "."], "sha": "fb9744f216273c7b24e65d037b1d621c08d7fde6", "url": "https://github.com/ioam/parambokeh/blob/fb9744f216273c7b24e65d037b1d621c08d7fde6/parambokeh/__init__.py#L53-L77", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/codeunit.py", "func_name": "CodeUnit.should_be_python", "original_string": "def should_be_python(self):\n        \"\"\"Does it seem like this file should contain Python?\n\n        This is used to decide if a file reported as part of the exection of\n        a program was really likely to have contained Python in the first\n        place.\n\n        \"\"\"\n        # Get the file extension.\n        _, ext = os.path.splitext(self.filename)\n\n        # Anything named *.py* should be Python.\n        if ext.startswith('.py'):\n            return True\n        # A file with no extension should be Python.\n        if not ext:\n            return True\n        # Everything else is probably not Python.\n        return False", "language": "python", "code": "def should_be_python(self):\n        \"\"\"Does it seem like this file should contain Python?\n\n        This is used to decide if a file reported as part of the exection of\n        a program was really likely to have contained Python in the first\n        place.\n\n        \"\"\"\n        # Get the file extension.\n        _, ext = os.path.splitext(self.filename)\n\n        # Anything named *.py* should be Python.\n        if ext.startswith('.py'):\n            return True\n        # A file with no extension should be Python.\n        if not ext:\n            return True\n        # Everything else is probably not Python.\n        return False", "code_tokens": ["def", "should_be_python", "(", "self", ")", ":", "# Get the file extension.", "_", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "filename", ")", "# Anything named *.py* should be Python.", "if", "ext", ".", "startswith", "(", "'.py'", ")", ":", "return", "True", "# A file with no extension should be Python.", "if", "not", "ext", ":", "return", "True", "# Everything else is probably not Python.", "return", "False"], "docstring": "Does it seem like this file should contain Python?\n\n        This is used to decide if a file reported as part of the exection of\n        a program was really likely to have contained Python in the first\n        place.", "docstring_tokens": ["Does", "it", "seem", "like", "this", "file", "should", "contain", "Python?"], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/codeunit.py#L127-L145", "partition": "test"}
{"repo": "zomux/deepy", "path": "deepy/utils/timer.py", "func_name": "Timer.report", "original_string": "def report(self):\n        \"\"\"\n        Report elapsed time.\n        \"\"\"\n        if not self.end_time:\n            self.end()\n        print (\"Time: {} mins\".format((self.end_time - self.start_time )/ 60))", "language": "python", "code": "def report(self):\n        \"\"\"\n        Report elapsed time.\n        \"\"\"\n        if not self.end_time:\n            self.end()\n        print (\"Time: {} mins\".format((self.end_time - self.start_time )/ 60))", "code_tokens": ["def", "report", "(", "self", ")", ":", "if", "not", "self", ".", "end_time", ":", "self", ".", "end", "(", ")", "print", "(", "\"Time: {} mins\"", ".", "format", "(", "(", "self", ".", "end_time", "-", "self", ".", "start_time", ")", "/", "60", ")", ")"], "docstring": "Report elapsed time.", "docstring_tokens": ["Report", "elapsed", "time", "."], "sha": "090fbad22a08a809b12951cd0d4984f5bd432698", "url": "https://github.com/zomux/deepy/blob/090fbad22a08a809b12951cd0d4984f5bd432698/deepy/utils/timer.py#L21-L27", "partition": "test"}
{"repo": "capitalone/giraffez", "path": "giraffez/load.py", "func_name": "TeradataBulkLoad.finish", "original_string": "def finish(self):\n        \"\"\"\n        Finishes the load job. Called automatically when the connection closes.\n\n        :return: The exit code returned when applying rows to the table\n        \"\"\"\n        if self.finished:\n            return self.exit_code\n        checkpoint_status = self.checkpoint()\n        self.exit_code = self._exit_code()\n        if self.exit_code != 0:\n            raise TeradataPTError(\"BulkLoad job finished with return code '{}'\".format(self.exit_code))\n        # TODO(chris): should this happen every time?\n        if self.applied_count > 0:\n            self._end_acquisition()\n            self._apply_rows()\n        self.exit_code = self._exit_code()\n        if self.exit_code != 0:\n            raise TeradataPTError(\"BulkLoad job finished with return code '{}'\".format(self.exit_code))\n        self.finished = True\n        return self.exit_code", "language": "python", "code": "def finish(self):\n        \"\"\"\n        Finishes the load job. Called automatically when the connection closes.\n\n        :return: The exit code returned when applying rows to the table\n        \"\"\"\n        if self.finished:\n            return self.exit_code\n        checkpoint_status = self.checkpoint()\n        self.exit_code = self._exit_code()\n        if self.exit_code != 0:\n            raise TeradataPTError(\"BulkLoad job finished with return code '{}'\".format(self.exit_code))\n        # TODO(chris): should this happen every time?\n        if self.applied_count > 0:\n            self._end_acquisition()\n            self._apply_rows()\n        self.exit_code = self._exit_code()\n        if self.exit_code != 0:\n            raise TeradataPTError(\"BulkLoad job finished with return code '{}'\".format(self.exit_code))\n        self.finished = True\n        return self.exit_code", "code_tokens": ["def", "finish", "(", "self", ")", ":", "if", "self", ".", "finished", ":", "return", "self", ".", "exit_code", "checkpoint_status", "=", "self", ".", "checkpoint", "(", ")", "self", ".", "exit_code", "=", "self", ".", "_exit_code", "(", ")", "if", "self", ".", "exit_code", "!=", "0", ":", "raise", "TeradataPTError", "(", "\"BulkLoad job finished with return code '{}'\"", ".", "format", "(", "self", ".", "exit_code", ")", ")", "# TODO(chris): should this happen every time?", "if", "self", ".", "applied_count", ">", "0", ":", "self", ".", "_end_acquisition", "(", ")", "self", ".", "_apply_rows", "(", ")", "self", ".", "exit_code", "=", "self", ".", "_exit_code", "(", ")", "if", "self", ".", "exit_code", "!=", "0", ":", "raise", "TeradataPTError", "(", "\"BulkLoad job finished with return code '{}'\"", ".", "format", "(", "self", ".", "exit_code", ")", ")", "self", ".", "finished", "=", "True", "return", "self", ".", "exit_code"], "docstring": "Finishes the load job. Called automatically when the connection closes.\n\n        :return: The exit code returned when applying rows to the table", "docstring_tokens": ["Finishes", "the", "load", "job", ".", "Called", "automatically", "when", "the", "connection", "closes", "."], "sha": "6b4d27eb1a1eaf188c6885c7364ef27e92b1b957", "url": "https://github.com/capitalone/giraffez/blob/6b4d27eb1a1eaf188c6885c7364ef27e92b1b957/giraffez/load.py#L180-L200", "partition": "test"}
{"repo": "HumanBrainProject/hbp-service-client", "path": "hbp_service_client/request/request_builder.py", "func_name": "RequestBuilder.throw", "original_string": "def throw(self, exception_class, should_throw):\n        '''Defines if the an exception should be thrown after the request is sent\n\n        Args:\n            exception_class (class): The class of the exception to instantiate\n            should_throw (function): The predicate that should indicate if the exception\n                should be thrown. This function will be called with the response as a parameter\n\n        Returns:\n            The request builder instance in order to chain calls\n        '''\n        return self.__copy_and_set('throws', self._throws + [(exception_class, should_throw)])", "language": "python", "code": "def throw(self, exception_class, should_throw):\n        '''Defines if the an exception should be thrown after the request is sent\n\n        Args:\n            exception_class (class): The class of the exception to instantiate\n            should_throw (function): The predicate that should indicate if the exception\n                should be thrown. This function will be called with the response as a parameter\n\n        Returns:\n            The request builder instance in order to chain calls\n        '''\n        return self.__copy_and_set('throws', self._throws + [(exception_class, should_throw)])", "code_tokens": ["def", "throw", "(", "self", ",", "exception_class", ",", "should_throw", ")", ":", "return", "self", ".", "__copy_and_set", "(", "'throws'", ",", "self", ".", "_throws", "+", "[", "(", "exception_class", ",", "should_throw", ")", "]", ")"], "docstring": "Defines if the an exception should be thrown after the request is sent\n\n        Args:\n            exception_class (class): The class of the exception to instantiate\n            should_throw (function): The predicate that should indicate if the exception\n                should be thrown. This function will be called with the response as a parameter\n\n        Returns:\n            The request builder instance in order to chain calls", "docstring_tokens": ["Defines", "if", "the", "an", "exception", "should", "be", "thrown", "after", "the", "request", "is", "sent"], "sha": "b338fb41a7f0e7b9d654ff28fcf13a56d03bff4d", "url": "https://github.com/HumanBrainProject/hbp-service-client/blob/b338fb41a7f0e7b9d654ff28fcf13a56d03bff4d/hbp_service_client/request/request_builder.py#L191-L202", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/distribute-0.6.31-py2.7.egg/setuptools/archive_util.py", "func_name": "unpack_directory", "original_string": "def unpack_directory(filename, extract_dir, progress_filter=default_filter):\n    \"\"\"\"Unpack\" a directory, using the same interface as for archives\n\n    Raises ``UnrecognizedFormat`` if `filename` is not a directory\n    \"\"\"\n    if not os.path.isdir(filename):\n        raise UnrecognizedFormat(\"%s is not a directory\" % (filename,))\n\n    paths = {filename:('',extract_dir)}\n    for base, dirs, files in os.walk(filename):\n        src,dst = paths[base]\n        for d in dirs:\n            paths[os.path.join(base,d)] = src+d+'/', os.path.join(dst,d)\n        for f in files:\n            name = src+f\n            target = os.path.join(dst,f)\n            target = progress_filter(src+f, target)\n            if not target:\n                continue    # skip non-files\n            ensure_directory(target)\n            f = os.path.join(base,f)\n            shutil.copyfile(f, target)\n            shutil.copystat(f, target)", "language": "python", "code": "def unpack_directory(filename, extract_dir, progress_filter=default_filter):\n    \"\"\"\"Unpack\" a directory, using the same interface as for archives\n\n    Raises ``UnrecognizedFormat`` if `filename` is not a directory\n    \"\"\"\n    if not os.path.isdir(filename):\n        raise UnrecognizedFormat(\"%s is not a directory\" % (filename,))\n\n    paths = {filename:('',extract_dir)}\n    for base, dirs, files in os.walk(filename):\n        src,dst = paths[base]\n        for d in dirs:\n            paths[os.path.join(base,d)] = src+d+'/', os.path.join(dst,d)\n        for f in files:\n            name = src+f\n            target = os.path.join(dst,f)\n            target = progress_filter(src+f, target)\n            if not target:\n                continue    # skip non-files\n            ensure_directory(target)\n            f = os.path.join(base,f)\n            shutil.copyfile(f, target)\n            shutil.copystat(f, target)", "code_tokens": ["def", "unpack_directory", "(", "filename", ",", "extract_dir", ",", "progress_filter", "=", "default_filter", ")", ":", "if", "not", "os", ".", "path", ".", "isdir", "(", "filename", ")", ":", "raise", "UnrecognizedFormat", "(", "\"%s is not a directory\"", "%", "(", "filename", ",", ")", ")", "paths", "=", "{", "filename", ":", "(", "''", ",", "extract_dir", ")", "}", "for", "base", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "filename", ")", ":", "src", ",", "dst", "=", "paths", "[", "base", "]", "for", "d", "in", "dirs", ":", "paths", "[", "os", ".", "path", ".", "join", "(", "base", ",", "d", ")", "]", "=", "src", "+", "d", "+", "'/'", ",", "os", ".", "path", ".", "join", "(", "dst", ",", "d", ")", "for", "f", "in", "files", ":", "name", "=", "src", "+", "f", "target", "=", "os", ".", "path", ".", "join", "(", "dst", ",", "f", ")", "target", "=", "progress_filter", "(", "src", "+", "f", ",", "target", ")", "if", "not", "target", ":", "continue", "# skip non-files", "ensure_directory", "(", "target", ")", "f", "=", "os", ".", "path", ".", "join", "(", "base", ",", "f", ")", "shutil", ".", "copyfile", "(", "f", ",", "target", ")", "shutil", ".", "copystat", "(", "f", ",", "target", ")"], "docstring": "Unpack\" a directory, using the same interface as for archives\n\n    Raises ``UnrecognizedFormat`` if `filename` is not a directory", "docstring_tokens": ["Unpack", "a", "directory", "using", "the", "same", "interface", "as", "for", "archives"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/distribute-0.6.31-py2.7.egg/setuptools/archive_util.py#L83-L105", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/importer.py", "func_name": "_cache_from_source", "original_string": "def _cache_from_source(path: str) -> str:\n    \"\"\"Return the path to the cached file for the given path. The original path\n    does not have to exist.\"\"\"\n    cache_path, cache_file = os.path.split(importlib.util.cache_from_source(path))\n    filename, _ = os.path.splitext(cache_file)\n    return os.path.join(cache_path, filename + \".lpyc\")", "language": "python", "code": "def _cache_from_source(path: str) -> str:\n    \"\"\"Return the path to the cached file for the given path. The original path\n    does not have to exist.\"\"\"\n    cache_path, cache_file = os.path.split(importlib.util.cache_from_source(path))\n    filename, _ = os.path.splitext(cache_file)\n    return os.path.join(cache_path, filename + \".lpyc\")", "code_tokens": ["def", "_cache_from_source", "(", "path", ":", "str", ")", "->", "str", ":", "cache_path", ",", "cache_file", "=", "os", ".", "path", ".", "split", "(", "importlib", ".", "util", ".", "cache_from_source", "(", "path", ")", ")", "filename", ",", "_", "=", "os", ".", "path", ".", "splitext", "(", "cache_file", ")", "return", "os", ".", "path", ".", "join", "(", "cache_path", ",", "filename", "+", "\".lpyc\"", ")"], "docstring": "Return the path to the cached file for the given path. The original path\n    does not have to exist.", "docstring_tokens": ["Return", "the", "path", "to", "the", "cached", "file", "for", "the", "given", "path", ".", "The", "original", "path", "does", "not", "have", "to", "exist", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/importer.py#L82-L87", "partition": "test"}
{"repo": "ONSdigital/sdc-cryptography", "path": "sdc/crypto/encrypter.py", "func_name": "encrypt", "original_string": "def encrypt(json, key_store, key_purpose):\n    \"\"\"This encrypts the supplied json and returns a jwe token.\n\n    :param str json: The json to be encrypted.\n    :param key_store: The key store.\n    :param str key_purpose: Context for the key.\n    :return: A jwe token.\n\n    \"\"\"\n    jwt_key = key_store.get_key_for_purpose_and_type(key_purpose, \"private\")\n\n    payload = JWTHelper.encode(json, jwt_key.kid, key_store, key_purpose)\n\n    jwe_key = key_store.get_key_for_purpose_and_type(key_purpose, \"public\")\n\n    return JWEHelper.encrypt(payload, jwe_key.kid, key_store, key_purpose)", "language": "python", "code": "def encrypt(json, key_store, key_purpose):\n    \"\"\"This encrypts the supplied json and returns a jwe token.\n\n    :param str json: The json to be encrypted.\n    :param key_store: The key store.\n    :param str key_purpose: Context for the key.\n    :return: A jwe token.\n\n    \"\"\"\n    jwt_key = key_store.get_key_for_purpose_and_type(key_purpose, \"private\")\n\n    payload = JWTHelper.encode(json, jwt_key.kid, key_store, key_purpose)\n\n    jwe_key = key_store.get_key_for_purpose_and_type(key_purpose, \"public\")\n\n    return JWEHelper.encrypt(payload, jwe_key.kid, key_store, key_purpose)", "code_tokens": ["def", "encrypt", "(", "json", ",", "key_store", ",", "key_purpose", ")", ":", "jwt_key", "=", "key_store", ".", "get_key_for_purpose_and_type", "(", "key_purpose", ",", "\"private\"", ")", "payload", "=", "JWTHelper", ".", "encode", "(", "json", ",", "jwt_key", ".", "kid", ",", "key_store", ",", "key_purpose", ")", "jwe_key", "=", "key_store", ".", "get_key_for_purpose_and_type", "(", "key_purpose", ",", "\"public\"", ")", "return", "JWEHelper", ".", "encrypt", "(", "payload", ",", "jwe_key", ".", "kid", ",", "key_store", ",", "key_purpose", ")"], "docstring": "This encrypts the supplied json and returns a jwe token.\n\n    :param str json: The json to be encrypted.\n    :param key_store: The key store.\n    :param str key_purpose: Context for the key.\n    :return: A jwe token.", "docstring_tokens": ["This", "encrypts", "the", "supplied", "json", "and", "returns", "a", "jwe", "token", "."], "sha": "846feb2b27b1c62d35ff2c290c05abcead68b23c", "url": "https://github.com/ONSdigital/sdc-cryptography/blob/846feb2b27b1c62d35ff2c290c05abcead68b23c/sdc/crypto/encrypter.py#L5-L20", "partition": "test"}
{"repo": "LionelAuroux/pyrser", "path": "pyrser/meta.py", "func_name": "enum", "original_string": "def enum(*sequential, **named):\n    \"\"\"\n    Build an enum statement\n    \"\"\"\n    #: build enums from parameter\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    enums['map'] = copy.copy(enums)\n    #: build reverse mapping\n    enums['rmap'] = {}\n    for key, value in enums.items():\n        if type(value) is int:\n            enums['rmap'][value] = key\n    return type('Enum', (), enums)", "language": "python", "code": "def enum(*sequential, **named):\n    \"\"\"\n    Build an enum statement\n    \"\"\"\n    #: build enums from parameter\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    enums['map'] = copy.copy(enums)\n    #: build reverse mapping\n    enums['rmap'] = {}\n    for key, value in enums.items():\n        if type(value) is int:\n            enums['rmap'][value] = key\n    return type('Enum', (), enums)", "code_tokens": ["def", "enum", "(", "*", "sequential", ",", "*", "*", "named", ")", ":", "#: build enums from parameter", "enums", "=", "dict", "(", "zip", "(", "sequential", ",", "range", "(", "len", "(", "sequential", ")", ")", ")", ",", "*", "*", "named", ")", "enums", "[", "'map'", "]", "=", "copy", ".", "copy", "(", "enums", ")", "#: build reverse mapping", "enums", "[", "'rmap'", "]", "=", "{", "}", "for", "key", ",", "value", "in", "enums", ".", "items", "(", ")", ":", "if", "type", "(", "value", ")", "is", "int", ":", "enums", "[", "'rmap'", "]", "[", "value", "]", "=", "key", "return", "type", "(", "'Enum'", ",", "(", ")", ",", "enums", ")"], "docstring": "Build an enum statement", "docstring_tokens": ["Build", "an", "enum", "statement"], "sha": "f153a97ef2b6bf915a1ed468c0252a9a59b754d5", "url": "https://github.com/LionelAuroux/pyrser/blob/f153a97ef2b6bf915a1ed468c0252a9a59b754d5/pyrser/meta.py#L8-L20", "partition": "test"}
{"repo": "urinieto/msaf", "path": "msaf/input_output.py", "func_name": "save_estimations", "original_string": "def save_estimations(file_struct, times, labels, boundaries_id, labels_id,\n                     **params):\n    \"\"\"Saves the segment estimations in a JAMS file.\n\n    Parameters\n    ----------\n    file_struct : FileStruct\n        Object with the different file paths of the current file.\n    times : np.array or list\n        Estimated boundary times.\n        If `list`, estimated hierarchical boundaries.\n    labels : np.array(N, 2)\n        Estimated labels (None in case we are only storing boundary\n        evaluations).\n    boundaries_id : str\n        Boundary algorithm identifier.\n    labels_id : str\n        Labels algorithm identifier.\n    params : dict\n        Dictionary with additional parameters for both algorithms.\n    \"\"\"\n    # Remove features if they exist\n    params.pop(\"features\", None)\n\n    # Get duration\n    dur = get_duration(file_struct.features_file)\n\n    # Convert to intervals and sanity check\n    if 'numpy' in str(type(times)):\n        # Flat check\n        inters = utils.times_to_intervals(times)\n        assert len(inters) == len(labels), \"Number of boundary intervals \" \\\n            \"(%d) and labels (%d) do not match\" % (len(inters), len(labels))\n        # Put into lists to simplify the writing process later\n        inters = [inters]\n        labels = [labels]\n    else:\n        # Hierarchical check\n        inters = []\n        for level in range(len(times)):\n            est_inters = utils.times_to_intervals(times[level])\n            inters.append(est_inters)\n            assert len(inters[level]) == len(labels[level]), \\\n                \"Number of boundary intervals (%d) and labels (%d) do not \" \\\n                \"match in level %d\" % (len(inters[level]), len(labels[level]),\n                                       level)\n\n    # Create new estimation\n    namespace = \"multi_segment\" if params[\"hier\"] else \"segment_open\"\n    ann = jams.Annotation(namespace=namespace)\n\n    # Find estimation in file\n    if os.path.isfile(file_struct.est_file):\n        jam = jams.load(file_struct.est_file, validate=False)\n        curr_ann = find_estimation(jam, boundaries_id, labels_id, params)\n        if curr_ann is not None:\n            curr_ann.data = ann.data  # cleanup all data\n            ann = curr_ann  # This will overwrite the existing estimation\n        else:\n            jam.annotations.append(ann)\n    else:\n        # Create new JAMS if it doesn't exist\n        jam = jams.JAMS()\n        jam.file_metadata.duration = dur\n        jam.annotations.append(ann)\n\n    # Save metadata and parameters\n    ann.annotation_metadata.version = msaf.__version__\n    ann.annotation_metadata.data_source = \"MSAF\"\n    sandbox = {}\n    sandbox[\"boundaries_id\"] = boundaries_id\n    sandbox[\"labels_id\"] = labels_id\n    sandbox[\"timestamp\"] = \\\n        datetime.datetime.today().strftime(\"%Y/%m/%d %H:%M:%S\")\n    for key in params:\n        sandbox[key] = params[key]\n    ann.sandbox = sandbox\n\n    # Save actual data\n    for i, (level_inters, level_labels) in enumerate(zip(inters, labels)):\n        for bound_inter, label in zip(level_inters, level_labels):\n            dur = float(bound_inter[1]) - float(bound_inter[0])\n            label = chr(int(label) + 65)\n            if params[\"hier\"]:\n                value = {\"label\": label, \"level\": i}\n            else:\n                value = label\n            ann.append(time=bound_inter[0], duration=dur,\n                       value=value)\n\n    # Write results\n    jam.save(file_struct.est_file)", "language": "python", "code": "def save_estimations(file_struct, times, labels, boundaries_id, labels_id,\n                     **params):\n    \"\"\"Saves the segment estimations in a JAMS file.\n\n    Parameters\n    ----------\n    file_struct : FileStruct\n        Object with the different file paths of the current file.\n    times : np.array or list\n        Estimated boundary times.\n        If `list`, estimated hierarchical boundaries.\n    labels : np.array(N, 2)\n        Estimated labels (None in case we are only storing boundary\n        evaluations).\n    boundaries_id : str\n        Boundary algorithm identifier.\n    labels_id : str\n        Labels algorithm identifier.\n    params : dict\n        Dictionary with additional parameters for both algorithms.\n    \"\"\"\n    # Remove features if they exist\n    params.pop(\"features\", None)\n\n    # Get duration\n    dur = get_duration(file_struct.features_file)\n\n    # Convert to intervals and sanity check\n    if 'numpy' in str(type(times)):\n        # Flat check\n        inters = utils.times_to_intervals(times)\n        assert len(inters) == len(labels), \"Number of boundary intervals \" \\\n            \"(%d) and labels (%d) do not match\" % (len(inters), len(labels))\n        # Put into lists to simplify the writing process later\n        inters = [inters]\n        labels = [labels]\n    else:\n        # Hierarchical check\n        inters = []\n        for level in range(len(times)):\n            est_inters = utils.times_to_intervals(times[level])\n            inters.append(est_inters)\n            assert len(inters[level]) == len(labels[level]), \\\n                \"Number of boundary intervals (%d) and labels (%d) do not \" \\\n                \"match in level %d\" % (len(inters[level]), len(labels[level]),\n                                       level)\n\n    # Create new estimation\n    namespace = \"multi_segment\" if params[\"hier\"] else \"segment_open\"\n    ann = jams.Annotation(namespace=namespace)\n\n    # Find estimation in file\n    if os.path.isfile(file_struct.est_file):\n        jam = jams.load(file_struct.est_file, validate=False)\n        curr_ann = find_estimation(jam, boundaries_id, labels_id, params)\n        if curr_ann is not None:\n            curr_ann.data = ann.data  # cleanup all data\n            ann = curr_ann  # This will overwrite the existing estimation\n        else:\n            jam.annotations.append(ann)\n    else:\n        # Create new JAMS if it doesn't exist\n        jam = jams.JAMS()\n        jam.file_metadata.duration = dur\n        jam.annotations.append(ann)\n\n    # Save metadata and parameters\n    ann.annotation_metadata.version = msaf.__version__\n    ann.annotation_metadata.data_source = \"MSAF\"\n    sandbox = {}\n    sandbox[\"boundaries_id\"] = boundaries_id\n    sandbox[\"labels_id\"] = labels_id\n    sandbox[\"timestamp\"] = \\\n        datetime.datetime.today().strftime(\"%Y/%m/%d %H:%M:%S\")\n    for key in params:\n        sandbox[key] = params[key]\n    ann.sandbox = sandbox\n\n    # Save actual data\n    for i, (level_inters, level_labels) in enumerate(zip(inters, labels)):\n        for bound_inter, label in zip(level_inters, level_labels):\n            dur = float(bound_inter[1]) - float(bound_inter[0])\n            label = chr(int(label) + 65)\n            if params[\"hier\"]:\n                value = {\"label\": label, \"level\": i}\n            else:\n                value = label\n            ann.append(time=bound_inter[0], duration=dur,\n                       value=value)\n\n    # Write results\n    jam.save(file_struct.est_file)", "code_tokens": ["def", "save_estimations", "(", "file_struct", ",", "times", ",", "labels", ",", "boundaries_id", ",", "labels_id", ",", "*", "*", "params", ")", ":", "# Remove features if they exist", "params", ".", "pop", "(", "\"features\"", ",", "None", ")", "# Get duration", "dur", "=", "get_duration", "(", "file_struct", ".", "features_file", ")", "# Convert to intervals and sanity check", "if", "'numpy'", "in", "str", "(", "type", "(", "times", ")", ")", ":", "# Flat check", "inters", "=", "utils", ".", "times_to_intervals", "(", "times", ")", "assert", "len", "(", "inters", ")", "==", "len", "(", "labels", ")", ",", "\"Number of boundary intervals \"", "\"(%d) and labels (%d) do not match\"", "%", "(", "len", "(", "inters", ")", ",", "len", "(", "labels", ")", ")", "# Put into lists to simplify the writing process later", "inters", "=", "[", "inters", "]", "labels", "=", "[", "labels", "]", "else", ":", "# Hierarchical check", "inters", "=", "[", "]", "for", "level", "in", "range", "(", "len", "(", "times", ")", ")", ":", "est_inters", "=", "utils", ".", "times_to_intervals", "(", "times", "[", "level", "]", ")", "inters", ".", "append", "(", "est_inters", ")", "assert", "len", "(", "inters", "[", "level", "]", ")", "==", "len", "(", "labels", "[", "level", "]", ")", ",", "\"Number of boundary intervals (%d) and labels (%d) do not \"", "\"match in level %d\"", "%", "(", "len", "(", "inters", "[", "level", "]", ")", ",", "len", "(", "labels", "[", "level", "]", ")", ",", "level", ")", "# Create new estimation", "namespace", "=", "\"multi_segment\"", "if", "params", "[", "\"hier\"", "]", "else", "\"segment_open\"", "ann", "=", "jams", ".", "Annotation", "(", "namespace", "=", "namespace", ")", "# Find estimation in file", "if", "os", ".", "path", ".", "isfile", "(", "file_struct", ".", "est_file", ")", ":", "jam", "=", "jams", ".", "load", "(", "file_struct", ".", "est_file", ",", "validate", "=", "False", ")", "curr_ann", "=", "find_estimation", "(", "jam", ",", "boundaries_id", ",", "labels_id", ",", "params", ")", "if", "curr_ann", "is", "not", "None", ":", "curr_ann", ".", "data", "=", "ann", ".", "data", "# cleanup all data", "ann", "=", "curr_ann", "# This will overwrite the existing estimation", "else", ":", "jam", ".", "annotations", ".", "append", "(", "ann", ")", "else", ":", "# Create new JAMS if it doesn't exist", "jam", "=", "jams", ".", "JAMS", "(", ")", "jam", ".", "file_metadata", ".", "duration", "=", "dur", "jam", ".", "annotations", ".", "append", "(", "ann", ")", "# Save metadata and parameters", "ann", ".", "annotation_metadata", ".", "version", "=", "msaf", ".", "__version__", "ann", ".", "annotation_metadata", ".", "data_source", "=", "\"MSAF\"", "sandbox", "=", "{", "}", "sandbox", "[", "\"boundaries_id\"", "]", "=", "boundaries_id", "sandbox", "[", "\"labels_id\"", "]", "=", "labels_id", "sandbox", "[", "\"timestamp\"", "]", "=", "datetime", ".", "datetime", ".", "today", "(", ")", ".", "strftime", "(", "\"%Y/%m/%d %H:%M:%S\"", ")", "for", "key", "in", "params", ":", "sandbox", "[", "key", "]", "=", "params", "[", "key", "]", "ann", ".", "sandbox", "=", "sandbox", "# Save actual data", "for", "i", ",", "(", "level_inters", ",", "level_labels", ")", "in", "enumerate", "(", "zip", "(", "inters", ",", "labels", ")", ")", ":", "for", "bound_inter", ",", "label", "in", "zip", "(", "level_inters", ",", "level_labels", ")", ":", "dur", "=", "float", "(", "bound_inter", "[", "1", "]", ")", "-", "float", "(", "bound_inter", "[", "0", "]", ")", "label", "=", "chr", "(", "int", "(", "label", ")", "+", "65", ")", "if", "params", "[", "\"hier\"", "]", ":", "value", "=", "{", "\"label\"", ":", "label", ",", "\"level\"", ":", "i", "}", "else", ":", "value", "=", "label", "ann", ".", "append", "(", "time", "=", "bound_inter", "[", "0", "]", ",", "duration", "=", "dur", ",", "value", "=", "value", ")", "# Write results", "jam", ".", "save", "(", "file_struct", ".", "est_file", ")"], "docstring": "Saves the segment estimations in a JAMS file.\n\n    Parameters\n    ----------\n    file_struct : FileStruct\n        Object with the different file paths of the current file.\n    times : np.array or list\n        Estimated boundary times.\n        If `list`, estimated hierarchical boundaries.\n    labels : np.array(N, 2)\n        Estimated labels (None in case we are only storing boundary\n        evaluations).\n    boundaries_id : str\n        Boundary algorithm identifier.\n    labels_id : str\n        Labels algorithm identifier.\n    params : dict\n        Dictionary with additional parameters for both algorithms.", "docstring_tokens": ["Saves", "the", "segment", "estimations", "in", "a", "JAMS", "file", "."], "sha": "9dbb57d77a1310465a65cc40f1641d083ca74385", "url": "https://github.com/urinieto/msaf/blob/9dbb57d77a1310465a65cc40f1641d083ca74385/msaf/input_output.py#L212-L303", "partition": "test"}
{"repo": "chrisjrn/registrasion", "path": "registrasion/reporting/reports.py", "func_name": "ReportView.get_form", "original_string": "def get_form(self, request):\n\n        ''' Creates an instance of self.form_type using request.GET '''\n\n        # Create a form instance\n        if self.form_type is not None:\n            form = self.form_type(request.GET)\n\n            # Pre-validate it\n            form.is_valid()\n        else:\n            form = None\n\n        return form", "language": "python", "code": "def get_form(self, request):\n\n        ''' Creates an instance of self.form_type using request.GET '''\n\n        # Create a form instance\n        if self.form_type is not None:\n            form = self.form_type(request.GET)\n\n            # Pre-validate it\n            form.is_valid()\n        else:\n            form = None\n\n        return form", "code_tokens": ["def", "get_form", "(", "self", ",", "request", ")", ":", "# Create a form instance", "if", "self", ".", "form_type", "is", "not", "None", ":", "form", "=", "self", ".", "form_type", "(", "request", ".", "GET", ")", "# Pre-validate it", "form", ".", "is_valid", "(", ")", "else", ":", "form", "=", "None", "return", "form"], "docstring": "Creates an instance of self.form_type using request.GET", "docstring_tokens": ["Creates", "an", "instance", "of", "self", ".", "form_type", "using", "request", ".", "GET"], "sha": "461d5846c6f9f3b7099322a94f5d9911564448e4", "url": "https://github.com/chrisjrn/registrasion/blob/461d5846c6f9f3b7099322a94f5d9911564448e4/registrasion/reporting/reports.py#L238-L251", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/server/blueprints/variants/controllers.py", "func_name": "sv_variant", "original_string": "def sv_variant(store, institute_id, case_name, variant_id=None, variant_obj=None, add_case=True,\n               get_overlapping=True):\n    \"\"\"Pre-process an SV variant entry for detail page.\n\n    Adds information to display variant\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        institute_id(str)\n        case_name(str)\n        variant_id(str)\n        variant_obj(dcit)\n        add_case(bool): If information about case files should be added\n\n    Returns:\n        detailed_information(dict): {\n            'institute': <institute_obj>,\n            'case': <case_obj>,\n            'variant': <variant_obj>,\n            'overlapping_snvs': <overlapping_snvs>,\n            'manual_rank_options': MANUAL_RANK_OPTIONS,\n            'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n        }\n    \"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n\n    if not variant_obj:\n        variant_obj = store.variant(variant_id)\n\n    if add_case:\n        # fill in information for pilup view\n        variant_case(store, case_obj, variant_obj)\n\n    # frequencies\n    variant_obj['frequencies'] = [\n        ('1000G', variant_obj.get('thousand_genomes_frequency')),\n        ('1000G (left)', variant_obj.get('thousand_genomes_frequency_left')),\n        ('1000G (right)', variant_obj.get('thousand_genomes_frequency_right')),\n        ('ClinGen CGH (benign)', variant_obj.get('clingen_cgh_benign')),\n        ('ClinGen CGH (pathogenic)', variant_obj.get('clingen_cgh_pathogenic')),\n        ('ClinGen NGI', variant_obj.get('clingen_ngi')),\n        ('SweGen', variant_obj.get('swegen')),\n        ('Decipher', variant_obj.get('decipher')),\n    ]\n\n    variant_obj['callers'] = callers(variant_obj, category='sv')\n\n    overlapping_snvs = []\n    if get_overlapping:\n        overlapping_snvs = (parse_variant(store, institute_obj, case_obj, variant) for variant in\n                            store.overlapping(variant_obj))\n\n    # parse_gene function is not called for SVs, but a link to ensembl gene is required\n    for gene_obj in variant_obj['genes']:\n        if gene_obj.get('common'):\n            ensembl_id = gene_obj['common']['ensembl_id']\n            try:\n                build = int(gene_obj['common'].get('build','37'))\n            except Exception:\n                build = 37\n            gene_obj['ensembl_link'] = ensembl(ensembl_id, build=build)\n\n    variant_obj['comments'] = store.events(institute_obj, case=case_obj,\n                                           variant_id=variant_obj['variant_id'], comments=True)\n\n    case_clinvars = store.case_to_clinVars(case_obj.get('display_name'))\n    if variant_id in case_clinvars:\n        variant_obj['clinvar_clinsig'] = case_clinvars.get(variant_id)['clinsig']\n\n    if not 'end_chrom' in variant_obj:\n        variant_obj['end_chrom'] = variant_obj['chromosome']\n\n    return {\n        'institute': institute_obj,\n        'case': case_obj,\n        'variant': variant_obj,\n        'overlapping_snvs': overlapping_snvs,\n        'manual_rank_options': MANUAL_RANK_OPTIONS,\n        'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n    }", "language": "python", "code": "def sv_variant(store, institute_id, case_name, variant_id=None, variant_obj=None, add_case=True,\n               get_overlapping=True):\n    \"\"\"Pre-process an SV variant entry for detail page.\n\n    Adds information to display variant\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        institute_id(str)\n        case_name(str)\n        variant_id(str)\n        variant_obj(dcit)\n        add_case(bool): If information about case files should be added\n\n    Returns:\n        detailed_information(dict): {\n            'institute': <institute_obj>,\n            'case': <case_obj>,\n            'variant': <variant_obj>,\n            'overlapping_snvs': <overlapping_snvs>,\n            'manual_rank_options': MANUAL_RANK_OPTIONS,\n            'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n        }\n    \"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n\n    if not variant_obj:\n        variant_obj = store.variant(variant_id)\n\n    if add_case:\n        # fill in information for pilup view\n        variant_case(store, case_obj, variant_obj)\n\n    # frequencies\n    variant_obj['frequencies'] = [\n        ('1000G', variant_obj.get('thousand_genomes_frequency')),\n        ('1000G (left)', variant_obj.get('thousand_genomes_frequency_left')),\n        ('1000G (right)', variant_obj.get('thousand_genomes_frequency_right')),\n        ('ClinGen CGH (benign)', variant_obj.get('clingen_cgh_benign')),\n        ('ClinGen CGH (pathogenic)', variant_obj.get('clingen_cgh_pathogenic')),\n        ('ClinGen NGI', variant_obj.get('clingen_ngi')),\n        ('SweGen', variant_obj.get('swegen')),\n        ('Decipher', variant_obj.get('decipher')),\n    ]\n\n    variant_obj['callers'] = callers(variant_obj, category='sv')\n\n    overlapping_snvs = []\n    if get_overlapping:\n        overlapping_snvs = (parse_variant(store, institute_obj, case_obj, variant) for variant in\n                            store.overlapping(variant_obj))\n\n    # parse_gene function is not called for SVs, but a link to ensembl gene is required\n    for gene_obj in variant_obj['genes']:\n        if gene_obj.get('common'):\n            ensembl_id = gene_obj['common']['ensembl_id']\n            try:\n                build = int(gene_obj['common'].get('build','37'))\n            except Exception:\n                build = 37\n            gene_obj['ensembl_link'] = ensembl(ensembl_id, build=build)\n\n    variant_obj['comments'] = store.events(institute_obj, case=case_obj,\n                                           variant_id=variant_obj['variant_id'], comments=True)\n\n    case_clinvars = store.case_to_clinVars(case_obj.get('display_name'))\n    if variant_id in case_clinvars:\n        variant_obj['clinvar_clinsig'] = case_clinvars.get(variant_id)['clinsig']\n\n    if not 'end_chrom' in variant_obj:\n        variant_obj['end_chrom'] = variant_obj['chromosome']\n\n    return {\n        'institute': institute_obj,\n        'case': case_obj,\n        'variant': variant_obj,\n        'overlapping_snvs': overlapping_snvs,\n        'manual_rank_options': MANUAL_RANK_OPTIONS,\n        'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n    }", "code_tokens": ["def", "sv_variant", "(", "store", ",", "institute_id", ",", "case_name", ",", "variant_id", "=", "None", ",", "variant_obj", "=", "None", ",", "add_case", "=", "True", ",", "get_overlapping", "=", "True", ")", ":", "institute_obj", ",", "case_obj", "=", "institute_and_case", "(", "store", ",", "institute_id", ",", "case_name", ")", "if", "not", "variant_obj", ":", "variant_obj", "=", "store", ".", "variant", "(", "variant_id", ")", "if", "add_case", ":", "# fill in information for pilup view", "variant_case", "(", "store", ",", "case_obj", ",", "variant_obj", ")", "# frequencies", "variant_obj", "[", "'frequencies'", "]", "=", "[", "(", "'1000G'", ",", "variant_obj", ".", "get", "(", "'thousand_genomes_frequency'", ")", ")", ",", "(", "'1000G (left)'", ",", "variant_obj", ".", "get", "(", "'thousand_genomes_frequency_left'", ")", ")", ",", "(", "'1000G (right)'", ",", "variant_obj", ".", "get", "(", "'thousand_genomes_frequency_right'", ")", ")", ",", "(", "'ClinGen CGH (benign)'", ",", "variant_obj", ".", "get", "(", "'clingen_cgh_benign'", ")", ")", ",", "(", "'ClinGen CGH (pathogenic)'", ",", "variant_obj", ".", "get", "(", "'clingen_cgh_pathogenic'", ")", ")", ",", "(", "'ClinGen NGI'", ",", "variant_obj", ".", "get", "(", "'clingen_ngi'", ")", ")", ",", "(", "'SweGen'", ",", "variant_obj", ".", "get", "(", "'swegen'", ")", ")", ",", "(", "'Decipher'", ",", "variant_obj", ".", "get", "(", "'decipher'", ")", ")", ",", "]", "variant_obj", "[", "'callers'", "]", "=", "callers", "(", "variant_obj", ",", "category", "=", "'sv'", ")", "overlapping_snvs", "=", "[", "]", "if", "get_overlapping", ":", "overlapping_snvs", "=", "(", "parse_variant", "(", "store", ",", "institute_obj", ",", "case_obj", ",", "variant", ")", "for", "variant", "in", "store", ".", "overlapping", "(", "variant_obj", ")", ")", "# parse_gene function is not called for SVs, but a link to ensembl gene is required", "for", "gene_obj", "in", "variant_obj", "[", "'genes'", "]", ":", "if", "gene_obj", ".", "get", "(", "'common'", ")", ":", "ensembl_id", "=", "gene_obj", "[", "'common'", "]", "[", "'ensembl_id'", "]", "try", ":", "build", "=", "int", "(", "gene_obj", "[", "'common'", "]", ".", "get", "(", "'build'", ",", "'37'", ")", ")", "except", "Exception", ":", "build", "=", "37", "gene_obj", "[", "'ensembl_link'", "]", "=", "ensembl", "(", "ensembl_id", ",", "build", "=", "build", ")", "variant_obj", "[", "'comments'", "]", "=", "store", ".", "events", "(", "institute_obj", ",", "case", "=", "case_obj", ",", "variant_id", "=", "variant_obj", "[", "'variant_id'", "]", ",", "comments", "=", "True", ")", "case_clinvars", "=", "store", ".", "case_to_clinVars", "(", "case_obj", ".", "get", "(", "'display_name'", ")", ")", "if", "variant_id", "in", "case_clinvars", ":", "variant_obj", "[", "'clinvar_clinsig'", "]", "=", "case_clinvars", ".", "get", "(", "variant_id", ")", "[", "'clinsig'", "]", "if", "not", "'end_chrom'", "in", "variant_obj", ":", "variant_obj", "[", "'end_chrom'", "]", "=", "variant_obj", "[", "'chromosome'", "]", "return", "{", "'institute'", ":", "institute_obj", ",", "'case'", ":", "case_obj", ",", "'variant'", ":", "variant_obj", ",", "'overlapping_snvs'", ":", "overlapping_snvs", ",", "'manual_rank_options'", ":", "MANUAL_RANK_OPTIONS", ",", "'dismiss_variant_options'", ":", "DISMISS_VARIANT_OPTIONS", "}"], "docstring": "Pre-process an SV variant entry for detail page.\n\n    Adds information to display variant\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        institute_id(str)\n        case_name(str)\n        variant_id(str)\n        variant_obj(dcit)\n        add_case(bool): If information about case files should be added\n\n    Returns:\n        detailed_information(dict): {\n            'institute': <institute_obj>,\n            'case': <case_obj>,\n            'variant': <variant_obj>,\n            'overlapping_snvs': <overlapping_snvs>,\n            'manual_rank_options': MANUAL_RANK_OPTIONS,\n            'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n        }", "docstring_tokens": ["Pre", "-", "process", "an", "SV", "variant", "entry", "for", "detail", "page", "."], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/server/blueprints/variants/controllers.py#L123-L202", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/archive.py", "func_name": "ArchiveManager._search_archives", "original_string": "def _search_archives(self, origin, backend_name, category, archived_after):\n        \"\"\"Search archives using filters.\"\"\"\n\n        for archive_path in self._search_files():\n            try:\n                archive = Archive(archive_path)\n            except ArchiveError:\n                continue\n\n            match = archive.origin == origin and \\\n                archive.backend_name == backend_name and \\\n                archive.category == category and \\\n                archive.created_on >= archived_after\n\n            if not match:\n                continue\n\n            yield archive_path, archive.created_on", "language": "python", "code": "def _search_archives(self, origin, backend_name, category, archived_after):\n        \"\"\"Search archives using filters.\"\"\"\n\n        for archive_path in self._search_files():\n            try:\n                archive = Archive(archive_path)\n            except ArchiveError:\n                continue\n\n            match = archive.origin == origin and \\\n                archive.backend_name == backend_name and \\\n                archive.category == category and \\\n                archive.created_on >= archived_after\n\n            if not match:\n                continue\n\n            yield archive_path, archive.created_on", "code_tokens": ["def", "_search_archives", "(", "self", ",", "origin", ",", "backend_name", ",", "category", ",", "archived_after", ")", ":", "for", "archive_path", "in", "self", ".", "_search_files", "(", ")", ":", "try", ":", "archive", "=", "Archive", "(", "archive_path", ")", "except", "ArchiveError", ":", "continue", "match", "=", "archive", ".", "origin", "==", "origin", "and", "archive", ".", "backend_name", "==", "backend_name", "and", "archive", ".", "category", "==", "category", "and", "archive", ".", "created_on", ">=", "archived_after", "if", "not", "match", ":", "continue", "yield", "archive_path", ",", "archive", ".", "created_on"], "docstring": "Search archives using filters.", "docstring_tokens": ["Search", "archives", "using", "filters", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/archive.py#L440-L457", "partition": "test"}
{"repo": "reingart/gui2py", "path": "gui/doc/ext/autosummary/__init__.py", "func_name": "get_documenter", "original_string": "def get_documenter(obj, parent):\n    \"\"\"Get an autodoc.Documenter class suitable for documenting the given\n    object.\n\n    *obj* is the Python object to be documented, and *parent* is an\n    another Python object (e.g. a module or a class) to which *obj*\n    belongs to.\n    \"\"\"\n    from sphinx.ext.autodoc import AutoDirective, DataDocumenter, \\\n         ModuleDocumenter\n\n    if inspect.ismodule(obj):\n        # ModuleDocumenter.can_document_member always returns False\n        return ModuleDocumenter\n\n    # Construct a fake documenter for *parent*\n    if parent is not None:\n        parent_doc_cls = get_documenter(parent, None)\n    else:\n        parent_doc_cls = ModuleDocumenter\n\n    if hasattr(parent, '__name__'):\n        parent_doc = parent_doc_cls(FakeDirective(), parent.__name__)\n    else:\n        parent_doc = parent_doc_cls(FakeDirective(), \"\")\n\n    # Get the corrent documenter class for *obj*\n    classes = [cls for cls in AutoDirective._registry.values()\n               if cls.can_document_member(obj, '', False, parent_doc)]\n    if classes:\n        classes.sort(key=lambda cls: cls.priority)\n        return classes[-1]\n    else:\n        return DataDocumenter", "language": "python", "code": "def get_documenter(obj, parent):\n    \"\"\"Get an autodoc.Documenter class suitable for documenting the given\n    object.\n\n    *obj* is the Python object to be documented, and *parent* is an\n    another Python object (e.g. a module or a class) to which *obj*\n    belongs to.\n    \"\"\"\n    from sphinx.ext.autodoc import AutoDirective, DataDocumenter, \\\n         ModuleDocumenter\n\n    if inspect.ismodule(obj):\n        # ModuleDocumenter.can_document_member always returns False\n        return ModuleDocumenter\n\n    # Construct a fake documenter for *parent*\n    if parent is not None:\n        parent_doc_cls = get_documenter(parent, None)\n    else:\n        parent_doc_cls = ModuleDocumenter\n\n    if hasattr(parent, '__name__'):\n        parent_doc = parent_doc_cls(FakeDirective(), parent.__name__)\n    else:\n        parent_doc = parent_doc_cls(FakeDirective(), \"\")\n\n    # Get the corrent documenter class for *obj*\n    classes = [cls for cls in AutoDirective._registry.values()\n               if cls.can_document_member(obj, '', False, parent_doc)]\n    if classes:\n        classes.sort(key=lambda cls: cls.priority)\n        return classes[-1]\n    else:\n        return DataDocumenter", "code_tokens": ["def", "get_documenter", "(", "obj", ",", "parent", ")", ":", "from", "sphinx", ".", "ext", ".", "autodoc", "import", "AutoDirective", ",", "DataDocumenter", ",", "ModuleDocumenter", "if", "inspect", ".", "ismodule", "(", "obj", ")", ":", "# ModuleDocumenter.can_document_member always returns False", "return", "ModuleDocumenter", "# Construct a fake documenter for *parent*", "if", "parent", "is", "not", "None", ":", "parent_doc_cls", "=", "get_documenter", "(", "parent", ",", "None", ")", "else", ":", "parent_doc_cls", "=", "ModuleDocumenter", "if", "hasattr", "(", "parent", ",", "'__name__'", ")", ":", "parent_doc", "=", "parent_doc_cls", "(", "FakeDirective", "(", ")", ",", "parent", ".", "__name__", ")", "else", ":", "parent_doc", "=", "parent_doc_cls", "(", "FakeDirective", "(", ")", ",", "\"\"", ")", "# Get the corrent documenter class for *obj*", "classes", "=", "[", "cls", "for", "cls", "in", "AutoDirective", ".", "_registry", ".", "values", "(", ")", "if", "cls", ".", "can_document_member", "(", "obj", ",", "''", ",", "False", ",", "parent_doc", ")", "]", "if", "classes", ":", "classes", ".", "sort", "(", "key", "=", "lambda", "cls", ":", "cls", ".", "priority", ")", "return", "classes", "[", "-", "1", "]", "else", ":", "return", "DataDocumenter"], "docstring": "Get an autodoc.Documenter class suitable for documenting the given\n    object.\n\n    *obj* is the Python object to be documented, and *parent* is an\n    another Python object (e.g. a module or a class) to which *obj*\n    belongs to.", "docstring_tokens": ["Get", "an", "autodoc", ".", "Documenter", "class", "suitable", "for", "documenting", "the", "given", "object", "."], "sha": "aca0a05f6fcde55c94ad7cc058671a06608b01a4", "url": "https://github.com/reingart/gui2py/blob/aca0a05f6fcde55c94ad7cc058671a06608b01a4/gui/doc/ext/autosummary/__init__.py#L132-L165", "partition": "test"}
{"repo": "google/gin-config", "path": "gin/config.py", "func_name": "parse_config_file", "original_string": "def parse_config_file(config_file, skip_unknown=False):\n  \"\"\"Parse a Gin config file.\n\n  Args:\n    config_file: The path to a Gin config file.\n    skip_unknown: A boolean indicating whether unknown configurables and imports\n      should be skipped instead of causing errors (alternatively a list of\n      configurable names to skip if unknown). See `parse_config` for additional\n      details.\n\n  Raises:\n    IOError: If `config_file` cannot be read using any register file reader.\n  \"\"\"\n  for reader, existence_check in _FILE_READERS:\n    if existence_check(config_file):\n      with reader(config_file) as f:\n        parse_config(f, skip_unknown=skip_unknown)\n        return\n  raise IOError('Unable to open file: {}'.format(config_file))", "language": "python", "code": "def parse_config_file(config_file, skip_unknown=False):\n  \"\"\"Parse a Gin config file.\n\n  Args:\n    config_file: The path to a Gin config file.\n    skip_unknown: A boolean indicating whether unknown configurables and imports\n      should be skipped instead of causing errors (alternatively a list of\n      configurable names to skip if unknown). See `parse_config` for additional\n      details.\n\n  Raises:\n    IOError: If `config_file` cannot be read using any register file reader.\n  \"\"\"\n  for reader, existence_check in _FILE_READERS:\n    if existence_check(config_file):\n      with reader(config_file) as f:\n        parse_config(f, skip_unknown=skip_unknown)\n        return\n  raise IOError('Unable to open file: {}'.format(config_file))", "code_tokens": ["def", "parse_config_file", "(", "config_file", ",", "skip_unknown", "=", "False", ")", ":", "for", "reader", ",", "existence_check", "in", "_FILE_READERS", ":", "if", "existence_check", "(", "config_file", ")", ":", "with", "reader", "(", "config_file", ")", "as", "f", ":", "parse_config", "(", "f", ",", "skip_unknown", "=", "skip_unknown", ")", "return", "raise", "IOError", "(", "'Unable to open file: {}'", ".", "format", "(", "config_file", ")", ")"], "docstring": "Parse a Gin config file.\n\n  Args:\n    config_file: The path to a Gin config file.\n    skip_unknown: A boolean indicating whether unknown configurables and imports\n      should be skipped instead of causing errors (alternatively a list of\n      configurable names to skip if unknown). See `parse_config` for additional\n      details.\n\n  Raises:\n    IOError: If `config_file` cannot be read using any register file reader.", "docstring_tokens": ["Parse", "a", "Gin", "config", "file", "."], "sha": "17a170e0a6711005d1c78e67cf493dc44674d44f", "url": "https://github.com/google/gin-config/blob/17a170e0a6711005d1c78e67cf493dc44674d44f/gin/config.py#L1420-L1438", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/jobs.py", "func_name": "BackfillJob._execute_for_run_dates", "original_string": "def _execute_for_run_dates(self, run_dates, ti_status, executor, pickle_id,\n                               start_date, session=None):\n        \"\"\"\n        Computes the dag runs and their respective task instances for\n        the given run dates and executes the task instances.\n        Returns a list of execution dates of the dag runs that were executed.\n\n        :param run_dates: Execution dates for dag runs\n        :type run_dates: list\n        :param ti_status: internal BackfillJob status structure to tis track progress\n        :type ti_status: BackfillJob._DagRunTaskStatus\n        :param executor: the executor to use, it must be previously started\n        :type executor: BaseExecutor\n        :param pickle_id: numeric id of the pickled dag, None if not pickled\n        :type pickle_id: int\n        :param start_date: backfill start date\n        :type start_date: datetime.datetime\n        :param session: the current session object\n        :type session: sqlalchemy.orm.session.Session\n        \"\"\"\n        for next_run_date in run_dates:\n            dag_run = self._get_dag_run(next_run_date, session=session)\n            tis_map = self._task_instances_for_dag_run(dag_run,\n                                                       session=session)\n            if dag_run is None:\n                continue\n\n            ti_status.active_runs.append(dag_run)\n            ti_status.to_run.update(tis_map or {})\n\n        processed_dag_run_dates = self._process_backfill_task_instances(\n            ti_status=ti_status,\n            executor=executor,\n            pickle_id=pickle_id,\n            start_date=start_date,\n            session=session)\n\n        ti_status.executed_dag_run_dates.update(processed_dag_run_dates)", "language": "python", "code": "def _execute_for_run_dates(self, run_dates, ti_status, executor, pickle_id,\n                               start_date, session=None):\n        \"\"\"\n        Computes the dag runs and their respective task instances for\n        the given run dates and executes the task instances.\n        Returns a list of execution dates of the dag runs that were executed.\n\n        :param run_dates: Execution dates for dag runs\n        :type run_dates: list\n        :param ti_status: internal BackfillJob status structure to tis track progress\n        :type ti_status: BackfillJob._DagRunTaskStatus\n        :param executor: the executor to use, it must be previously started\n        :type executor: BaseExecutor\n        :param pickle_id: numeric id of the pickled dag, None if not pickled\n        :type pickle_id: int\n        :param start_date: backfill start date\n        :type start_date: datetime.datetime\n        :param session: the current session object\n        :type session: sqlalchemy.orm.session.Session\n        \"\"\"\n        for next_run_date in run_dates:\n            dag_run = self._get_dag_run(next_run_date, session=session)\n            tis_map = self._task_instances_for_dag_run(dag_run,\n                                                       session=session)\n            if dag_run is None:\n                continue\n\n            ti_status.active_runs.append(dag_run)\n            ti_status.to_run.update(tis_map or {})\n\n        processed_dag_run_dates = self._process_backfill_task_instances(\n            ti_status=ti_status,\n            executor=executor,\n            pickle_id=pickle_id,\n            start_date=start_date,\n            session=session)\n\n        ti_status.executed_dag_run_dates.update(processed_dag_run_dates)", "code_tokens": ["def", "_execute_for_run_dates", "(", "self", ",", "run_dates", ",", "ti_status", ",", "executor", ",", "pickle_id", ",", "start_date", ",", "session", "=", "None", ")", ":", "for", "next_run_date", "in", "run_dates", ":", "dag_run", "=", "self", ".", "_get_dag_run", "(", "next_run_date", ",", "session", "=", "session", ")", "tis_map", "=", "self", ".", "_task_instances_for_dag_run", "(", "dag_run", ",", "session", "=", "session", ")", "if", "dag_run", "is", "None", ":", "continue", "ti_status", ".", "active_runs", ".", "append", "(", "dag_run", ")", "ti_status", ".", "to_run", ".", "update", "(", "tis_map", "or", "{", "}", ")", "processed_dag_run_dates", "=", "self", ".", "_process_backfill_task_instances", "(", "ti_status", "=", "ti_status", ",", "executor", "=", "executor", ",", "pickle_id", "=", "pickle_id", ",", "start_date", "=", "start_date", ",", "session", "=", "session", ")", "ti_status", ".", "executed_dag_run_dates", ".", "update", "(", "processed_dag_run_dates", ")"], "docstring": "Computes the dag runs and their respective task instances for\n        the given run dates and executes the task instances.\n        Returns a list of execution dates of the dag runs that were executed.\n\n        :param run_dates: Execution dates for dag runs\n        :type run_dates: list\n        :param ti_status: internal BackfillJob status structure to tis track progress\n        :type ti_status: BackfillJob._DagRunTaskStatus\n        :param executor: the executor to use, it must be previously started\n        :type executor: BaseExecutor\n        :param pickle_id: numeric id of the pickled dag, None if not pickled\n        :type pickle_id: int\n        :param start_date: backfill start date\n        :type start_date: datetime.datetime\n        :param session: the current session object\n        :type session: sqlalchemy.orm.session.Session", "docstring_tokens": ["Computes", "the", "dag", "runs", "and", "their", "respective", "task", "instances", "for", "the", "given", "run", "dates", "and", "executes", "the", "task", "instances", ".", "Returns", "a", "list", "of", "execution", "dates", "of", "the", "dag", "runs", "that", "were", "executed", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L2405-L2442", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/simulator/simModel.py", "func_name": "mkUpdater", "original_string": "def mkUpdater(nextVal: Value, invalidate: bool):\n    \"\"\"\n    Create value updater for simulation\n\n    :param nextVal: instance of Value which will be asssiggned to signal\n    :param invalidate: flag which tells if value has been compromised\n        and if it should be invaidated\n    :return: function(value) -> tuple(valueHasChangedFlag, nextVal)\n    \"\"\"\n\n    def updater(currentVal):\n        _nextVal = nextVal.clone()\n        if invalidate:\n            _nextVal.vldMask = 0\n        return (valueHasChanged(currentVal, _nextVal), _nextVal)\n    return updater", "language": "python", "code": "def mkUpdater(nextVal: Value, invalidate: bool):\n    \"\"\"\n    Create value updater for simulation\n\n    :param nextVal: instance of Value which will be asssiggned to signal\n    :param invalidate: flag which tells if value has been compromised\n        and if it should be invaidated\n    :return: function(value) -> tuple(valueHasChangedFlag, nextVal)\n    \"\"\"\n\n    def updater(currentVal):\n        _nextVal = nextVal.clone()\n        if invalidate:\n            _nextVal.vldMask = 0\n        return (valueHasChanged(currentVal, _nextVal), _nextVal)\n    return updater", "code_tokens": ["def", "mkUpdater", "(", "nextVal", ":", "Value", ",", "invalidate", ":", "bool", ")", ":", "def", "updater", "(", "currentVal", ")", ":", "_nextVal", "=", "nextVal", ".", "clone", "(", ")", "if", "invalidate", ":", "_nextVal", ".", "vldMask", "=", "0", "return", "(", "valueHasChanged", "(", "currentVal", ",", "_nextVal", ")", ",", "_nextVal", ")", "return", "updater"], "docstring": "Create value updater for simulation\n\n    :param nextVal: instance of Value which will be asssiggned to signal\n    :param invalidate: flag which tells if value has been compromised\n        and if it should be invaidated\n    :return: function(value) -> tuple(valueHasChangedFlag, nextVal)", "docstring_tokens": ["Create", "value", "updater", "for", "simulation"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/simulator/simModel.py#L77-L92", "partition": "test"}
{"repo": "Music-Moo/music2storage", "path": "music2storage/connection.py", "func_name": "ConnectionHandler.use_storage_service", "original_string": "def use_storage_service(self, service_name, custom_path):\n        \"\"\"\n        Sets the current storage service to service_name and runs the connect method on the service.\n\n        :param str service_name: Name of the storage service\n        :param str custom_path: Custom path where to download tracks for local storage (optional, and must already exist, use absolute paths only)\n        \"\"\"\n\n        try:\n            self.current_storage = self.storage_services[service_name]\n        except KeyError:\n            if service_name == 'google drive':\n                self.storage_services['google drive'] = GoogleDrive()\n                self.current_storage = self.storage_services['google drive']\n                self.current_storage.connect()\n            elif service_name == 'dropbox':\n                log.error('Dropbox is not supported yet.')\n            elif service_name == 'local':\n                self.storage_services['local'] = LocalStorage(custom_path=custom_path)\n                self.current_storage = self.storage_services['local']\n                self.current_storage.connect()\n            else:\n                log.error('Storage service name is not recognized.')", "language": "python", "code": "def use_storage_service(self, service_name, custom_path):\n        \"\"\"\n        Sets the current storage service to service_name and runs the connect method on the service.\n\n        :param str service_name: Name of the storage service\n        :param str custom_path: Custom path where to download tracks for local storage (optional, and must already exist, use absolute paths only)\n        \"\"\"\n\n        try:\n            self.current_storage = self.storage_services[service_name]\n        except KeyError:\n            if service_name == 'google drive':\n                self.storage_services['google drive'] = GoogleDrive()\n                self.current_storage = self.storage_services['google drive']\n                self.current_storage.connect()\n            elif service_name == 'dropbox':\n                log.error('Dropbox is not supported yet.')\n            elif service_name == 'local':\n                self.storage_services['local'] = LocalStorage(custom_path=custom_path)\n                self.current_storage = self.storage_services['local']\n                self.current_storage.connect()\n            else:\n                log.error('Storage service name is not recognized.')", "code_tokens": ["def", "use_storage_service", "(", "self", ",", "service_name", ",", "custom_path", ")", ":", "try", ":", "self", ".", "current_storage", "=", "self", ".", "storage_services", "[", "service_name", "]", "except", "KeyError", ":", "if", "service_name", "==", "'google drive'", ":", "self", ".", "storage_services", "[", "'google drive'", "]", "=", "GoogleDrive", "(", ")", "self", ".", "current_storage", "=", "self", ".", "storage_services", "[", "'google drive'", "]", "self", ".", "current_storage", ".", "connect", "(", ")", "elif", "service_name", "==", "'dropbox'", ":", "log", ".", "error", "(", "'Dropbox is not supported yet.'", ")", "elif", "service_name", "==", "'local'", ":", "self", ".", "storage_services", "[", "'local'", "]", "=", "LocalStorage", "(", "custom_path", "=", "custom_path", ")", "self", ".", "current_storage", "=", "self", ".", "storage_services", "[", "'local'", "]", "self", ".", "current_storage", ".", "connect", "(", ")", "else", ":", "log", ".", "error", "(", "'Storage service name is not recognized.'", ")"], "docstring": "Sets the current storage service to service_name and runs the connect method on the service.\n\n        :param str service_name: Name of the storage service\n        :param str custom_path: Custom path where to download tracks for local storage (optional, and must already exist, use absolute paths only)", "docstring_tokens": ["Sets", "the", "current", "storage", "service", "to", "service_name", "and", "runs", "the", "connect", "method", "on", "the", "service", "."], "sha": "de12b9046dd227fc8c1512b5060e7f5fcd8b0ee2", "url": "https://github.com/Music-Moo/music2storage/blob/de12b9046dd227fc8c1512b5060e7f5fcd8b0ee2/music2storage/connection.py#L36-L58", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/synthesizer/interfaceLevel/unitImplHelpers.py", "func_name": "UnitImplHelpers._cleanAsSubunit", "original_string": "def _cleanAsSubunit(self):\n        \"\"\"Disconnect internal signals so unit can be reused by parent unit\"\"\"\n        for pi in self._entity.ports:\n            pi.connectInternSig()\n        for i in chain(self._interfaces, self._private_interfaces):\n            i._clean()", "language": "python", "code": "def _cleanAsSubunit(self):\n        \"\"\"Disconnect internal signals so unit can be reused by parent unit\"\"\"\n        for pi in self._entity.ports:\n            pi.connectInternSig()\n        for i in chain(self._interfaces, self._private_interfaces):\n            i._clean()", "code_tokens": ["def", "_cleanAsSubunit", "(", "self", ")", ":", "for", "pi", "in", "self", ".", "_entity", ".", "ports", ":", "pi", ".", "connectInternSig", "(", ")", "for", "i", "in", "chain", "(", "self", ".", "_interfaces", ",", "self", ".", "_private_interfaces", ")", ":", "i", ".", "_clean", "(", ")"], "docstring": "Disconnect internal signals so unit can be reused by parent unit", "docstring_tokens": ["Disconnect", "internal", "signals", "so", "unit", "can", "be", "reused", "by", "parent", "unit"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/synthesizer/interfaceLevel/unitImplHelpers.py#L109-L114", "partition": "test"}
{"repo": "bolt-project/bolt", "path": "bolt/spark/chunk.py", "func_name": "ChunkedArray.getmask", "original_string": "def getmask(inds, n):\n        \"\"\"\n        Obtain a binary mask by setting a subset of entries to true.\n\n        Parameters\n        ----------\n        inds : array-like\n            Which indices to set as true.\n\n        n : int\n            The length of the target mask.\n        \"\"\"\n        inds = asarray(inds, 'int')\n        mask = zeros(n, dtype=bool)\n        mask[inds] = True\n        return mask", "language": "python", "code": "def getmask(inds, n):\n        \"\"\"\n        Obtain a binary mask by setting a subset of entries to true.\n\n        Parameters\n        ----------\n        inds : array-like\n            Which indices to set as true.\n\n        n : int\n            The length of the target mask.\n        \"\"\"\n        inds = asarray(inds, 'int')\n        mask = zeros(n, dtype=bool)\n        mask[inds] = True\n        return mask", "code_tokens": ["def", "getmask", "(", "inds", ",", "n", ")", ":", "inds", "=", "asarray", "(", "inds", ",", "'int'", ")", "mask", "=", "zeros", "(", "n", ",", "dtype", "=", "bool", ")", "mask", "[", "inds", "]", "=", "True", "return", "mask"], "docstring": "Obtain a binary mask by setting a subset of entries to true.\n\n        Parameters\n        ----------\n        inds : array-like\n            Which indices to set as true.\n\n        n : int\n            The length of the target mask.", "docstring_tokens": ["Obtain", "a", "binary", "mask", "by", "setting", "a", "subset", "of", "entries", "to", "true", "."], "sha": "9cd7104aa085498da3097b72696184b9d3651c51", "url": "https://github.com/bolt-project/bolt/blob/9cd7104aa085498da3097b72696184b9d3651c51/bolt/spark/chunk.py#L621-L636", "partition": "test"}
{"repo": "openvenues/pypostal", "path": "postal/near_dupe.py", "func_name": "near_dupe_hashes", "original_string": "def near_dupe_hashes(labels, values, languages=None, **kw):\n    \"\"\"\n    Hash the given address into normalized strings that can be used to group similar\n    addresses together for more detailed pairwise comparison. This can be thought of\n    as the blocking function in record linkage or locally-sensitive hashing in the\n    document near-duplicate detection.\n\n    Required\n    --------\n    @param labels: array of component labels as either Unicode or UTF-8 encoded strings\n                   e.g. [\"house_number\", \"road\", \"postcode\"]\n    @param values: array of component values as either Unicode or UTF-8 encoded strings\n                   e.g. [\"123\", \"Broadway\", \"11216\"]. Note len(values) must be equal to\n                   len(labels).\n\n    Options\n    -------\n    @param languages: a tuple or list of ISO language code strings (e.g. \"en\", \"fr\", \"de\", etc.)\n                      to use in expansion. If None is passed, use language classifier\n                      to detect language automatically.\n    @param with_name: use name in the hashes\n    @param with_address: use house_number & street in the hashes\n    @param with_unit: use secondary unit as part of the hashes\n    @param with_city_or_equivalent: use the city, city_district, suburb, or island name as one of\n                                    the geo qualifiers\n    @param with_small_containing_boundaries: use small containing boundaries (currently state_district)\n                                             as one of the geo qualifiers\n    @param with_postal_code: use postal code as one of the geo qualifiers\n    @param with_latlon: use geohash + neighbors as one of the geo qualifiers\n    @param latitude: latitude (Y coordinate)\n    @param longitude: longitude (X coordinate)\n    @param geohash_precision: geohash tile size (default = 6)\n    @param name_and_address_keys: include keys with name + address + geo\n    @param name_only_keys: include keys with name + geo\n    @param address_only_keys: include keys with address + geo\n    \"\"\"\n    return _near_dupe.near_dupe_hashes(labels, values, languages=languages, **kw)", "language": "python", "code": "def near_dupe_hashes(labels, values, languages=None, **kw):\n    \"\"\"\n    Hash the given address into normalized strings that can be used to group similar\n    addresses together for more detailed pairwise comparison. This can be thought of\n    as the blocking function in record linkage or locally-sensitive hashing in the\n    document near-duplicate detection.\n\n    Required\n    --------\n    @param labels: array of component labels as either Unicode or UTF-8 encoded strings\n                   e.g. [\"house_number\", \"road\", \"postcode\"]\n    @param values: array of component values as either Unicode or UTF-8 encoded strings\n                   e.g. [\"123\", \"Broadway\", \"11216\"]. Note len(values) must be equal to\n                   len(labels).\n\n    Options\n    -------\n    @param languages: a tuple or list of ISO language code strings (e.g. \"en\", \"fr\", \"de\", etc.)\n                      to use in expansion. If None is passed, use language classifier\n                      to detect language automatically.\n    @param with_name: use name in the hashes\n    @param with_address: use house_number & street in the hashes\n    @param with_unit: use secondary unit as part of the hashes\n    @param with_city_or_equivalent: use the city, city_district, suburb, or island name as one of\n                                    the geo qualifiers\n    @param with_small_containing_boundaries: use small containing boundaries (currently state_district)\n                                             as one of the geo qualifiers\n    @param with_postal_code: use postal code as one of the geo qualifiers\n    @param with_latlon: use geohash + neighbors as one of the geo qualifiers\n    @param latitude: latitude (Y coordinate)\n    @param longitude: longitude (X coordinate)\n    @param geohash_precision: geohash tile size (default = 6)\n    @param name_and_address_keys: include keys with name + address + geo\n    @param name_only_keys: include keys with name + geo\n    @param address_only_keys: include keys with address + geo\n    \"\"\"\n    return _near_dupe.near_dupe_hashes(labels, values, languages=languages, **kw)", "code_tokens": ["def", "near_dupe_hashes", "(", "labels", ",", "values", ",", "languages", "=", "None", ",", "*", "*", "kw", ")", ":", "return", "_near_dupe", ".", "near_dupe_hashes", "(", "labels", ",", "values", ",", "languages", "=", "languages", ",", "*", "*", "kw", ")"], "docstring": "Hash the given address into normalized strings that can be used to group similar\n    addresses together for more detailed pairwise comparison. This can be thought of\n    as the blocking function in record linkage or locally-sensitive hashing in the\n    document near-duplicate detection.\n\n    Required\n    --------\n    @param labels: array of component labels as either Unicode or UTF-8 encoded strings\n                   e.g. [\"house_number\", \"road\", \"postcode\"]\n    @param values: array of component values as either Unicode or UTF-8 encoded strings\n                   e.g. [\"123\", \"Broadway\", \"11216\"]. Note len(values) must be equal to\n                   len(labels).\n\n    Options\n    -------\n    @param languages: a tuple or list of ISO language code strings (e.g. \"en\", \"fr\", \"de\", etc.)\n                      to use in expansion. If None is passed, use language classifier\n                      to detect language automatically.\n    @param with_name: use name in the hashes\n    @param with_address: use house_number & street in the hashes\n    @param with_unit: use secondary unit as part of the hashes\n    @param with_city_or_equivalent: use the city, city_district, suburb, or island name as one of\n                                    the geo qualifiers\n    @param with_small_containing_boundaries: use small containing boundaries (currently state_district)\n                                             as one of the geo qualifiers\n    @param with_postal_code: use postal code as one of the geo qualifiers\n    @param with_latlon: use geohash + neighbors as one of the geo qualifiers\n    @param latitude: latitude (Y coordinate)\n    @param longitude: longitude (X coordinate)\n    @param geohash_precision: geohash tile size (default = 6)\n    @param name_and_address_keys: include keys with name + address + geo\n    @param name_only_keys: include keys with name + geo\n    @param address_only_keys: include keys with address + geo", "docstring_tokens": ["Hash", "the", "given", "address", "into", "normalized", "strings", "that", "can", "be", "used", "to", "group", "similar", "addresses", "together", "for", "more", "detailed", "pairwise", "comparison", ".", "This", "can", "be", "thought", "of", "as", "the", "blocking", "function", "in", "record", "linkage", "or", "locally", "-", "sensitive", "hashing", "in", "the", "document", "near", "-", "duplicate", "detection", "."], "sha": "1c0fd96b5e2463b7015cd3625ac276db520c69fe", "url": "https://github.com/openvenues/pypostal/blob/1c0fd96b5e2463b7015cd3625ac276db520c69fe/postal/near_dupe.py#L6-L42", "partition": "test"}
{"repo": "ashleysommer/sanic-cors", "path": "sanic_cors/decorator.py", "func_name": "cross_origin", "original_string": "def cross_origin(app, *args, **kwargs):\n    \"\"\"\n    This function is the decorator which is used to wrap a Sanic route with.\n    In the simplest case, simply use the default parameters to allow all\n    origins in what is the most permissive configuration. If this method\n    modifies state or performs authentication which may be brute-forced, you\n    should add some degree of protection, such as Cross Site Forgery\n    Request protection.\n\n    :param origins:\n        The origin, or list of origins to allow requests from.\n        The origin(s) may be regular expressions, case-sensitive strings,\n        or else an asterisk\n\n        Default : '*'\n    :type origins: list, string or regex\n\n    :param methods:\n        The method or list of methods which the allowed origins are allowed to\n        access for non-simple requests.\n\n        Default : [GET, HEAD, POST, OPTIONS, PUT, PATCH, DELETE]\n    :type methods: list or string\n\n    :param expose_headers:\n        The header or list which are safe to expose to the API of a CORS API\n        specification.\n\n        Default : None\n    :type expose_headers: list or string\n\n    :param allow_headers:\n        The header or list of header field names which can be used when this\n        resource is accessed by allowed origins. The header(s) may be regular\n        expressions, case-sensitive strings, or else an asterisk.\n\n        Default : '*', allow all headers\n    :type allow_headers: list, string or regex\n\n    :param supports_credentials:\n        Allows users to make authenticated requests. If true, injects the\n        `Access-Control-Allow-Credentials` header in responses. This allows\n        cookies and credentials to be submitted across domains.\n\n        :note: This option cannot be used in conjuction with a '*' origin\n\n        Default : False\n    :type supports_credentials: bool\n\n    :param max_age:\n        The maximum time for which this CORS request maybe cached. This value\n        is set as the `Access-Control-Max-Age` header.\n\n        Default : None\n    :type max_age: timedelta, integer, string or None\n\n    :param send_wildcard: If True, and the origins parameter is `*`, a wildcard\n        `Access-Control-Allow-Origin` header is sent, rather than the\n        request's `Origin` header.\n\n        Default : False\n    :type send_wildcard: bool\n\n    :param vary_header:\n        If True, the header Vary: Origin will be returned as per the W3\n        implementation guidelines.\n\n        Setting this header when the `Access-Control-Allow-Origin` is\n        dynamically generated (e.g. when there is more than one allowed\n        origin, and an Origin than '*' is returned) informs CDNs and other\n        caches that the CORS headers are dynamic, and cannot be cached.\n\n        If False, the Vary header will never be injected or altered.\n\n        Default : True\n    :type vary_header: bool\n\n    :param automatic_options:\n        Only applies to the `cross_origin` decorator. If True, Sanic-CORS will\n        override Sanic's default OPTIONS handling to return CORS headers for\n        OPTIONS requests.\n\n        Default : True\n    :type automatic_options: bool\n\n    \"\"\"\n    _options = kwargs\n    _real_decorator = cors.decorate(app, *args, run_middleware=False, with_context=False, **kwargs)\n\n    def wrapped_decorator(f):\n        spf = SanicPluginsFramework(app)  # get the singleton from the app\n        try:\n            plugin = spf.register_plugin(cors, skip_reg=True)\n        except ValueError as e:\n            # this is normal, if this plugin has been registered previously\n            assert e.args and len(e.args) > 1\n            plugin = e.args[1]\n        context = cors.get_context_from_spf(spf)\n        log = context.log\n        log(logging.DEBUG, \"Enabled {:s} for cross_origin using options: {}\".format(str(f), str(_options)))\n        return _real_decorator(f)\n\n    return wrapped_decorator", "language": "python", "code": "def cross_origin(app, *args, **kwargs):\n    \"\"\"\n    This function is the decorator which is used to wrap a Sanic route with.\n    In the simplest case, simply use the default parameters to allow all\n    origins in what is the most permissive configuration. If this method\n    modifies state or performs authentication which may be brute-forced, you\n    should add some degree of protection, such as Cross Site Forgery\n    Request protection.\n\n    :param origins:\n        The origin, or list of origins to allow requests from.\n        The origin(s) may be regular expressions, case-sensitive strings,\n        or else an asterisk\n\n        Default : '*'\n    :type origins: list, string or regex\n\n    :param methods:\n        The method or list of methods which the allowed origins are allowed to\n        access for non-simple requests.\n\n        Default : [GET, HEAD, POST, OPTIONS, PUT, PATCH, DELETE]\n    :type methods: list or string\n\n    :param expose_headers:\n        The header or list which are safe to expose to the API of a CORS API\n        specification.\n\n        Default : None\n    :type expose_headers: list or string\n\n    :param allow_headers:\n        The header or list of header field names which can be used when this\n        resource is accessed by allowed origins. The header(s) may be regular\n        expressions, case-sensitive strings, or else an asterisk.\n\n        Default : '*', allow all headers\n    :type allow_headers: list, string or regex\n\n    :param supports_credentials:\n        Allows users to make authenticated requests. If true, injects the\n        `Access-Control-Allow-Credentials` header in responses. This allows\n        cookies and credentials to be submitted across domains.\n\n        :note: This option cannot be used in conjuction with a '*' origin\n\n        Default : False\n    :type supports_credentials: bool\n\n    :param max_age:\n        The maximum time for which this CORS request maybe cached. This value\n        is set as the `Access-Control-Max-Age` header.\n\n        Default : None\n    :type max_age: timedelta, integer, string or None\n\n    :param send_wildcard: If True, and the origins parameter is `*`, a wildcard\n        `Access-Control-Allow-Origin` header is sent, rather than the\n        request's `Origin` header.\n\n        Default : False\n    :type send_wildcard: bool\n\n    :param vary_header:\n        If True, the header Vary: Origin will be returned as per the W3\n        implementation guidelines.\n\n        Setting this header when the `Access-Control-Allow-Origin` is\n        dynamically generated (e.g. when there is more than one allowed\n        origin, and an Origin than '*' is returned) informs CDNs and other\n        caches that the CORS headers are dynamic, and cannot be cached.\n\n        If False, the Vary header will never be injected or altered.\n\n        Default : True\n    :type vary_header: bool\n\n    :param automatic_options:\n        Only applies to the `cross_origin` decorator. If True, Sanic-CORS will\n        override Sanic's default OPTIONS handling to return CORS headers for\n        OPTIONS requests.\n\n        Default : True\n    :type automatic_options: bool\n\n    \"\"\"\n    _options = kwargs\n    _real_decorator = cors.decorate(app, *args, run_middleware=False, with_context=False, **kwargs)\n\n    def wrapped_decorator(f):\n        spf = SanicPluginsFramework(app)  # get the singleton from the app\n        try:\n            plugin = spf.register_plugin(cors, skip_reg=True)\n        except ValueError as e:\n            # this is normal, if this plugin has been registered previously\n            assert e.args and len(e.args) > 1\n            plugin = e.args[1]\n        context = cors.get_context_from_spf(spf)\n        log = context.log\n        log(logging.DEBUG, \"Enabled {:s} for cross_origin using options: {}\".format(str(f), str(_options)))\n        return _real_decorator(f)\n\n    return wrapped_decorator", "code_tokens": ["def", "cross_origin", "(", "app", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "_options", "=", "kwargs", "_real_decorator", "=", "cors", ".", "decorate", "(", "app", ",", "*", "args", ",", "run_middleware", "=", "False", ",", "with_context", "=", "False", ",", "*", "*", "kwargs", ")", "def", "wrapped_decorator", "(", "f", ")", ":", "spf", "=", "SanicPluginsFramework", "(", "app", ")", "# get the singleton from the app", "try", ":", "plugin", "=", "spf", ".", "register_plugin", "(", "cors", ",", "skip_reg", "=", "True", ")", "except", "ValueError", "as", "e", ":", "# this is normal, if this plugin has been registered previously", "assert", "e", ".", "args", "and", "len", "(", "e", ".", "args", ")", ">", "1", "plugin", "=", "e", ".", "args", "[", "1", "]", "context", "=", "cors", ".", "get_context_from_spf", "(", "spf", ")", "log", "=", "context", ".", "log", "log", "(", "logging", ".", "DEBUG", ",", "\"Enabled {:s} for cross_origin using options: {}\"", ".", "format", "(", "str", "(", "f", ")", ",", "str", "(", "_options", ")", ")", ")", "return", "_real_decorator", "(", "f", ")", "return", "wrapped_decorator"], "docstring": "This function is the decorator which is used to wrap a Sanic route with.\n    In the simplest case, simply use the default parameters to allow all\n    origins in what is the most permissive configuration. If this method\n    modifies state or performs authentication which may be brute-forced, you\n    should add some degree of protection, such as Cross Site Forgery\n    Request protection.\n\n    :param origins:\n        The origin, or list of origins to allow requests from.\n        The origin(s) may be regular expressions, case-sensitive strings,\n        or else an asterisk\n\n        Default : '*'\n    :type origins: list, string or regex\n\n    :param methods:\n        The method or list of methods which the allowed origins are allowed to\n        access for non-simple requests.\n\n        Default : [GET, HEAD, POST, OPTIONS, PUT, PATCH, DELETE]\n    :type methods: list or string\n\n    :param expose_headers:\n        The header or list which are safe to expose to the API of a CORS API\n        specification.\n\n        Default : None\n    :type expose_headers: list or string\n\n    :param allow_headers:\n        The header or list of header field names which can be used when this\n        resource is accessed by allowed origins. The header(s) may be regular\n        expressions, case-sensitive strings, or else an asterisk.\n\n        Default : '*', allow all headers\n    :type allow_headers: list, string or regex\n\n    :param supports_credentials:\n        Allows users to make authenticated requests. If true, injects the\n        `Access-Control-Allow-Credentials` header in responses. This allows\n        cookies and credentials to be submitted across domains.\n\n        :note: This option cannot be used in conjuction with a '*' origin\n\n        Default : False\n    :type supports_credentials: bool\n\n    :param max_age:\n        The maximum time for which this CORS request maybe cached. This value\n        is set as the `Access-Control-Max-Age` header.\n\n        Default : None\n    :type max_age: timedelta, integer, string or None\n\n    :param send_wildcard: If True, and the origins parameter is `*`, a wildcard\n        `Access-Control-Allow-Origin` header is sent, rather than the\n        request's `Origin` header.\n\n        Default : False\n    :type send_wildcard: bool\n\n    :param vary_header:\n        If True, the header Vary: Origin will be returned as per the W3\n        implementation guidelines.\n\n        Setting this header when the `Access-Control-Allow-Origin` is\n        dynamically generated (e.g. when there is more than one allowed\n        origin, and an Origin than '*' is returned) informs CDNs and other\n        caches that the CORS headers are dynamic, and cannot be cached.\n\n        If False, the Vary header will never be injected or altered.\n\n        Default : True\n    :type vary_header: bool\n\n    :param automatic_options:\n        Only applies to the `cross_origin` decorator. If True, Sanic-CORS will\n        override Sanic's default OPTIONS handling to return CORS headers for\n        OPTIONS requests.\n\n        Default : True\n    :type automatic_options: bool", "docstring_tokens": ["This", "function", "is", "the", "decorator", "which", "is", "used", "to", "wrap", "a", "Sanic", "route", "with", ".", "In", "the", "simplest", "case", "simply", "use", "the", "default", "parameters", "to", "allow", "all", "origins", "in", "what", "is", "the", "most", "permissive", "configuration", ".", "If", "this", "method", "modifies", "state", "or", "performs", "authentication", "which", "may", "be", "brute", "-", "forced", "you", "should", "add", "some", "degree", "of", "protection", "such", "as", "Cross", "Site", "Forgery", "Request", "protection", "."], "sha": "f3d68def8cf859398b3c83e4109d815f1f038ea2", "url": "https://github.com/ashleysommer/sanic-cors/blob/f3d68def8cf859398b3c83e4109d815f1f038ea2/sanic_cors/decorator.py#L18-L120", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/utils/shared_utils.py", "func_name": "get_human_readable_bytes", "original_string": "def get_human_readable_bytes(size):\n    \"\"\"\n    Convert given number of bytes into a human readable representation, i.e. add prefix such as kb, Mb, Gb,\n    etc. The `size` argument must be a non-negative integer.\n\n    :param size: integer representing byte size of something\n    :return: string representation of the size, in human-readable form\n    \"\"\"\n    if size == 0: return \"0\"\n    if size is None: return \"\"\n    assert_is_type(size, int)\n    assert size >= 0, \"`size` cannot be negative, got %d\" % size\n    suffixes = \"PTGMk\"\n    maxl = len(suffixes)\n    for i in range(maxl + 1):\n        shift = (maxl - i) * 10\n        if size >> shift == 0: continue\n        ndigits = 0\n        for nd in [3, 2, 1]:\n            if size >> (shift + 12 - nd * 3) == 0:\n                ndigits = nd\n                break\n        if ndigits == 0 or size == (size >> shift) << shift:\n            rounded_val = str(size >> shift)\n        else:\n            rounded_val = \"%.*f\" % (ndigits, size / (1 << shift))\n        return \"%s %sb\" % (rounded_val, suffixes[i] if i < maxl else \"\")", "language": "python", "code": "def get_human_readable_bytes(size):\n    \"\"\"\n    Convert given number of bytes into a human readable representation, i.e. add prefix such as kb, Mb, Gb,\n    etc. The `size` argument must be a non-negative integer.\n\n    :param size: integer representing byte size of something\n    :return: string representation of the size, in human-readable form\n    \"\"\"\n    if size == 0: return \"0\"\n    if size is None: return \"\"\n    assert_is_type(size, int)\n    assert size >= 0, \"`size` cannot be negative, got %d\" % size\n    suffixes = \"PTGMk\"\n    maxl = len(suffixes)\n    for i in range(maxl + 1):\n        shift = (maxl - i) * 10\n        if size >> shift == 0: continue\n        ndigits = 0\n        for nd in [3, 2, 1]:\n            if size >> (shift + 12 - nd * 3) == 0:\n                ndigits = nd\n                break\n        if ndigits == 0 or size == (size >> shift) << shift:\n            rounded_val = str(size >> shift)\n        else:\n            rounded_val = \"%.*f\" % (ndigits, size / (1 << shift))\n        return \"%s %sb\" % (rounded_val, suffixes[i] if i < maxl else \"\")", "code_tokens": ["def", "get_human_readable_bytes", "(", "size", ")", ":", "if", "size", "==", "0", ":", "return", "\"0\"", "if", "size", "is", "None", ":", "return", "\"\"", "assert_is_type", "(", "size", ",", "int", ")", "assert", "size", ">=", "0", ",", "\"`size` cannot be negative, got %d\"", "%", "size", "suffixes", "=", "\"PTGMk\"", "maxl", "=", "len", "(", "suffixes", ")", "for", "i", "in", "range", "(", "maxl", "+", "1", ")", ":", "shift", "=", "(", "maxl", "-", "i", ")", "*", "10", "if", "size", ">>", "shift", "==", "0", ":", "continue", "ndigits", "=", "0", "for", "nd", "in", "[", "3", ",", "2", ",", "1", "]", ":", "if", "size", ">>", "(", "shift", "+", "12", "-", "nd", "*", "3", ")", "==", "0", ":", "ndigits", "=", "nd", "break", "if", "ndigits", "==", "0", "or", "size", "==", "(", "size", ">>", "shift", ")", "<<", "shift", ":", "rounded_val", "=", "str", "(", "size", ">>", "shift", ")", "else", ":", "rounded_val", "=", "\"%.*f\"", "%", "(", "ndigits", ",", "size", "/", "(", "1", "<<", "shift", ")", ")", "return", "\"%s %sb\"", "%", "(", "rounded_val", ",", "suffixes", "[", "i", "]", "if", "i", "<", "maxl", "else", "\"\"", ")"], "docstring": "Convert given number of bytes into a human readable representation, i.e. add prefix such as kb, Mb, Gb,\n    etc. The `size` argument must be a non-negative integer.\n\n    :param size: integer representing byte size of something\n    :return: string representation of the size, in human-readable form", "docstring_tokens": ["Convert", "given", "number", "of", "bytes", "into", "a", "human", "readable", "representation", "i", ".", "e", ".", "add", "prefix", "such", "as", "kb", "Mb", "Gb", "etc", ".", "The", "size", "argument", "must", "be", "a", "non", "-", "negative", "integer", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/utils/shared_utils.py#L253-L279", "partition": "test"}
{"repo": "buzzfeed/phonon", "path": "phonon/nodelist.py", "func_name": "Nodelist.get_last_updated", "original_string": "def get_last_updated(self, node_id=None):\n        \"\"\"\n        Returns the time a particular node has been last refreshed.\n\n        :param string node_id: optional, the connection id of the node to retrieve\n\n        :rtype: int\n        :returns: Returns a unix timestamp if it exists, otherwise None\n        \"\"\"\n        if not node_id:\n            node_id = self.conn.id\n\n        dt = self.conn.client.hget(self.nodelist_key, node_id)\n        return int(dt) if dt else None", "language": "python", "code": "def get_last_updated(self, node_id=None):\n        \"\"\"\n        Returns the time a particular node has been last refreshed.\n\n        :param string node_id: optional, the connection id of the node to retrieve\n\n        :rtype: int\n        :returns: Returns a unix timestamp if it exists, otherwise None\n        \"\"\"\n        if not node_id:\n            node_id = self.conn.id\n\n        dt = self.conn.client.hget(self.nodelist_key, node_id)\n        return int(dt) if dt else None", "code_tokens": ["def", "get_last_updated", "(", "self", ",", "node_id", "=", "None", ")", ":", "if", "not", "node_id", ":", "node_id", "=", "self", ".", "conn", ".", "id", "dt", "=", "self", ".", "conn", ".", "client", ".", "hget", "(", "self", ".", "nodelist_key", ",", "node_id", ")", "return", "int", "(", "dt", ")", "if", "dt", "else", "None"], "docstring": "Returns the time a particular node has been last refreshed.\n\n        :param string node_id: optional, the connection id of the node to retrieve\n\n        :rtype: int\n        :returns: Returns a unix timestamp if it exists, otherwise None", "docstring_tokens": ["Returns", "the", "time", "a", "particular", "node", "has", "been", "last", "refreshed", "."], "sha": "32fd036d64fab19c554e841f162466f6eb28b50f", "url": "https://github.com/buzzfeed/phonon/blob/32fd036d64fab19c554e841f162466f6eb28b50f/phonon/nodelist.py#L94-L107", "partition": "test"}
{"repo": "HumanBrainProject/hbp-service-client", "path": "hbp_service_client/storage_service/api.py", "func_name": "ApiClient.copy_file_content", "original_string": "def copy_file_content(self, file_id, source_file):\n        '''Copy file content from source file to target file.\n\n        Args:\n            file_id (str): The UUID of the file whose content is written.\n            source_file (str): The UUID of the file whose content is copied.\n\n        Returns:\n            None\n\n        Raises:\n        StorageArgumentException: Invalid arguments\n        StorageForbiddenException: Server response code 403\n        StorageNotFoundException: Server response code 404\n        StorageException: other 400-600 error codes\n        '''\n        if not is_valid_uuid(file_id):\n            raise StorageArgumentException(\n                'Invalid UUID for file_id: {0}'.format(file_id))\n\n        if not is_valid_uuid(source_file):\n            raise StorageArgumentException(\n                'Invalid UUID for source_file: {0}'.format(source_file))\n\n        self._authenticated_request \\\n            .to_endpoint('file/{}/content/'.format(file_id)) \\\n            .with_headers({'X-Copy-From': source_file}) \\\n            .put()", "language": "python", "code": "def copy_file_content(self, file_id, source_file):\n        '''Copy file content from source file to target file.\n\n        Args:\n            file_id (str): The UUID of the file whose content is written.\n            source_file (str): The UUID of the file whose content is copied.\n\n        Returns:\n            None\n\n        Raises:\n        StorageArgumentException: Invalid arguments\n        StorageForbiddenException: Server response code 403\n        StorageNotFoundException: Server response code 404\n        StorageException: other 400-600 error codes\n        '''\n        if not is_valid_uuid(file_id):\n            raise StorageArgumentException(\n                'Invalid UUID for file_id: {0}'.format(file_id))\n\n        if not is_valid_uuid(source_file):\n            raise StorageArgumentException(\n                'Invalid UUID for source_file: {0}'.format(source_file))\n\n        self._authenticated_request \\\n            .to_endpoint('file/{}/content/'.format(file_id)) \\\n            .with_headers({'X-Copy-From': source_file}) \\\n            .put()", "code_tokens": ["def", "copy_file_content", "(", "self", ",", "file_id", ",", "source_file", ")", ":", "if", "not", "is_valid_uuid", "(", "file_id", ")", ":", "raise", "StorageArgumentException", "(", "'Invalid UUID for file_id: {0}'", ".", "format", "(", "file_id", ")", ")", "if", "not", "is_valid_uuid", "(", "source_file", ")", ":", "raise", "StorageArgumentException", "(", "'Invalid UUID for source_file: {0}'", ".", "format", "(", "source_file", ")", ")", "self", ".", "_authenticated_request", ".", "to_endpoint", "(", "'file/{}/content/'", ".", "format", "(", "file_id", ")", ")", ".", "with_headers", "(", "{", "'X-Copy-From'", ":", "source_file", "}", ")", ".", "put", "(", ")"], "docstring": "Copy file content from source file to target file.\n\n        Args:\n            file_id (str): The UUID of the file whose content is written.\n            source_file (str): The UUID of the file whose content is copied.\n\n        Returns:\n            None\n\n        Raises:\n        StorageArgumentException: Invalid arguments\n        StorageForbiddenException: Server response code 403\n        StorageNotFoundException: Server response code 404\n        StorageException: other 400-600 error codes", "docstring_tokens": ["Copy", "file", "content", "from", "source", "file", "to", "target", "file", "."], "sha": "b338fb41a7f0e7b9d654ff28fcf13a56d03bff4d", "url": "https://github.com/HumanBrainProject/hbp-service-client/blob/b338fb41a7f0e7b9d654ff28fcf13a56d03bff4d/hbp_service_client/storage_service/api.py#L870-L897", "partition": "test"}
{"repo": "pmacosta/peng", "path": "peng/functions.py", "func_name": "peng_float", "original_string": "def peng_float(snum):\n    r\"\"\"\n    Return floating point equivalent of a number represented in engineering notation.\n\n    :param snum: Number\n    :type  snum: :ref:`EngineeringNotationNumber`\n\n    :rtype: string\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.functions.peng_float\n\n    :raises: RuntimeError (Argument \\`snum\\` is not valid)\n\n    .. [[[end]]]\n\n    For example:\n\n        >>> import peng\n        >>> peng.peng_float(peng.peng(1235.6789E3, 3, False))\n        1236000.0\n    \"\"\"\n    # This can be coded as peng_mant(snum)*(peng_power(snum)[1]), but the\n    # \"function unrolling\" is about 4x faster\n    snum = snum.rstrip()\n    power = _SUFFIX_POWER_DICT[\" \" if snum[-1].isdigit() else snum[-1]]\n    return float(snum if snum[-1].isdigit() else snum[:-1]) * power", "language": "python", "code": "def peng_float(snum):\n    r\"\"\"\n    Return floating point equivalent of a number represented in engineering notation.\n\n    :param snum: Number\n    :type  snum: :ref:`EngineeringNotationNumber`\n\n    :rtype: string\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.functions.peng_float\n\n    :raises: RuntimeError (Argument \\`snum\\` is not valid)\n\n    .. [[[end]]]\n\n    For example:\n\n        >>> import peng\n        >>> peng.peng_float(peng.peng(1235.6789E3, 3, False))\n        1236000.0\n    \"\"\"\n    # This can be coded as peng_mant(snum)*(peng_power(snum)[1]), but the\n    # \"function unrolling\" is about 4x faster\n    snum = snum.rstrip()\n    power = _SUFFIX_POWER_DICT[\" \" if snum[-1].isdigit() else snum[-1]]\n    return float(snum if snum[-1].isdigit() else snum[:-1]) * power", "code_tokens": ["def", "peng_float", "(", "snum", ")", ":", "# This can be coded as peng_mant(snum)*(peng_power(snum)[1]), but the", "# \"function unrolling\" is about 4x faster", "snum", "=", "snum", ".", "rstrip", "(", ")", "power", "=", "_SUFFIX_POWER_DICT", "[", "\" \"", "if", "snum", "[", "-", "1", "]", ".", "isdigit", "(", ")", "else", "snum", "[", "-", "1", "]", "]", "return", "float", "(", "snum", "if", "snum", "[", "-", "1", "]", ".", "isdigit", "(", ")", "else", "snum", "[", ":", "-", "1", "]", ")", "*", "power"], "docstring": "r\"\"\"\n    Return floating point equivalent of a number represented in engineering notation.\n\n    :param snum: Number\n    :type  snum: :ref:`EngineeringNotationNumber`\n\n    :rtype: string\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.functions.peng_float\n\n    :raises: RuntimeError (Argument \\`snum\\` is not valid)\n\n    .. [[[end]]]\n\n    For example:\n\n        >>> import peng\n        >>> peng.peng_float(peng.peng(1235.6789E3, 3, False))\n        1236000.0", "docstring_tokens": ["r", "Return", "floating", "point", "equivalent", "of", "a", "number", "represented", "in", "engineering", "notation", "."], "sha": "976935377adaa3de26fc5677aceb2cdfbd6f93a7", "url": "https://github.com/pmacosta/peng/blob/976935377adaa3de26fc5677aceb2cdfbd6f93a7/peng/functions.py#L484-L511", "partition": "test"}
{"repo": "Toblerity/rtree", "path": "rtree/index.py", "func_name": "Index.nearest", "original_string": "def nearest(self, coordinates, num_results=1, objects=False):\n        \"\"\"Returns the ``k``-nearest objects to the given coordinates.\n\n        :param coordinates: sequence or array\n            This may be an object that satisfies the numpy array\n            protocol, providing the index's dimension * 2 coordinate\n            pairs representing the `mink` and `maxk` coordinates in\n            each dimension defining the bounds of the query window.\n\n        :param num_results: integer\n            The number of results to return nearest to the given coordinates.\n            If two index entries are equidistant, *both* are returned.\n            This property means that :attr:`num_results` may return more\n            items than specified\n\n        :param objects: True / False / 'raw'\n            If True, the nearest method will return index objects that\n            were pickled when they were stored with each index entry, as\n            well as the id and bounds of the index entries.\n            If 'raw', it will return the object as entered into the database\n            without the :class:`rtree.index.Item` wrapper.\n\n        Example of finding the three items nearest to this one::\n\n            >>> from rtree import index\n            >>> idx = index.Index()\n            >>> idx.insert(4321, (34.37, 26.73, 49.37, 41.73), obj=42)\n            >>> hits = idx.nearest((0, 0, 10, 10), 3, objects=True)\n        \"\"\"\n        if objects:\n            return self._nearest_obj(coordinates, num_results, objects)\n        p_mins, p_maxs = self.get_coordinate_pointers(coordinates)\n\n        p_num_results = ctypes.pointer(ctypes.c_uint64(num_results))\n\n        it = ctypes.pointer(ctypes.c_int64())\n\n        core.rt.Index_NearestNeighbors_id(self.handle,\n                                          p_mins,\n                                          p_maxs,\n                                          self.properties.dimension,\n                                          ctypes.byref(it),\n                                          p_num_results)\n\n        return self._get_ids(it, p_num_results.contents.value)", "language": "python", "code": "def nearest(self, coordinates, num_results=1, objects=False):\n        \"\"\"Returns the ``k``-nearest objects to the given coordinates.\n\n        :param coordinates: sequence or array\n            This may be an object that satisfies the numpy array\n            protocol, providing the index's dimension * 2 coordinate\n            pairs representing the `mink` and `maxk` coordinates in\n            each dimension defining the bounds of the query window.\n\n        :param num_results: integer\n            The number of results to return nearest to the given coordinates.\n            If two index entries are equidistant, *both* are returned.\n            This property means that :attr:`num_results` may return more\n            items than specified\n\n        :param objects: True / False / 'raw'\n            If True, the nearest method will return index objects that\n            were pickled when they were stored with each index entry, as\n            well as the id and bounds of the index entries.\n            If 'raw', it will return the object as entered into the database\n            without the :class:`rtree.index.Item` wrapper.\n\n        Example of finding the three items nearest to this one::\n\n            >>> from rtree import index\n            >>> idx = index.Index()\n            >>> idx.insert(4321, (34.37, 26.73, 49.37, 41.73), obj=42)\n            >>> hits = idx.nearest((0, 0, 10, 10), 3, objects=True)\n        \"\"\"\n        if objects:\n            return self._nearest_obj(coordinates, num_results, objects)\n        p_mins, p_maxs = self.get_coordinate_pointers(coordinates)\n\n        p_num_results = ctypes.pointer(ctypes.c_uint64(num_results))\n\n        it = ctypes.pointer(ctypes.c_int64())\n\n        core.rt.Index_NearestNeighbors_id(self.handle,\n                                          p_mins,\n                                          p_maxs,\n                                          self.properties.dimension,\n                                          ctypes.byref(it),\n                                          p_num_results)\n\n        return self._get_ids(it, p_num_results.contents.value)", "code_tokens": ["def", "nearest", "(", "self", ",", "coordinates", ",", "num_results", "=", "1", ",", "objects", "=", "False", ")", ":", "if", "objects", ":", "return", "self", ".", "_nearest_obj", "(", "coordinates", ",", "num_results", ",", "objects", ")", "p_mins", ",", "p_maxs", "=", "self", ".", "get_coordinate_pointers", "(", "coordinates", ")", "p_num_results", "=", "ctypes", ".", "pointer", "(", "ctypes", ".", "c_uint64", "(", "num_results", ")", ")", "it", "=", "ctypes", ".", "pointer", "(", "ctypes", ".", "c_int64", "(", ")", ")", "core", ".", "rt", ".", "Index_NearestNeighbors_id", "(", "self", ".", "handle", ",", "p_mins", ",", "p_maxs", ",", "self", ".", "properties", ".", "dimension", ",", "ctypes", ".", "byref", "(", "it", ")", ",", "p_num_results", ")", "return", "self", ".", "_get_ids", "(", "it", ",", "p_num_results", ".", "contents", ".", "value", ")"], "docstring": "Returns the ``k``-nearest objects to the given coordinates.\n\n        :param coordinates: sequence or array\n            This may be an object that satisfies the numpy array\n            protocol, providing the index's dimension * 2 coordinate\n            pairs representing the `mink` and `maxk` coordinates in\n            each dimension defining the bounds of the query window.\n\n        :param num_results: integer\n            The number of results to return nearest to the given coordinates.\n            If two index entries are equidistant, *both* are returned.\n            This property means that :attr:`num_results` may return more\n            items than specified\n\n        :param objects: True / False / 'raw'\n            If True, the nearest method will return index objects that\n            were pickled when they were stored with each index entry, as\n            well as the id and bounds of the index entries.\n            If 'raw', it will return the object as entered into the database\n            without the :class:`rtree.index.Item` wrapper.\n\n        Example of finding the three items nearest to this one::\n\n            >>> from rtree import index\n            >>> idx = index.Index()\n            >>> idx.insert(4321, (34.37, 26.73, 49.37, 41.73), obj=42)\n            >>> hits = idx.nearest((0, 0, 10, 10), 3, objects=True)", "docstring_tokens": ["Returns", "the", "k", "-", "nearest", "objects", "to", "the", "given", "coordinates", "."], "sha": "5d33357c8e88f1a8344415dc15a7d2440211b281", "url": "https://github.com/Toblerity/rtree/blob/5d33357c8e88f1a8344415dc15a7d2440211b281/rtree/index.py#L560-L604", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/message/message_handler_mix_in.py", "func_name": "MessagesHandlerMixIn.add_message", "original_string": "def add_message(\n        self,\n        msg_descr,\n        line=None,\n        node=None,\n        args=None,\n        confidence=UNDEFINED,\n        col_offset=None,\n    ):\n        \"\"\"Adds a message given by ID or name.\n\n        If provided, the message string is expanded using args.\n\n        AST checkers must provide the node argument (but may optionally\n        provide line if the line number is different), raw and token checkers\n        must provide the line argument.\n        \"\"\"\n        message_definitions = self.msgs_store.get_message_definitions(msg_descr)\n        for message_definition in message_definitions:\n            self.add_one_message(\n                message_definition, line, node, args, confidence, col_offset\n            )", "language": "python", "code": "def add_message(\n        self,\n        msg_descr,\n        line=None,\n        node=None,\n        args=None,\n        confidence=UNDEFINED,\n        col_offset=None,\n    ):\n        \"\"\"Adds a message given by ID or name.\n\n        If provided, the message string is expanded using args.\n\n        AST checkers must provide the node argument (but may optionally\n        provide line if the line number is different), raw and token checkers\n        must provide the line argument.\n        \"\"\"\n        message_definitions = self.msgs_store.get_message_definitions(msg_descr)\n        for message_definition in message_definitions:\n            self.add_one_message(\n                message_definition, line, node, args, confidence, col_offset\n            )", "code_tokens": ["def", "add_message", "(", "self", ",", "msg_descr", ",", "line", "=", "None", ",", "node", "=", "None", ",", "args", "=", "None", ",", "confidence", "=", "UNDEFINED", ",", "col_offset", "=", "None", ",", ")", ":", "message_definitions", "=", "self", ".", "msgs_store", ".", "get_message_definitions", "(", "msg_descr", ")", "for", "message_definition", "in", "message_definitions", ":", "self", ".", "add_one_message", "(", "message_definition", ",", "line", ",", "node", ",", "args", ",", "confidence", ",", "col_offset", ")"], "docstring": "Adds a message given by ID or name.\n\n        If provided, the message string is expanded using args.\n\n        AST checkers must provide the node argument (but may optionally\n        provide line if the line number is different), raw and token checkers\n        must provide the line argument.", "docstring_tokens": ["Adds", "a", "message", "given", "by", "ID", "or", "name", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/message/message_handler_mix_in.py#L251-L272", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/html/notebook/notebookapp.py", "func_name": "random_ports", "original_string": "def random_ports(port, n):\n    \"\"\"Generate a list of n random ports near the given port.\n\n    The first 5 ports will be sequential, and the remaining n-5 will be\n    randomly selected in the range [port-2*n, port+2*n].\n    \"\"\"\n    for i in range(min(5, n)):\n        yield port + i\n    for i in range(n-5):\n        yield port + random.randint(-2*n, 2*n)", "language": "python", "code": "def random_ports(port, n):\n    \"\"\"Generate a list of n random ports near the given port.\n\n    The first 5 ports will be sequential, and the remaining n-5 will be\n    randomly selected in the range [port-2*n, port+2*n].\n    \"\"\"\n    for i in range(min(5, n)):\n        yield port + i\n    for i in range(n-5):\n        yield port + random.randint(-2*n, 2*n)", "code_tokens": ["def", "random_ports", "(", "port", ",", "n", ")", ":", "for", "i", "in", "range", "(", "min", "(", "5", ",", "n", ")", ")", ":", "yield", "port", "+", "i", "for", "i", "in", "range", "(", "n", "-", "5", ")", ":", "yield", "port", "+", "random", ".", "randint", "(", "-", "2", "*", "n", ",", "2", "*", "n", ")"], "docstring": "Generate a list of n random ports near the given port.\n\n    The first 5 ports will be sequential, and the remaining n-5 will be\n    randomly selected in the range [port-2*n, port+2*n].", "docstring_tokens": ["Generate", "a", "list", "of", "n", "random", "ports", "near", "the", "given", "port", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/html/notebook/notebookapp.py#L102-L111", "partition": "test"}
{"repo": "reingart/gui2py", "path": "gui/windows/window.py", "func_name": "Window._set_icon", "original_string": "def _set_icon(self, icon=None):\r\n        \"\"\"Set icon based on resource values\"\"\"\r\n        if icon is not None:\r\n            try:\r\n                wx_icon = wx.Icon(icon, wx.BITMAP_TYPE_ICO)\r\n                self.wx_obj.SetIcon(wx_icon)\r\n            except:\r\n                pass", "language": "python", "code": "def _set_icon(self, icon=None):\r\n        \"\"\"Set icon based on resource values\"\"\"\r\n        if icon is not None:\r\n            try:\r\n                wx_icon = wx.Icon(icon, wx.BITMAP_TYPE_ICO)\r\n                self.wx_obj.SetIcon(wx_icon)\r\n            except:\r\n                pass", "code_tokens": ["def", "_set_icon", "(", "self", ",", "icon", "=", "None", ")", ":", "if", "icon", "is", "not", "None", ":", "try", ":", "wx_icon", "=", "wx", ".", "Icon", "(", "icon", ",", "wx", ".", "BITMAP_TYPE_ICO", ")", "self", ".", "wx_obj", ".", "SetIcon", "(", "wx_icon", ")", "except", ":", "pass"], "docstring": "Set icon based on resource values", "docstring_tokens": ["Set", "icon", "based", "on", "resource", "values"], "sha": "aca0a05f6fcde55c94ad7cc058671a06608b01a4", "url": "https://github.com/reingart/gui2py/blob/aca0a05f6fcde55c94ad7cc058671a06608b01a4/gui/windows/window.py#L78-L85", "partition": "test"}
{"repo": "blockstack/zone-file-py", "path": "blockstack_zones/parse_zone_file.py", "func_name": "parse_lines", "original_string": "def parse_lines(text, ignore_invalid=False):\n    \"\"\"\n    Parse a zonefile into a dict.\n    @text must be flattened--each record must be on one line.\n    Also, all comments must be removed.\n    \"\"\"\n    json_zone_file = defaultdict(list)\n    record_lines = text.split(\"\\n\")\n    parser = make_parser()\n\n    for record_line in record_lines:\n        record_token = tokenize_line(record_line)\n        try:\n            json_zone_file = parse_line(parser, record_token, json_zone_file)\n        except InvalidLineException:\n            if ignore_invalid:\n                continue\n            else:\n                raise\n\n    return json_zone_file", "language": "python", "code": "def parse_lines(text, ignore_invalid=False):\n    \"\"\"\n    Parse a zonefile into a dict.\n    @text must be flattened--each record must be on one line.\n    Also, all comments must be removed.\n    \"\"\"\n    json_zone_file = defaultdict(list)\n    record_lines = text.split(\"\\n\")\n    parser = make_parser()\n\n    for record_line in record_lines:\n        record_token = tokenize_line(record_line)\n        try:\n            json_zone_file = parse_line(parser, record_token, json_zone_file)\n        except InvalidLineException:\n            if ignore_invalid:\n                continue\n            else:\n                raise\n\n    return json_zone_file", "code_tokens": ["def", "parse_lines", "(", "text", ",", "ignore_invalid", "=", "False", ")", ":", "json_zone_file", "=", "defaultdict", "(", "list", ")", "record_lines", "=", "text", ".", "split", "(", "\"\\n\"", ")", "parser", "=", "make_parser", "(", ")", "for", "record_line", "in", "record_lines", ":", "record_token", "=", "tokenize_line", "(", "record_line", ")", "try", ":", "json_zone_file", "=", "parse_line", "(", "parser", ",", "record_token", ",", "json_zone_file", ")", "except", "InvalidLineException", ":", "if", "ignore_invalid", ":", "continue", "else", ":", "raise", "return", "json_zone_file"], "docstring": "Parse a zonefile into a dict.\n    @text must be flattened--each record must be on one line.\n    Also, all comments must be removed.", "docstring_tokens": ["Parse", "a", "zonefile", "into", "a", "dict", "."], "sha": "c1078c8c3c28f0881bc9a3af53d4972c4a6862d0", "url": "https://github.com/blockstack/zone-file-py/blob/c1078c8c3c28f0881bc9a3af53d4972c4a6862d0/blockstack_zones/parse_zone_file.py#L362-L382", "partition": "test"}
{"repo": "ybrs/single-beat", "path": "singlebeat/beat.py", "func_name": "Process.cli_command_resume", "original_string": "def cli_command_resume(self, msg):\n        \"\"\"\\\n        sets state to waiting - so we resume spawning children\n        \"\"\"\n        if self.state == State.PAUSED:\n            self.state = State.WAITING", "language": "python", "code": "def cli_command_resume(self, msg):\n        \"\"\"\\\n        sets state to waiting - so we resume spawning children\n        \"\"\"\n        if self.state == State.PAUSED:\n            self.state = State.WAITING", "code_tokens": ["def", "cli_command_resume", "(", "self", ",", "msg", ")", ":", "if", "self", ".", "state", "==", "State", ".", "PAUSED", ":", "self", ".", "state", "=", "State", ".", "WAITING"], "docstring": "\\\n        sets state to waiting - so we resume spawning children", "docstring_tokens": ["\\", "sets", "state", "to", "waiting", "-", "so", "we", "resume", "spawning", "children"], "sha": "d036b62d2531710dfd806e9dc2a8d67c77616082", "url": "https://github.com/ybrs/single-beat/blob/d036b62d2531710dfd806e9dc2a8d67c77616082/singlebeat/beat.py#L348-L353", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/beat.py", "func_name": "__beat_local_score", "original_string": "def __beat_local_score(onset_envelope, period):\n    '''Construct the local score for an onset envlope and given period'''\n\n    window = np.exp(-0.5 * (np.arange(-period, period+1)*32.0/period)**2)\n    return scipy.signal.convolve(__normalize_onsets(onset_envelope),\n                                 window,\n                                 'same')", "language": "python", "code": "def __beat_local_score(onset_envelope, period):\n    '''Construct the local score for an onset envlope and given period'''\n\n    window = np.exp(-0.5 * (np.arange(-period, period+1)*32.0/period)**2)\n    return scipy.signal.convolve(__normalize_onsets(onset_envelope),\n                                 window,\n                                 'same')", "code_tokens": ["def", "__beat_local_score", "(", "onset_envelope", ",", "period", ")", ":", "window", "=", "np", ".", "exp", "(", "-", "0.5", "*", "(", "np", ".", "arange", "(", "-", "period", ",", "period", "+", "1", ")", "*", "32.0", "/", "period", ")", "**", "2", ")", "return", "scipy", ".", "signal", ".", "convolve", "(", "__normalize_onsets", "(", "onset_envelope", ")", ",", "window", ",", "'same'", ")"], "docstring": "Construct the local score for an onset envlope and given period", "docstring_tokens": ["Construct", "the", "local", "score", "for", "an", "onset", "envlope", "and", "given", "period"], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/beat.py#L408-L414", "partition": "test"}
{"repo": "bjoernricks/python-quilt", "path": "quilt/utils.py", "func_name": "Process.run", "original_string": "def run(self, suppress_output=False, inputdata=None, **kw):\n        \"\"\"Run command as a subprocess and wait until it is finished.\n\n        The command should be given as a list of strings to avoid problems\n        with shell quoting.  If the command exits with a return code other\n        than 0, a SubprocessError is raised.\n        \"\"\"\n        if inputdata is not None:\n            kw[\"stdin\"] = subprocess.PIPE\n        if suppress_output:\n            kw[\"stdout\"] = open(os.devnull, \"w\")\n            kw[\"stderr\"] = kw[\"stdout\"]\n        try:\n            try:\n                process = subprocess.Popen(self.cmd, **kw)\n            finally:\n                if suppress_output:\n                    kw[\"stdout\"].close()\n        except OSError as e:\n            msg = \"Failed starting command {!r}: {}\".format(self.cmd, e)\n            raise QuiltError(msg)\n\n        if inputdata is not None:\n            process.stdin.write(inputdata)\n            process.stdin.close()\n        ret = process.wait()\n        if ret != 0:\n            raise SubprocessError(self.cmd, ret)", "language": "python", "code": "def run(self, suppress_output=False, inputdata=None, **kw):\n        \"\"\"Run command as a subprocess and wait until it is finished.\n\n        The command should be given as a list of strings to avoid problems\n        with shell quoting.  If the command exits with a return code other\n        than 0, a SubprocessError is raised.\n        \"\"\"\n        if inputdata is not None:\n            kw[\"stdin\"] = subprocess.PIPE\n        if suppress_output:\n            kw[\"stdout\"] = open(os.devnull, \"w\")\n            kw[\"stderr\"] = kw[\"stdout\"]\n        try:\n            try:\n                process = subprocess.Popen(self.cmd, **kw)\n            finally:\n                if suppress_output:\n                    kw[\"stdout\"].close()\n        except OSError as e:\n            msg = \"Failed starting command {!r}: {}\".format(self.cmd, e)\n            raise QuiltError(msg)\n\n        if inputdata is not None:\n            process.stdin.write(inputdata)\n            process.stdin.close()\n        ret = process.wait()\n        if ret != 0:\n            raise SubprocessError(self.cmd, ret)", "code_tokens": ["def", "run", "(", "self", ",", "suppress_output", "=", "False", ",", "inputdata", "=", "None", ",", "*", "*", "kw", ")", ":", "if", "inputdata", "is", "not", "None", ":", "kw", "[", "\"stdin\"", "]", "=", "subprocess", ".", "PIPE", "if", "suppress_output", ":", "kw", "[", "\"stdout\"", "]", "=", "open", "(", "os", ".", "devnull", ",", "\"w\"", ")", "kw", "[", "\"stderr\"", "]", "=", "kw", "[", "\"stdout\"", "]", "try", ":", "try", ":", "process", "=", "subprocess", ".", "Popen", "(", "self", ".", "cmd", ",", "*", "*", "kw", ")", "finally", ":", "if", "suppress_output", ":", "kw", "[", "\"stdout\"", "]", ".", "close", "(", ")", "except", "OSError", "as", "e", ":", "msg", "=", "\"Failed starting command {!r}: {}\"", ".", "format", "(", "self", ".", "cmd", ",", "e", ")", "raise", "QuiltError", "(", "msg", ")", "if", "inputdata", "is", "not", "None", ":", "process", ".", "stdin", ".", "write", "(", "inputdata", ")", "process", ".", "stdin", ".", "close", "(", ")", "ret", "=", "process", ".", "wait", "(", ")", "if", "ret", "!=", "0", ":", "raise", "SubprocessError", "(", "self", ".", "cmd", ",", "ret", ")"], "docstring": "Run command as a subprocess and wait until it is finished.\n\n        The command should be given as a list of strings to avoid problems\n        with shell quoting.  If the command exits with a return code other\n        than 0, a SubprocessError is raised.", "docstring_tokens": ["Run", "command", "as", "a", "subprocess", "and", "wait", "until", "it", "is", "finished", "."], "sha": "fae88237f601848cc34d073584d9dcb409f01777", "url": "https://github.com/bjoernricks/python-quilt/blob/fae88237f601848cc34d073584d9dcb409f01777/quilt/utils.py#L76-L103", "partition": "test"}
{"repo": "LionelAuroux/pyrser", "path": "pyrser/dsl.py", "func_name": "add_rules", "original_string": "def add_rules(self, bnf, r) -> bool:\n    \"\"\"Attach a parser tree to the dict of rules\"\"\"\n    bnf[r.rulename] = r.parser_tree\n    return True", "language": "python", "code": "def add_rules(self, bnf, r) -> bool:\n    \"\"\"Attach a parser tree to the dict of rules\"\"\"\n    bnf[r.rulename] = r.parser_tree\n    return True", "code_tokens": ["def", "add_rules", "(", "self", ",", "bnf", ",", "r", ")", "->", "bool", ":", "bnf", "[", "r", ".", "rulename", "]", "=", "r", ".", "parser_tree", "return", "True"], "docstring": "Attach a parser tree to the dict of rules", "docstring_tokens": ["Attach", "a", "parser", "tree", "to", "the", "dict", "of", "rules"], "sha": "f153a97ef2b6bf915a1ed468c0252a9a59b754d5", "url": "https://github.com/LionelAuroux/pyrser/blob/f153a97ef2b6bf915a1ed468c0252a9a59b754d5/pyrser/dsl.py#L517-L520", "partition": "test"}
{"repo": "neurodata/ndio", "path": "ndio/convert/volume.py", "func_name": "from_voxels", "original_string": "def from_voxels(voxels):\n    \"\"\"\n    Converts a voxel list to an ndarray.\n\n    Arguments:\n        voxels (tuple[]): A list of coordinates indicating coordinates of\n            populated voxels in an ndarray.\n\n    Returns:\n        numpy.ndarray The result of the transformation.\n    \"\"\"\n    dimensions = len(voxels[0])\n\n    for d in range(len(dimensions)):\n        size.append(max([i[d] for i in voxels]))\n\n    result = numpy.zeros(dimensions)\n\n    for v in voxels:\n        result[v] = 1\n\n    return result", "language": "python", "code": "def from_voxels(voxels):\n    \"\"\"\n    Converts a voxel list to an ndarray.\n\n    Arguments:\n        voxels (tuple[]): A list of coordinates indicating coordinates of\n            populated voxels in an ndarray.\n\n    Returns:\n        numpy.ndarray The result of the transformation.\n    \"\"\"\n    dimensions = len(voxels[0])\n\n    for d in range(len(dimensions)):\n        size.append(max([i[d] for i in voxels]))\n\n    result = numpy.zeros(dimensions)\n\n    for v in voxels:\n        result[v] = 1\n\n    return result", "code_tokens": ["def", "from_voxels", "(", "voxels", ")", ":", "dimensions", "=", "len", "(", "voxels", "[", "0", "]", ")", "for", "d", "in", "range", "(", "len", "(", "dimensions", ")", ")", ":", "size", ".", "append", "(", "max", "(", "[", "i", "[", "d", "]", "for", "i", "in", "voxels", "]", ")", ")", "result", "=", "numpy", ".", "zeros", "(", "dimensions", ")", "for", "v", "in", "voxels", ":", "result", "[", "v", "]", "=", "1", "return", "result"], "docstring": "Converts a voxel list to an ndarray.\n\n    Arguments:\n        voxels (tuple[]): A list of coordinates indicating coordinates of\n            populated voxels in an ndarray.\n\n    Returns:\n        numpy.ndarray The result of the transformation.", "docstring_tokens": ["Converts", "a", "voxel", "list", "to", "an", "ndarray", "."], "sha": "792dd5816bc770b05a3db2f4327da42ff6253531", "url": "https://github.com/neurodata/ndio/blob/792dd5816bc770b05a3db2f4327da42ff6253531/ndio/convert/volume.py#L20-L41", "partition": "test"}
{"repo": "xmikos/soapy_power", "path": "soapypower/__main__.py", "func_name": "specific_gains", "original_string": "def specific_gains(string):\n    \"\"\"Convert string with gains of individual amplification elements to dict\"\"\"\n    if not string:\n        return {}\n\n    gains = {}\n    for gain in string.split(','):\n        amp_name, value = gain.split('=')\n        gains[amp_name.strip()] = float(value.strip())\n    return gains", "language": "python", "code": "def specific_gains(string):\n    \"\"\"Convert string with gains of individual amplification elements to dict\"\"\"\n    if not string:\n        return {}\n\n    gains = {}\n    for gain in string.split(','):\n        amp_name, value = gain.split('=')\n        gains[amp_name.strip()] = float(value.strip())\n    return gains", "code_tokens": ["def", "specific_gains", "(", "string", ")", ":", "if", "not", "string", ":", "return", "{", "}", "gains", "=", "{", "}", "for", "gain", "in", "string", ".", "split", "(", "','", ")", ":", "amp_name", ",", "value", "=", "gain", ".", "split", "(", "'='", ")", "gains", "[", "amp_name", ".", "strip", "(", ")", "]", "=", "float", "(", "value", ".", "strip", "(", ")", ")", "return", "gains"], "docstring": "Convert string with gains of individual amplification elements to dict", "docstring_tokens": ["Convert", "string", "with", "gains", "of", "individual", "amplification", "elements", "to", "dict"], "sha": "46e12659b8d08af764dc09a1f31b0e85a68f808f", "url": "https://github.com/xmikos/soapy_power/blob/46e12659b8d08af764dc09a1f31b0e85a68f808f/soapypower/__main__.py#L36-L45", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_api_base_hook.py", "func_name": "GoogleCloudBaseHook.catch_http_exception", "original_string": "def catch_http_exception(func):\n        \"\"\"\n        Function decorator that intercepts HTTP Errors and raises AirflowException\n        with more informative message.\n        \"\"\"\n\n        @functools.wraps(func)\n        def wrapper_decorator(self, *args, **kwargs):\n            try:\n                return func(self, *args, **kwargs)\n            except GoogleAPICallError as e:\n                if isinstance(e, AlreadyExists):\n                    raise e\n                else:\n                    self.log.error('The request failed:\\n%s', str(e))\n                    raise AirflowException(e)\n            except RetryError as e:\n                self.log.error('The request failed due to a retryable error and retry attempts failed.')\n                raise AirflowException(e)\n            except ValueError as e:\n                self.log.error('The request failed, the parameters are invalid.')\n                raise AirflowException(e)\n            except HttpError as e:\n                self.log.error('The request failed:\\n%s', str(e))\n                raise AirflowException(e)\n\n        return wrapper_decorator", "language": "python", "code": "def catch_http_exception(func):\n        \"\"\"\n        Function decorator that intercepts HTTP Errors and raises AirflowException\n        with more informative message.\n        \"\"\"\n\n        @functools.wraps(func)\n        def wrapper_decorator(self, *args, **kwargs):\n            try:\n                return func(self, *args, **kwargs)\n            except GoogleAPICallError as e:\n                if isinstance(e, AlreadyExists):\n                    raise e\n                else:\n                    self.log.error('The request failed:\\n%s', str(e))\n                    raise AirflowException(e)\n            except RetryError as e:\n                self.log.error('The request failed due to a retryable error and retry attempts failed.')\n                raise AirflowException(e)\n            except ValueError as e:\n                self.log.error('The request failed, the parameters are invalid.')\n                raise AirflowException(e)\n            except HttpError as e:\n                self.log.error('The request failed:\\n%s', str(e))\n                raise AirflowException(e)\n\n        return wrapper_decorator", "code_tokens": ["def", "catch_http_exception", "(", "func", ")", ":", "@", "functools", ".", "wraps", "(", "func", ")", "def", "wrapper_decorator", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "try", ":", "return", "func", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", "except", "GoogleAPICallError", "as", "e", ":", "if", "isinstance", "(", "e", ",", "AlreadyExists", ")", ":", "raise", "e", "else", ":", "self", ".", "log", ".", "error", "(", "'The request failed:\\n%s'", ",", "str", "(", "e", ")", ")", "raise", "AirflowException", "(", "e", ")", "except", "RetryError", "as", "e", ":", "self", ".", "log", ".", "error", "(", "'The request failed due to a retryable error and retry attempts failed.'", ")", "raise", "AirflowException", "(", "e", ")", "except", "ValueError", "as", "e", ":", "self", ".", "log", ".", "error", "(", "'The request failed, the parameters are invalid.'", ")", "raise", "AirflowException", "(", "e", ")", "except", "HttpError", "as", "e", ":", "self", ".", "log", ".", "error", "(", "'The request failed:\\n%s'", ",", "str", "(", "e", ")", ")", "raise", "AirflowException", "(", "e", ")", "return", "wrapper_decorator"], "docstring": "Function decorator that intercepts HTTP Errors and raises AirflowException\n        with more informative message.", "docstring_tokens": ["Function", "decorator", "that", "intercepts", "HTTP", "Errors", "and", "raises", "AirflowException", "with", "more", "informative", "message", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_api_base_hook.py#L166-L192", "partition": "test"}
{"repo": "Yubico/python-u2flib-host", "path": "u2flib_host/utils.py", "func_name": "u2str", "original_string": "def u2str(data):\n    \"\"\"Recursively converts unicode objects to UTF-8 encoded byte strings.\"\"\"\n    if isinstance(data, dict):\n        return {u2str(k): u2str(v) for k, v in data.items()}\n    elif isinstance(data, list):\n        return [u2str(x) for x in data]\n    elif isinstance(data, text_type):\n        return data.encode('utf-8')\n    else:\n        return data", "language": "python", "code": "def u2str(data):\n    \"\"\"Recursively converts unicode objects to UTF-8 encoded byte strings.\"\"\"\n    if isinstance(data, dict):\n        return {u2str(k): u2str(v) for k, v in data.items()}\n    elif isinstance(data, list):\n        return [u2str(x) for x in data]\n    elif isinstance(data, text_type):\n        return data.encode('utf-8')\n    else:\n        return data", "code_tokens": ["def", "u2str", "(", "data", ")", ":", "if", "isinstance", "(", "data", ",", "dict", ")", ":", "return", "{", "u2str", "(", "k", ")", ":", "u2str", "(", "v", ")", "for", "k", ",", "v", "in", "data", ".", "items", "(", ")", "}", "elif", "isinstance", "(", "data", ",", "list", ")", ":", "return", "[", "u2str", "(", "x", ")", "for", "x", "in", "data", "]", "elif", "isinstance", "(", "data", ",", "text_type", ")", ":", "return", "data", ".", "encode", "(", "'utf-8'", ")", "else", ":", "return", "data"], "docstring": "Recursively converts unicode objects to UTF-8 encoded byte strings.", "docstring_tokens": ["Recursively", "converts", "unicode", "objects", "to", "UTF", "-", "8", "encoded", "byte", "strings", "."], "sha": "eadc4dbf3bf516e74ea00d2e5690742a535834cb", "url": "https://github.com/Yubico/python-u2flib-host/blob/eadc4dbf3bf516e74ea00d2e5690742a535834cb/u2flib_host/utils.py#L40-L49", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/data.py", "func_name": "CoverageData.read_file", "original_string": "def read_file(self, filename):\n        \"\"\"Read the coverage data from `filename`.\"\"\"\n        self.lines, self.arcs = self._read_file(filename)", "language": "python", "code": "def read_file(self, filename):\n        \"\"\"Read the coverage data from `filename`.\"\"\"\n        self.lines, self.arcs = self._read_file(filename)", "code_tokens": ["def", "read_file", "(", "self", ",", "filename", ")", ":", "self", ".", "lines", ",", "self", ".", "arcs", "=", "self", ".", "_read_file", "(", "filename", ")"], "docstring": "Read the coverage data from `filename`.", "docstring_tokens": ["Read", "the", "coverage", "data", "from", "filename", "."], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/data.py#L137-L139", "partition": "test"}
{"repo": "datacamp/pythonwhat", "path": "pythonwhat/checks/has_funcs.py", "func_name": "has_equal_ast", "original_string": "def has_equal_ast(state, incorrect_msg=None, code=None, exact=True, append=None):\n    \"\"\"Test whether abstract syntax trees match between the student and solution code.\n\n    ``has_equal_ast()`` can be used in two ways:\n\n    * As a robust version of ``has_code()``. By setting ``code``, you can look for the AST representation of ``code`` in the student's submission.\n      But be aware that ``a`` and ``a = 1`` won't match, as reading and assigning are not the same in an AST.\n      Use ``ast.dump(ast.parse(code))`` to see an AST representation of ``code``.\n    * As an expression-based check when using more advanced SCT chain, e.g. to compare the equality of expressions to set function arguments.\n\n    Args:\n        incorrect_msg: message displayed when ASTs mismatch. When you specify ``code`` yourself, you have to specify this.\n        code: optional code to use instead of the solution AST.\n        exact: whether the representations must match exactly. If false, the solution AST\n               only needs to be contained within the student AST (similar to using test student typed).\n               Defaults to ``True``, unless the ``code`` argument has been specified.\n\n    :Example:\n\n        Student and Solution Code::\n\n            dict(a = 'value').keys()\n\n        SCT::\n\n            # all pass\n            Ex().has_equal_ast()\n            Ex().has_equal_ast(code = \"dict(a = 'value').keys()\")\n            Ex().has_equal_ast(code = \"dict(a = 'value')\", exact = False)\n\n        Student and Solution Code::\n\n            import numpy as np\n            arr = np.array([1, 2, 3, 4, 5])\n            np.mean(arr)\n\n        SCT::\n\n            # Check underlying value of arugment a of np.mean:\n            Ex().check_function('numpy.mean').check_args('a').has_equal_ast()\n\n            # Only check AST equality of expression used to specify argument a:\n            Ex().check_function('numpy.mean').check_args('a').has_equal_ast()\n\n    \"\"\"\n    if utils.v2_only():\n        state.assert_is_not([\"object_assignments\"], \"has_equal_ast\", [\"check_object\"])\n        state.assert_is_not([\"function_calls\"], \"has_equal_ast\", [\"check_function\"])\n\n    if code and incorrect_msg is None:\n        raise InstructorError(\n            \"If you manually specify the code to match inside has_equal_ast(), \"\n            \"you have to explicitly set the `incorrect_msg` argument.\"\n        )\n\n    if (\n        append is None\n    ):  # if not specified, set to False if incorrect_msg was manually specified\n        append = incorrect_msg is None\n    if incorrect_msg is None:\n        incorrect_msg = \"Expected `{{sol_str}}`, but got `{{stu_str}}`.\"\n\n    def parse_tree(tree):\n        # get contents of module.body if only 1 element\n        crnt = (\n            tree.body[0]\n            if isinstance(tree, ast.Module) and len(tree.body) == 1\n            else tree\n        )\n\n        # remove Expr if it exists\n        return ast.dump(crnt.value if isinstance(crnt, ast.Expr) else crnt)\n\n    stu_rep = parse_tree(state.student_ast)\n    sol_rep = parse_tree(state.solution_ast if not code else ast.parse(code))\n\n    fmt_kwargs = {\n        \"sol_str\": state.solution_code if not code else code,\n        \"stu_str\": state.student_code,\n    }\n\n    _msg = state.build_message(incorrect_msg, fmt_kwargs, append=append)\n\n    if exact and not code:\n        state.do_test(EqualTest(stu_rep, sol_rep, Feedback(_msg, state)))\n    elif not sol_rep in stu_rep:\n        state.report(Feedback(_msg, state))\n\n    return state", "language": "python", "code": "def has_equal_ast(state, incorrect_msg=None, code=None, exact=True, append=None):\n    \"\"\"Test whether abstract syntax trees match between the student and solution code.\n\n    ``has_equal_ast()`` can be used in two ways:\n\n    * As a robust version of ``has_code()``. By setting ``code``, you can look for the AST representation of ``code`` in the student's submission.\n      But be aware that ``a`` and ``a = 1`` won't match, as reading and assigning are not the same in an AST.\n      Use ``ast.dump(ast.parse(code))`` to see an AST representation of ``code``.\n    * As an expression-based check when using more advanced SCT chain, e.g. to compare the equality of expressions to set function arguments.\n\n    Args:\n        incorrect_msg: message displayed when ASTs mismatch. When you specify ``code`` yourself, you have to specify this.\n        code: optional code to use instead of the solution AST.\n        exact: whether the representations must match exactly. If false, the solution AST\n               only needs to be contained within the student AST (similar to using test student typed).\n               Defaults to ``True``, unless the ``code`` argument has been specified.\n\n    :Example:\n\n        Student and Solution Code::\n\n            dict(a = 'value').keys()\n\n        SCT::\n\n            # all pass\n            Ex().has_equal_ast()\n            Ex().has_equal_ast(code = \"dict(a = 'value').keys()\")\n            Ex().has_equal_ast(code = \"dict(a = 'value')\", exact = False)\n\n        Student and Solution Code::\n\n            import numpy as np\n            arr = np.array([1, 2, 3, 4, 5])\n            np.mean(arr)\n\n        SCT::\n\n            # Check underlying value of arugment a of np.mean:\n            Ex().check_function('numpy.mean').check_args('a').has_equal_ast()\n\n            # Only check AST equality of expression used to specify argument a:\n            Ex().check_function('numpy.mean').check_args('a').has_equal_ast()\n\n    \"\"\"\n    if utils.v2_only():\n        state.assert_is_not([\"object_assignments\"], \"has_equal_ast\", [\"check_object\"])\n        state.assert_is_not([\"function_calls\"], \"has_equal_ast\", [\"check_function\"])\n\n    if code and incorrect_msg is None:\n        raise InstructorError(\n            \"If you manually specify the code to match inside has_equal_ast(), \"\n            \"you have to explicitly set the `incorrect_msg` argument.\"\n        )\n\n    if (\n        append is None\n    ):  # if not specified, set to False if incorrect_msg was manually specified\n        append = incorrect_msg is None\n    if incorrect_msg is None:\n        incorrect_msg = \"Expected `{{sol_str}}`, but got `{{stu_str}}`.\"\n\n    def parse_tree(tree):\n        # get contents of module.body if only 1 element\n        crnt = (\n            tree.body[0]\n            if isinstance(tree, ast.Module) and len(tree.body) == 1\n            else tree\n        )\n\n        # remove Expr if it exists\n        return ast.dump(crnt.value if isinstance(crnt, ast.Expr) else crnt)\n\n    stu_rep = parse_tree(state.student_ast)\n    sol_rep = parse_tree(state.solution_ast if not code else ast.parse(code))\n\n    fmt_kwargs = {\n        \"sol_str\": state.solution_code if not code else code,\n        \"stu_str\": state.student_code,\n    }\n\n    _msg = state.build_message(incorrect_msg, fmt_kwargs, append=append)\n\n    if exact and not code:\n        state.do_test(EqualTest(stu_rep, sol_rep, Feedback(_msg, state)))\n    elif not sol_rep in stu_rep:\n        state.report(Feedback(_msg, state))\n\n    return state", "code_tokens": ["def", "has_equal_ast", "(", "state", ",", "incorrect_msg", "=", "None", ",", "code", "=", "None", ",", "exact", "=", "True", ",", "append", "=", "None", ")", ":", "if", "utils", ".", "v2_only", "(", ")", ":", "state", ".", "assert_is_not", "(", "[", "\"object_assignments\"", "]", ",", "\"has_equal_ast\"", ",", "[", "\"check_object\"", "]", ")", "state", ".", "assert_is_not", "(", "[", "\"function_calls\"", "]", ",", "\"has_equal_ast\"", ",", "[", "\"check_function\"", "]", ")", "if", "code", "and", "incorrect_msg", "is", "None", ":", "raise", "InstructorError", "(", "\"If you manually specify the code to match inside has_equal_ast(), \"", "\"you have to explicitly set the `incorrect_msg` argument.\"", ")", "if", "(", "append", "is", "None", ")", ":", "# if not specified, set to False if incorrect_msg was manually specified", "append", "=", "incorrect_msg", "is", "None", "if", "incorrect_msg", "is", "None", ":", "incorrect_msg", "=", "\"Expected `{{sol_str}}`, but got `{{stu_str}}`.\"", "def", "parse_tree", "(", "tree", ")", ":", "# get contents of module.body if only 1 element", "crnt", "=", "(", "tree", ".", "body", "[", "0", "]", "if", "isinstance", "(", "tree", ",", "ast", ".", "Module", ")", "and", "len", "(", "tree", ".", "body", ")", "==", "1", "else", "tree", ")", "# remove Expr if it exists", "return", "ast", ".", "dump", "(", "crnt", ".", "value", "if", "isinstance", "(", "crnt", ",", "ast", ".", "Expr", ")", "else", "crnt", ")", "stu_rep", "=", "parse_tree", "(", "state", ".", "student_ast", ")", "sol_rep", "=", "parse_tree", "(", "state", ".", "solution_ast", "if", "not", "code", "else", "ast", ".", "parse", "(", "code", ")", ")", "fmt_kwargs", "=", "{", "\"sol_str\"", ":", "state", ".", "solution_code", "if", "not", "code", "else", "code", ",", "\"stu_str\"", ":", "state", ".", "student_code", ",", "}", "_msg", "=", "state", ".", "build_message", "(", "incorrect_msg", ",", "fmt_kwargs", ",", "append", "=", "append", ")", "if", "exact", "and", "not", "code", ":", "state", ".", "do_test", "(", "EqualTest", "(", "stu_rep", ",", "sol_rep", ",", "Feedback", "(", "_msg", ",", "state", ")", ")", ")", "elif", "not", "sol_rep", "in", "stu_rep", ":", "state", ".", "report", "(", "Feedback", "(", "_msg", ",", "state", ")", ")", "return", "state"], "docstring": "Test whether abstract syntax trees match between the student and solution code.\n\n    ``has_equal_ast()`` can be used in two ways:\n\n    * As a robust version of ``has_code()``. By setting ``code``, you can look for the AST representation of ``code`` in the student's submission.\n      But be aware that ``a`` and ``a = 1`` won't match, as reading and assigning are not the same in an AST.\n      Use ``ast.dump(ast.parse(code))`` to see an AST representation of ``code``.\n    * As an expression-based check when using more advanced SCT chain, e.g. to compare the equality of expressions to set function arguments.\n\n    Args:\n        incorrect_msg: message displayed when ASTs mismatch. When you specify ``code`` yourself, you have to specify this.\n        code: optional code to use instead of the solution AST.\n        exact: whether the representations must match exactly. If false, the solution AST\n               only needs to be contained within the student AST (similar to using test student typed).\n               Defaults to ``True``, unless the ``code`` argument has been specified.\n\n    :Example:\n\n        Student and Solution Code::\n\n            dict(a = 'value').keys()\n\n        SCT::\n\n            # all pass\n            Ex().has_equal_ast()\n            Ex().has_equal_ast(code = \"dict(a = 'value').keys()\")\n            Ex().has_equal_ast(code = \"dict(a = 'value')\", exact = False)\n\n        Student and Solution Code::\n\n            import numpy as np\n            arr = np.array([1, 2, 3, 4, 5])\n            np.mean(arr)\n\n        SCT::\n\n            # Check underlying value of arugment a of np.mean:\n            Ex().check_function('numpy.mean').check_args('a').has_equal_ast()\n\n            # Only check AST equality of expression used to specify argument a:\n            Ex().check_function('numpy.mean').check_args('a').has_equal_ast()", "docstring_tokens": ["Test", "whether", "abstract", "syntax", "trees", "match", "between", "the", "student", "and", "solution", "code", "."], "sha": "ffbf7f8436a51f77c22f3bed75ba3bc37a5c666f", "url": "https://github.com/datacamp/pythonwhat/blob/ffbf7f8436a51f77c22f3bed75ba3bc37a5c666f/pythonwhat/checks/has_funcs.py#L112-L200", "partition": "test"}
{"repo": "assemblerflow/flowcraft", "path": "flowcraft/templates/process_assembly_mapping.py", "func_name": "filter_bam", "original_string": "def filter_bam(coverage_info, bam_file, min_coverage, output_bam):\n    \"\"\"Uses Samtools to filter a BAM file according to minimum coverage\n\n    Provided with a minimum coverage value, this function will use Samtools\n    to filter a BAM file. This is performed to apply the same filter to\n    the BAM file as the one applied to the assembly file in\n    :py:func:`filter_assembly`.\n\n    Parameters\n    ----------\n    coverage_info : OrderedDict or dict\n        Dictionary containing the coverage information for each contig.\n    bam_file : str\n        Path to the BAM file.\n    min_coverage : int\n        Minimum coverage required for a contig to pass the filter.\n    output_bam : str\n        Path to the generated filtered BAM file.\n    \"\"\"\n\n    # Get list of contigs that will be kept\n    contig_list = [x for x, vals in coverage_info.items()\n                   if vals[\"cov\"] >= min_coverage]\n\n    cli = [\n        \"samtools\",\n        \"view\",\n        \"-bh\",\n        \"-F\",\n        \"4\",\n        \"-o\",\n        output_bam,\n        \"-@\",\n        \"1\",\n        bam_file,\n    ]\n\n    cli += contig_list\n\n    logger.debug(\"Runnig samtools view subprocess with command: {}\".format(\n        cli))\n\n    p = subprocess.Popen(cli, stdout=PIPE, stderr=PIPE)\n    stdout, stderr = p.communicate()\n\n    # Attempt to decode STDERR output from bytes. If unsuccessful, coerce to\n    # string\n    try:\n        stderr = stderr.decode(\"utf8\")\n        stdout = stdout.decode(\"utf8\")\n    except (UnicodeDecodeError, AttributeError):\n        stderr = str(stderr)\n        stdout = str(stdout)\n\n    logger.info(\"Finished samtools view subprocess with STDOUT:\\\\n\"\n                \"======================================\\\\n{}\".format(stdout))\n    logger.info(\"Fished samtools view subprocesswith STDERR:\\\\n\"\n                \"======================================\\\\n{}\".format(stderr))\n    logger.info(\"Finished samtools view with return code: {}\".format(\n        p.returncode))\n\n    if not p.returncode:\n        # Create index\n        cli = [\n            \"samtools\",\n            \"index\",\n            output_bam\n        ]\n\n        logger.debug(\"Runnig samtools index subprocess with command: \"\n                     \"{}\".format(cli))\n\n        p = subprocess.Popen(cli, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = p.communicate()\n\n        try:\n            stderr = stderr.decode(\"utf8\")\n            stdout = stdout.decode(\"utf8\")\n        except (UnicodeDecodeError, AttributeError):\n            stderr = str(stderr)\n            stdout = str(stdout)\n\n        logger.info(\"Finished samtools index subprocess with STDOUT:\\\\n\"\n                    \"======================================\\\\n{}\".format(\n            stdout))\n        logger.info(\"Fished samtools index subprocesswith STDERR:\\\\n\"\n                    \"======================================\\\\n{}\".format(\n            stderr))\n        logger.info(\"Finished samtools index with return code: {}\".format(\n            p.returncode))", "language": "python", "code": "def filter_bam(coverage_info, bam_file, min_coverage, output_bam):\n    \"\"\"Uses Samtools to filter a BAM file according to minimum coverage\n\n    Provided with a minimum coverage value, this function will use Samtools\n    to filter a BAM file. This is performed to apply the same filter to\n    the BAM file as the one applied to the assembly file in\n    :py:func:`filter_assembly`.\n\n    Parameters\n    ----------\n    coverage_info : OrderedDict or dict\n        Dictionary containing the coverage information for each contig.\n    bam_file : str\n        Path to the BAM file.\n    min_coverage : int\n        Minimum coverage required for a contig to pass the filter.\n    output_bam : str\n        Path to the generated filtered BAM file.\n    \"\"\"\n\n    # Get list of contigs that will be kept\n    contig_list = [x for x, vals in coverage_info.items()\n                   if vals[\"cov\"] >= min_coverage]\n\n    cli = [\n        \"samtools\",\n        \"view\",\n        \"-bh\",\n        \"-F\",\n        \"4\",\n        \"-o\",\n        output_bam,\n        \"-@\",\n        \"1\",\n        bam_file,\n    ]\n\n    cli += contig_list\n\n    logger.debug(\"Runnig samtools view subprocess with command: {}\".format(\n        cli))\n\n    p = subprocess.Popen(cli, stdout=PIPE, stderr=PIPE)\n    stdout, stderr = p.communicate()\n\n    # Attempt to decode STDERR output from bytes. If unsuccessful, coerce to\n    # string\n    try:\n        stderr = stderr.decode(\"utf8\")\n        stdout = stdout.decode(\"utf8\")\n    except (UnicodeDecodeError, AttributeError):\n        stderr = str(stderr)\n        stdout = str(stdout)\n\n    logger.info(\"Finished samtools view subprocess with STDOUT:\\\\n\"\n                \"======================================\\\\n{}\".format(stdout))\n    logger.info(\"Fished samtools view subprocesswith STDERR:\\\\n\"\n                \"======================================\\\\n{}\".format(stderr))\n    logger.info(\"Finished samtools view with return code: {}\".format(\n        p.returncode))\n\n    if not p.returncode:\n        # Create index\n        cli = [\n            \"samtools\",\n            \"index\",\n            output_bam\n        ]\n\n        logger.debug(\"Runnig samtools index subprocess with command: \"\n                     \"{}\".format(cli))\n\n        p = subprocess.Popen(cli, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = p.communicate()\n\n        try:\n            stderr = stderr.decode(\"utf8\")\n            stdout = stdout.decode(\"utf8\")\n        except (UnicodeDecodeError, AttributeError):\n            stderr = str(stderr)\n            stdout = str(stdout)\n\n        logger.info(\"Finished samtools index subprocess with STDOUT:\\\\n\"\n                    \"======================================\\\\n{}\".format(\n            stdout))\n        logger.info(\"Fished samtools index subprocesswith STDERR:\\\\n\"\n                    \"======================================\\\\n{}\".format(\n            stderr))\n        logger.info(\"Finished samtools index with return code: {}\".format(\n            p.returncode))", "code_tokens": ["def", "filter_bam", "(", "coverage_info", ",", "bam_file", ",", "min_coverage", ",", "output_bam", ")", ":", "# Get list of contigs that will be kept", "contig_list", "=", "[", "x", "for", "x", ",", "vals", "in", "coverage_info", ".", "items", "(", ")", "if", "vals", "[", "\"cov\"", "]", ">=", "min_coverage", "]", "cli", "=", "[", "\"samtools\"", ",", "\"view\"", ",", "\"-bh\"", ",", "\"-F\"", ",", "\"4\"", ",", "\"-o\"", ",", "output_bam", ",", "\"-@\"", ",", "\"1\"", ",", "bam_file", ",", "]", "cli", "+=", "contig_list", "logger", ".", "debug", "(", "\"Runnig samtools view subprocess with command: {}\"", ".", "format", "(", "cli", ")", ")", "p", "=", "subprocess", ".", "Popen", "(", "cli", ",", "stdout", "=", "PIPE", ",", "stderr", "=", "PIPE", ")", "stdout", ",", "stderr", "=", "p", ".", "communicate", "(", ")", "# Attempt to decode STDERR output from bytes. If unsuccessful, coerce to", "# string", "try", ":", "stderr", "=", "stderr", ".", "decode", "(", "\"utf8\"", ")", "stdout", "=", "stdout", ".", "decode", "(", "\"utf8\"", ")", "except", "(", "UnicodeDecodeError", ",", "AttributeError", ")", ":", "stderr", "=", "str", "(", "stderr", ")", "stdout", "=", "str", "(", "stdout", ")", "logger", ".", "info", "(", "\"Finished samtools view subprocess with STDOUT:\\\\n\"", "\"======================================\\\\n{}\"", ".", "format", "(", "stdout", ")", ")", "logger", ".", "info", "(", "\"Fished samtools view subprocesswith STDERR:\\\\n\"", "\"======================================\\\\n{}\"", ".", "format", "(", "stderr", ")", ")", "logger", ".", "info", "(", "\"Finished samtools view with return code: {}\"", ".", "format", "(", "p", ".", "returncode", ")", ")", "if", "not", "p", ".", "returncode", ":", "# Create index", "cli", "=", "[", "\"samtools\"", ",", "\"index\"", ",", "output_bam", "]", "logger", ".", "debug", "(", "\"Runnig samtools index subprocess with command: \"", "\"{}\"", ".", "format", "(", "cli", ")", ")", "p", "=", "subprocess", ".", "Popen", "(", "cli", ",", "stdout", "=", "PIPE", ",", "stderr", "=", "PIPE", ")", "stdout", ",", "stderr", "=", "p", ".", "communicate", "(", ")", "try", ":", "stderr", "=", "stderr", ".", "decode", "(", "\"utf8\"", ")", "stdout", "=", "stdout", ".", "decode", "(", "\"utf8\"", ")", "except", "(", "UnicodeDecodeError", ",", "AttributeError", ")", ":", "stderr", "=", "str", "(", "stderr", ")", "stdout", "=", "str", "(", "stdout", ")", "logger", ".", "info", "(", "\"Finished samtools index subprocess with STDOUT:\\\\n\"", "\"======================================\\\\n{}\"", ".", "format", "(", "stdout", ")", ")", "logger", ".", "info", "(", "\"Fished samtools index subprocesswith STDERR:\\\\n\"", "\"======================================\\\\n{}\"", ".", "format", "(", "stderr", ")", ")", "logger", ".", "info", "(", "\"Finished samtools index with return code: {}\"", ".", "format", "(", "p", ".", "returncode", ")", ")"], "docstring": "Uses Samtools to filter a BAM file according to minimum coverage\n\n    Provided with a minimum coverage value, this function will use Samtools\n    to filter a BAM file. This is performed to apply the same filter to\n    the BAM file as the one applied to the assembly file in\n    :py:func:`filter_assembly`.\n\n    Parameters\n    ----------\n    coverage_info : OrderedDict or dict\n        Dictionary containing the coverage information for each contig.\n    bam_file : str\n        Path to the BAM file.\n    min_coverage : int\n        Minimum coverage required for a contig to pass the filter.\n    output_bam : str\n        Path to the generated filtered BAM file.", "docstring_tokens": ["Uses", "Samtools", "to", "filter", "a", "BAM", "file", "according", "to", "minimum", "coverage"], "sha": "fc3f4bddded1efc76006600016dc71a06dd908c0", "url": "https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/templates/process_assembly_mapping.py#L212-L301", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/flask/ctx.py", "func_name": "RequestContext.match_request", "original_string": "def match_request(self):\n        \"\"\"Can be overridden by a subclass to hook into the matching\n        of the request.\n        \"\"\"\n        try:\n            url_rule, self.request.view_args = \\\n                self.url_adapter.match(return_rule=True)\n            self.request.url_rule = url_rule\n        except HTTPException as e:\n            self.request.routing_exception = e", "language": "python", "code": "def match_request(self):\n        \"\"\"Can be overridden by a subclass to hook into the matching\n        of the request.\n        \"\"\"\n        try:\n            url_rule, self.request.view_args = \\\n                self.url_adapter.match(return_rule=True)\n            self.request.url_rule = url_rule\n        except HTTPException as e:\n            self.request.routing_exception = e", "code_tokens": ["def", "match_request", "(", "self", ")", ":", "try", ":", "url_rule", ",", "self", ".", "request", ".", "view_args", "=", "self", ".", "url_adapter", ".", "match", "(", "return_rule", "=", "True", ")", "self", ".", "request", ".", "url_rule", "=", "url_rule", "except", "HTTPException", "as", "e", ":", "self", ".", "request", ".", "routing_exception", "=", "e"], "docstring": "Can be overridden by a subclass to hook into the matching\n        of the request.", "docstring_tokens": ["Can", "be", "overridden", "by", "a", "subclass", "to", "hook", "into", "the", "matching", "of", "the", "request", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/flask/ctx.py#L280-L289", "partition": "test"}
{"repo": "neurosynth/neurosynth", "path": "neurosynth/analysis/classify.py", "func_name": "Classifier.cross_val_fit", "original_string": "def cross_val_fit(self, X, y, cross_val='4-Fold', scoring='accuracy',\n                      feat_select=None, class_weight='auto'):\n        \"\"\" Fits X to outcomes y, using clf and cv_method \"\"\"\n\n        from sklearn import cross_validation\n\n        self.X = X\n        self.y = y\n\n        self.set_class_weight(class_weight=class_weight, y=y)\n\n        # Set cross validator\n        if isinstance(cross_val, string_types):\n            if re.match('.*-Fold', cross_val) is not None:\n                n = int(cross_val.split('-')[0])\n                self.cver = cross_validation.StratifiedKFold(self.y, n)\n            else:\n                raise Exception('Unrecognized cross validation method')\n        else:\n            self.cver = cross_val\n\n        if feat_select is not None:\n            self.features_selected = []\n\n        # Perform cross-validated classification\n        from sklearn.grid_search import GridSearchCV\n        if isinstance(self.clf, GridSearchCV):\n            import warnings\n\n            if feat_select is not None:\n                warnings.warn(\n                    \"Cross-validated feature selection not supported with \"\n                    \"GridSearchCV\")\n            self.clf.set_params(cv=self.cver, scoring=scoring)\n\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore', category=UserWarning)\n                self.clf = self.clf.fit(X, y)\n\n            self.cvs = self.clf.best_score_\n        else:\n            self.cvs = self.feat_select_cvs(\n                feat_select=feat_select, scoring=scoring)\n\n        if feat_select is not None:\n            fs = feature_selection(\n                feat_select, X, y)\n            self.features_selected.append(fs)\n\n            X = X[:, fs]\n\n        self.clf.fit(X, y)\n\n        return self.cvs.mean()", "language": "python", "code": "def cross_val_fit(self, X, y, cross_val='4-Fold', scoring='accuracy',\n                      feat_select=None, class_weight='auto'):\n        \"\"\" Fits X to outcomes y, using clf and cv_method \"\"\"\n\n        from sklearn import cross_validation\n\n        self.X = X\n        self.y = y\n\n        self.set_class_weight(class_weight=class_weight, y=y)\n\n        # Set cross validator\n        if isinstance(cross_val, string_types):\n            if re.match('.*-Fold', cross_val) is not None:\n                n = int(cross_val.split('-')[0])\n                self.cver = cross_validation.StratifiedKFold(self.y, n)\n            else:\n                raise Exception('Unrecognized cross validation method')\n        else:\n            self.cver = cross_val\n\n        if feat_select is not None:\n            self.features_selected = []\n\n        # Perform cross-validated classification\n        from sklearn.grid_search import GridSearchCV\n        if isinstance(self.clf, GridSearchCV):\n            import warnings\n\n            if feat_select is not None:\n                warnings.warn(\n                    \"Cross-validated feature selection not supported with \"\n                    \"GridSearchCV\")\n            self.clf.set_params(cv=self.cver, scoring=scoring)\n\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore', category=UserWarning)\n                self.clf = self.clf.fit(X, y)\n\n            self.cvs = self.clf.best_score_\n        else:\n            self.cvs = self.feat_select_cvs(\n                feat_select=feat_select, scoring=scoring)\n\n        if feat_select is not None:\n            fs = feature_selection(\n                feat_select, X, y)\n            self.features_selected.append(fs)\n\n            X = X[:, fs]\n\n        self.clf.fit(X, y)\n\n        return self.cvs.mean()", "code_tokens": ["def", "cross_val_fit", "(", "self", ",", "X", ",", "y", ",", "cross_val", "=", "'4-Fold'", ",", "scoring", "=", "'accuracy'", ",", "feat_select", "=", "None", ",", "class_weight", "=", "'auto'", ")", ":", "from", "sklearn", "import", "cross_validation", "self", ".", "X", "=", "X", "self", ".", "y", "=", "y", "self", ".", "set_class_weight", "(", "class_weight", "=", "class_weight", ",", "y", "=", "y", ")", "# Set cross validator", "if", "isinstance", "(", "cross_val", ",", "string_types", ")", ":", "if", "re", ".", "match", "(", "'.*-Fold'", ",", "cross_val", ")", "is", "not", "None", ":", "n", "=", "int", "(", "cross_val", ".", "split", "(", "'-'", ")", "[", "0", "]", ")", "self", ".", "cver", "=", "cross_validation", ".", "StratifiedKFold", "(", "self", ".", "y", ",", "n", ")", "else", ":", "raise", "Exception", "(", "'Unrecognized cross validation method'", ")", "else", ":", "self", ".", "cver", "=", "cross_val", "if", "feat_select", "is", "not", "None", ":", "self", ".", "features_selected", "=", "[", "]", "# Perform cross-validated classification", "from", "sklearn", ".", "grid_search", "import", "GridSearchCV", "if", "isinstance", "(", "self", ".", "clf", ",", "GridSearchCV", ")", ":", "import", "warnings", "if", "feat_select", "is", "not", "None", ":", "warnings", ".", "warn", "(", "\"Cross-validated feature selection not supported with \"", "\"GridSearchCV\"", ")", "self", ".", "clf", ".", "set_params", "(", "cv", "=", "self", ".", "cver", ",", "scoring", "=", "scoring", ")", "with", "warnings", ".", "catch_warnings", "(", ")", ":", "warnings", ".", "simplefilter", "(", "'ignore'", ",", "category", "=", "UserWarning", ")", "self", ".", "clf", "=", "self", ".", "clf", ".", "fit", "(", "X", ",", "y", ")", "self", ".", "cvs", "=", "self", ".", "clf", ".", "best_score_", "else", ":", "self", ".", "cvs", "=", "self", ".", "feat_select_cvs", "(", "feat_select", "=", "feat_select", ",", "scoring", "=", "scoring", ")", "if", "feat_select", "is", "not", "None", ":", "fs", "=", "feature_selection", "(", "feat_select", ",", "X", ",", "y", ")", "self", ".", "features_selected", ".", "append", "(", "fs", ")", "X", "=", "X", "[", ":", ",", "fs", "]", "self", ".", "clf", ".", "fit", "(", "X", ",", "y", ")", "return", "self", ".", "cvs", ".", "mean", "(", ")"], "docstring": "Fits X to outcomes y, using clf and cv_method", "docstring_tokens": ["Fits", "X", "to", "outcomes", "y", "using", "clf", "and", "cv_method"], "sha": "948ce7edce15d7df693446e76834e0c23bfe8f11", "url": "https://github.com/neurosynth/neurosynth/blob/948ce7edce15d7df693446e76834e0c23bfe8f11/neurosynth/analysis/classify.py#L333-L386", "partition": "test"}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/iqiyi.py", "func_name": "Iqiyi.download", "original_string": "def download(self, **kwargs):\n        \"\"\"Override the original one\n        Ugly ugly dirty hack\"\"\"\n        if 'json_output' in kwargs and kwargs['json_output']:\n            json_output.output(self)\n        elif 'info_only' in kwargs and kwargs['info_only']:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Display the stream\n                stream_id = kwargs['stream_id']\n                if 'index' not in kwargs:\n                    self.p(stream_id)\n                else:\n                    self.p_i(stream_id)\n            else:\n                # Display all available streams\n                if 'index' not in kwargs:\n                    self.p([])\n                else:\n                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n                    self.p_i(stream_id)\n\n        else:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Download the stream\n                stream_id = kwargs['stream_id']\n            else:\n                # Download stream with the best quality\n                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n\n            if 'index' not in kwargs:\n                self.p(stream_id)\n            else:\n                self.p_i(stream_id)\n\n            if stream_id in self.streams:\n                urls = self.streams[stream_id]['src']\n                ext = self.streams[stream_id]['container']\n                total_size = self.streams[stream_id]['size']\n            else:\n                urls = self.dash_streams[stream_id]['src']\n                ext = self.dash_streams[stream_id]['container']\n                total_size = self.dash_streams[stream_id]['size']\n\n            if not urls:\n                log.wtf('[Failed] Cannot extract video source.')\n            # For legacy main()\n            \n            #Here's the change!!\n            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n\n            if not kwargs['caption']:\n                print('Skipping captions.')\n                return\n            for lang in self.caption_tracks:\n                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n                print('Saving %s ... ' % filename, end=\"\", flush=True)\n                srt = self.caption_tracks[lang]\n                with open(os.path.join(kwargs['output_dir'], filename),\n                          'w', encoding='utf-8') as x:\n                    x.write(srt)\n                print('Done.')", "language": "python", "code": "def download(self, **kwargs):\n        \"\"\"Override the original one\n        Ugly ugly dirty hack\"\"\"\n        if 'json_output' in kwargs and kwargs['json_output']:\n            json_output.output(self)\n        elif 'info_only' in kwargs and kwargs['info_only']:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Display the stream\n                stream_id = kwargs['stream_id']\n                if 'index' not in kwargs:\n                    self.p(stream_id)\n                else:\n                    self.p_i(stream_id)\n            else:\n                # Display all available streams\n                if 'index' not in kwargs:\n                    self.p([])\n                else:\n                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n                    self.p_i(stream_id)\n\n        else:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Download the stream\n                stream_id = kwargs['stream_id']\n            else:\n                # Download stream with the best quality\n                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n\n            if 'index' not in kwargs:\n                self.p(stream_id)\n            else:\n                self.p_i(stream_id)\n\n            if stream_id in self.streams:\n                urls = self.streams[stream_id]['src']\n                ext = self.streams[stream_id]['container']\n                total_size = self.streams[stream_id]['size']\n            else:\n                urls = self.dash_streams[stream_id]['src']\n                ext = self.dash_streams[stream_id]['container']\n                total_size = self.dash_streams[stream_id]['size']\n\n            if not urls:\n                log.wtf('[Failed] Cannot extract video source.')\n            # For legacy main()\n            \n            #Here's the change!!\n            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n\n            if not kwargs['caption']:\n                print('Skipping captions.')\n                return\n            for lang in self.caption_tracks:\n                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n                print('Saving %s ... ' % filename, end=\"\", flush=True)\n                srt = self.caption_tracks[lang]\n                with open(os.path.join(kwargs['output_dir'], filename),\n                          'w', encoding='utf-8') as x:\n                    x.write(srt)\n                print('Done.')", "code_tokens": ["def", "download", "(", "self", ",", "*", "*", "kwargs", ")", ":", "if", "'json_output'", "in", "kwargs", "and", "kwargs", "[", "'json_output'", "]", ":", "json_output", ".", "output", "(", "self", ")", "elif", "'info_only'", "in", "kwargs", "and", "kwargs", "[", "'info_only'", "]", ":", "if", "'stream_id'", "in", "kwargs", "and", "kwargs", "[", "'stream_id'", "]", ":", "# Display the stream", "stream_id", "=", "kwargs", "[", "'stream_id'", "]", "if", "'index'", "not", "in", "kwargs", ":", "self", ".", "p", "(", "stream_id", ")", "else", ":", "self", ".", "p_i", "(", "stream_id", ")", "else", ":", "# Display all available streams", "if", "'index'", "not", "in", "kwargs", ":", "self", ".", "p", "(", "[", "]", ")", "else", ":", "stream_id", "=", "self", ".", "streams_sorted", "[", "0", "]", "[", "'id'", "]", "if", "'id'", "in", "self", ".", "streams_sorted", "[", "0", "]", "else", "self", ".", "streams_sorted", "[", "0", "]", "[", "'itag'", "]", "self", ".", "p_i", "(", "stream_id", ")", "else", ":", "if", "'stream_id'", "in", "kwargs", "and", "kwargs", "[", "'stream_id'", "]", ":", "# Download the stream", "stream_id", "=", "kwargs", "[", "'stream_id'", "]", "else", ":", "# Download stream with the best quality", "stream_id", "=", "self", ".", "streams_sorted", "[", "0", "]", "[", "'id'", "]", "if", "'id'", "in", "self", ".", "streams_sorted", "[", "0", "]", "else", "self", ".", "streams_sorted", "[", "0", "]", "[", "'itag'", "]", "if", "'index'", "not", "in", "kwargs", ":", "self", ".", "p", "(", "stream_id", ")", "else", ":", "self", ".", "p_i", "(", "stream_id", ")", "if", "stream_id", "in", "self", ".", "streams", ":", "urls", "=", "self", ".", "streams", "[", "stream_id", "]", "[", "'src'", "]", "ext", "=", "self", ".", "streams", "[", "stream_id", "]", "[", "'container'", "]", "total_size", "=", "self", ".", "streams", "[", "stream_id", "]", "[", "'size'", "]", "else", ":", "urls", "=", "self", ".", "dash_streams", "[", "stream_id", "]", "[", "'src'", "]", "ext", "=", "self", ".", "dash_streams", "[", "stream_id", "]", "[", "'container'", "]", "total_size", "=", "self", ".", "dash_streams", "[", "stream_id", "]", "[", "'size'", "]", "if", "not", "urls", ":", "log", ".", "wtf", "(", "'[Failed] Cannot extract video source.'", ")", "# For legacy main()", "#Here's the change!!", "download_url_ffmpeg", "(", "urls", "[", "0", "]", ",", "self", ".", "title", ",", "'mp4'", ",", "output_dir", "=", "kwargs", "[", "'output_dir'", "]", ",", "merge", "=", "kwargs", "[", "'merge'", "]", ",", "stream", "=", "False", ")", "if", "not", "kwargs", "[", "'caption'", "]", ":", "print", "(", "'Skipping captions.'", ")", "return", "for", "lang", "in", "self", ".", "caption_tracks", ":", "filename", "=", "'%s.%s.srt'", "%", "(", "get_filename", "(", "self", ".", "title", ")", ",", "lang", ")", "print", "(", "'Saving %s ... '", "%", "filename", ",", "end", "=", "\"\"", ",", "flush", "=", "True", ")", "srt", "=", "self", ".", "caption_tracks", "[", "lang", "]", "with", "open", "(", "os", ".", "path", ".", "join", "(", "kwargs", "[", "'output_dir'", "]", ",", "filename", ")", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "x", ":", "x", ".", "write", "(", "srt", ")", "print", "(", "'Done.'", ")"], "docstring": "Override the original one\n        Ugly ugly dirty hack", "docstring_tokens": ["Override", "the", "original", "one", "Ugly", "ugly", "dirty", "hack"], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/iqiyi.py#L158-L218", "partition": "test"}
{"repo": "boto/s3transfer", "path": "s3transfer/manager.py", "func_name": "TransferManager.download", "original_string": "def download(self, bucket, key, fileobj, extra_args=None,\n                 subscribers=None):\n        \"\"\"Downloads a file from S3\n\n        :type bucket: str\n        :param bucket: The name of the bucket to download from\n\n        :type key: str\n        :param key: The name of the key to download from\n\n        :type fileobj: str or seekable file-like object\n        :param fileobj: The name of a file to download or a seekable file-like\n            object to download. It is recommended to use a filename because\n            file-like objects may result in higher memory usage.\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments that may be passed to the\n            client operation\n\n        :type subscribers: list(s3transfer.subscribers.BaseSubscriber)\n        :param subscribers: The list of subscribers to be invoked in the\n            order provided based on the event emit during the process of\n            the transfer request.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :returns: Transfer future representing the download\n        \"\"\"\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = []\n        self._validate_all_known_args(extra_args, self.ALLOWED_DOWNLOAD_ARGS)\n        call_args = CallArgs(\n            bucket=bucket, key=key, fileobj=fileobj, extra_args=extra_args,\n            subscribers=subscribers\n        )\n        extra_main_kwargs = {'io_executor': self._io_executor}\n        if self._bandwidth_limiter:\n            extra_main_kwargs['bandwidth_limiter'] = self._bandwidth_limiter\n        return self._submit_transfer(\n            call_args, DownloadSubmissionTask, extra_main_kwargs)", "language": "python", "code": "def download(self, bucket, key, fileobj, extra_args=None,\n                 subscribers=None):\n        \"\"\"Downloads a file from S3\n\n        :type bucket: str\n        :param bucket: The name of the bucket to download from\n\n        :type key: str\n        :param key: The name of the key to download from\n\n        :type fileobj: str or seekable file-like object\n        :param fileobj: The name of a file to download or a seekable file-like\n            object to download. It is recommended to use a filename because\n            file-like objects may result in higher memory usage.\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments that may be passed to the\n            client operation\n\n        :type subscribers: list(s3transfer.subscribers.BaseSubscriber)\n        :param subscribers: The list of subscribers to be invoked in the\n            order provided based on the event emit during the process of\n            the transfer request.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :returns: Transfer future representing the download\n        \"\"\"\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = []\n        self._validate_all_known_args(extra_args, self.ALLOWED_DOWNLOAD_ARGS)\n        call_args = CallArgs(\n            bucket=bucket, key=key, fileobj=fileobj, extra_args=extra_args,\n            subscribers=subscribers\n        )\n        extra_main_kwargs = {'io_executor': self._io_executor}\n        if self._bandwidth_limiter:\n            extra_main_kwargs['bandwidth_limiter'] = self._bandwidth_limiter\n        return self._submit_transfer(\n            call_args, DownloadSubmissionTask, extra_main_kwargs)", "code_tokens": ["def", "download", "(", "self", ",", "bucket", ",", "key", ",", "fileobj", ",", "extra_args", "=", "None", ",", "subscribers", "=", "None", ")", ":", "if", "extra_args", "is", "None", ":", "extra_args", "=", "{", "}", "if", "subscribers", "is", "None", ":", "subscribers", "=", "[", "]", "self", ".", "_validate_all_known_args", "(", "extra_args", ",", "self", ".", "ALLOWED_DOWNLOAD_ARGS", ")", "call_args", "=", "CallArgs", "(", "bucket", "=", "bucket", ",", "key", "=", "key", ",", "fileobj", "=", "fileobj", ",", "extra_args", "=", "extra_args", ",", "subscribers", "=", "subscribers", ")", "extra_main_kwargs", "=", "{", "'io_executor'", ":", "self", ".", "_io_executor", "}", "if", "self", ".", "_bandwidth_limiter", ":", "extra_main_kwargs", "[", "'bandwidth_limiter'", "]", "=", "self", ".", "_bandwidth_limiter", "return", "self", ".", "_submit_transfer", "(", "call_args", ",", "DownloadSubmissionTask", ",", "extra_main_kwargs", ")"], "docstring": "Downloads a file from S3\n\n        :type bucket: str\n        :param bucket: The name of the bucket to download from\n\n        :type key: str\n        :param key: The name of the key to download from\n\n        :type fileobj: str or seekable file-like object\n        :param fileobj: The name of a file to download or a seekable file-like\n            object to download. It is recommended to use a filename because\n            file-like objects may result in higher memory usage.\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments that may be passed to the\n            client operation\n\n        :type subscribers: list(s3transfer.subscribers.BaseSubscriber)\n        :param subscribers: The list of subscribers to be invoked in the\n            order provided based on the event emit during the process of\n            the transfer request.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :returns: Transfer future representing the download", "docstring_tokens": ["Downloads", "a", "file", "from", "S3"], "sha": "2aead638c8385d8ae0b1756b2de17e8fad45fffa", "url": "https://github.com/boto/s3transfer/blob/2aead638c8385d8ae0b1756b2de17e8fad45fffa/s3transfer/manager.py#L305-L345", "partition": "test"}
{"repo": "pyca/pyopenssl", "path": "src/OpenSSL/SSL.py", "func_name": "Connection.get_alpn_proto_negotiated", "original_string": "def get_alpn_proto_negotiated(self):\n        \"\"\"\n        Get the protocol that was negotiated by ALPN.\n\n        :returns: A bytestring of the protocol name.  If no protocol has been\n            negotiated yet, returns an empty string.\n        \"\"\"\n        data = _ffi.new(\"unsigned char **\")\n        data_len = _ffi.new(\"unsigned int *\")\n\n        _lib.SSL_get0_alpn_selected(self._ssl, data, data_len)\n\n        if not data_len:\n            return b''\n\n        return _ffi.buffer(data[0], data_len[0])[:]", "language": "python", "code": "def get_alpn_proto_negotiated(self):\n        \"\"\"\n        Get the protocol that was negotiated by ALPN.\n\n        :returns: A bytestring of the protocol name.  If no protocol has been\n            negotiated yet, returns an empty string.\n        \"\"\"\n        data = _ffi.new(\"unsigned char **\")\n        data_len = _ffi.new(\"unsigned int *\")\n\n        _lib.SSL_get0_alpn_selected(self._ssl, data, data_len)\n\n        if not data_len:\n            return b''\n\n        return _ffi.buffer(data[0], data_len[0])[:]", "code_tokens": ["def", "get_alpn_proto_negotiated", "(", "self", ")", ":", "data", "=", "_ffi", ".", "new", "(", "\"unsigned char **\"", ")", "data_len", "=", "_ffi", ".", "new", "(", "\"unsigned int *\"", ")", "_lib", ".", "SSL_get0_alpn_selected", "(", "self", ".", "_ssl", ",", "data", ",", "data_len", ")", "if", "not", "data_len", ":", "return", "b''", "return", "_ffi", ".", "buffer", "(", "data", "[", "0", "]", ",", "data_len", "[", "0", "]", ")", "[", ":", "]"], "docstring": "Get the protocol that was negotiated by ALPN.\n\n        :returns: A bytestring of the protocol name.  If no protocol has been\n            negotiated yet, returns an empty string.", "docstring_tokens": ["Get", "the", "protocol", "that", "was", "negotiated", "by", "ALPN", "."], "sha": "1fbe064c50fd030948141d7d630673761525b0d0", "url": "https://github.com/pyca/pyopenssl/blob/1fbe064c50fd030948141d7d630673761525b0d0/src/OpenSSL/SSL.py#L2471-L2486", "partition": "test"}
{"repo": "lepture/flask-oauthlib", "path": "flask_oauthlib/contrib/apps.py", "func_name": "RemoteAppFactory.register_to", "original_string": "def register_to(self, oauth, name=None, **kwargs):\n        \"\"\"Creates a remote app and registers it.\"\"\"\n        kwargs = self._process_kwargs(\n            name=(name or self.default_name), **kwargs)\n        return oauth.remote_app(**kwargs)", "language": "python", "code": "def register_to(self, oauth, name=None, **kwargs):\n        \"\"\"Creates a remote app and registers it.\"\"\"\n        kwargs = self._process_kwargs(\n            name=(name or self.default_name), **kwargs)\n        return oauth.remote_app(**kwargs)", "code_tokens": ["def", "register_to", "(", "self", ",", "oauth", ",", "name", "=", "None", ",", "*", "*", "kwargs", ")", ":", "kwargs", "=", "self", ".", "_process_kwargs", "(", "name", "=", "(", "name", "or", "self", ".", "default_name", ")", ",", "*", "*", "kwargs", ")", "return", "oauth", ".", "remote_app", "(", "*", "*", "kwargs", ")"], "docstring": "Creates a remote app and registers it.", "docstring_tokens": ["Creates", "a", "remote", "app", "and", "registers", "it", "."], "sha": "9e6f152a5bb360e7496210da21561c3e6d41b0e1", "url": "https://github.com/lepture/flask-oauthlib/blob/9e6f152a5bb360e7496210da21561c3e6d41b0e1/flask_oauthlib/contrib/apps.py#L57-L61", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/git.py", "func_name": "GitRepository._fetch_pack", "original_string": "def _fetch_pack(self):\n        \"\"\"Fetch changes and store them in a pack.\"\"\"\n\n        def prepare_refs(refs):\n            return [ref.hash.encode('utf-8') for ref in refs\n                    if not ref.refname.endswith('^{}')]\n\n        def determine_wants(refs):\n            remote_refs = prepare_refs(self._discover_refs(remote=True))\n            local_refs = prepare_refs(self._discover_refs())\n            wants = [ref for ref in remote_refs if ref not in local_refs]\n            return wants\n\n        client, repo_path = dulwich.client.get_transport_and_path(self.uri)\n        repo = dulwich.repo.Repo(self.dirpath)\n        fd = io.BytesIO()\n\n        local_refs = self._discover_refs()\n        graph_walker = _GraphWalker(local_refs)\n\n        result = client.fetch_pack(repo_path,\n                                   determine_wants,\n                                   graph_walker,\n                                   fd.write)\n        refs = [GitRef(ref_hash.decode('utf-8'), ref_name.decode('utf-8'))\n                for ref_name, ref_hash in result.refs.items()]\n\n        if len(fd.getvalue()) > 0:\n            fd.seek(0)\n            pack = repo.object_store.add_thin_pack(fd.read, None)\n            pack_name = pack.name().decode('utf-8')\n        else:\n            pack_name = None\n\n        return (pack_name, refs)", "language": "python", "code": "def _fetch_pack(self):\n        \"\"\"Fetch changes and store them in a pack.\"\"\"\n\n        def prepare_refs(refs):\n            return [ref.hash.encode('utf-8') for ref in refs\n                    if not ref.refname.endswith('^{}')]\n\n        def determine_wants(refs):\n            remote_refs = prepare_refs(self._discover_refs(remote=True))\n            local_refs = prepare_refs(self._discover_refs())\n            wants = [ref for ref in remote_refs if ref not in local_refs]\n            return wants\n\n        client, repo_path = dulwich.client.get_transport_and_path(self.uri)\n        repo = dulwich.repo.Repo(self.dirpath)\n        fd = io.BytesIO()\n\n        local_refs = self._discover_refs()\n        graph_walker = _GraphWalker(local_refs)\n\n        result = client.fetch_pack(repo_path,\n                                   determine_wants,\n                                   graph_walker,\n                                   fd.write)\n        refs = [GitRef(ref_hash.decode('utf-8'), ref_name.decode('utf-8'))\n                for ref_name, ref_hash in result.refs.items()]\n\n        if len(fd.getvalue()) > 0:\n            fd.seek(0)\n            pack = repo.object_store.add_thin_pack(fd.read, None)\n            pack_name = pack.name().decode('utf-8')\n        else:\n            pack_name = None\n\n        return (pack_name, refs)", "code_tokens": ["def", "_fetch_pack", "(", "self", ")", ":", "def", "prepare_refs", "(", "refs", ")", ":", "return", "[", "ref", ".", "hash", ".", "encode", "(", "'utf-8'", ")", "for", "ref", "in", "refs", "if", "not", "ref", ".", "refname", ".", "endswith", "(", "'^{}'", ")", "]", "def", "determine_wants", "(", "refs", ")", ":", "remote_refs", "=", "prepare_refs", "(", "self", ".", "_discover_refs", "(", "remote", "=", "True", ")", ")", "local_refs", "=", "prepare_refs", "(", "self", ".", "_discover_refs", "(", ")", ")", "wants", "=", "[", "ref", "for", "ref", "in", "remote_refs", "if", "ref", "not", "in", "local_refs", "]", "return", "wants", "client", ",", "repo_path", "=", "dulwich", ".", "client", ".", "get_transport_and_path", "(", "self", ".", "uri", ")", "repo", "=", "dulwich", ".", "repo", ".", "Repo", "(", "self", ".", "dirpath", ")", "fd", "=", "io", ".", "BytesIO", "(", ")", "local_refs", "=", "self", ".", "_discover_refs", "(", ")", "graph_walker", "=", "_GraphWalker", "(", "local_refs", ")", "result", "=", "client", ".", "fetch_pack", "(", "repo_path", ",", "determine_wants", ",", "graph_walker", ",", "fd", ".", "write", ")", "refs", "=", "[", "GitRef", "(", "ref_hash", ".", "decode", "(", "'utf-8'", ")", ",", "ref_name", ".", "decode", "(", "'utf-8'", ")", ")", "for", "ref_name", ",", "ref_hash", "in", "result", ".", "refs", ".", "items", "(", ")", "]", "if", "len", "(", "fd", ".", "getvalue", "(", ")", ")", ">", "0", ":", "fd", ".", "seek", "(", "0", ")", "pack", "=", "repo", ".", "object_store", ".", "add_thin_pack", "(", "fd", ".", "read", ",", "None", ")", "pack_name", "=", "pack", ".", "name", "(", ")", ".", "decode", "(", "'utf-8'", ")", "else", ":", "pack_name", "=", "None", "return", "(", "pack_name", ",", "refs", ")"], "docstring": "Fetch changes and store them in a pack.", "docstring_tokens": ["Fetch", "changes", "and", "store", "them", "in", "a", "pack", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/git.py#L1083-L1117", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/strings.py", "func_name": "StringConstantChecker.process_non_raw_string_token", "original_string": "def process_non_raw_string_token(self, prefix, string_body, start_row):\n        \"\"\"check for bad escapes in a non-raw string.\n\n        prefix: lowercase string of eg 'ur' string prefix markers.\n        string_body: the un-parsed body of the string, not including the quote\n        marks.\n        start_row: integer line number in the source.\n        \"\"\"\n        # Walk through the string; if we see a backslash then escape the next\n        # character, and skip over it.  If we see a non-escaped character,\n        # alert, and continue.\n        #\n        # Accept a backslash when it escapes a backslash, or a quote, or\n        # end-of-line, or one of the letters that introduce a special escape\n        # sequence <http://docs.python.org/reference/lexical_analysis.html>\n        #\n        # TODO(mbp): Maybe give a separate warning about the rarely-used\n        # \\a \\b \\v \\f?\n        #\n        # TODO(mbp): We could give the column of the problem character, but\n        # add_message doesn't seem to have a way to pass it through at present.\n        i = 0\n        while True:\n            i = string_body.find(\"\\\\\", i)\n            if i == -1:\n                break\n            # There must be a next character; having a backslash at the end\n            # of the string would be a SyntaxError.\n            next_char = string_body[i + 1]\n            match = string_body[i : i + 2]\n            if next_char in self.UNICODE_ESCAPE_CHARACTERS:\n                if \"u\" in prefix:\n                    pass\n                elif (_PY3K or self._unicode_literals) and \"b\" not in prefix:\n                    pass  # unicode by default\n                else:\n                    self.add_message(\n                        \"anomalous-unicode-escape-in-string\",\n                        line=start_row,\n                        args=(match,),\n                    )\n            elif next_char not in self.ESCAPE_CHARACTERS:\n                self.add_message(\n                    \"anomalous-backslash-in-string\", line=start_row, args=(match,)\n                )\n            # Whether it was a valid escape or not, backslash followed by\n            # another character can always be consumed whole: the second\n            # character can never be the start of a new backslash escape.\n            i += 2", "language": "python", "code": "def process_non_raw_string_token(self, prefix, string_body, start_row):\n        \"\"\"check for bad escapes in a non-raw string.\n\n        prefix: lowercase string of eg 'ur' string prefix markers.\n        string_body: the un-parsed body of the string, not including the quote\n        marks.\n        start_row: integer line number in the source.\n        \"\"\"\n        # Walk through the string; if we see a backslash then escape the next\n        # character, and skip over it.  If we see a non-escaped character,\n        # alert, and continue.\n        #\n        # Accept a backslash when it escapes a backslash, or a quote, or\n        # end-of-line, or one of the letters that introduce a special escape\n        # sequence <http://docs.python.org/reference/lexical_analysis.html>\n        #\n        # TODO(mbp): Maybe give a separate warning about the rarely-used\n        # \\a \\b \\v \\f?\n        #\n        # TODO(mbp): We could give the column of the problem character, but\n        # add_message doesn't seem to have a way to pass it through at present.\n        i = 0\n        while True:\n            i = string_body.find(\"\\\\\", i)\n            if i == -1:\n                break\n            # There must be a next character; having a backslash at the end\n            # of the string would be a SyntaxError.\n            next_char = string_body[i + 1]\n            match = string_body[i : i + 2]\n            if next_char in self.UNICODE_ESCAPE_CHARACTERS:\n                if \"u\" in prefix:\n                    pass\n                elif (_PY3K or self._unicode_literals) and \"b\" not in prefix:\n                    pass  # unicode by default\n                else:\n                    self.add_message(\n                        \"anomalous-unicode-escape-in-string\",\n                        line=start_row,\n                        args=(match,),\n                    )\n            elif next_char not in self.ESCAPE_CHARACTERS:\n                self.add_message(\n                    \"anomalous-backslash-in-string\", line=start_row, args=(match,)\n                )\n            # Whether it was a valid escape or not, backslash followed by\n            # another character can always be consumed whole: the second\n            # character can never be the start of a new backslash escape.\n            i += 2", "code_tokens": ["def", "process_non_raw_string_token", "(", "self", ",", "prefix", ",", "string_body", ",", "start_row", ")", ":", "# Walk through the string; if we see a backslash then escape the next", "# character, and skip over it.  If we see a non-escaped character,", "# alert, and continue.", "#", "# Accept a backslash when it escapes a backslash, or a quote, or", "# end-of-line, or one of the letters that introduce a special escape", "# sequence <http://docs.python.org/reference/lexical_analysis.html>", "#", "# TODO(mbp): Maybe give a separate warning about the rarely-used", "# \\a \\b \\v \\f?", "#", "# TODO(mbp): We could give the column of the problem character, but", "# add_message doesn't seem to have a way to pass it through at present.", "i", "=", "0", "while", "True", ":", "i", "=", "string_body", ".", "find", "(", "\"\\\\\"", ",", "i", ")", "if", "i", "==", "-", "1", ":", "break", "# There must be a next character; having a backslash at the end", "# of the string would be a SyntaxError.", "next_char", "=", "string_body", "[", "i", "+", "1", "]", "match", "=", "string_body", "[", "i", ":", "i", "+", "2", "]", "if", "next_char", "in", "self", ".", "UNICODE_ESCAPE_CHARACTERS", ":", "if", "\"u\"", "in", "prefix", ":", "pass", "elif", "(", "_PY3K", "or", "self", ".", "_unicode_literals", ")", "and", "\"b\"", "not", "in", "prefix", ":", "pass", "# unicode by default", "else", ":", "self", ".", "add_message", "(", "\"anomalous-unicode-escape-in-string\"", ",", "line", "=", "start_row", ",", "args", "=", "(", "match", ",", ")", ",", ")", "elif", "next_char", "not", "in", "self", ".", "ESCAPE_CHARACTERS", ":", "self", ".", "add_message", "(", "\"anomalous-backslash-in-string\"", ",", "line", "=", "start_row", ",", "args", "=", "(", "match", ",", ")", ")", "# Whether it was a valid escape or not, backslash followed by", "# another character can always be consumed whole: the second", "# character can never be the start of a new backslash escape.", "i", "+=", "2"], "docstring": "check for bad escapes in a non-raw string.\n\n        prefix: lowercase string of eg 'ur' string prefix markers.\n        string_body: the un-parsed body of the string, not including the quote\n        marks.\n        start_row: integer line number in the source.", "docstring_tokens": ["check", "for", "bad", "escapes", "in", "a", "non", "-", "raw", "string", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/strings.py#L693-L741", "partition": "test"}
{"repo": "maxcountryman/flask-uploads", "path": "flask_uploads.py", "func_name": "configure_uploads", "original_string": "def configure_uploads(app, upload_sets):\n    \"\"\"\n    Call this after the app has been configured. It will go through all the\n    upload sets, get their configuration, and store the configuration on the\n    app. It will also register the uploads module if it hasn't been set. This\n    can be called multiple times with different upload sets.\n\n    .. versionchanged:: 0.1.3\n       The uploads module/blueprint will only be registered if it is needed\n       to serve the upload sets.\n\n    :param app: The `~flask.Flask` instance to get the configuration from.\n    :param upload_sets: The `UploadSet` instances to configure.\n    \"\"\"\n    if isinstance(upload_sets, UploadSet):\n        upload_sets = (upload_sets,)\n\n    if not hasattr(app, 'upload_set_config'):\n        app.upload_set_config = {}\n    set_config = app.upload_set_config\n    defaults = dict(dest=app.config.get('UPLOADS_DEFAULT_DEST'),\n                    url=app.config.get('UPLOADS_DEFAULT_URL'))\n\n    for uset in upload_sets:\n        config = config_for_set(uset, app, defaults)\n        set_config[uset.name] = config\n\n    should_serve = any(s.base_url is None for s in set_config.values())\n    if '_uploads' not in app.blueprints and should_serve:\n        app.register_blueprint(uploads_mod)", "language": "python", "code": "def configure_uploads(app, upload_sets):\n    \"\"\"\n    Call this after the app has been configured. It will go through all the\n    upload sets, get their configuration, and store the configuration on the\n    app. It will also register the uploads module if it hasn't been set. This\n    can be called multiple times with different upload sets.\n\n    .. versionchanged:: 0.1.3\n       The uploads module/blueprint will only be registered if it is needed\n       to serve the upload sets.\n\n    :param app: The `~flask.Flask` instance to get the configuration from.\n    :param upload_sets: The `UploadSet` instances to configure.\n    \"\"\"\n    if isinstance(upload_sets, UploadSet):\n        upload_sets = (upload_sets,)\n\n    if not hasattr(app, 'upload_set_config'):\n        app.upload_set_config = {}\n    set_config = app.upload_set_config\n    defaults = dict(dest=app.config.get('UPLOADS_DEFAULT_DEST'),\n                    url=app.config.get('UPLOADS_DEFAULT_URL'))\n\n    for uset in upload_sets:\n        config = config_for_set(uset, app, defaults)\n        set_config[uset.name] = config\n\n    should_serve = any(s.base_url is None for s in set_config.values())\n    if '_uploads' not in app.blueprints and should_serve:\n        app.register_blueprint(uploads_mod)", "code_tokens": ["def", "configure_uploads", "(", "app", ",", "upload_sets", ")", ":", "if", "isinstance", "(", "upload_sets", ",", "UploadSet", ")", ":", "upload_sets", "=", "(", "upload_sets", ",", ")", "if", "not", "hasattr", "(", "app", ",", "'upload_set_config'", ")", ":", "app", ".", "upload_set_config", "=", "{", "}", "set_config", "=", "app", ".", "upload_set_config", "defaults", "=", "dict", "(", "dest", "=", "app", ".", "config", ".", "get", "(", "'UPLOADS_DEFAULT_DEST'", ")", ",", "url", "=", "app", ".", "config", ".", "get", "(", "'UPLOADS_DEFAULT_URL'", ")", ")", "for", "uset", "in", "upload_sets", ":", "config", "=", "config_for_set", "(", "uset", ",", "app", ",", "defaults", ")", "set_config", "[", "uset", ".", "name", "]", "=", "config", "should_serve", "=", "any", "(", "s", ".", "base_url", "is", "None", "for", "s", "in", "set_config", ".", "values", "(", ")", ")", "if", "'_uploads'", "not", "in", "app", ".", "blueprints", "and", "should_serve", ":", "app", ".", "register_blueprint", "(", "uploads_mod", ")"], "docstring": "Call this after the app has been configured. It will go through all the\n    upload sets, get their configuration, and store the configuration on the\n    app. It will also register the uploads module if it hasn't been set. This\n    can be called multiple times with different upload sets.\n\n    .. versionchanged:: 0.1.3\n       The uploads module/blueprint will only be registered if it is needed\n       to serve the upload sets.\n\n    :param app: The `~flask.Flask` instance to get the configuration from.\n    :param upload_sets: The `UploadSet` instances to configure.", "docstring_tokens": ["Call", "this", "after", "the", "app", "has", "been", "configured", ".", "It", "will", "go", "through", "all", "the", "upload", "sets", "get", "their", "configuration", "and", "store", "the", "configuration", "on", "the", "app", ".", "It", "will", "also", "register", "the", "uploads", "module", "if", "it", "hasn", "t", "been", "set", ".", "This", "can", "be", "called", "multiple", "times", "with", "different", "upload", "sets", "."], "sha": "dc24fa0c53d605876e5b4502cadffdf1a4345b1d", "url": "https://github.com/maxcountryman/flask-uploads/blob/dc24fa0c53d605876e5b4502cadffdf1a4345b1d/flask_uploads.py#L193-L222", "partition": "test"}
{"repo": "3DLIRIOUS/MeshLabXML", "path": "meshlabxml/util.py", "func_name": "check_list", "original_string": "def check_list(var, num_terms):\n    \"\"\" Check if a variable is a list and is the correct length.\n\n    If variable is not a list it will make it a list of the correct length with\n    all terms identical.\n    \"\"\"\n    if not isinstance(var, list):\n        if isinstance(var, tuple):\n            var = list(var)\n        else:\n            var = [var]\n        for _ in range(1, num_terms):\n            var.append(var[0])\n    if len(var) != num_terms:\n        print(\n            '\"%s\" has the wrong number of terms; it needs %s. Exiting ...' %\n            (var, num_terms))\n        sys.exit(1)\n    return var", "language": "python", "code": "def check_list(var, num_terms):\n    \"\"\" Check if a variable is a list and is the correct length.\n\n    If variable is not a list it will make it a list of the correct length with\n    all terms identical.\n    \"\"\"\n    if not isinstance(var, list):\n        if isinstance(var, tuple):\n            var = list(var)\n        else:\n            var = [var]\n        for _ in range(1, num_terms):\n            var.append(var[0])\n    if len(var) != num_terms:\n        print(\n            '\"%s\" has the wrong number of terms; it needs %s. Exiting ...' %\n            (var, num_terms))\n        sys.exit(1)\n    return var", "code_tokens": ["def", "check_list", "(", "var", ",", "num_terms", ")", ":", "if", "not", "isinstance", "(", "var", ",", "list", ")", ":", "if", "isinstance", "(", "var", ",", "tuple", ")", ":", "var", "=", "list", "(", "var", ")", "else", ":", "var", "=", "[", "var", "]", "for", "_", "in", "range", "(", "1", ",", "num_terms", ")", ":", "var", ".", "append", "(", "var", "[", "0", "]", ")", "if", "len", "(", "var", ")", "!=", "num_terms", ":", "print", "(", "'\"%s\" has the wrong number of terms; it needs %s. Exiting ...'", "%", "(", "var", ",", "num_terms", ")", ")", "sys", ".", "exit", "(", "1", ")", "return", "var"], "docstring": "Check if a variable is a list and is the correct length.\n\n    If variable is not a list it will make it a list of the correct length with\n    all terms identical.", "docstring_tokens": ["Check", "if", "a", "variable", "is", "a", "list", "and", "is", "the", "correct", "length", "."], "sha": "177cce21e92baca500f56a932d66bd9a33257af8", "url": "https://github.com/3DLIRIOUS/MeshLabXML/blob/177cce21e92baca500f56a932d66bd9a33257af8/meshlabxml/util.py#L69-L87", "partition": "test"}
{"repo": "elliterate/capybara.py", "path": "capybara/utils.py", "func_name": "inner_content", "original_string": "def inner_content(node):\n    \"\"\"\n    Returns the inner content of a given XML node, including tags.\n\n    Args:\n        node (lxml.etree.Element): The node whose inner content is desired.\n\n    Returns:\n        str: The inner content of the node.\n    \"\"\"\n\n    from lxml import etree\n\n    # Include text content at the start of the node.\n    parts = [node.text]\n\n    for child in node.getchildren():\n        # Include the child serialized to raw XML.\n        parts.append(etree.tostring(child, encoding=\"utf-8\"))\n\n        # Include any text following the child.\n        parts.append(child.tail)\n\n    # Discard any non-existent text parts and return.\n    return \"\".join(filter(None, parts))", "language": "python", "code": "def inner_content(node):\n    \"\"\"\n    Returns the inner content of a given XML node, including tags.\n\n    Args:\n        node (lxml.etree.Element): The node whose inner content is desired.\n\n    Returns:\n        str: The inner content of the node.\n    \"\"\"\n\n    from lxml import etree\n\n    # Include text content at the start of the node.\n    parts = [node.text]\n\n    for child in node.getchildren():\n        # Include the child serialized to raw XML.\n        parts.append(etree.tostring(child, encoding=\"utf-8\"))\n\n        # Include any text following the child.\n        parts.append(child.tail)\n\n    # Discard any non-existent text parts and return.\n    return \"\".join(filter(None, parts))", "code_tokens": ["def", "inner_content", "(", "node", ")", ":", "from", "lxml", "import", "etree", "# Include text content at the start of the node.", "parts", "=", "[", "node", ".", "text", "]", "for", "child", "in", "node", ".", "getchildren", "(", ")", ":", "# Include the child serialized to raw XML.", "parts", ".", "append", "(", "etree", ".", "tostring", "(", "child", ",", "encoding", "=", "\"utf-8\"", ")", ")", "# Include any text following the child.", "parts", ".", "append", "(", "child", ".", "tail", ")", "# Discard any non-existent text parts and return.", "return", "\"\"", ".", "join", "(", "filter", "(", "None", ",", "parts", ")", ")"], "docstring": "Returns the inner content of a given XML node, including tags.\n\n    Args:\n        node (lxml.etree.Element): The node whose inner content is desired.\n\n    Returns:\n        str: The inner content of the node.", "docstring_tokens": ["Returns", "the", "inner", "content", "of", "a", "given", "XML", "node", "including", "tags", "."], "sha": "0c6ae449cc37e4445ec3cd6af95674533beedc6c", "url": "https://github.com/elliterate/capybara.py/blob/0c6ae449cc37e4445ec3cd6af95674533beedc6c/capybara/utils.py#L75-L99", "partition": "test"}
{"repo": "dagster-io/dagster", "path": "python_modules/dagster/dagster/core/errors.py", "func_name": "user_code_error_boundary", "original_string": "def user_code_error_boundary(error_cls, msg, **kwargs):\n    '''\n    Wraps the execution of user-space code in an error boundary. This places a uniform\n    policy around an user code invoked by the framework. This ensures that all user\n    errors are wrapped in the DagsterUserCodeExecutionError, and that the original stack\n    trace of the user error is preserved, so that it can be reported without confusing\n    framework code in the stack trace, if a tool author wishes to do so. This has\n    been especially help in a notebooking context.\n    '''\n    check.str_param(msg, 'msg')\n    check.subclass_param(error_cls, 'error_cls', DagsterUserCodeExecutionError)\n\n    try:\n        yield\n    except Exception as e:  # pylint: disable=W0703\n        if isinstance(e, DagsterError):\n            # The system has thrown an error that is part of the user-framework contract\n            raise e\n        else:\n            # An exception has been thrown by user code and computation should cease\n            # with the error reported further up the stack\n            raise_from(\n                error_cls(msg, user_exception=e, original_exc_info=sys.exc_info(), **kwargs), e\n            )", "language": "python", "code": "def user_code_error_boundary(error_cls, msg, **kwargs):\n    '''\n    Wraps the execution of user-space code in an error boundary. This places a uniform\n    policy around an user code invoked by the framework. This ensures that all user\n    errors are wrapped in the DagsterUserCodeExecutionError, and that the original stack\n    trace of the user error is preserved, so that it can be reported without confusing\n    framework code in the stack trace, if a tool author wishes to do so. This has\n    been especially help in a notebooking context.\n    '''\n    check.str_param(msg, 'msg')\n    check.subclass_param(error_cls, 'error_cls', DagsterUserCodeExecutionError)\n\n    try:\n        yield\n    except Exception as e:  # pylint: disable=W0703\n        if isinstance(e, DagsterError):\n            # The system has thrown an error that is part of the user-framework contract\n            raise e\n        else:\n            # An exception has been thrown by user code and computation should cease\n            # with the error reported further up the stack\n            raise_from(\n                error_cls(msg, user_exception=e, original_exc_info=sys.exc_info(), **kwargs), e\n            )", "code_tokens": ["def", "user_code_error_boundary", "(", "error_cls", ",", "msg", ",", "*", "*", "kwargs", ")", ":", "check", ".", "str_param", "(", "msg", ",", "'msg'", ")", "check", ".", "subclass_param", "(", "error_cls", ",", "'error_cls'", ",", "DagsterUserCodeExecutionError", ")", "try", ":", "yield", "except", "Exception", "as", "e", ":", "# pylint: disable=W0703", "if", "isinstance", "(", "e", ",", "DagsterError", ")", ":", "# The system has thrown an error that is part of the user-framework contract", "raise", "e", "else", ":", "# An exception has been thrown by user code and computation should cease", "# with the error reported further up the stack", "raise_from", "(", "error_cls", "(", "msg", ",", "user_exception", "=", "e", ",", "original_exc_info", "=", "sys", ".", "exc_info", "(", ")", ",", "*", "*", "kwargs", ")", ",", "e", ")"], "docstring": "Wraps the execution of user-space code in an error boundary. This places a uniform\n    policy around an user code invoked by the framework. This ensures that all user\n    errors are wrapped in the DagsterUserCodeExecutionError, and that the original stack\n    trace of the user error is preserved, so that it can be reported without confusing\n    framework code in the stack trace, if a tool author wishes to do so. This has\n    been especially help in a notebooking context.", "docstring_tokens": ["Wraps", "the", "execution", "of", "user", "-", "space", "code", "in", "an", "error", "boundary", ".", "This", "places", "a", "uniform", "policy", "around", "an", "user", "code", "invoked", "by", "the", "framework", ".", "This", "ensures", "that", "all", "user", "errors", "are", "wrapped", "in", "the", "DagsterUserCodeExecutionError", "and", "that", "the", "original", "stack", "trace", "of", "the", "user", "error", "is", "preserved", "so", "that", "it", "can", "be", "reported", "without", "confusing", "framework", "code", "in", "the", "stack", "trace", "if", "a", "tool", "author", "wishes", "to", "do", "so", ".", "This", "has", "been", "especially", "help", "in", "a", "notebooking", "context", "."], "sha": "4119f8c773089de64831b1dfb9e168e353d401dc", "url": "https://github.com/dagster-io/dagster/blob/4119f8c773089de64831b1dfb9e168e353d401dc/python_modules/dagster/dagster/core/errors.py#L164-L187", "partition": "test"}
{"repo": "Jaymon/pyt", "path": "pyt/path.py", "func_name": "PathFinder.paths", "original_string": "def paths(self):\n        '''\n        given a basedir, yield all test modules paths recursively found in\n        basedir that are test modules\n\n        return -- generator\n        '''\n        module_name = getattr(self, 'module_name', '')\n        module_prefix = getattr(self, 'prefix', '')\n        filepath = getattr(self, 'filepath', '')\n\n        if filepath:\n            if os.path.isabs(filepath):\n                yield filepath\n\n            else:\n                yield os.path.join(self.basedir, filepath)\n\n        else:\n            if module_prefix:\n                basedirs = self._find_prefix_paths(self.basedir, module_prefix)\n            else:\n                basedirs = [self.basedir]\n\n            for basedir in basedirs:\n                try:\n                    if module_name:\n                        path = self._find_module_path(basedir, module_name)\n\n                    else:\n                        path = basedir\n\n                    if os.path.isfile(path):\n                        logger.debug('Module path: {}'.format(path))\n                        yield path\n\n                    else:\n                        seen_paths = set()\n                        for root, dirs, files in self.walk(path):\n                            for basename in files:\n                                if basename.startswith(\"__init__\"):\n                                    if self._is_module_path(root):\n                                        filepath = os.path.join(root, basename)\n                                        if filepath not in seen_paths:\n                                            logger.debug('Module package path: {}'.format(filepath))\n                                            seen_paths.add(filepath)\n                                            yield filepath\n\n                                else:\n                                    fileroot = os.path.splitext(basename)[0]\n                                    for pf in self.module_postfixes:\n                                        if fileroot.endswith(pf):\n                                            filepath = os.path.join(root, basename)\n                                            if filepath not in seen_paths:\n                                                logger.debug('Module postfix path: {}'.format(filepath))\n                                                seen_paths.add(filepath)\n                                                yield filepath\n\n                                    for pf in self.module_prefixes:\n                                        if fileroot.startswith(pf):\n                                            filepath = os.path.join(root, basename)\n                                            if filepath not in seen_paths:\n                                                logger.debug('Module prefix path: {}'.format(filepath))\n                                                seen_paths.add(filepath)\n                                                yield filepath\n\n                except IOError as e:\n                    # we failed to find a suitable path\n                    logger.warning(e, exc_info=True)\n                    pass", "language": "python", "code": "def paths(self):\n        '''\n        given a basedir, yield all test modules paths recursively found in\n        basedir that are test modules\n\n        return -- generator\n        '''\n        module_name = getattr(self, 'module_name', '')\n        module_prefix = getattr(self, 'prefix', '')\n        filepath = getattr(self, 'filepath', '')\n\n        if filepath:\n            if os.path.isabs(filepath):\n                yield filepath\n\n            else:\n                yield os.path.join(self.basedir, filepath)\n\n        else:\n            if module_prefix:\n                basedirs = self._find_prefix_paths(self.basedir, module_prefix)\n            else:\n                basedirs = [self.basedir]\n\n            for basedir in basedirs:\n                try:\n                    if module_name:\n                        path = self._find_module_path(basedir, module_name)\n\n                    else:\n                        path = basedir\n\n                    if os.path.isfile(path):\n                        logger.debug('Module path: {}'.format(path))\n                        yield path\n\n                    else:\n                        seen_paths = set()\n                        for root, dirs, files in self.walk(path):\n                            for basename in files:\n                                if basename.startswith(\"__init__\"):\n                                    if self._is_module_path(root):\n                                        filepath = os.path.join(root, basename)\n                                        if filepath not in seen_paths:\n                                            logger.debug('Module package path: {}'.format(filepath))\n                                            seen_paths.add(filepath)\n                                            yield filepath\n\n                                else:\n                                    fileroot = os.path.splitext(basename)[0]\n                                    for pf in self.module_postfixes:\n                                        if fileroot.endswith(pf):\n                                            filepath = os.path.join(root, basename)\n                                            if filepath not in seen_paths:\n                                                logger.debug('Module postfix path: {}'.format(filepath))\n                                                seen_paths.add(filepath)\n                                                yield filepath\n\n                                    for pf in self.module_prefixes:\n                                        if fileroot.startswith(pf):\n                                            filepath = os.path.join(root, basename)\n                                            if filepath not in seen_paths:\n                                                logger.debug('Module prefix path: {}'.format(filepath))\n                                                seen_paths.add(filepath)\n                                                yield filepath\n\n                except IOError as e:\n                    # we failed to find a suitable path\n                    logger.warning(e, exc_info=True)\n                    pass", "code_tokens": ["def", "paths", "(", "self", ")", ":", "module_name", "=", "getattr", "(", "self", ",", "'module_name'", ",", "''", ")", "module_prefix", "=", "getattr", "(", "self", ",", "'prefix'", ",", "''", ")", "filepath", "=", "getattr", "(", "self", ",", "'filepath'", ",", "''", ")", "if", "filepath", ":", "if", "os", ".", "path", ".", "isabs", "(", "filepath", ")", ":", "yield", "filepath", "else", ":", "yield", "os", ".", "path", ".", "join", "(", "self", ".", "basedir", ",", "filepath", ")", "else", ":", "if", "module_prefix", ":", "basedirs", "=", "self", ".", "_find_prefix_paths", "(", "self", ".", "basedir", ",", "module_prefix", ")", "else", ":", "basedirs", "=", "[", "self", ".", "basedir", "]", "for", "basedir", "in", "basedirs", ":", "try", ":", "if", "module_name", ":", "path", "=", "self", ".", "_find_module_path", "(", "basedir", ",", "module_name", ")", "else", ":", "path", "=", "basedir", "if", "os", ".", "path", ".", "isfile", "(", "path", ")", ":", "logger", ".", "debug", "(", "'Module path: {}'", ".", "format", "(", "path", ")", ")", "yield", "path", "else", ":", "seen_paths", "=", "set", "(", ")", "for", "root", ",", "dirs", ",", "files", "in", "self", ".", "walk", "(", "path", ")", ":", "for", "basename", "in", "files", ":", "if", "basename", ".", "startswith", "(", "\"__init__\"", ")", ":", "if", "self", ".", "_is_module_path", "(", "root", ")", ":", "filepath", "=", "os", ".", "path", ".", "join", "(", "root", ",", "basename", ")", "if", "filepath", "not", "in", "seen_paths", ":", "logger", ".", "debug", "(", "'Module package path: {}'", ".", "format", "(", "filepath", ")", ")", "seen_paths", ".", "add", "(", "filepath", ")", "yield", "filepath", "else", ":", "fileroot", "=", "os", ".", "path", ".", "splitext", "(", "basename", ")", "[", "0", "]", "for", "pf", "in", "self", ".", "module_postfixes", ":", "if", "fileroot", ".", "endswith", "(", "pf", ")", ":", "filepath", "=", "os", ".", "path", ".", "join", "(", "root", ",", "basename", ")", "if", "filepath", "not", "in", "seen_paths", ":", "logger", ".", "debug", "(", "'Module postfix path: {}'", ".", "format", "(", "filepath", ")", ")", "seen_paths", ".", "add", "(", "filepath", ")", "yield", "filepath", "for", "pf", "in", "self", ".", "module_prefixes", ":", "if", "fileroot", ".", "startswith", "(", "pf", ")", ":", "filepath", "=", "os", ".", "path", ".", "join", "(", "root", ",", "basename", ")", "if", "filepath", "not", "in", "seen_paths", ":", "logger", ".", "debug", "(", "'Module prefix path: {}'", ".", "format", "(", "filepath", ")", ")", "seen_paths", ".", "add", "(", "filepath", ")", "yield", "filepath", "except", "IOError", "as", "e", ":", "# we failed to find a suitable path", "logger", ".", "warning", "(", "e", ",", "exc_info", "=", "True", ")", "pass"], "docstring": "given a basedir, yield all test modules paths recursively found in\n        basedir that are test modules\n\n        return -- generator", "docstring_tokens": ["given", "a", "basedir", "yield", "all", "test", "modules", "paths", "recursively", "found", "in", "basedir", "that", "are", "test", "modules"], "sha": "801581fd0ae238158134bde1c937fa199fa626b2", "url": "https://github.com/Jaymon/pyt/blob/801581fd0ae238158134bde1c937fa199fa626b2/pyt/path.py#L490-L559", "partition": "test"}
{"repo": "mshroyer/pointfree", "path": "pointfree.py", "func_name": "pfprint", "original_string": "def pfprint(item, end='\\n', file=None):\n    \"\"\"Prints an item.\n\n    :param item: The item to print\n    :param end: String to append to the end of printed output\n    :param file: File to which output is printed\n    :rtype: None\n\n    Example::\n\n        >>> from operator import add\n\n        >>> fn = pfreduce(add, initial=0) >> pfprint\n        >>> fn([1, 2, 3, 4])\n        10\n\n    \"\"\"\n\n    # Can't just make sys.stdout the file argument's default value, because\n    # then we would be capturing the stdout file descriptor, and then\n    # doctest -- which works by redefining sys.stdout -- would fail:\n    if file is None:\n        file = sys.stdout\n\n    print(item, end=end, file=file)", "language": "python", "code": "def pfprint(item, end='\\n', file=None):\n    \"\"\"Prints an item.\n\n    :param item: The item to print\n    :param end: String to append to the end of printed output\n    :param file: File to which output is printed\n    :rtype: None\n\n    Example::\n\n        >>> from operator import add\n\n        >>> fn = pfreduce(add, initial=0) >> pfprint\n        >>> fn([1, 2, 3, 4])\n        10\n\n    \"\"\"\n\n    # Can't just make sys.stdout the file argument's default value, because\n    # then we would be capturing the stdout file descriptor, and then\n    # doctest -- which works by redefining sys.stdout -- would fail:\n    if file is None:\n        file = sys.stdout\n\n    print(item, end=end, file=file)", "code_tokens": ["def", "pfprint", "(", "item", ",", "end", "=", "'\\n'", ",", "file", "=", "None", ")", ":", "# Can't just make sys.stdout the file argument's default value, because", "# then we would be capturing the stdout file descriptor, and then", "# doctest -- which works by redefining sys.stdout -- would fail:", "if", "file", "is", "None", ":", "file", "=", "sys", ".", "stdout", "print", "(", "item", ",", "end", "=", "end", ",", "file", "=", "file", ")"], "docstring": "Prints an item.\n\n    :param item: The item to print\n    :param end: String to append to the end of printed output\n    :param file: File to which output is printed\n    :rtype: None\n\n    Example::\n\n        >>> from operator import add\n\n        >>> fn = pfreduce(add, initial=0) >> pfprint\n        >>> fn([1, 2, 3, 4])\n        10", "docstring_tokens": ["Prints", "an", "item", "."], "sha": "a25ecb3f0cd583e0730ecdde83018e5089711854", "url": "https://github.com/mshroyer/pointfree/blob/a25ecb3f0cd583e0730ecdde83018e5089711854/pointfree.py#L678-L702", "partition": "test"}
{"repo": "Julian/Ivoire", "path": "ivoire/transform.py", "func_name": "ExampleLoader.register", "original_string": "def register(cls):\n        \"\"\"\n        Register the path hook.\n\n        \"\"\"\n\n        cls._finder = FileFinder.path_hook((cls, [cls.suffix]))\n        sys.path_hooks.append(cls._finder)", "language": "python", "code": "def register(cls):\n        \"\"\"\n        Register the path hook.\n\n        \"\"\"\n\n        cls._finder = FileFinder.path_hook((cls, [cls.suffix]))\n        sys.path_hooks.append(cls._finder)", "code_tokens": ["def", "register", "(", "cls", ")", ":", "cls", ".", "_finder", "=", "FileFinder", ".", "path_hook", "(", "(", "cls", ",", "[", "cls", ".", "suffix", "]", ")", ")", "sys", ".", "path_hooks", ".", "append", "(", "cls", ".", "_finder", ")"], "docstring": "Register the path hook.", "docstring_tokens": ["Register", "the", "path", "hook", "."], "sha": "5b8218cffa409ed733cf850a6fde16fafb8fc2af", "url": "https://github.com/Julian/Ivoire/blob/5b8218cffa409ed733cf850a6fde16fafb8fc2af/ivoire/transform.py#L160-L167", "partition": "test"}
{"repo": "authomatic/authomatic", "path": "authomatic/providers/oauth2.py", "func_name": "OAuth2.decode_state", "original_string": "def decode_state(cls, state, param='user_state'):\n        \"\"\"\n        Decode state and return param.\n\n        :param str state:\n            state parameter passed through by provider\n\n        :param str param:\n            key to query from decoded state variable. Options include 'csrf'\n            and 'user_state'.\n\n        :returns:\n            string value from decoded state\n\n        \"\"\"\n        if state and cls.supports_user_state:\n            # urlsafe_b64 may include = which the browser quotes so must\n            # unquote Cast to str to void b64decode translation error. Base64\n            # should be str compatible.\n            return json.loads(base64.urlsafe_b64decode(\n                unquote(str(state))).decode('utf-8'))[param]\n        else:\n            return state if param == 'csrf' else ''", "language": "python", "code": "def decode_state(cls, state, param='user_state'):\n        \"\"\"\n        Decode state and return param.\n\n        :param str state:\n            state parameter passed through by provider\n\n        :param str param:\n            key to query from decoded state variable. Options include 'csrf'\n            and 'user_state'.\n\n        :returns:\n            string value from decoded state\n\n        \"\"\"\n        if state and cls.supports_user_state:\n            # urlsafe_b64 may include = which the browser quotes so must\n            # unquote Cast to str to void b64decode translation error. Base64\n            # should be str compatible.\n            return json.loads(base64.urlsafe_b64decode(\n                unquote(str(state))).decode('utf-8'))[param]\n        else:\n            return state if param == 'csrf' else ''", "code_tokens": ["def", "decode_state", "(", "cls", ",", "state", ",", "param", "=", "'user_state'", ")", ":", "if", "state", "and", "cls", ".", "supports_user_state", ":", "# urlsafe_b64 may include = which the browser quotes so must", "# unquote Cast to str to void b64decode translation error. Base64", "# should be str compatible.", "return", "json", ".", "loads", "(", "base64", ".", "urlsafe_b64decode", "(", "unquote", "(", "str", "(", "state", ")", ")", ")", ".", "decode", "(", "'utf-8'", ")", ")", "[", "param", "]", "else", ":", "return", "state", "if", "param", "==", "'csrf'", "else", "''"], "docstring": "Decode state and return param.\n\n        :param str state:\n            state parameter passed through by provider\n\n        :param str param:\n            key to query from decoded state variable. Options include 'csrf'\n            and 'user_state'.\n\n        :returns:\n            string value from decoded state", "docstring_tokens": ["Decode", "state", "and", "return", "param", "."], "sha": "90a9ce60cc405ae8a2bf5c3713acd5d78579a04e", "url": "https://github.com/authomatic/authomatic/blob/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e/authomatic/providers/oauth2.py#L262-L284", "partition": "test"}
{"repo": "elliterate/capybara.py", "path": "capybara/node/finders.py", "func_name": "FindersMixin.find_first", "original_string": "def find_first(self, *args, **kwargs):\n        \"\"\"\n        Find the first element on the page matching the given selector and options, or None if no\n        element matches.\n\n        By default, no waiting behavior occurs. However, if ``capybara.wait_on_first_by_default``\n        is set to true, it will trigger Capybara's waiting behavior for a minimum of 1 matching\n        element to be found.\n\n        Args:\n            *args: Variable length argument list for :class:`SelectorQuery`.\n            **kwargs: Arbitrary keyword arguments for :class:`SelectorQuery`.\n\n        Returns:\n            Element: The found element or None.\n        \"\"\"\n\n        if capybara.wait_on_first_by_default:\n            kwargs.setdefault(\"minimum\", 1)\n\n        try:\n            result = self.find_all(*args, **kwargs)\n            return result[0] if len(result) > 0 else None\n        except ExpectationNotMet:\n            return None", "language": "python", "code": "def find_first(self, *args, **kwargs):\n        \"\"\"\n        Find the first element on the page matching the given selector and options, or None if no\n        element matches.\n\n        By default, no waiting behavior occurs. However, if ``capybara.wait_on_first_by_default``\n        is set to true, it will trigger Capybara's waiting behavior for a minimum of 1 matching\n        element to be found.\n\n        Args:\n            *args: Variable length argument list for :class:`SelectorQuery`.\n            **kwargs: Arbitrary keyword arguments for :class:`SelectorQuery`.\n\n        Returns:\n            Element: The found element or None.\n        \"\"\"\n\n        if capybara.wait_on_first_by_default:\n            kwargs.setdefault(\"minimum\", 1)\n\n        try:\n            result = self.find_all(*args, **kwargs)\n            return result[0] if len(result) > 0 else None\n        except ExpectationNotMet:\n            return None", "code_tokens": ["def", "find_first", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "if", "capybara", ".", "wait_on_first_by_default", ":", "kwargs", ".", "setdefault", "(", "\"minimum\"", ",", "1", ")", "try", ":", "result", "=", "self", ".", "find_all", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "result", "[", "0", "]", "if", "len", "(", "result", ")", ">", "0", "else", "None", "except", "ExpectationNotMet", ":", "return", "None"], "docstring": "Find the first element on the page matching the given selector and options, or None if no\n        element matches.\n\n        By default, no waiting behavior occurs. However, if ``capybara.wait_on_first_by_default``\n        is set to true, it will trigger Capybara's waiting behavior for a minimum of 1 matching\n        element to be found.\n\n        Args:\n            *args: Variable length argument list for :class:`SelectorQuery`.\n            **kwargs: Arbitrary keyword arguments for :class:`SelectorQuery`.\n\n        Returns:\n            Element: The found element or None.", "docstring_tokens": ["Find", "the", "first", "element", "on", "the", "page", "matching", "the", "given", "selector", "and", "options", "or", "None", "if", "no", "element", "matches", "."], "sha": "0c6ae449cc37e4445ec3cd6af95674533beedc6c", "url": "https://github.com/elliterate/capybara.py/blob/0c6ae449cc37e4445ec3cd6af95674533beedc6c/capybara/node/finders.py#L210-L234", "partition": "test"}
{"repo": "ecometrica/grandfatherson", "path": "grandfatherson/filters.py", "func_name": "Days.mask", "original_string": "def mask(cls, dt, **options):\n        \"\"\"\n        Return a datetime with the same value as ``dt``, to a\n        resolution of days.\n        \"\"\"\n        return dt.replace(hour=0, minute=0, second=0, microsecond=0)", "language": "python", "code": "def mask(cls, dt, **options):\n        \"\"\"\n        Return a datetime with the same value as ``dt``, to a\n        resolution of days.\n        \"\"\"\n        return dt.replace(hour=0, minute=0, second=0, microsecond=0)", "code_tokens": ["def", "mask", "(", "cls", ",", "dt", ",", "*", "*", "options", ")", ":", "return", "dt", ".", "replace", "(", "hour", "=", "0", ",", "minute", "=", "0", ",", "second", "=", "0", ",", "microsecond", "=", "0", ")"], "docstring": "Return a datetime with the same value as ``dt``, to a\n        resolution of days.", "docstring_tokens": ["Return", "a", "datetime", "with", "the", "same", "value", "as", "dt", "to", "a", "resolution", "of", "days", "."], "sha": "b166e4e44887960c3066ebd28eecadfae19561e1", "url": "https://github.com/ecometrica/grandfatherson/blob/b166e4e44887960c3066ebd28eecadfae19561e1/grandfatherson/filters.py#L124-L129", "partition": "test"}
{"repo": "mohan3d/PyOpenload", "path": "openload/openload.py", "func_name": "OpenLoad.list_folder", "original_string": "def list_folder(self, folder_id=None):\n        \"\"\"Request a list of files and folders in specified folder.\n\n        Note:\n            if folder_id is not provided, ``Home`` folder will be listed\n\n        Args:\n            folder_id (:obj:`str`, optional): id of the folder to be listed.\n\n        Returns:\n            dict: dictionary containing only two keys (\"folders\", \"files\"), \\\n                  each key represents a list of dictionaries. ::\n\n                      {\n                        \"folders\": [\n                          {\n                            \"id\": \"5144\",\n                            \"name\": \".videothumb\"\n                          },\n                          {\n                            \"id\": \"5792\",\n                            \"name\": \".subtitles\"\n                          },\n                          ...\n                        ],\n                        \"files\": [\n                          {\n                            \"name\": \"big_buck_bunny.mp4.mp4\",\n                            \"sha1\": \"c6531f5ce9669d6547023d92aea4805b7c45d133\",\n                            \"folderid\": \"4258\",\n                            \"upload_at\": \"1419791256\",\n                            \"status\": \"active\",\n                            \"size\": \"5114011\",\n                            \"content_type\": \"video/mp4\",\n                            \"download_count\": \"48\",\n                            \"cstatus\": \"ok\",\n                            \"link\": \"https://openload.co/f/UPPjeAk--30/big_buck_bunny.mp4.mp4\",\n                            \"linkextid\": \"UPPjeAk--30\"\n                          },\n                          ...\n                        ]\n                      }\n\n        \"\"\"\n        params = {'folder': folder_id} if folder_id else {}\n\n        return self._get('file/listfolder', params=params)", "language": "python", "code": "def list_folder(self, folder_id=None):\n        \"\"\"Request a list of files and folders in specified folder.\n\n        Note:\n            if folder_id is not provided, ``Home`` folder will be listed\n\n        Args:\n            folder_id (:obj:`str`, optional): id of the folder to be listed.\n\n        Returns:\n            dict: dictionary containing only two keys (\"folders\", \"files\"), \\\n                  each key represents a list of dictionaries. ::\n\n                      {\n                        \"folders\": [\n                          {\n                            \"id\": \"5144\",\n                            \"name\": \".videothumb\"\n                          },\n                          {\n                            \"id\": \"5792\",\n                            \"name\": \".subtitles\"\n                          },\n                          ...\n                        ],\n                        \"files\": [\n                          {\n                            \"name\": \"big_buck_bunny.mp4.mp4\",\n                            \"sha1\": \"c6531f5ce9669d6547023d92aea4805b7c45d133\",\n                            \"folderid\": \"4258\",\n                            \"upload_at\": \"1419791256\",\n                            \"status\": \"active\",\n                            \"size\": \"5114011\",\n                            \"content_type\": \"video/mp4\",\n                            \"download_count\": \"48\",\n                            \"cstatus\": \"ok\",\n                            \"link\": \"https://openload.co/f/UPPjeAk--30/big_buck_bunny.mp4.mp4\",\n                            \"linkextid\": \"UPPjeAk--30\"\n                          },\n                          ...\n                        ]\n                      }\n\n        \"\"\"\n        params = {'folder': folder_id} if folder_id else {}\n\n        return self._get('file/listfolder', params=params)", "code_tokens": ["def", "list_folder", "(", "self", ",", "folder_id", "=", "None", ")", ":", "params", "=", "{", "'folder'", ":", "folder_id", "}", "if", "folder_id", "else", "{", "}", "return", "self", ".", "_get", "(", "'file/listfolder'", ",", "params", "=", "params", ")"], "docstring": "Request a list of files and folders in specified folder.\n\n        Note:\n            if folder_id is not provided, ``Home`` folder will be listed\n\n        Args:\n            folder_id (:obj:`str`, optional): id of the folder to be listed.\n\n        Returns:\n            dict: dictionary containing only two keys (\"folders\", \"files\"), \\\n                  each key represents a list of dictionaries. ::\n\n                      {\n                        \"folders\": [\n                          {\n                            \"id\": \"5144\",\n                            \"name\": \".videothumb\"\n                          },\n                          {\n                            \"id\": \"5792\",\n                            \"name\": \".subtitles\"\n                          },\n                          ...\n                        ],\n                        \"files\": [\n                          {\n                            \"name\": \"big_buck_bunny.mp4.mp4\",\n                            \"sha1\": \"c6531f5ce9669d6547023d92aea4805b7c45d133\",\n                            \"folderid\": \"4258\",\n                            \"upload_at\": \"1419791256\",\n                            \"status\": \"active\",\n                            \"size\": \"5114011\",\n                            \"content_type\": \"video/mp4\",\n                            \"download_count\": \"48\",\n                            \"cstatus\": \"ok\",\n                            \"link\": \"https://openload.co/f/UPPjeAk--30/big_buck_bunny.mp4.mp4\",\n                            \"linkextid\": \"UPPjeAk--30\"\n                          },\n                          ...\n                        ]\n                      }", "docstring_tokens": ["Request", "a", "list", "of", "files", "and", "folders", "in", "specified", "folder", "."], "sha": "7f9353915ca5546926ef07be9395c6de60e761b1", "url": "https://github.com/mohan3d/PyOpenload/blob/7f9353915ca5546926ef07be9395c6de60e761b1/openload/openload.py#L333-L379", "partition": "test"}
{"repo": "rbarrois/aionotify", "path": "aionotify/base.py", "func_name": "Watcher.get_event", "original_string": "def get_event(self):\n        \"\"\"Fetch an event.\n\n        This coroutine will swallow events for removed watches.\n        \"\"\"\n        while True:\n            prefix = yield from self._stream.readexactly(PREFIX.size)\n            if prefix == b'':\n                # We got closed, return None.\n                return\n            wd, flags, cookie, length = PREFIX.unpack(prefix)\n            path = yield from self._stream.readexactly(length)\n\n            # All async performed, time to look at the event's content.\n            if wd not in self.aliases:\n                # Event for a removed watch, skip it.\n                continue\n\n            decoded_path = struct.unpack('%ds' % length, path)[0].rstrip(b'\\x00').decode('utf-8')\n            return Event(\n                flags=flags,\n                cookie=cookie,\n                name=decoded_path,\n                alias=self.aliases[wd],\n            )", "language": "python", "code": "def get_event(self):\n        \"\"\"Fetch an event.\n\n        This coroutine will swallow events for removed watches.\n        \"\"\"\n        while True:\n            prefix = yield from self._stream.readexactly(PREFIX.size)\n            if prefix == b'':\n                # We got closed, return None.\n                return\n            wd, flags, cookie, length = PREFIX.unpack(prefix)\n            path = yield from self._stream.readexactly(length)\n\n            # All async performed, time to look at the event's content.\n            if wd not in self.aliases:\n                # Event for a removed watch, skip it.\n                continue\n\n            decoded_path = struct.unpack('%ds' % length, path)[0].rstrip(b'\\x00').decode('utf-8')\n            return Event(\n                flags=flags,\n                cookie=cookie,\n                name=decoded_path,\n                alias=self.aliases[wd],\n            )", "code_tokens": ["def", "get_event", "(", "self", ")", ":", "while", "True", ":", "prefix", "=", "yield", "from", "self", ".", "_stream", ".", "readexactly", "(", "PREFIX", ".", "size", ")", "if", "prefix", "==", "b''", ":", "# We got closed, return None.", "return", "wd", ",", "flags", ",", "cookie", ",", "length", "=", "PREFIX", ".", "unpack", "(", "prefix", ")", "path", "=", "yield", "from", "self", ".", "_stream", ".", "readexactly", "(", "length", ")", "# All async performed, time to look at the event's content.", "if", "wd", "not", "in", "self", ".", "aliases", ":", "# Event for a removed watch, skip it.", "continue", "decoded_path", "=", "struct", ".", "unpack", "(", "'%ds'", "%", "length", ",", "path", ")", "[", "0", "]", ".", "rstrip", "(", "b'\\x00'", ")", ".", "decode", "(", "'utf-8'", ")", "return", "Event", "(", "flags", "=", "flags", ",", "cookie", "=", "cookie", ",", "name", "=", "decoded_path", ",", "alias", "=", "self", ".", "aliases", "[", "wd", "]", ",", ")"], "docstring": "Fetch an event.\n\n        This coroutine will swallow events for removed watches.", "docstring_tokens": ["Fetch", "an", "event", "."], "sha": "6cfa35b26a2660f77f29a92d3efb7d1dde685b43", "url": "https://github.com/rbarrois/aionotify/blob/6cfa35b26a2660f77f29a92d3efb7d1dde685b43/aionotify/base.py#L109-L133", "partition": "test"}
{"repo": "nvdv/vprof", "path": "vprof/profiler.py", "func_name": "Profiler._profile_package", "original_string": "def _profile_package(self):\n        \"\"\"Runs cProfile on a package.\"\"\"\n        prof = cProfile.Profile()\n        prof.enable()\n        try:\n            runpy.run_path(self._run_object, run_name='__main__')\n        except SystemExit:\n            pass\n        prof.disable()\n        prof_stats = pstats.Stats(prof)\n        prof_stats.calc_callees()\n        return {\n            'objectName': self._object_name,\n            'callStats': self._transform_stats(prof_stats),\n            'totalTime': prof_stats.total_tt,\n            'primitiveCalls': prof_stats.prim_calls,\n            'totalCalls': prof_stats.total_calls,\n            'timestamp': int(time.time())\n        }", "language": "python", "code": "def _profile_package(self):\n        \"\"\"Runs cProfile on a package.\"\"\"\n        prof = cProfile.Profile()\n        prof.enable()\n        try:\n            runpy.run_path(self._run_object, run_name='__main__')\n        except SystemExit:\n            pass\n        prof.disable()\n        prof_stats = pstats.Stats(prof)\n        prof_stats.calc_callees()\n        return {\n            'objectName': self._object_name,\n            'callStats': self._transform_stats(prof_stats),\n            'totalTime': prof_stats.total_tt,\n            'primitiveCalls': prof_stats.prim_calls,\n            'totalCalls': prof_stats.total_calls,\n            'timestamp': int(time.time())\n        }", "code_tokens": ["def", "_profile_package", "(", "self", ")", ":", "prof", "=", "cProfile", ".", "Profile", "(", ")", "prof", ".", "enable", "(", ")", "try", ":", "runpy", ".", "run_path", "(", "self", ".", "_run_object", ",", "run_name", "=", "'__main__'", ")", "except", "SystemExit", ":", "pass", "prof", ".", "disable", "(", ")", "prof_stats", "=", "pstats", ".", "Stats", "(", "prof", ")", "prof_stats", ".", "calc_callees", "(", ")", "return", "{", "'objectName'", ":", "self", ".", "_object_name", ",", "'callStats'", ":", "self", ".", "_transform_stats", "(", "prof_stats", ")", ",", "'totalTime'", ":", "prof_stats", ".", "total_tt", ",", "'primitiveCalls'", ":", "prof_stats", ".", "prim_calls", ",", "'totalCalls'", ":", "prof_stats", ".", "total_calls", ",", "'timestamp'", ":", "int", "(", "time", ".", "time", "(", ")", ")", "}"], "docstring": "Runs cProfile on a package.", "docstring_tokens": ["Runs", "cProfile", "on", "a", "package", "."], "sha": "4c3ff78f8920ab10cb9c00b14143452aa09ff6bb", "url": "https://github.com/nvdv/vprof/blob/4c3ff78f8920ab10cb9c00b14143452aa09ff6bb/vprof/profiler.py#L36-L54", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/triangular.py", "func_name": "_broadcast_to", "original_string": "def _broadcast_to(tensor_to_broadcast, target_tensors):\n  \"\"\"Helper to broadcast a tensor using a list of target tensors.\"\"\"\n  output = tensor_to_broadcast\n  for tensor in target_tensors:\n    output += tf.zeros_like(tensor)\n  return output", "language": "python", "code": "def _broadcast_to(tensor_to_broadcast, target_tensors):\n  \"\"\"Helper to broadcast a tensor using a list of target tensors.\"\"\"\n  output = tensor_to_broadcast\n  for tensor in target_tensors:\n    output += tf.zeros_like(tensor)\n  return output", "code_tokens": ["def", "_broadcast_to", "(", "tensor_to_broadcast", ",", "target_tensors", ")", ":", "output", "=", "tensor_to_broadcast", "for", "tensor", "in", "target_tensors", ":", "output", "+=", "tf", ".", "zeros_like", "(", "tensor", ")", "return", "output"], "docstring": "Helper to broadcast a tensor using a list of target tensors.", "docstring_tokens": ["Helper", "to", "broadcast", "a", "tensor", "using", "a", "list", "of", "target", "tensors", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/triangular.py#L33-L38", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/nose/plugins/xunit.py", "func_name": "Xunit.configure", "original_string": "def configure(self, options, config):\n        \"\"\"Configures the xunit plugin.\"\"\"\n        Plugin.configure(self, options, config)\n        self.config = config\n        if self.enabled:\n            self.stats = {'errors': 0,\n                          'failures': 0,\n                          'passes': 0,\n                          'skipped': 0\n                          }\n            self.errorlist = []\n            self.error_report_file = codecs.open(options.xunit_file, 'w',\n                                                 self.encoding, 'replace')", "language": "python", "code": "def configure(self, options, config):\n        \"\"\"Configures the xunit plugin.\"\"\"\n        Plugin.configure(self, options, config)\n        self.config = config\n        if self.enabled:\n            self.stats = {'errors': 0,\n                          'failures': 0,\n                          'passes': 0,\n                          'skipped': 0\n                          }\n            self.errorlist = []\n            self.error_report_file = codecs.open(options.xunit_file, 'w',\n                                                 self.encoding, 'replace')", "code_tokens": ["def", "configure", "(", "self", ",", "options", ",", "config", ")", ":", "Plugin", ".", "configure", "(", "self", ",", "options", ",", "config", ")", "self", ".", "config", "=", "config", "if", "self", ".", "enabled", ":", "self", ".", "stats", "=", "{", "'errors'", ":", "0", ",", "'failures'", ":", "0", ",", "'passes'", ":", "0", ",", "'skipped'", ":", "0", "}", "self", ".", "errorlist", "=", "[", "]", "self", ".", "error_report_file", "=", "codecs", ".", "open", "(", "options", ".", "xunit_file", ",", "'w'", ",", "self", ".", "encoding", ",", "'replace'", ")"], "docstring": "Configures the xunit plugin.", "docstring_tokens": ["Configures", "the", "xunit", "plugin", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/nose/plugins/xunit.py#L150-L162", "partition": "test"}
{"repo": "h2non/pook", "path": "pook/mock.py", "func_name": "Mock.content", "original_string": "def content(self, value):\n        \"\"\"\n        Defines the ``Content-Type`` outgoing header value to match.\n\n        You can pass one of the following type aliases instead of the full\n        MIME type representation:\n\n        - ``json`` = ``application/json``\n        - ``xml`` = ``application/xml``\n        - ``html`` = ``text/html``\n        - ``text`` = ``text/plain``\n        - ``urlencoded`` = ``application/x-www-form-urlencoded``\n        - ``form`` = ``application/x-www-form-urlencoded``\n        - ``form-data`` = ``application/x-www-form-urlencoded``\n\n        Arguments:\n            value (str): type alias or header value to match.\n\n        Returns:\n            self: current Mock instance.\n        \"\"\"\n        header = {'Content-Type': TYPES.get(value, value)}\n        self._request.headers = header\n        self.add_matcher(matcher('HeadersMatcher', header))", "language": "python", "code": "def content(self, value):\n        \"\"\"\n        Defines the ``Content-Type`` outgoing header value to match.\n\n        You can pass one of the following type aliases instead of the full\n        MIME type representation:\n\n        - ``json`` = ``application/json``\n        - ``xml`` = ``application/xml``\n        - ``html`` = ``text/html``\n        - ``text`` = ``text/plain``\n        - ``urlencoded`` = ``application/x-www-form-urlencoded``\n        - ``form`` = ``application/x-www-form-urlencoded``\n        - ``form-data`` = ``application/x-www-form-urlencoded``\n\n        Arguments:\n            value (str): type alias or header value to match.\n\n        Returns:\n            self: current Mock instance.\n        \"\"\"\n        header = {'Content-Type': TYPES.get(value, value)}\n        self._request.headers = header\n        self.add_matcher(matcher('HeadersMatcher', header))", "code_tokens": ["def", "content", "(", "self", ",", "value", ")", ":", "header", "=", "{", "'Content-Type'", ":", "TYPES", ".", "get", "(", "value", ",", "value", ")", "}", "self", ".", "_request", ".", "headers", "=", "header", "self", ".", "add_matcher", "(", "matcher", "(", "'HeadersMatcher'", ",", "header", ")", ")"], "docstring": "Defines the ``Content-Type`` outgoing header value to match.\n\n        You can pass one of the following type aliases instead of the full\n        MIME type representation:\n\n        - ``json`` = ``application/json``\n        - ``xml`` = ``application/xml``\n        - ``html`` = ``text/html``\n        - ``text`` = ``text/plain``\n        - ``urlencoded`` = ``application/x-www-form-urlencoded``\n        - ``form`` = ``application/x-www-form-urlencoded``\n        - ``form-data`` = ``application/x-www-form-urlencoded``\n\n        Arguments:\n            value (str): type alias or header value to match.\n\n        Returns:\n            self: current Mock instance.", "docstring_tokens": ["Defines", "the", "Content", "-", "Type", "outgoing", "header", "value", "to", "match", "."], "sha": "e64094e41e4d89d98d2d29af7608ef27dc50cf19", "url": "https://github.com/h2non/pook/blob/e64094e41e4d89d98d2d29af7608ef27dc50cf19/pook/mock.py#L294-L317", "partition": "test"}
{"repo": "LionelAuroux/pyrser", "path": "pyrser/parsing/base.py", "func_name": "BasicParser.read_until_eof", "original_string": "def read_until_eof(self) -> bool:\n        \"\"\"Consume all the stream. Same as EOF in BNF.\"\"\"\n        if self.read_eof():\n            return True\n        # TODO: read ALL\n        self._stream.save_context()\n        while not self.read_eof():\n            self._stream.incpos()\n        return self._stream.validate_context()", "language": "python", "code": "def read_until_eof(self) -> bool:\n        \"\"\"Consume all the stream. Same as EOF in BNF.\"\"\"\n        if self.read_eof():\n            return True\n        # TODO: read ALL\n        self._stream.save_context()\n        while not self.read_eof():\n            self._stream.incpos()\n        return self._stream.validate_context()", "code_tokens": ["def", "read_until_eof", "(", "self", ")", "->", "bool", ":", "if", "self", ".", "read_eof", "(", ")", ":", "return", "True", "# TODO: read ALL", "self", ".", "_stream", ".", "save_context", "(", ")", "while", "not", "self", ".", "read_eof", "(", ")", ":", "self", ".", "_stream", ".", "incpos", "(", ")", "return", "self", ".", "_stream", ".", "validate_context", "(", ")"], "docstring": "Consume all the stream. Same as EOF in BNF.", "docstring_tokens": ["Consume", "all", "the", "stream", ".", "Same", "as", "EOF", "in", "BNF", "."], "sha": "f153a97ef2b6bf915a1ed468c0252a9a59b754d5", "url": "https://github.com/LionelAuroux/pyrser/blob/f153a97ef2b6bf915a1ed468c0252a9a59b754d5/pyrser/parsing/base.py#L304-L312", "partition": "test"}
{"repo": "erichiggins/gaek", "path": "gaek/ndb_json.py", "func_name": "NdbDecoder.decode", "original_string": "def decode(self, val):\n    \"\"\"Override of the default decode method that also uses decode_date.\"\"\"\n    # First try the date decoder.\n    new_val = self.decode_date(val)\n    if val != new_val:\n      return new_val\n    # Fall back to the default decoder.\n    return json.JSONDecoder.decode(self, val)", "language": "python", "code": "def decode(self, val):\n    \"\"\"Override of the default decode method that also uses decode_date.\"\"\"\n    # First try the date decoder.\n    new_val = self.decode_date(val)\n    if val != new_val:\n      return new_val\n    # Fall back to the default decoder.\n    return json.JSONDecoder.decode(self, val)", "code_tokens": ["def", "decode", "(", "self", ",", "val", ")", ":", "# First try the date decoder.", "new_val", "=", "self", ".", "decode_date", "(", "val", ")", "if", "val", "!=", "new_val", ":", "return", "new_val", "# Fall back to the default decoder.", "return", "json", ".", "JSONDecoder", ".", "decode", "(", "self", ",", "val", ")"], "docstring": "Override of the default decode method that also uses decode_date.", "docstring_tokens": ["Override", "of", "the", "default", "decode", "method", "that", "also", "uses", "decode_date", "."], "sha": "eb6bbc2d2688302834f97fd97891592e8b9659f2", "url": "https://github.com/erichiggins/gaek/blob/eb6bbc2d2688302834f97fd97891592e8b9659f2/gaek/ndb_json.py#L162-L169", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/text_messages_hmc.py", "func_name": "text_messages_joint_log_prob", "original_string": "def text_messages_joint_log_prob(count_data, lambda_1, lambda_2, tau):\n  \"\"\"Joint log probability function.\"\"\"\n  alpha = (1. / tf.reduce_mean(input_tensor=count_data))\n  rv_lambda = tfd.Exponential(rate=alpha)\n\n  rv_tau = tfd.Uniform()\n\n  lambda_ = tf.gather(\n      [lambda_1, lambda_2],\n      indices=tf.cast(\n          tau * tf.cast(tf.size(input=count_data), dtype=tf.float32) <= tf.cast(\n              tf.range(tf.size(input=count_data)), dtype=tf.float32),\n          dtype=tf.int32))\n  rv_observation = tfd.Poisson(rate=lambda_)\n\n  return (rv_lambda.log_prob(lambda_1) + rv_lambda.log_prob(lambda_2) +\n          rv_tau.log_prob(tau) +\n          tf.reduce_sum(input_tensor=rv_observation.log_prob(count_data)))", "language": "python", "code": "def text_messages_joint_log_prob(count_data, lambda_1, lambda_2, tau):\n  \"\"\"Joint log probability function.\"\"\"\n  alpha = (1. / tf.reduce_mean(input_tensor=count_data))\n  rv_lambda = tfd.Exponential(rate=alpha)\n\n  rv_tau = tfd.Uniform()\n\n  lambda_ = tf.gather(\n      [lambda_1, lambda_2],\n      indices=tf.cast(\n          tau * tf.cast(tf.size(input=count_data), dtype=tf.float32) <= tf.cast(\n              tf.range(tf.size(input=count_data)), dtype=tf.float32),\n          dtype=tf.int32))\n  rv_observation = tfd.Poisson(rate=lambda_)\n\n  return (rv_lambda.log_prob(lambda_1) + rv_lambda.log_prob(lambda_2) +\n          rv_tau.log_prob(tau) +\n          tf.reduce_sum(input_tensor=rv_observation.log_prob(count_data)))", "code_tokens": ["def", "text_messages_joint_log_prob", "(", "count_data", ",", "lambda_1", ",", "lambda_2", ",", "tau", ")", ":", "alpha", "=", "(", "1.", "/", "tf", ".", "reduce_mean", "(", "input_tensor", "=", "count_data", ")", ")", "rv_lambda", "=", "tfd", ".", "Exponential", "(", "rate", "=", "alpha", ")", "rv_tau", "=", "tfd", ".", "Uniform", "(", ")", "lambda_", "=", "tf", ".", "gather", "(", "[", "lambda_1", ",", "lambda_2", "]", ",", "indices", "=", "tf", ".", "cast", "(", "tau", "*", "tf", ".", "cast", "(", "tf", ".", "size", "(", "input", "=", "count_data", ")", ",", "dtype", "=", "tf", ".", "float32", ")", "<=", "tf", ".", "cast", "(", "tf", ".", "range", "(", "tf", ".", "size", "(", "input", "=", "count_data", ")", ")", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "dtype", "=", "tf", ".", "int32", ")", ")", "rv_observation", "=", "tfd", ".", "Poisson", "(", "rate", "=", "lambda_", ")", "return", "(", "rv_lambda", ".", "log_prob", "(", "lambda_1", ")", "+", "rv_lambda", ".", "log_prob", "(", "lambda_2", ")", "+", "rv_tau", ".", "log_prob", "(", "tau", ")", "+", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "rv_observation", ".", "log_prob", "(", "count_data", ")", ")", ")"], "docstring": "Joint log probability function.", "docstring_tokens": ["Joint", "log", "probability", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/text_messages_hmc.py#L44-L61", "partition": "test"}
{"repo": "intel-analytics/BigDL", "path": "pyspark/bigdl/optim/optimizer.py", "func_name": "SequentialSchedule.add", "original_string": "def add(self, scheduler, max_iteration, bigdl_type=\"float\"):\n        \"\"\"\n        Add a learning rate scheduler to the contained `schedules`\n\n        :param scheduler: learning rate scheduler to be add\n        :param max_iteration: iteration numbers this scheduler will run\n        \"\"\"\n        return callBigDlFunc(bigdl_type, \"addScheduler\", self.value, scheduler, max_iteration)", "language": "python", "code": "def add(self, scheduler, max_iteration, bigdl_type=\"float\"):\n        \"\"\"\n        Add a learning rate scheduler to the contained `schedules`\n\n        :param scheduler: learning rate scheduler to be add\n        :param max_iteration: iteration numbers this scheduler will run\n        \"\"\"\n        return callBigDlFunc(bigdl_type, \"addScheduler\", self.value, scheduler, max_iteration)", "code_tokens": ["def", "add", "(", "self", ",", "scheduler", ",", "max_iteration", ",", "bigdl_type", "=", "\"float\"", ")", ":", "return", "callBigDlFunc", "(", "bigdl_type", ",", "\"addScheduler\"", ",", "self", ".", "value", ",", "scheduler", ",", "max_iteration", ")"], "docstring": "Add a learning rate scheduler to the contained `schedules`\n\n        :param scheduler: learning rate scheduler to be add\n        :param max_iteration: iteration numbers this scheduler will run", "docstring_tokens": ["Add", "a", "learning", "rate", "scheduler", "to", "the", "contained", "schedules"], "sha": "e9c19788285986ab789a2e2998f9a85d7524779f", "url": "https://github.com/intel-analytics/BigDL/blob/e9c19788285986ab789a2e2998f9a85d7524779f/pyspark/bigdl/optim/optimizer.py#L425-L432", "partition": "test"}
{"repo": "chrisjrn/registrasion", "path": "registrasion/controllers/cart.py", "func_name": "CartController._recalculate_discounts", "original_string": "def _recalculate_discounts(self):\n        ''' Calculates all of the discounts available for this product.'''\n\n        # Delete the existing entries.\n        commerce.DiscountItem.objects.filter(cart=self.cart).delete()\n\n        # Order the products such that the most expensive ones are\n        # processed first.\n        product_items = self.cart.productitem_set.all().select_related(\n            \"product\", \"product__category\"\n        ).order_by(\"-product__price\")\n\n        products = [i.product for i in product_items]\n        discounts = DiscountController.available_discounts(\n            self.cart.user,\n            [],\n            products,\n        )\n\n        # The highest-value discounts will apply to the highest-value\n        # products first, because of the order_by clause\n        for item in product_items:\n            self._add_discount(item.product, item.quantity, discounts)", "language": "python", "code": "def _recalculate_discounts(self):\n        ''' Calculates all of the discounts available for this product.'''\n\n        # Delete the existing entries.\n        commerce.DiscountItem.objects.filter(cart=self.cart).delete()\n\n        # Order the products such that the most expensive ones are\n        # processed first.\n        product_items = self.cart.productitem_set.all().select_related(\n            \"product\", \"product__category\"\n        ).order_by(\"-product__price\")\n\n        products = [i.product for i in product_items]\n        discounts = DiscountController.available_discounts(\n            self.cart.user,\n            [],\n            products,\n        )\n\n        # The highest-value discounts will apply to the highest-value\n        # products first, because of the order_by clause\n        for item in product_items:\n            self._add_discount(item.product, item.quantity, discounts)", "code_tokens": ["def", "_recalculate_discounts", "(", "self", ")", ":", "# Delete the existing entries.", "commerce", ".", "DiscountItem", ".", "objects", ".", "filter", "(", "cart", "=", "self", ".", "cart", ")", ".", "delete", "(", ")", "# Order the products such that the most expensive ones are", "# processed first.", "product_items", "=", "self", ".", "cart", ".", "productitem_set", ".", "all", "(", ")", ".", "select_related", "(", "\"product\"", ",", "\"product__category\"", ")", ".", "order_by", "(", "\"-product__price\"", ")", "products", "=", "[", "i", ".", "product", "for", "i", "in", "product_items", "]", "discounts", "=", "DiscountController", ".", "available_discounts", "(", "self", ".", "cart", ".", "user", ",", "[", "]", ",", "products", ",", ")", "# The highest-value discounts will apply to the highest-value", "# products first, because of the order_by clause", "for", "item", "in", "product_items", ":", "self", ".", "_add_discount", "(", "item", ".", "product", ",", "item", ".", "quantity", ",", "discounts", ")"], "docstring": "Calculates all of the discounts available for this product.", "docstring_tokens": ["Calculates", "all", "of", "the", "discounts", "available", "for", "this", "product", "."], "sha": "461d5846c6f9f3b7099322a94f5d9911564448e4", "url": "https://github.com/chrisjrn/registrasion/blob/461d5846c6f9f3b7099322a94f5d9911564448e4/registrasion/controllers/cart.py#L438-L460", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/datastore_hook.py", "func_name": "DatastoreHook.import_from_storage_bucket", "original_string": "def import_from_storage_bucket(self, bucket, file, namespace=None, entity_filter=None, labels=None):\n        \"\"\"\n        Import a backup from Cloud Storage to Cloud Datastore.\n\n        .. note::\n            Keep in mind that this requests the Admin API not the Data API.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import\n\n        :param bucket: The name of the Cloud Storage bucket.\n        :type bucket: str\n        :param file: the metadata file written by the projects.export operation.\n        :type file: str\n        :param namespace: The Cloud Storage namespace path.\n        :type namespace: str\n        :param entity_filter: specify which kinds/namespaces are to be imported.\n        :type entity_filter: dict\n        :param labels: Client-assigned labels.\n        :type labels: dict of str\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"\n        admin_conn = self.get_conn()\n\n        input_url = 'gs://' + '/'.join(filter(None, [bucket, namespace, file]))\n        if not entity_filter:\n            entity_filter = {}\n        if not labels:\n            labels = {}\n        body = {\n            'inputUrl': input_url,\n            'entityFilter': entity_filter,\n            'labels': labels,\n        }\n        resp = (admin_conn\n                .projects()\n                .import_(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp", "language": "python", "code": "def import_from_storage_bucket(self, bucket, file, namespace=None, entity_filter=None, labels=None):\n        \"\"\"\n        Import a backup from Cloud Storage to Cloud Datastore.\n\n        .. note::\n            Keep in mind that this requests the Admin API not the Data API.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import\n\n        :param bucket: The name of the Cloud Storage bucket.\n        :type bucket: str\n        :param file: the metadata file written by the projects.export operation.\n        :type file: str\n        :param namespace: The Cloud Storage namespace path.\n        :type namespace: str\n        :param entity_filter: specify which kinds/namespaces are to be imported.\n        :type entity_filter: dict\n        :param labels: Client-assigned labels.\n        :type labels: dict of str\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"\n        admin_conn = self.get_conn()\n\n        input_url = 'gs://' + '/'.join(filter(None, [bucket, namespace, file]))\n        if not entity_filter:\n            entity_filter = {}\n        if not labels:\n            labels = {}\n        body = {\n            'inputUrl': input_url,\n            'entityFilter': entity_filter,\n            'labels': labels,\n        }\n        resp = (admin_conn\n                .projects()\n                .import_(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp", "code_tokens": ["def", "import_from_storage_bucket", "(", "self", ",", "bucket", ",", "file", ",", "namespace", "=", "None", ",", "entity_filter", "=", "None", ",", "labels", "=", "None", ")", ":", "admin_conn", "=", "self", ".", "get_conn", "(", ")", "input_url", "=", "'gs://'", "+", "'/'", ".", "join", "(", "filter", "(", "None", ",", "[", "bucket", ",", "namespace", ",", "file", "]", ")", ")", "if", "not", "entity_filter", ":", "entity_filter", "=", "{", "}", "if", "not", "labels", ":", "labels", "=", "{", "}", "body", "=", "{", "'inputUrl'", ":", "input_url", ",", "'entityFilter'", ":", "entity_filter", ",", "'labels'", ":", "labels", ",", "}", "resp", "=", "(", "admin_conn", ".", "projects", "(", ")", ".", "import_", "(", "projectId", "=", "self", ".", "project_id", ",", "body", "=", "body", ")", ".", "execute", "(", "num_retries", "=", "self", ".", "num_retries", ")", ")", "return", "resp"], "docstring": "Import a backup from Cloud Storage to Cloud Datastore.\n\n        .. note::\n            Keep in mind that this requests the Admin API not the Data API.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import\n\n        :param bucket: The name of the Cloud Storage bucket.\n        :type bucket: str\n        :param file: the metadata file written by the projects.export operation.\n        :type file: str\n        :param namespace: The Cloud Storage namespace path.\n        :type namespace: str\n        :param entity_filter: specify which kinds/namespaces are to be imported.\n        :type entity_filter: dict\n        :param labels: Client-assigned labels.\n        :type labels: dict of str\n        :return: a resource operation instance.\n        :rtype: dict", "docstring_tokens": ["Import", "a", "backup", "from", "Cloud", "Storage", "to", "Cloud", "Datastore", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L297-L337", "partition": "test"}
{"repo": "neurosynth/neurosynth", "path": "neurosynth/analysis/network.py", "func_name": "coactivation", "original_string": "def coactivation(dataset, seed, threshold=0.0, output_dir='.', prefix='', r=6):\n    \"\"\" Compute and save coactivation map given input image as seed.\n\n    This is essentially just a wrapper for a meta-analysis defined\n    by the contrast between those studies that activate within the seed\n    and those that don't.\n\n    Args:\n      dataset: a Dataset instance containing study and activation data.\n      seed: either a Nifti or Analyze image defining the boundaries of the\n        seed, or a list of triples (x/y/z) defining the seed(s). Note that\n        voxels do not need to be contiguous to define a seed--all supra-\n        threshold voxels will be lumped together.\n      threshold: optional float indicating the threshold above which voxels\n        are considered to be part of the seed ROI (default = 0)\n      r: optional integer indicating radius (in mm) of spheres to grow\n        (only used if seed is a list of coordinates).\n      output_dir: output directory to write to. Defaults to current.\n        If none, defaults to using the first part of the seed filename.\n      prefix: optional string to prepend to all coactivation images.\n\n    Output:\n      A set of meta-analysis images identical to that generated by\n      meta.MetaAnalysis.\n    \"\"\"\n\n    if isinstance(seed, string_types):\n        ids = dataset.get_studies(mask=seed, activation_threshold=threshold)\n    else:\n        ids = dataset.get_studies(peaks=seed, r=r,\n                                  activation_threshold=threshold)\n\n    ma = meta.MetaAnalysis(dataset, ids)\n    ma.save_results(output_dir, prefix)", "language": "python", "code": "def coactivation(dataset, seed, threshold=0.0, output_dir='.', prefix='', r=6):\n    \"\"\" Compute and save coactivation map given input image as seed.\n\n    This is essentially just a wrapper for a meta-analysis defined\n    by the contrast between those studies that activate within the seed\n    and those that don't.\n\n    Args:\n      dataset: a Dataset instance containing study and activation data.\n      seed: either a Nifti or Analyze image defining the boundaries of the\n        seed, or a list of triples (x/y/z) defining the seed(s). Note that\n        voxels do not need to be contiguous to define a seed--all supra-\n        threshold voxels will be lumped together.\n      threshold: optional float indicating the threshold above which voxels\n        are considered to be part of the seed ROI (default = 0)\n      r: optional integer indicating radius (in mm) of spheres to grow\n        (only used if seed is a list of coordinates).\n      output_dir: output directory to write to. Defaults to current.\n        If none, defaults to using the first part of the seed filename.\n      prefix: optional string to prepend to all coactivation images.\n\n    Output:\n      A set of meta-analysis images identical to that generated by\n      meta.MetaAnalysis.\n    \"\"\"\n\n    if isinstance(seed, string_types):\n        ids = dataset.get_studies(mask=seed, activation_threshold=threshold)\n    else:\n        ids = dataset.get_studies(peaks=seed, r=r,\n                                  activation_threshold=threshold)\n\n    ma = meta.MetaAnalysis(dataset, ids)\n    ma.save_results(output_dir, prefix)", "code_tokens": ["def", "coactivation", "(", "dataset", ",", "seed", ",", "threshold", "=", "0.0", ",", "output_dir", "=", "'.'", ",", "prefix", "=", "''", ",", "r", "=", "6", ")", ":", "if", "isinstance", "(", "seed", ",", "string_types", ")", ":", "ids", "=", "dataset", ".", "get_studies", "(", "mask", "=", "seed", ",", "activation_threshold", "=", "threshold", ")", "else", ":", "ids", "=", "dataset", ".", "get_studies", "(", "peaks", "=", "seed", ",", "r", "=", "r", ",", "activation_threshold", "=", "threshold", ")", "ma", "=", "meta", ".", "MetaAnalysis", "(", "dataset", ",", "ids", ")", "ma", ".", "save_results", "(", "output_dir", ",", "prefix", ")"], "docstring": "Compute and save coactivation map given input image as seed.\n\n    This is essentially just a wrapper for a meta-analysis defined\n    by the contrast between those studies that activate within the seed\n    and those that don't.\n\n    Args:\n      dataset: a Dataset instance containing study and activation data.\n      seed: either a Nifti or Analyze image defining the boundaries of the\n        seed, or a list of triples (x/y/z) defining the seed(s). Note that\n        voxels do not need to be contiguous to define a seed--all supra-\n        threshold voxels will be lumped together.\n      threshold: optional float indicating the threshold above which voxels\n        are considered to be part of the seed ROI (default = 0)\n      r: optional integer indicating radius (in mm) of spheres to grow\n        (only used if seed is a list of coordinates).\n      output_dir: output directory to write to. Defaults to current.\n        If none, defaults to using the first part of the seed filename.\n      prefix: optional string to prepend to all coactivation images.\n\n    Output:\n      A set of meta-analysis images identical to that generated by\n      meta.MetaAnalysis.", "docstring_tokens": ["Compute", "and", "save", "coactivation", "map", "given", "input", "image", "as", "seed", "."], "sha": "948ce7edce15d7df693446e76834e0c23bfe8f11", "url": "https://github.com/neurosynth/neurosynth/blob/948ce7edce15d7df693446e76834e0c23bfe8f11/neurosynth/analysis/network.py#L7-L40", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/results.py", "func_name": "Analysis.branch_stats", "original_string": "def branch_stats(self):\n        \"\"\"Get stats about branches.\n\n        Returns a dict mapping line numbers to a tuple:\n        (total_exits, taken_exits).\n        \"\"\"\n\n        exit_counts = self.parser.exit_counts()\n        missing_arcs = self.missing_branch_arcs()\n        stats = {}\n        for lnum in self.branch_lines():\n            exits = exit_counts[lnum]\n            try:\n                missing = len(missing_arcs[lnum])\n            except KeyError:\n                missing = 0\n            stats[lnum] = (exits, exits - missing)\n        return stats", "language": "python", "code": "def branch_stats(self):\n        \"\"\"Get stats about branches.\n\n        Returns a dict mapping line numbers to a tuple:\n        (total_exits, taken_exits).\n        \"\"\"\n\n        exit_counts = self.parser.exit_counts()\n        missing_arcs = self.missing_branch_arcs()\n        stats = {}\n        for lnum in self.branch_lines():\n            exits = exit_counts[lnum]\n            try:\n                missing = len(missing_arcs[lnum])\n            except KeyError:\n                missing = 0\n            stats[lnum] = (exits, exits - missing)\n        return stats", "code_tokens": ["def", "branch_stats", "(", "self", ")", ":", "exit_counts", "=", "self", ".", "parser", ".", "exit_counts", "(", ")", "missing_arcs", "=", "self", ".", "missing_branch_arcs", "(", ")", "stats", "=", "{", "}", "for", "lnum", "in", "self", ".", "branch_lines", "(", ")", ":", "exits", "=", "exit_counts", "[", "lnum", "]", "try", ":", "missing", "=", "len", "(", "missing_arcs", "[", "lnum", "]", ")", "except", "KeyError", ":", "missing", "=", "0", "stats", "[", "lnum", "]", "=", "(", "exits", ",", "exits", "-", "missing", ")", "return", "stats"], "docstring": "Get stats about branches.\n\n        Returns a dict mapping line numbers to a tuple:\n        (total_exits, taken_exits).", "docstring_tokens": ["Get", "stats", "about", "branches", "."], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/results.py#L169-L186", "partition": "test"}
{"repo": "MisterY/price-database", "path": "pricedb/cli.py", "func_name": "last", "original_string": "def last(symbol: str):\n    \"\"\" displays last price, for symbol if provided \"\"\"\n    app = PriceDbApplication()\n\n    # convert to uppercase\n    if symbol:\n        symbol = symbol.upper()\n        # extract namespace\n        sec_symbol = SecuritySymbol(\"\", \"\")\n        sec_symbol.parse(symbol)\n\n        latest = app.get_latest_price(sec_symbol)\n        assert isinstance(latest, PriceModel)\n        print(f\"{latest}\")\n    else:\n        # Show the latest prices available for all securities.\n        latest = app.get_latest_prices()\n        for price in latest:\n            print(f\"{price}\")", "language": "python", "code": "def last(symbol: str):\n    \"\"\" displays last price, for symbol if provided \"\"\"\n    app = PriceDbApplication()\n\n    # convert to uppercase\n    if symbol:\n        symbol = symbol.upper()\n        # extract namespace\n        sec_symbol = SecuritySymbol(\"\", \"\")\n        sec_symbol.parse(symbol)\n\n        latest = app.get_latest_price(sec_symbol)\n        assert isinstance(latest, PriceModel)\n        print(f\"{latest}\")\n    else:\n        # Show the latest prices available for all securities.\n        latest = app.get_latest_prices()\n        for price in latest:\n            print(f\"{price}\")", "code_tokens": ["def", "last", "(", "symbol", ":", "str", ")", ":", "app", "=", "PriceDbApplication", "(", ")", "# convert to uppercase", "if", "symbol", ":", "symbol", "=", "symbol", ".", "upper", "(", ")", "# extract namespace", "sec_symbol", "=", "SecuritySymbol", "(", "\"\"", ",", "\"\"", ")", "sec_symbol", ".", "parse", "(", "symbol", ")", "latest", "=", "app", ".", "get_latest_price", "(", "sec_symbol", ")", "assert", "isinstance", "(", "latest", ",", "PriceModel", ")", "print", "(", "f\"{latest}\"", ")", "else", ":", "# Show the latest prices available for all securities.", "latest", "=", "app", ".", "get_latest_prices", "(", ")", "for", "price", "in", "latest", ":", "print", "(", "f\"{price}\"", ")"], "docstring": "displays last price, for symbol if provided", "docstring_tokens": ["displays", "last", "price", "for", "symbol", "if", "provided"], "sha": "b4fd366b7763891c690fe3000b8840e656da023e", "url": "https://github.com/MisterY/price-database/blob/b4fd366b7763891c690fe3000b8840e656da023e/pricedb/cli.py#L76-L94", "partition": "test"}
{"repo": "PythonSanSebastian/docstamp", "path": "docstamp/qrcode.py", "func_name": "save_into_qrcode", "original_string": "def save_into_qrcode(text, out_filepath, color='', box_size=10, pixel_size=1850):\n    \"\"\" Save `text` in a qrcode svg image file.\n\n    Parameters\n    ----------\n    text: str\n        The string to be codified in the QR image.\n\n    out_filepath: str\n        Path to the output file\n\n    color: str\n        A RGB color expressed in 6 hexadecimal values.\n\n    box_size: scalar\n        Size of the QR code boxes.\n    \"\"\"\n    try:\n        qr = qrcode.QRCode(version=1, error_correction=qrcode.constants.ERROR_CORRECT_L,\n                           box_size=box_size, border=0, )\n        qr.add_data(text)\n        qr.make(fit=True)\n    except Exception as exc:\n        raise Exception('Error trying to generate QR code '\n                        ' from `vcard_string`: {}'.format(text)) from exc\n    else:\n        img = qr.make_image(image_factory=qrcode.image.svg.SvgPathImage)\n\n    _ = _qrcode_to_file(img, out_filepath)\n\n    if color:\n        replace_file_content(out_filepath, 'fill:#000000', 'fill:#{}'.format(color))", "language": "python", "code": "def save_into_qrcode(text, out_filepath, color='', box_size=10, pixel_size=1850):\n    \"\"\" Save `text` in a qrcode svg image file.\n\n    Parameters\n    ----------\n    text: str\n        The string to be codified in the QR image.\n\n    out_filepath: str\n        Path to the output file\n\n    color: str\n        A RGB color expressed in 6 hexadecimal values.\n\n    box_size: scalar\n        Size of the QR code boxes.\n    \"\"\"\n    try:\n        qr = qrcode.QRCode(version=1, error_correction=qrcode.constants.ERROR_CORRECT_L,\n                           box_size=box_size, border=0, )\n        qr.add_data(text)\n        qr.make(fit=True)\n    except Exception as exc:\n        raise Exception('Error trying to generate QR code '\n                        ' from `vcard_string`: {}'.format(text)) from exc\n    else:\n        img = qr.make_image(image_factory=qrcode.image.svg.SvgPathImage)\n\n    _ = _qrcode_to_file(img, out_filepath)\n\n    if color:\n        replace_file_content(out_filepath, 'fill:#000000', 'fill:#{}'.format(color))", "code_tokens": ["def", "save_into_qrcode", "(", "text", ",", "out_filepath", ",", "color", "=", "''", ",", "box_size", "=", "10", ",", "pixel_size", "=", "1850", ")", ":", "try", ":", "qr", "=", "qrcode", ".", "QRCode", "(", "version", "=", "1", ",", "error_correction", "=", "qrcode", ".", "constants", ".", "ERROR_CORRECT_L", ",", "box_size", "=", "box_size", ",", "border", "=", "0", ",", ")", "qr", ".", "add_data", "(", "text", ")", "qr", ".", "make", "(", "fit", "=", "True", ")", "except", "Exception", "as", "exc", ":", "raise", "Exception", "(", "'Error trying to generate QR code '", "' from `vcard_string`: {}'", ".", "format", "(", "text", ")", ")", "from", "exc", "else", ":", "img", "=", "qr", ".", "make_image", "(", "image_factory", "=", "qrcode", ".", "image", ".", "svg", ".", "SvgPathImage", ")", "_", "=", "_qrcode_to_file", "(", "img", ",", "out_filepath", ")", "if", "color", ":", "replace_file_content", "(", "out_filepath", ",", "'fill:#000000'", ",", "'fill:#{}'", ".", "format", "(", "color", ")", ")"], "docstring": "Save `text` in a qrcode svg image file.\n\n    Parameters\n    ----------\n    text: str\n        The string to be codified in the QR image.\n\n    out_filepath: str\n        Path to the output file\n\n    color: str\n        A RGB color expressed in 6 hexadecimal values.\n\n    box_size: scalar\n        Size of the QR code boxes.", "docstring_tokens": ["Save", "text", "in", "a", "qrcode", "svg", "image", "file", "."], "sha": "b43808f2e15351b0b2f0b7eade9c7ef319c9e646", "url": "https://github.com/PythonSanSebastian/docstamp/blob/b43808f2e15351b0b2f0b7eade9c7ef319c9e646/docstamp/qrcode.py#L10-L41", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/runtime.py", "func_name": "_fn_with_meta", "original_string": "def _fn_with_meta(f, meta: Optional[lmap.Map]):\n    \"\"\"Return a new function with the given meta. If the function f already\n    has a meta map, then merge the \"\"\"\n\n    if not isinstance(meta, lmap.Map):\n        raise TypeError(\"meta must be a map\")\n\n    if inspect.iscoroutinefunction(f):\n\n        @functools.wraps(f)\n        async def wrapped_f(*args, **kwargs):\n            return await f(*args, **kwargs)\n\n    else:\n\n        @functools.wraps(f)  # type: ignore\n        def wrapped_f(*args, **kwargs):\n            return f(*args, **kwargs)\n\n    wrapped_f.meta = (  # type: ignore\n        f.meta.update(meta)\n        if hasattr(f, \"meta\") and isinstance(f.meta, lmap.Map)\n        else meta\n    )\n    wrapped_f.with_meta = partial(_fn_with_meta, wrapped_f)  # type: ignore\n    return wrapped_f", "language": "python", "code": "def _fn_with_meta(f, meta: Optional[lmap.Map]):\n    \"\"\"Return a new function with the given meta. If the function f already\n    has a meta map, then merge the \"\"\"\n\n    if not isinstance(meta, lmap.Map):\n        raise TypeError(\"meta must be a map\")\n\n    if inspect.iscoroutinefunction(f):\n\n        @functools.wraps(f)\n        async def wrapped_f(*args, **kwargs):\n            return await f(*args, **kwargs)\n\n    else:\n\n        @functools.wraps(f)  # type: ignore\n        def wrapped_f(*args, **kwargs):\n            return f(*args, **kwargs)\n\n    wrapped_f.meta = (  # type: ignore\n        f.meta.update(meta)\n        if hasattr(f, \"meta\") and isinstance(f.meta, lmap.Map)\n        else meta\n    )\n    wrapped_f.with_meta = partial(_fn_with_meta, wrapped_f)  # type: ignore\n    return wrapped_f", "code_tokens": ["def", "_fn_with_meta", "(", "f", ",", "meta", ":", "Optional", "[", "lmap", ".", "Map", "]", ")", ":", "if", "not", "isinstance", "(", "meta", ",", "lmap", ".", "Map", ")", ":", "raise", "TypeError", "(", "\"meta must be a map\"", ")", "if", "inspect", ".", "iscoroutinefunction", "(", "f", ")", ":", "@", "functools", ".", "wraps", "(", "f", ")", "async", "def", "wrapped_f", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "return", "await", "f", "(", "*", "args", ",", "*", "*", "kwargs", ")", "else", ":", "@", "functools", ".", "wraps", "(", "f", ")", "# type: ignore", "def", "wrapped_f", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "return", "f", "(", "*", "args", ",", "*", "*", "kwargs", ")", "wrapped_f", ".", "meta", "=", "(", "# type: ignore", "f", ".", "meta", ".", "update", "(", "meta", ")", "if", "hasattr", "(", "f", ",", "\"meta\"", ")", "and", "isinstance", "(", "f", ".", "meta", ",", "lmap", ".", "Map", ")", "else", "meta", ")", "wrapped_f", ".", "with_meta", "=", "partial", "(", "_fn_with_meta", ",", "wrapped_f", ")", "# type: ignore", "return", "wrapped_f"], "docstring": "Return a new function with the given meta. If the function f already\n    has a meta map, then merge the", "docstring_tokens": ["Return", "a", "new", "function", "with", "the", "given", "meta", ".", "If", "the", "function", "f", "already", "has", "a", "meta", "map", "then", "merge", "the"], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/runtime.py#L1218-L1243", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/slack.py", "func_name": "SlackClient.user", "original_string": "def user(self, user_id):\n        \"\"\"Fetch user info.\"\"\"\n\n        resource = self.RUSER_INFO\n\n        params = {\n            self.PUSER: user_id\n        }\n\n        response = self._fetch(resource, params)\n\n        return response", "language": "python", "code": "def user(self, user_id):\n        \"\"\"Fetch user info.\"\"\"\n\n        resource = self.RUSER_INFO\n\n        params = {\n            self.PUSER: user_id\n        }\n\n        response = self._fetch(resource, params)\n\n        return response", "code_tokens": ["def", "user", "(", "self", ",", "user_id", ")", ":", "resource", "=", "self", ".", "RUSER_INFO", "params", "=", "{", "self", ".", "PUSER", ":", "user_id", "}", "response", "=", "self", ".", "_fetch", "(", "resource", ",", "params", ")", "return", "response"], "docstring": "Fetch user info.", "docstring_tokens": ["Fetch", "user", "info", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/slack.py#L376-L387", "partition": "test"}
{"repo": "funilrys/PyFunceble", "path": "PyFunceble/mining.py", "func_name": "Mining.mine", "original_string": "def mine(self):  # pragma: no cover\n        \"\"\"\n        Search for domain or URL related to the original URL or domain.\n\n        :return: The mined domains or URL.\n        :rtype: dict\n        \"\"\"\n\n        if PyFunceble.CONFIGURATION[\"mining\"]:\n            # The mining is activated.\n\n            try:\n                # We get the history.\n                history = PyFunceble.requests.get(\n                    self.to_get,\n                    timeout=PyFunceble.CONFIGURATION[\"seconds_before_http_timeout\"],\n                    headers=self.headers,\n                ).history\n\n                # We initiate a dictionnary which will save the\n                # list of mined links.\n                mined = {self.to_get_bare: []}\n\n                for element in history:\n                    # We loop through the history.\n\n                    # We update the element.\n                    element = element.url\n\n                    if PyFunceble.INTERN[\"to_test_type\"] == \"url\":\n                        # We are testing a full url.\n\n                        # We get the element to append.\n                        to_append = Check().is_url_valid(element, return_base=False)\n                    elif PyFunceble.INTERN[\"to_test_type\"] == \"domain\":\n                        # We are testing a domain.\n\n                        # We get the element to append.\n                        to_append = Check().is_url_valid(element, return_base=True)\n                    else:\n                        raise Exception(\"Unknown tested.\")\n\n                    if to_append:\n                        # There is something to append.\n\n                        if to_append.endswith(\":80\"):\n                            # The port is present.\n\n                            # We get rid of it.\n                            to_append = to_append[:-3]\n\n                        if to_append != self.to_get_bare:\n                            # The element to append is different as\n                            # the element we are globally testing.\n\n                            # We append the element to append to the\n                            # list of mined links.\n                            mined[self.to_get_bare].append(to_append)\n\n                if mined[self.to_get_bare]:\n                    # There is something in the list of mined links.\n\n                    # We return the whole element.\n                    return mined\n\n                # There is nothing in the list of mined links.\n\n                # We return None.\n                return None\n\n            except (\n                PyFunceble.requests.ConnectionError,\n                PyFunceble.requests.exceptions.Timeout,\n                PyFunceble.requests.exceptions.InvalidURL,\n                PyFunceble.socket.timeout,\n                urllib3_exceptions.InvalidHeader,\n                UnicodeDecodeError,  # The probability that this happend in production is minimal.\n            ):\n                # Something went wrong.\n\n                # We return None.\n                return None\n        return None", "language": "python", "code": "def mine(self):  # pragma: no cover\n        \"\"\"\n        Search for domain or URL related to the original URL or domain.\n\n        :return: The mined domains or URL.\n        :rtype: dict\n        \"\"\"\n\n        if PyFunceble.CONFIGURATION[\"mining\"]:\n            # The mining is activated.\n\n            try:\n                # We get the history.\n                history = PyFunceble.requests.get(\n                    self.to_get,\n                    timeout=PyFunceble.CONFIGURATION[\"seconds_before_http_timeout\"],\n                    headers=self.headers,\n                ).history\n\n                # We initiate a dictionnary which will save the\n                # list of mined links.\n                mined = {self.to_get_bare: []}\n\n                for element in history:\n                    # We loop through the history.\n\n                    # We update the element.\n                    element = element.url\n\n                    if PyFunceble.INTERN[\"to_test_type\"] == \"url\":\n                        # We are testing a full url.\n\n                        # We get the element to append.\n                        to_append = Check().is_url_valid(element, return_base=False)\n                    elif PyFunceble.INTERN[\"to_test_type\"] == \"domain\":\n                        # We are testing a domain.\n\n                        # We get the element to append.\n                        to_append = Check().is_url_valid(element, return_base=True)\n                    else:\n                        raise Exception(\"Unknown tested.\")\n\n                    if to_append:\n                        # There is something to append.\n\n                        if to_append.endswith(\":80\"):\n                            # The port is present.\n\n                            # We get rid of it.\n                            to_append = to_append[:-3]\n\n                        if to_append != self.to_get_bare:\n                            # The element to append is different as\n                            # the element we are globally testing.\n\n                            # We append the element to append to the\n                            # list of mined links.\n                            mined[self.to_get_bare].append(to_append)\n\n                if mined[self.to_get_bare]:\n                    # There is something in the list of mined links.\n\n                    # We return the whole element.\n                    return mined\n\n                # There is nothing in the list of mined links.\n\n                # We return None.\n                return None\n\n            except (\n                PyFunceble.requests.ConnectionError,\n                PyFunceble.requests.exceptions.Timeout,\n                PyFunceble.requests.exceptions.InvalidURL,\n                PyFunceble.socket.timeout,\n                urllib3_exceptions.InvalidHeader,\n                UnicodeDecodeError,  # The probability that this happend in production is minimal.\n            ):\n                # Something went wrong.\n\n                # We return None.\n                return None\n        return None", "code_tokens": ["def", "mine", "(", "self", ")", ":", "# pragma: no cover", "if", "PyFunceble", ".", "CONFIGURATION", "[", "\"mining\"", "]", ":", "# The mining is activated.", "try", ":", "# We get the history.", "history", "=", "PyFunceble", ".", "requests", ".", "get", "(", "self", ".", "to_get", ",", "timeout", "=", "PyFunceble", ".", "CONFIGURATION", "[", "\"seconds_before_http_timeout\"", "]", ",", "headers", "=", "self", ".", "headers", ",", ")", ".", "history", "# We initiate a dictionnary which will save the", "# list of mined links.", "mined", "=", "{", "self", ".", "to_get_bare", ":", "[", "]", "}", "for", "element", "in", "history", ":", "# We loop through the history.", "# We update the element.", "element", "=", "element", ".", "url", "if", "PyFunceble", ".", "INTERN", "[", "\"to_test_type\"", "]", "==", "\"url\"", ":", "# We are testing a full url.", "# We get the element to append.", "to_append", "=", "Check", "(", ")", ".", "is_url_valid", "(", "element", ",", "return_base", "=", "False", ")", "elif", "PyFunceble", ".", "INTERN", "[", "\"to_test_type\"", "]", "==", "\"domain\"", ":", "# We are testing a domain.", "# We get the element to append.", "to_append", "=", "Check", "(", ")", ".", "is_url_valid", "(", "element", ",", "return_base", "=", "True", ")", "else", ":", "raise", "Exception", "(", "\"Unknown tested.\"", ")", "if", "to_append", ":", "# There is something to append.", "if", "to_append", ".", "endswith", "(", "\":80\"", ")", ":", "# The port is present.", "# We get rid of it.", "to_append", "=", "to_append", "[", ":", "-", "3", "]", "if", "to_append", "!=", "self", ".", "to_get_bare", ":", "# The element to append is different as", "# the element we are globally testing.", "# We append the element to append to the", "# list of mined links.", "mined", "[", "self", ".", "to_get_bare", "]", ".", "append", "(", "to_append", ")", "if", "mined", "[", "self", ".", "to_get_bare", "]", ":", "# There is something in the list of mined links.", "# We return the whole element.", "return", "mined", "# There is nothing in the list of mined links.", "# We return None.", "return", "None", "except", "(", "PyFunceble", ".", "requests", ".", "ConnectionError", ",", "PyFunceble", ".", "requests", ".", "exceptions", ".", "Timeout", ",", "PyFunceble", ".", "requests", ".", "exceptions", ".", "InvalidURL", ",", "PyFunceble", ".", "socket", ".", "timeout", ",", "urllib3_exceptions", ".", "InvalidHeader", ",", "UnicodeDecodeError", ",", "# The probability that this happend in production is minimal.", ")", ":", "# Something went wrong.", "# We return None.", "return", "None", "return", "None"], "docstring": "Search for domain or URL related to the original URL or domain.\n\n        :return: The mined domains or URL.\n        :rtype: dict", "docstring_tokens": ["Search", "for", "domain", "or", "URL", "related", "to", "the", "original", "URL", "or", "domain", "."], "sha": "cdf69cbde120199171f7158e1c33635753e6e2f5", "url": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/mining.py#L122-L204", "partition": "test"}
{"repo": "lambdalisue/django-codemirror-widget", "path": "codemirror/widgets.py", "func_name": "CodeMirrorTextarea.render", "original_string": "def render(self, name, value, attrs=None):\n        u\"\"\"Render CodeMirrorTextarea\"\"\"\n        if self.js_var_format is not None:\n            js_var_bit = 'var %s = ' % (self.js_var_format % name)\n        else:\n            js_var_bit = ''\n        output = [super(CodeMirrorTextarea, self).render(name, value, attrs),\n            '<script type=\"text/javascript\">%sCodeMirror.fromTextArea(document.getElementById(%s), %s);</script>' %\n                (js_var_bit, '\"id_%s\"' % name, self.option_json)]\n        return mark_safe('\\n'.join(output))", "language": "python", "code": "def render(self, name, value, attrs=None):\n        u\"\"\"Render CodeMirrorTextarea\"\"\"\n        if self.js_var_format is not None:\n            js_var_bit = 'var %s = ' % (self.js_var_format % name)\n        else:\n            js_var_bit = ''\n        output = [super(CodeMirrorTextarea, self).render(name, value, attrs),\n            '<script type=\"text/javascript\">%sCodeMirror.fromTextArea(document.getElementById(%s), %s);</script>' %\n                (js_var_bit, '\"id_%s\"' % name, self.option_json)]\n        return mark_safe('\\n'.join(output))", "code_tokens": ["def", "render", "(", "self", ",", "name", ",", "value", ",", "attrs", "=", "None", ")", ":", "if", "self", ".", "js_var_format", "is", "not", "None", ":", "js_var_bit", "=", "'var %s = '", "%", "(", "self", ".", "js_var_format", "%", "name", ")", "else", ":", "js_var_bit", "=", "''", "output", "=", "[", "super", "(", "CodeMirrorTextarea", ",", "self", ")", ".", "render", "(", "name", ",", "value", ",", "attrs", ")", ",", "'<script type=\"text/javascript\">%sCodeMirror.fromTextArea(document.getElementById(%s), %s);</script>'", "%", "(", "js_var_bit", ",", "'\"id_%s\"'", "%", "name", ",", "self", ".", "option_json", ")", "]", "return", "mark_safe", "(", "'\\n'", ".", "join", "(", "output", ")", ")"], "docstring": "u\"\"\"Render CodeMirrorTextarea", "docstring_tokens": ["u", "Render", "CodeMirrorTextarea"], "sha": "e795ade2c0c18b462e729feafa616a3047998b4b", "url": "https://github.com/lambdalisue/django-codemirror-widget/blob/e795ade2c0c18b462e729feafa616a3047998b4b/codemirror/widgets.py#L162-L171", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/external/ssh/tunnel.py", "func_name": "_try_passwordless_openssh", "original_string": "def _try_passwordless_openssh(server, keyfile):\n    \"\"\"Try passwordless login with shell ssh command.\"\"\"\n    if pexpect is None:\n        raise ImportError(\"pexpect unavailable, use paramiko\")\n    cmd = 'ssh -f '+ server\n    if keyfile:\n        cmd += ' -i ' + keyfile\n    cmd += ' exit'\n    p = pexpect.spawn(cmd)\n    while True:\n        try:\n            p.expect('[Pp]assword:', timeout=.1)\n        except pexpect.TIMEOUT:\n            continue\n        except pexpect.EOF:\n            return True\n        else:\n            return False", "language": "python", "code": "def _try_passwordless_openssh(server, keyfile):\n    \"\"\"Try passwordless login with shell ssh command.\"\"\"\n    if pexpect is None:\n        raise ImportError(\"pexpect unavailable, use paramiko\")\n    cmd = 'ssh -f '+ server\n    if keyfile:\n        cmd += ' -i ' + keyfile\n    cmd += ' exit'\n    p = pexpect.spawn(cmd)\n    while True:\n        try:\n            p.expect('[Pp]assword:', timeout=.1)\n        except pexpect.TIMEOUT:\n            continue\n        except pexpect.EOF:\n            return True\n        else:\n            return False", "code_tokens": ["def", "_try_passwordless_openssh", "(", "server", ",", "keyfile", ")", ":", "if", "pexpect", "is", "None", ":", "raise", "ImportError", "(", "\"pexpect unavailable, use paramiko\"", ")", "cmd", "=", "'ssh -f '", "+", "server", "if", "keyfile", ":", "cmd", "+=", "' -i '", "+", "keyfile", "cmd", "+=", "' exit'", "p", "=", "pexpect", ".", "spawn", "(", "cmd", ")", "while", "True", ":", "try", ":", "p", ".", "expect", "(", "'[Pp]assword:'", ",", "timeout", "=", ".1", ")", "except", "pexpect", ".", "TIMEOUT", ":", "continue", "except", "pexpect", ".", "EOF", ":", "return", "True", "else", ":", "return", "False"], "docstring": "Try passwordless login with shell ssh command.", "docstring_tokens": ["Try", "passwordless", "login", "with", "shell", "ssh", "command", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/external/ssh/tunnel.py#L89-L106", "partition": "test"}
{"repo": "assemblerflow/flowcraft", "path": "flowcraft/generator/process.py", "func_name": "Process.update_attributes", "original_string": "def update_attributes(self, attr_dict):\n        \"\"\"Updates the directives attribute from a dictionary object.\n\n        This will only update the directives for processes that have been\n        defined in the subclass.\n\n        Parameters\n        ----------\n        attr_dict : dict\n            Dictionary containing the attributes that will be used to update\n            the process attributes and/or directives.\n\n        \"\"\"\n\n        # Update directives\n        # Allowed attributes to write\n        valid_directives = [\"pid\", \"ignore_type\", \"ignore_pid\", \"extra_input\",\n                            \"group\", \"input_type\"]\n\n        for attribute, val in attr_dict.items():\n\n            # If the attribute has a valid directive key, update that\n            # directive\n            if attribute in valid_directives and hasattr(self, attribute):\n                setattr(self, attribute, val)\n\n            # The params attribute is special, in the sense that it provides\n            # information for the self.params attribute.\n            elif attribute == \"params\":\n                for name, value in val.items():\n                    if name in self.params:\n                        self.params[name][\"default\"] = value\n                    else:\n                        raise eh.ProcessError(\n                            \"The parameter name '{}' does not exist for \"\n                            \"component '{}'\".format(name, self.template))\n\n            else:\n                for p in self.directives:\n                    self.directives[p][attribute] = val", "language": "python", "code": "def update_attributes(self, attr_dict):\n        \"\"\"Updates the directives attribute from a dictionary object.\n\n        This will only update the directives for processes that have been\n        defined in the subclass.\n\n        Parameters\n        ----------\n        attr_dict : dict\n            Dictionary containing the attributes that will be used to update\n            the process attributes and/or directives.\n\n        \"\"\"\n\n        # Update directives\n        # Allowed attributes to write\n        valid_directives = [\"pid\", \"ignore_type\", \"ignore_pid\", \"extra_input\",\n                            \"group\", \"input_type\"]\n\n        for attribute, val in attr_dict.items():\n\n            # If the attribute has a valid directive key, update that\n            # directive\n            if attribute in valid_directives and hasattr(self, attribute):\n                setattr(self, attribute, val)\n\n            # The params attribute is special, in the sense that it provides\n            # information for the self.params attribute.\n            elif attribute == \"params\":\n                for name, value in val.items():\n                    if name in self.params:\n                        self.params[name][\"default\"] = value\n                    else:\n                        raise eh.ProcessError(\n                            \"The parameter name '{}' does not exist for \"\n                            \"component '{}'\".format(name, self.template))\n\n            else:\n                for p in self.directives:\n                    self.directives[p][attribute] = val", "code_tokens": ["def", "update_attributes", "(", "self", ",", "attr_dict", ")", ":", "# Update directives", "# Allowed attributes to write", "valid_directives", "=", "[", "\"pid\"", ",", "\"ignore_type\"", ",", "\"ignore_pid\"", ",", "\"extra_input\"", ",", "\"group\"", ",", "\"input_type\"", "]", "for", "attribute", ",", "val", "in", "attr_dict", ".", "items", "(", ")", ":", "# If the attribute has a valid directive key, update that", "# directive", "if", "attribute", "in", "valid_directives", "and", "hasattr", "(", "self", ",", "attribute", ")", ":", "setattr", "(", "self", ",", "attribute", ",", "val", ")", "# The params attribute is special, in the sense that it provides", "# information for the self.params attribute.", "elif", "attribute", "==", "\"params\"", ":", "for", "name", ",", "value", "in", "val", ".", "items", "(", ")", ":", "if", "name", "in", "self", ".", "params", ":", "self", ".", "params", "[", "name", "]", "[", "\"default\"", "]", "=", "value", "else", ":", "raise", "eh", ".", "ProcessError", "(", "\"The parameter name '{}' does not exist for \"", "\"component '{}'\"", ".", "format", "(", "name", ",", "self", ".", "template", ")", ")", "else", ":", "for", "p", "in", "self", ".", "directives", ":", "self", ".", "directives", "[", "p", "]", "[", "attribute", "]", "=", "val"], "docstring": "Updates the directives attribute from a dictionary object.\n\n        This will only update the directives for processes that have been\n        defined in the subclass.\n\n        Parameters\n        ----------\n        attr_dict : dict\n            Dictionary containing the attributes that will be used to update\n            the process attributes and/or directives.", "docstring_tokens": ["Updates", "the", "directives", "attribute", "from", "a", "dictionary", "object", "."], "sha": "fc3f4bddded1efc76006600016dc71a06dd908c0", "url": "https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/generator/process.py#L569-L608", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlDatabaseHook.reserve_free_tcp_port", "original_string": "def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind(('127.0.0.1', 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]", "language": "python", "code": "def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind(('127.0.0.1', 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]", "code_tokens": ["def", "reserve_free_tcp_port", "(", "self", ")", ":", "self", ".", "reserved_tcp_socket", "=", "socket", ".", "socket", "(", "socket", ".", "AF_INET", ",", "socket", ".", "SOCK_STREAM", ")", "self", ".", "reserved_tcp_socket", ".", "bind", "(", "(", "'127.0.0.1'", ",", "0", ")", ")", "self", ".", "sql_proxy_tcp_port", "=", "self", ".", "reserved_tcp_socket", ".", "getsockname", "(", ")", "[", "1", "]"], "docstring": "Reserve free TCP port to be used by Cloud SQL Proxy", "docstring_tokens": ["Reserve", "free", "TCP", "port", "to", "be", "used", "by", "Cloud", "SQL", "Proxy"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L984-L990", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/commands/setup/setup_scout.py", "func_name": "database", "original_string": "def database(context, institute_name, user_name, user_mail, api_key):\n    \"\"\"Setup a scout database.\"\"\"\n    LOG.info(\"Running scout setup database\")\n\n    # Fetch the omim information\n    api_key = api_key or context.obj.get('omim_api_key')\n    if not api_key:\n        LOG.warning(\"Please provide a omim api key with --api-key\")\n        context.abort()\n\n    institute_name = institute_name or context.obj['institute_name']\n    user_name = user_name or context.obj['user_name']\n    user_mail = user_mail or context.obj['user_mail']\n\n    adapter = context.obj['adapter']\n\n    LOG.info(\"Setting up database %s\", context.obj['mongodb'])\n    \n    setup_scout(\n        adapter=adapter,\n        institute_id=institute_name, \n        user_name=user_name, \n        user_mail = user_mail, \n        api_key=api_key\n    )", "language": "python", "code": "def database(context, institute_name, user_name, user_mail, api_key):\n    \"\"\"Setup a scout database.\"\"\"\n    LOG.info(\"Running scout setup database\")\n\n    # Fetch the omim information\n    api_key = api_key or context.obj.get('omim_api_key')\n    if not api_key:\n        LOG.warning(\"Please provide a omim api key with --api-key\")\n        context.abort()\n\n    institute_name = institute_name or context.obj['institute_name']\n    user_name = user_name or context.obj['user_name']\n    user_mail = user_mail or context.obj['user_mail']\n\n    adapter = context.obj['adapter']\n\n    LOG.info(\"Setting up database %s\", context.obj['mongodb'])\n    \n    setup_scout(\n        adapter=adapter,\n        institute_id=institute_name, \n        user_name=user_name, \n        user_mail = user_mail, \n        api_key=api_key\n    )", "code_tokens": ["def", "database", "(", "context", ",", "institute_name", ",", "user_name", ",", "user_mail", ",", "api_key", ")", ":", "LOG", ".", "info", "(", "\"Running scout setup database\"", ")", "# Fetch the omim information", "api_key", "=", "api_key", "or", "context", ".", "obj", ".", "get", "(", "'omim_api_key'", ")", "if", "not", "api_key", ":", "LOG", ".", "warning", "(", "\"Please provide a omim api key with --api-key\"", ")", "context", ".", "abort", "(", ")", "institute_name", "=", "institute_name", "or", "context", ".", "obj", "[", "'institute_name'", "]", "user_name", "=", "user_name", "or", "context", ".", "obj", "[", "'user_name'", "]", "user_mail", "=", "user_mail", "or", "context", ".", "obj", "[", "'user_mail'", "]", "adapter", "=", "context", ".", "obj", "[", "'adapter'", "]", "LOG", ".", "info", "(", "\"Setting up database %s\"", ",", "context", ".", "obj", "[", "'mongodb'", "]", ")", "setup_scout", "(", "adapter", "=", "adapter", ",", "institute_id", "=", "institute_name", ",", "user_name", "=", "user_name", ",", "user_mail", "=", "user_mail", ",", "api_key", "=", "api_key", ")"], "docstring": "Setup a scout database.", "docstring_tokens": ["Setup", "a", "scout", "database", "."], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/commands/setup/setup_scout.py#L46-L70", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_spanner_hook.py", "func_name": "CloudSpannerHook.create_database", "original_string": "def create_database(self, instance_id, database_id, ddl_statements, project_id=None):\n        \"\"\"\n        Creates a new database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database to create in Cloud Spanner.\n        :type database_id: str\n        :param ddl_statements: The string list containing DDL for the new database.\n        :type ddl_statements: list[str]\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :return: None\n        \"\"\"\n\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id,\n                                     ddl_statements=ddl_statements)\n        try:\n            operation = database.create()  # type: Operation\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()\n            self.log.info(result)\n        return", "language": "python", "code": "def create_database(self, instance_id, database_id, ddl_statements, project_id=None):\n        \"\"\"\n        Creates a new database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database to create in Cloud Spanner.\n        :type database_id: str\n        :param ddl_statements: The string list containing DDL for the new database.\n        :type ddl_statements: list[str]\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :return: None\n        \"\"\"\n\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id,\n                                     ddl_statements=ddl_statements)\n        try:\n            operation = database.create()  # type: Operation\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()\n            self.log.info(result)\n        return", "code_tokens": ["def", "create_database", "(", "self", ",", "instance_id", ",", "database_id", ",", "ddl_statements", ",", "project_id", "=", "None", ")", ":", "instance", "=", "self", ".", "_get_client", "(", "project_id", "=", "project_id", ")", ".", "instance", "(", "instance_id", "=", "instance_id", ")", "if", "not", "instance", ".", "exists", "(", ")", ":", "raise", "AirflowException", "(", "\"The instance {} does not exist in project {} !\"", ".", "format", "(", "instance_id", ",", "project_id", ")", ")", "database", "=", "instance", ".", "database", "(", "database_id", "=", "database_id", ",", "ddl_statements", "=", "ddl_statements", ")", "try", ":", "operation", "=", "database", ".", "create", "(", ")", "# type: Operation", "except", "GoogleAPICallError", "as", "e", ":", "self", ".", "log", ".", "error", "(", "'An error occurred: %s. Exiting.'", ",", "e", ".", "message", ")", "raise", "e", "if", "operation", ":", "result", "=", "operation", ".", "result", "(", ")", "self", ".", "log", ".", "info", "(", "result", ")", "return"], "docstring": "Creates a new database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database to create in Cloud Spanner.\n        :type database_id: str\n        :param ddl_statements: The string list containing DDL for the new database.\n        :type ddl_statements: list[str]\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :return: None", "docstring_tokens": ["Creates", "a", "new", "database", "in", "Cloud", "Spanner", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_spanner_hook.py#L212-L244", "partition": "test"}
{"repo": "NarrativeScience/lsi", "path": "src/lsi/utils/stream.py", "func_name": "stream_command_dicts", "original_string": "def stream_command_dicts(commands, parallel=False):\n    \"\"\"\n    Takes a list of dictionaries with keys corresponding to ``stream_command``\n    arguments, and runs all concurrently.\n\n    :param commands: A list of dictionaries, the keys of which should line up\n                     with the arguments to ``stream_command`` function.\n    :type commands: ``list`` of ``dict``\n    :param parallel: If true, commands will be run in parallel.\n    :type parallel: ``bool``\n    \"\"\"\n    if parallel is True:\n        threads = []\n        for command in commands:\n            target = lambda: stream_command(**command)\n            thread = Thread(target=target)\n            thread.start()\n            threads.append(thread)\n        for t in threads:\n            t.join()\n    else:\n        for command in commands:\n            stream_command(**command)", "language": "python", "code": "def stream_command_dicts(commands, parallel=False):\n    \"\"\"\n    Takes a list of dictionaries with keys corresponding to ``stream_command``\n    arguments, and runs all concurrently.\n\n    :param commands: A list of dictionaries, the keys of which should line up\n                     with the arguments to ``stream_command`` function.\n    :type commands: ``list`` of ``dict``\n    :param parallel: If true, commands will be run in parallel.\n    :type parallel: ``bool``\n    \"\"\"\n    if parallel is True:\n        threads = []\n        for command in commands:\n            target = lambda: stream_command(**command)\n            thread = Thread(target=target)\n            thread.start()\n            threads.append(thread)\n        for t in threads:\n            t.join()\n    else:\n        for command in commands:\n            stream_command(**command)", "code_tokens": ["def", "stream_command_dicts", "(", "commands", ",", "parallel", "=", "False", ")", ":", "if", "parallel", "is", "True", ":", "threads", "=", "[", "]", "for", "command", "in", "commands", ":", "target", "=", "lambda", ":", "stream_command", "(", "*", "*", "command", ")", "thread", "=", "Thread", "(", "target", "=", "target", ")", "thread", ".", "start", "(", ")", "threads", ".", "append", "(", "thread", ")", "for", "t", "in", "threads", ":", "t", ".", "join", "(", ")", "else", ":", "for", "command", "in", "commands", ":", "stream_command", "(", "*", "*", "command", ")"], "docstring": "Takes a list of dictionaries with keys corresponding to ``stream_command``\n    arguments, and runs all concurrently.\n\n    :param commands: A list of dictionaries, the keys of which should line up\n                     with the arguments to ``stream_command`` function.\n    :type commands: ``list`` of ``dict``\n    :param parallel: If true, commands will be run in parallel.\n    :type parallel: ``bool``", "docstring_tokens": ["Takes", "a", "list", "of", "dictionaries", "with", "keys", "corresponding", "to", "stream_command", "arguments", "and", "runs", "all", "concurrently", "."], "sha": "7d901b03fdb1a34ef795e5412bfe9685d948e32d", "url": "https://github.com/NarrativeScience/lsi/blob/7d901b03fdb1a34ef795e5412bfe9685d948e32d/src/lsi/utils/stream.py#L86-L108", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/werkzeug/contrib/cache.py", "func_name": "BaseCache.inc", "original_string": "def inc(self, key, delta=1):\n        \"\"\"Increments the value of a key by `delta`.  If the key does\n        not yet exist it is initialized with `delta`.\n\n        For supporting caches this is an atomic operation.\n\n        :param key: the key to increment.\n        :param delta: the delta to add.\n        :returns: The new value or ``None`` for backend errors.\n        \"\"\"\n        value = (self.get(key) or 0) + delta\n        return value if self.set(key, value) else None", "language": "python", "code": "def inc(self, key, delta=1):\n        \"\"\"Increments the value of a key by `delta`.  If the key does\n        not yet exist it is initialized with `delta`.\n\n        For supporting caches this is an atomic operation.\n\n        :param key: the key to increment.\n        :param delta: the delta to add.\n        :returns: The new value or ``None`` for backend errors.\n        \"\"\"\n        value = (self.get(key) or 0) + delta\n        return value if self.set(key, value) else None", "code_tokens": ["def", "inc", "(", "self", ",", "key", ",", "delta", "=", "1", ")", ":", "value", "=", "(", "self", ".", "get", "(", "key", ")", "or", "0", ")", "+", "delta", "return", "value", "if", "self", ".", "set", "(", "key", ",", "value", ")", "else", "None"], "docstring": "Increments the value of a key by `delta`.  If the key does\n        not yet exist it is initialized with `delta`.\n\n        For supporting caches this is an atomic operation.\n\n        :param key: the key to increment.\n        :param delta: the delta to add.\n        :returns: The new value or ``None`` for backend errors.", "docstring_tokens": ["Increments", "the", "value", "of", "a", "key", "by", "delta", ".", "If", "the", "key", "does", "not", "yet", "exist", "it", "is", "initialized", "with", "delta", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/werkzeug/contrib/cache.py#L206-L217", "partition": "test"}
{"repo": "pmacosta/peng", "path": "peng/wave_functions.py", "func_name": "ffti", "original_string": "def ffti(wave, npoints=None, indep_min=None, indep_max=None):\n    r\"\"\"\n    Return the imaginary part of the Fast Fourier Transform of a waveform.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param npoints: Number of points to use in the transform. If **npoints**\n                    is less than the size of the independent variable vector\n                    the waveform is truncated; if **npoints** is greater than\n                    the size of the independent variable vector, the waveform\n                    is zero-padded\n    :type  npoints: positive integer\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.ffti\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`npoints\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n     * RuntimeError (Non-uniform sampling)\n\n    .. [[[end]]]\n    \"\"\"\n    return imag(fft(wave, npoints, indep_min, indep_max))", "language": "python", "code": "def ffti(wave, npoints=None, indep_min=None, indep_max=None):\n    r\"\"\"\n    Return the imaginary part of the Fast Fourier Transform of a waveform.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param npoints: Number of points to use in the transform. If **npoints**\n                    is less than the size of the independent variable vector\n                    the waveform is truncated; if **npoints** is greater than\n                    the size of the independent variable vector, the waveform\n                    is zero-padded\n    :type  npoints: positive integer\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.ffti\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`npoints\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n     * RuntimeError (Non-uniform sampling)\n\n    .. [[[end]]]\n    \"\"\"\n    return imag(fft(wave, npoints, indep_min, indep_max))", "code_tokens": ["def", "ffti", "(", "wave", ",", "npoints", "=", "None", ",", "indep_min", "=", "None", ",", "indep_max", "=", "None", ")", ":", "return", "imag", "(", "fft", "(", "wave", ",", "npoints", ",", "indep_min", ",", "indep_max", ")", ")"], "docstring": "r\"\"\"\n    Return the imaginary part of the Fast Fourier Transform of a waveform.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param npoints: Number of points to use in the transform. If **npoints**\n                    is less than the size of the independent variable vector\n                    the waveform is truncated; if **npoints** is greater than\n                    the size of the independent variable vector, the waveform\n                    is zero-padded\n    :type  npoints: positive integer\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.ffti\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`npoints\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n     * RuntimeError (Non-uniform sampling)\n\n    .. [[[end]]]", "docstring_tokens": ["r", "Return", "the", "imaginary", "part", "of", "the", "Fast", "Fourier", "Transform", "of", "a", "waveform", "."], "sha": "976935377adaa3de26fc5677aceb2cdfbd6f93a7", "url": "https://github.com/pmacosta/peng/blob/976935377adaa3de26fc5677aceb2cdfbd6f93a7/peng/wave_functions.py#L624-L666", "partition": "test"}
{"repo": "katerina7479/pypdflite", "path": "pypdflite/pdfobjects/pdffont.py", "func_name": "PDFFont._set_style", "original_string": "def _set_style(self, style=None):\r\n        \"\"\" Style should be a string, containing the letters 'B' for bold,\r\n        'U' for underline, or 'I' for italic, or should be '', for no style.\r\n        Symbol will not be underlined. The underline style can further be\r\n        modified by specifying the underline thickness and position.\r\n\r\n        \"\"\"\r\n        if style is None:\r\n            self.style = ''\r\n            self.underline = False\r\n        # No syling for symbol\r\n        elif self.family == ('symbol' or 'zapfdingbats'):\r\n            self.style = ''\r\n            self.underline = False\r\n\r\n        self.style = style.upper()\r\n            # SetUnderline\r\n\r\n        if 'U' in self.style or self.style == 'U':\r\n            self.underline = True\r\n        else:\r\n            self.underline = False", "language": "python", "code": "def _set_style(self, style=None):\r\n        \"\"\" Style should be a string, containing the letters 'B' for bold,\r\n        'U' for underline, or 'I' for italic, or should be '', for no style.\r\n        Symbol will not be underlined. The underline style can further be\r\n        modified by specifying the underline thickness and position.\r\n\r\n        \"\"\"\r\n        if style is None:\r\n            self.style = ''\r\n            self.underline = False\r\n        # No syling for symbol\r\n        elif self.family == ('symbol' or 'zapfdingbats'):\r\n            self.style = ''\r\n            self.underline = False\r\n\r\n        self.style = style.upper()\r\n            # SetUnderline\r\n\r\n        if 'U' in self.style or self.style == 'U':\r\n            self.underline = True\r\n        else:\r\n            self.underline = False", "code_tokens": ["def", "_set_style", "(", "self", ",", "style", "=", "None", ")", ":", "if", "style", "is", "None", ":", "self", ".", "style", "=", "''", "self", ".", "underline", "=", "False", "# No syling for symbol\r", "elif", "self", ".", "family", "==", "(", "'symbol'", "or", "'zapfdingbats'", ")", ":", "self", ".", "style", "=", "''", "self", ".", "underline", "=", "False", "self", ".", "style", "=", "style", ".", "upper", "(", ")", "# SetUnderline\r", "if", "'U'", "in", "self", ".", "style", "or", "self", ".", "style", "==", "'U'", ":", "self", ".", "underline", "=", "True", "else", ":", "self", ".", "underline", "=", "False"], "docstring": "Style should be a string, containing the letters 'B' for bold,\r\n        'U' for underline, or 'I' for italic, or should be '', for no style.\r\n        Symbol will not be underlined. The underline style can further be\r\n        modified by specifying the underline thickness and position.", "docstring_tokens": ["Style", "should", "be", "a", "string", "containing", "the", "letters", "B", "for", "bold", "U", "for", "underline", "or", "I", "for", "italic", "or", "should", "be", "for", "no", "style", ".", "Symbol", "will", "not", "be", "underlined", ".", "The", "underline", "style", "can", "further", "be", "modified", "by", "specifying", "the", "underline", "thickness", "and", "position", "."], "sha": "ac2501f30d6619eae9dea5644717575ca9263d0a", "url": "https://github.com/katerina7479/pypdflite/blob/ac2501f30d6619eae9dea5644717575ca9263d0a/pypdflite/pdfobjects/pdffont.py#L47-L68", "partition": "test"}
{"repo": "open-mmlab/mmcv", "path": "mmcv/runner/runner.py", "func_name": "Runner.register_hook", "original_string": "def register_hook(self, hook, priority='NORMAL'):\n        \"\"\"Register a hook into the hook list.\n\n        Args:\n            hook (:obj:`Hook`): The hook to be registered.\n            priority (int or str or :obj:`Priority`): Hook priority.\n                Lower value means higher priority.\n        \"\"\"\n        assert isinstance(hook, Hook)\n        if hasattr(hook, 'priority'):\n            raise ValueError('\"priority\" is a reserved attribute for hooks')\n        priority = get_priority(priority)\n        hook.priority = priority\n        # insert the hook to a sorted list\n        inserted = False\n        for i in range(len(self._hooks) - 1, -1, -1):\n            if priority >= self._hooks[i].priority:\n                self._hooks.insert(i + 1, hook)\n                inserted = True\n                break\n        if not inserted:\n            self._hooks.insert(0, hook)", "language": "python", "code": "def register_hook(self, hook, priority='NORMAL'):\n        \"\"\"Register a hook into the hook list.\n\n        Args:\n            hook (:obj:`Hook`): The hook to be registered.\n            priority (int or str or :obj:`Priority`): Hook priority.\n                Lower value means higher priority.\n        \"\"\"\n        assert isinstance(hook, Hook)\n        if hasattr(hook, 'priority'):\n            raise ValueError('\"priority\" is a reserved attribute for hooks')\n        priority = get_priority(priority)\n        hook.priority = priority\n        # insert the hook to a sorted list\n        inserted = False\n        for i in range(len(self._hooks) - 1, -1, -1):\n            if priority >= self._hooks[i].priority:\n                self._hooks.insert(i + 1, hook)\n                inserted = True\n                break\n        if not inserted:\n            self._hooks.insert(0, hook)", "code_tokens": ["def", "register_hook", "(", "self", ",", "hook", ",", "priority", "=", "'NORMAL'", ")", ":", "assert", "isinstance", "(", "hook", ",", "Hook", ")", "if", "hasattr", "(", "hook", ",", "'priority'", ")", ":", "raise", "ValueError", "(", "'\"priority\" is a reserved attribute for hooks'", ")", "priority", "=", "get_priority", "(", "priority", ")", "hook", ".", "priority", "=", "priority", "# insert the hook to a sorted list", "inserted", "=", "False", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_hooks", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "if", "priority", ">=", "self", ".", "_hooks", "[", "i", "]", ".", "priority", ":", "self", ".", "_hooks", ".", "insert", "(", "i", "+", "1", ",", "hook", ")", "inserted", "=", "True", "break", "if", "not", "inserted", ":", "self", ".", "_hooks", ".", "insert", "(", "0", ",", "hook", ")"], "docstring": "Register a hook into the hook list.\n\n        Args:\n            hook (:obj:`Hook`): The hook to be registered.\n            priority (int or str or :obj:`Priority`): Hook priority.\n                Lower value means higher priority.", "docstring_tokens": ["Register", "a", "hook", "into", "the", "hook", "list", "."], "sha": "0d77f61450aab4dde8b8585a577cc496acb95d7f", "url": "https://github.com/open-mmlab/mmcv/blob/0d77f61450aab4dde8b8585a577cc496acb95d7f/mmcv/runner/runner.py#L194-L215", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/disentangled_vae.py", "func_name": "LearnableMultivariateNormalDiag.call", "original_string": "def call(self, inputs):\n    \"\"\"Runs the model to generate multivariate normal distribution.\n\n    Args:\n      inputs: Unused.\n\n    Returns:\n      A MultivariateNormalDiag distribution with event shape\n      [dimensions], batch shape [], and sample shape [sample_shape,\n      dimensions].\n    \"\"\"\n    del inputs  # unused\n    with tf.compat.v1.name_scope(self._name):\n      return tfd.MultivariateNormalDiag(self.loc, self.scale_diag)", "language": "python", "code": "def call(self, inputs):\n    \"\"\"Runs the model to generate multivariate normal distribution.\n\n    Args:\n      inputs: Unused.\n\n    Returns:\n      A MultivariateNormalDiag distribution with event shape\n      [dimensions], batch shape [], and sample shape [sample_shape,\n      dimensions].\n    \"\"\"\n    del inputs  # unused\n    with tf.compat.v1.name_scope(self._name):\n      return tfd.MultivariateNormalDiag(self.loc, self.scale_diag)", "code_tokens": ["def", "call", "(", "self", ",", "inputs", ")", ":", "del", "inputs", "# unused", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "self", ".", "_name", ")", ":", "return", "tfd", ".", "MultivariateNormalDiag", "(", "self", ".", "loc", ",", "self", ".", "scale_diag", ")"], "docstring": "Runs the model to generate multivariate normal distribution.\n\n    Args:\n      inputs: Unused.\n\n    Returns:\n      A MultivariateNormalDiag distribution with event shape\n      [dimensions], batch shape [], and sample shape [sample_shape,\n      dimensions].", "docstring_tokens": ["Runs", "the", "model", "to", "generate", "multivariate", "normal", "distribution", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/disentangled_vae.py#L216-L229", "partition": "test"}
{"repo": "jazzband/django-ddp", "path": "dddp/__init__.py", "func_name": "ThreadLocal.get", "original_string": "def get(self, name, factory, *factory_args, **factory_kwargs):\n        \"\"\"Get attribute, creating if required using specified factory.\"\"\"\n        update_thread_local = getattr(factory, 'update_thread_local', True)\n        if (not update_thread_local) or (name not in self.__dict__):\n            obj = factory(*factory_args, **factory_kwargs)\n            if update_thread_local:\n                setattr(self, name, obj)\n            return obj\n        return getattr(self, name)", "language": "python", "code": "def get(self, name, factory, *factory_args, **factory_kwargs):\n        \"\"\"Get attribute, creating if required using specified factory.\"\"\"\n        update_thread_local = getattr(factory, 'update_thread_local', True)\n        if (not update_thread_local) or (name not in self.__dict__):\n            obj = factory(*factory_args, **factory_kwargs)\n            if update_thread_local:\n                setattr(self, name, obj)\n            return obj\n        return getattr(self, name)", "code_tokens": ["def", "get", "(", "self", ",", "name", ",", "factory", ",", "*", "factory_args", ",", "*", "*", "factory_kwargs", ")", ":", "update_thread_local", "=", "getattr", "(", "factory", ",", "'update_thread_local'", ",", "True", ")", "if", "(", "not", "update_thread_local", ")", "or", "(", "name", "not", "in", "self", ".", "__dict__", ")", ":", "obj", "=", "factory", "(", "*", "factory_args", ",", "*", "*", "factory_kwargs", ")", "if", "update_thread_local", ":", "setattr", "(", "self", ",", "name", ",", "obj", ")", "return", "obj", "return", "getattr", "(", "self", ",", "name", ")"], "docstring": "Get attribute, creating if required using specified factory.", "docstring_tokens": ["Get", "attribute", "creating", "if", "required", "using", "specified", "factory", "."], "sha": "1e1954b06fe140346acea43582515991685e4e01", "url": "https://github.com/jazzband/django-ddp/blob/1e1954b06fe140346acea43582515991685e4e01/dddp/__init__.py#L111-L119", "partition": "test"}
{"repo": "rwl/godot", "path": "godot/edge.py", "func_name": "Edge._parse_xdot_directive", "original_string": "def _parse_xdot_directive(self, name, new):\n        \"\"\" Handles parsing Xdot drawing directives.\n        \"\"\"\n        parser = XdotAttrParser()\n        components = parser.parse_xdot_data(new)\n\n        # The absolute coordinate of the drawing container wrt graph origin.\n        x1 = min( [c.x for c in components] )\n        y1 = min( [c.y for c in components] )\n\n        print \"X1/Y1:\", name, x1, y1\n\n        # Components are positioned relative to their container. This\n        # function positions the bottom-left corner of the components at\n        # their origin rather than relative to the graph.\n#        move_to_origin( components )\n\n        for c in components:\n            if isinstance(c, Ellipse):\n                component.x_origin -= x1\n                component.y_origin -= y1\n#                c.position = [ c.x - x1, c.y - y1 ]\n\n            elif isinstance(c, (Polygon, BSpline)):\n                print \"Points:\", c.points\n                c.points = [ (t[0] - x1, t[1] - y1) for t in c.points ]\n                print \"Points:\", c.points\n\n            elif isinstance(c, Text):\n#                font = str_to_font( str(c.pen.font) )\n                c.text_x, c.text_y = c.x - x1, c.y - y1\n\n        container = Container(auto_size=True,\n            position=[ x1, y1 ],\n            bgcolor=\"yellow\")\n\n        container.add( *components )\n\n        if name == \"_draw_\":\n            self.drawing = container\n        elif name == \"_hdraw_\":\n            self.arrowhead_drawing = container\n        else:\n            raise", "language": "python", "code": "def _parse_xdot_directive(self, name, new):\n        \"\"\" Handles parsing Xdot drawing directives.\n        \"\"\"\n        parser = XdotAttrParser()\n        components = parser.parse_xdot_data(new)\n\n        # The absolute coordinate of the drawing container wrt graph origin.\n        x1 = min( [c.x for c in components] )\n        y1 = min( [c.y for c in components] )\n\n        print \"X1/Y1:\", name, x1, y1\n\n        # Components are positioned relative to their container. This\n        # function positions the bottom-left corner of the components at\n        # their origin rather than relative to the graph.\n#        move_to_origin( components )\n\n        for c in components:\n            if isinstance(c, Ellipse):\n                component.x_origin -= x1\n                component.y_origin -= y1\n#                c.position = [ c.x - x1, c.y - y1 ]\n\n            elif isinstance(c, (Polygon, BSpline)):\n                print \"Points:\", c.points\n                c.points = [ (t[0] - x1, t[1] - y1) for t in c.points ]\n                print \"Points:\", c.points\n\n            elif isinstance(c, Text):\n#                font = str_to_font( str(c.pen.font) )\n                c.text_x, c.text_y = c.x - x1, c.y - y1\n\n        container = Container(auto_size=True,\n            position=[ x1, y1 ],\n            bgcolor=\"yellow\")\n\n        container.add( *components )\n\n        if name == \"_draw_\":\n            self.drawing = container\n        elif name == \"_hdraw_\":\n            self.arrowhead_drawing = container\n        else:\n            raise", "code_tokens": ["def", "_parse_xdot_directive", "(", "self", ",", "name", ",", "new", ")", ":", "parser", "=", "XdotAttrParser", "(", ")", "components", "=", "parser", ".", "parse_xdot_data", "(", "new", ")", "# The absolute coordinate of the drawing container wrt graph origin.", "x1", "=", "min", "(", "[", "c", ".", "x", "for", "c", "in", "components", "]", ")", "y1", "=", "min", "(", "[", "c", ".", "y", "for", "c", "in", "components", "]", ")", "print", "\"X1/Y1:\"", ",", "name", ",", "x1", ",", "y1", "# Components are positioned relative to their container. This", "# function positions the bottom-left corner of the components at", "# their origin rather than relative to the graph.", "#        move_to_origin( components )", "for", "c", "in", "components", ":", "if", "isinstance", "(", "c", ",", "Ellipse", ")", ":", "component", ".", "x_origin", "-=", "x1", "component", ".", "y_origin", "-=", "y1", "#                c.position = [ c.x - x1, c.y - y1 ]", "elif", "isinstance", "(", "c", ",", "(", "Polygon", ",", "BSpline", ")", ")", ":", "print", "\"Points:\"", ",", "c", ".", "points", "c", ".", "points", "=", "[", "(", "t", "[", "0", "]", "-", "x1", ",", "t", "[", "1", "]", "-", "y1", ")", "for", "t", "in", "c", ".", "points", "]", "print", "\"Points:\"", ",", "c", ".", "points", "elif", "isinstance", "(", "c", ",", "Text", ")", ":", "#                font = str_to_font( str(c.pen.font) )", "c", ".", "text_x", ",", "c", ".", "text_y", "=", "c", ".", "x", "-", "x1", ",", "c", ".", "y", "-", "y1", "container", "=", "Container", "(", "auto_size", "=", "True", ",", "position", "=", "[", "x1", ",", "y1", "]", ",", "bgcolor", "=", "\"yellow\"", ")", "container", ".", "add", "(", "*", "components", ")", "if", "name", "==", "\"_draw_\"", ":", "self", ".", "drawing", "=", "container", "elif", "name", "==", "\"_hdraw_\"", ":", "self", ".", "arrowhead_drawing", "=", "container", "else", ":", "raise"], "docstring": "Handles parsing Xdot drawing directives.", "docstring_tokens": ["Handles", "parsing", "Xdot", "drawing", "directives", "."], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/edge.py#L733-L776", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/oinspect.py", "func_name": "getsource", "original_string": "def getsource(obj,is_binary=False):\n    \"\"\"Wrapper around inspect.getsource.\n\n    This can be modified by other projects to provide customized source\n    extraction.\n\n    Inputs:\n\n    - obj: an object whose source code we will attempt to extract.\n\n    Optional inputs:\n\n    - is_binary: whether the object is known to come from a binary source.\n    This implementation will skip returning any output for binary objects, but\n    custom extractors may know how to meaningfully process them.\"\"\"\n\n    if is_binary:\n        return None\n    else:\n        # get source if obj was decorated with @decorator\n        if hasattr(obj,\"__wrapped__\"):\n            obj = obj.__wrapped__\n        try:\n            src = inspect.getsource(obj)\n        except TypeError:\n            if hasattr(obj,'__class__'):\n                src = inspect.getsource(obj.__class__)\n        return src", "language": "python", "code": "def getsource(obj,is_binary=False):\n    \"\"\"Wrapper around inspect.getsource.\n\n    This can be modified by other projects to provide customized source\n    extraction.\n\n    Inputs:\n\n    - obj: an object whose source code we will attempt to extract.\n\n    Optional inputs:\n\n    - is_binary: whether the object is known to come from a binary source.\n    This implementation will skip returning any output for binary objects, but\n    custom extractors may know how to meaningfully process them.\"\"\"\n\n    if is_binary:\n        return None\n    else:\n        # get source if obj was decorated with @decorator\n        if hasattr(obj,\"__wrapped__\"):\n            obj = obj.__wrapped__\n        try:\n            src = inspect.getsource(obj)\n        except TypeError:\n            if hasattr(obj,'__class__'):\n                src = inspect.getsource(obj.__class__)\n        return src", "code_tokens": ["def", "getsource", "(", "obj", ",", "is_binary", "=", "False", ")", ":", "if", "is_binary", ":", "return", "None", "else", ":", "# get source if obj was decorated with @decorator", "if", "hasattr", "(", "obj", ",", "\"__wrapped__\"", ")", ":", "obj", "=", "obj", ".", "__wrapped__", "try", ":", "src", "=", "inspect", ".", "getsource", "(", "obj", ")", "except", "TypeError", ":", "if", "hasattr", "(", "obj", ",", "'__class__'", ")", ":", "src", "=", "inspect", ".", "getsource", "(", "obj", ".", "__class__", ")", "return", "src"], "docstring": "Wrapper around inspect.getsource.\n\n    This can be modified by other projects to provide customized source\n    extraction.\n\n    Inputs:\n\n    - obj: an object whose source code we will attempt to extract.\n\n    Optional inputs:\n\n    - is_binary: whether the object is known to come from a binary source.\n    This implementation will skip returning any output for binary objects, but\n    custom extractors may know how to meaningfully process them.", "docstring_tokens": ["Wrapper", "around", "inspect", ".", "getsource", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/oinspect.py#L118-L145", "partition": "test"}
{"repo": "SmBe19/praw-OAuth2Util", "path": "OAuth2Util/OAuth2Util.py", "func_name": "OAuth2Util._start_webserver", "original_string": "def _start_webserver(self, authorize_url=None):\n\t\t\"\"\"\n\t\tStart the webserver that will receive the code\n\t\t\"\"\"\n\t\tserver_address = (SERVER_URL, SERVER_PORT)\n\t\tself.server = HTTPServer(server_address, OAuth2UtilRequestHandler)\n\t\tself.server.response_code = None\n\t\tself.server.authorize_url = authorize_url\n\t\tt = Thread(target=self.server.serve_forever)\n\t\tt.daemon = True\n\t\tt.start()", "language": "python", "code": "def _start_webserver(self, authorize_url=None):\n\t\t\"\"\"\n\t\tStart the webserver that will receive the code\n\t\t\"\"\"\n\t\tserver_address = (SERVER_URL, SERVER_PORT)\n\t\tself.server = HTTPServer(server_address, OAuth2UtilRequestHandler)\n\t\tself.server.response_code = None\n\t\tself.server.authorize_url = authorize_url\n\t\tt = Thread(target=self.server.serve_forever)\n\t\tt.daemon = True\n\t\tt.start()", "code_tokens": ["def", "_start_webserver", "(", "self", ",", "authorize_url", "=", "None", ")", ":", "server_address", "=", "(", "SERVER_URL", ",", "SERVER_PORT", ")", "self", ".", "server", "=", "HTTPServer", "(", "server_address", ",", "OAuth2UtilRequestHandler", ")", "self", ".", "server", ".", "response_code", "=", "None", "self", ".", "server", ".", "authorize_url", "=", "authorize_url", "t", "=", "Thread", "(", "target", "=", "self", ".", "server", ".", "serve_forever", ")", "t", ".", "daemon", "=", "True", "t", ".", "start", "(", ")"], "docstring": "Start the webserver that will receive the code", "docstring_tokens": ["Start", "the", "webserver", "that", "will", "receive", "the", "code"], "sha": "ca0a2d4d7eefcc681aac92c9cd4b83cd9ea6c5fe", "url": "https://github.com/SmBe19/praw-OAuth2Util/blob/ca0a2d4d7eefcc681aac92c9cd4b83cd9ea6c5fe/OAuth2Util/OAuth2Util.py#L222-L232", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/compiler/nodes.py", "func_name": "Node.fix_missing_locations", "original_string": "def fix_missing_locations(\n        self, start_loc: Optional[Tuple[int, int]] = None\n    ) -> \"Node\":\n        \"\"\"Return a transformed copy of this node with location in this node's\n        environment updated to match the `start_loc` if given, or using its\n        existing location otherwise. All child nodes will be recursively\n        transformed and replaced. Child nodes will use their parent node\n        location if they do not have one.\"\"\"\n        if self.env.line is None or self.env.col is None:\n            loc = start_loc\n        else:\n            loc = (self.env.line, self.env.col)\n\n        assert loc is not None and all(\n            [e is not None for e in loc]\n        ), \"Must specify location information\"\n\n        new_attrs: MutableMapping[str, Union[NodeEnv, Node, Iterable[Node]]] = {\n            \"env\": attr.evolve(self.env, line=loc[0], col=loc[1])\n        }\n        for child_kw in self.children:\n            child_attr = munge(child_kw.name)\n            assert child_attr != \"env\", \"Node environment already set\"\n\n            if child_attr.endswith(\"s\"):\n                iter_child: Iterable[Node] = getattr(self, child_attr)\n                assert iter_child is not None, \"Listed child must not be none\"\n                new_children = []\n                for item in iter_child:\n                    new_children.append(item.fix_missing_locations(start_loc))\n                new_attrs[child_attr] = vec.vector(new_children)\n            else:\n                child: Node = getattr(self, child_attr)\n                assert child is not None, \"Listed child must not be none\"\n                new_attrs[child_attr] = child.fix_missing_locations(start_loc)\n\n        return self.assoc(**new_attrs)", "language": "python", "code": "def fix_missing_locations(\n        self, start_loc: Optional[Tuple[int, int]] = None\n    ) -> \"Node\":\n        \"\"\"Return a transformed copy of this node with location in this node's\n        environment updated to match the `start_loc` if given, or using its\n        existing location otherwise. All child nodes will be recursively\n        transformed and replaced. Child nodes will use their parent node\n        location if they do not have one.\"\"\"\n        if self.env.line is None or self.env.col is None:\n            loc = start_loc\n        else:\n            loc = (self.env.line, self.env.col)\n\n        assert loc is not None and all(\n            [e is not None for e in loc]\n        ), \"Must specify location information\"\n\n        new_attrs: MutableMapping[str, Union[NodeEnv, Node, Iterable[Node]]] = {\n            \"env\": attr.evolve(self.env, line=loc[0], col=loc[1])\n        }\n        for child_kw in self.children:\n            child_attr = munge(child_kw.name)\n            assert child_attr != \"env\", \"Node environment already set\"\n\n            if child_attr.endswith(\"s\"):\n                iter_child: Iterable[Node] = getattr(self, child_attr)\n                assert iter_child is not None, \"Listed child must not be none\"\n                new_children = []\n                for item in iter_child:\n                    new_children.append(item.fix_missing_locations(start_loc))\n                new_attrs[child_attr] = vec.vector(new_children)\n            else:\n                child: Node = getattr(self, child_attr)\n                assert child is not None, \"Listed child must not be none\"\n                new_attrs[child_attr] = child.fix_missing_locations(start_loc)\n\n        return self.assoc(**new_attrs)", "code_tokens": ["def", "fix_missing_locations", "(", "self", ",", "start_loc", ":", "Optional", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "None", ")", "->", "\"Node\"", ":", "if", "self", ".", "env", ".", "line", "is", "None", "or", "self", ".", "env", ".", "col", "is", "None", ":", "loc", "=", "start_loc", "else", ":", "loc", "=", "(", "self", ".", "env", ".", "line", ",", "self", ".", "env", ".", "col", ")", "assert", "loc", "is", "not", "None", "and", "all", "(", "[", "e", "is", "not", "None", "for", "e", "in", "loc", "]", ")", ",", "\"Must specify location information\"", "new_attrs", ":", "MutableMapping", "[", "str", ",", "Union", "[", "NodeEnv", ",", "Node", ",", "Iterable", "[", "Node", "]", "]", "]", "=", "{", "\"env\"", ":", "attr", ".", "evolve", "(", "self", ".", "env", ",", "line", "=", "loc", "[", "0", "]", ",", "col", "=", "loc", "[", "1", "]", ")", "}", "for", "child_kw", "in", "self", ".", "children", ":", "child_attr", "=", "munge", "(", "child_kw", ".", "name", ")", "assert", "child_attr", "!=", "\"env\"", ",", "\"Node environment already set\"", "if", "child_attr", ".", "endswith", "(", "\"s\"", ")", ":", "iter_child", ":", "Iterable", "[", "Node", "]", "=", "getattr", "(", "self", ",", "child_attr", ")", "assert", "iter_child", "is", "not", "None", ",", "\"Listed child must not be none\"", "new_children", "=", "[", "]", "for", "item", "in", "iter_child", ":", "new_children", ".", "append", "(", "item", ".", "fix_missing_locations", "(", "start_loc", ")", ")", "new_attrs", "[", "child_attr", "]", "=", "vec", ".", "vector", "(", "new_children", ")", "else", ":", "child", ":", "Node", "=", "getattr", "(", "self", ",", "child_attr", ")", "assert", "child", "is", "not", "None", ",", "\"Listed child must not be none\"", "new_attrs", "[", "child_attr", "]", "=", "child", ".", "fix_missing_locations", "(", "start_loc", ")", "return", "self", ".", "assoc", "(", "*", "*", "new_attrs", ")"], "docstring": "Return a transformed copy of this node with location in this node's\n        environment updated to match the `start_loc` if given, or using its\n        existing location otherwise. All child nodes will be recursively\n        transformed and replaced. Child nodes will use their parent node\n        location if they do not have one.", "docstring_tokens": ["Return", "a", "transformed", "copy", "of", "this", "node", "with", "location", "in", "this", "node", "s", "environment", "updated", "to", "match", "the", "start_loc", "if", "given", "or", "using", "its", "existing", "location", "otherwise", ".", "All", "child", "nodes", "will", "be", "recursively", "transformed", "and", "replaced", ".", "Child", "nodes", "will", "use", "their", "parent", "node", "location", "if", "they", "do", "not", "have", "one", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/compiler/nodes.py#L179-L215", "partition": "test"}
{"repo": "myint/autoflake", "path": "autoflake.py", "func_name": "_split_comma_separated", "original_string": "def _split_comma_separated(string):\n    \"\"\"Return a set of strings.\"\"\"\n    return set(text.strip() for text in string.split(',') if text.strip())", "language": "python", "code": "def _split_comma_separated(string):\n    \"\"\"Return a set of strings.\"\"\"\n    return set(text.strip() for text in string.split(',') if text.strip())", "code_tokens": ["def", "_split_comma_separated", "(", "string", ")", ":", "return", "set", "(", "text", ".", "strip", "(", ")", "for", "text", "in", "string", ".", "split", "(", "','", ")", "if", "text", ".", "strip", "(", ")", ")"], "docstring": "Return a set of strings.", "docstring_tokens": ["Return", "a", "set", "of", "strings", "."], "sha": "68fea68646922b920d55975f9f2adaeafd84df4f", "url": "https://github.com/myint/autoflake/blob/68fea68646922b920d55975f9f2adaeafd84df4f/autoflake.py#L726-L728", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/naturalnaming.py", "func_name": "NNGroupNode.f_load_child", "original_string": "def f_load_child(self, name, recursive=False, load_data=pypetconstants.LOAD_DATA,\n                     max_depth=None):\n        \"\"\"Loads a child or recursively a subtree from disk.\n\n        :param name:\n\n            Name of child to load. If grouped ('groupA.groupB.childC') the path along the way\n            to last node in the chain is loaded. Shortcuts are NOT allowed!\n\n        :param recursive:\n\n            Whether recursively all nodes below the last child should be loaded, too.\n            Note that links are never evaluated recursively. Only the linked node\n            will be loaded if it does not exist in the tree, yet. Any nodes or links\n            of this linked node are not loaded.\n\n        :param load_data:\n\n            Flag how to load the data.\n            For how to choose 'load_data' see :ref:`more-on-loading`.\n\n        :param max_depth:\n\n            In case `recursive` is `True`, you can specify the maximum depth to load\n            load data relative from current node. Leave `None` if you don't want to limit\n            the depth.\n\n        :returns:\n\n            The loaded child, in case of grouping ('groupA.groupB.childC') the last\n            node (here 'childC') is returned.\n\n        \"\"\"\n\n        traj = self._nn_interface._root_instance\n        storage_service = traj.v_storage_service\n\n        storage_service.load(pypetconstants.TREE, self, name,\n                             trajectory_name=traj.v_name,\n                             load_data=load_data,\n                             recursive=recursive,\n                             max_depth=max_depth)\n\n        return self.f_get(name, shortcuts=False)", "language": "python", "code": "def f_load_child(self, name, recursive=False, load_data=pypetconstants.LOAD_DATA,\n                     max_depth=None):\n        \"\"\"Loads a child or recursively a subtree from disk.\n\n        :param name:\n\n            Name of child to load. If grouped ('groupA.groupB.childC') the path along the way\n            to last node in the chain is loaded. Shortcuts are NOT allowed!\n\n        :param recursive:\n\n            Whether recursively all nodes below the last child should be loaded, too.\n            Note that links are never evaluated recursively. Only the linked node\n            will be loaded if it does not exist in the tree, yet. Any nodes or links\n            of this linked node are not loaded.\n\n        :param load_data:\n\n            Flag how to load the data.\n            For how to choose 'load_data' see :ref:`more-on-loading`.\n\n        :param max_depth:\n\n            In case `recursive` is `True`, you can specify the maximum depth to load\n            load data relative from current node. Leave `None` if you don't want to limit\n            the depth.\n\n        :returns:\n\n            The loaded child, in case of grouping ('groupA.groupB.childC') the last\n            node (here 'childC') is returned.\n\n        \"\"\"\n\n        traj = self._nn_interface._root_instance\n        storage_service = traj.v_storage_service\n\n        storage_service.load(pypetconstants.TREE, self, name,\n                             trajectory_name=traj.v_name,\n                             load_data=load_data,\n                             recursive=recursive,\n                             max_depth=max_depth)\n\n        return self.f_get(name, shortcuts=False)", "code_tokens": ["def", "f_load_child", "(", "self", ",", "name", ",", "recursive", "=", "False", ",", "load_data", "=", "pypetconstants", ".", "LOAD_DATA", ",", "max_depth", "=", "None", ")", ":", "traj", "=", "self", ".", "_nn_interface", ".", "_root_instance", "storage_service", "=", "traj", ".", "v_storage_service", "storage_service", ".", "load", "(", "pypetconstants", ".", "TREE", ",", "self", ",", "name", ",", "trajectory_name", "=", "traj", ".", "v_name", ",", "load_data", "=", "load_data", ",", "recursive", "=", "recursive", ",", "max_depth", "=", "max_depth", ")", "return", "self", ".", "f_get", "(", "name", ",", "shortcuts", "=", "False", ")"], "docstring": "Loads a child or recursively a subtree from disk.\n\n        :param name:\n\n            Name of child to load. If grouped ('groupA.groupB.childC') the path along the way\n            to last node in the chain is loaded. Shortcuts are NOT allowed!\n\n        :param recursive:\n\n            Whether recursively all nodes below the last child should be loaded, too.\n            Note that links are never evaluated recursively. Only the linked node\n            will be loaded if it does not exist in the tree, yet. Any nodes or links\n            of this linked node are not loaded.\n\n        :param load_data:\n\n            Flag how to load the data.\n            For how to choose 'load_data' see :ref:`more-on-loading`.\n\n        :param max_depth:\n\n            In case `recursive` is `True`, you can specify the maximum depth to load\n            load data relative from current node. Leave `None` if you don't want to limit\n            the depth.\n\n        :returns:\n\n            The loaded child, in case of grouping ('groupA.groupB.childC') the last\n            node (here 'childC') is returned.", "docstring_tokens": ["Loads", "a", "child", "or", "recursively", "a", "subtree", "from", "disk", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/naturalnaming.py#L3335-L3378", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/pie/objects.py", "func_name": "SecretData.validate", "original_string": "def validate(self):\n        \"\"\"\n        Verify that the contents of the SecretData object are valid.\n\n        Raises:\n            TypeError: if the types of any SecretData attributes are invalid.\n        \"\"\"\n        if not isinstance(self.value, bytes):\n            raise TypeError(\"secret value must be bytes\")\n        elif not isinstance(self.data_type, enums.SecretDataType):\n            raise TypeError(\"secret data type must be a SecretDataType \"\n                            \"enumeration\")\n\n        mask_count = len(self.cryptographic_usage_masks)\n        for i in range(mask_count):\n            mask = self.cryptographic_usage_masks[i]\n            if not isinstance(mask, enums.CryptographicUsageMask):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\n                    \"secret data mask {0} must be a CryptographicUsageMask \"\n                    \"enumeration\".format(position))\n\n        name_count = len(self.names)\n        for i in range(name_count):\n            name = self.names[i]\n            if not isinstance(name, six.string_types):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\"secret data name {0} must be a string\".format(\n                    position))", "language": "python", "code": "def validate(self):\n        \"\"\"\n        Verify that the contents of the SecretData object are valid.\n\n        Raises:\n            TypeError: if the types of any SecretData attributes are invalid.\n        \"\"\"\n        if not isinstance(self.value, bytes):\n            raise TypeError(\"secret value must be bytes\")\n        elif not isinstance(self.data_type, enums.SecretDataType):\n            raise TypeError(\"secret data type must be a SecretDataType \"\n                            \"enumeration\")\n\n        mask_count = len(self.cryptographic_usage_masks)\n        for i in range(mask_count):\n            mask = self.cryptographic_usage_masks[i]\n            if not isinstance(mask, enums.CryptographicUsageMask):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\n                    \"secret data mask {0} must be a CryptographicUsageMask \"\n                    \"enumeration\".format(position))\n\n        name_count = len(self.names)\n        for i in range(name_count):\n            name = self.names[i]\n            if not isinstance(name, six.string_types):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\"secret data name {0} must be a string\".format(\n                    position))", "code_tokens": ["def", "validate", "(", "self", ")", ":", "if", "not", "isinstance", "(", "self", ".", "value", ",", "bytes", ")", ":", "raise", "TypeError", "(", "\"secret value must be bytes\"", ")", "elif", "not", "isinstance", "(", "self", ".", "data_type", ",", "enums", ".", "SecretDataType", ")", ":", "raise", "TypeError", "(", "\"secret data type must be a SecretDataType \"", "\"enumeration\"", ")", "mask_count", "=", "len", "(", "self", ".", "cryptographic_usage_masks", ")", "for", "i", "in", "range", "(", "mask_count", ")", ":", "mask", "=", "self", ".", "cryptographic_usage_masks", "[", "i", "]", "if", "not", "isinstance", "(", "mask", ",", "enums", ".", "CryptographicUsageMask", ")", ":", "position", "=", "\"({0} in list)\"", ".", "format", "(", "i", ")", "raise", "TypeError", "(", "\"secret data mask {0} must be a CryptographicUsageMask \"", "\"enumeration\"", ".", "format", "(", "position", ")", ")", "name_count", "=", "len", "(", "self", ".", "names", ")", "for", "i", "in", "range", "(", "name_count", ")", ":", "name", "=", "self", ".", "names", "[", "i", "]", "if", "not", "isinstance", "(", "name", ",", "six", ".", "string_types", ")", ":", "position", "=", "\"({0} in list)\"", ".", "format", "(", "i", ")", "raise", "TypeError", "(", "\"secret data name {0} must be a string\"", ".", "format", "(", "position", ")", ")"], "docstring": "Verify that the contents of the SecretData object are valid.\n\n        Raises:\n            TypeError: if the types of any SecretData attributes are invalid.", "docstring_tokens": ["Verify", "that", "the", "contents", "of", "the", "SecretData", "object", "are", "valid", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/pie/objects.py#L1291-L1319", "partition": "test"}
{"repo": "uw-it-aca/uw-restclients-canvas", "path": "uw_canvas/enrollments.py", "func_name": "Enrollments.enroll_user", "original_string": "def enroll_user(self, course_id, user_id, enrollment_type, params=None):\n        \"\"\"\n        Enroll a user into a course.\n\n        https://canvas.instructure.com/doc/api/enrollments.html#method.enrollments_api.create\n        \"\"\"\n        url = COURSES_API.format(course_id) + \"/enrollments\"\n\n        if not params:\n            params = {}\n\n        params[\"user_id\"] = user_id\n        params[\"type\"] = enrollment_type\n\n        data = self._post_resource(url, {\"enrollment\": params})\n        return CanvasEnrollment(data=data)", "language": "python", "code": "def enroll_user(self, course_id, user_id, enrollment_type, params=None):\n        \"\"\"\n        Enroll a user into a course.\n\n        https://canvas.instructure.com/doc/api/enrollments.html#method.enrollments_api.create\n        \"\"\"\n        url = COURSES_API.format(course_id) + \"/enrollments\"\n\n        if not params:\n            params = {}\n\n        params[\"user_id\"] = user_id\n        params[\"type\"] = enrollment_type\n\n        data = self._post_resource(url, {\"enrollment\": params})\n        return CanvasEnrollment(data=data)", "code_tokens": ["def", "enroll_user", "(", "self", ",", "course_id", ",", "user_id", ",", "enrollment_type", ",", "params", "=", "None", ")", ":", "url", "=", "COURSES_API", ".", "format", "(", "course_id", ")", "+", "\"/enrollments\"", "if", "not", "params", ":", "params", "=", "{", "}", "params", "[", "\"user_id\"", "]", "=", "user_id", "params", "[", "\"type\"", "]", "=", "enrollment_type", "data", "=", "self", ".", "_post_resource", "(", "url", ",", "{", "\"enrollment\"", ":", "params", "}", ")", "return", "CanvasEnrollment", "(", "data", "=", "data", ")"], "docstring": "Enroll a user into a course.\n\n        https://canvas.instructure.com/doc/api/enrollments.html#method.enrollments_api.create", "docstring_tokens": ["Enroll", "a", "user", "into", "a", "course", "."], "sha": "9845faf33d49a8f06908efc22640c001116d6ea2", "url": "https://github.com/uw-it-aca/uw-restclients-canvas/blob/9845faf33d49a8f06908efc22640c001116d6ea2/uw_canvas/enrollments.py#L85-L100", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/core/spectrum.py", "func_name": "_spectrogram", "original_string": "def _spectrogram(y=None, S=None, n_fft=2048, hop_length=512, power=1,\n                 win_length=None, window='hann', center=True, pad_mode='reflect'):\n    '''Helper function to retrieve a magnitude spectrogram.\n\n    This is primarily used in feature extraction functions that can operate on\n    either audio time-series or spectrogram input.\n\n\n    Parameters\n    ----------\n    y : None or np.ndarray [ndim=1]\n        If provided, an audio time series\n\n    S : None or np.ndarray\n        Spectrogram input, optional\n\n    n_fft : int > 0\n        STFT window size\n\n    hop_length : int > 0\n        STFT hop length\n\n    power : float > 0\n        Exponent for the magnitude spectrogram,\n        e.g., 1 for energy, 2 for power, etc.\n\n    win_length : int <= n_fft [scalar]\n        Each frame of audio is windowed by `window()`.\n        The window will be of length `win_length` and then padded\n        with zeros to match `n_fft`.\n\n        If unspecified, defaults to ``win_length = n_fft``.\n\n    window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n        - a window specification (string, tuple, or number);\n          see `scipy.signal.get_window`\n        - a window function, such as `scipy.signal.hanning`\n        - a vector or array of length `n_fft`\n\n        .. see also:: `filters.get_window`\n\n    center : boolean\n        - If `True`, the signal `y` is padded so that frame\n          `t` is centered at `y[t * hop_length]`.\n        - If `False`, then frame `t` begins at `y[t * hop_length]`\n\n    pad_mode : string\n        If `center=True`, the padding mode to use at the edges of the signal.\n        By default, STFT uses reflection padding.\n\n\n    Returns\n    -------\n    S_out : np.ndarray [dtype=np.float32]\n        - If `S` is provided as input, then `S_out == S`\n        - Else, `S_out = |stft(y, ...)|**power`\n\n    n_fft : int > 0\n        - If `S` is provided, then `n_fft` is inferred from `S`\n        - Else, copied from input\n    '''\n\n    if S is not None:\n        # Infer n_fft from spectrogram shape\n        n_fft = 2 * (S.shape[0] - 1)\n    else:\n        # Otherwise, compute a magnitude spectrogram from input\n        S = np.abs(stft(y, n_fft=n_fft, hop_length=hop_length,\n                        win_length=win_length, center=center,\n                        window=window, pad_mode=pad_mode))**power\n\n    return S, n_fft", "language": "python", "code": "def _spectrogram(y=None, S=None, n_fft=2048, hop_length=512, power=1,\n                 win_length=None, window='hann', center=True, pad_mode='reflect'):\n    '''Helper function to retrieve a magnitude spectrogram.\n\n    This is primarily used in feature extraction functions that can operate on\n    either audio time-series or spectrogram input.\n\n\n    Parameters\n    ----------\n    y : None or np.ndarray [ndim=1]\n        If provided, an audio time series\n\n    S : None or np.ndarray\n        Spectrogram input, optional\n\n    n_fft : int > 0\n        STFT window size\n\n    hop_length : int > 0\n        STFT hop length\n\n    power : float > 0\n        Exponent for the magnitude spectrogram,\n        e.g., 1 for energy, 2 for power, etc.\n\n    win_length : int <= n_fft [scalar]\n        Each frame of audio is windowed by `window()`.\n        The window will be of length `win_length` and then padded\n        with zeros to match `n_fft`.\n\n        If unspecified, defaults to ``win_length = n_fft``.\n\n    window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n        - a window specification (string, tuple, or number);\n          see `scipy.signal.get_window`\n        - a window function, such as `scipy.signal.hanning`\n        - a vector or array of length `n_fft`\n\n        .. see also:: `filters.get_window`\n\n    center : boolean\n        - If `True`, the signal `y` is padded so that frame\n          `t` is centered at `y[t * hop_length]`.\n        - If `False`, then frame `t` begins at `y[t * hop_length]`\n\n    pad_mode : string\n        If `center=True`, the padding mode to use at the edges of the signal.\n        By default, STFT uses reflection padding.\n\n\n    Returns\n    -------\n    S_out : np.ndarray [dtype=np.float32]\n        - If `S` is provided as input, then `S_out == S`\n        - Else, `S_out = |stft(y, ...)|**power`\n\n    n_fft : int > 0\n        - If `S` is provided, then `n_fft` is inferred from `S`\n        - Else, copied from input\n    '''\n\n    if S is not None:\n        # Infer n_fft from spectrogram shape\n        n_fft = 2 * (S.shape[0] - 1)\n    else:\n        # Otherwise, compute a magnitude spectrogram from input\n        S = np.abs(stft(y, n_fft=n_fft, hop_length=hop_length,\n                        win_length=win_length, center=center,\n                        window=window, pad_mode=pad_mode))**power\n\n    return S, n_fft", "code_tokens": ["def", "_spectrogram", "(", "y", "=", "None", ",", "S", "=", "None", ",", "n_fft", "=", "2048", ",", "hop_length", "=", "512", ",", "power", "=", "1", ",", "win_length", "=", "None", ",", "window", "=", "'hann'", ",", "center", "=", "True", ",", "pad_mode", "=", "'reflect'", ")", ":", "if", "S", "is", "not", "None", ":", "# Infer n_fft from spectrogram shape", "n_fft", "=", "2", "*", "(", "S", ".", "shape", "[", "0", "]", "-", "1", ")", "else", ":", "# Otherwise, compute a magnitude spectrogram from input", "S", "=", "np", ".", "abs", "(", "stft", "(", "y", ",", "n_fft", "=", "n_fft", ",", "hop_length", "=", "hop_length", ",", "win_length", "=", "win_length", ",", "center", "=", "center", ",", "window", "=", "window", ",", "pad_mode", "=", "pad_mode", ")", ")", "**", "power", "return", "S", ",", "n_fft"], "docstring": "Helper function to retrieve a magnitude spectrogram.\n\n    This is primarily used in feature extraction functions that can operate on\n    either audio time-series or spectrogram input.\n\n\n    Parameters\n    ----------\n    y : None or np.ndarray [ndim=1]\n        If provided, an audio time series\n\n    S : None or np.ndarray\n        Spectrogram input, optional\n\n    n_fft : int > 0\n        STFT window size\n\n    hop_length : int > 0\n        STFT hop length\n\n    power : float > 0\n        Exponent for the magnitude spectrogram,\n        e.g., 1 for energy, 2 for power, etc.\n\n    win_length : int <= n_fft [scalar]\n        Each frame of audio is windowed by `window()`.\n        The window will be of length `win_length` and then padded\n        with zeros to match `n_fft`.\n\n        If unspecified, defaults to ``win_length = n_fft``.\n\n    window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n        - a window specification (string, tuple, or number);\n          see `scipy.signal.get_window`\n        - a window function, such as `scipy.signal.hanning`\n        - a vector or array of length `n_fft`\n\n        .. see also:: `filters.get_window`\n\n    center : boolean\n        - If `True`, the signal `y` is padded so that frame\n          `t` is centered at `y[t * hop_length]`.\n        - If `False`, then frame `t` begins at `y[t * hop_length]`\n\n    pad_mode : string\n        If `center=True`, the padding mode to use at the edges of the signal.\n        By default, STFT uses reflection padding.\n\n\n    Returns\n    -------\n    S_out : np.ndarray [dtype=np.float32]\n        - If `S` is provided as input, then `S_out == S`\n        - Else, `S_out = |stft(y, ...)|**power`\n\n    n_fft : int > 0\n        - If `S` is provided, then `n_fft` is inferred from `S`\n        - Else, copied from input", "docstring_tokens": ["Helper", "function", "to", "retrieve", "a", "magnitude", "spectrogram", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/core/spectrum.py#L1565-L1636", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/monte_carlo.py", "func_name": "expectation_importance_sampler_logspace", "original_string": "def expectation_importance_sampler_logspace(\n    log_f,\n    log_p,\n    sampling_dist_q,\n    z=None,\n    n=None,\n    seed=None,\n    name='expectation_importance_sampler_logspace'):\n  r\"\"\"Importance sampling with a positive function, in log-space.\n\n  With \\\\(p(z) := exp^{log_p(z)}\\\\), and \\\\(f(z) = exp{log_f(z)}\\\\),\n  this `Op` returns\n\n  \\\\(Log[ n^{-1} sum_{i=1}^n [ f(z_i) p(z_i) / q(z_i) ] ],  z_i ~ q,\\\\)\n  \\\\(\\approx Log[ E_q[ f(Z) p(Z) / q(Z) ] ]\\\\)\n  \\\\(=       Log[E_p[f(Z)]]\\\\)\n\n  This integral is done in log-space with max-subtraction to better handle the\n  often extreme values that `f(z) p(z) / q(z)` can take on.\n\n  In contrast to `expectation_importance_sampler`, this `Op` returns values in\n  log-space.\n\n\n  User supplies either `Tensor` of samples `z`, or number of samples to draw `n`\n\n  Args:\n    log_f: Callable mapping samples from `sampling_dist_q` to `Tensors` with\n      shape broadcastable to `q.batch_shape`.\n      For example, `log_f` works \"just like\" `sampling_dist_q.log_prob`.\n    log_p:  Callable mapping samples from `sampling_dist_q` to `Tensors` with\n      shape broadcastable to `q.batch_shape`.\n      For example, `log_p` works \"just like\" `q.log_prob`.\n    sampling_dist_q:  The sampling distribution.\n      `tfp.distributions.Distribution`.\n      `float64` `dtype` recommended.\n      `log_p` and `q` should be supported on the same set.\n    z:  `Tensor` of samples from `q`, produced by `q.sample` for some `n`.\n    n:  Integer `Tensor`.  Number of samples to generate if `z` is not provided.\n    seed:  Python integer to seed the random number generator.\n    name:  A name to give this `Op`.\n\n  Returns:\n    Logarithm of the importance sampling estimate.  `Tensor` with `shape` equal\n      to batch shape of `q`, and `dtype` = `q.dtype`.\n  \"\"\"\n  q = sampling_dist_q\n  with tf.name_scope(name):\n    z = _get_samples(q, z, n, seed)\n    log_values = log_f(z) + log_p(z) - q.log_prob(z)\n    return _logspace_mean(log_values)", "language": "python", "code": "def expectation_importance_sampler_logspace(\n    log_f,\n    log_p,\n    sampling_dist_q,\n    z=None,\n    n=None,\n    seed=None,\n    name='expectation_importance_sampler_logspace'):\n  r\"\"\"Importance sampling with a positive function, in log-space.\n\n  With \\\\(p(z) := exp^{log_p(z)}\\\\), and \\\\(f(z) = exp{log_f(z)}\\\\),\n  this `Op` returns\n\n  \\\\(Log[ n^{-1} sum_{i=1}^n [ f(z_i) p(z_i) / q(z_i) ] ],  z_i ~ q,\\\\)\n  \\\\(\\approx Log[ E_q[ f(Z) p(Z) / q(Z) ] ]\\\\)\n  \\\\(=       Log[E_p[f(Z)]]\\\\)\n\n  This integral is done in log-space with max-subtraction to better handle the\n  often extreme values that `f(z) p(z) / q(z)` can take on.\n\n  In contrast to `expectation_importance_sampler`, this `Op` returns values in\n  log-space.\n\n\n  User supplies either `Tensor` of samples `z`, or number of samples to draw `n`\n\n  Args:\n    log_f: Callable mapping samples from `sampling_dist_q` to `Tensors` with\n      shape broadcastable to `q.batch_shape`.\n      For example, `log_f` works \"just like\" `sampling_dist_q.log_prob`.\n    log_p:  Callable mapping samples from `sampling_dist_q` to `Tensors` with\n      shape broadcastable to `q.batch_shape`.\n      For example, `log_p` works \"just like\" `q.log_prob`.\n    sampling_dist_q:  The sampling distribution.\n      `tfp.distributions.Distribution`.\n      `float64` `dtype` recommended.\n      `log_p` and `q` should be supported on the same set.\n    z:  `Tensor` of samples from `q`, produced by `q.sample` for some `n`.\n    n:  Integer `Tensor`.  Number of samples to generate if `z` is not provided.\n    seed:  Python integer to seed the random number generator.\n    name:  A name to give this `Op`.\n\n  Returns:\n    Logarithm of the importance sampling estimate.  `Tensor` with `shape` equal\n      to batch shape of `q`, and `dtype` = `q.dtype`.\n  \"\"\"\n  q = sampling_dist_q\n  with tf.name_scope(name):\n    z = _get_samples(q, z, n, seed)\n    log_values = log_f(z) + log_p(z) - q.log_prob(z)\n    return _logspace_mean(log_values)", "code_tokens": ["def", "expectation_importance_sampler_logspace", "(", "log_f", ",", "log_p", ",", "sampling_dist_q", ",", "z", "=", "None", ",", "n", "=", "None", ",", "seed", "=", "None", ",", "name", "=", "'expectation_importance_sampler_logspace'", ")", ":", "q", "=", "sampling_dist_q", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "z", "=", "_get_samples", "(", "q", ",", "z", ",", "n", ",", "seed", ")", "log_values", "=", "log_f", "(", "z", ")", "+", "log_p", "(", "z", ")", "-", "q", ".", "log_prob", "(", "z", ")", "return", "_logspace_mean", "(", "log_values", ")"], "docstring": "r\"\"\"Importance sampling with a positive function, in log-space.\n\n  With \\\\(p(z) := exp^{log_p(z)}\\\\), and \\\\(f(z) = exp{log_f(z)}\\\\),\n  this `Op` returns\n\n  \\\\(Log[ n^{-1} sum_{i=1}^n [ f(z_i) p(z_i) / q(z_i) ] ],  z_i ~ q,\\\\)\n  \\\\(\\approx Log[ E_q[ f(Z) p(Z) / q(Z) ] ]\\\\)\n  \\\\(=       Log[E_p[f(Z)]]\\\\)\n\n  This integral is done in log-space with max-subtraction to better handle the\n  often extreme values that `f(z) p(z) / q(z)` can take on.\n\n  In contrast to `expectation_importance_sampler`, this `Op` returns values in\n  log-space.\n\n\n  User supplies either `Tensor` of samples `z`, or number of samples to draw `n`\n\n  Args:\n    log_f: Callable mapping samples from `sampling_dist_q` to `Tensors` with\n      shape broadcastable to `q.batch_shape`.\n      For example, `log_f` works \"just like\" `sampling_dist_q.log_prob`.\n    log_p:  Callable mapping samples from `sampling_dist_q` to `Tensors` with\n      shape broadcastable to `q.batch_shape`.\n      For example, `log_p` works \"just like\" `q.log_prob`.\n    sampling_dist_q:  The sampling distribution.\n      `tfp.distributions.Distribution`.\n      `float64` `dtype` recommended.\n      `log_p` and `q` should be supported on the same set.\n    z:  `Tensor` of samples from `q`, produced by `q.sample` for some `n`.\n    n:  Integer `Tensor`.  Number of samples to generate if `z` is not provided.\n    seed:  Python integer to seed the random number generator.\n    name:  A name to give this `Op`.\n\n  Returns:\n    Logarithm of the importance sampling estimate.  `Tensor` with `shape` equal\n      to batch shape of `q`, and `dtype` = `q.dtype`.", "docstring_tokens": ["r", "Importance", "sampling", "with", "a", "positive", "function", "in", "log", "-", "space", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/monte_carlo.py#L102-L152", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/pyreverse/diagrams.py", "func_name": "ClassDiagram.get_relationship", "original_string": "def get_relationship(self, from_object, relation_type):\n        \"\"\"return a relation ship or None\n        \"\"\"\n        for rel in self.relationships.get(relation_type, ()):\n            if rel.from_object is from_object:\n                return rel\n        raise KeyError(relation_type)", "language": "python", "code": "def get_relationship(self, from_object, relation_type):\n        \"\"\"return a relation ship or None\n        \"\"\"\n        for rel in self.relationships.get(relation_type, ()):\n            if rel.from_object is from_object:\n                return rel\n        raise KeyError(relation_type)", "code_tokens": ["def", "get_relationship", "(", "self", ",", "from_object", ",", "relation_type", ")", ":", "for", "rel", "in", "self", ".", "relationships", ".", "get", "(", "relation_type", ",", "(", ")", ")", ":", "if", "rel", ".", "from_object", "is", "from_object", ":", "return", "rel", "raise", "KeyError", "(", "relation_type", ")"], "docstring": "return a relation ship or None", "docstring_tokens": ["return", "a", "relation", "ship", "or", "None"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/pyreverse/diagrams.py#L74-L80", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/spark_submit_hook.py", "func_name": "SparkSubmitHook._process_spark_status_log", "original_string": "def _process_spark_status_log(self, itr):\n        \"\"\"\n        parses the logs of the spark driver status query process\n\n        :param itr: An iterator which iterates over the input of the subprocess\n        \"\"\"\n        # Consume the iterator\n        for line in itr:\n            line = line.strip()\n\n            # Check if the log line is about the driver status and extract the status.\n            if \"driverState\" in line:\n                self._driver_status = line.split(' : ')[1] \\\n                    .replace(',', '').replace('\\\"', '').strip()\n\n            self.log.debug(\"spark driver status log: {}\".format(line))", "language": "python", "code": "def _process_spark_status_log(self, itr):\n        \"\"\"\n        parses the logs of the spark driver status query process\n\n        :param itr: An iterator which iterates over the input of the subprocess\n        \"\"\"\n        # Consume the iterator\n        for line in itr:\n            line = line.strip()\n\n            # Check if the log line is about the driver status and extract the status.\n            if \"driverState\" in line:\n                self._driver_status = line.split(' : ')[1] \\\n                    .replace(',', '').replace('\\\"', '').strip()\n\n            self.log.debug(\"spark driver status log: {}\".format(line))", "code_tokens": ["def", "_process_spark_status_log", "(", "self", ",", "itr", ")", ":", "# Consume the iterator", "for", "line", "in", "itr", ":", "line", "=", "line", ".", "strip", "(", ")", "# Check if the log line is about the driver status and extract the status.", "if", "\"driverState\"", "in", "line", ":", "self", ".", "_driver_status", "=", "line", ".", "split", "(", "' : '", ")", "[", "1", "]", ".", "replace", "(", "','", ",", "''", ")", ".", "replace", "(", "'\\\"'", ",", "''", ")", ".", "strip", "(", ")", "self", ".", "log", ".", "debug", "(", "\"spark driver status log: {}\"", ".", "format", "(", "line", ")", ")"], "docstring": "parses the logs of the spark driver status query process\n\n        :param itr: An iterator which iterates over the input of the subprocess", "docstring_tokens": ["parses", "the", "logs", "of", "the", "spark", "driver", "status", "query", "process"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/spark_submit_hook.py#L431-L446", "partition": "test"}
{"repo": "authomatic/authomatic", "path": "authomatic/providers/__init__.py", "func_name": "BaseProvider.csrf_generator", "original_string": "def csrf_generator(secret):\n        \"\"\"\n        Generates CSRF token.\n\n        Inspired by this article:\n        http://blog.ptsecurity.com/2012/10/random-number-security-in-python.html\n\n        :returns:\n            :class:`str` Random unguessable string.\n\n        \"\"\"\n\n        # Create hash from random string plus salt.\n        hashed = hashlib.md5(uuid.uuid4().bytes + six.b(secret)).hexdigest()\n\n        # Each time return random portion of the hash.\n        span = 5\n        shift = random.randint(0, span)\n        return hashed[shift:shift - span - 1]", "language": "python", "code": "def csrf_generator(secret):\n        \"\"\"\n        Generates CSRF token.\n\n        Inspired by this article:\n        http://blog.ptsecurity.com/2012/10/random-number-security-in-python.html\n\n        :returns:\n            :class:`str` Random unguessable string.\n\n        \"\"\"\n\n        # Create hash from random string plus salt.\n        hashed = hashlib.md5(uuid.uuid4().bytes + six.b(secret)).hexdigest()\n\n        # Each time return random portion of the hash.\n        span = 5\n        shift = random.randint(0, span)\n        return hashed[shift:shift - span - 1]", "code_tokens": ["def", "csrf_generator", "(", "secret", ")", ":", "# Create hash from random string plus salt.", "hashed", "=", "hashlib", ".", "md5", "(", "uuid", ".", "uuid4", "(", ")", ".", "bytes", "+", "six", ".", "b", "(", "secret", ")", ")", ".", "hexdigest", "(", ")", "# Each time return random portion of the hash.", "span", "=", "5", "shift", "=", "random", ".", "randint", "(", "0", ",", "span", ")", "return", "hashed", "[", "shift", ":", "shift", "-", "span", "-", "1", "]"], "docstring": "Generates CSRF token.\n\n        Inspired by this article:\n        http://blog.ptsecurity.com/2012/10/random-number-security-in-python.html\n\n        :returns:\n            :class:`str` Random unguessable string.", "docstring_tokens": ["Generates", "CSRF", "token", "."], "sha": "90a9ce60cc405ae8a2bf5c3713acd5d78579a04e", "url": "https://github.com/authomatic/authomatic/blob/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e/authomatic/providers/__init__.py#L315-L333", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/sensors/hdfs_sensor.py", "func_name": "HdfsSensorRegex.poke", "original_string": "def poke(self, context):\n        \"\"\"\n        poke matching files in a directory with self.regex\n\n        :return: Bool depending on the search criteria\n        \"\"\"\n        sb = self.hook(self.hdfs_conn_id).get_conn()\n        self.log.info(\n            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern\n        )\n        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if\n                  f['file_type'] == 'f' and\n                  self.regex.match(f['path'].replace('%s/' % self.filepath, ''))]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        return bool(result)", "language": "python", "code": "def poke(self, context):\n        \"\"\"\n        poke matching files in a directory with self.regex\n\n        :return: Bool depending on the search criteria\n        \"\"\"\n        sb = self.hook(self.hdfs_conn_id).get_conn()\n        self.log.info(\n            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern\n        )\n        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if\n                  f['file_type'] == 'f' and\n                  self.regex.match(f['path'].replace('%s/' % self.filepath, ''))]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        return bool(result)", "code_tokens": ["def", "poke", "(", "self", ",", "context", ")", ":", "sb", "=", "self", ".", "hook", "(", "self", ".", "hdfs_conn_id", ")", ".", "get_conn", "(", ")", "self", ".", "log", ".", "info", "(", "'Poking for %s to be a directory with files matching %s'", ",", "self", ".", "filepath", ",", "self", ".", "regex", ".", "pattern", ")", "result", "=", "[", "f", "for", "f", "in", "sb", ".", "ls", "(", "[", "self", ".", "filepath", "]", ",", "include_toplevel", "=", "False", ")", "if", "f", "[", "'file_type'", "]", "==", "'f'", "and", "self", ".", "regex", ".", "match", "(", "f", "[", "'path'", "]", ".", "replace", "(", "'%s/'", "%", "self", ".", "filepath", ",", "''", ")", ")", "]", "result", "=", "self", ".", "filter_for_ignored_ext", "(", "result", ",", "self", ".", "ignored_ext", ",", "self", ".", "ignore_copying", ")", "result", "=", "self", ".", "filter_for_filesize", "(", "result", ",", "self", ".", "file_size", ")", "return", "bool", "(", "result", ")"], "docstring": "poke matching files in a directory with self.regex\n\n        :return: Bool depending on the search criteria", "docstring_tokens": ["poke", "matching", "files", "in", "a", "directory", "with", "self", ".", "regex"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/hdfs_sensor.py#L30-L46", "partition": "test"}
{"repo": "neherlab/treetime", "path": "treetime/treeregression.py", "func_name": "TreeRegression._calculate_averages", "original_string": "def _calculate_averages(self):\n        \"\"\"\n        calculate the weighted sums of the tip and branch values and\n        their second moments.\n        \"\"\"\n        for n in self.tree.get_nonterminals(order='postorder'):\n            Q = np.zeros(6, dtype=float)\n            for c in n:\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                Q += self.propagate_averages(c, tv, bv, var)\n            n.Q=Q\n\n        for n in self.tree.find_clades(order='preorder'):\n            O = np.zeros(6, dtype=float)\n            if n==self.tree.root:\n                n.Qtot = n.Q\n                continue\n\n            for c in n.up:\n                if c==n:\n                    continue\n\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                O += self.propagate_averages(c, tv, bv, var)\n\n            if n.up!=self.tree.root:\n                c = n.up\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                O += self.propagate_averages(c, tv, bv, var, outgroup=True)\n            n.O = O\n\n            if not n.is_terminal():\n                tv = self.tip_value(n)\n                bv = self.branch_value(n)\n                var = self.branch_variance(n)\n                n.Qtot = n.Q + self.propagate_averages(n, tv, bv, var, outgroup=True)", "language": "python", "code": "def _calculate_averages(self):\n        \"\"\"\n        calculate the weighted sums of the tip and branch values and\n        their second moments.\n        \"\"\"\n        for n in self.tree.get_nonterminals(order='postorder'):\n            Q = np.zeros(6, dtype=float)\n            for c in n:\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                Q += self.propagate_averages(c, tv, bv, var)\n            n.Q=Q\n\n        for n in self.tree.find_clades(order='preorder'):\n            O = np.zeros(6, dtype=float)\n            if n==self.tree.root:\n                n.Qtot = n.Q\n                continue\n\n            for c in n.up:\n                if c==n:\n                    continue\n\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                O += self.propagate_averages(c, tv, bv, var)\n\n            if n.up!=self.tree.root:\n                c = n.up\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                O += self.propagate_averages(c, tv, bv, var, outgroup=True)\n            n.O = O\n\n            if not n.is_terminal():\n                tv = self.tip_value(n)\n                bv = self.branch_value(n)\n                var = self.branch_variance(n)\n                n.Qtot = n.Q + self.propagate_averages(n, tv, bv, var, outgroup=True)", "code_tokens": ["def", "_calculate_averages", "(", "self", ")", ":", "for", "n", "in", "self", ".", "tree", ".", "get_nonterminals", "(", "order", "=", "'postorder'", ")", ":", "Q", "=", "np", ".", "zeros", "(", "6", ",", "dtype", "=", "float", ")", "for", "c", "in", "n", ":", "tv", "=", "self", ".", "tip_value", "(", "c", ")", "bv", "=", "self", ".", "branch_value", "(", "c", ")", "var", "=", "self", ".", "branch_variance", "(", "c", ")", "Q", "+=", "self", ".", "propagate_averages", "(", "c", ",", "tv", ",", "bv", ",", "var", ")", "n", ".", "Q", "=", "Q", "for", "n", "in", "self", ".", "tree", ".", "find_clades", "(", "order", "=", "'preorder'", ")", ":", "O", "=", "np", ".", "zeros", "(", "6", ",", "dtype", "=", "float", ")", "if", "n", "==", "self", ".", "tree", ".", "root", ":", "n", ".", "Qtot", "=", "n", ".", "Q", "continue", "for", "c", "in", "n", ".", "up", ":", "if", "c", "==", "n", ":", "continue", "tv", "=", "self", ".", "tip_value", "(", "c", ")", "bv", "=", "self", ".", "branch_value", "(", "c", ")", "var", "=", "self", ".", "branch_variance", "(", "c", ")", "O", "+=", "self", ".", "propagate_averages", "(", "c", ",", "tv", ",", "bv", ",", "var", ")", "if", "n", ".", "up", "!=", "self", ".", "tree", ".", "root", ":", "c", "=", "n", ".", "up", "tv", "=", "self", ".", "tip_value", "(", "c", ")", "bv", "=", "self", ".", "branch_value", "(", "c", ")", "var", "=", "self", ".", "branch_variance", "(", "c", ")", "O", "+=", "self", ".", "propagate_averages", "(", "c", ",", "tv", ",", "bv", ",", "var", ",", "outgroup", "=", "True", ")", "n", ".", "O", "=", "O", "if", "not", "n", ".", "is_terminal", "(", ")", ":", "tv", "=", "self", ".", "tip_value", "(", "n", ")", "bv", "=", "self", ".", "branch_value", "(", "n", ")", "var", "=", "self", ".", "branch_variance", "(", "n", ")", "n", ".", "Qtot", "=", "n", ".", "Q", "+", "self", ".", "propagate_averages", "(", "n", ",", "tv", ",", "bv", ",", "var", ",", "outgroup", "=", "True", ")"], "docstring": "calculate the weighted sums of the tip and branch values and\n        their second moments.", "docstring_tokens": ["calculate", "the", "weighted", "sums", "of", "the", "tip", "and", "branch", "values", "and", "their", "second", "moments", "."], "sha": "f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0", "url": "https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/treeregression.py#L179-L220", "partition": "test"}
{"repo": "zomux/deepy", "path": "deepy/dataset/sequence.py", "func_name": "SequentialDataset._pad", "original_string": "def _pad(self, side, length):\n        \"\"\"\n        Pad sequences to given length in the left or right side.\n        \"\"\"\n        if self._train_set:\n            self._train_set = pad_dataset(self._train_set, side, length)\n        if self._valid_set:\n            self._valid_set = pad_dataset(self._valid_set, side, length)\n        if self._test_set:\n            self._test_set = pad_dataset(self._test_set, side, length)", "language": "python", "code": "def _pad(self, side, length):\n        \"\"\"\n        Pad sequences to given length in the left or right side.\n        \"\"\"\n        if self._train_set:\n            self._train_set = pad_dataset(self._train_set, side, length)\n        if self._valid_set:\n            self._valid_set = pad_dataset(self._valid_set, side, length)\n        if self._test_set:\n            self._test_set = pad_dataset(self._test_set, side, length)", "code_tokens": ["def", "_pad", "(", "self", ",", "side", ",", "length", ")", ":", "if", "self", ".", "_train_set", ":", "self", ".", "_train_set", "=", "pad_dataset", "(", "self", ".", "_train_set", ",", "side", ",", "length", ")", "if", "self", ".", "_valid_set", ":", "self", ".", "_valid_set", "=", "pad_dataset", "(", "self", ".", "_valid_set", ",", "side", ",", "length", ")", "if", "self", ".", "_test_set", ":", "self", ".", "_test_set", "=", "pad_dataset", "(", "self", ".", "_test_set", ",", "side", ",", "length", ")"], "docstring": "Pad sequences to given length in the left or right side.", "docstring_tokens": ["Pad", "sequences", "to", "given", "length", "in", "the", "left", "or", "right", "side", "."], "sha": "090fbad22a08a809b12951cd0d4984f5bd432698", "url": "https://github.com/zomux/deepy/blob/090fbad22a08a809b12951cd0d4984f5bd432698/deepy/dataset/sequence.py#L15-L24", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/linalg.py", "func_name": "matrix_rank", "original_string": "def matrix_rank(a, tol=None, validate_args=False, name=None):\n  \"\"\"Compute the matrix rank; the number of non-zero SVD singular values.\n\n  Arguments:\n    a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be\n      pseudo-inverted.\n    tol: Threshold below which the singular value is counted as \"zero\".\n      Default value: `None` (i.e., `eps * max(rows, cols) * max(singular_val)`).\n    validate_args: When `True`, additional assertions might be embedded in the\n      graph.\n      Default value: `False` (i.e., no graph assertions are added).\n    name: Python `str` prefixed to ops created by this function.\n      Default value: \"matrix_rank\".\n\n  Returns:\n    matrix_rank: (Batch of) `int32` scalars representing the number of non-zero\n      singular values.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'matrix_rank', [a, tol]):\n    a = tf.convert_to_tensor(value=a, dtype_hint=tf.float32, name='a')\n    assertions = _maybe_validate_matrix(a, validate_args)\n    if assertions:\n      with tf.control_dependencies(assertions):\n        a = tf.identity(a)\n    s = tf.linalg.svd(a, compute_uv=False)\n    if tol is None:\n      if a.shape[-2:].is_fully_defined():\n        m = np.max(a.shape[-2:].as_list())\n      else:\n        m = tf.reduce_max(input_tensor=tf.shape(input=a)[-2:])\n      eps = np.finfo(a.dtype.as_numpy_dtype).eps\n      tol = (eps * tf.cast(m, a.dtype) *\n             tf.reduce_max(input_tensor=s, axis=-1, keepdims=True))\n    return tf.reduce_sum(input_tensor=tf.cast(s > tol, tf.int32), axis=-1)", "language": "python", "code": "def matrix_rank(a, tol=None, validate_args=False, name=None):\n  \"\"\"Compute the matrix rank; the number of non-zero SVD singular values.\n\n  Arguments:\n    a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be\n      pseudo-inverted.\n    tol: Threshold below which the singular value is counted as \"zero\".\n      Default value: `None` (i.e., `eps * max(rows, cols) * max(singular_val)`).\n    validate_args: When `True`, additional assertions might be embedded in the\n      graph.\n      Default value: `False` (i.e., no graph assertions are added).\n    name: Python `str` prefixed to ops created by this function.\n      Default value: \"matrix_rank\".\n\n  Returns:\n    matrix_rank: (Batch of) `int32` scalars representing the number of non-zero\n      singular values.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'matrix_rank', [a, tol]):\n    a = tf.convert_to_tensor(value=a, dtype_hint=tf.float32, name='a')\n    assertions = _maybe_validate_matrix(a, validate_args)\n    if assertions:\n      with tf.control_dependencies(assertions):\n        a = tf.identity(a)\n    s = tf.linalg.svd(a, compute_uv=False)\n    if tol is None:\n      if a.shape[-2:].is_fully_defined():\n        m = np.max(a.shape[-2:].as_list())\n      else:\n        m = tf.reduce_max(input_tensor=tf.shape(input=a)[-2:])\n      eps = np.finfo(a.dtype.as_numpy_dtype).eps\n      tol = (eps * tf.cast(m, a.dtype) *\n             tf.reduce_max(input_tensor=s, axis=-1, keepdims=True))\n    return tf.reduce_sum(input_tensor=tf.cast(s > tol, tf.int32), axis=-1)", "code_tokens": ["def", "matrix_rank", "(", "a", ",", "tol", "=", "None", ",", "validate_args", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'matrix_rank'", ",", "[", "a", ",", "tol", "]", ")", ":", "a", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "a", ",", "dtype_hint", "=", "tf", ".", "float32", ",", "name", "=", "'a'", ")", "assertions", "=", "_maybe_validate_matrix", "(", "a", ",", "validate_args", ")", "if", "assertions", ":", "with", "tf", ".", "control_dependencies", "(", "assertions", ")", ":", "a", "=", "tf", ".", "identity", "(", "a", ")", "s", "=", "tf", ".", "linalg", ".", "svd", "(", "a", ",", "compute_uv", "=", "False", ")", "if", "tol", "is", "None", ":", "if", "a", ".", "shape", "[", "-", "2", ":", "]", ".", "is_fully_defined", "(", ")", ":", "m", "=", "np", ".", "max", "(", "a", ".", "shape", "[", "-", "2", ":", "]", ".", "as_list", "(", ")", ")", "else", ":", "m", "=", "tf", ".", "reduce_max", "(", "input_tensor", "=", "tf", ".", "shape", "(", "input", "=", "a", ")", "[", "-", "2", ":", "]", ")", "eps", "=", "np", ".", "finfo", "(", "a", ".", "dtype", ".", "as_numpy_dtype", ")", ".", "eps", "tol", "=", "(", "eps", "*", "tf", ".", "cast", "(", "m", ",", "a", ".", "dtype", ")", "*", "tf", ".", "reduce_max", "(", "input_tensor", "=", "s", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", ")", "return", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "tf", ".", "cast", "(", "s", ">", "tol", ",", "tf", ".", "int32", ")", ",", "axis", "=", "-", "1", ")"], "docstring": "Compute the matrix rank; the number of non-zero SVD singular values.\n\n  Arguments:\n    a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be\n      pseudo-inverted.\n    tol: Threshold below which the singular value is counted as \"zero\".\n      Default value: `None` (i.e., `eps * max(rows, cols) * max(singular_val)`).\n    validate_args: When `True`, additional assertions might be embedded in the\n      graph.\n      Default value: `False` (i.e., no graph assertions are added).\n    name: Python `str` prefixed to ops created by this function.\n      Default value: \"matrix_rank\".\n\n  Returns:\n    matrix_rank: (Batch of) `int32` scalars representing the number of non-zero\n      singular values.", "docstring_tokens": ["Compute", "the", "matrix", "rank", ";", "the", "number", "of", "non", "-", "zero", "SVD", "singular", "values", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/linalg.py#L48-L81", "partition": "test"}
{"repo": "authomatic/authomatic", "path": "authomatic/core.py", "func_name": "Credentials.valid", "original_string": "def valid(self):\n        \"\"\"\n        ``True`` if credentials are valid, ``False`` if expired.\n        \"\"\"\n\n        if self.expiration_time:\n            return self.expiration_time > int(time.time())\n        else:\n            return True", "language": "python", "code": "def valid(self):\n        \"\"\"\n        ``True`` if credentials are valid, ``False`` if expired.\n        \"\"\"\n\n        if self.expiration_time:\n            return self.expiration_time > int(time.time())\n        else:\n            return True", "code_tokens": ["def", "valid", "(", "self", ")", ":", "if", "self", ".", "expiration_time", ":", "return", "self", ".", "expiration_time", ">", "int", "(", "time", ".", "time", "(", ")", ")", "else", ":", "return", "True"], "docstring": "``True`` if credentials are valid, ``False`` if expired.", "docstring_tokens": ["True", "if", "credentials", "are", "valid", "False", "if", "expired", "."], "sha": "90a9ce60cc405ae8a2bf5c3713acd5d78579a04e", "url": "https://github.com/authomatic/authomatic/blob/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e/authomatic/core.py#L761-L769", "partition": "test"}
{"repo": "djangobot/djangobot", "path": "djangobot/client.py", "func_name": "SlackClientProtocol.sendSlack", "original_string": "def sendSlack(self, message):\n        \"\"\"\n        Send message to Slack\n        \"\"\"\n        channel = message.get('channel', 'general')\n        self.sendMessage(self.make_message(message['text'], channel))", "language": "python", "code": "def sendSlack(self, message):\n        \"\"\"\n        Send message to Slack\n        \"\"\"\n        channel = message.get('channel', 'general')\n        self.sendMessage(self.make_message(message['text'], channel))", "code_tokens": ["def", "sendSlack", "(", "self", ",", "message", ")", ":", "channel", "=", "message", ".", "get", "(", "'channel'", ",", "'general'", ")", "self", ".", "sendMessage", "(", "self", ".", "make_message", "(", "message", "[", "'text'", "]", ",", "channel", ")", ")"], "docstring": "Send message to Slack", "docstring_tokens": ["Send", "message", "to", "Slack"], "sha": "0ec951891812ea4114c27a08c790f63d0f0fd254", "url": "https://github.com/djangobot/djangobot/blob/0ec951891812ea4114c27a08c790f63d0f0fd254/djangobot/client.py#L111-L116", "partition": "test"}
{"repo": "deepmipt/DeepPavlov", "path": "deeppavlov/core/common/metrics_registry.py", "func_name": "register_metric", "original_string": "def register_metric(metric_name: str) -> Callable[..., Any]:\n    \"\"\"Decorator for metric registration.\"\"\"\n    def decorate(fn):\n        fn_name = fn.__module__ + ':' + fn.__name__\n        if metric_name in _REGISTRY and _REGISTRY[metric_name] != fn_name:\n            log.warning('\"{}\" is already registered as a metric name, the old function will be ignored'\n                        .format(metric_name))\n        _REGISTRY[metric_name] = fn_name\n        return fn\n    return decorate", "language": "python", "code": "def register_metric(metric_name: str) -> Callable[..., Any]:\n    \"\"\"Decorator for metric registration.\"\"\"\n    def decorate(fn):\n        fn_name = fn.__module__ + ':' + fn.__name__\n        if metric_name in _REGISTRY and _REGISTRY[metric_name] != fn_name:\n            log.warning('\"{}\" is already registered as a metric name, the old function will be ignored'\n                        .format(metric_name))\n        _REGISTRY[metric_name] = fn_name\n        return fn\n    return decorate", "code_tokens": ["def", "register_metric", "(", "metric_name", ":", "str", ")", "->", "Callable", "[", "...", ",", "Any", "]", ":", "def", "decorate", "(", "fn", ")", ":", "fn_name", "=", "fn", ".", "__module__", "+", "':'", "+", "fn", ".", "__name__", "if", "metric_name", "in", "_REGISTRY", "and", "_REGISTRY", "[", "metric_name", "]", "!=", "fn_name", ":", "log", ".", "warning", "(", "'\"{}\" is already registered as a metric name, the old function will be ignored'", ".", "format", "(", "metric_name", ")", ")", "_REGISTRY", "[", "metric_name", "]", "=", "fn_name", "return", "fn", "return", "decorate"], "docstring": "Decorator for metric registration.", "docstring_tokens": ["Decorator", "for", "metric", "registration", "."], "sha": "f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c", "url": "https://github.com/deepmipt/DeepPavlov/blob/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c/deeppavlov/core/common/metrics_registry.py#L30-L39", "partition": "test"}
{"repo": "dialoguemd/multi-method", "path": "dialogue/multi_method/__init__.py", "func_name": "multi", "original_string": "def multi(dispatch_fn, default=None):\n    \"\"\"A decorator for a function to dispatch on.\n\n    The value returned by the dispatch function is used to look up the\n    implementation function based on its dispatch key.\n\n    The dispatch function is available using the `dispatch_fn` function.\n    \"\"\"\n\n    def _inner(*args, **kwargs):\n        dispatch_value = dispatch_fn(*args, **kwargs)\n        f = _inner.__multi__.get(dispatch_value, _inner.__multi_default__)\n        if f is None:\n            raise Exception(\n                f\"No implementation of {dispatch_fn.__name__} \"\n                f\"for dispatch value {dispatch_value}\"\n            )\n        return f(*args, **kwargs)\n\n    _inner.__multi__ = {}\n    _inner.__multi_default__ = default\n    _inner.__dispatch_fn__ = dispatch_fn\n    return _inner", "language": "python", "code": "def multi(dispatch_fn, default=None):\n    \"\"\"A decorator for a function to dispatch on.\n\n    The value returned by the dispatch function is used to look up the\n    implementation function based on its dispatch key.\n\n    The dispatch function is available using the `dispatch_fn` function.\n    \"\"\"\n\n    def _inner(*args, **kwargs):\n        dispatch_value = dispatch_fn(*args, **kwargs)\n        f = _inner.__multi__.get(dispatch_value, _inner.__multi_default__)\n        if f is None:\n            raise Exception(\n                f\"No implementation of {dispatch_fn.__name__} \"\n                f\"for dispatch value {dispatch_value}\"\n            )\n        return f(*args, **kwargs)\n\n    _inner.__multi__ = {}\n    _inner.__multi_default__ = default\n    _inner.__dispatch_fn__ = dispatch_fn\n    return _inner", "code_tokens": ["def", "multi", "(", "dispatch_fn", ",", "default", "=", "None", ")", ":", "def", "_inner", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "dispatch_value", "=", "dispatch_fn", "(", "*", "args", ",", "*", "*", "kwargs", ")", "f", "=", "_inner", ".", "__multi__", ".", "get", "(", "dispatch_value", ",", "_inner", ".", "__multi_default__", ")", "if", "f", "is", "None", ":", "raise", "Exception", "(", "f\"No implementation of {dispatch_fn.__name__} \"", "f\"for dispatch value {dispatch_value}\"", ")", "return", "f", "(", "*", "args", ",", "*", "*", "kwargs", ")", "_inner", ".", "__multi__", "=", "{", "}", "_inner", ".", "__multi_default__", "=", "default", "_inner", ".", "__dispatch_fn__", "=", "dispatch_fn", "return", "_inner"], "docstring": "A decorator for a function to dispatch on.\n\n    The value returned by the dispatch function is used to look up the\n    implementation function based on its dispatch key.\n\n    The dispatch function is available using the `dispatch_fn` function.", "docstring_tokens": ["A", "decorator", "for", "a", "function", "to", "dispatch", "on", "."], "sha": "8b405d4c5ad74a2a36a4ecf88283262defa2e737", "url": "https://github.com/dialoguemd/multi-method/blob/8b405d4c5ad74a2a36a4ecf88283262defa2e737/dialogue/multi_method/__init__.py#L5-L27", "partition": "test"}
{"repo": "elyase/masstable", "path": "masstable/masstable.py", "func_name": "Table.rmse", "original_string": "def rmse(self, relative_to='AME2003'):\n        \"\"\"Calculate root mean squared error\n\n        Parameters\n        ----------\n        relative_to : string,\n            a valid mass table name.\n\n        Example:\n        ----------\n        >>> template = '{0:10}|{1:^6.2f}|{2:^6.2f}|{3:^6.2f}'\n        >>> print 'Model      ', 'AME95 ', 'AME03 ', 'AME12 '  #  Table header\n        ... for name in Table.names:\n        ...     print template.format(name, Table(name).rmse(relative_to='AME1995'),\n        ...                             Table(name).rmse(relative_to='AME2003'),\n        ...                             Table(name).rmse(relative_to='AME2012'))\n        Model       AME95  AME03  AME12\n        AME2003   | 0.13 | 0.00 | 0.13\n        AME2003all| 0.42 | 0.40 | 0.71\n        AME2012   | 0.16 | 0.13 | 0.00\n        AME2012all| 0.43 | 0.43 | 0.69\n        AME1995   | 0.00 | 0.13 | 0.16\n        AME1995all| 0.00 | 0.17 | 0.21\n        DUZU      | 0.52 | 0.52 | 0.76\n        FRDM95    | 0.79 | 0.78 | 0.95\n        KTUY05    | 0.78 | 0.77 | 1.03\n        ETFSI12   | 0.84 | 0.84 | 1.04\n        HFB14     | 0.84 | 0.83 | 1.02\n        \"\"\"\n\n        error = self.error(relative_to=relative_to)\n        return math.sqrt((error.df ** 2).mean())", "language": "python", "code": "def rmse(self, relative_to='AME2003'):\n        \"\"\"Calculate root mean squared error\n\n        Parameters\n        ----------\n        relative_to : string,\n            a valid mass table name.\n\n        Example:\n        ----------\n        >>> template = '{0:10}|{1:^6.2f}|{2:^6.2f}|{3:^6.2f}'\n        >>> print 'Model      ', 'AME95 ', 'AME03 ', 'AME12 '  #  Table header\n        ... for name in Table.names:\n        ...     print template.format(name, Table(name).rmse(relative_to='AME1995'),\n        ...                             Table(name).rmse(relative_to='AME2003'),\n        ...                             Table(name).rmse(relative_to='AME2012'))\n        Model       AME95  AME03  AME12\n        AME2003   | 0.13 | 0.00 | 0.13\n        AME2003all| 0.42 | 0.40 | 0.71\n        AME2012   | 0.16 | 0.13 | 0.00\n        AME2012all| 0.43 | 0.43 | 0.69\n        AME1995   | 0.00 | 0.13 | 0.16\n        AME1995all| 0.00 | 0.17 | 0.21\n        DUZU      | 0.52 | 0.52 | 0.76\n        FRDM95    | 0.79 | 0.78 | 0.95\n        KTUY05    | 0.78 | 0.77 | 1.03\n        ETFSI12   | 0.84 | 0.84 | 1.04\n        HFB14     | 0.84 | 0.83 | 1.02\n        \"\"\"\n\n        error = self.error(relative_to=relative_to)\n        return math.sqrt((error.df ** 2).mean())", "code_tokens": ["def", "rmse", "(", "self", ",", "relative_to", "=", "'AME2003'", ")", ":", "error", "=", "self", ".", "error", "(", "relative_to", "=", "relative_to", ")", "return", "math", ".", "sqrt", "(", "(", "error", ".", "df", "**", "2", ")", ".", "mean", "(", ")", ")"], "docstring": "Calculate root mean squared error\n\n        Parameters\n        ----------\n        relative_to : string,\n            a valid mass table name.\n\n        Example:\n        ----------\n        >>> template = '{0:10}|{1:^6.2f}|{2:^6.2f}|{3:^6.2f}'\n        >>> print 'Model      ', 'AME95 ', 'AME03 ', 'AME12 '  #  Table header\n        ... for name in Table.names:\n        ...     print template.format(name, Table(name).rmse(relative_to='AME1995'),\n        ...                             Table(name).rmse(relative_to='AME2003'),\n        ...                             Table(name).rmse(relative_to='AME2012'))\n        Model       AME95  AME03  AME12\n        AME2003   | 0.13 | 0.00 | 0.13\n        AME2003all| 0.42 | 0.40 | 0.71\n        AME2012   | 0.16 | 0.13 | 0.00\n        AME2012all| 0.43 | 0.43 | 0.69\n        AME1995   | 0.00 | 0.13 | 0.16\n        AME1995all| 0.00 | 0.17 | 0.21\n        DUZU      | 0.52 | 0.52 | 0.76\n        FRDM95    | 0.79 | 0.78 | 0.95\n        KTUY05    | 0.78 | 0.77 | 1.03\n        ETFSI12   | 0.84 | 0.84 | 1.04\n        HFB14     | 0.84 | 0.83 | 1.02", "docstring_tokens": ["Calculate", "root", "mean", "squared", "error"], "sha": "3eb72b22cd3337bc5c6bb95bb7bb73fdbe6ae9e2", "url": "https://github.com/elyase/masstable/blob/3eb72b22cd3337bc5c6bb95bb7bb73fdbe6ae9e2/masstable/masstable.py#L383-L414", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/dagcircuit/dagcircuit.py", "func_name": "DAGCircuit.named_nodes", "original_string": "def named_nodes(self, *names):\n        \"\"\"Get the set of \"op\" nodes with the given name.\"\"\"\n        named_nodes = []\n        for node in self._multi_graph.nodes():\n            if node.type == 'op' and node.op.name in names:\n                named_nodes.append(node)\n        return named_nodes", "language": "python", "code": "def named_nodes(self, *names):\n        \"\"\"Get the set of \"op\" nodes with the given name.\"\"\"\n        named_nodes = []\n        for node in self._multi_graph.nodes():\n            if node.type == 'op' and node.op.name in names:\n                named_nodes.append(node)\n        return named_nodes", "code_tokens": ["def", "named_nodes", "(", "self", ",", "*", "names", ")", ":", "named_nodes", "=", "[", "]", "for", "node", "in", "self", ".", "_multi_graph", ".", "nodes", "(", ")", ":", "if", "node", ".", "type", "==", "'op'", "and", "node", ".", "op", ".", "name", "in", "names", ":", "named_nodes", ".", "append", "(", "node", ")", "return", "named_nodes"], "docstring": "Get the set of \"op\" nodes with the given name.", "docstring_tokens": ["Get", "the", "set", "of", "op", "nodes", "with", "the", "given", "name", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/dagcircuit/dagcircuit.py#L1020-L1026", "partition": "test"}
{"repo": "ekzhu/datasketch", "path": "datasketch/lshensemble_partition.py", "func_name": "_compute_best_partitions", "original_string": "def _compute_best_partitions(num_part, sizes, nfps):\n    \"\"\"Computes the optimal partitions given the size distributions\n    and computed number of expected false positives for all sub-intervals.\n\n    Args:\n        num_part (int): The number of partitions to create.\n        sizes (numpy.array): The complete domain of set sizes in sorted order.\n        nfps (numpy.array): The computed number of expected false positives\n            for all sub-intervals; axis-0 is for the indexes of lower bounds and\n            axis-1 is for the indexes of upper bounds.\n\n    Returns:\n        partitions (list): list of lower and upper bounds of set sizes for\n            all partitions.\n        total_nfps (float): total number of expected false positives from all\n            partitions.\n        cost (numpy.array): a N x p-1 matrix of the computed optimal NFPs for\n            all sub-problems given upper bound set size and number of partitions.\n    \"\"\"\n\n    if num_part < 2:\n        raise ValueError(\"num_part cannot be less than 2\")\n    if num_part > len(sizes):\n        raise ValueError(\"num_part cannot be greater than the domain size of \"\n                \"all set sizes\")\n\n    # If number of partitions is 2, then simply find the upper bound\n    # of the first partition.\n    if num_part == 2:\n        total_nfps, u = min((nfps[0, u1]+nfps[u1+1, len(sizes)-1], u1)\n            for u1 in range(0, len(sizes)-1))\n        return [(sizes[0], sizes[u]), (sizes[u+1], sizes[-1]),], \\\n                total_nfps, None\n\n    # Initialize subproblem total NFPs.\n    cost = np.zeros((len(sizes), num_part-2))\n\n    # Note: p is the number of partitions in the subproblem.\n    # p2i translates the number of partition into the index in the matrix.\n    p2i = lambda p : p - 2\n\n    # Compute p >= 2 until before p = num_part.\n    for p in range(2, num_part):\n        # Compute best partition for subproblems with increasing\n        # max index u, starting from the smallest possible u given the p.\n        # The smallest possible u can be considered as the max index that\n        # generates p partitions each with only one size.\n        for u in range(p-1, len(sizes)):\n            if p == 2:\n                cost[u, p2i(p)] = min(nfps[0, u1]+nfps[u1+1,u]\n                        for u1 in range(u))\n            else:\n                cost[u, p2i(p)] = min(cost[u1, p2i(p-1)] + nfps[u1+1, u]\n                        for u1 in range((p-1)-1, u))\n    p = num_part\n    # Find the optimal upper bound index of the 2nd right-most partition given\n    # the number of partitions (p).\n    total_nfps, u = min((cost[u1, p2i(p-1)]+nfps[u1+1, len(sizes)-1], u1)\n            for u1 in range((p-1)-1, len(sizes)-1))\n    partitions = [(sizes[u+1], sizes[-1]),]\n    p -= 1\n    # Back track to find the best partitions.\n    while p > 1:\n        # Find the optimal upper bound index of the 2nd right-most partition\n        # givne the number of partitions (p) and upper bound index (u) in this\n        # sub-problem.\n        _, u1_best = min((cost[u1, p2i(p)]+nfps[u1+1, u], u1)\n                for u1 in range((p-1)-1, u))\n        partitions.insert(0, (sizes[u1_best+1], sizes[u]))\n        u = u1_best\n        p -= 1\n    partitions.insert(0, (sizes[0], sizes[u]))\n    return [partitions, total_nfps, cost]", "language": "python", "code": "def _compute_best_partitions(num_part, sizes, nfps):\n    \"\"\"Computes the optimal partitions given the size distributions\n    and computed number of expected false positives for all sub-intervals.\n\n    Args:\n        num_part (int): The number of partitions to create.\n        sizes (numpy.array): The complete domain of set sizes in sorted order.\n        nfps (numpy.array): The computed number of expected false positives\n            for all sub-intervals; axis-0 is for the indexes of lower bounds and\n            axis-1 is for the indexes of upper bounds.\n\n    Returns:\n        partitions (list): list of lower and upper bounds of set sizes for\n            all partitions.\n        total_nfps (float): total number of expected false positives from all\n            partitions.\n        cost (numpy.array): a N x p-1 matrix of the computed optimal NFPs for\n            all sub-problems given upper bound set size and number of partitions.\n    \"\"\"\n\n    if num_part < 2:\n        raise ValueError(\"num_part cannot be less than 2\")\n    if num_part > len(sizes):\n        raise ValueError(\"num_part cannot be greater than the domain size of \"\n                \"all set sizes\")\n\n    # If number of partitions is 2, then simply find the upper bound\n    # of the first partition.\n    if num_part == 2:\n        total_nfps, u = min((nfps[0, u1]+nfps[u1+1, len(sizes)-1], u1)\n            for u1 in range(0, len(sizes)-1))\n        return [(sizes[0], sizes[u]), (sizes[u+1], sizes[-1]),], \\\n                total_nfps, None\n\n    # Initialize subproblem total NFPs.\n    cost = np.zeros((len(sizes), num_part-2))\n\n    # Note: p is the number of partitions in the subproblem.\n    # p2i translates the number of partition into the index in the matrix.\n    p2i = lambda p : p - 2\n\n    # Compute p >= 2 until before p = num_part.\n    for p in range(2, num_part):\n        # Compute best partition for subproblems with increasing\n        # max index u, starting from the smallest possible u given the p.\n        # The smallest possible u can be considered as the max index that\n        # generates p partitions each with only one size.\n        for u in range(p-1, len(sizes)):\n            if p == 2:\n                cost[u, p2i(p)] = min(nfps[0, u1]+nfps[u1+1,u]\n                        for u1 in range(u))\n            else:\n                cost[u, p2i(p)] = min(cost[u1, p2i(p-1)] + nfps[u1+1, u]\n                        for u1 in range((p-1)-1, u))\n    p = num_part\n    # Find the optimal upper bound index of the 2nd right-most partition given\n    # the number of partitions (p).\n    total_nfps, u = min((cost[u1, p2i(p-1)]+nfps[u1+1, len(sizes)-1], u1)\n            for u1 in range((p-1)-1, len(sizes)-1))\n    partitions = [(sizes[u+1], sizes[-1]),]\n    p -= 1\n    # Back track to find the best partitions.\n    while p > 1:\n        # Find the optimal upper bound index of the 2nd right-most partition\n        # givne the number of partitions (p) and upper bound index (u) in this\n        # sub-problem.\n        _, u1_best = min((cost[u1, p2i(p)]+nfps[u1+1, u], u1)\n                for u1 in range((p-1)-1, u))\n        partitions.insert(0, (sizes[u1_best+1], sizes[u]))\n        u = u1_best\n        p -= 1\n    partitions.insert(0, (sizes[0], sizes[u]))\n    return [partitions, total_nfps, cost]", "code_tokens": ["def", "_compute_best_partitions", "(", "num_part", ",", "sizes", ",", "nfps", ")", ":", "if", "num_part", "<", "2", ":", "raise", "ValueError", "(", "\"num_part cannot be less than 2\"", ")", "if", "num_part", ">", "len", "(", "sizes", ")", ":", "raise", "ValueError", "(", "\"num_part cannot be greater than the domain size of \"", "\"all set sizes\"", ")", "# If number of partitions is 2, then simply find the upper bound", "# of the first partition.", "if", "num_part", "==", "2", ":", "total_nfps", ",", "u", "=", "min", "(", "(", "nfps", "[", "0", ",", "u1", "]", "+", "nfps", "[", "u1", "+", "1", ",", "len", "(", "sizes", ")", "-", "1", "]", ",", "u1", ")", "for", "u1", "in", "range", "(", "0", ",", "len", "(", "sizes", ")", "-", "1", ")", ")", "return", "[", "(", "sizes", "[", "0", "]", ",", "sizes", "[", "u", "]", ")", ",", "(", "sizes", "[", "u", "+", "1", "]", ",", "sizes", "[", "-", "1", "]", ")", ",", "]", ",", "total_nfps", ",", "None", "# Initialize subproblem total NFPs.", "cost", "=", "np", ".", "zeros", "(", "(", "len", "(", "sizes", ")", ",", "num_part", "-", "2", ")", ")", "# Note: p is the number of partitions in the subproblem.", "# p2i translates the number of partition into the index in the matrix.", "p2i", "=", "lambda", "p", ":", "p", "-", "2", "# Compute p >= 2 until before p = num_part.", "for", "p", "in", "range", "(", "2", ",", "num_part", ")", ":", "# Compute best partition for subproblems with increasing", "# max index u, starting from the smallest possible u given the p.", "# The smallest possible u can be considered as the max index that", "# generates p partitions each with only one size.", "for", "u", "in", "range", "(", "p", "-", "1", ",", "len", "(", "sizes", ")", ")", ":", "if", "p", "==", "2", ":", "cost", "[", "u", ",", "p2i", "(", "p", ")", "]", "=", "min", "(", "nfps", "[", "0", ",", "u1", "]", "+", "nfps", "[", "u1", "+", "1", ",", "u", "]", "for", "u1", "in", "range", "(", "u", ")", ")", "else", ":", "cost", "[", "u", ",", "p2i", "(", "p", ")", "]", "=", "min", "(", "cost", "[", "u1", ",", "p2i", "(", "p", "-", "1", ")", "]", "+", "nfps", "[", "u1", "+", "1", ",", "u", "]", "for", "u1", "in", "range", "(", "(", "p", "-", "1", ")", "-", "1", ",", "u", ")", ")", "p", "=", "num_part", "# Find the optimal upper bound index of the 2nd right-most partition given", "# the number of partitions (p).", "total_nfps", ",", "u", "=", "min", "(", "(", "cost", "[", "u1", ",", "p2i", "(", "p", "-", "1", ")", "]", "+", "nfps", "[", "u1", "+", "1", ",", "len", "(", "sizes", ")", "-", "1", "]", ",", "u1", ")", "for", "u1", "in", "range", "(", "(", "p", "-", "1", ")", "-", "1", ",", "len", "(", "sizes", ")", "-", "1", ")", ")", "partitions", "=", "[", "(", "sizes", "[", "u", "+", "1", "]", ",", "sizes", "[", "-", "1", "]", ")", ",", "]", "p", "-=", "1", "# Back track to find the best partitions.", "while", "p", ">", "1", ":", "# Find the optimal upper bound index of the 2nd right-most partition", "# givne the number of partitions (p) and upper bound index (u) in this", "# sub-problem.", "_", ",", "u1_best", "=", "min", "(", "(", "cost", "[", "u1", ",", "p2i", "(", "p", ")", "]", "+", "nfps", "[", "u1", "+", "1", ",", "u", "]", ",", "u1", ")", "for", "u1", "in", "range", "(", "(", "p", "-", "1", ")", "-", "1", ",", "u", ")", ")", "partitions", ".", "insert", "(", "0", ",", "(", "sizes", "[", "u1_best", "+", "1", "]", ",", "sizes", "[", "u", "]", ")", ")", "u", "=", "u1_best", "p", "-=", "1", "partitions", ".", "insert", "(", "0", ",", "(", "sizes", "[", "0", "]", ",", "sizes", "[", "u", "]", ")", ")", "return", "[", "partitions", ",", "total_nfps", ",", "cost", "]"], "docstring": "Computes the optimal partitions given the size distributions\n    and computed number of expected false positives for all sub-intervals.\n\n    Args:\n        num_part (int): The number of partitions to create.\n        sizes (numpy.array): The complete domain of set sizes in sorted order.\n        nfps (numpy.array): The computed number of expected false positives\n            for all sub-intervals; axis-0 is for the indexes of lower bounds and\n            axis-1 is for the indexes of upper bounds.\n\n    Returns:\n        partitions (list): list of lower and upper bounds of set sizes for\n            all partitions.\n        total_nfps (float): total number of expected false positives from all\n            partitions.\n        cost (numpy.array): a N x p-1 matrix of the computed optimal NFPs for\n            all sub-problems given upper bound set size and number of partitions.", "docstring_tokens": ["Computes", "the", "optimal", "partitions", "given", "the", "size", "distributions", "and", "computed", "number", "of", "expected", "false", "positives", "for", "all", "sub", "-", "intervals", "."], "sha": "b3e4129987890a2beb04f2c0b6dc618ae35f2e14", "url": "https://github.com/ekzhu/datasketch/blob/b3e4129987890a2beb04f2c0b6dc618ae35f2e14/datasketch/lshensemble_partition.py#L96-L168", "partition": "test"}
{"repo": "chaoss/grimoirelab-kingarthur", "path": "arthur/scheduler.py", "func_name": "_JobScheduler.cancel_job_task", "original_string": "def cancel_job_task(self, task_id):\n        \"\"\"Cancel the job related to the given task.\"\"\"\n\n        try:\n            self._rwlock.writer_acquire()\n\n            job_id = self._tasks.get(task_id, None)\n\n            if job_id:\n                self._cancel_job(job_id)\n            else:\n                logger.warning(\"Task %s set to be removed was not found\",\n                               task_id)\n        finally:\n            self._rwlock.writer_release()", "language": "python", "code": "def cancel_job_task(self, task_id):\n        \"\"\"Cancel the job related to the given task.\"\"\"\n\n        try:\n            self._rwlock.writer_acquire()\n\n            job_id = self._tasks.get(task_id, None)\n\n            if job_id:\n                self._cancel_job(job_id)\n            else:\n                logger.warning(\"Task %s set to be removed was not found\",\n                               task_id)\n        finally:\n            self._rwlock.writer_release()", "code_tokens": ["def", "cancel_job_task", "(", "self", ",", "task_id", ")", ":", "try", ":", "self", ".", "_rwlock", ".", "writer_acquire", "(", ")", "job_id", "=", "self", ".", "_tasks", ".", "get", "(", "task_id", ",", "None", ")", "if", "job_id", ":", "self", ".", "_cancel_job", "(", "job_id", ")", "else", ":", "logger", ".", "warning", "(", "\"Task %s set to be removed was not found\"", ",", "task_id", ")", "finally", ":", "self", ".", "_rwlock", ".", "writer_release", "(", ")"], "docstring": "Cancel the job related to the given task.", "docstring_tokens": ["Cancel", "the", "job", "related", "to", "the", "given", "task", "."], "sha": "9d6a638bee68d5e5c511f045eeebf06340fd3252", "url": "https://github.com/chaoss/grimoirelab-kingarthur/blob/9d6a638bee68d5e5c511f045eeebf06340fd3252/arthur/scheduler.py#L136-L150", "partition": "test"}
{"repo": "boundary/pulse-api-cli", "path": "boundary/metric_markdown.py", "func_name": "MetricMarkdown.outputFieldMarkdown", "original_string": "def outputFieldMarkdown(self):\n        \"\"\"\n        Sends the field definitions ot standard out\n        \"\"\"\n        f, d = self.getFieldsColumnLengths()\n\n        fc, dc = self.printFieldsHeader(f, d)\n        f = max(fc, f)\n        d = max(dc, d)\n        self.printFields(f, d)", "language": "python", "code": "def outputFieldMarkdown(self):\n        \"\"\"\n        Sends the field definitions ot standard out\n        \"\"\"\n        f, d = self.getFieldsColumnLengths()\n\n        fc, dc = self.printFieldsHeader(f, d)\n        f = max(fc, f)\n        d = max(dc, d)\n        self.printFields(f, d)", "code_tokens": ["def", "outputFieldMarkdown", "(", "self", ")", ":", "f", ",", "d", "=", "self", ".", "getFieldsColumnLengths", "(", ")", "fc", ",", "dc", "=", "self", ".", "printFieldsHeader", "(", "f", ",", "d", ")", "f", "=", "max", "(", "fc", ",", "f", ")", "d", "=", "max", "(", "dc", ",", "d", ")", "self", ".", "printFields", "(", "f", ",", "d", ")"], "docstring": "Sends the field definitions ot standard out", "docstring_tokens": ["Sends", "the", "field", "definitions", "ot", "standard", "out"], "sha": "b01ca65b442eed19faac309c9d62bbc3cb2c098f", "url": "https://github.com/boundary/pulse-api-cli/blob/b01ca65b442eed19faac309c9d62bbc3cb2c098f/boundary/metric_markdown.py#L201-L210", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/pulse/timeslots.py", "func_name": "TimeslotCollection.ch_start_time", "original_string": "def ch_start_time(self, *channels: List[Channel]) -> int:\n        \"\"\"Return earliest start time in this collection.\n\n        Args:\n            *channels: Channels over which to obtain start_time.\n        \"\"\"\n        intervals = list(itertools.chain(*(self._table[chan] for chan in channels\n                                           if chan in self._table)))\n        if intervals:\n            return min((interval.begin for interval in intervals))\n        return 0", "language": "python", "code": "def ch_start_time(self, *channels: List[Channel]) -> int:\n        \"\"\"Return earliest start time in this collection.\n\n        Args:\n            *channels: Channels over which to obtain start_time.\n        \"\"\"\n        intervals = list(itertools.chain(*(self._table[chan] for chan in channels\n                                           if chan in self._table)))\n        if intervals:\n            return min((interval.begin for interval in intervals))\n        return 0", "code_tokens": ["def", "ch_start_time", "(", "self", ",", "*", "channels", ":", "List", "[", "Channel", "]", ")", "->", "int", ":", "intervals", "=", "list", "(", "itertools", ".", "chain", "(", "*", "(", "self", ".", "_table", "[", "chan", "]", "for", "chan", "in", "channels", "if", "chan", "in", "self", ".", "_table", ")", ")", ")", "if", "intervals", ":", "return", "min", "(", "(", "interval", ".", "begin", "for", "interval", "in", "intervals", ")", ")", "return", "0"], "docstring": "Return earliest start time in this collection.\n\n        Args:\n            *channels: Channels over which to obtain start_time.", "docstring_tokens": ["Return", "earliest", "start", "time", "in", "this", "collection", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/pulse/timeslots.py#L179-L189", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/server/blueprints/variants/views.py", "func_name": "sv_variant", "original_string": "def sv_variant(institute_id, case_name, variant_id):\n    \"\"\"Display a specific structural variant.\"\"\"\n    data = controllers.sv_variant(store, institute_id, case_name, variant_id)\n    return data", "language": "python", "code": "def sv_variant(institute_id, case_name, variant_id):\n    \"\"\"Display a specific structural variant.\"\"\"\n    data = controllers.sv_variant(store, institute_id, case_name, variant_id)\n    return data", "code_tokens": ["def", "sv_variant", "(", "institute_id", ",", "case_name", ",", "variant_id", ")", ":", "data", "=", "controllers", ".", "sv_variant", "(", "store", ",", "institute_id", ",", "case_name", ",", "variant_id", ")", "return", "data"], "docstring": "Display a specific structural variant.", "docstring_tokens": ["Display", "a", "specific", "structural", "variant", "."], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/server/blueprints/variants/views.py#L299-L302", "partition": "test"}
{"repo": "tomMoral/loky", "path": "loky/backend/semaphore_tracker.py", "func_name": "main", "original_string": "def main(fd, verbose=0):\n    '''Run semaphore tracker.'''\n    # protect the process from ^C and \"killall python\" etc\n    signal.signal(signal.SIGINT, signal.SIG_IGN)\n    signal.signal(signal.SIGTERM, signal.SIG_IGN)\n\n    if _HAVE_SIGMASK:\n        signal.pthread_sigmask(signal.SIG_UNBLOCK, _IGNORED_SIGNALS)\n\n    for f in (sys.stdin, sys.stdout):\n        try:\n            f.close()\n        except Exception:\n            pass\n\n    if verbose:  # pragma: no cover\n        sys.stderr.write(\"Main semaphore tracker is running\\n\")\n        sys.stderr.flush()\n\n    cache = set()\n    try:\n        # keep track of registered/unregistered semaphores\n        with os.fdopen(fd, 'rb') as f:\n            for line in f:\n                try:\n                    cmd, name = line.strip().split(b':')\n                    if cmd == b'REGISTER':\n                        name = name.decode('ascii')\n                        cache.add(name)\n                        if verbose:  # pragma: no cover\n                            sys.stderr.write(\"[SemaphoreTracker] register {}\\n\"\n                                             .format(name))\n                            sys.stderr.flush()\n                    elif cmd == b'UNREGISTER':\n                        name = name.decode('ascii')\n                        cache.remove(name)\n                        if verbose:  # pragma: no cover\n                            sys.stderr.write(\"[SemaphoreTracker] unregister {}\"\n                                             \": cache({})\\n\"\n                                             .format(name, len(cache)))\n                            sys.stderr.flush()\n                    elif cmd == b'PROBE':\n                        pass\n                    else:\n                        raise RuntimeError('unrecognized command %r' % cmd)\n                except BaseException:\n                    try:\n                        sys.excepthook(*sys.exc_info())\n                    except BaseException:\n                        pass\n    finally:\n        # all processes have terminated; cleanup any remaining semaphores\n        if cache:\n            try:\n                warnings.warn('semaphore_tracker: There appear to be %d '\n                              'leaked semaphores to clean up at shutdown' %\n                              len(cache))\n            except Exception:\n                pass\n        for name in cache:\n            # For some reason the process which created and registered this\n            # semaphore has failed to unregister it. Presumably it has died.\n            # We therefore unlink it.\n            try:\n                try:\n                    sem_unlink(name)\n                    if verbose:  # pragma: no cover\n                        sys.stderr.write(\"[SemaphoreTracker] unlink {}\\n\"\n                                         .format(name))\n                        sys.stderr.flush()\n                except Exception as e:\n                    warnings.warn('semaphore_tracker: %s: %r' % (name, e))\n            finally:\n                pass\n\n    if verbose:  # pragma: no cover\n        sys.stderr.write(\"semaphore tracker shut down\\n\")\n        sys.stderr.flush()", "language": "python", "code": "def main(fd, verbose=0):\n    '''Run semaphore tracker.'''\n    # protect the process from ^C and \"killall python\" etc\n    signal.signal(signal.SIGINT, signal.SIG_IGN)\n    signal.signal(signal.SIGTERM, signal.SIG_IGN)\n\n    if _HAVE_SIGMASK:\n        signal.pthread_sigmask(signal.SIG_UNBLOCK, _IGNORED_SIGNALS)\n\n    for f in (sys.stdin, sys.stdout):\n        try:\n            f.close()\n        except Exception:\n            pass\n\n    if verbose:  # pragma: no cover\n        sys.stderr.write(\"Main semaphore tracker is running\\n\")\n        sys.stderr.flush()\n\n    cache = set()\n    try:\n        # keep track of registered/unregistered semaphores\n        with os.fdopen(fd, 'rb') as f:\n            for line in f:\n                try:\n                    cmd, name = line.strip().split(b':')\n                    if cmd == b'REGISTER':\n                        name = name.decode('ascii')\n                        cache.add(name)\n                        if verbose:  # pragma: no cover\n                            sys.stderr.write(\"[SemaphoreTracker] register {}\\n\"\n                                             .format(name))\n                            sys.stderr.flush()\n                    elif cmd == b'UNREGISTER':\n                        name = name.decode('ascii')\n                        cache.remove(name)\n                        if verbose:  # pragma: no cover\n                            sys.stderr.write(\"[SemaphoreTracker] unregister {}\"\n                                             \": cache({})\\n\"\n                                             .format(name, len(cache)))\n                            sys.stderr.flush()\n                    elif cmd == b'PROBE':\n                        pass\n                    else:\n                        raise RuntimeError('unrecognized command %r' % cmd)\n                except BaseException:\n                    try:\n                        sys.excepthook(*sys.exc_info())\n                    except BaseException:\n                        pass\n    finally:\n        # all processes have terminated; cleanup any remaining semaphores\n        if cache:\n            try:\n                warnings.warn('semaphore_tracker: There appear to be %d '\n                              'leaked semaphores to clean up at shutdown' %\n                              len(cache))\n            except Exception:\n                pass\n        for name in cache:\n            # For some reason the process which created and registered this\n            # semaphore has failed to unregister it. Presumably it has died.\n            # We therefore unlink it.\n            try:\n                try:\n                    sem_unlink(name)\n                    if verbose:  # pragma: no cover\n                        sys.stderr.write(\"[SemaphoreTracker] unlink {}\\n\"\n                                         .format(name))\n                        sys.stderr.flush()\n                except Exception as e:\n                    warnings.warn('semaphore_tracker: %s: %r' % (name, e))\n            finally:\n                pass\n\n    if verbose:  # pragma: no cover\n        sys.stderr.write(\"semaphore tracker shut down\\n\")\n        sys.stderr.flush()", "code_tokens": ["def", "main", "(", "fd", ",", "verbose", "=", "0", ")", ":", "# protect the process from ^C and \"killall python\" etc", "signal", ".", "signal", "(", "signal", ".", "SIGINT", ",", "signal", ".", "SIG_IGN", ")", "signal", ".", "signal", "(", "signal", ".", "SIGTERM", ",", "signal", ".", "SIG_IGN", ")", "if", "_HAVE_SIGMASK", ":", "signal", ".", "pthread_sigmask", "(", "signal", ".", "SIG_UNBLOCK", ",", "_IGNORED_SIGNALS", ")", "for", "f", "in", "(", "sys", ".", "stdin", ",", "sys", ".", "stdout", ")", ":", "try", ":", "f", ".", "close", "(", ")", "except", "Exception", ":", "pass", "if", "verbose", ":", "# pragma: no cover", "sys", ".", "stderr", ".", "write", "(", "\"Main semaphore tracker is running\\n\"", ")", "sys", ".", "stderr", ".", "flush", "(", ")", "cache", "=", "set", "(", ")", "try", ":", "# keep track of registered/unregistered semaphores", "with", "os", ".", "fdopen", "(", "fd", ",", "'rb'", ")", "as", "f", ":", "for", "line", "in", "f", ":", "try", ":", "cmd", ",", "name", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "b':'", ")", "if", "cmd", "==", "b'REGISTER'", ":", "name", "=", "name", ".", "decode", "(", "'ascii'", ")", "cache", ".", "add", "(", "name", ")", "if", "verbose", ":", "# pragma: no cover", "sys", ".", "stderr", ".", "write", "(", "\"[SemaphoreTracker] register {}\\n\"", ".", "format", "(", "name", ")", ")", "sys", ".", "stderr", ".", "flush", "(", ")", "elif", "cmd", "==", "b'UNREGISTER'", ":", "name", "=", "name", ".", "decode", "(", "'ascii'", ")", "cache", ".", "remove", "(", "name", ")", "if", "verbose", ":", "# pragma: no cover", "sys", ".", "stderr", ".", "write", "(", "\"[SemaphoreTracker] unregister {}\"", "\": cache({})\\n\"", ".", "format", "(", "name", ",", "len", "(", "cache", ")", ")", ")", "sys", ".", "stderr", ".", "flush", "(", ")", "elif", "cmd", "==", "b'PROBE'", ":", "pass", "else", ":", "raise", "RuntimeError", "(", "'unrecognized command %r'", "%", "cmd", ")", "except", "BaseException", ":", "try", ":", "sys", ".", "excepthook", "(", "*", "sys", ".", "exc_info", "(", ")", ")", "except", "BaseException", ":", "pass", "finally", ":", "# all processes have terminated; cleanup any remaining semaphores", "if", "cache", ":", "try", ":", "warnings", ".", "warn", "(", "'semaphore_tracker: There appear to be %d '", "'leaked semaphores to clean up at shutdown'", "%", "len", "(", "cache", ")", ")", "except", "Exception", ":", "pass", "for", "name", "in", "cache", ":", "# For some reason the process which created and registered this", "# semaphore has failed to unregister it. Presumably it has died.", "# We therefore unlink it.", "try", ":", "try", ":", "sem_unlink", "(", "name", ")", "if", "verbose", ":", "# pragma: no cover", "sys", ".", "stderr", ".", "write", "(", "\"[SemaphoreTracker] unlink {}\\n\"", ".", "format", "(", "name", ")", ")", "sys", ".", "stderr", ".", "flush", "(", ")", "except", "Exception", "as", "e", ":", "warnings", ".", "warn", "(", "'semaphore_tracker: %s: %r'", "%", "(", "name", ",", "e", ")", ")", "finally", ":", "pass", "if", "verbose", ":", "# pragma: no cover", "sys", ".", "stderr", ".", "write", "(", "\"semaphore tracker shut down\\n\"", ")", "sys", ".", "stderr", ".", "flush", "(", ")"], "docstring": "Run semaphore tracker.", "docstring_tokens": ["Run", "semaphore", "tracker", "."], "sha": "dc2d941d8285a96f3a5b666a4bd04875b0b25984", "url": "https://github.com/tomMoral/loky/blob/dc2d941d8285a96f3a5b666a4bd04875b0b25984/loky/backend/semaphore_tracker.py#L170-L247", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py", "func_name": "ServiceManagementService.change_deployment_configuration", "original_string": "def change_deployment_configuration(self, service_name, deployment_name,\n                                        configuration,\n                                        treat_warnings_as_error=False,\n                                        mode='Auto', extended_properties=None):\n        '''\n        Initiates a change to the deployment configuration.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name of the deployment.\n        configuration:\n            The base-64 encoded service configuration file for the deployment.\n        treat_warnings_as_error:\n            Indicates whether to treat package validation warnings as errors.\n            If set to true, the Created Deployment operation fails if there\n            are validation warnings on the service package.\n        mode:\n            If set to Manual, WalkUpgradeDomain must be called to apply the\n            update. If set to Auto, the Windows Azure platform will\n            automatically apply the update To each upgrade domain for the\n            service. Possible values are: Auto, Manual\n        extended_properties:\n            Dictionary containing name/value pairs of storage account\n            properties. You can have a maximum of 50 extended property\n            name/value pairs. The maximum length of the Name element is 64\n            characters, only alphanumeric characters and underscores are valid\n            in the Name, and the name must start with a letter. The value has\n            a maximum length of 255 characters.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('configuration', configuration)\n        return self._perform_post(\n            self._get_deployment_path_using_name(\n                service_name, deployment_name) + '/?comp=config',\n            _XmlSerializer.change_deployment_to_xml(\n                configuration,\n                treat_warnings_as_error,\n                mode,\n                extended_properties),\n            as_async=True)", "language": "python", "code": "def change_deployment_configuration(self, service_name, deployment_name,\n                                        configuration,\n                                        treat_warnings_as_error=False,\n                                        mode='Auto', extended_properties=None):\n        '''\n        Initiates a change to the deployment configuration.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name of the deployment.\n        configuration:\n            The base-64 encoded service configuration file for the deployment.\n        treat_warnings_as_error:\n            Indicates whether to treat package validation warnings as errors.\n            If set to true, the Created Deployment operation fails if there\n            are validation warnings on the service package.\n        mode:\n            If set to Manual, WalkUpgradeDomain must be called to apply the\n            update. If set to Auto, the Windows Azure platform will\n            automatically apply the update To each upgrade domain for the\n            service. Possible values are: Auto, Manual\n        extended_properties:\n            Dictionary containing name/value pairs of storage account\n            properties. You can have a maximum of 50 extended property\n            name/value pairs. The maximum length of the Name element is 64\n            characters, only alphanumeric characters and underscores are valid\n            in the Name, and the name must start with a letter. The value has\n            a maximum length of 255 characters.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('configuration', configuration)\n        return self._perform_post(\n            self._get_deployment_path_using_name(\n                service_name, deployment_name) + '/?comp=config',\n            _XmlSerializer.change_deployment_to_xml(\n                configuration,\n                treat_warnings_as_error,\n                mode,\n                extended_properties),\n            as_async=True)", "code_tokens": ["def", "change_deployment_configuration", "(", "self", ",", "service_name", ",", "deployment_name", ",", "configuration", ",", "treat_warnings_as_error", "=", "False", ",", "mode", "=", "'Auto'", ",", "extended_properties", "=", "None", ")", ":", "_validate_not_none", "(", "'service_name'", ",", "service_name", ")", "_validate_not_none", "(", "'deployment_name'", ",", "deployment_name", ")", "_validate_not_none", "(", "'configuration'", ",", "configuration", ")", "return", "self", ".", "_perform_post", "(", "self", ".", "_get_deployment_path_using_name", "(", "service_name", ",", "deployment_name", ")", "+", "'/?comp=config'", ",", "_XmlSerializer", ".", "change_deployment_to_xml", "(", "configuration", ",", "treat_warnings_as_error", ",", "mode", ",", "extended_properties", ")", ",", "as_async", "=", "True", ")"], "docstring": "Initiates a change to the deployment configuration.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name of the deployment.\n        configuration:\n            The base-64 encoded service configuration file for the deployment.\n        treat_warnings_as_error:\n            Indicates whether to treat package validation warnings as errors.\n            If set to true, the Created Deployment operation fails if there\n            are validation warnings on the service package.\n        mode:\n            If set to Manual, WalkUpgradeDomain must be called to apply the\n            update. If set to Auto, the Windows Azure platform will\n            automatically apply the update To each upgrade domain for the\n            service. Possible values are: Auto, Manual\n        extended_properties:\n            Dictionary containing name/value pairs of storage account\n            properties. You can have a maximum of 50 extended property\n            name/value pairs. The maximum length of the Name element is 64\n            characters, only alphanumeric characters and underscores are valid\n            in the Name, and the name must start with a letter. The value has\n            a maximum length of 255 characters.", "docstring_tokens": ["Initiates", "a", "change", "to", "the", "deployment", "configuration", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py#L592-L633", "partition": "test"}
{"repo": "shopkick/flawless", "path": "flawless/server/service.py", "func_name": "FlawlessServiceBaseClass._matches_filepath_pattern", "original_string": "def _matches_filepath_pattern(self, filepath):\n        '''Given a filepath, and a list of regex patterns, this function returns true\n        if filepath matches any one of those patterns'''\n        if not self.only_blame_patterns:\n            return True\n\n        for pattern in self.only_blame_patterns:\n            if pattern.match(filepath):\n                return True\n        return False", "language": "python", "code": "def _matches_filepath_pattern(self, filepath):\n        '''Given a filepath, and a list of regex patterns, this function returns true\n        if filepath matches any one of those patterns'''\n        if not self.only_blame_patterns:\n            return True\n\n        for pattern in self.only_blame_patterns:\n            if pattern.match(filepath):\n                return True\n        return False", "code_tokens": ["def", "_matches_filepath_pattern", "(", "self", ",", "filepath", ")", ":", "if", "not", "self", ".", "only_blame_patterns", ":", "return", "True", "for", "pattern", "in", "self", ".", "only_blame_patterns", ":", "if", "pattern", ".", "match", "(", "filepath", ")", ":", "return", "True", "return", "False"], "docstring": "Given a filepath, and a list of regex patterns, this function returns true\n        if filepath matches any one of those patterns", "docstring_tokens": ["Given", "a", "filepath", "and", "a", "list", "of", "regex", "patterns", "this", "function", "returns", "true", "if", "filepath", "matches", "any", "one", "of", "those", "patterns"], "sha": "c54b63ca1991c153e6f75080536f6df445aacc64", "url": "https://github.com/shopkick/flawless/blob/c54b63ca1991c153e6f75080536f6df445aacc64/flawless/server/service.py#L227-L236", "partition": "test"}
{"repo": "laplacesdemon/django-youtube", "path": "django_youtube/views.py", "func_name": "video_list", "original_string": "def video_list(request, username=None):\n    \"\"\"\n    list of videos of a user\n    if username does not set, shows the currently logged in user\n    \"\"\"\n\n    # If user is not authenticated and username is None, raise an error\n    if username is None and not request.user.is_authenticated():\n        from django.http import Http404\n        raise Http404\n\n    from django.contrib.auth.models import User\n    user = User.objects.get(username=username) if username else request.user\n\n    # loop through the videos of the user\n    videos = Video.objects.filter(user=user).all()\n    video_params = []\n    for video in videos:\n        video_params.append(_video_params(request, video.video_id))\n\n    return render_to_response(\n        \"django_youtube/videos.html\",\n        {\"video_params\": video_params},\n        context_instance=RequestContext(request)\n    )", "language": "python", "code": "def video_list(request, username=None):\n    \"\"\"\n    list of videos of a user\n    if username does not set, shows the currently logged in user\n    \"\"\"\n\n    # If user is not authenticated and username is None, raise an error\n    if username is None and not request.user.is_authenticated():\n        from django.http import Http404\n        raise Http404\n\n    from django.contrib.auth.models import User\n    user = User.objects.get(username=username) if username else request.user\n\n    # loop through the videos of the user\n    videos = Video.objects.filter(user=user).all()\n    video_params = []\n    for video in videos:\n        video_params.append(_video_params(request, video.video_id))\n\n    return render_to_response(\n        \"django_youtube/videos.html\",\n        {\"video_params\": video_params},\n        context_instance=RequestContext(request)\n    )", "code_tokens": ["def", "video_list", "(", "request", ",", "username", "=", "None", ")", ":", "# If user is not authenticated and username is None, raise an error", "if", "username", "is", "None", "and", "not", "request", ".", "user", ".", "is_authenticated", "(", ")", ":", "from", "django", ".", "http", "import", "Http404", "raise", "Http404", "from", "django", ".", "contrib", ".", "auth", ".", "models", "import", "User", "user", "=", "User", ".", "objects", ".", "get", "(", "username", "=", "username", ")", "if", "username", "else", "request", ".", "user", "# loop through the videos of the user", "videos", "=", "Video", ".", "objects", ".", "filter", "(", "user", "=", "user", ")", ".", "all", "(", ")", "video_params", "=", "[", "]", "for", "video", "in", "videos", ":", "video_params", ".", "append", "(", "_video_params", "(", "request", ",", "video", ".", "video_id", ")", ")", "return", "render_to_response", "(", "\"django_youtube/videos.html\"", ",", "{", "\"video_params\"", ":", "video_params", "}", ",", "context_instance", "=", "RequestContext", "(", "request", ")", ")"], "docstring": "list of videos of a user\n    if username does not set, shows the currently logged in user", "docstring_tokens": ["list", "of", "videos", "of", "a", "user", "if", "username", "does", "not", "set", "shows", "the", "currently", "logged", "in", "user"], "sha": "8051ef372473eccb053f773c68e2e5e1b2cfb538", "url": "https://github.com/laplacesdemon/django-youtube/blob/8051ef372473eccb053f773c68e2e5e1b2cfb538/django_youtube/views.py#L93-L117", "partition": "test"}
{"repo": "kgiusti/pyngus", "path": "examples/rpc-server.py", "func_name": "SocketConnection.process_input", "original_string": "def process_input(self):\n        \"\"\"Called when socket is read-ready\"\"\"\n        try:\n            pyngus.read_socket_input(self.connection, self.socket)\n        except Exception as e:\n            LOG.error(\"Exception on socket read: %s\", str(e))\n            self.connection.close_input()\n            self.connection.close()\n        self.connection.process(time.time())", "language": "python", "code": "def process_input(self):\n        \"\"\"Called when socket is read-ready\"\"\"\n        try:\n            pyngus.read_socket_input(self.connection, self.socket)\n        except Exception as e:\n            LOG.error(\"Exception on socket read: %s\", str(e))\n            self.connection.close_input()\n            self.connection.close()\n        self.connection.process(time.time())", "code_tokens": ["def", "process_input", "(", "self", ")", ":", "try", ":", "pyngus", ".", "read_socket_input", "(", "self", ".", "connection", ",", "self", ".", "socket", ")", "except", "Exception", "as", "e", ":", "LOG", ".", "error", "(", "\"Exception on socket read: %s\"", ",", "str", "(", "e", ")", ")", "self", ".", "connection", ".", "close_input", "(", ")", "self", ".", "connection", ".", "close", "(", ")", "self", ".", "connection", ".", "process", "(", "time", ".", "time", "(", ")", ")"], "docstring": "Called when socket is read-ready", "docstring_tokens": ["Called", "when", "socket", "is", "read", "-", "ready"], "sha": "5392392046989f1bb84ba938c30e4d48311075f1", "url": "https://github.com/kgiusti/pyngus/blob/5392392046989f1bb84ba938c30e4d48311075f1/examples/rpc-server.py#L92-L100", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/nose/config.py", "func_name": "Config.configureLogging", "original_string": "def configureLogging(self):\n        \"\"\"Configure logging for nose, or optionally other packages. Any logger\n        name may be set with the debug option, and that logger will be set to\n        debug level and be assigned the same handler as the nose loggers, unless\n        it already has a handler.\n        \"\"\"\n        if self.loggingConfig:\n            from logging.config import fileConfig\n            fileConfig(self.loggingConfig)\n            return\n\n        format = logging.Formatter('%(name)s: %(levelname)s: %(message)s')\n        if self.debugLog:\n            handler = logging.FileHandler(self.debugLog)\n        else:\n            handler = logging.StreamHandler(self.logStream)\n        handler.setFormatter(format)\n\n        logger = logging.getLogger('nose')\n        logger.propagate = 0\n\n        # only add our default handler if there isn't already one there\n        # this avoids annoying duplicate log messages.\n        if handler not in logger.handlers:\n            logger.addHandler(handler)\n\n        # default level\n        lvl = logging.WARNING\n        if self.verbosity >= 5:\n            lvl = 0\n        elif self.verbosity >= 4:\n            lvl = logging.DEBUG\n        elif self.verbosity >= 3:\n            lvl = logging.INFO\n        logger.setLevel(lvl)\n\n        # individual overrides\n        if self.debug:\n            # no blanks\n            debug_loggers = [ name for name in self.debug.split(',')\n                              if name ]\n            for logger_name in debug_loggers:\n                l = logging.getLogger(logger_name)\n                l.setLevel(logging.DEBUG)\n                if not l.handlers and not logger_name.startswith('nose'):\n                    l.addHandler(handler)", "language": "python", "code": "def configureLogging(self):\n        \"\"\"Configure logging for nose, or optionally other packages. Any logger\n        name may be set with the debug option, and that logger will be set to\n        debug level and be assigned the same handler as the nose loggers, unless\n        it already has a handler.\n        \"\"\"\n        if self.loggingConfig:\n            from logging.config import fileConfig\n            fileConfig(self.loggingConfig)\n            return\n\n        format = logging.Formatter('%(name)s: %(levelname)s: %(message)s')\n        if self.debugLog:\n            handler = logging.FileHandler(self.debugLog)\n        else:\n            handler = logging.StreamHandler(self.logStream)\n        handler.setFormatter(format)\n\n        logger = logging.getLogger('nose')\n        logger.propagate = 0\n\n        # only add our default handler if there isn't already one there\n        # this avoids annoying duplicate log messages.\n        if handler not in logger.handlers:\n            logger.addHandler(handler)\n\n        # default level\n        lvl = logging.WARNING\n        if self.verbosity >= 5:\n            lvl = 0\n        elif self.verbosity >= 4:\n            lvl = logging.DEBUG\n        elif self.verbosity >= 3:\n            lvl = logging.INFO\n        logger.setLevel(lvl)\n\n        # individual overrides\n        if self.debug:\n            # no blanks\n            debug_loggers = [ name for name in self.debug.split(',')\n                              if name ]\n            for logger_name in debug_loggers:\n                l = logging.getLogger(logger_name)\n                l.setLevel(logging.DEBUG)\n                if not l.handlers and not logger_name.startswith('nose'):\n                    l.addHandler(handler)", "code_tokens": ["def", "configureLogging", "(", "self", ")", ":", "if", "self", ".", "loggingConfig", ":", "from", "logging", ".", "config", "import", "fileConfig", "fileConfig", "(", "self", ".", "loggingConfig", ")", "return", "format", "=", "logging", ".", "Formatter", "(", "'%(name)s: %(levelname)s: %(message)s'", ")", "if", "self", ".", "debugLog", ":", "handler", "=", "logging", ".", "FileHandler", "(", "self", ".", "debugLog", ")", "else", ":", "handler", "=", "logging", ".", "StreamHandler", "(", "self", ".", "logStream", ")", "handler", ".", "setFormatter", "(", "format", ")", "logger", "=", "logging", ".", "getLogger", "(", "'nose'", ")", "logger", ".", "propagate", "=", "0", "# only add our default handler if there isn't already one there", "# this avoids annoying duplicate log messages.", "if", "handler", "not", "in", "logger", ".", "handlers", ":", "logger", ".", "addHandler", "(", "handler", ")", "# default level", "lvl", "=", "logging", ".", "WARNING", "if", "self", ".", "verbosity", ">=", "5", ":", "lvl", "=", "0", "elif", "self", ".", "verbosity", ">=", "4", ":", "lvl", "=", "logging", ".", "DEBUG", "elif", "self", ".", "verbosity", ">=", "3", ":", "lvl", "=", "logging", ".", "INFO", "logger", ".", "setLevel", "(", "lvl", ")", "# individual overrides", "if", "self", ".", "debug", ":", "# no blanks", "debug_loggers", "=", "[", "name", "for", "name", "in", "self", ".", "debug", ".", "split", "(", "','", ")", "if", "name", "]", "for", "logger_name", "in", "debug_loggers", ":", "l", "=", "logging", ".", "getLogger", "(", "logger_name", ")", "l", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "if", "not", "l", ".", "handlers", "and", "not", "logger_name", ".", "startswith", "(", "'nose'", ")", ":", "l", ".", "addHandler", "(", "handler", ")"], "docstring": "Configure logging for nose, or optionally other packages. Any logger\n        name may be set with the debug option, and that logger will be set to\n        debug level and be assigned the same handler as the nose loggers, unless\n        it already has a handler.", "docstring_tokens": ["Configure", "logging", "for", "nose", "or", "optionally", "other", "packages", ".", "Any", "logger", "name", "may", "be", "set", "with", "the", "debug", "option", "and", "that", "logger", "will", "be", "set", "to", "debug", "level", "and", "be", "assigned", "the", "same", "handler", "as", "the", "nose", "loggers", "unless", "it", "already", "has", "a", "handler", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/nose/config.py#L341-L386", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/dagcircuit/dagcircuit.py", "func_name": "DAGCircuit.serial_layers", "original_string": "def serial_layers(self):\n        \"\"\"Yield a layer for all gates of this circuit.\n\n        A serial layer is a circuit with one gate. The layers have the\n        same structure as in layers().\n        \"\"\"\n        for next_node in self.topological_op_nodes():\n            new_layer = DAGCircuit()\n            for qreg in self.qregs.values():\n                new_layer.add_qreg(qreg)\n            for creg in self.cregs.values():\n                new_layer.add_creg(creg)\n            # Save the support of the operation we add to the layer\n            support_list = []\n            # Operation data\n            op = copy.copy(next_node.op)\n            qa = copy.copy(next_node.qargs)\n            ca = copy.copy(next_node.cargs)\n            co = copy.copy(next_node.condition)\n            _ = self._bits_in_condition(co)\n\n            # Add node to new_layer\n            new_layer.apply_operation_back(op, qa, ca, co)\n            # Add operation to partition\n            if next_node.name not in [\"barrier\",\n                                      \"snapshot\", \"save\", \"load\", \"noise\"]:\n                support_list.append(list(qa))\n            l_dict = {\"graph\": new_layer, \"partition\": support_list}\n            yield l_dict", "language": "python", "code": "def serial_layers(self):\n        \"\"\"Yield a layer for all gates of this circuit.\n\n        A serial layer is a circuit with one gate. The layers have the\n        same structure as in layers().\n        \"\"\"\n        for next_node in self.topological_op_nodes():\n            new_layer = DAGCircuit()\n            for qreg in self.qregs.values():\n                new_layer.add_qreg(qreg)\n            for creg in self.cregs.values():\n                new_layer.add_creg(creg)\n            # Save the support of the operation we add to the layer\n            support_list = []\n            # Operation data\n            op = copy.copy(next_node.op)\n            qa = copy.copy(next_node.qargs)\n            ca = copy.copy(next_node.cargs)\n            co = copy.copy(next_node.condition)\n            _ = self._bits_in_condition(co)\n\n            # Add node to new_layer\n            new_layer.apply_operation_back(op, qa, ca, co)\n            # Add operation to partition\n            if next_node.name not in [\"barrier\",\n                                      \"snapshot\", \"save\", \"load\", \"noise\"]:\n                support_list.append(list(qa))\n            l_dict = {\"graph\": new_layer, \"partition\": support_list}\n            yield l_dict", "code_tokens": ["def", "serial_layers", "(", "self", ")", ":", "for", "next_node", "in", "self", ".", "topological_op_nodes", "(", ")", ":", "new_layer", "=", "DAGCircuit", "(", ")", "for", "qreg", "in", "self", ".", "qregs", ".", "values", "(", ")", ":", "new_layer", ".", "add_qreg", "(", "qreg", ")", "for", "creg", "in", "self", ".", "cregs", ".", "values", "(", ")", ":", "new_layer", ".", "add_creg", "(", "creg", ")", "# Save the support of the operation we add to the layer", "support_list", "=", "[", "]", "# Operation data", "op", "=", "copy", ".", "copy", "(", "next_node", ".", "op", ")", "qa", "=", "copy", ".", "copy", "(", "next_node", ".", "qargs", ")", "ca", "=", "copy", ".", "copy", "(", "next_node", ".", "cargs", ")", "co", "=", "copy", ".", "copy", "(", "next_node", ".", "condition", ")", "_", "=", "self", ".", "_bits_in_condition", "(", "co", ")", "# Add node to new_layer", "new_layer", ".", "apply_operation_back", "(", "op", ",", "qa", ",", "ca", ",", "co", ")", "# Add operation to partition", "if", "next_node", ".", "name", "not", "in", "[", "\"barrier\"", ",", "\"snapshot\"", ",", "\"save\"", ",", "\"load\"", ",", "\"noise\"", "]", ":", "support_list", ".", "append", "(", "list", "(", "qa", ")", ")", "l_dict", "=", "{", "\"graph\"", ":", "new_layer", ",", "\"partition\"", ":", "support_list", "}", "yield", "l_dict"], "docstring": "Yield a layer for all gates of this circuit.\n\n        A serial layer is a circuit with one gate. The layers have the\n        same structure as in layers().", "docstring_tokens": ["Yield", "a", "layer", "for", "all", "gates", "of", "this", "circuit", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/dagcircuit/dagcircuit.py#L1303-L1331", "partition": "test"}
{"repo": "ndrlslz/ternya", "path": "ternya/process.py", "func_name": "cinder_process", "original_string": "def cinder_process(body, message):\n    \"\"\"\n    This function deal with the cinder notification.\n\n    First, find process from customer_process that not include wildcard.\n    if not find from customer_process, then find process from customer_process_wildcard.\n    if not find from customer_process_wildcard, then use ternya default process.\n    :param body: dict of openstack notification.\n    :param message: kombu Message class\n    :return:\n    \"\"\"\n    event_type = body['event_type']\n    process = cinder_customer_process.get(event_type)\n    if process is not None:\n        process(body, message)\n    else:\n        matched = False\n        process_wildcard = None\n        for pattern in cinder_customer_process_wildcard.keys():\n            if pattern.match(event_type):\n                process_wildcard = cinder_customer_process_wildcard.get(pattern)\n                matched = True\n                break\n        if matched:\n            process_wildcard(body, message)\n        else:\n            default_process(body, message)\n    message.ack()", "language": "python", "code": "def cinder_process(body, message):\n    \"\"\"\n    This function deal with the cinder notification.\n\n    First, find process from customer_process that not include wildcard.\n    if not find from customer_process, then find process from customer_process_wildcard.\n    if not find from customer_process_wildcard, then use ternya default process.\n    :param body: dict of openstack notification.\n    :param message: kombu Message class\n    :return:\n    \"\"\"\n    event_type = body['event_type']\n    process = cinder_customer_process.get(event_type)\n    if process is not None:\n        process(body, message)\n    else:\n        matched = False\n        process_wildcard = None\n        for pattern in cinder_customer_process_wildcard.keys():\n            if pattern.match(event_type):\n                process_wildcard = cinder_customer_process_wildcard.get(pattern)\n                matched = True\n                break\n        if matched:\n            process_wildcard(body, message)\n        else:\n            default_process(body, message)\n    message.ack()", "code_tokens": ["def", "cinder_process", "(", "body", ",", "message", ")", ":", "event_type", "=", "body", "[", "'event_type'", "]", "process", "=", "cinder_customer_process", ".", "get", "(", "event_type", ")", "if", "process", "is", "not", "None", ":", "process", "(", "body", ",", "message", ")", "else", ":", "matched", "=", "False", "process_wildcard", "=", "None", "for", "pattern", "in", "cinder_customer_process_wildcard", ".", "keys", "(", ")", ":", "if", "pattern", ".", "match", "(", "event_type", ")", ":", "process_wildcard", "=", "cinder_customer_process_wildcard", ".", "get", "(", "pattern", ")", "matched", "=", "True", "break", "if", "matched", ":", "process_wildcard", "(", "body", ",", "message", ")", "else", ":", "default_process", "(", "body", ",", "message", ")", "message", ".", "ack", "(", ")"], "docstring": "This function deal with the cinder notification.\n\n    First, find process from customer_process that not include wildcard.\n    if not find from customer_process, then find process from customer_process_wildcard.\n    if not find from customer_process_wildcard, then use ternya default process.\n    :param body: dict of openstack notification.\n    :param message: kombu Message class\n    :return:", "docstring_tokens": ["This", "function", "deal", "with", "the", "cinder", "notification", "."], "sha": "c05aec10029e645d63ff04313dbcf2644743481f", "url": "https://github.com/ndrlslz/ternya/blob/c05aec10029e645d63ff04313dbcf2644743481f/ternya/process.py#L57-L84", "partition": "test"}
{"repo": "mfcovington/django-project-home-templatetags", "path": "project_home_tags/templatetags/project_home.py", "func_name": "home_url", "original_string": "def home_url():\n    \"\"\"Get project's home URL based on settings.PROJECT_HOME_NAMESPACE.\n\n    Returns None if PROJECT_HOME_NAMESPACE is not defined in settings.\n    \"\"\"\n    try:\n        return reverse(home_namespace)\n    except Exception:\n        url = home_namespace\n        try:\n            validate_url = URLValidator()\n            if '://' not in url:\n                url = 'http://' + url\n            validate_url(url)\n            return(url)\n        except ValidationError:\n            return None", "language": "python", "code": "def home_url():\n    \"\"\"Get project's home URL based on settings.PROJECT_HOME_NAMESPACE.\n\n    Returns None if PROJECT_HOME_NAMESPACE is not defined in settings.\n    \"\"\"\n    try:\n        return reverse(home_namespace)\n    except Exception:\n        url = home_namespace\n        try:\n            validate_url = URLValidator()\n            if '://' not in url:\n                url = 'http://' + url\n            validate_url(url)\n            return(url)\n        except ValidationError:\n            return None", "code_tokens": ["def", "home_url", "(", ")", ":", "try", ":", "return", "reverse", "(", "home_namespace", ")", "except", "Exception", ":", "url", "=", "home_namespace", "try", ":", "validate_url", "=", "URLValidator", "(", ")", "if", "'://'", "not", "in", "url", ":", "url", "=", "'http://'", "+", "url", "validate_url", "(", "url", ")", "return", "(", "url", ")", "except", "ValidationError", ":", "return", "None"], "docstring": "Get project's home URL based on settings.PROJECT_HOME_NAMESPACE.\n\n    Returns None if PROJECT_HOME_NAMESPACE is not defined in settings.", "docstring_tokens": ["Get", "project", "s", "home", "URL", "based", "on", "settings", ".", "PROJECT_HOME_NAMESPACE", "."], "sha": "abc660906086088792c5e5e7be6ecd151c2ccddb", "url": "https://github.com/mfcovington/django-project-home-templatetags/blob/abc660906086088792c5e5e7be6ecd151c2ccddb/project_home_tags/templatetags/project_home.py#L20-L36", "partition": "test"}
{"repo": "Terrance/SkPy", "path": "skpy/conn.py", "func_name": "SkypeConnection.syncEndpoints", "original_string": "def syncEndpoints(self):\n        \"\"\"\n        Retrieve all current endpoints for the connected user.\n        \"\"\"\n        self.endpoints[\"all\"] = []\n        for json in self(\"GET\", \"{0}/users/ME/presenceDocs/messagingService\".format(self.msgsHost),\n                         params={\"view\": \"expanded\"}, auth=self.Auth.RegToken).json().get(\"endpointPresenceDocs\", []):\n            id = json.get(\"link\", \"\").split(\"/\")[7]\n            self.endpoints[\"all\"].append(SkypeEndpoint(self, id))", "language": "python", "code": "def syncEndpoints(self):\n        \"\"\"\n        Retrieve all current endpoints for the connected user.\n        \"\"\"\n        self.endpoints[\"all\"] = []\n        for json in self(\"GET\", \"{0}/users/ME/presenceDocs/messagingService\".format(self.msgsHost),\n                         params={\"view\": \"expanded\"}, auth=self.Auth.RegToken).json().get(\"endpointPresenceDocs\", []):\n            id = json.get(\"link\", \"\").split(\"/\")[7]\n            self.endpoints[\"all\"].append(SkypeEndpoint(self, id))", "code_tokens": ["def", "syncEndpoints", "(", "self", ")", ":", "self", ".", "endpoints", "[", "\"all\"", "]", "=", "[", "]", "for", "json", "in", "self", "(", "\"GET\"", ",", "\"{0}/users/ME/presenceDocs/messagingService\"", ".", "format", "(", "self", ".", "msgsHost", ")", ",", "params", "=", "{", "\"view\"", ":", "\"expanded\"", "}", ",", "auth", "=", "self", ".", "Auth", ".", "RegToken", ")", ".", "json", "(", ")", ".", "get", "(", "\"endpointPresenceDocs\"", ",", "[", "]", ")", ":", "id", "=", "json", ".", "get", "(", "\"link\"", ",", "\"\"", ")", ".", "split", "(", "\"/\"", ")", "[", "7", "]", "self", ".", "endpoints", "[", "\"all\"", "]", ".", "append", "(", "SkypeEndpoint", "(", "self", ",", "id", ")", ")"], "docstring": "Retrieve all current endpoints for the connected user.", "docstring_tokens": ["Retrieve", "all", "current", "endpoints", "for", "the", "connected", "user", "."], "sha": "0f9489c94e8ec4d3effab4314497428872a80ad1", "url": "https://github.com/Terrance/SkPy/blob/0f9489c94e8ec4d3effab4314497428872a80ad1/skpy/conn.py#L436-L444", "partition": "test"}
{"repo": "funilrys/PyFunceble", "path": "PyFunceble/mining.py", "func_name": "Mining.process", "original_string": "def process(self):  # pragma: no cover\n        \"\"\"\n        Process the logic and structuration of the mining database.\n        \"\"\"\n\n        if PyFunceble.CONFIGURATION[\"mining\"]:\n            # The mining is activated.\n\n            # We load the mining logic.\n            mined = self.mine()\n\n            if mined:\n                # The mined data is not empty or None.\n\n                # We add the mined data to the global database.\n                self._add(mined)\n\n                # And we finally backup everything.\n                self._backup()", "language": "python", "code": "def process(self):  # pragma: no cover\n        \"\"\"\n        Process the logic and structuration of the mining database.\n        \"\"\"\n\n        if PyFunceble.CONFIGURATION[\"mining\"]:\n            # The mining is activated.\n\n            # We load the mining logic.\n            mined = self.mine()\n\n            if mined:\n                # The mined data is not empty or None.\n\n                # We add the mined data to the global database.\n                self._add(mined)\n\n                # And we finally backup everything.\n                self._backup()", "code_tokens": ["def", "process", "(", "self", ")", ":", "# pragma: no cover", "if", "PyFunceble", ".", "CONFIGURATION", "[", "\"mining\"", "]", ":", "# The mining is activated.", "# We load the mining logic.", "mined", "=", "self", ".", "mine", "(", ")", "if", "mined", ":", "# The mined data is not empty or None.", "# We add the mined data to the global database.", "self", ".", "_add", "(", "mined", ")", "# And we finally backup everything.", "self", ".", "_backup", "(", ")"], "docstring": "Process the logic and structuration of the mining database.", "docstring_tokens": ["Process", "the", "logic", "and", "structuration", "of", "the", "mining", "database", "."], "sha": "cdf69cbde120199171f7158e1c33635753e6e2f5", "url": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/mining.py#L376-L394", "partition": "test"}
{"repo": "ToucanToco/toucan-data-sdk", "path": "toucan_data_sdk/utils/decorators.py", "func_name": "log_time", "original_string": "def log_time(logger):\n    \"\"\"\n    Decorator to log the execution time of a function\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            start = time.time()\n            result = func(*args, **kwargs)\n            end = time.time()\n            _log_time(logger, func.__name__, start, end)\n            return result\n        return wrapper\n    return decorator", "language": "python", "code": "def log_time(logger):\n    \"\"\"\n    Decorator to log the execution time of a function\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            start = time.time()\n            result = func(*args, **kwargs)\n            end = time.time()\n            _log_time(logger, func.__name__, start, end)\n            return result\n        return wrapper\n    return decorator", "code_tokens": ["def", "log_time", "(", "logger", ")", ":", "def", "decorator", "(", "func", ")", ":", "@", "wraps", "(", "func", ")", "def", "wrapper", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "start", "=", "time", ".", "time", "(", ")", "result", "=", "func", "(", "*", "args", ",", "*", "*", "kwargs", ")", "end", "=", "time", ".", "time", "(", ")", "_log_time", "(", "logger", ",", "func", ".", "__name__", ",", "start", ",", "end", ")", "return", "result", "return", "wrapper", "return", "decorator"], "docstring": "Decorator to log the execution time of a function", "docstring_tokens": ["Decorator", "to", "log", "the", "execution", "time", "of", "a", "function"], "sha": "c3ca874e1b64f4bdcc2edda750a72d45d1561d8a", "url": "https://github.com/ToucanToco/toucan-data-sdk/blob/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a/toucan_data_sdk/utils/decorators.py#L123-L136", "partition": "test"}
{"repo": "pyca/pyopenssl", "path": "src/OpenSSL/SSL.py", "func_name": "Connection.get_cipher_name", "original_string": "def get_cipher_name(self):\n        \"\"\"\n        Obtain the name of the currently used cipher.\n\n        :returns: The name of the currently used cipher or :obj:`None`\n            if no connection has been established.\n        :rtype: :class:`unicode` or :class:`NoneType`\n\n        .. versionadded:: 0.15\n        \"\"\"\n        cipher = _lib.SSL_get_current_cipher(self._ssl)\n        if cipher == _ffi.NULL:\n            return None\n        else:\n            name = _ffi.string(_lib.SSL_CIPHER_get_name(cipher))\n            return name.decode(\"utf-8\")", "language": "python", "code": "def get_cipher_name(self):\n        \"\"\"\n        Obtain the name of the currently used cipher.\n\n        :returns: The name of the currently used cipher or :obj:`None`\n            if no connection has been established.\n        :rtype: :class:`unicode` or :class:`NoneType`\n\n        .. versionadded:: 0.15\n        \"\"\"\n        cipher = _lib.SSL_get_current_cipher(self._ssl)\n        if cipher == _ffi.NULL:\n            return None\n        else:\n            name = _ffi.string(_lib.SSL_CIPHER_get_name(cipher))\n            return name.decode(\"utf-8\")", "code_tokens": ["def", "get_cipher_name", "(", "self", ")", ":", "cipher", "=", "_lib", ".", "SSL_get_current_cipher", "(", "self", ".", "_ssl", ")", "if", "cipher", "==", "_ffi", ".", "NULL", ":", "return", "None", "else", ":", "name", "=", "_ffi", ".", "string", "(", "_lib", ".", "SSL_CIPHER_get_name", "(", "cipher", ")", ")", "return", "name", ".", "decode", "(", "\"utf-8\"", ")"], "docstring": "Obtain the name of the currently used cipher.\n\n        :returns: The name of the currently used cipher or :obj:`None`\n            if no connection has been established.\n        :rtype: :class:`unicode` or :class:`NoneType`\n\n        .. versionadded:: 0.15", "docstring_tokens": ["Obtain", "the", "name", "of", "the", "currently", "used", "cipher", "."], "sha": "1fbe064c50fd030948141d7d630673761525b0d0", "url": "https://github.com/pyca/pyopenssl/blob/1fbe064c50fd030948141d7d630673761525b0d0/src/OpenSSL/SSL.py#L2357-L2372", "partition": "test"}
{"repo": "glottobank/python-newick", "path": "src/newick.py", "func_name": "load", "original_string": "def load(fp, strip_comments=False, **kw):\n    \"\"\"\n    Load a list of trees from an open Newick formatted file.\n\n    :param fp: open file handle.\n    :param strip_comments: Flag signaling whether to strip comments enclosed in square \\\n    brackets.\n    :param kw: Keyword arguments are passed through to `Node.create`.\n    :return: List of Node objects.\n    \"\"\"\n    kw['strip_comments'] = strip_comments\n    return loads(fp.read(), **kw)", "language": "python", "code": "def load(fp, strip_comments=False, **kw):\n    \"\"\"\n    Load a list of trees from an open Newick formatted file.\n\n    :param fp: open file handle.\n    :param strip_comments: Flag signaling whether to strip comments enclosed in square \\\n    brackets.\n    :param kw: Keyword arguments are passed through to `Node.create`.\n    :return: List of Node objects.\n    \"\"\"\n    kw['strip_comments'] = strip_comments\n    return loads(fp.read(), **kw)", "code_tokens": ["def", "load", "(", "fp", ",", "strip_comments", "=", "False", ",", "*", "*", "kw", ")", ":", "kw", "[", "'strip_comments'", "]", "=", "strip_comments", "return", "loads", "(", "fp", ".", "read", "(", ")", ",", "*", "*", "kw", ")"], "docstring": "Load a list of trees from an open Newick formatted file.\n\n    :param fp: open file handle.\n    :param strip_comments: Flag signaling whether to strip comments enclosed in square \\\n    brackets.\n    :param kw: Keyword arguments are passed through to `Node.create`.\n    :return: List of Node objects.", "docstring_tokens": ["Load", "a", "list", "of", "trees", "from", "an", "open", "Newick", "formatted", "file", "."], "sha": "e8d4d1e4610f271d0f0e5cb86c0e0360b43bd702", "url": "https://github.com/glottobank/python-newick/blob/e8d4d1e4610f271d0f0e5cb86c0e0360b43bd702/src/newick.py#L386-L397", "partition": "test"}
{"repo": "jaraco/jaraco.itertools", "path": "jaraco/itertools.py", "func_name": "every_other", "original_string": "def every_other(iterable):\n\t\"\"\"\n\tYield every other item from the iterable\n\n\t>>> ' '.join(every_other('abcdefg'))\n\t'a c e g'\n\t\"\"\"\n\titems = iter(iterable)\n\twhile True:\n\t\ttry:\n\t\t\tyield next(items)\n\t\t\tnext(items)\n\t\texcept StopIteration:\n\t\t\treturn", "language": "python", "code": "def every_other(iterable):\n\t\"\"\"\n\tYield every other item from the iterable\n\n\t>>> ' '.join(every_other('abcdefg'))\n\t'a c e g'\n\t\"\"\"\n\titems = iter(iterable)\n\twhile True:\n\t\ttry:\n\t\t\tyield next(items)\n\t\t\tnext(items)\n\t\texcept StopIteration:\n\t\t\treturn", "code_tokens": ["def", "every_other", "(", "iterable", ")", ":", "items", "=", "iter", "(", "iterable", ")", "while", "True", ":", "try", ":", "yield", "next", "(", "items", ")", "next", "(", "items", ")", "except", "StopIteration", ":", "return"], "docstring": "Yield every other item from the iterable\n\n\t>>> ' '.join(every_other('abcdefg'))\n\t'a c e g'", "docstring_tokens": ["Yield", "every", "other", "item", "from", "the", "iterable"], "sha": "0dc47c8924fa3d9ab676c3a6e195f03f728b72c6", "url": "https://github.com/jaraco/jaraco.itertools/blob/0dc47c8924fa3d9ab676c3a6e195f03f728b72c6/jaraco/itertools.py#L573-L586", "partition": "test"}
{"repo": "neogenix/jsonklog", "path": "jsonklog/handlers/mongodbhandler.py", "func_name": "MongoDBHandler.emit", "original_string": "def emit(self, record):\n\n        \"\"\" pymongo expects a dict \"\"\"\n        msg = self.format(record)\n\n        if not isinstance(msg, dict):\n            msg = json.loads(msg)\n\n        self.collection.insert(msg)", "language": "python", "code": "def emit(self, record):\n\n        \"\"\" pymongo expects a dict \"\"\"\n        msg = self.format(record)\n\n        if not isinstance(msg, dict):\n            msg = json.loads(msg)\n\n        self.collection.insert(msg)", "code_tokens": ["def", "emit", "(", "self", ",", "record", ")", ":", "msg", "=", "self", ".", "format", "(", "record", ")", "if", "not", "isinstance", "(", "msg", ",", "dict", ")", ":", "msg", "=", "json", ".", "loads", "(", "msg", ")", "self", ".", "collection", ".", "insert", "(", "msg", ")"], "docstring": "pymongo expects a dict", "docstring_tokens": ["pymongo", "expects", "a", "dict"], "sha": "ac4b8f5b75b4a0be60ecad9e71d624bad08c3fa1", "url": "https://github.com/neogenix/jsonklog/blob/ac4b8f5b75b4a0be60ecad9e71d624bad08c3fa1/jsonklog/handlers/mongodbhandler.py#L35-L43", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/werkzeug/debug/__init__.py", "func_name": "DebuggedApplication.get_resource", "original_string": "def get_resource(self, request, filename):\n        \"\"\"Return a static resource from the shared folder.\"\"\"\n        filename = join(dirname(__file__), 'shared', basename(filename))\n        if isfile(filename):\n            mimetype = mimetypes.guess_type(filename)[0] \\\n                or 'application/octet-stream'\n            f = open(filename, 'rb')\n            try:\n                return Response(f.read(), mimetype=mimetype)\n            finally:\n                f.close()\n        return Response('Not Found', status=404)", "language": "python", "code": "def get_resource(self, request, filename):\n        \"\"\"Return a static resource from the shared folder.\"\"\"\n        filename = join(dirname(__file__), 'shared', basename(filename))\n        if isfile(filename):\n            mimetype = mimetypes.guess_type(filename)[0] \\\n                or 'application/octet-stream'\n            f = open(filename, 'rb')\n            try:\n                return Response(f.read(), mimetype=mimetype)\n            finally:\n                f.close()\n        return Response('Not Found', status=404)", "code_tokens": ["def", "get_resource", "(", "self", ",", "request", ",", "filename", ")", ":", "filename", "=", "join", "(", "dirname", "(", "__file__", ")", ",", "'shared'", ",", "basename", "(", "filename", ")", ")", "if", "isfile", "(", "filename", ")", ":", "mimetype", "=", "mimetypes", ".", "guess_type", "(", "filename", ")", "[", "0", "]", "or", "'application/octet-stream'", "f", "=", "open", "(", "filename", ",", "'rb'", ")", "try", ":", "return", "Response", "(", "f", ".", "read", "(", ")", ",", "mimetype", "=", "mimetype", ")", "finally", ":", "f", ".", "close", "(", ")", "return", "Response", "(", "'Not Found'", ",", "status", "=", "404", ")"], "docstring": "Return a static resource from the shared folder.", "docstring_tokens": ["Return", "a", "static", "resource", "from", "the", "shared", "folder", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/werkzeug/debug/__init__.py#L146-L157", "partition": "test"}
{"repo": "hustlzp/Flask-Boost", "path": "flask_boost/project/application/utils/decorators.py", "func_name": "jsonify", "original_string": "def jsonify(func):\n    \"\"\"JSON decorator.\"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        r = func(*args, **kwargs)\n        if isinstance(r, tuple):\n            code, data = r\n        else:\n            code, data = 200, r\n        return Response(json.dumps(data), status=code, mimetype='application/json')\n\n    return wrapper", "language": "python", "code": "def jsonify(func):\n    \"\"\"JSON decorator.\"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        r = func(*args, **kwargs)\n        if isinstance(r, tuple):\n            code, data = r\n        else:\n            code, data = 200, r\n        return Response(json.dumps(data), status=code, mimetype='application/json')\n\n    return wrapper", "code_tokens": ["def", "jsonify", "(", "func", ")", ":", "@", "functools", ".", "wraps", "(", "func", ")", "def", "wrapper", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "r", "=", "func", "(", "*", "args", ",", "*", "*", "kwargs", ")", "if", "isinstance", "(", "r", ",", "tuple", ")", ":", "code", ",", "data", "=", "r", "else", ":", "code", ",", "data", "=", "200", ",", "r", "return", "Response", "(", "json", ".", "dumps", "(", "data", ")", ",", "status", "=", "code", ",", "mimetype", "=", "'application/json'", ")", "return", "wrapper"], "docstring": "JSON decorator.", "docstring_tokens": ["JSON", "decorator", "."], "sha": "d0308408ebb248dd752b77123b845f8ec637fab2", "url": "https://github.com/hustlzp/Flask-Boost/blob/d0308408ebb248dd752b77123b845f8ec637fab2/flask_boost/project/application/utils/decorators.py#L6-L18", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/completer.py", "func_name": "IPCompleter.rlcomplete", "original_string": "def rlcomplete(self, text, state):\n        \"\"\"Return the state-th possible completion for 'text'.\n\n        This is called successively with state == 0, 1, 2, ... until it\n        returns None.  The completion should begin with 'text'.\n\n        Parameters\n        ----------\n          text : string\n            Text to perform the completion on.\n\n          state : int\n            Counter used by readline.\n        \"\"\"\n        if state==0:\n\n            self.line_buffer = line_buffer = self.readline.get_line_buffer()\n            cursor_pos = self.readline.get_endidx()\n\n            #io.rprint(\"\\nRLCOMPLETE: %r %r %r\" %\n            #          (text, line_buffer, cursor_pos) ) # dbg\n\n            # if there is only a tab on a line with only whitespace, instead of\n            # the mostly useless 'do you want to see all million completions'\n            # message, just do the right thing and give the user his tab!\n            # Incidentally, this enables pasting of tabbed text from an editor\n            # (as long as autoindent is off).\n\n            # It should be noted that at least pyreadline still shows file\n            # completions - is there a way around it?\n\n            # don't apply this on 'dumb' terminals, such as emacs buffers, so\n            # we don't interfere with their own tab-completion mechanism.\n            if not (self.dumb_terminal or line_buffer.strip()):\n                self.readline.insert_text('\\t')\n                sys.stdout.flush()\n                return None\n\n            # Note: debugging exceptions that may occur in completion is very\n            # tricky, because readline unconditionally silences them.  So if\n            # during development you suspect a bug in the completion code, turn\n            # this flag on temporarily by uncommenting the second form (don't\n            # flip the value in the first line, as the '# dbg' marker can be\n            # automatically detected and is used elsewhere).\n            DEBUG = False\n            #DEBUG = True # dbg\n            if DEBUG:\n                try:\n                    self.complete(text, line_buffer, cursor_pos)\n                except:\n                    import traceback; traceback.print_exc()\n            else:\n                # The normal production version is here\n\n                # This method computes the self.matches array\n                self.complete(text, line_buffer, cursor_pos)\n\n        try:\n            return self.matches[state]\n        except IndexError:\n            return None", "language": "python", "code": "def rlcomplete(self, text, state):\n        \"\"\"Return the state-th possible completion for 'text'.\n\n        This is called successively with state == 0, 1, 2, ... until it\n        returns None.  The completion should begin with 'text'.\n\n        Parameters\n        ----------\n          text : string\n            Text to perform the completion on.\n\n          state : int\n            Counter used by readline.\n        \"\"\"\n        if state==0:\n\n            self.line_buffer = line_buffer = self.readline.get_line_buffer()\n            cursor_pos = self.readline.get_endidx()\n\n            #io.rprint(\"\\nRLCOMPLETE: %r %r %r\" %\n            #          (text, line_buffer, cursor_pos) ) # dbg\n\n            # if there is only a tab on a line with only whitespace, instead of\n            # the mostly useless 'do you want to see all million completions'\n            # message, just do the right thing and give the user his tab!\n            # Incidentally, this enables pasting of tabbed text from an editor\n            # (as long as autoindent is off).\n\n            # It should be noted that at least pyreadline still shows file\n            # completions - is there a way around it?\n\n            # don't apply this on 'dumb' terminals, such as emacs buffers, so\n            # we don't interfere with their own tab-completion mechanism.\n            if not (self.dumb_terminal or line_buffer.strip()):\n                self.readline.insert_text('\\t')\n                sys.stdout.flush()\n                return None\n\n            # Note: debugging exceptions that may occur in completion is very\n            # tricky, because readline unconditionally silences them.  So if\n            # during development you suspect a bug in the completion code, turn\n            # this flag on temporarily by uncommenting the second form (don't\n            # flip the value in the first line, as the '# dbg' marker can be\n            # automatically detected and is used elsewhere).\n            DEBUG = False\n            #DEBUG = True # dbg\n            if DEBUG:\n                try:\n                    self.complete(text, line_buffer, cursor_pos)\n                except:\n                    import traceback; traceback.print_exc()\n            else:\n                # The normal production version is here\n\n                # This method computes the self.matches array\n                self.complete(text, line_buffer, cursor_pos)\n\n        try:\n            return self.matches[state]\n        except IndexError:\n            return None", "code_tokens": ["def", "rlcomplete", "(", "self", ",", "text", ",", "state", ")", ":", "if", "state", "==", "0", ":", "self", ".", "line_buffer", "=", "line_buffer", "=", "self", ".", "readline", ".", "get_line_buffer", "(", ")", "cursor_pos", "=", "self", ".", "readline", ".", "get_endidx", "(", ")", "#io.rprint(\"\\nRLCOMPLETE: %r %r %r\" %", "#          (text, line_buffer, cursor_pos) ) # dbg", "# if there is only a tab on a line with only whitespace, instead of", "# the mostly useless 'do you want to see all million completions'", "# message, just do the right thing and give the user his tab!", "# Incidentally, this enables pasting of tabbed text from an editor", "# (as long as autoindent is off).", "# It should be noted that at least pyreadline still shows file", "# completions - is there a way around it?", "# don't apply this on 'dumb' terminals, such as emacs buffers, so", "# we don't interfere with their own tab-completion mechanism.", "if", "not", "(", "self", ".", "dumb_terminal", "or", "line_buffer", ".", "strip", "(", ")", ")", ":", "self", ".", "readline", ".", "insert_text", "(", "'\\t'", ")", "sys", ".", "stdout", ".", "flush", "(", ")", "return", "None", "# Note: debugging exceptions that may occur in completion is very", "# tricky, because readline unconditionally silences them.  So if", "# during development you suspect a bug in the completion code, turn", "# this flag on temporarily by uncommenting the second form (don't", "# flip the value in the first line, as the '# dbg' marker can be", "# automatically detected and is used elsewhere).", "DEBUG", "=", "False", "#DEBUG = True # dbg", "if", "DEBUG", ":", "try", ":", "self", ".", "complete", "(", "text", ",", "line_buffer", ",", "cursor_pos", ")", "except", ":", "import", "traceback", "traceback", ".", "print_exc", "(", ")", "else", ":", "# The normal production version is here", "# This method computes the self.matches array", "self", ".", "complete", "(", "text", ",", "line_buffer", ",", "cursor_pos", ")", "try", ":", "return", "self", ".", "matches", "[", "state", "]", "except", "IndexError", ":", "return", "None"], "docstring": "Return the state-th possible completion for 'text'.\n\n        This is called successively with state == 0, 1, 2, ... until it\n        returns None.  The completion should begin with 'text'.\n\n        Parameters\n        ----------\n          text : string\n            Text to perform the completion on.\n\n          state : int\n            Counter used by readline.", "docstring_tokens": ["Return", "the", "state", "-", "th", "possible", "completion", "for", "text", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/completer.py#L873-L933", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/runtime.py", "func_name": "Namespace.__refer_all", "original_string": "def __refer_all(cls, refers: lmap.Map, other_ns_interns: lmap.Map) -> lmap.Map:\n        \"\"\"Refer all _public_ interns from another namespace.\"\"\"\n        final_refers = refers\n        for entry in other_ns_interns:\n            s: sym.Symbol = entry.key\n            var: Var = entry.value\n            if not var.is_private:\n                final_refers = final_refers.assoc(s, var)\n        return final_refers", "language": "python", "code": "def __refer_all(cls, refers: lmap.Map, other_ns_interns: lmap.Map) -> lmap.Map:\n        \"\"\"Refer all _public_ interns from another namespace.\"\"\"\n        final_refers = refers\n        for entry in other_ns_interns:\n            s: sym.Symbol = entry.key\n            var: Var = entry.value\n            if not var.is_private:\n                final_refers = final_refers.assoc(s, var)\n        return final_refers", "code_tokens": ["def", "__refer_all", "(", "cls", ",", "refers", ":", "lmap", ".", "Map", ",", "other_ns_interns", ":", "lmap", ".", "Map", ")", "->", "lmap", ".", "Map", ":", "final_refers", "=", "refers", "for", "entry", "in", "other_ns_interns", ":", "s", ":", "sym", ".", "Symbol", "=", "entry", ".", "key", "var", ":", "Var", "=", "entry", ".", "value", "if", "not", "var", ".", "is_private", ":", "final_refers", "=", "final_refers", ".", "assoc", "(", "s", ",", "var", ")", "return", "final_refers"], "docstring": "Refer all _public_ interns from another namespace.", "docstring_tokens": ["Refer", "all", "_public_", "interns", "from", "another", "namespace", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/runtime.py#L517-L525", "partition": "test"}
{"repo": "funilrys/PyFunceble", "path": "PyFunceble/config.py", "func_name": "Load._install_directory_structure_file", "original_string": "def _install_directory_structure_file(cls):\n        \"\"\"\n        Download the latest version of `dir_structure_production.json`.\n        \"\"\"\n\n        # We initiate the link to the public suffix configuration.\n        # It is not hard coded because this method is called only if we\n        # are sure that the configuration file exist.\n        dir_structure_link = PyFunceble.CONFIGURATION[\"links\"][\"dir_structure\"]\n\n        # We update the link according to our current version.\n        dir_structure_link = Version(True).right_url_from_version(dir_structure_link)\n\n        # We set the destination of the downloaded file.\n        destination = (\n            PyFunceble.CURRENT_DIRECTORY\n            + PyFunceble.CONFIGURATION[\"outputs\"][\"default_files\"][\"dir_structure\"]\n        )\n\n        if not Version(True).is_cloned() or not PyFunceble.path.isfile(destination):\n            # The current version is not the cloned version.\n\n            # We Download the link content and return the download status.\n            data = Download(dir_structure_link, destination, return_data=True).text()\n\n            File(destination).write(data, overwrite=True)\n            return True\n\n        # We are in the cloned version.\n\n        # We do not need to download the file, so we are returning None.\n        return None", "language": "python", "code": "def _install_directory_structure_file(cls):\n        \"\"\"\n        Download the latest version of `dir_structure_production.json`.\n        \"\"\"\n\n        # We initiate the link to the public suffix configuration.\n        # It is not hard coded because this method is called only if we\n        # are sure that the configuration file exist.\n        dir_structure_link = PyFunceble.CONFIGURATION[\"links\"][\"dir_structure\"]\n\n        # We update the link according to our current version.\n        dir_structure_link = Version(True).right_url_from_version(dir_structure_link)\n\n        # We set the destination of the downloaded file.\n        destination = (\n            PyFunceble.CURRENT_DIRECTORY\n            + PyFunceble.CONFIGURATION[\"outputs\"][\"default_files\"][\"dir_structure\"]\n        )\n\n        if not Version(True).is_cloned() or not PyFunceble.path.isfile(destination):\n            # The current version is not the cloned version.\n\n            # We Download the link content and return the download status.\n            data = Download(dir_structure_link, destination, return_data=True).text()\n\n            File(destination).write(data, overwrite=True)\n            return True\n\n        # We are in the cloned version.\n\n        # We do not need to download the file, so we are returning None.\n        return None", "code_tokens": ["def", "_install_directory_structure_file", "(", "cls", ")", ":", "# We initiate the link to the public suffix configuration.", "# It is not hard coded because this method is called only if we", "# are sure that the configuration file exist.", "dir_structure_link", "=", "PyFunceble", ".", "CONFIGURATION", "[", "\"links\"", "]", "[", "\"dir_structure\"", "]", "# We update the link according to our current version.", "dir_structure_link", "=", "Version", "(", "True", ")", ".", "right_url_from_version", "(", "dir_structure_link", ")", "# We set the destination of the downloaded file.", "destination", "=", "(", "PyFunceble", ".", "CURRENT_DIRECTORY", "+", "PyFunceble", ".", "CONFIGURATION", "[", "\"outputs\"", "]", "[", "\"default_files\"", "]", "[", "\"dir_structure\"", "]", ")", "if", "not", "Version", "(", "True", ")", ".", "is_cloned", "(", ")", "or", "not", "PyFunceble", ".", "path", ".", "isfile", "(", "destination", ")", ":", "# The current version is not the cloned version.", "# We Download the link content and return the download status.", "data", "=", "Download", "(", "dir_structure_link", ",", "destination", ",", "return_data", "=", "True", ")", ".", "text", "(", ")", "File", "(", "destination", ")", ".", "write", "(", "data", ",", "overwrite", "=", "True", ")", "return", "True", "# We are in the cloned version.", "# We do not need to download the file, so we are returning None.", "return", "None"], "docstring": "Download the latest version of `dir_structure_production.json`.", "docstring_tokens": ["Download", "the", "latest", "version", "of", "dir_structure_production", ".", "json", "."], "sha": "cdf69cbde120199171f7158e1c33635753e6e2f5", "url": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/config.py#L352-L383", "partition": "test"}
{"repo": "LionelAuroux/pyrser", "path": "pyrser/dsl.py", "func_name": "hook_param", "original_string": "def hook_param(self, hook, p):\n    \"\"\"Parse a hook parameter\"\"\"\n    hook.listparam.append(p.pair)\n    return True", "language": "python", "code": "def hook_param(self, hook, p):\n    \"\"\"Parse a hook parameter\"\"\"\n    hook.listparam.append(p.pair)\n    return True", "code_tokens": ["def", "hook_param", "(", "self", ",", "hook", ",", "p", ")", ":", "hook", ".", "listparam", ".", "append", "(", "p", ".", "pair", ")", "return", "True"], "docstring": "Parse a hook parameter", "docstring_tokens": ["Parse", "a", "hook", "parameter"], "sha": "f153a97ef2b6bf915a1ed468c0252a9a59b754d5", "url": "https://github.com/LionelAuroux/pyrser/blob/f153a97ef2b6bf915a1ed468c0252a9a59b754d5/pyrser/dsl.py#L715-L718", "partition": "test"}
{"repo": "mblayman/httpony", "path": "httpony/application.py", "func_name": "make_app", "original_string": "def make_app():\n    \"\"\"Make a WSGI app that has all the HTTPie pieces baked in.\"\"\"\n    env = Environment()\n    # STDIN is ignored because HTTPony runs a server that doesn't care.\n    # Additionally, it is needed or else pytest blows up.\n    args = parser.parse_args(args=['/', '--ignore-stdin'], env=env)\n    args.output_options = 'HB'  # Output only requests.\n    server = 'HTTPony/{0}'.format(__version__)\n\n    def application(environ, start_response):\n        # The WSGI server puts content length and type in the environment\n        # even when not provided with the request. Drop them if they are empty.\n        if environ.get('CONTENT_LENGTH') == '':\n            del environ['CONTENT_LENGTH']\n        if environ.get('CONTENT_TYPE') == '':\n            del environ['CONTENT_TYPE']\n\n        wrequest = WerkzeugRequest(environ)\n        data = wrequest.get_data()\n        request = Request(\n            method=wrequest.method,\n            url=wrequest.url,\n            headers=wrequest.headers,\n            data=data,\n        )\n        prepared = request.prepare()\n\n        stream = streams.build_output_stream(\n            args, env, prepared, response=None,\n            output_options=args.output_options)\n        streams.write_stream(stream, env.stdout, env.stdout_isatty)\n\n        # When there is data in the request, give the next one breathing room.\n        if data:\n            print(\"\\n\", file=env.stdout)\n\n        # Make dreams come true.\n        response = Response(headers={'Server': server})\n        return response(environ, start_response)\n\n    return application", "language": "python", "code": "def make_app():\n    \"\"\"Make a WSGI app that has all the HTTPie pieces baked in.\"\"\"\n    env = Environment()\n    # STDIN is ignored because HTTPony runs a server that doesn't care.\n    # Additionally, it is needed or else pytest blows up.\n    args = parser.parse_args(args=['/', '--ignore-stdin'], env=env)\n    args.output_options = 'HB'  # Output only requests.\n    server = 'HTTPony/{0}'.format(__version__)\n\n    def application(environ, start_response):\n        # The WSGI server puts content length and type in the environment\n        # even when not provided with the request. Drop them if they are empty.\n        if environ.get('CONTENT_LENGTH') == '':\n            del environ['CONTENT_LENGTH']\n        if environ.get('CONTENT_TYPE') == '':\n            del environ['CONTENT_TYPE']\n\n        wrequest = WerkzeugRequest(environ)\n        data = wrequest.get_data()\n        request = Request(\n            method=wrequest.method,\n            url=wrequest.url,\n            headers=wrequest.headers,\n            data=data,\n        )\n        prepared = request.prepare()\n\n        stream = streams.build_output_stream(\n            args, env, prepared, response=None,\n            output_options=args.output_options)\n        streams.write_stream(stream, env.stdout, env.stdout_isatty)\n\n        # When there is data in the request, give the next one breathing room.\n        if data:\n            print(\"\\n\", file=env.stdout)\n\n        # Make dreams come true.\n        response = Response(headers={'Server': server})\n        return response(environ, start_response)\n\n    return application", "code_tokens": ["def", "make_app", "(", ")", ":", "env", "=", "Environment", "(", ")", "# STDIN is ignored because HTTPony runs a server that doesn't care.", "# Additionally, it is needed or else pytest blows up.", "args", "=", "parser", ".", "parse_args", "(", "args", "=", "[", "'/'", ",", "'--ignore-stdin'", "]", ",", "env", "=", "env", ")", "args", ".", "output_options", "=", "'HB'", "# Output only requests.", "server", "=", "'HTTPony/{0}'", ".", "format", "(", "__version__", ")", "def", "application", "(", "environ", ",", "start_response", ")", ":", "# The WSGI server puts content length and type in the environment", "# even when not provided with the request. Drop them if they are empty.", "if", "environ", ".", "get", "(", "'CONTENT_LENGTH'", ")", "==", "''", ":", "del", "environ", "[", "'CONTENT_LENGTH'", "]", "if", "environ", ".", "get", "(", "'CONTENT_TYPE'", ")", "==", "''", ":", "del", "environ", "[", "'CONTENT_TYPE'", "]", "wrequest", "=", "WerkzeugRequest", "(", "environ", ")", "data", "=", "wrequest", ".", "get_data", "(", ")", "request", "=", "Request", "(", "method", "=", "wrequest", ".", "method", ",", "url", "=", "wrequest", ".", "url", ",", "headers", "=", "wrequest", ".", "headers", ",", "data", "=", "data", ",", ")", "prepared", "=", "request", ".", "prepare", "(", ")", "stream", "=", "streams", ".", "build_output_stream", "(", "args", ",", "env", ",", "prepared", ",", "response", "=", "None", ",", "output_options", "=", "args", ".", "output_options", ")", "streams", ".", "write_stream", "(", "stream", ",", "env", ".", "stdout", ",", "env", ".", "stdout_isatty", ")", "# When there is data in the request, give the next one breathing room.", "if", "data", ":", "print", "(", "\"\\n\"", ",", "file", "=", "env", ".", "stdout", ")", "# Make dreams come true.", "response", "=", "Response", "(", "headers", "=", "{", "'Server'", ":", "server", "}", ")", "return", "response", "(", "environ", ",", "start_response", ")", "return", "application"], "docstring": "Make a WSGI app that has all the HTTPie pieces baked in.", "docstring_tokens": ["Make", "a", "WSGI", "app", "that", "has", "all", "the", "HTTPie", "pieces", "baked", "in", "."], "sha": "5af404d647a8dac8a043b64ea09882589b3b5247", "url": "https://github.com/mblayman/httpony/blob/5af404d647a8dac8a043b64ea09882589b3b5247/httpony/application.py#L15-L55", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/inputsplitter.py", "func_name": "IPythonInputSplitter._line_mode_cell_append", "original_string": "def _line_mode_cell_append(self, lines):\n        \"\"\"Append new content for a cell magic in line mode.\n        \"\"\"\n        # Only store the raw input.  Lines beyond the first one are only only\n        # stored for history purposes; for execution the caller will grab the\n        # magic pieces from cell_magic_parts and will assemble the cell body\n        self._store(lines, self._buffer_raw, 'source_raw')\n        self.cell_magic_parts.append(lines)\n        # Find out if the last stored block has a whitespace line as its\n        # last line and also this line is whitespace, case in which we're\n        # done (two contiguous blank lines signal termination).  Note that\n        # the storage logic *enforces* that every stored block is\n        # newline-terminated, so we grab everything but the last character\n        # so we can have the body of the block alone.\n        last_block = self.cell_magic_parts[-1]\n        self._is_complete = last_blank(last_block) and lines.isspace()\n        return self._is_complete", "language": "python", "code": "def _line_mode_cell_append(self, lines):\n        \"\"\"Append new content for a cell magic in line mode.\n        \"\"\"\n        # Only store the raw input.  Lines beyond the first one are only only\n        # stored for history purposes; for execution the caller will grab the\n        # magic pieces from cell_magic_parts and will assemble the cell body\n        self._store(lines, self._buffer_raw, 'source_raw')\n        self.cell_magic_parts.append(lines)\n        # Find out if the last stored block has a whitespace line as its\n        # last line and also this line is whitespace, case in which we're\n        # done (two contiguous blank lines signal termination).  Note that\n        # the storage logic *enforces* that every stored block is\n        # newline-terminated, so we grab everything but the last character\n        # so we can have the body of the block alone.\n        last_block = self.cell_magic_parts[-1]\n        self._is_complete = last_blank(last_block) and lines.isspace()\n        return self._is_complete", "code_tokens": ["def", "_line_mode_cell_append", "(", "self", ",", "lines", ")", ":", "# Only store the raw input.  Lines beyond the first one are only only", "# stored for history purposes; for execution the caller will grab the", "# magic pieces from cell_magic_parts and will assemble the cell body", "self", ".", "_store", "(", "lines", ",", "self", ".", "_buffer_raw", ",", "'source_raw'", ")", "self", ".", "cell_magic_parts", ".", "append", "(", "lines", ")", "# Find out if the last stored block has a whitespace line as its", "# last line and also this line is whitespace, case in which we're", "# done (two contiguous blank lines signal termination).  Note that", "# the storage logic *enforces* that every stored block is", "# newline-terminated, so we grab everything but the last character", "# so we can have the body of the block alone.", "last_block", "=", "self", ".", "cell_magic_parts", "[", "-", "1", "]", "self", ".", "_is_complete", "=", "last_blank", "(", "last_block", ")", "and", "lines", ".", "isspace", "(", ")", "return", "self", ".", "_is_complete"], "docstring": "Append new content for a cell magic in line mode.", "docstring_tokens": ["Append", "new", "content", "for", "a", "cell", "magic", "in", "line", "mode", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/inputsplitter.py#L798-L814", "partition": "test"}
{"repo": "RRZE-HPC/kerncraft", "path": "kerncraft/cacheprediction.py", "func_name": "CacheSimulationPredictor.get_infos", "original_string": "def get_infos(self):\n        \"\"\"Return verbose information about the predictor.\"\"\"\n        first_dim_factor = self.first_dim_factor\n        infos = {'memory hierarchy': [], 'cache stats': self.stats,\n                 'cachelines in stats': first_dim_factor}\n        for cache_level, cache_info in list(enumerate(self.machine['memory hierarchy'])):\n            infos['memory hierarchy'].append({\n                'index': len(infos['memory hierarchy']),\n                'level': '{}'.format(cache_info['level']),\n                'total loads': self.stats[cache_level]['LOAD_byte']/first_dim_factor,\n                'total misses': self.stats[cache_level]['MISS_byte']/first_dim_factor,\n                'total hits': self.stats[cache_level]['HIT_byte']/first_dim_factor,\n                'total stores': self.stats[cache_level]['STORE_byte']/first_dim_factor,\n                'total evicts': self.stats[cache_level]['EVICT_byte']/first_dim_factor,\n                'total lines load': self.stats[cache_level]['LOAD_count']/first_dim_factor,\n                'total lines misses': self.stats[cache_level]['MISS_count']/first_dim_factor,\n                'total lines hits': self.stats[cache_level]['HIT_count']/first_dim_factor,\n                'total lines stores': self.stats[cache_level]['STORE_count']/first_dim_factor,\n                'total lines evicts': self.stats[cache_level]['EVICT_count']/first_dim_factor,\n                'cycles': None})\n        return infos", "language": "python", "code": "def get_infos(self):\n        \"\"\"Return verbose information about the predictor.\"\"\"\n        first_dim_factor = self.first_dim_factor\n        infos = {'memory hierarchy': [], 'cache stats': self.stats,\n                 'cachelines in stats': first_dim_factor}\n        for cache_level, cache_info in list(enumerate(self.machine['memory hierarchy'])):\n            infos['memory hierarchy'].append({\n                'index': len(infos['memory hierarchy']),\n                'level': '{}'.format(cache_info['level']),\n                'total loads': self.stats[cache_level]['LOAD_byte']/first_dim_factor,\n                'total misses': self.stats[cache_level]['MISS_byte']/first_dim_factor,\n                'total hits': self.stats[cache_level]['HIT_byte']/first_dim_factor,\n                'total stores': self.stats[cache_level]['STORE_byte']/first_dim_factor,\n                'total evicts': self.stats[cache_level]['EVICT_byte']/first_dim_factor,\n                'total lines load': self.stats[cache_level]['LOAD_count']/first_dim_factor,\n                'total lines misses': self.stats[cache_level]['MISS_count']/first_dim_factor,\n                'total lines hits': self.stats[cache_level]['HIT_count']/first_dim_factor,\n                'total lines stores': self.stats[cache_level]['STORE_count']/first_dim_factor,\n                'total lines evicts': self.stats[cache_level]['EVICT_count']/first_dim_factor,\n                'cycles': None})\n        return infos", "code_tokens": ["def", "get_infos", "(", "self", ")", ":", "first_dim_factor", "=", "self", ".", "first_dim_factor", "infos", "=", "{", "'memory hierarchy'", ":", "[", "]", ",", "'cache stats'", ":", "self", ".", "stats", ",", "'cachelines in stats'", ":", "first_dim_factor", "}", "for", "cache_level", ",", "cache_info", "in", "list", "(", "enumerate", "(", "self", ".", "machine", "[", "'memory hierarchy'", "]", ")", ")", ":", "infos", "[", "'memory hierarchy'", "]", ".", "append", "(", "{", "'index'", ":", "len", "(", "infos", "[", "'memory hierarchy'", "]", ")", ",", "'level'", ":", "'{}'", ".", "format", "(", "cache_info", "[", "'level'", "]", ")", ",", "'total loads'", ":", "self", ".", "stats", "[", "cache_level", "]", "[", "'LOAD_byte'", "]", "/", "first_dim_factor", ",", "'total misses'", ":", "self", ".", "stats", "[", "cache_level", "]", "[", "'MISS_byte'", "]", "/", "first_dim_factor", ",", "'total hits'", ":", "self", ".", "stats", "[", "cache_level", "]", "[", "'HIT_byte'", "]", "/", "first_dim_factor", ",", "'total stores'", ":", "self", ".", "stats", "[", "cache_level", "]", "[", "'STORE_byte'", "]", "/", "first_dim_factor", ",", "'total evicts'", ":", "self", ".", "stats", "[", "cache_level", "]", "[", "'EVICT_byte'", "]", "/", "first_dim_factor", ",", "'total lines load'", ":", "self", ".", "stats", "[", "cache_level", "]", "[", "'LOAD_count'", "]", "/", "first_dim_factor", ",", "'total lines misses'", ":", "self", ".", "stats", "[", "cache_level", "]", "[", "'MISS_count'", "]", "/", "first_dim_factor", ",", "'total lines hits'", ":", "self", ".", "stats", "[", "cache_level", "]", "[", "'HIT_count'", "]", "/", "first_dim_factor", ",", "'total lines stores'", ":", "self", ".", "stats", "[", "cache_level", "]", "[", "'STORE_count'", "]", "/", "first_dim_factor", ",", "'total lines evicts'", ":", "self", ".", "stats", "[", "cache_level", "]", "[", "'EVICT_count'", "]", "/", "first_dim_factor", ",", "'cycles'", ":", "None", "}", ")", "return", "infos"], "docstring": "Return verbose information about the predictor.", "docstring_tokens": ["Return", "verbose", "information", "about", "the", "predictor", "."], "sha": "c60baf8043e4da8d8d66da7575021c2f4c6c78af", "url": "https://github.com/RRZE-HPC/kerncraft/blob/c60baf8043e4da8d8d66da7575021c2f4c6c78af/kerncraft/cacheprediction.py#L511-L531", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/base.py", "func_name": "BasicChecker.open", "original_string": "def open(self):\n        \"\"\"initialize visit variables and statistics\n        \"\"\"\n        self._tryfinallys = []\n        self.stats = self.linter.add_stats(module=0, function=0, method=0, class_=0)", "language": "python", "code": "def open(self):\n        \"\"\"initialize visit variables and statistics\n        \"\"\"\n        self._tryfinallys = []\n        self.stats = self.linter.add_stats(module=0, function=0, method=0, class_=0)", "code_tokens": ["def", "open", "(", "self", ")", ":", "self", ".", "_tryfinallys", "=", "[", "]", "self", ".", "stats", "=", "self", ".", "linter", ".", "add_stats", "(", "module", "=", "0", ",", "function", "=", "0", ",", "method", "=", "0", ",", "class_", "=", "0", ")"], "docstring": "initialize visit variables and statistics", "docstring_tokens": ["initialize", "visit", "variables", "and", "statistics"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/base.py#L997-L1001", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/extensions/initializer.py", "func_name": "initialize", "original_string": "def initialize(self, params, qubits):\n    \"\"\"Apply initialize to circuit.\"\"\"\n    if isinstance(qubits, QuantumRegister):\n        qubits = qubits[:]\n    else:\n        qubits = _convert_to_bits([qubits], [qbit for qreg in self.qregs for qbit in qreg])[0]\n    return self.append(Initialize(params), qubits)", "language": "python", "code": "def initialize(self, params, qubits):\n    \"\"\"Apply initialize to circuit.\"\"\"\n    if isinstance(qubits, QuantumRegister):\n        qubits = qubits[:]\n    else:\n        qubits = _convert_to_bits([qubits], [qbit for qreg in self.qregs for qbit in qreg])[0]\n    return self.append(Initialize(params), qubits)", "code_tokens": ["def", "initialize", "(", "self", ",", "params", ",", "qubits", ")", ":", "if", "isinstance", "(", "qubits", ",", "QuantumRegister", ")", ":", "qubits", "=", "qubits", "[", ":", "]", "else", ":", "qubits", "=", "_convert_to_bits", "(", "[", "qubits", "]", ",", "[", "qbit", "for", "qreg", "in", "self", ".", "qregs", "for", "qbit", "in", "qreg", "]", ")", "[", "0", "]", "return", "self", ".", "append", "(", "Initialize", "(", "params", ")", ",", "qubits", ")"], "docstring": "Apply initialize to circuit.", "docstring_tokens": ["Apply", "initialize", "to", "circuit", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/extensions/initializer.py#L236-L242", "partition": "test"}
{"repo": "quantopian/pgcontents", "path": "pgcontents/pgmanager.py", "func_name": "PostgresContentsManager.get_file_id", "original_string": "def get_file_id(self, path):\n        \"\"\"\n        Get the id of a file in the database.  This function is specific to\n        this implementation of ContentsManager and is not in the base class.\n        \"\"\"\n        with self.engine.begin() as db:\n            try:\n                file_id = get_file_id(db, self.user_id, path)\n            except NoSuchFile:\n                self.no_such_entity(path)\n\n        return file_id", "language": "python", "code": "def get_file_id(self, path):\n        \"\"\"\n        Get the id of a file in the database.  This function is specific to\n        this implementation of ContentsManager and is not in the base class.\n        \"\"\"\n        with self.engine.begin() as db:\n            try:\n                file_id = get_file_id(db, self.user_id, path)\n            except NoSuchFile:\n                self.no_such_entity(path)\n\n        return file_id", "code_tokens": ["def", "get_file_id", "(", "self", ",", "path", ")", ":", "with", "self", ".", "engine", ".", "begin", "(", ")", "as", "db", ":", "try", ":", "file_id", "=", "get_file_id", "(", "db", ",", "self", ".", "user_id", ",", "path", ")", "except", "NoSuchFile", ":", "self", ".", "no_such_entity", "(", "path", ")", "return", "file_id"], "docstring": "Get the id of a file in the database.  This function is specific to\n        this implementation of ContentsManager and is not in the base class.", "docstring_tokens": ["Get", "the", "id", "of", "a", "file", "in", "the", "database", ".", "This", "function", "is", "specific", "to", "this", "implementation", "of", "ContentsManager", "and", "is", "not", "in", "the", "base", "class", "."], "sha": "ed36268b7917332d16868208e1e565742a8753e1", "url": "https://github.com/quantopian/pgcontents/blob/ed36268b7917332d16868208e1e565742a8753e1/pgcontents/pgmanager.py#L166-L177", "partition": "test"}
{"repo": "Yelp/py_zipkin", "path": "py_zipkin/encoding/_decoders.py", "func_name": "_V1ThriftDecoder._convert_trace_id_to_string", "original_string": "def _convert_trace_id_to_string(self, trace_id, trace_id_high=None):\n        \"\"\"\n        Converts the provided traceId hex value with optional high bits\n        to a string.\n\n        :param trace_id: the value of the trace ID\n        :type trace_id: int\n        :param trace_id_high: the high bits of the trace ID\n        :type trace_id: int\n        :returns: trace_id_high + trace_id as a string\n        \"\"\"\n        if trace_id_high is not None:\n            result = bytearray(32)\n            self._write_hex_long(result, 0, trace_id_high)\n            self._write_hex_long(result, 16, trace_id)\n            return result.decode(\"utf8\")\n\n        result = bytearray(16)\n        self._write_hex_long(result, 0, trace_id)\n        return result.decode(\"utf8\")", "language": "python", "code": "def _convert_trace_id_to_string(self, trace_id, trace_id_high=None):\n        \"\"\"\n        Converts the provided traceId hex value with optional high bits\n        to a string.\n\n        :param trace_id: the value of the trace ID\n        :type trace_id: int\n        :param trace_id_high: the high bits of the trace ID\n        :type trace_id: int\n        :returns: trace_id_high + trace_id as a string\n        \"\"\"\n        if trace_id_high is not None:\n            result = bytearray(32)\n            self._write_hex_long(result, 0, trace_id_high)\n            self._write_hex_long(result, 16, trace_id)\n            return result.decode(\"utf8\")\n\n        result = bytearray(16)\n        self._write_hex_long(result, 0, trace_id)\n        return result.decode(\"utf8\")", "code_tokens": ["def", "_convert_trace_id_to_string", "(", "self", ",", "trace_id", ",", "trace_id_high", "=", "None", ")", ":", "if", "trace_id_high", "is", "not", "None", ":", "result", "=", "bytearray", "(", "32", ")", "self", ".", "_write_hex_long", "(", "result", ",", "0", ",", "trace_id_high", ")", "self", ".", "_write_hex_long", "(", "result", ",", "16", ",", "trace_id", ")", "return", "result", ".", "decode", "(", "\"utf8\"", ")", "result", "=", "bytearray", "(", "16", ")", "self", ".", "_write_hex_long", "(", "result", ",", "0", ",", "trace_id", ")", "return", "result", ".", "decode", "(", "\"utf8\"", ")"], "docstring": "Converts the provided traceId hex value with optional high bits\n        to a string.\n\n        :param trace_id: the value of the trace ID\n        :type trace_id: int\n        :param trace_id_high: the high bits of the trace ID\n        :type trace_id: int\n        :returns: trace_id_high + trace_id as a string", "docstring_tokens": ["Converts", "the", "provided", "traceId", "hex", "value", "with", "optional", "high", "bits", "to", "a", "string", "."], "sha": "0944d9a3fb1f1798dbb276694aeed99f2b4283ba", "url": "https://github.com/Yelp/py_zipkin/blob/0944d9a3fb1f1798dbb276694aeed99f2b4283ba/py_zipkin/encoding/_decoders.py#L237-L256", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/parallel/client/client.py", "func_name": "Client.hub_history", "original_string": "def hub_history(self):\n        \"\"\"Get the Hub's history\n\n        Just like the Client, the Hub has a history, which is a list of msg_ids.\n        This will contain the history of all clients, and, depending on configuration,\n        may contain history across multiple cluster sessions.\n\n        Any msg_id returned here is a valid argument to `get_result`.\n\n        Returns\n        -------\n\n        msg_ids : list of strs\n                list of all msg_ids, ordered by task submission time.\n        \"\"\"\n\n        self.session.send(self._query_socket, \"history_request\", content={})\n        idents, msg = self.session.recv(self._query_socket, 0)\n\n        if self.debug:\n            pprint(msg)\n        content = msg['content']\n        if content['status'] != 'ok':\n            raise self._unwrap_exception(content)\n        else:\n            return content['history']", "language": "python", "code": "def hub_history(self):\n        \"\"\"Get the Hub's history\n\n        Just like the Client, the Hub has a history, which is a list of msg_ids.\n        This will contain the history of all clients, and, depending on configuration,\n        may contain history across multiple cluster sessions.\n\n        Any msg_id returned here is a valid argument to `get_result`.\n\n        Returns\n        -------\n\n        msg_ids : list of strs\n                list of all msg_ids, ordered by task submission time.\n        \"\"\"\n\n        self.session.send(self._query_socket, \"history_request\", content={})\n        idents, msg = self.session.recv(self._query_socket, 0)\n\n        if self.debug:\n            pprint(msg)\n        content = msg['content']\n        if content['status'] != 'ok':\n            raise self._unwrap_exception(content)\n        else:\n            return content['history']", "code_tokens": ["def", "hub_history", "(", "self", ")", ":", "self", ".", "session", ".", "send", "(", "self", ".", "_query_socket", ",", "\"history_request\"", ",", "content", "=", "{", "}", ")", "idents", ",", "msg", "=", "self", ".", "session", ".", "recv", "(", "self", ".", "_query_socket", ",", "0", ")", "if", "self", ".", "debug", ":", "pprint", "(", "msg", ")", "content", "=", "msg", "[", "'content'", "]", "if", "content", "[", "'status'", "]", "!=", "'ok'", ":", "raise", "self", ".", "_unwrap_exception", "(", "content", ")", "else", ":", "return", "content", "[", "'history'", "]"], "docstring": "Get the Hub's history\n\n        Just like the Client, the Hub has a history, which is a list of msg_ids.\n        This will contain the history of all clients, and, depending on configuration,\n        may contain history across multiple cluster sessions.\n\n        Any msg_id returned here is a valid argument to `get_result`.\n\n        Returns\n        -------\n\n        msg_ids : list of strs\n                list of all msg_ids, ordered by task submission time.", "docstring_tokens": ["Get", "the", "Hub", "s", "history"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/parallel/client/client.py#L1650-L1675", "partition": "test"}
{"repo": "Jaza/s3-saver", "path": "s3_saver.py", "func_name": "S3Saver._delete_local", "original_string": "def _delete_local(self, filename):\n        \"\"\"Deletes the specified file from the local filesystem.\"\"\"\n\n        if os.path.exists(filename):\n            os.remove(filename)", "language": "python", "code": "def _delete_local(self, filename):\n        \"\"\"Deletes the specified file from the local filesystem.\"\"\"\n\n        if os.path.exists(filename):\n            os.remove(filename)", "code_tokens": ["def", "_delete_local", "(", "self", ",", "filename", ")", ":", "if", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "os", ".", "remove", "(", "filename", ")"], "docstring": "Deletes the specified file from the local filesystem.", "docstring_tokens": ["Deletes", "the", "specified", "file", "from", "the", "local", "filesystem", "."], "sha": "81dc4447d76c2fc0b0238fb96fa70e879612e355", "url": "https://github.com/Jaza/s3-saver/blob/81dc4447d76c2fc0b0238fb96fa70e879612e355/s3_saver.py#L53-L57", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/classes.py", "func_name": "ClassChecker._is_mandatory_method_param", "original_string": "def _is_mandatory_method_param(self, node):\n        \"\"\"Check if astroid.Name corresponds to first attribute variable name\n\n        Name is `self` for method, `cls` for classmethod and `mcs` for metaclass.\n        \"\"\"\n        return (\n            self._first_attrs\n            and isinstance(node, astroid.Name)\n            and node.name == self._first_attrs[-1]\n        )", "language": "python", "code": "def _is_mandatory_method_param(self, node):\n        \"\"\"Check if astroid.Name corresponds to first attribute variable name\n\n        Name is `self` for method, `cls` for classmethod and `mcs` for metaclass.\n        \"\"\"\n        return (\n            self._first_attrs\n            and isinstance(node, astroid.Name)\n            and node.name == self._first_attrs[-1]\n        )", "code_tokens": ["def", "_is_mandatory_method_param", "(", "self", ",", "node", ")", ":", "return", "(", "self", ".", "_first_attrs", "and", "isinstance", "(", "node", ",", "astroid", ".", "Name", ")", "and", "node", ".", "name", "==", "self", ".", "_first_attrs", "[", "-", "1", "]", ")"], "docstring": "Check if astroid.Name corresponds to first attribute variable name\n\n        Name is `self` for method, `cls` for classmethod and `mcs` for metaclass.", "docstring_tokens": ["Check", "if", "astroid", ".", "Name", "corresponds", "to", "first", "attribute", "variable", "name"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/classes.py#L1590-L1599", "partition": "test"}
{"repo": "celiao/tmdbsimple", "path": "tmdbsimple/search.py", "func_name": "Search.keyword", "original_string": "def keyword(self, **kwargs):\n        \"\"\"\n        Search for keywords by name.\n\n        Args:\n            query: CGI escpaed string.\n            page: (optional) Minimum value of 1. Expected value is an integer.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_path('keyword')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response", "language": "python", "code": "def keyword(self, **kwargs):\n        \"\"\"\n        Search for keywords by name.\n\n        Args:\n            query: CGI escpaed string.\n            page: (optional) Minimum value of 1. Expected value is an integer.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_path('keyword')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response", "code_tokens": ["def", "keyword", "(", "self", ",", "*", "*", "kwargs", ")", ":", "path", "=", "self", ".", "_get_path", "(", "'keyword'", ")", "response", "=", "self", ".", "_GET", "(", "path", ",", "kwargs", ")", "self", ".", "_set_attrs_to_values", "(", "response", ")", "return", "response"], "docstring": "Search for keywords by name.\n\n        Args:\n            query: CGI escpaed string.\n            page: (optional) Minimum value of 1. Expected value is an integer.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.", "docstring_tokens": ["Search", "for", "keywords", "by", "name", "."], "sha": "ff17893110c99771d6398a62c35d36dd9735f4b9", "url": "https://github.com/celiao/tmdbsimple/blob/ff17893110c99771d6398a62c35d36dd9735f4b9/tmdbsimple/search.py#L149-L164", "partition": "test"}
{"repo": "mrsarm/mongotail", "path": "mongotail/err.py", "func_name": "error", "original_string": "def error(msg, exit_code):\n    \"\"\"\n    Print `msg` error and exit with status `exit_code`\n    \"\"\"\n    sys.stderr.write(\"%s\\ntry 'mongotail --help' for more information\\n\" % msg)\n    sys.stderr.flush()\n    exit(exit_code)", "language": "python", "code": "def error(msg, exit_code):\n    \"\"\"\n    Print `msg` error and exit with status `exit_code`\n    \"\"\"\n    sys.stderr.write(\"%s\\ntry 'mongotail --help' for more information\\n\" % msg)\n    sys.stderr.flush()\n    exit(exit_code)", "code_tokens": ["def", "error", "(", "msg", ",", "exit_code", ")", ":", "sys", ".", "stderr", ".", "write", "(", "\"%s\\ntry 'mongotail --help' for more information\\n\"", "%", "msg", ")", "sys", ".", "stderr", ".", "flush", "(", ")", "exit", "(", "exit_code", ")"], "docstring": "Print `msg` error and exit with status `exit_code`", "docstring_tokens": ["Print", "msg", "error", "and", "exit", "with", "status", "exit_code"], "sha": "82ba74e32eff92faa320833a8d19c58555f9cd49", "url": "https://github.com/mrsarm/mongotail/blob/82ba74e32eff92faa320833a8d19c58555f9cd49/mongotail/err.py#L33-L39", "partition": "test"}
{"repo": "opencast/pyCA", "path": "pyca/ui/jsonapi.py", "func_name": "event", "original_string": "def event(uid):\n    '''Return a specific events JSON\n    '''\n    db = get_session()\n    event = db.query(RecordedEvent).filter(RecordedEvent.uid == uid).first() \\\n        or db.query(UpcomingEvent).filter(UpcomingEvent.uid == uid).first()\n\n    if event:\n        return make_data_response(event.serialize())\n    return make_error_response('No event with specified uid', 404)", "language": "python", "code": "def event(uid):\n    '''Return a specific events JSON\n    '''\n    db = get_session()\n    event = db.query(RecordedEvent).filter(RecordedEvent.uid == uid).first() \\\n        or db.query(UpcomingEvent).filter(UpcomingEvent.uid == uid).first()\n\n    if event:\n        return make_data_response(event.serialize())\n    return make_error_response('No event with specified uid', 404)", "code_tokens": ["def", "event", "(", "uid", ")", ":", "db", "=", "get_session", "(", ")", "event", "=", "db", ".", "query", "(", "RecordedEvent", ")", ".", "filter", "(", "RecordedEvent", ".", "uid", "==", "uid", ")", ".", "first", "(", ")", "or", "db", ".", "query", "(", "UpcomingEvent", ")", ".", "filter", "(", "UpcomingEvent", ".", "uid", "==", "uid", ")", ".", "first", "(", ")", "if", "event", ":", "return", "make_data_response", "(", "event", ".", "serialize", "(", ")", ")", "return", "make_error_response", "(", "'No event with specified uid'", ",", "404", ")"], "docstring": "Return a specific events JSON", "docstring_tokens": ["Return", "a", "specific", "events", "JSON"], "sha": "c89b168d4780d157e1b3f7676628c1b131956a88", "url": "https://github.com/opencast/pyCA/blob/c89b168d4780d157e1b3f7676628c1b131956a88/pyca/ui/jsonapi.py#L69-L78", "partition": "test"}
{"repo": "librosa/librosa", "path": "examples/adjust_tuning.py", "func_name": "adjust_tuning", "original_string": "def adjust_tuning(input_file, output_file):\n    '''Load audio, estimate tuning, apply pitch correction, and save.'''\n    print('Loading ', input_file)\n    y, sr = librosa.load(input_file)\n\n    print('Separating harmonic component ... ')\n    y_harm = librosa.effects.harmonic(y)\n\n    print('Estimating tuning ... ')\n    # Just track the pitches associated with high magnitude\n    tuning = librosa.estimate_tuning(y=y_harm, sr=sr)\n\n    print('{:+0.2f} cents'.format(100 * tuning))\n    print('Applying pitch-correction of {:+0.2f} cents'.format(-100 * tuning))\n    y_tuned = librosa.effects.pitch_shift(y, sr, -tuning)\n\n    print('Saving tuned audio to: ', output_file)\n    librosa.output.write_wav(output_file, y_tuned, sr)", "language": "python", "code": "def adjust_tuning(input_file, output_file):\n    '''Load audio, estimate tuning, apply pitch correction, and save.'''\n    print('Loading ', input_file)\n    y, sr = librosa.load(input_file)\n\n    print('Separating harmonic component ... ')\n    y_harm = librosa.effects.harmonic(y)\n\n    print('Estimating tuning ... ')\n    # Just track the pitches associated with high magnitude\n    tuning = librosa.estimate_tuning(y=y_harm, sr=sr)\n\n    print('{:+0.2f} cents'.format(100 * tuning))\n    print('Applying pitch-correction of {:+0.2f} cents'.format(-100 * tuning))\n    y_tuned = librosa.effects.pitch_shift(y, sr, -tuning)\n\n    print('Saving tuned audio to: ', output_file)\n    librosa.output.write_wav(output_file, y_tuned, sr)", "code_tokens": ["def", "adjust_tuning", "(", "input_file", ",", "output_file", ")", ":", "print", "(", "'Loading '", ",", "input_file", ")", "y", ",", "sr", "=", "librosa", ".", "load", "(", "input_file", ")", "print", "(", "'Separating harmonic component ... '", ")", "y_harm", "=", "librosa", ".", "effects", ".", "harmonic", "(", "y", ")", "print", "(", "'Estimating tuning ... '", ")", "# Just track the pitches associated with high magnitude", "tuning", "=", "librosa", ".", "estimate_tuning", "(", "y", "=", "y_harm", ",", "sr", "=", "sr", ")", "print", "(", "'{:+0.2f} cents'", ".", "format", "(", "100", "*", "tuning", ")", ")", "print", "(", "'Applying pitch-correction of {:+0.2f} cents'", ".", "format", "(", "-", "100", "*", "tuning", ")", ")", "y_tuned", "=", "librosa", ".", "effects", ".", "pitch_shift", "(", "y", ",", "sr", ",", "-", "tuning", ")", "print", "(", "'Saving tuned audio to: '", ",", "output_file", ")", "librosa", ".", "output", ".", "write_wav", "(", "output_file", ",", "y_tuned", ",", "sr", ")"], "docstring": "Load audio, estimate tuning, apply pitch correction, and save.", "docstring_tokens": ["Load", "audio", "estimate", "tuning", "apply", "pitch", "correction", "and", "save", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/examples/adjust_tuning.py#L15-L32", "partition": "test"}
{"repo": "neurodata/ndio", "path": "ndio/remote/ndingest.py", "func_name": "NDIngest.channel_dict", "original_string": "def channel_dict(self, channel_name, datatype, channel_type, data_url,\n                     file_format, file_type, exceptions, resolution,\n                     windowrange, readonly):\n        \"\"\"\n        Generate the project dictionary.\n        \"\"\"\n        channel_dict = {}\n        channel_dict['channel_name'] = channel_name\n        channel_dict['datatype'] = datatype\n        channel_dict['channel_type'] = channel_type\n        if exceptions is not None:\n            channel_dict['exceptions'] = exceptions\n        if resolution is not None:\n            channel_dict['resolution'] = resolution\n        if windowrange is not None:\n            channel_dict['windowrange'] = windowrange\n        if readonly is not None:\n            channel_dict['readonly'] = readonly\n        channel_dict['data_url'] = data_url\n        channel_dict['file_format'] = file_format\n        channel_dict['file_type'] = file_type\n        return channel_dict", "language": "python", "code": "def channel_dict(self, channel_name, datatype, channel_type, data_url,\n                     file_format, file_type, exceptions, resolution,\n                     windowrange, readonly):\n        \"\"\"\n        Generate the project dictionary.\n        \"\"\"\n        channel_dict = {}\n        channel_dict['channel_name'] = channel_name\n        channel_dict['datatype'] = datatype\n        channel_dict['channel_type'] = channel_type\n        if exceptions is not None:\n            channel_dict['exceptions'] = exceptions\n        if resolution is not None:\n            channel_dict['resolution'] = resolution\n        if windowrange is not None:\n            channel_dict['windowrange'] = windowrange\n        if readonly is not None:\n            channel_dict['readonly'] = readonly\n        channel_dict['data_url'] = data_url\n        channel_dict['file_format'] = file_format\n        channel_dict['file_type'] = file_type\n        return channel_dict", "code_tokens": ["def", "channel_dict", "(", "self", ",", "channel_name", ",", "datatype", ",", "channel_type", ",", "data_url", ",", "file_format", ",", "file_type", ",", "exceptions", ",", "resolution", ",", "windowrange", ",", "readonly", ")", ":", "channel_dict", "=", "{", "}", "channel_dict", "[", "'channel_name'", "]", "=", "channel_name", "channel_dict", "[", "'datatype'", "]", "=", "datatype", "channel_dict", "[", "'channel_type'", "]", "=", "channel_type", "if", "exceptions", "is", "not", "None", ":", "channel_dict", "[", "'exceptions'", "]", "=", "exceptions", "if", "resolution", "is", "not", "None", ":", "channel_dict", "[", "'resolution'", "]", "=", "resolution", "if", "windowrange", "is", "not", "None", ":", "channel_dict", "[", "'windowrange'", "]", "=", "windowrange", "if", "readonly", "is", "not", "None", ":", "channel_dict", "[", "'readonly'", "]", "=", "readonly", "channel_dict", "[", "'data_url'", "]", "=", "data_url", "channel_dict", "[", "'file_format'", "]", "=", "file_format", "channel_dict", "[", "'file_type'", "]", "=", "file_type", "return", "channel_dict"], "docstring": "Generate the project dictionary.", "docstring_tokens": ["Generate", "the", "project", "dictionary", "."], "sha": "792dd5816bc770b05a3db2f4327da42ff6253531", "url": "https://github.com/neurodata/ndio/blob/792dd5816bc770b05a3db2f4327da42ff6253531/ndio/remote/ndingest.py#L238-L259", "partition": "test"}
{"repo": "assemblerflow/flowcraft", "path": "flowcraft/templates/fastqc_report.py", "func_name": "get_summary", "original_string": "def get_summary(summary_file):\n    \"\"\"Parses a FastQC summary report file and returns it as a dictionary.\n\n    This function parses a typical FastQC summary report file, retrieving\n    only the information on the first two columns. For instance, a line could\n    be::\n\n        'PASS\tBasic Statistics\tSH10762A_1.fastq.gz'\n\n    This parser will build a dictionary with the string in the second column\n    as a key and the QC result as the value. In this case, the returned\n    ``dict`` would be something like::\n\n        {\"Basic Statistics\": \"PASS\"}\n\n    Parameters\n    ----------\n    summary_file: str\n        Path to FastQC summary report.\n\n    Returns\n    -------\n    summary_info: :py:data:`OrderedDict`\n        Returns the information of the FastQC summary report as an ordered\n        dictionary, with the categories as strings and the QC result as values.\n\n    \"\"\"\n\n    summary_info = OrderedDict()\n    logger.debug(\"Retrieving summary information from file: {}\".format(\n        summary_file))\n\n    with open(summary_file) as fh:\n        for line in fh:\n            # Skip empty lines\n            if not line.strip():\n                continue\n            # Populate summary info\n            fields = [x.strip() for x in line.split(\"\\t\")]\n            summary_info[fields[1]] = fields[0]\n\n    logger.debug(\"Retrieved summary information from file: {}\".format(\n        summary_info))\n\n    return summary_info", "language": "python", "code": "def get_summary(summary_file):\n    \"\"\"Parses a FastQC summary report file and returns it as a dictionary.\n\n    This function parses a typical FastQC summary report file, retrieving\n    only the information on the first two columns. For instance, a line could\n    be::\n\n        'PASS\tBasic Statistics\tSH10762A_1.fastq.gz'\n\n    This parser will build a dictionary with the string in the second column\n    as a key and the QC result as the value. In this case, the returned\n    ``dict`` would be something like::\n\n        {\"Basic Statistics\": \"PASS\"}\n\n    Parameters\n    ----------\n    summary_file: str\n        Path to FastQC summary report.\n\n    Returns\n    -------\n    summary_info: :py:data:`OrderedDict`\n        Returns the information of the FastQC summary report as an ordered\n        dictionary, with the categories as strings and the QC result as values.\n\n    \"\"\"\n\n    summary_info = OrderedDict()\n    logger.debug(\"Retrieving summary information from file: {}\".format(\n        summary_file))\n\n    with open(summary_file) as fh:\n        for line in fh:\n            # Skip empty lines\n            if not line.strip():\n                continue\n            # Populate summary info\n            fields = [x.strip() for x in line.split(\"\\t\")]\n            summary_info[fields[1]] = fields[0]\n\n    logger.debug(\"Retrieved summary information from file: {}\".format(\n        summary_info))\n\n    return summary_info", "code_tokens": ["def", "get_summary", "(", "summary_file", ")", ":", "summary_info", "=", "OrderedDict", "(", ")", "logger", ".", "debug", "(", "\"Retrieving summary information from file: {}\"", ".", "format", "(", "summary_file", ")", ")", "with", "open", "(", "summary_file", ")", "as", "fh", ":", "for", "line", "in", "fh", ":", "# Skip empty lines", "if", "not", "line", ".", "strip", "(", ")", ":", "continue", "# Populate summary info", "fields", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "line", ".", "split", "(", "\"\\t\"", ")", "]", "summary_info", "[", "fields", "[", "1", "]", "]", "=", "fields", "[", "0", "]", "logger", ".", "debug", "(", "\"Retrieved summary information from file: {}\"", ".", "format", "(", "summary_info", ")", ")", "return", "summary_info"], "docstring": "Parses a FastQC summary report file and returns it as a dictionary.\n\n    This function parses a typical FastQC summary report file, retrieving\n    only the information on the first two columns. For instance, a line could\n    be::\n\n        'PASS\tBasic Statistics\tSH10762A_1.fastq.gz'\n\n    This parser will build a dictionary with the string in the second column\n    as a key and the QC result as the value. In this case, the returned\n    ``dict`` would be something like::\n\n        {\"Basic Statistics\": \"PASS\"}\n\n    Parameters\n    ----------\n    summary_file: str\n        Path to FastQC summary report.\n\n    Returns\n    -------\n    summary_info: :py:data:`OrderedDict`\n        Returns the information of the FastQC summary report as an ordered\n        dictionary, with the categories as strings and the QC result as values.", "docstring_tokens": ["Parses", "a", "FastQC", "summary", "report", "file", "and", "returns", "it", "as", "a", "dictionary", "."], "sha": "fc3f4bddded1efc76006600016dc71a06dd908c0", "url": "https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/templates/fastqc_report.py#L362-L406", "partition": "test"}
{"repo": "c-soft/satel_integra", "path": "satel_integra/satel_integra.py", "func_name": "AsyncSatel.start_monitoring", "original_string": "async def start_monitoring(self):\n        \"\"\"Start monitoring for interesting events.\"\"\"\n        data = generate_query(\n            b'\\x7F\\x01\\xDC\\x99\\x80\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00')\n\n        await self._send_data(data)\n        resp = await self._read_data()\n\n        if resp is None:\n            _LOGGER.warning(\"Start monitoring - no data!\")\n            return\n\n        if resp[1:2] != b'\\xFF':\n            _LOGGER.warning(\"Monitoring not accepted.\")", "language": "python", "code": "async def start_monitoring(self):\n        \"\"\"Start monitoring for interesting events.\"\"\"\n        data = generate_query(\n            b'\\x7F\\x01\\xDC\\x99\\x80\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00')\n\n        await self._send_data(data)\n        resp = await self._read_data()\n\n        if resp is None:\n            _LOGGER.warning(\"Start monitoring - no data!\")\n            return\n\n        if resp[1:2] != b'\\xFF':\n            _LOGGER.warning(\"Monitoring not accepted.\")", "code_tokens": ["async", "def", "start_monitoring", "(", "self", ")", ":", "data", "=", "generate_query", "(", "b'\\x7F\\x01\\xDC\\x99\\x80\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00'", ")", "await", "self", ".", "_send_data", "(", "data", ")", "resp", "=", "await", "self", ".", "_read_data", "(", ")", "if", "resp", "is", "None", ":", "_LOGGER", ".", "warning", "(", "\"Start monitoring - no data!\"", ")", "return", "if", "resp", "[", "1", ":", "2", "]", "!=", "b'\\xFF'", ":", "_LOGGER", ".", "warning", "(", "\"Monitoring not accepted.\"", ")"], "docstring": "Start monitoring for interesting events.", "docstring_tokens": ["Start", "monitoring", "for", "interesting", "events", "."], "sha": "3b6d2020d1e10dc5aa40f30ee4ecc0f3a053eb3c", "url": "https://github.com/c-soft/satel_integra/blob/3b6d2020d1e10dc5aa40f30ee4ecc0f3a053eb3c/satel_integra/satel_integra.py#L188-L201", "partition": "test"}
{"repo": "CiscoDevNet/webexteamssdk", "path": "webexteamssdk/models/mixins/person.py", "func_name": "PersonBasicPropertiesMixin.lastActivity", "original_string": "def lastActivity(self):\n        \"\"\"The date and time of the person's last activity.\"\"\"\n        last_activity = self._json_data.get('lastActivity')\n        if last_activity:\n            return WebexTeamsDateTime.strptime(last_activity)\n        else:\n            return None", "language": "python", "code": "def lastActivity(self):\n        \"\"\"The date and time of the person's last activity.\"\"\"\n        last_activity = self._json_data.get('lastActivity')\n        if last_activity:\n            return WebexTeamsDateTime.strptime(last_activity)\n        else:\n            return None", "code_tokens": ["def", "lastActivity", "(", "self", ")", ":", "last_activity", "=", "self", ".", "_json_data", ".", "get", "(", "'lastActivity'", ")", "if", "last_activity", ":", "return", "WebexTeamsDateTime", ".", "strptime", "(", "last_activity", ")", "else", ":", "return", "None"], "docstring": "The date and time of the person's last activity.", "docstring_tokens": ["The", "date", "and", "time", "of", "the", "person", "s", "last", "activity", "."], "sha": "6fc2cc3557e080ba4b2a380664cb2a0532ae45cd", "url": "https://github.com/CiscoDevNet/webexteamssdk/blob/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd/webexteamssdk/models/mixins/person.py#L111-L117", "partition": "test"}
{"repo": "addok/addok", "path": "addok/shell.py", "func_name": "Cmd.do_BESTSCORE", "original_string": "def do_BESTSCORE(self, word):\n        \"\"\"Return document linked to word with higher score.\n        BESTSCORE lilas\"\"\"\n        key = keys.token_key(indexed_string(word)[0])\n        for _id, score in DB.zrevrange(key, 0, 20, withscores=True):\n            result = Result(_id)\n            print(white(result), blue(score), green(result._id))", "language": "python", "code": "def do_BESTSCORE(self, word):\n        \"\"\"Return document linked to word with higher score.\n        BESTSCORE lilas\"\"\"\n        key = keys.token_key(indexed_string(word)[0])\n        for _id, score in DB.zrevrange(key, 0, 20, withscores=True):\n            result = Result(_id)\n            print(white(result), blue(score), green(result._id))", "code_tokens": ["def", "do_BESTSCORE", "(", "self", ",", "word", ")", ":", "key", "=", "keys", ".", "token_key", "(", "indexed_string", "(", "word", ")", "[", "0", "]", ")", "for", "_id", ",", "score", "in", "DB", ".", "zrevrange", "(", "key", ",", "0", ",", "20", ",", "withscores", "=", "True", ")", ":", "result", "=", "Result", "(", "_id", ")", "print", "(", "white", "(", "result", ")", ",", "blue", "(", "score", ")", ",", "green", "(", "result", ".", "_id", ")", ")"], "docstring": "Return document linked to word with higher score.\n        BESTSCORE lilas", "docstring_tokens": ["Return", "document", "linked", "to", "word", "with", "higher", "score", ".", "BESTSCORE", "lilas"], "sha": "46a270d76ec778d2b445c2be753e5c6ba070a9b2", "url": "https://github.com/addok/addok/blob/46a270d76ec778d2b445c2be753e5c6ba070a9b2/addok/shell.py#L386-L392", "partition": "test"}
{"repo": "Valuehorizon/valuehorizon-forex", "path": "forex/models.py", "func_name": "convert_currency", "original_string": "def convert_currency(from_symbol, to_symbol, value, date):\n    \"\"\"\n    Converts an amount of money from one currency to another on a specified date.\n    \"\"\"\n    if from_symbol == to_symbol:\n        return value\n\n    factor = conversion_factor(from_symbol, to_symbol, date)\n\n    if type(value) == float:\n        output = value * float(factor)\n    elif type(value) == Decimal:\n        output = Decimal(format(value * factor, '.%sf' % str(PRICE_PRECISION)))\n    elif type(value) in [np.float16, np.float32, np.float64, np.float128, np.float]:\n        output = float(value) * float(factor)\n    else:\n        output = None\n\n    return output", "language": "python", "code": "def convert_currency(from_symbol, to_symbol, value, date):\n    \"\"\"\n    Converts an amount of money from one currency to another on a specified date.\n    \"\"\"\n    if from_symbol == to_symbol:\n        return value\n\n    factor = conversion_factor(from_symbol, to_symbol, date)\n\n    if type(value) == float:\n        output = value * float(factor)\n    elif type(value) == Decimal:\n        output = Decimal(format(value * factor, '.%sf' % str(PRICE_PRECISION)))\n    elif type(value) in [np.float16, np.float32, np.float64, np.float128, np.float]:\n        output = float(value) * float(factor)\n    else:\n        output = None\n\n    return output", "code_tokens": ["def", "convert_currency", "(", "from_symbol", ",", "to_symbol", ",", "value", ",", "date", ")", ":", "if", "from_symbol", "==", "to_symbol", ":", "return", "value", "factor", "=", "conversion_factor", "(", "from_symbol", ",", "to_symbol", ",", "date", ")", "if", "type", "(", "value", ")", "==", "float", ":", "output", "=", "value", "*", "float", "(", "factor", ")", "elif", "type", "(", "value", ")", "==", "Decimal", ":", "output", "=", "Decimal", "(", "format", "(", "value", "*", "factor", ",", "'.%sf'", "%", "str", "(", "PRICE_PRECISION", ")", ")", ")", "elif", "type", "(", "value", ")", "in", "[", "np", ".", "float16", ",", "np", ".", "float32", ",", "np", ".", "float64", ",", "np", ".", "float128", ",", "np", ".", "float", "]", ":", "output", "=", "float", "(", "value", ")", "*", "float", "(", "factor", ")", "else", ":", "output", "=", "None", "return", "output"], "docstring": "Converts an amount of money from one currency to another on a specified date.", "docstring_tokens": ["Converts", "an", "amount", "of", "money", "from", "one", "currency", "to", "another", "on", "a", "specified", "date", "."], "sha": "e921379ae6c9d07ddad87a1fd3b5bb8fdfc74cb8", "url": "https://github.com/Valuehorizon/valuehorizon-forex/blob/e921379ae6c9d07ddad87a1fd3b5bb8fdfc74cb8/forex/models.py#L232-L250", "partition": "test"}
{"repo": "sourcesimian/pyTelegramBotAPI", "path": "TelegramBotAPI/types/field.py", "func_name": "Field.setup_types", "original_string": "def setup_types(self):\n        \"\"\"\n        The Message object has a circular reference on itself, thus we have to allow\n        Type referencing by name. Here we lookup any Types referenced by name and\n        replace with the real class.\n        \"\"\"\n        def load(t):\n            from TelegramBotAPI.types.type import Type\n            if isinstance(t, str):\n                return Type._type(t)\n            assert issubclass(t, Type)\n            return t\n        self.types = [load(t) for t in self.types]", "language": "python", "code": "def setup_types(self):\n        \"\"\"\n        The Message object has a circular reference on itself, thus we have to allow\n        Type referencing by name. Here we lookup any Types referenced by name and\n        replace with the real class.\n        \"\"\"\n        def load(t):\n            from TelegramBotAPI.types.type import Type\n            if isinstance(t, str):\n                return Type._type(t)\n            assert issubclass(t, Type)\n            return t\n        self.types = [load(t) for t in self.types]", "code_tokens": ["def", "setup_types", "(", "self", ")", ":", "def", "load", "(", "t", ")", ":", "from", "TelegramBotAPI", ".", "types", ".", "type", "import", "Type", "if", "isinstance", "(", "t", ",", "str", ")", ":", "return", "Type", ".", "_type", "(", "t", ")", "assert", "issubclass", "(", "t", ",", "Type", ")", "return", "t", "self", ".", "types", "=", "[", "load", "(", "t", ")", "for", "t", "in", "self", ".", "types", "]"], "docstring": "The Message object has a circular reference on itself, thus we have to allow\n        Type referencing by name. Here we lookup any Types referenced by name and\n        replace with the real class.", "docstring_tokens": ["The", "Message", "object", "has", "a", "circular", "reference", "on", "itself", "thus", "we", "have", "to", "allow", "Type", "referencing", "by", "name", ".", "Here", "we", "lookup", "any", "Types", "referenced", "by", "name", "and", "replace", "with", "the", "real", "class", "."], "sha": "bf233ac7923cc97c2036a604922c8be57817db40", "url": "https://github.com/sourcesimian/pyTelegramBotAPI/blob/bf233ac7923cc97c2036a604922c8be57817db40/TelegramBotAPI/types/field.py#L24-L36", "partition": "test"}
{"repo": "zenodo/zenodo-accessrequests", "path": "zenodo_accessrequests/views/requests.py", "func_name": "is_embargoed", "original_string": "def is_embargoed(record):\n    \"\"\"Template filter to check if a record is embargoed.\"\"\"\n    return record.get('access_right') == 'embargoed' and \\\n        record.get('embargo_date') and \\\n        record.get('embargo_date') > datetime.utcnow().date()", "language": "python", "code": "def is_embargoed(record):\n    \"\"\"Template filter to check if a record is embargoed.\"\"\"\n    return record.get('access_right') == 'embargoed' and \\\n        record.get('embargo_date') and \\\n        record.get('embargo_date') > datetime.utcnow().date()", "code_tokens": ["def", "is_embargoed", "(", "record", ")", ":", "return", "record", ".", "get", "(", "'access_right'", ")", "==", "'embargoed'", "and", "record", ".", "get", "(", "'embargo_date'", ")", "and", "record", ".", "get", "(", "'embargo_date'", ")", ">", "datetime", ".", "utcnow", "(", ")", ".", "date", "(", ")"], "docstring": "Template filter to check if a record is embargoed.", "docstring_tokens": ["Template", "filter", "to", "check", "if", "a", "record", "is", "embargoed", "."], "sha": "ce2cf3f1425d02ba4f3ad3202cfca43a1892558a", "url": "https://github.com/zenodo/zenodo-accessrequests/blob/ce2cf3f1425d02ba4f3ad3202cfca43a1892558a/zenodo_accessrequests/views/requests.py#L63-L67", "partition": "test"}
{"repo": "LionelAuroux/pyrser", "path": "pyrser/parsing/base.py", "func_name": "BasicParser.eval_hook", "original_string": "def eval_hook(self, name: str, ctx: list) -> Node:\n        \"\"\"Evaluate the hook by its name\"\"\"\n        if name not in self.__class__._hooks:\n            # TODO: don't always throw error, could have return True by default\n            self.diagnostic.notify(\n                error.Severity.ERROR,\n                \"Unknown hook : %s\" % name,\n                error.LocationInfo.from_stream(self._stream, is_error=True)\n            )\n            raise self.diagnostic\n        self._lastRule = '#' + name\n        res = self.__class__._hooks[name](self, *ctx)\n        if type(res) is not bool:\n            raise TypeError(\"Your hook %r didn't return a bool value\" % name)\n        return res", "language": "python", "code": "def eval_hook(self, name: str, ctx: list) -> Node:\n        \"\"\"Evaluate the hook by its name\"\"\"\n        if name not in self.__class__._hooks:\n            # TODO: don't always throw error, could have return True by default\n            self.diagnostic.notify(\n                error.Severity.ERROR,\n                \"Unknown hook : %s\" % name,\n                error.LocationInfo.from_stream(self._stream, is_error=True)\n            )\n            raise self.diagnostic\n        self._lastRule = '#' + name\n        res = self.__class__._hooks[name](self, *ctx)\n        if type(res) is not bool:\n            raise TypeError(\"Your hook %r didn't return a bool value\" % name)\n        return res", "code_tokens": ["def", "eval_hook", "(", "self", ",", "name", ":", "str", ",", "ctx", ":", "list", ")", "->", "Node", ":", "if", "name", "not", "in", "self", ".", "__class__", ".", "_hooks", ":", "# TODO: don't always throw error, could have return True by default", "self", ".", "diagnostic", ".", "notify", "(", "error", ".", "Severity", ".", "ERROR", ",", "\"Unknown hook : %s\"", "%", "name", ",", "error", ".", "LocationInfo", ".", "from_stream", "(", "self", ".", "_stream", ",", "is_error", "=", "True", ")", ")", "raise", "self", ".", "diagnostic", "self", ".", "_lastRule", "=", "'#'", "+", "name", "res", "=", "self", ".", "__class__", ".", "_hooks", "[", "name", "]", "(", "self", ",", "*", "ctx", ")", "if", "type", "(", "res", ")", "is", "not", "bool", ":", "raise", "TypeError", "(", "\"Your hook %r didn't return a bool value\"", "%", "name", ")", "return", "res"], "docstring": "Evaluate the hook by its name", "docstring_tokens": ["Evaluate", "the", "hook", "by", "its", "name"], "sha": "f153a97ef2b6bf915a1ed468c0252a9a59b754d5", "url": "https://github.com/LionelAuroux/pyrser/blob/f153a97ef2b6bf915a1ed468c0252a9a59b754d5/pyrser/parsing/base.py#L234-L248", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py", "func_name": "_secant2_inner", "original_string": "def _secant2_inner(value_and_gradients_function,\n                   initial_args,\n                   val_0,\n                   val_c,\n                   f_lim,\n                   sufficient_decrease_param,\n                   curvature_param):\n  \"\"\"Helper function for secant square.\"\"\"\n  # Apply the `update` function on active branch members to squeeze their\n  # bracketing interval.\n  update_result = update(value_and_gradients_function,\n                         initial_args.left,\n                         initial_args.right,\n                         val_c,\n                         f_lim,\n                         active=initial_args.active)\n\n  # Update active and failed flags, update left/right on non-failed entries.\n  active = initial_args.active & ~update_result.failed\n  failed = initial_args.failed | update_result.failed\n  val_left = val_where(active, update_result.left, initial_args.left)\n  val_right = val_where(active, update_result.right, initial_args.right)\n\n  # Check if new `c` points should be generated.\n  updated_left = active & tf.equal(val_left.x, val_c.x)\n  updated_right = active & tf.equal(val_right.x, val_c.x)\n  is_new = updated_left | updated_right\n\n  next_c = tf.where(updated_left,\n                    _secant(initial_args.left, val_left),\n                    val_c.x)\n  next_c = tf.where(updated_right,\n                    _secant(initial_args.right, val_right),\n                    next_c)\n  in_range = (val_left.x <= next_c) & (next_c <= val_right.x)\n\n  # Figure out if an extra function evaluation is needed for new `c` points.\n  needs_extra_eval = tf.reduce_any(input_tensor=in_range & is_new)\n  num_evals = initial_args.num_evals + update_result.num_evals\n  num_evals = num_evals + tf.cast(needs_extra_eval, num_evals.dtype)\n\n  next_args = _Secant2Result(\n      active=active & in_range,  # No longer active if `c` is out of range.\n      converged=initial_args.converged,\n      failed=failed,\n      num_evals=num_evals,\n      left=val_left,\n      right=val_right)\n\n  def _apply_inner_update():\n    next_val_c = prefer_static.cond(\n        needs_extra_eval,\n        (lambda: value_and_gradients_function(next_c)),\n        (lambda: val_c))\n    return _secant2_inner_update(\n        value_and_gradients_function, next_args, val_0, next_val_c, f_lim,\n        sufficient_decrease_param, curvature_param)\n\n  return prefer_static.cond(\n      tf.reduce_any(input_tensor=next_args.active),\n      _apply_inner_update,\n      lambda: next_args)", "language": "python", "code": "def _secant2_inner(value_and_gradients_function,\n                   initial_args,\n                   val_0,\n                   val_c,\n                   f_lim,\n                   sufficient_decrease_param,\n                   curvature_param):\n  \"\"\"Helper function for secant square.\"\"\"\n  # Apply the `update` function on active branch members to squeeze their\n  # bracketing interval.\n  update_result = update(value_and_gradients_function,\n                         initial_args.left,\n                         initial_args.right,\n                         val_c,\n                         f_lim,\n                         active=initial_args.active)\n\n  # Update active and failed flags, update left/right on non-failed entries.\n  active = initial_args.active & ~update_result.failed\n  failed = initial_args.failed | update_result.failed\n  val_left = val_where(active, update_result.left, initial_args.left)\n  val_right = val_where(active, update_result.right, initial_args.right)\n\n  # Check if new `c` points should be generated.\n  updated_left = active & tf.equal(val_left.x, val_c.x)\n  updated_right = active & tf.equal(val_right.x, val_c.x)\n  is_new = updated_left | updated_right\n\n  next_c = tf.where(updated_left,\n                    _secant(initial_args.left, val_left),\n                    val_c.x)\n  next_c = tf.where(updated_right,\n                    _secant(initial_args.right, val_right),\n                    next_c)\n  in_range = (val_left.x <= next_c) & (next_c <= val_right.x)\n\n  # Figure out if an extra function evaluation is needed for new `c` points.\n  needs_extra_eval = tf.reduce_any(input_tensor=in_range & is_new)\n  num_evals = initial_args.num_evals + update_result.num_evals\n  num_evals = num_evals + tf.cast(needs_extra_eval, num_evals.dtype)\n\n  next_args = _Secant2Result(\n      active=active & in_range,  # No longer active if `c` is out of range.\n      converged=initial_args.converged,\n      failed=failed,\n      num_evals=num_evals,\n      left=val_left,\n      right=val_right)\n\n  def _apply_inner_update():\n    next_val_c = prefer_static.cond(\n        needs_extra_eval,\n        (lambda: value_and_gradients_function(next_c)),\n        (lambda: val_c))\n    return _secant2_inner_update(\n        value_and_gradients_function, next_args, val_0, next_val_c, f_lim,\n        sufficient_decrease_param, curvature_param)\n\n  return prefer_static.cond(\n      tf.reduce_any(input_tensor=next_args.active),\n      _apply_inner_update,\n      lambda: next_args)", "code_tokens": ["def", "_secant2_inner", "(", "value_and_gradients_function", ",", "initial_args", ",", "val_0", ",", "val_c", ",", "f_lim", ",", "sufficient_decrease_param", ",", "curvature_param", ")", ":", "# Apply the `update` function on active branch members to squeeze their", "# bracketing interval.", "update_result", "=", "update", "(", "value_and_gradients_function", ",", "initial_args", ".", "left", ",", "initial_args", ".", "right", ",", "val_c", ",", "f_lim", ",", "active", "=", "initial_args", ".", "active", ")", "# Update active and failed flags, update left/right on non-failed entries.", "active", "=", "initial_args", ".", "active", "&", "~", "update_result", ".", "failed", "failed", "=", "initial_args", ".", "failed", "|", "update_result", ".", "failed", "val_left", "=", "val_where", "(", "active", ",", "update_result", ".", "left", ",", "initial_args", ".", "left", ")", "val_right", "=", "val_where", "(", "active", ",", "update_result", ".", "right", ",", "initial_args", ".", "right", ")", "# Check if new `c` points should be generated.", "updated_left", "=", "active", "&", "tf", ".", "equal", "(", "val_left", ".", "x", ",", "val_c", ".", "x", ")", "updated_right", "=", "active", "&", "tf", ".", "equal", "(", "val_right", ".", "x", ",", "val_c", ".", "x", ")", "is_new", "=", "updated_left", "|", "updated_right", "next_c", "=", "tf", ".", "where", "(", "updated_left", ",", "_secant", "(", "initial_args", ".", "left", ",", "val_left", ")", ",", "val_c", ".", "x", ")", "next_c", "=", "tf", ".", "where", "(", "updated_right", ",", "_secant", "(", "initial_args", ".", "right", ",", "val_right", ")", ",", "next_c", ")", "in_range", "=", "(", "val_left", ".", "x", "<=", "next_c", ")", "&", "(", "next_c", "<=", "val_right", ".", "x", ")", "# Figure out if an extra function evaluation is needed for new `c` points.", "needs_extra_eval", "=", "tf", ".", "reduce_any", "(", "input_tensor", "=", "in_range", "&", "is_new", ")", "num_evals", "=", "initial_args", ".", "num_evals", "+", "update_result", ".", "num_evals", "num_evals", "=", "num_evals", "+", "tf", ".", "cast", "(", "needs_extra_eval", ",", "num_evals", ".", "dtype", ")", "next_args", "=", "_Secant2Result", "(", "active", "=", "active", "&", "in_range", ",", "# No longer active if `c` is out of range.", "converged", "=", "initial_args", ".", "converged", ",", "failed", "=", "failed", ",", "num_evals", "=", "num_evals", ",", "left", "=", "val_left", ",", "right", "=", "val_right", ")", "def", "_apply_inner_update", "(", ")", ":", "next_val_c", "=", "prefer_static", ".", "cond", "(", "needs_extra_eval", ",", "(", "lambda", ":", "value_and_gradients_function", "(", "next_c", ")", ")", ",", "(", "lambda", ":", "val_c", ")", ")", "return", "_secant2_inner_update", "(", "value_and_gradients_function", ",", "next_args", ",", "val_0", ",", "next_val_c", ",", "f_lim", ",", "sufficient_decrease_param", ",", "curvature_param", ")", "return", "prefer_static", ".", "cond", "(", "tf", ".", "reduce_any", "(", "input_tensor", "=", "next_args", ".", "active", ")", ",", "_apply_inner_update", ",", "lambda", ":", "next_args", ")"], "docstring": "Helper function for secant square.", "docstring_tokens": ["Helper", "function", "for", "secant", "square", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py#L177-L238", "partition": "test"}
{"repo": "ricobl/django-importer", "path": "django_importer/importers/csv_importer.py", "func_name": "CSVImporter.get_items", "original_string": "def get_items(self):\n        \"\"\"\n        Iterator to read the rows of the CSV file.\n        \"\"\"\n        # Get the csv reader\n        reader = csv.reader(self.source)\n        # Get the headers from the first line\n        headers = reader.next()\n        # Read each line yielding a dictionary mapping\n        # the column headers to the row values\n        for row in reader:\n            # Skip empty rows\n            if not row:\n                continue\n            yield dict(zip(headers, row))", "language": "python", "code": "def get_items(self):\n        \"\"\"\n        Iterator to read the rows of the CSV file.\n        \"\"\"\n        # Get the csv reader\n        reader = csv.reader(self.source)\n        # Get the headers from the first line\n        headers = reader.next()\n        # Read each line yielding a dictionary mapping\n        # the column headers to the row values\n        for row in reader:\n            # Skip empty rows\n            if not row:\n                continue\n            yield dict(zip(headers, row))", "code_tokens": ["def", "get_items", "(", "self", ")", ":", "# Get the csv reader", "reader", "=", "csv", ".", "reader", "(", "self", ".", "source", ")", "# Get the headers from the first line", "headers", "=", "reader", ".", "next", "(", ")", "# Read each line yielding a dictionary mapping", "# the column headers to the row values", "for", "row", "in", "reader", ":", "# Skip empty rows", "if", "not", "row", ":", "continue", "yield", "dict", "(", "zip", "(", "headers", ",", "row", ")", ")"], "docstring": "Iterator to read the rows of the CSV file.", "docstring_tokens": ["Iterator", "to", "read", "the", "rows", "of", "the", "CSV", "file", "."], "sha": "6967adfa7a286be7aaf59d3f33c6637270bd9df6", "url": "https://github.com/ricobl/django-importer/blob/6967adfa7a286be7aaf59d3f33c6637270bd9df6/django_importer/importers/csv_importer.py#L29-L43", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/wheel/install.py", "func_name": "parse_version", "original_string": "def parse_version(version):\n    \"\"\"Use parse_version from pkg_resources or distutils as available.\"\"\"\n    global parse_version\n    try:\n        from pkg_resources import parse_version\n    except ImportError:\n        from distutils.version import LooseVersion as parse_version\n    return parse_version(version)", "language": "python", "code": "def parse_version(version):\n    \"\"\"Use parse_version from pkg_resources or distutils as available.\"\"\"\n    global parse_version\n    try:\n        from pkg_resources import parse_version\n    except ImportError:\n        from distutils.version import LooseVersion as parse_version\n    return parse_version(version)", "code_tokens": ["def", "parse_version", "(", "version", ")", ":", "global", "parse_version", "try", ":", "from", "pkg_resources", "import", "parse_version", "except", "ImportError", ":", "from", "distutils", ".", "version", "import", "LooseVersion", "as", "parse_version", "return", "parse_version", "(", "version", ")"], "docstring": "Use parse_version from pkg_resources or distutils as available.", "docstring_tokens": ["Use", "parse_version", "from", "pkg_resources", "or", "distutils", "as", "available", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/wheel/install.py#L42-L49", "partition": "test"}
{"repo": "treycucco/pyebnf", "path": "pyebnf/util.py", "func_name": "point_to_source", "original_string": "def point_to_source(source, position, fmt=(2, True, \"~~~~~\", \"^\")):\n  \"\"\"Point to a position in source code.\n\n  source is the text we're pointing in.\n  position is a 2-tuple of (line_number, character_number) to point to.\n  fmt is a 4-tuple of formatting parameters, they are:\n    name               default  description\n    ----               -------  -----------\n    surrounding_lines  2        the number of lines above and below the target line to print\n    show_line_numbers  True     if true line numbers will be generated for the output_lines\n    tail_body          \"~~~~~\"  the body of the tail\n    pointer_char       \"^\"      the character that will point to the position\n  \"\"\"\n\n  surrounding_lines, show_line_numbers, tail_body, pointer_char = fmt\n\n  line_no, char_no = position\n\n  lines = source.split(\"\\n\")\n  line = lines[line_no]\n\n  if char_no >= len(tail_body):\n    tail = \" \" * (char_no - len(tail_body)) + tail_body + pointer_char\n  else:\n    tail = \" \" * char_no + pointer_char + tail_body\n\n  if show_line_numbers:\n    line_no_width = int(math.ceil(math.log10(max(1, line_no + surrounding_lines))) + 1)\n    line_fmt = \"{0:\" + str(line_no_width) + \"}: {1}\"\n  else:\n    line_fmt = \"{1}\"\n\n  pivot = line_no + 1\n  output_lines = [(pivot, line), (\"\", tail)]\n  for i in range(surrounding_lines):\n    upper_ofst = i + 1\n    upper_idx = line_no + upper_ofst\n    lower_ofst = -upper_ofst\n    lower_idx = line_no + lower_ofst\n\n    if lower_idx >= 0:\n      output_lines.insert(0, (pivot + lower_ofst, lines[lower_idx]))\n    if upper_idx < len(lines):\n      output_lines.append((pivot + upper_ofst, lines[upper_idx]))\n\n  return \"\\n\".join(line_fmt.format(n, c) for n, c in output_lines)", "language": "python", "code": "def point_to_source(source, position, fmt=(2, True, \"~~~~~\", \"^\")):\n  \"\"\"Point to a position in source code.\n\n  source is the text we're pointing in.\n  position is a 2-tuple of (line_number, character_number) to point to.\n  fmt is a 4-tuple of formatting parameters, they are:\n    name               default  description\n    ----               -------  -----------\n    surrounding_lines  2        the number of lines above and below the target line to print\n    show_line_numbers  True     if true line numbers will be generated for the output_lines\n    tail_body          \"~~~~~\"  the body of the tail\n    pointer_char       \"^\"      the character that will point to the position\n  \"\"\"\n\n  surrounding_lines, show_line_numbers, tail_body, pointer_char = fmt\n\n  line_no, char_no = position\n\n  lines = source.split(\"\\n\")\n  line = lines[line_no]\n\n  if char_no >= len(tail_body):\n    tail = \" \" * (char_no - len(tail_body)) + tail_body + pointer_char\n  else:\n    tail = \" \" * char_no + pointer_char + tail_body\n\n  if show_line_numbers:\n    line_no_width = int(math.ceil(math.log10(max(1, line_no + surrounding_lines))) + 1)\n    line_fmt = \"{0:\" + str(line_no_width) + \"}: {1}\"\n  else:\n    line_fmt = \"{1}\"\n\n  pivot = line_no + 1\n  output_lines = [(pivot, line), (\"\", tail)]\n  for i in range(surrounding_lines):\n    upper_ofst = i + 1\n    upper_idx = line_no + upper_ofst\n    lower_ofst = -upper_ofst\n    lower_idx = line_no + lower_ofst\n\n    if lower_idx >= 0:\n      output_lines.insert(0, (pivot + lower_ofst, lines[lower_idx]))\n    if upper_idx < len(lines):\n      output_lines.append((pivot + upper_ofst, lines[upper_idx]))\n\n  return \"\\n\".join(line_fmt.format(n, c) for n, c in output_lines)", "code_tokens": ["def", "point_to_source", "(", "source", ",", "position", ",", "fmt", "=", "(", "2", ",", "True", ",", "\"~~~~~\"", ",", "\"^\"", ")", ")", ":", "surrounding_lines", ",", "show_line_numbers", ",", "tail_body", ",", "pointer_char", "=", "fmt", "line_no", ",", "char_no", "=", "position", "lines", "=", "source", ".", "split", "(", "\"\\n\"", ")", "line", "=", "lines", "[", "line_no", "]", "if", "char_no", ">=", "len", "(", "tail_body", ")", ":", "tail", "=", "\" \"", "*", "(", "char_no", "-", "len", "(", "tail_body", ")", ")", "+", "tail_body", "+", "pointer_char", "else", ":", "tail", "=", "\" \"", "*", "char_no", "+", "pointer_char", "+", "tail_body", "if", "show_line_numbers", ":", "line_no_width", "=", "int", "(", "math", ".", "ceil", "(", "math", ".", "log10", "(", "max", "(", "1", ",", "line_no", "+", "surrounding_lines", ")", ")", ")", "+", "1", ")", "line_fmt", "=", "\"{0:\"", "+", "str", "(", "line_no_width", ")", "+", "\"}: {1}\"", "else", ":", "line_fmt", "=", "\"{1}\"", "pivot", "=", "line_no", "+", "1", "output_lines", "=", "[", "(", "pivot", ",", "line", ")", ",", "(", "\"\"", ",", "tail", ")", "]", "for", "i", "in", "range", "(", "surrounding_lines", ")", ":", "upper_ofst", "=", "i", "+", "1", "upper_idx", "=", "line_no", "+", "upper_ofst", "lower_ofst", "=", "-", "upper_ofst", "lower_idx", "=", "line_no", "+", "lower_ofst", "if", "lower_idx", ">=", "0", ":", "output_lines", ".", "insert", "(", "0", ",", "(", "pivot", "+", "lower_ofst", ",", "lines", "[", "lower_idx", "]", ")", ")", "if", "upper_idx", "<", "len", "(", "lines", ")", ":", "output_lines", ".", "append", "(", "(", "pivot", "+", "upper_ofst", ",", "lines", "[", "upper_idx", "]", ")", ")", "return", "\"\\n\"", ".", "join", "(", "line_fmt", ".", "format", "(", "n", ",", "c", ")", "for", "n", ",", "c", "in", "output_lines", ")"], "docstring": "Point to a position in source code.\n\n  source is the text we're pointing in.\n  position is a 2-tuple of (line_number, character_number) to point to.\n  fmt is a 4-tuple of formatting parameters, they are:\n    name               default  description\n    ----               -------  -----------\n    surrounding_lines  2        the number of lines above and below the target line to print\n    show_line_numbers  True     if true line numbers will be generated for the output_lines\n    tail_body          \"~~~~~\"  the body of the tail\n    pointer_char       \"^\"      the character that will point to the position", "docstring_tokens": ["Point", "to", "a", "position", "in", "source", "code", "."], "sha": "3634ddabbe5d73508bcc20f4a591f86a46634e1d", "url": "https://github.com/treycucco/pyebnf/blob/3634ddabbe5d73508bcc20f4a591f86a46634e1d/pyebnf/util.py#L67-L112", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/config.py", "func_name": "OptionsManagerMixIn.global_set_option", "original_string": "def global_set_option(self, opt, value):\n        \"\"\"set option on the correct option provider\"\"\"\n        self._all_options[opt].set_option(opt, value)", "language": "python", "code": "def global_set_option(self, opt, value):\n        \"\"\"set option on the correct option provider\"\"\"\n        self._all_options[opt].set_option(opt, value)", "code_tokens": ["def", "global_set_option", "(", "self", ",", "opt", ",", "value", ")", ":", "self", ".", "_all_options", "[", "opt", "]", ".", "set_option", "(", "opt", ",", "value", ")"], "docstring": "set option on the correct option provider", "docstring_tokens": ["set", "option", "on", "the", "correct", "option", "provider"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/config.py#L643-L645", "partition": "test"}
{"repo": "trp07/messages", "path": "messages/_config.py", "func_name": "update_config_pwd", "original_string": "def update_config_pwd(msg, cfg):\n    \"\"\"\n    Updates the profile's auth entry with values set by the user.\n    This will overwrite existing values.\n\n    Args:\n        :msg: (Message class) an instance of a message class.\n        :cfg: (jsonconfig.Config) config instance.\n    \"\"\"\n    msg_type = msg.__class__.__name__.lower()\n    key_fmt = msg.profile + \"_\" + msg_type\n    if isinstance(msg._auth, (MutableSequence, tuple)):\n        cfg.pwd[key_fmt] = \" :: \".join(msg._auth)\n    else:\n        cfg.pwd[key_fmt] = msg._auth", "language": "python", "code": "def update_config_pwd(msg, cfg):\n    \"\"\"\n    Updates the profile's auth entry with values set by the user.\n    This will overwrite existing values.\n\n    Args:\n        :msg: (Message class) an instance of a message class.\n        :cfg: (jsonconfig.Config) config instance.\n    \"\"\"\n    msg_type = msg.__class__.__name__.lower()\n    key_fmt = msg.profile + \"_\" + msg_type\n    if isinstance(msg._auth, (MutableSequence, tuple)):\n        cfg.pwd[key_fmt] = \" :: \".join(msg._auth)\n    else:\n        cfg.pwd[key_fmt] = msg._auth", "code_tokens": ["def", "update_config_pwd", "(", "msg", ",", "cfg", ")", ":", "msg_type", "=", "msg", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "key_fmt", "=", "msg", ".", "profile", "+", "\"_\"", "+", "msg_type", "if", "isinstance", "(", "msg", ".", "_auth", ",", "(", "MutableSequence", ",", "tuple", ")", ")", ":", "cfg", ".", "pwd", "[", "key_fmt", "]", "=", "\" :: \"", ".", "join", "(", "msg", ".", "_auth", ")", "else", ":", "cfg", ".", "pwd", "[", "key_fmt", "]", "=", "msg", ".", "_auth"], "docstring": "Updates the profile's auth entry with values set by the user.\n    This will overwrite existing values.\n\n    Args:\n        :msg: (Message class) an instance of a message class.\n        :cfg: (jsonconfig.Config) config instance.", "docstring_tokens": ["Updates", "the", "profile", "s", "auth", "entry", "with", "values", "set", "by", "the", "user", ".", "This", "will", "overwrite", "existing", "values", "."], "sha": "7789ebc960335a59ea5d319fceed3dd349023648", "url": "https://github.com/trp07/messages/blob/7789ebc960335a59ea5d319fceed3dd349023648/messages/_config.py#L150-L164", "partition": "test"}
{"repo": "opencast/pyCA", "path": "pyca/utils.py", "func_name": "register_ca", "original_string": "def register_ca(status='idle'):\n    '''Register this capture agent at the Matterhorn admin server so that it\n    shows up in the admin interface.\n\n    :param address: Address of the capture agent web ui\n    :param status: Current status of the capture agent\n    '''\n    # If this is a backup CA we don't tell the Matterhorn core that we are\n    # here.  We will just run silently in the background:\n    if config()['agent']['backup_mode']:\n        return\n    params = [('address', config()['ui']['url']), ('state', status)]\n    name = urlquote(config()['agent']['name'].encode('utf-8'), safe='')\n    url = '%s/agents/%s' % (config()['service-capture.admin'][0], name)\n    try:\n        response = http_request(url, params).decode('utf-8')\n        if response:\n            logger.info(response)\n    except pycurl.error as e:\n        logger.warning('Could not set agent state to %s: %s' % (status, e))", "language": "python", "code": "def register_ca(status='idle'):\n    '''Register this capture agent at the Matterhorn admin server so that it\n    shows up in the admin interface.\n\n    :param address: Address of the capture agent web ui\n    :param status: Current status of the capture agent\n    '''\n    # If this is a backup CA we don't tell the Matterhorn core that we are\n    # here.  We will just run silently in the background:\n    if config()['agent']['backup_mode']:\n        return\n    params = [('address', config()['ui']['url']), ('state', status)]\n    name = urlquote(config()['agent']['name'].encode('utf-8'), safe='')\n    url = '%s/agents/%s' % (config()['service-capture.admin'][0], name)\n    try:\n        response = http_request(url, params).decode('utf-8')\n        if response:\n            logger.info(response)\n    except pycurl.error as e:\n        logger.warning('Could not set agent state to %s: %s' % (status, e))", "code_tokens": ["def", "register_ca", "(", "status", "=", "'idle'", ")", ":", "# If this is a backup CA we don't tell the Matterhorn core that we are", "# here.  We will just run silently in the background:", "if", "config", "(", ")", "[", "'agent'", "]", "[", "'backup_mode'", "]", ":", "return", "params", "=", "[", "(", "'address'", ",", "config", "(", ")", "[", "'ui'", "]", "[", "'url'", "]", ")", ",", "(", "'state'", ",", "status", ")", "]", "name", "=", "urlquote", "(", "config", "(", ")", "[", "'agent'", "]", "[", "'name'", "]", ".", "encode", "(", "'utf-8'", ")", ",", "safe", "=", "''", ")", "url", "=", "'%s/agents/%s'", "%", "(", "config", "(", ")", "[", "'service-capture.admin'", "]", "[", "0", "]", ",", "name", ")", "try", ":", "response", "=", "http_request", "(", "url", ",", "params", ")", ".", "decode", "(", "'utf-8'", ")", "if", "response", ":", "logger", ".", "info", "(", "response", ")", "except", "pycurl", ".", "error", "as", "e", ":", "logger", ".", "warning", "(", "'Could not set agent state to %s: %s'", "%", "(", "status", ",", "e", ")", ")"], "docstring": "Register this capture agent at the Matterhorn admin server so that it\n    shows up in the admin interface.\n\n    :param address: Address of the capture agent web ui\n    :param status: Current status of the capture agent", "docstring_tokens": ["Register", "this", "capture", "agent", "at", "the", "Matterhorn", "admin", "server", "so", "that", "it", "shows", "up", "in", "the", "admin", "interface", "."], "sha": "c89b168d4780d157e1b3f7676628c1b131956a88", "url": "https://github.com/opencast/pyCA/blob/c89b168d4780d157e1b3f7676628c1b131956a88/pyca/utils.py#L131-L150", "partition": "test"}
{"repo": "singularityhub/sregistry-cli", "path": "sregistry/main/base/http.py", "func_name": "paginate_get", "original_string": "def paginate_get(self, url, \n                 headers=None, \n                 return_json=True,\n                 start_page=None):\n    '''paginate_call is a wrapper for get to paginate results\n    '''\n\n    geturl = '%s&page=1' %(url)\n    if start_page is not None:\n        geturl = '%s&page=%s' %(url,start_page)\n\n    results = []\n    while geturl is not None:\n        result = self._get(url, headers=headers, return_json=return_json)\n        # If we have pagination:\n        if isinstance(result, dict):\n            if 'results' in result:\n                results = results + result['results']\n            geturl = result['next']\n        # No pagination is a list\n        else:\n            return result\n    return results", "language": "python", "code": "def paginate_get(self, url, \n                 headers=None, \n                 return_json=True,\n                 start_page=None):\n    '''paginate_call is a wrapper for get to paginate results\n    '''\n\n    geturl = '%s&page=1' %(url)\n    if start_page is not None:\n        geturl = '%s&page=%s' %(url,start_page)\n\n    results = []\n    while geturl is not None:\n        result = self._get(url, headers=headers, return_json=return_json)\n        # If we have pagination:\n        if isinstance(result, dict):\n            if 'results' in result:\n                results = results + result['results']\n            geturl = result['next']\n        # No pagination is a list\n        else:\n            return result\n    return results", "code_tokens": ["def", "paginate_get", "(", "self", ",", "url", ",", "headers", "=", "None", ",", "return_json", "=", "True", ",", "start_page", "=", "None", ")", ":", "geturl", "=", "'%s&page=1'", "%", "(", "url", ")", "if", "start_page", "is", "not", "None", ":", "geturl", "=", "'%s&page=%s'", "%", "(", "url", ",", "start_page", ")", "results", "=", "[", "]", "while", "geturl", "is", "not", "None", ":", "result", "=", "self", ".", "_get", "(", "url", ",", "headers", "=", "headers", ",", "return_json", "=", "return_json", ")", "# If we have pagination:", "if", "isinstance", "(", "result", ",", "dict", ")", ":", "if", "'results'", "in", "result", ":", "results", "=", "results", "+", "result", "[", "'results'", "]", "geturl", "=", "result", "[", "'next'", "]", "# No pagination is a list", "else", ":", "return", "result", "return", "results"], "docstring": "paginate_call is a wrapper for get to paginate results", "docstring_tokens": ["paginate_call", "is", "a", "wrapper", "for", "get", "to", "paginate", "results"], "sha": "abc96140a1d15b5e96d83432e1e0e1f4f8f36331", "url": "https://github.com/singularityhub/sregistry-cli/blob/abc96140a1d15b5e96d83432e1e0e1f4f8f36331/sregistry/main/base/http.py#L118-L140", "partition": "test"}
{"repo": "elliterate/capybara.py", "path": "capybara/session_matchers.py", "func_name": "SessionMatchersMixin.assert_no_current_path", "original_string": "def assert_no_current_path(self, path, **kwargs):\n        \"\"\"\n        Asserts that the page doesn't have the given path.\n\n        Args:\n            path (str | RegexObject): The string or regex that the current \"path\" should match.\n            **kwargs: Arbitrary keyword arguments for :class:`CurrentPathQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.\n        \"\"\"\n\n        query = CurrentPathQuery(path, **kwargs)\n\n        @self.document.synchronize\n        def assert_no_current_path():\n            if query.resolves_for(self):\n                raise ExpectationNotMet(query.negative_failure_message)\n\n        assert_no_current_path()\n\n        return True", "language": "python", "code": "def assert_no_current_path(self, path, **kwargs):\n        \"\"\"\n        Asserts that the page doesn't have the given path.\n\n        Args:\n            path (str | RegexObject): The string or regex that the current \"path\" should match.\n            **kwargs: Arbitrary keyword arguments for :class:`CurrentPathQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.\n        \"\"\"\n\n        query = CurrentPathQuery(path, **kwargs)\n\n        @self.document.synchronize\n        def assert_no_current_path():\n            if query.resolves_for(self):\n                raise ExpectationNotMet(query.negative_failure_message)\n\n        assert_no_current_path()\n\n        return True", "code_tokens": ["def", "assert_no_current_path", "(", "self", ",", "path", ",", "*", "*", "kwargs", ")", ":", "query", "=", "CurrentPathQuery", "(", "path", ",", "*", "*", "kwargs", ")", "@", "self", ".", "document", ".", "synchronize", "def", "assert_no_current_path", "(", ")", ":", "if", "query", ".", "resolves_for", "(", "self", ")", ":", "raise", "ExpectationNotMet", "(", "query", ".", "negative_failure_message", ")", "assert_no_current_path", "(", ")", "return", "True"], "docstring": "Asserts that the page doesn't have the given path.\n\n        Args:\n            path (str | RegexObject): The string or regex that the current \"path\" should match.\n            **kwargs: Arbitrary keyword arguments for :class:`CurrentPathQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.", "docstring_tokens": ["Asserts", "that", "the", "page", "doesn", "t", "have", "the", "given", "path", "."], "sha": "0c6ae449cc37e4445ec3cd6af95674533beedc6c", "url": "https://github.com/elliterate/capybara.py/blob/0c6ae449cc37e4445ec3cd6af95674533beedc6c/capybara/session_matchers.py#L32-L56", "partition": "test"}
{"repo": "dopefishh/pympi", "path": "pympi/Praat.py", "func_name": "TextGrid.to_eaf", "original_string": "def to_eaf(self, skipempty=True, pointlength=0.1):\n        \"\"\"Convert the object to an pympi.Elan.Eaf object\n\n        :param int pointlength: Length of respective interval from points in\n                                seconds\n        :param bool skipempty: Skip the empty annotations\n        :returns: :class:`pympi.Elan.Eaf` object\n        :raises ImportError: If the Eaf module can't be loaded.\n        :raises ValueError: If the pointlength is not strictly positive.\n        \"\"\"\n        from pympi.Elan import Eaf\n        eaf_out = Eaf()\n        if pointlength <= 0:\n            raise ValueError('Pointlength should be strictly positive')\n        for tier in self.get_tiers():\n            eaf_out.add_tier(tier.name)\n            for ann in tier.get_intervals(True):\n                if tier.tier_type == 'TextTier':\n                    ann = (ann[0], ann[0]+pointlength, ann[1])\n                if ann[2].strip() or not skipempty:\n                    eaf_out.add_annotation(tier.name, int(round(ann[0]*1000)),\n                                           int(round(ann[1]*1000)), ann[2])\n        return eaf_out", "language": "python", "code": "def to_eaf(self, skipempty=True, pointlength=0.1):\n        \"\"\"Convert the object to an pympi.Elan.Eaf object\n\n        :param int pointlength: Length of respective interval from points in\n                                seconds\n        :param bool skipempty: Skip the empty annotations\n        :returns: :class:`pympi.Elan.Eaf` object\n        :raises ImportError: If the Eaf module can't be loaded.\n        :raises ValueError: If the pointlength is not strictly positive.\n        \"\"\"\n        from pympi.Elan import Eaf\n        eaf_out = Eaf()\n        if pointlength <= 0:\n            raise ValueError('Pointlength should be strictly positive')\n        for tier in self.get_tiers():\n            eaf_out.add_tier(tier.name)\n            for ann in tier.get_intervals(True):\n                if tier.tier_type == 'TextTier':\n                    ann = (ann[0], ann[0]+pointlength, ann[1])\n                if ann[2].strip() or not skipempty:\n                    eaf_out.add_annotation(tier.name, int(round(ann[0]*1000)),\n                                           int(round(ann[1]*1000)), ann[2])\n        return eaf_out", "code_tokens": ["def", "to_eaf", "(", "self", ",", "skipempty", "=", "True", ",", "pointlength", "=", "0.1", ")", ":", "from", "pympi", ".", "Elan", "import", "Eaf", "eaf_out", "=", "Eaf", "(", ")", "if", "pointlength", "<=", "0", ":", "raise", "ValueError", "(", "'Pointlength should be strictly positive'", ")", "for", "tier", "in", "self", ".", "get_tiers", "(", ")", ":", "eaf_out", ".", "add_tier", "(", "tier", ".", "name", ")", "for", "ann", "in", "tier", ".", "get_intervals", "(", "True", ")", ":", "if", "tier", ".", "tier_type", "==", "'TextTier'", ":", "ann", "=", "(", "ann", "[", "0", "]", ",", "ann", "[", "0", "]", "+", "pointlength", ",", "ann", "[", "1", "]", ")", "if", "ann", "[", "2", "]", ".", "strip", "(", ")", "or", "not", "skipempty", ":", "eaf_out", ".", "add_annotation", "(", "tier", ".", "name", ",", "int", "(", "round", "(", "ann", "[", "0", "]", "*", "1000", ")", ")", ",", "int", "(", "round", "(", "ann", "[", "1", "]", "*", "1000", ")", ")", ",", "ann", "[", "2", "]", ")", "return", "eaf_out"], "docstring": "Convert the object to an pympi.Elan.Eaf object\n\n        :param int pointlength: Length of respective interval from points in\n                                seconds\n        :param bool skipempty: Skip the empty annotations\n        :returns: :class:`pympi.Elan.Eaf` object\n        :raises ImportError: If the Eaf module can't be loaded.\n        :raises ValueError: If the pointlength is not strictly positive.", "docstring_tokens": ["Convert", "the", "object", "to", "an", "pympi", ".", "Elan", ".", "Eaf", "object"], "sha": "79c747cde45b5ba203ed93154d8c123ac9c3ef56", "url": "https://github.com/dopefishh/pympi/blob/79c747cde45b5ba203ed93154d8c123ac9c3ef56/pympi/Praat.py#L297-L319", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/utils/encoding.py", "func_name": "get_stream_enc", "original_string": "def get_stream_enc(stream, default=None):\n    \"\"\"Return the given stream's encoding or a default.\n\n    There are cases where sys.std* might not actually be a stream, so\n    check for the encoding attribute prior to returning it, and return\n    a default if it doesn't exist or evaluates as False. `default'\n    is None if not provided.\n    \"\"\"\n    if not hasattr(stream, 'encoding') or not stream.encoding:\n        return default\n    else:\n        return stream.encoding", "language": "python", "code": "def get_stream_enc(stream, default=None):\n    \"\"\"Return the given stream's encoding or a default.\n\n    There are cases where sys.std* might not actually be a stream, so\n    check for the encoding attribute prior to returning it, and return\n    a default if it doesn't exist or evaluates as False. `default'\n    is None if not provided.\n    \"\"\"\n    if not hasattr(stream, 'encoding') or not stream.encoding:\n        return default\n    else:\n        return stream.encoding", "code_tokens": ["def", "get_stream_enc", "(", "stream", ",", "default", "=", "None", ")", ":", "if", "not", "hasattr", "(", "stream", ",", "'encoding'", ")", "or", "not", "stream", ".", "encoding", ":", "return", "default", "else", ":", "return", "stream", ".", "encoding"], "docstring": "Return the given stream's encoding or a default.\n\n    There are cases where sys.std* might not actually be a stream, so\n    check for the encoding attribute prior to returning it, and return\n    a default if it doesn't exist or evaluates as False. `default'\n    is None if not provided.", "docstring_tokens": ["Return", "the", "given", "stream", "s", "encoding", "or", "a", "default", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/utils/encoding.py#L20-L31", "partition": "test"}
{"repo": "python/performance", "path": "performance/benchmarks/__init__.py", "func_name": "expand_benchmark_name", "original_string": "def expand_benchmark_name(bm_name, bench_groups):\n    \"\"\"Recursively expand name benchmark names.\n\n    Args:\n        bm_name: string naming a benchmark or benchmark group.\n\n    Yields:\n        Names of actual benchmarks, with all group names fully expanded.\n    \"\"\"\n    expansion = bench_groups.get(bm_name)\n    if expansion:\n        for name in expansion:\n            for name in expand_benchmark_name(name, bench_groups):\n                yield name\n    else:\n        yield bm_name", "language": "python", "code": "def expand_benchmark_name(bm_name, bench_groups):\n    \"\"\"Recursively expand name benchmark names.\n\n    Args:\n        bm_name: string naming a benchmark or benchmark group.\n\n    Yields:\n        Names of actual benchmarks, with all group names fully expanded.\n    \"\"\"\n    expansion = bench_groups.get(bm_name)\n    if expansion:\n        for name in expansion:\n            for name in expand_benchmark_name(name, bench_groups):\n                yield name\n    else:\n        yield bm_name", "code_tokens": ["def", "expand_benchmark_name", "(", "bm_name", ",", "bench_groups", ")", ":", "expansion", "=", "bench_groups", ".", "get", "(", "bm_name", ")", "if", "expansion", ":", "for", "name", "in", "expansion", ":", "for", "name", "in", "expand_benchmark_name", "(", "name", ",", "bench_groups", ")", ":", "yield", "name", "else", ":", "yield", "bm_name"], "docstring": "Recursively expand name benchmark names.\n\n    Args:\n        bm_name: string naming a benchmark or benchmark group.\n\n    Yields:\n        Names of actual benchmarks, with all group names fully expanded.", "docstring_tokens": ["Recursively", "expand", "name", "benchmark", "names", "."], "sha": "2a9524c0a5714e85106671bc61d750e800fe17db", "url": "https://github.com/python/performance/blob/2a9524c0a5714e85106671bc61d750e800fe17db/performance/benchmarks/__init__.py#L344-L359", "partition": "test"}
{"repo": "nickw444/flask-ldap3-login", "path": "flask_ldap3_login/__init__.py", "func_name": "LDAP3LoginManager.init_app", "original_string": "def init_app(self, app):\n        '''\n        Configures this extension with the given app. This registers an\n        ``teardown_appcontext`` call, and attaches this ``LDAP3LoginManager``\n        to it as ``app.ldap3_login_manager``.\n\n        Args:\n            app (flask.Flask): The flask app to initialise with\n        '''\n\n        app.ldap3_login_manager = self\n\n        servers = list(self._server_pool)\n        for s in servers:\n            self._server_pool.remove(s)\n\n        self.init_config(app.config)\n\n        if hasattr(app, 'teardown_appcontext'):\n            app.teardown_appcontext(self.teardown)\n        else:  # pragma: no cover\n            app.teardown_request(self.teardown)\n\n        self.app = app", "language": "python", "code": "def init_app(self, app):\n        '''\n        Configures this extension with the given app. This registers an\n        ``teardown_appcontext`` call, and attaches this ``LDAP3LoginManager``\n        to it as ``app.ldap3_login_manager``.\n\n        Args:\n            app (flask.Flask): The flask app to initialise with\n        '''\n\n        app.ldap3_login_manager = self\n\n        servers = list(self._server_pool)\n        for s in servers:\n            self._server_pool.remove(s)\n\n        self.init_config(app.config)\n\n        if hasattr(app, 'teardown_appcontext'):\n            app.teardown_appcontext(self.teardown)\n        else:  # pragma: no cover\n            app.teardown_request(self.teardown)\n\n        self.app = app", "code_tokens": ["def", "init_app", "(", "self", ",", "app", ")", ":", "app", ".", "ldap3_login_manager", "=", "self", "servers", "=", "list", "(", "self", ".", "_server_pool", ")", "for", "s", "in", "servers", ":", "self", ".", "_server_pool", ".", "remove", "(", "s", ")", "self", ".", "init_config", "(", "app", ".", "config", ")", "if", "hasattr", "(", "app", ",", "'teardown_appcontext'", ")", ":", "app", ".", "teardown_appcontext", "(", "self", ".", "teardown", ")", "else", ":", "# pragma: no cover", "app", ".", "teardown_request", "(", "self", ".", "teardown", ")", "self", ".", "app", "=", "app"], "docstring": "Configures this extension with the given app. This registers an\n        ``teardown_appcontext`` call, and attaches this ``LDAP3LoginManager``\n        to it as ``app.ldap3_login_manager``.\n\n        Args:\n            app (flask.Flask): The flask app to initialise with", "docstring_tokens": ["Configures", "this", "extension", "with", "the", "given", "app", ".", "This", "registers", "an", "teardown_appcontext", "call", "and", "attaches", "this", "LDAP3LoginManager", "to", "it", "as", "app", ".", "ldap3_login_manager", "."], "sha": "3cf0faff52d0e04d4813119a2ba36d706e6fb31f", "url": "https://github.com/nickw444/flask-ldap3-login/blob/3cf0faff52d0e04d4813119a2ba36d706e6fb31f/flask_ldap3_login/__init__.py#L63-L86", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/utils/traitlets.py", "func_name": "HasTraits.class_traits", "original_string": "def class_traits(cls, **metadata):\n        \"\"\"Get a list of all the traits of this class.\n\n        This method is just like the :meth:`traits` method, but is unbound.\n\n        The TraitTypes returned don't know anything about the values\n        that the various HasTrait's instances are holding.\n\n        This follows the same algorithm as traits does and does not allow\n        for any simple way of specifying merely that a metadata name\n        exists, but has any value.  This is because get_metadata returns\n        None if a metadata key doesn't exist.\n        \"\"\"\n        traits = dict([memb for memb in getmembers(cls) if \\\n                     isinstance(memb[1], TraitType)])\n\n        if len(metadata) == 0:\n            return traits\n\n        for meta_name, meta_eval in metadata.items():\n            if type(meta_eval) is not FunctionType:\n                metadata[meta_name] = _SimpleTest(meta_eval)\n\n        result = {}\n        for name, trait in traits.items():\n            for meta_name, meta_eval in metadata.items():\n                if not meta_eval(trait.get_metadata(meta_name)):\n                    break\n            else:\n                result[name] = trait\n\n        return result", "language": "python", "code": "def class_traits(cls, **metadata):\n        \"\"\"Get a list of all the traits of this class.\n\n        This method is just like the :meth:`traits` method, but is unbound.\n\n        The TraitTypes returned don't know anything about the values\n        that the various HasTrait's instances are holding.\n\n        This follows the same algorithm as traits does and does not allow\n        for any simple way of specifying merely that a metadata name\n        exists, but has any value.  This is because get_metadata returns\n        None if a metadata key doesn't exist.\n        \"\"\"\n        traits = dict([memb for memb in getmembers(cls) if \\\n                     isinstance(memb[1], TraitType)])\n\n        if len(metadata) == 0:\n            return traits\n\n        for meta_name, meta_eval in metadata.items():\n            if type(meta_eval) is not FunctionType:\n                metadata[meta_name] = _SimpleTest(meta_eval)\n\n        result = {}\n        for name, trait in traits.items():\n            for meta_name, meta_eval in metadata.items():\n                if not meta_eval(trait.get_metadata(meta_name)):\n                    break\n            else:\n                result[name] = trait\n\n        return result", "code_tokens": ["def", "class_traits", "(", "cls", ",", "*", "*", "metadata", ")", ":", "traits", "=", "dict", "(", "[", "memb", "for", "memb", "in", "getmembers", "(", "cls", ")", "if", "isinstance", "(", "memb", "[", "1", "]", ",", "TraitType", ")", "]", ")", "if", "len", "(", "metadata", ")", "==", "0", ":", "return", "traits", "for", "meta_name", ",", "meta_eval", "in", "metadata", ".", "items", "(", ")", ":", "if", "type", "(", "meta_eval", ")", "is", "not", "FunctionType", ":", "metadata", "[", "meta_name", "]", "=", "_SimpleTest", "(", "meta_eval", ")", "result", "=", "{", "}", "for", "name", ",", "trait", "in", "traits", ".", "items", "(", ")", ":", "for", "meta_name", ",", "meta_eval", "in", "metadata", ".", "items", "(", ")", ":", "if", "not", "meta_eval", "(", "trait", ".", "get_metadata", "(", "meta_name", ")", ")", ":", "break", "else", ":", "result", "[", "name", "]", "=", "trait", "return", "result"], "docstring": "Get a list of all the traits of this class.\n\n        This method is just like the :meth:`traits` method, but is unbound.\n\n        The TraitTypes returned don't know anything about the values\n        that the various HasTrait's instances are holding.\n\n        This follows the same algorithm as traits does and does not allow\n        for any simple way of specifying merely that a metadata name\n        exists, but has any value.  This is because get_metadata returns\n        None if a metadata key doesn't exist.", "docstring_tokens": ["Get", "a", "list", "of", "all", "the", "traits", "of", "this", "class", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/utils/traitlets.py#L529-L560", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/gerrit.py", "func_name": "GerritClient.version", "original_string": "def version(self):\n        \"\"\"Return the Gerrit server version.\"\"\"\n\n        if self._version:\n            return self._version\n\n        cmd = self.gerrit_cmd + \" %s \" % (GerritClient.CMD_VERSION)\n\n        logger.debug(\"Getting version: %s\" % (cmd))\n        raw_data = self.__execute(cmd)\n        raw_data = str(raw_data, \"UTF-8\")\n        logger.debug(\"Gerrit version: %s\" % (raw_data))\n\n        # output: gerrit version 2.10-rc1-988-g333a9dd\n        m = re.match(GerritClient.VERSION_REGEX, raw_data)\n\n        if not m:\n            cause = \"Invalid gerrit version %s\" % raw_data\n            raise BackendError(cause=cause)\n\n        try:\n            mayor = int(m.group(1))\n            minor = int(m.group(2))\n        except Exception:\n            cause = \"Gerrit client could not determine the server version.\"\n            raise BackendError(cause=cause)\n\n        self._version = [mayor, minor]\n        return self._version", "language": "python", "code": "def version(self):\n        \"\"\"Return the Gerrit server version.\"\"\"\n\n        if self._version:\n            return self._version\n\n        cmd = self.gerrit_cmd + \" %s \" % (GerritClient.CMD_VERSION)\n\n        logger.debug(\"Getting version: %s\" % (cmd))\n        raw_data = self.__execute(cmd)\n        raw_data = str(raw_data, \"UTF-8\")\n        logger.debug(\"Gerrit version: %s\" % (raw_data))\n\n        # output: gerrit version 2.10-rc1-988-g333a9dd\n        m = re.match(GerritClient.VERSION_REGEX, raw_data)\n\n        if not m:\n            cause = \"Invalid gerrit version %s\" % raw_data\n            raise BackendError(cause=cause)\n\n        try:\n            mayor = int(m.group(1))\n            minor = int(m.group(2))\n        except Exception:\n            cause = \"Gerrit client could not determine the server version.\"\n            raise BackendError(cause=cause)\n\n        self._version = [mayor, minor]\n        return self._version", "code_tokens": ["def", "version", "(", "self", ")", ":", "if", "self", ".", "_version", ":", "return", "self", ".", "_version", "cmd", "=", "self", ".", "gerrit_cmd", "+", "\" %s \"", "%", "(", "GerritClient", ".", "CMD_VERSION", ")", "logger", ".", "debug", "(", "\"Getting version: %s\"", "%", "(", "cmd", ")", ")", "raw_data", "=", "self", ".", "__execute", "(", "cmd", ")", "raw_data", "=", "str", "(", "raw_data", ",", "\"UTF-8\"", ")", "logger", ".", "debug", "(", "\"Gerrit version: %s\"", "%", "(", "raw_data", ")", ")", "# output: gerrit version 2.10-rc1-988-g333a9dd", "m", "=", "re", ".", "match", "(", "GerritClient", ".", "VERSION_REGEX", ",", "raw_data", ")", "if", "not", "m", ":", "cause", "=", "\"Invalid gerrit version %s\"", "%", "raw_data", "raise", "BackendError", "(", "cause", "=", "cause", ")", "try", ":", "mayor", "=", "int", "(", "m", ".", "group", "(", "1", ")", ")", "minor", "=", "int", "(", "m", ".", "group", "(", "2", ")", ")", "except", "Exception", ":", "cause", "=", "\"Gerrit client could not determine the server version.\"", "raise", "BackendError", "(", "cause", "=", "cause", ")", "self", ".", "_version", "=", "[", "mayor", ",", "minor", "]", "return", "self", ".", "_version"], "docstring": "Return the Gerrit server version.", "docstring_tokens": ["Return", "the", "Gerrit", "server", "version", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/gerrit.py#L327-L355", "partition": "test"}
{"repo": "snare/scruffy", "path": "scruffy/state.py", "func_name": "State.save", "original_string": "def save(self):\n        \"\"\"\n        Save the state to a file.\n        \"\"\"\n        with open(self.path, 'w') as f:\n            f.write(yaml.dump(dict(self.d)))", "language": "python", "code": "def save(self):\n        \"\"\"\n        Save the state to a file.\n        \"\"\"\n        with open(self.path, 'w') as f:\n            f.write(yaml.dump(dict(self.d)))", "code_tokens": ["def", "save", "(", "self", ")", ":", "with", "open", "(", "self", ".", "path", ",", "'w'", ")", "as", "f", ":", "f", ".", "write", "(", "yaml", ".", "dump", "(", "dict", "(", "self", ".", "d", ")", ")", ")"], "docstring": "Save the state to a file.", "docstring_tokens": ["Save", "the", "state", "to", "a", "file", "."], "sha": "0fedc08cfdb6db927ff93c09f25f24ce5a04c541", "url": "https://github.com/snare/scruffy/blob/0fedc08cfdb6db927ff93c09f25f24ce5a04c541/scruffy/state.py#L54-L59", "partition": "test"}
{"repo": "ralphbean/bugwarrior", "path": "bugwarrior/config.py", "func_name": "oracle_eval", "original_string": "def oracle_eval(command):\n    \"\"\" Retrieve password from the given command \"\"\"\n    p = subprocess.Popen(\n        command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    p.wait()\n    if p.returncode == 0:\n        return p.stdout.readline().strip().decode('utf-8')\n    else:\n        die(\n            \"Error retrieving password: `{command}` returned '{error}'\".format(\n                command=command, error=p.stderr.read().strip()))", "language": "python", "code": "def oracle_eval(command):\n    \"\"\" Retrieve password from the given command \"\"\"\n    p = subprocess.Popen(\n        command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    p.wait()\n    if p.returncode == 0:\n        return p.stdout.readline().strip().decode('utf-8')\n    else:\n        die(\n            \"Error retrieving password: `{command}` returned '{error}'\".format(\n                command=command, error=p.stderr.read().strip()))", "code_tokens": ["def", "oracle_eval", "(", "command", ")", ":", "p", "=", "subprocess", ".", "Popen", "(", "command", ",", "shell", "=", "True", ",", "stdout", "=", "subprocess", ".", "PIPE", ",", "stderr", "=", "subprocess", ".", "PIPE", ")", "p", ".", "wait", "(", ")", "if", "p", ".", "returncode", "==", "0", ":", "return", "p", ".", "stdout", ".", "readline", "(", ")", ".", "strip", "(", ")", ".", "decode", "(", "'utf-8'", ")", "else", ":", "die", "(", "\"Error retrieving password: `{command}` returned '{error}'\"", ".", "format", "(", "command", "=", "command", ",", "error", "=", "p", ".", "stderr", ".", "read", "(", ")", ".", "strip", "(", ")", ")", ")"], "docstring": "Retrieve password from the given command", "docstring_tokens": ["Retrieve", "password", "from", "the", "given", "command"], "sha": "b2a5108f7b40cb0c437509b64eaa28f941f7ac8b", "url": "https://github.com/ralphbean/bugwarrior/blob/b2a5108f7b40cb0c437509b64eaa28f941f7ac8b/bugwarrior/config.py#L98-L108", "partition": "test"}
{"repo": "MaxStrange/AudioSegment", "path": "algorithms/asa.py", "func_name": "_update_segmentation_mask", "original_string": "def _update_segmentation_mask(segmentation_mask, onset_fronts, offset_fronts, onset_front_id, offset_front_id_most_overlap):\n    \"\"\"\n    Returns an updated segmentation mask such that the input `segmentation_mask` has been updated by segmenting between\n    `onset_front_id` and `offset_front_id`, as found in `onset_fronts` and `offset_fronts`, respectively.\n\n    This function also returns the onset_fronts and offset_fronts matrices, updated so that any fronts that are of\n    less than 3 channels wide are removed.\n\n    This function also returns a boolean value indicating whether the onset channel went to completion.\n\n    Specifically, segments by doing the following:\n\n    - Going across frequencies in the onset_front,\n    - add the segment mask ID (the onset front ID) to all samples between the onset_front and the offset_front,\n      if the offset_front is in that frequency.\n\n    Possible scenarios:\n\n    Fronts line up completely:\n\n    ::\n\n        |   |       S S S\n        |   |  =>   S S S\n        |   |       S S S\n        |   |       S S S\n\n    Onset front starts before offset front:\n\n    ::\n\n        |           |\n        |   |       S S S\n        |   |  =>   S S S\n        |   |       S S S\n\n    Onset front ends after offset front:\n\n    ::\n\n        |   |       S S S\n        |   |  =>   S S S\n        |   |       S S S\n        |           |\n\n    Onset front starts before and ends after offset front:\n\n    ::\n\n        |           |\n        |   |  =>   S S S\n        |   |       S S S\n        |           |\n\n    The above three options in reverse:\n\n    ::\n\n            |       |S S|           |\n        |S S|       |S S|       |S S|\n        |S S|       |S S|       |S S|\n        |S S|           |           |\n\n    There is one last scenario:\n\n    ::\n\n        |   |\n        \\   /\n         \\ /\n         / \\\n        |   |\n\n    Where the offset and onset fronts cross one another. If this happens, we simply\n    reverse the indices and accept:\n\n    ::\n\n        |sss|\n        \\sss/\n         \\s/\n         /s\\\n        |sss|\n\n    The other option would be to destroy the offset front from the crossover point on, and\n    then search for a new offset front for the rest of the onset front.\n    \"\"\"\n    # Get the portions of the onset and offset fronts that overlap and are consecutive\n    onset_front_overlap, offset_front_overlap = _get_consecutive_and_overlapping_fronts(onset_fronts, offset_fronts, onset_front_id, offset_front_id_most_overlap)\n    onset_front = _get_front_idxs_from_id(onset_fronts, onset_front_id)\n    offset_front = _get_front_idxs_from_id(offset_fronts, offset_front_id_most_overlap)\n    msg = \"Onset front {} and offset front {} result in consecutive overlapping portions of (on) {} and (off) {}, one of which is empty\".format(\n        onset_front, offset_front, onset_front_overlap, offset_front_overlap\n    )\n    assert onset_front_overlap, msg\n    assert offset_front_overlap, msg\n    onset_front = onset_front_overlap\n    offset_front = offset_front_overlap\n\n    # Figure out which frequencies will go in the segment\n    flow_on, _slow_on = onset_front[0]\n    fhigh_on, _shigh_on = onset_front[-1]\n    flow_off, _slow_off = offset_front[0]\n    fhigh_off, _shigh_off = offset_front[-1]\n    flow = max(flow_on, flow_off)\n    fhigh = min(fhigh_on, fhigh_off)\n\n    # Update all the masks with the segment\n    for fidx, _freqchan in enumerate(segmentation_mask[flow:fhigh + 1, :], start=flow):\n        assert fidx >= flow, \"Frequency index is {}, but we should have started at {}\".format(fidx, flow)\n        assert (fidx - flow) < len(onset_front), \"Frequency index {} minus starting frequency {} is too large for nfrequencies {} in onset front {}\".format(\n            fidx, flow, len(onset_front), onset_front\n        )\n        assert (fidx - flow) < len(offset_front), \"Frequency index {} minus starting frequency {} is too large for nfrequencies {} in offset front {}\".format(\n            fidx, flow, len(offset_front), offset_front\n        )\n        _, beg = onset_front[fidx - flow]\n        _, end = offset_front[fidx - flow]\n        if beg > end:\n            end, beg = beg, end\n        assert end >= beg\n        segmentation_mask[fidx, beg:end + 1] = onset_front_id\n        onset_fronts[fidx, (beg + 1):(end + 1)] = 0\n        offset_fronts[fidx, (beg + 1):(end + 1)] = 0\n    nfreqs_used_in_onset_front = (fidx - flow) + 1\n\n    # Update the other masks to delete fronts that have been used\n    indexes = np.arange(flow, fhigh + 1, 1, dtype=np.int64)\n    onset_front_sample_idxs_across_freqs = np.array([s for _, s in onset_front])\n    onset_front_sample_idxs_across_freqs_up_to_break = onset_front_sample_idxs_across_freqs[:nfreqs_used_in_onset_front]\n    offset_front_sample_idxs_across_freqs = np.array([s for _, s in offset_front])\n    offset_front_sample_idxs_across_freqs_up_to_break = offset_front_sample_idxs_across_freqs[:nfreqs_used_in_onset_front]\n\n    ## Remove the offset front from where we started to where we ended\n    offset_fronts[indexes[:nfreqs_used_in_onset_front], offset_front_sample_idxs_across_freqs_up_to_break] = 0\n\n    ## Remove the onset front from where we started to where we ended\n    onset_fronts[indexes[:nfreqs_used_in_onset_front], onset_front_sample_idxs_across_freqs_up_to_break] = 0\n\n    # Determine if we matched the entire onset front by checking if there is any more of this onset front in onset_fronts\n    whole_onset_front_matched = onset_front_id not in np.unique(onset_fronts)\n\n    return whole_onset_front_matched", "language": "python", "code": "def _update_segmentation_mask(segmentation_mask, onset_fronts, offset_fronts, onset_front_id, offset_front_id_most_overlap):\n    \"\"\"\n    Returns an updated segmentation mask such that the input `segmentation_mask` has been updated by segmenting between\n    `onset_front_id` and `offset_front_id`, as found in `onset_fronts` and `offset_fronts`, respectively.\n\n    This function also returns the onset_fronts and offset_fronts matrices, updated so that any fronts that are of\n    less than 3 channels wide are removed.\n\n    This function also returns a boolean value indicating whether the onset channel went to completion.\n\n    Specifically, segments by doing the following:\n\n    - Going across frequencies in the onset_front,\n    - add the segment mask ID (the onset front ID) to all samples between the onset_front and the offset_front,\n      if the offset_front is in that frequency.\n\n    Possible scenarios:\n\n    Fronts line up completely:\n\n    ::\n\n        |   |       S S S\n        |   |  =>   S S S\n        |   |       S S S\n        |   |       S S S\n\n    Onset front starts before offset front:\n\n    ::\n\n        |           |\n        |   |       S S S\n        |   |  =>   S S S\n        |   |       S S S\n\n    Onset front ends after offset front:\n\n    ::\n\n        |   |       S S S\n        |   |  =>   S S S\n        |   |       S S S\n        |           |\n\n    Onset front starts before and ends after offset front:\n\n    ::\n\n        |           |\n        |   |  =>   S S S\n        |   |       S S S\n        |           |\n\n    The above three options in reverse:\n\n    ::\n\n            |       |S S|           |\n        |S S|       |S S|       |S S|\n        |S S|       |S S|       |S S|\n        |S S|           |           |\n\n    There is one last scenario:\n\n    ::\n\n        |   |\n        \\   /\n         \\ /\n         / \\\n        |   |\n\n    Where the offset and onset fronts cross one another. If this happens, we simply\n    reverse the indices and accept:\n\n    ::\n\n        |sss|\n        \\sss/\n         \\s/\n         /s\\\n        |sss|\n\n    The other option would be to destroy the offset front from the crossover point on, and\n    then search for a new offset front for the rest of the onset front.\n    \"\"\"\n    # Get the portions of the onset and offset fronts that overlap and are consecutive\n    onset_front_overlap, offset_front_overlap = _get_consecutive_and_overlapping_fronts(onset_fronts, offset_fronts, onset_front_id, offset_front_id_most_overlap)\n    onset_front = _get_front_idxs_from_id(onset_fronts, onset_front_id)\n    offset_front = _get_front_idxs_from_id(offset_fronts, offset_front_id_most_overlap)\n    msg = \"Onset front {} and offset front {} result in consecutive overlapping portions of (on) {} and (off) {}, one of which is empty\".format(\n        onset_front, offset_front, onset_front_overlap, offset_front_overlap\n    )\n    assert onset_front_overlap, msg\n    assert offset_front_overlap, msg\n    onset_front = onset_front_overlap\n    offset_front = offset_front_overlap\n\n    # Figure out which frequencies will go in the segment\n    flow_on, _slow_on = onset_front[0]\n    fhigh_on, _shigh_on = onset_front[-1]\n    flow_off, _slow_off = offset_front[0]\n    fhigh_off, _shigh_off = offset_front[-1]\n    flow = max(flow_on, flow_off)\n    fhigh = min(fhigh_on, fhigh_off)\n\n    # Update all the masks with the segment\n    for fidx, _freqchan in enumerate(segmentation_mask[flow:fhigh + 1, :], start=flow):\n        assert fidx >= flow, \"Frequency index is {}, but we should have started at {}\".format(fidx, flow)\n        assert (fidx - flow) < len(onset_front), \"Frequency index {} minus starting frequency {} is too large for nfrequencies {} in onset front {}\".format(\n            fidx, flow, len(onset_front), onset_front\n        )\n        assert (fidx - flow) < len(offset_front), \"Frequency index {} minus starting frequency {} is too large for nfrequencies {} in offset front {}\".format(\n            fidx, flow, len(offset_front), offset_front\n        )\n        _, beg = onset_front[fidx - flow]\n        _, end = offset_front[fidx - flow]\n        if beg > end:\n            end, beg = beg, end\n        assert end >= beg\n        segmentation_mask[fidx, beg:end + 1] = onset_front_id\n        onset_fronts[fidx, (beg + 1):(end + 1)] = 0\n        offset_fronts[fidx, (beg + 1):(end + 1)] = 0\n    nfreqs_used_in_onset_front = (fidx - flow) + 1\n\n    # Update the other masks to delete fronts that have been used\n    indexes = np.arange(flow, fhigh + 1, 1, dtype=np.int64)\n    onset_front_sample_idxs_across_freqs = np.array([s for _, s in onset_front])\n    onset_front_sample_idxs_across_freqs_up_to_break = onset_front_sample_idxs_across_freqs[:nfreqs_used_in_onset_front]\n    offset_front_sample_idxs_across_freqs = np.array([s for _, s in offset_front])\n    offset_front_sample_idxs_across_freqs_up_to_break = offset_front_sample_idxs_across_freqs[:nfreqs_used_in_onset_front]\n\n    ## Remove the offset front from where we started to where we ended\n    offset_fronts[indexes[:nfreqs_used_in_onset_front], offset_front_sample_idxs_across_freqs_up_to_break] = 0\n\n    ## Remove the onset front from where we started to where we ended\n    onset_fronts[indexes[:nfreqs_used_in_onset_front], onset_front_sample_idxs_across_freqs_up_to_break] = 0\n\n    # Determine if we matched the entire onset front by checking if there is any more of this onset front in onset_fronts\n    whole_onset_front_matched = onset_front_id not in np.unique(onset_fronts)\n\n    return whole_onset_front_matched", "code_tokens": ["def", "_update_segmentation_mask", "(", "segmentation_mask", ",", "onset_fronts", ",", "offset_fronts", ",", "onset_front_id", ",", "offset_front_id_most_overlap", ")", ":", "# Get the portions of the onset and offset fronts that overlap and are consecutive", "onset_front_overlap", ",", "offset_front_overlap", "=", "_get_consecutive_and_overlapping_fronts", "(", "onset_fronts", ",", "offset_fronts", ",", "onset_front_id", ",", "offset_front_id_most_overlap", ")", "onset_front", "=", "_get_front_idxs_from_id", "(", "onset_fronts", ",", "onset_front_id", ")", "offset_front", "=", "_get_front_idxs_from_id", "(", "offset_fronts", ",", "offset_front_id_most_overlap", ")", "msg", "=", "\"Onset front {} and offset front {} result in consecutive overlapping portions of (on) {} and (off) {}, one of which is empty\"", ".", "format", "(", "onset_front", ",", "offset_front", ",", "onset_front_overlap", ",", "offset_front_overlap", ")", "assert", "onset_front_overlap", ",", "msg", "assert", "offset_front_overlap", ",", "msg", "onset_front", "=", "onset_front_overlap", "offset_front", "=", "offset_front_overlap", "# Figure out which frequencies will go in the segment", "flow_on", ",", "_slow_on", "=", "onset_front", "[", "0", "]", "fhigh_on", ",", "_shigh_on", "=", "onset_front", "[", "-", "1", "]", "flow_off", ",", "_slow_off", "=", "offset_front", "[", "0", "]", "fhigh_off", ",", "_shigh_off", "=", "offset_front", "[", "-", "1", "]", "flow", "=", "max", "(", "flow_on", ",", "flow_off", ")", "fhigh", "=", "min", "(", "fhigh_on", ",", "fhigh_off", ")", "# Update all the masks with the segment", "for", "fidx", ",", "_freqchan", "in", "enumerate", "(", "segmentation_mask", "[", "flow", ":", "fhigh", "+", "1", ",", ":", "]", ",", "start", "=", "flow", ")", ":", "assert", "fidx", ">=", "flow", ",", "\"Frequency index is {}, but we should have started at {}\"", ".", "format", "(", "fidx", ",", "flow", ")", "assert", "(", "fidx", "-", "flow", ")", "<", "len", "(", "onset_front", ")", ",", "\"Frequency index {} minus starting frequency {} is too large for nfrequencies {} in onset front {}\"", ".", "format", "(", "fidx", ",", "flow", ",", "len", "(", "onset_front", ")", ",", "onset_front", ")", "assert", "(", "fidx", "-", "flow", ")", "<", "len", "(", "offset_front", ")", ",", "\"Frequency index {} minus starting frequency {} is too large for nfrequencies {} in offset front {}\"", ".", "format", "(", "fidx", ",", "flow", ",", "len", "(", "offset_front", ")", ",", "offset_front", ")", "_", ",", "beg", "=", "onset_front", "[", "fidx", "-", "flow", "]", "_", ",", "end", "=", "offset_front", "[", "fidx", "-", "flow", "]", "if", "beg", ">", "end", ":", "end", ",", "beg", "=", "beg", ",", "end", "assert", "end", ">=", "beg", "segmentation_mask", "[", "fidx", ",", "beg", ":", "end", "+", "1", "]", "=", "onset_front_id", "onset_fronts", "[", "fidx", ",", "(", "beg", "+", "1", ")", ":", "(", "end", "+", "1", ")", "]", "=", "0", "offset_fronts", "[", "fidx", ",", "(", "beg", "+", "1", ")", ":", "(", "end", "+", "1", ")", "]", "=", "0", "nfreqs_used_in_onset_front", "=", "(", "fidx", "-", "flow", ")", "+", "1", "# Update the other masks to delete fronts that have been used", "indexes", "=", "np", ".", "arange", "(", "flow", ",", "fhigh", "+", "1", ",", "1", ",", "dtype", "=", "np", ".", "int64", ")", "onset_front_sample_idxs_across_freqs", "=", "np", ".", "array", "(", "[", "s", "for", "_", ",", "s", "in", "onset_front", "]", ")", "onset_front_sample_idxs_across_freqs_up_to_break", "=", "onset_front_sample_idxs_across_freqs", "[", ":", "nfreqs_used_in_onset_front", "]", "offset_front_sample_idxs_across_freqs", "=", "np", ".", "array", "(", "[", "s", "for", "_", ",", "s", "in", "offset_front", "]", ")", "offset_front_sample_idxs_across_freqs_up_to_break", "=", "offset_front_sample_idxs_across_freqs", "[", ":", "nfreqs_used_in_onset_front", "]", "## Remove the offset front from where we started to where we ended", "offset_fronts", "[", "indexes", "[", ":", "nfreqs_used_in_onset_front", "]", ",", "offset_front_sample_idxs_across_freqs_up_to_break", "]", "=", "0", "## Remove the onset front from where we started to where we ended", "onset_fronts", "[", "indexes", "[", ":", "nfreqs_used_in_onset_front", "]", ",", "onset_front_sample_idxs_across_freqs_up_to_break", "]", "=", "0", "# Determine if we matched the entire onset front by checking if there is any more of this onset front in onset_fronts", "whole_onset_front_matched", "=", "onset_front_id", "not", "in", "np", ".", "unique", "(", "onset_fronts", ")", "return", "whole_onset_front_matched"], "docstring": "Returns an updated segmentation mask such that the input `segmentation_mask` has been updated by segmenting between\n    `onset_front_id` and `offset_front_id`, as found in `onset_fronts` and `offset_fronts`, respectively.\n\n    This function also returns the onset_fronts and offset_fronts matrices, updated so that any fronts that are of\n    less than 3 channels wide are removed.\n\n    This function also returns a boolean value indicating whether the onset channel went to completion.\n\n    Specifically, segments by doing the following:\n\n    - Going across frequencies in the onset_front,\n    - add the segment mask ID (the onset front ID) to all samples between the onset_front and the offset_front,\n      if the offset_front is in that frequency.\n\n    Possible scenarios:\n\n    Fronts line up completely:\n\n    ::\n\n        |   |       S S S\n        |   |  =>   S S S\n        |   |       S S S\n        |   |       S S S\n\n    Onset front starts before offset front:\n\n    ::\n\n        |           |\n        |   |       S S S\n        |   |  =>   S S S\n        |   |       S S S\n\n    Onset front ends after offset front:\n\n    ::\n\n        |   |       S S S\n        |   |  =>   S S S\n        |   |       S S S\n        |           |\n\n    Onset front starts before and ends after offset front:\n\n    ::\n\n        |           |\n        |   |  =>   S S S\n        |   |       S S S\n        |           |\n\n    The above three options in reverse:\n\n    ::\n\n            |       |S S|           |\n        |S S|       |S S|       |S S|\n        |S S|       |S S|       |S S|\n        |S S|           |           |\n\n    There is one last scenario:\n\n    ::\n\n        |   |\n        \\   /\n         \\ /\n         / \\\n        |   |\n\n    Where the offset and onset fronts cross one another. If this happens, we simply\n    reverse the indices and accept:\n\n    ::\n\n        |sss|\n        \\sss/\n         \\s/\n         /s\\\n        |sss|\n\n    The other option would be to destroy the offset front from the crossover point on, and\n    then search for a new offset front for the rest of the onset front.", "docstring_tokens": ["Returns", "an", "updated", "segmentation", "mask", "such", "that", "the", "input", "segmentation_mask", "has", "been", "updated", "by", "segmenting", "between", "onset_front_id", "and", "offset_front_id", "as", "found", "in", "onset_fronts", "and", "offset_fronts", "respectively", "."], "sha": "1daefb8de626ddff3ff7016697c3ad31d262ecd6", "url": "https://github.com/MaxStrange/AudioSegment/blob/1daefb8de626ddff3ff7016697c3ad31d262ecd6/algorithms/asa.py#L433-L575", "partition": "test"}
{"repo": "oscarbranson/latools", "path": "latools/filtering/pca.py", "func_name": "pca_plot", "original_string": "def pca_plot(pca, dt, xlabs=None, mode='scatter', lognorm=True):\n    \"\"\"\n    Plot a fitted PCA, and all components.\n    \"\"\"\n    \n    nc = pca.n_components\n    f = np.arange(pca.n_features_)\n    cs = list(itertools.combinations(range(nc), 2))\n    \n    ind = ~np.apply_along_axis(any, 1, np.isnan(dt))\n\n    cylim = (pca.components_.min(), pca.components_.max())\n    yd = cylim[1] - cylim[0]\n    \n    # Make figure\n    fig, axs = plt.subplots(nc, nc, figsize=[3 * nc, nc * 3], tight_layout=True)\n            \n    for x, y in zip(*np.triu_indices(nc)):\n        if x == y:\n            tax = axs[x, y]\n            tax.bar(f, pca.components_[x], 0.8)\n            tax.set_xticks([])\n            tax.axhline(0, zorder=-1, c=(0,0,0,0.6))\n\n            # labels            \n            tax.set_ylim(cylim[0] - 0.2 * yd,\n                         cylim[1] + 0.2 * yd)\n\n            for xi, yi, lab in zip(f, pca.components_[x], xlabs):\n                if yi > 0:\n                    yo = yd * 0.03\n                    va = 'bottom'\n                else:\n                    yo = yd * -0.02\n                    va = 'top'\n\n                tax.text(xi, yi + yo, lab, ha='center', va=va, rotation=90, fontsize=8)\n\n        else:\n            xv = dt[ind, x]\n            yv = dt[ind, y]\n\n            if mode == 'scatter':\n                axs[x, y].scatter(xv, yv, alpha=0.2)\n                axs[y, x].scatter(yv, xv, alpha=0.2)\n            if mode == 'hist2d':\n                if lognorm:\n                    norm = mpl.colors.LogNorm()\n                else:\n                    norm = None\n                axs[x, y].hist2d(xv, yv, 50, cmap=plt.cm.Blues, norm=norm)\n                axs[y, x].hist2d(yv, xv, 50, cmap=plt.cm.Blues, norm=norm)\n\n        if x == 0:\n            axs[y, x].set_ylabel('PC{:.0f}'.format(y + 1))\n        if y == nc - 1:\n            axs[y, x].set_xlabel('PC{:.0f}'.format(x + 1))\n        \n    return fig, axs, xv, yv", "language": "python", "code": "def pca_plot(pca, dt, xlabs=None, mode='scatter', lognorm=True):\n    \"\"\"\n    Plot a fitted PCA, and all components.\n    \"\"\"\n    \n    nc = pca.n_components\n    f = np.arange(pca.n_features_)\n    cs = list(itertools.combinations(range(nc), 2))\n    \n    ind = ~np.apply_along_axis(any, 1, np.isnan(dt))\n\n    cylim = (pca.components_.min(), pca.components_.max())\n    yd = cylim[1] - cylim[0]\n    \n    # Make figure\n    fig, axs = plt.subplots(nc, nc, figsize=[3 * nc, nc * 3], tight_layout=True)\n            \n    for x, y in zip(*np.triu_indices(nc)):\n        if x == y:\n            tax = axs[x, y]\n            tax.bar(f, pca.components_[x], 0.8)\n            tax.set_xticks([])\n            tax.axhline(0, zorder=-1, c=(0,0,0,0.6))\n\n            # labels            \n            tax.set_ylim(cylim[0] - 0.2 * yd,\n                         cylim[1] + 0.2 * yd)\n\n            for xi, yi, lab in zip(f, pca.components_[x], xlabs):\n                if yi > 0:\n                    yo = yd * 0.03\n                    va = 'bottom'\n                else:\n                    yo = yd * -0.02\n                    va = 'top'\n\n                tax.text(xi, yi + yo, lab, ha='center', va=va, rotation=90, fontsize=8)\n\n        else:\n            xv = dt[ind, x]\n            yv = dt[ind, y]\n\n            if mode == 'scatter':\n                axs[x, y].scatter(xv, yv, alpha=0.2)\n                axs[y, x].scatter(yv, xv, alpha=0.2)\n            if mode == 'hist2d':\n                if lognorm:\n                    norm = mpl.colors.LogNorm()\n                else:\n                    norm = None\n                axs[x, y].hist2d(xv, yv, 50, cmap=plt.cm.Blues, norm=norm)\n                axs[y, x].hist2d(yv, xv, 50, cmap=plt.cm.Blues, norm=norm)\n\n        if x == 0:\n            axs[y, x].set_ylabel('PC{:.0f}'.format(y + 1))\n        if y == nc - 1:\n            axs[y, x].set_xlabel('PC{:.0f}'.format(x + 1))\n        \n    return fig, axs, xv, yv", "code_tokens": ["def", "pca_plot", "(", "pca", ",", "dt", ",", "xlabs", "=", "None", ",", "mode", "=", "'scatter'", ",", "lognorm", "=", "True", ")", ":", "nc", "=", "pca", ".", "n_components", "f", "=", "np", ".", "arange", "(", "pca", ".", "n_features_", ")", "cs", "=", "list", "(", "itertools", ".", "combinations", "(", "range", "(", "nc", ")", ",", "2", ")", ")", "ind", "=", "~", "np", ".", "apply_along_axis", "(", "any", ",", "1", ",", "np", ".", "isnan", "(", "dt", ")", ")", "cylim", "=", "(", "pca", ".", "components_", ".", "min", "(", ")", ",", "pca", ".", "components_", ".", "max", "(", ")", ")", "yd", "=", "cylim", "[", "1", "]", "-", "cylim", "[", "0", "]", "# Make figure", "fig", ",", "axs", "=", "plt", ".", "subplots", "(", "nc", ",", "nc", ",", "figsize", "=", "[", "3", "*", "nc", ",", "nc", "*", "3", "]", ",", "tight_layout", "=", "True", ")", "for", "x", ",", "y", "in", "zip", "(", "*", "np", ".", "triu_indices", "(", "nc", ")", ")", ":", "if", "x", "==", "y", ":", "tax", "=", "axs", "[", "x", ",", "y", "]", "tax", ".", "bar", "(", "f", ",", "pca", ".", "components_", "[", "x", "]", ",", "0.8", ")", "tax", ".", "set_xticks", "(", "[", "]", ")", "tax", ".", "axhline", "(", "0", ",", "zorder", "=", "-", "1", ",", "c", "=", "(", "0", ",", "0", ",", "0", ",", "0.6", ")", ")", "# labels            ", "tax", ".", "set_ylim", "(", "cylim", "[", "0", "]", "-", "0.2", "*", "yd", ",", "cylim", "[", "1", "]", "+", "0.2", "*", "yd", ")", "for", "xi", ",", "yi", ",", "lab", "in", "zip", "(", "f", ",", "pca", ".", "components_", "[", "x", "]", ",", "xlabs", ")", ":", "if", "yi", ">", "0", ":", "yo", "=", "yd", "*", "0.03", "va", "=", "'bottom'", "else", ":", "yo", "=", "yd", "*", "-", "0.02", "va", "=", "'top'", "tax", ".", "text", "(", "xi", ",", "yi", "+", "yo", ",", "lab", ",", "ha", "=", "'center'", ",", "va", "=", "va", ",", "rotation", "=", "90", ",", "fontsize", "=", "8", ")", "else", ":", "xv", "=", "dt", "[", "ind", ",", "x", "]", "yv", "=", "dt", "[", "ind", ",", "y", "]", "if", "mode", "==", "'scatter'", ":", "axs", "[", "x", ",", "y", "]", ".", "scatter", "(", "xv", ",", "yv", ",", "alpha", "=", "0.2", ")", "axs", "[", "y", ",", "x", "]", ".", "scatter", "(", "yv", ",", "xv", ",", "alpha", "=", "0.2", ")", "if", "mode", "==", "'hist2d'", ":", "if", "lognorm", ":", "norm", "=", "mpl", ".", "colors", ".", "LogNorm", "(", ")", "else", ":", "norm", "=", "None", "axs", "[", "x", ",", "y", "]", ".", "hist2d", "(", "xv", ",", "yv", ",", "50", ",", "cmap", "=", "plt", ".", "cm", ".", "Blues", ",", "norm", "=", "norm", ")", "axs", "[", "y", ",", "x", "]", ".", "hist2d", "(", "yv", ",", "xv", ",", "50", ",", "cmap", "=", "plt", ".", "cm", ".", "Blues", ",", "norm", "=", "norm", ")", "if", "x", "==", "0", ":", "axs", "[", "y", ",", "x", "]", ".", "set_ylabel", "(", "'PC{:.0f}'", ".", "format", "(", "y", "+", "1", ")", ")", "if", "y", "==", "nc", "-", "1", ":", "axs", "[", "y", ",", "x", "]", ".", "set_xlabel", "(", "'PC{:.0f}'", ".", "format", "(", "x", "+", "1", ")", ")", "return", "fig", ",", "axs", ",", "xv", ",", "yv"], "docstring": "Plot a fitted PCA, and all components.", "docstring_tokens": ["Plot", "a", "fitted", "PCA", "and", "all", "components", "."], "sha": "cd25a650cfee318152f234d992708511f7047fbe", "url": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/filtering/pca.py#L45-L103", "partition": "test"}
{"repo": "ibm-watson-iot/iot-python", "path": "src/wiotp/sdk/api/registry/types.py", "func_name": "DeviceTypes.create", "original_string": "def create(self, deviceType):\n        \"\"\"\n        Register one or more new device types, each request can contain a maximum of 512KB.\n        \"\"\"\n\n        r = self._apiClient.post(\"api/v0002/device/types\", deviceType)\n\n        if r.status_code == 201:\n            return DeviceType(apiClient=self._apiClient, **r.json())\n        else:\n            raise ApiException(r)", "language": "python", "code": "def create(self, deviceType):\n        \"\"\"\n        Register one or more new device types, each request can contain a maximum of 512KB.\n        \"\"\"\n\n        r = self._apiClient.post(\"api/v0002/device/types\", deviceType)\n\n        if r.status_code == 201:\n            return DeviceType(apiClient=self._apiClient, **r.json())\n        else:\n            raise ApiException(r)", "code_tokens": ["def", "create", "(", "self", ",", "deviceType", ")", ":", "r", "=", "self", ".", "_apiClient", ".", "post", "(", "\"api/v0002/device/types\"", ",", "deviceType", ")", "if", "r", ".", "status_code", "==", "201", ":", "return", "DeviceType", "(", "apiClient", "=", "self", ".", "_apiClient", ",", "*", "*", "r", ".", "json", "(", ")", ")", "else", ":", "raise", "ApiException", "(", "r", ")"], "docstring": "Register one or more new device types, each request can contain a maximum of 512KB.", "docstring_tokens": ["Register", "one", "or", "more", "new", "device", "types", "each", "request", "can", "contain", "a", "maximum", "of", "512KB", "."], "sha": "195f05adce3fba4ec997017e41e02ebd85c0c4cc", "url": "https://github.com/ibm-watson-iot/iot-python/blob/195f05adce3fba4ec997017e41e02ebd85c0c4cc/src/wiotp/sdk/api/registry/types.py#L139-L149", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/utils/dag_processing.py", "func_name": "DagFileProcessorManager.end", "original_string": "def end(self):\n        \"\"\"\n        Kill all child processes on exit since we don't want to leave\n        them as orphaned.\n        \"\"\"\n        pids_to_kill = self.get_all_pids()\n        if len(pids_to_kill) > 0:\n            # First try SIGTERM\n            this_process = psutil.Process(os.getpid())\n            # Only check child processes to ensure that we don't have a case\n            # where we kill the wrong process because a child process died\n            # but the PID got reused.\n            child_processes = [x for x in this_process.children(recursive=True)\n                               if x.is_running() and x.pid in pids_to_kill]\n            for child in child_processes:\n                self.log.info(\"Terminating child PID: %s\", child.pid)\n                child.terminate()\n            # TODO: Remove magic number\n            timeout = 5\n            self.log.info(\"Waiting up to %s seconds for processes to exit...\", timeout)\n            try:\n                psutil.wait_procs(\n                    child_processes, timeout=timeout,\n                    callback=lambda x: self.log.info('Terminated PID %s', x.pid))\n            except psutil.TimeoutExpired:\n                self.log.debug(\"Ran out of time while waiting for processes to exit\")\n\n            # Then SIGKILL\n            child_processes = [x for x in this_process.children(recursive=True)\n                               if x.is_running() and x.pid in pids_to_kill]\n            if len(child_processes) > 0:\n                self.log.info(\"SIGKILL processes that did not terminate gracefully\")\n                for child in child_processes:\n                    self.log.info(\"Killing child PID: %s\", child.pid)\n                    child.kill()\n                    child.wait()", "language": "python", "code": "def end(self):\n        \"\"\"\n        Kill all child processes on exit since we don't want to leave\n        them as orphaned.\n        \"\"\"\n        pids_to_kill = self.get_all_pids()\n        if len(pids_to_kill) > 0:\n            # First try SIGTERM\n            this_process = psutil.Process(os.getpid())\n            # Only check child processes to ensure that we don't have a case\n            # where we kill the wrong process because a child process died\n            # but the PID got reused.\n            child_processes = [x for x in this_process.children(recursive=True)\n                               if x.is_running() and x.pid in pids_to_kill]\n            for child in child_processes:\n                self.log.info(\"Terminating child PID: %s\", child.pid)\n                child.terminate()\n            # TODO: Remove magic number\n            timeout = 5\n            self.log.info(\"Waiting up to %s seconds for processes to exit...\", timeout)\n            try:\n                psutil.wait_procs(\n                    child_processes, timeout=timeout,\n                    callback=lambda x: self.log.info('Terminated PID %s', x.pid))\n            except psutil.TimeoutExpired:\n                self.log.debug(\"Ran out of time while waiting for processes to exit\")\n\n            # Then SIGKILL\n            child_processes = [x for x in this_process.children(recursive=True)\n                               if x.is_running() and x.pid in pids_to_kill]\n            if len(child_processes) > 0:\n                self.log.info(\"SIGKILL processes that did not terminate gracefully\")\n                for child in child_processes:\n                    self.log.info(\"Killing child PID: %s\", child.pid)\n                    child.kill()\n                    child.wait()", "code_tokens": ["def", "end", "(", "self", ")", ":", "pids_to_kill", "=", "self", ".", "get_all_pids", "(", ")", "if", "len", "(", "pids_to_kill", ")", ">", "0", ":", "# First try SIGTERM", "this_process", "=", "psutil", ".", "Process", "(", "os", ".", "getpid", "(", ")", ")", "# Only check child processes to ensure that we don't have a case", "# where we kill the wrong process because a child process died", "# but the PID got reused.", "child_processes", "=", "[", "x", "for", "x", "in", "this_process", ".", "children", "(", "recursive", "=", "True", ")", "if", "x", ".", "is_running", "(", ")", "and", "x", ".", "pid", "in", "pids_to_kill", "]", "for", "child", "in", "child_processes", ":", "self", ".", "log", ".", "info", "(", "\"Terminating child PID: %s\"", ",", "child", ".", "pid", ")", "child", ".", "terminate", "(", ")", "# TODO: Remove magic number", "timeout", "=", "5", "self", ".", "log", ".", "info", "(", "\"Waiting up to %s seconds for processes to exit...\"", ",", "timeout", ")", "try", ":", "psutil", ".", "wait_procs", "(", "child_processes", ",", "timeout", "=", "timeout", ",", "callback", "=", "lambda", "x", ":", "self", ".", "log", ".", "info", "(", "'Terminated PID %s'", ",", "x", ".", "pid", ")", ")", "except", "psutil", ".", "TimeoutExpired", ":", "self", ".", "log", ".", "debug", "(", "\"Ran out of time while waiting for processes to exit\"", ")", "# Then SIGKILL", "child_processes", "=", "[", "x", "for", "x", "in", "this_process", ".", "children", "(", "recursive", "=", "True", ")", "if", "x", ".", "is_running", "(", ")", "and", "x", ".", "pid", "in", "pids_to_kill", "]", "if", "len", "(", "child_processes", ")", ">", "0", ":", "self", ".", "log", ".", "info", "(", "\"SIGKILL processes that did not terminate gracefully\"", ")", "for", "child", "in", "child_processes", ":", "self", ".", "log", ".", "info", "(", "\"Killing child PID: %s\"", ",", "child", ".", "pid", ")", "child", ".", "kill", "(", ")", "child", ".", "wait", "(", ")"], "docstring": "Kill all child processes on exit since we don't want to leave\n        them as orphaned.", "docstring_tokens": ["Kill", "all", "child", "processes", "on", "exit", "since", "we", "don", "t", "want", "to", "leave", "them", "as", "orphaned", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1285-L1320", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/display.py", "func_name": "display_html", "original_string": "def display_html(*objs, **kwargs):\n    \"\"\"Display the HTML representation of an object.\n\n    Parameters\n    ----------\n    objs : tuple of objects\n        The Python objects to display, or if raw=True raw HTML data to\n        display.\n    raw : bool\n        Are the data objects raw data or Python objects that need to be\n        formatted before display? [default: False]\n    \"\"\"\n    raw = kwargs.pop('raw',False)\n    if raw:\n        for obj in objs:\n            publish_html(obj)\n    else:\n        display(*objs, include=['text/plain','text/html'])", "language": "python", "code": "def display_html(*objs, **kwargs):\n    \"\"\"Display the HTML representation of an object.\n\n    Parameters\n    ----------\n    objs : tuple of objects\n        The Python objects to display, or if raw=True raw HTML data to\n        display.\n    raw : bool\n        Are the data objects raw data or Python objects that need to be\n        formatted before display? [default: False]\n    \"\"\"\n    raw = kwargs.pop('raw',False)\n    if raw:\n        for obj in objs:\n            publish_html(obj)\n    else:\n        display(*objs, include=['text/plain','text/html'])", "code_tokens": ["def", "display_html", "(", "*", "objs", ",", "*", "*", "kwargs", ")", ":", "raw", "=", "kwargs", ".", "pop", "(", "'raw'", ",", "False", ")", "if", "raw", ":", "for", "obj", "in", "objs", ":", "publish_html", "(", "obj", ")", "else", ":", "display", "(", "*", "objs", ",", "include", "=", "[", "'text/plain'", ",", "'text/html'", "]", ")"], "docstring": "Display the HTML representation of an object.\n\n    Parameters\n    ----------\n    objs : tuple of objects\n        The Python objects to display, or if raw=True raw HTML data to\n        display.\n    raw : bool\n        Are the data objects raw data or Python objects that need to be\n        formatted before display? [default: False]", "docstring_tokens": ["Display", "the", "HTML", "representation", "of", "an", "object", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/display.py#L87-L104", "partition": "test"}
{"repo": "myint/autoflake", "path": "autoflake.py", "func_name": "standard_paths", "original_string": "def standard_paths():\n    \"\"\"Yield paths to standard modules.\"\"\"\n    for is_plat_spec in [True, False]:\n        path = distutils.sysconfig.get_python_lib(standard_lib=True,\n                                                  plat_specific=is_plat_spec)\n\n        for name in os.listdir(path):\n            yield name\n\n        try:\n            for name in os.listdir(os.path.join(path, 'lib-dynload')):\n                yield name\n        except OSError:  # pragma: no cover\n            pass", "language": "python", "code": "def standard_paths():\n    \"\"\"Yield paths to standard modules.\"\"\"\n    for is_plat_spec in [True, False]:\n        path = distutils.sysconfig.get_python_lib(standard_lib=True,\n                                                  plat_specific=is_plat_spec)\n\n        for name in os.listdir(path):\n            yield name\n\n        try:\n            for name in os.listdir(os.path.join(path, 'lib-dynload')):\n                yield name\n        except OSError:  # pragma: no cover\n            pass", "code_tokens": ["def", "standard_paths", "(", ")", ":", "for", "is_plat_spec", "in", "[", "True", ",", "False", "]", ":", "path", "=", "distutils", ".", "sysconfig", ".", "get_python_lib", "(", "standard_lib", "=", "True", ",", "plat_specific", "=", "is_plat_spec", ")", "for", "name", "in", "os", ".", "listdir", "(", "path", ")", ":", "yield", "name", "try", ":", "for", "name", "in", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'lib-dynload'", ")", ")", ":", "yield", "name", "except", "OSError", ":", "# pragma: no cover", "pass"], "docstring": "Yield paths to standard modules.", "docstring_tokens": ["Yield", "paths", "to", "standard", "modules", "."], "sha": "68fea68646922b920d55975f9f2adaeafd84df4f", "url": "https://github.com/myint/autoflake/blob/68fea68646922b920d55975f9f2adaeafd84df4f/autoflake.py#L69-L82", "partition": "test"}
{"repo": "JukeboxPipeline/jukedj", "path": "src/jukedj/models.py", "func_name": "create_all_tasks", "original_string": "def create_all_tasks(element):\n    \"\"\"Create all tasks for the element\n\n    :param element: The shot or asset that needs tasks\n    :type element: :class:`muke.models.Shot` | :class:`muke.models.Asset`\n    :returns: None\n    :rtype: None\n    :raises: None\n    \"\"\"\n    prj = element.project\n    if isinstance(element, Asset):\n        flag=True\n    else:\n        flag=False\n    deps = prj.department_set.filter(assetflag=flag)\n    for d in deps:\n        t = Task(project=prj, department=d, element=element)\n        t.full_clean()\n        t.save()", "language": "python", "code": "def create_all_tasks(element):\n    \"\"\"Create all tasks for the element\n\n    :param element: The shot or asset that needs tasks\n    :type element: :class:`muke.models.Shot` | :class:`muke.models.Asset`\n    :returns: None\n    :rtype: None\n    :raises: None\n    \"\"\"\n    prj = element.project\n    if isinstance(element, Asset):\n        flag=True\n    else:\n        flag=False\n    deps = prj.department_set.filter(assetflag=flag)\n    for d in deps:\n        t = Task(project=prj, department=d, element=element)\n        t.full_clean()\n        t.save()", "code_tokens": ["def", "create_all_tasks", "(", "element", ")", ":", "prj", "=", "element", ".", "project", "if", "isinstance", "(", "element", ",", "Asset", ")", ":", "flag", "=", "True", "else", ":", "flag", "=", "False", "deps", "=", "prj", ".", "department_set", ".", "filter", "(", "assetflag", "=", "flag", ")", "for", "d", "in", "deps", ":", "t", "=", "Task", "(", "project", "=", "prj", ",", "department", "=", "d", ",", "element", "=", "element", ")", "t", ".", "full_clean", "(", ")", "t", ".", "save", "(", ")"], "docstring": "Create all tasks for the element\n\n    :param element: The shot or asset that needs tasks\n    :type element: :class:`muke.models.Shot` | :class:`muke.models.Asset`\n    :returns: None\n    :rtype: None\n    :raises: None", "docstring_tokens": ["Create", "all", "tasks", "for", "the", "element"], "sha": "d4159961c819c26792a278981ee68106ee15f3f3", "url": "https://github.com/JukeboxPipeline/jukedj/blob/d4159961c819c26792a278981ee68106ee15f3f3/src/jukedj/models.py#L490-L508", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/onset.py", "func_name": "onset_backtrack", "original_string": "def onset_backtrack(events, energy):\n    '''Backtrack detected onset events to the nearest preceding local\n    minimum of an energy function.\n\n    This function can be used to roll back the timing of detected onsets\n    from a detected peak amplitude to the preceding minimum.\n\n    This is most useful when using onsets to determine slice points for\n    segmentation, as described by [1]_.\n\n    .. [1] Jehan, Tristan.\n           \"Creating music by listening\"\n           Doctoral dissertation\n           Massachusetts Institute of Technology, 2005.\n\n    Parameters\n    ----------\n    events : np.ndarray, dtype=int\n        List of onset event frame indices, as computed by `onset_detect`\n\n    energy : np.ndarray, shape=(m,)\n        An energy function\n\n    Returns\n    -------\n    events_backtracked : np.ndarray, shape=events.shape\n        The input events matched to nearest preceding minima of `energy`.\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      offset=30, duration=2.0)\n    >>> oenv = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> # Detect events without backtracking\n    >>> onset_raw = librosa.onset.onset_detect(onset_envelope=oenv,\n    ...                                        backtrack=False)\n    >>> # Backtrack the events using the onset envelope\n    >>> onset_bt = librosa.onset.onset_backtrack(onset_raw, oenv)\n    >>> # Backtrack the events using the RMS values\n    >>> rms = librosa.feature.rms(S=np.abs(librosa.stft(y=y)))\n    >>> onset_bt_rms = librosa.onset.onset_backtrack(onset_raw, rms[0])\n\n    >>> # Plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure()\n    >>> plt.subplot(2,1,1)\n    >>> plt.plot(oenv, label='Onset strength')\n    >>> plt.vlines(onset_raw, 0, oenv.max(), label='Raw onsets')\n    >>> plt.vlines(onset_bt, 0, oenv.max(), label='Backtracked', color='r')\n    >>> plt.legend(frameon=True, framealpha=0.75)\n    >>> plt.subplot(2,1,2)\n    >>> plt.plot(rms[0], label='RMS')\n    >>> plt.vlines(onset_bt_rms, 0, rms.max(), label='Backtracked (RMS)', color='r')\n    >>> plt.legend(frameon=True, framealpha=0.75)\n    '''\n\n    # Find points where energy is non-increasing\n    # all points:  energy[i] <= energy[i-1]\n    # tail points: energy[i] < energy[i+1]\n    minima = np.flatnonzero((energy[1:-1] <= energy[:-2]) &\n                            (energy[1:-1] < energy[2:]))\n\n    # Pad on a 0, just in case we have onsets with no preceding minimum\n    # Shift by one to account for slicing in minima detection\n    minima = util.fix_frames(1 + minima, x_min=0)\n\n    # Only match going left from the detected events\n    return minima[util.match_events(events, minima, right=False)]", "language": "python", "code": "def onset_backtrack(events, energy):\n    '''Backtrack detected onset events to the nearest preceding local\n    minimum of an energy function.\n\n    This function can be used to roll back the timing of detected onsets\n    from a detected peak amplitude to the preceding minimum.\n\n    This is most useful when using onsets to determine slice points for\n    segmentation, as described by [1]_.\n\n    .. [1] Jehan, Tristan.\n           \"Creating music by listening\"\n           Doctoral dissertation\n           Massachusetts Institute of Technology, 2005.\n\n    Parameters\n    ----------\n    events : np.ndarray, dtype=int\n        List of onset event frame indices, as computed by `onset_detect`\n\n    energy : np.ndarray, shape=(m,)\n        An energy function\n\n    Returns\n    -------\n    events_backtracked : np.ndarray, shape=events.shape\n        The input events matched to nearest preceding minima of `energy`.\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      offset=30, duration=2.0)\n    >>> oenv = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> # Detect events without backtracking\n    >>> onset_raw = librosa.onset.onset_detect(onset_envelope=oenv,\n    ...                                        backtrack=False)\n    >>> # Backtrack the events using the onset envelope\n    >>> onset_bt = librosa.onset.onset_backtrack(onset_raw, oenv)\n    >>> # Backtrack the events using the RMS values\n    >>> rms = librosa.feature.rms(S=np.abs(librosa.stft(y=y)))\n    >>> onset_bt_rms = librosa.onset.onset_backtrack(onset_raw, rms[0])\n\n    >>> # Plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure()\n    >>> plt.subplot(2,1,1)\n    >>> plt.plot(oenv, label='Onset strength')\n    >>> plt.vlines(onset_raw, 0, oenv.max(), label='Raw onsets')\n    >>> plt.vlines(onset_bt, 0, oenv.max(), label='Backtracked', color='r')\n    >>> plt.legend(frameon=True, framealpha=0.75)\n    >>> plt.subplot(2,1,2)\n    >>> plt.plot(rms[0], label='RMS')\n    >>> plt.vlines(onset_bt_rms, 0, rms.max(), label='Backtracked (RMS)', color='r')\n    >>> plt.legend(frameon=True, framealpha=0.75)\n    '''\n\n    # Find points where energy is non-increasing\n    # all points:  energy[i] <= energy[i-1]\n    # tail points: energy[i] < energy[i+1]\n    minima = np.flatnonzero((energy[1:-1] <= energy[:-2]) &\n                            (energy[1:-1] < energy[2:]))\n\n    # Pad on a 0, just in case we have onsets with no preceding minimum\n    # Shift by one to account for slicing in minima detection\n    minima = util.fix_frames(1 + minima, x_min=0)\n\n    # Only match going left from the detected events\n    return minima[util.match_events(events, minima, right=False)]", "code_tokens": ["def", "onset_backtrack", "(", "events", ",", "energy", ")", ":", "# Find points where energy is non-increasing", "# all points:  energy[i] <= energy[i-1]", "# tail points: energy[i] < energy[i+1]", "minima", "=", "np", ".", "flatnonzero", "(", "(", "energy", "[", "1", ":", "-", "1", "]", "<=", "energy", "[", ":", "-", "2", "]", ")", "&", "(", "energy", "[", "1", ":", "-", "1", "]", "<", "energy", "[", "2", ":", "]", ")", ")", "# Pad on a 0, just in case we have onsets with no preceding minimum", "# Shift by one to account for slicing in minima detection", "minima", "=", "util", ".", "fix_frames", "(", "1", "+", "minima", ",", "x_min", "=", "0", ")", "# Only match going left from the detected events", "return", "minima", "[", "util", ".", "match_events", "(", "events", ",", "minima", ",", "right", "=", "False", ")", "]"], "docstring": "Backtrack detected onset events to the nearest preceding local\n    minimum of an energy function.\n\n    This function can be used to roll back the timing of detected onsets\n    from a detected peak amplitude to the preceding minimum.\n\n    This is most useful when using onsets to determine slice points for\n    segmentation, as described by [1]_.\n\n    .. [1] Jehan, Tristan.\n           \"Creating music by listening\"\n           Doctoral dissertation\n           Massachusetts Institute of Technology, 2005.\n\n    Parameters\n    ----------\n    events : np.ndarray, dtype=int\n        List of onset event frame indices, as computed by `onset_detect`\n\n    energy : np.ndarray, shape=(m,)\n        An energy function\n\n    Returns\n    -------\n    events_backtracked : np.ndarray, shape=events.shape\n        The input events matched to nearest preceding minima of `energy`.\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      offset=30, duration=2.0)\n    >>> oenv = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> # Detect events without backtracking\n    >>> onset_raw = librosa.onset.onset_detect(onset_envelope=oenv,\n    ...                                        backtrack=False)\n    >>> # Backtrack the events using the onset envelope\n    >>> onset_bt = librosa.onset.onset_backtrack(onset_raw, oenv)\n    >>> # Backtrack the events using the RMS values\n    >>> rms = librosa.feature.rms(S=np.abs(librosa.stft(y=y)))\n    >>> onset_bt_rms = librosa.onset.onset_backtrack(onset_raw, rms[0])\n\n    >>> # Plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure()\n    >>> plt.subplot(2,1,1)\n    >>> plt.plot(oenv, label='Onset strength')\n    >>> plt.vlines(onset_raw, 0, oenv.max(), label='Raw onsets')\n    >>> plt.vlines(onset_bt, 0, oenv.max(), label='Backtracked', color='r')\n    >>> plt.legend(frameon=True, framealpha=0.75)\n    >>> plt.subplot(2,1,2)\n    >>> plt.plot(rms[0], label='RMS')\n    >>> plt.vlines(onset_bt_rms, 0, rms.max(), label='Backtracked (RMS)', color='r')\n    >>> plt.legend(frameon=True, framealpha=0.75)", "docstring_tokens": ["Backtrack", "detected", "onset", "events", "to", "the", "nearest", "preceding", "local", "minimum", "of", "an", "energy", "function", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/onset.py#L336-L403", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/utils.py", "func_name": "message_to_dict", "original_string": "def message_to_dict(msg):\n    \"\"\"Convert an email message into a dictionary.\n\n    This function transforms an `email.message.Message` object\n    into a dictionary. Headers are stored as key:value pairs\n    while the body of the message is stored inside `body` key.\n    Body may have two other keys inside, 'plain', for plain body\n    messages and 'html', for HTML encoded messages.\n\n    The returned dictionary has the type `requests.structures.CaseInsensitiveDict`\n    due to same headers with different case formats can appear in\n    the same message.\n\n    :param msg: email message of type `email.message.Message`\n\n    :returns : dictionary of type `requests.structures.CaseInsensitiveDict`\n\n    :raises ParseError: when an error occurs transforming the message\n        to a dictionary\n    \"\"\"\n    def parse_headers(msg):\n        headers = {}\n\n        for header, value in msg.items():\n            hv = []\n\n            for text, charset in email.header.decode_header(value):\n                if type(text) == bytes:\n                    charset = charset if charset else 'utf-8'\n                    try:\n                        text = text.decode(charset, errors='surrogateescape')\n                    except (UnicodeError, LookupError):\n                        # Try again with a 7bit encoding\n                        text = text.decode('ascii', errors='surrogateescape')\n                hv.append(text)\n\n            v = ' '.join(hv)\n            headers[header] = v if v else None\n\n        return headers\n\n    def parse_payload(msg):\n        body = {}\n\n        if not msg.is_multipart():\n            payload = decode_payload(msg)\n            subtype = msg.get_content_subtype()\n            body[subtype] = [payload]\n        else:\n            # Include all the attached texts if it is multipart\n            # Ignores binary parts by default\n            for part in email.iterators.typed_subpart_iterator(msg):\n                payload = decode_payload(part)\n                subtype = part.get_content_subtype()\n                body.setdefault(subtype, []).append(payload)\n\n        return {k: '\\n'.join(v) for k, v in body.items()}\n\n    def decode_payload(msg_or_part):\n        charset = msg_or_part.get_content_charset('utf-8')\n        payload = msg_or_part.get_payload(decode=True)\n\n        try:\n            payload = payload.decode(charset, errors='surrogateescape')\n        except (UnicodeError, LookupError):\n            # Try again with a 7bit encoding\n            payload = payload.decode('ascii', errors='surrogateescape')\n        return payload\n\n    # The function starts here\n    message = requests.structures.CaseInsensitiveDict()\n\n    if isinstance(msg, mailbox.mboxMessage):\n        message['unixfrom'] = msg.get_from()\n    else:\n        message['unixfrom'] = None\n\n    try:\n        for k, v in parse_headers(msg).items():\n            message[k] = v\n        message['body'] = parse_payload(msg)\n    except UnicodeError as e:\n        raise ParseError(cause=str(e))\n\n    return message", "language": "python", "code": "def message_to_dict(msg):\n    \"\"\"Convert an email message into a dictionary.\n\n    This function transforms an `email.message.Message` object\n    into a dictionary. Headers are stored as key:value pairs\n    while the body of the message is stored inside `body` key.\n    Body may have two other keys inside, 'plain', for plain body\n    messages and 'html', for HTML encoded messages.\n\n    The returned dictionary has the type `requests.structures.CaseInsensitiveDict`\n    due to same headers with different case formats can appear in\n    the same message.\n\n    :param msg: email message of type `email.message.Message`\n\n    :returns : dictionary of type `requests.structures.CaseInsensitiveDict`\n\n    :raises ParseError: when an error occurs transforming the message\n        to a dictionary\n    \"\"\"\n    def parse_headers(msg):\n        headers = {}\n\n        for header, value in msg.items():\n            hv = []\n\n            for text, charset in email.header.decode_header(value):\n                if type(text) == bytes:\n                    charset = charset if charset else 'utf-8'\n                    try:\n                        text = text.decode(charset, errors='surrogateescape')\n                    except (UnicodeError, LookupError):\n                        # Try again with a 7bit encoding\n                        text = text.decode('ascii', errors='surrogateescape')\n                hv.append(text)\n\n            v = ' '.join(hv)\n            headers[header] = v if v else None\n\n        return headers\n\n    def parse_payload(msg):\n        body = {}\n\n        if not msg.is_multipart():\n            payload = decode_payload(msg)\n            subtype = msg.get_content_subtype()\n            body[subtype] = [payload]\n        else:\n            # Include all the attached texts if it is multipart\n            # Ignores binary parts by default\n            for part in email.iterators.typed_subpart_iterator(msg):\n                payload = decode_payload(part)\n                subtype = part.get_content_subtype()\n                body.setdefault(subtype, []).append(payload)\n\n        return {k: '\\n'.join(v) for k, v in body.items()}\n\n    def decode_payload(msg_or_part):\n        charset = msg_or_part.get_content_charset('utf-8')\n        payload = msg_or_part.get_payload(decode=True)\n\n        try:\n            payload = payload.decode(charset, errors='surrogateescape')\n        except (UnicodeError, LookupError):\n            # Try again with a 7bit encoding\n            payload = payload.decode('ascii', errors='surrogateescape')\n        return payload\n\n    # The function starts here\n    message = requests.structures.CaseInsensitiveDict()\n\n    if isinstance(msg, mailbox.mboxMessage):\n        message['unixfrom'] = msg.get_from()\n    else:\n        message['unixfrom'] = None\n\n    try:\n        for k, v in parse_headers(msg).items():\n            message[k] = v\n        message['body'] = parse_payload(msg)\n    except UnicodeError as e:\n        raise ParseError(cause=str(e))\n\n    return message", "code_tokens": ["def", "message_to_dict", "(", "msg", ")", ":", "def", "parse_headers", "(", "msg", ")", ":", "headers", "=", "{", "}", "for", "header", ",", "value", "in", "msg", ".", "items", "(", ")", ":", "hv", "=", "[", "]", "for", "text", ",", "charset", "in", "email", ".", "header", ".", "decode_header", "(", "value", ")", ":", "if", "type", "(", "text", ")", "==", "bytes", ":", "charset", "=", "charset", "if", "charset", "else", "'utf-8'", "try", ":", "text", "=", "text", ".", "decode", "(", "charset", ",", "errors", "=", "'surrogateescape'", ")", "except", "(", "UnicodeError", ",", "LookupError", ")", ":", "# Try again with a 7bit encoding", "text", "=", "text", ".", "decode", "(", "'ascii'", ",", "errors", "=", "'surrogateescape'", ")", "hv", ".", "append", "(", "text", ")", "v", "=", "' '", ".", "join", "(", "hv", ")", "headers", "[", "header", "]", "=", "v", "if", "v", "else", "None", "return", "headers", "def", "parse_payload", "(", "msg", ")", ":", "body", "=", "{", "}", "if", "not", "msg", ".", "is_multipart", "(", ")", ":", "payload", "=", "decode_payload", "(", "msg", ")", "subtype", "=", "msg", ".", "get_content_subtype", "(", ")", "body", "[", "subtype", "]", "=", "[", "payload", "]", "else", ":", "# Include all the attached texts if it is multipart", "# Ignores binary parts by default", "for", "part", "in", "email", ".", "iterators", ".", "typed_subpart_iterator", "(", "msg", ")", ":", "payload", "=", "decode_payload", "(", "part", ")", "subtype", "=", "part", ".", "get_content_subtype", "(", ")", "body", ".", "setdefault", "(", "subtype", ",", "[", "]", ")", ".", "append", "(", "payload", ")", "return", "{", "k", ":", "'\\n'", ".", "join", "(", "v", ")", "for", "k", ",", "v", "in", "body", ".", "items", "(", ")", "}", "def", "decode_payload", "(", "msg_or_part", ")", ":", "charset", "=", "msg_or_part", ".", "get_content_charset", "(", "'utf-8'", ")", "payload", "=", "msg_or_part", ".", "get_payload", "(", "decode", "=", "True", ")", "try", ":", "payload", "=", "payload", ".", "decode", "(", "charset", ",", "errors", "=", "'surrogateescape'", ")", "except", "(", "UnicodeError", ",", "LookupError", ")", ":", "# Try again with a 7bit encoding", "payload", "=", "payload", ".", "decode", "(", "'ascii'", ",", "errors", "=", "'surrogateescape'", ")", "return", "payload", "# The function starts here", "message", "=", "requests", ".", "structures", ".", "CaseInsensitiveDict", "(", ")", "if", "isinstance", "(", "msg", ",", "mailbox", ".", "mboxMessage", ")", ":", "message", "[", "'unixfrom'", "]", "=", "msg", ".", "get_from", "(", ")", "else", ":", "message", "[", "'unixfrom'", "]", "=", "None", "try", ":", "for", "k", ",", "v", "in", "parse_headers", "(", "msg", ")", ".", "items", "(", ")", ":", "message", "[", "k", "]", "=", "v", "message", "[", "'body'", "]", "=", "parse_payload", "(", "msg", ")", "except", "UnicodeError", "as", "e", ":", "raise", "ParseError", "(", "cause", "=", "str", "(", "e", ")", ")", "return", "message"], "docstring": "Convert an email message into a dictionary.\n\n    This function transforms an `email.message.Message` object\n    into a dictionary. Headers are stored as key:value pairs\n    while the body of the message is stored inside `body` key.\n    Body may have two other keys inside, 'plain', for plain body\n    messages and 'html', for HTML encoded messages.\n\n    The returned dictionary has the type `requests.structures.CaseInsensitiveDict`\n    due to same headers with different case formats can appear in\n    the same message.\n\n    :param msg: email message of type `email.message.Message`\n\n    :returns : dictionary of type `requests.structures.CaseInsensitiveDict`\n\n    :raises ParseError: when an error occurs transforming the message\n        to a dictionary", "docstring_tokens": ["Convert", "an", "email", "message", "into", "a", "dictionary", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/utils.py#L105-L189", "partition": "test"}
{"repo": "jaraco/jaraco.util", "path": "jaraco/util/subprocess.py", "func_name": "Popen_nonblocking", "original_string": "def Popen_nonblocking(*args, **kwargs):\n\t\"\"\"\n\tOpen a subprocess without blocking. Return a process handle with any\n\toutput streams replaced by queues of lines from that stream.\n\n\tUsage::\n\n\t\tproc = Popen_nonblocking(..., stdout=subprocess.PIPE)\n\t\ttry:\n\t\t\tout_line = proc.stdout.get_nowait()\n\t\texcept queue.Empty:\n\t\t\t\"no output available\"\n\t\telse:\n\t\t\thandle_output(out_line)\n\t\"\"\"\n\tkwargs.setdefault('close_fds', 'posix' in sys.builtin_module_names)\n\tkwargs.setdefault('bufsize', 1)\n\tproc = subprocess.Popen(*args, **kwargs)\n\tif proc.stdout:\n\t\tq = queue.Queue()\n\t\tt = threading.Thread(\n\t\t\ttarget=enqueue_lines,\n\t\t\targs=(proc.stdout, q))\n\t\tproc.stdout = q\n\t\t# thread dies with the parent\n\t\tt.daemon = True\n\t\tt.start()\n\tif proc.stderr:\n\t\tq = queue.Queue()\n\t\tt = threading.Thread(\n\t\t\ttarget=enqueue_lines,\n\t\t\targs=(proc.stderr, q))\n\t\tproc.stderr = q\n\t\tt.daemon = True\n\t\tt.start()\n\treturn proc", "language": "python", "code": "def Popen_nonblocking(*args, **kwargs):\n\t\"\"\"\n\tOpen a subprocess without blocking. Return a process handle with any\n\toutput streams replaced by queues of lines from that stream.\n\n\tUsage::\n\n\t\tproc = Popen_nonblocking(..., stdout=subprocess.PIPE)\n\t\ttry:\n\t\t\tout_line = proc.stdout.get_nowait()\n\t\texcept queue.Empty:\n\t\t\t\"no output available\"\n\t\telse:\n\t\t\thandle_output(out_line)\n\t\"\"\"\n\tkwargs.setdefault('close_fds', 'posix' in sys.builtin_module_names)\n\tkwargs.setdefault('bufsize', 1)\n\tproc = subprocess.Popen(*args, **kwargs)\n\tif proc.stdout:\n\t\tq = queue.Queue()\n\t\tt = threading.Thread(\n\t\t\ttarget=enqueue_lines,\n\t\t\targs=(proc.stdout, q))\n\t\tproc.stdout = q\n\t\t# thread dies with the parent\n\t\tt.daemon = True\n\t\tt.start()\n\tif proc.stderr:\n\t\tq = queue.Queue()\n\t\tt = threading.Thread(\n\t\t\ttarget=enqueue_lines,\n\t\t\targs=(proc.stderr, q))\n\t\tproc.stderr = q\n\t\tt.daemon = True\n\t\tt.start()\n\treturn proc", "code_tokens": ["def", "Popen_nonblocking", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "kwargs", ".", "setdefault", "(", "'close_fds'", ",", "'posix'", "in", "sys", ".", "builtin_module_names", ")", "kwargs", ".", "setdefault", "(", "'bufsize'", ",", "1", ")", "proc", "=", "subprocess", ".", "Popen", "(", "*", "args", ",", "*", "*", "kwargs", ")", "if", "proc", ".", "stdout", ":", "q", "=", "queue", ".", "Queue", "(", ")", "t", "=", "threading", ".", "Thread", "(", "target", "=", "enqueue_lines", ",", "args", "=", "(", "proc", ".", "stdout", ",", "q", ")", ")", "proc", ".", "stdout", "=", "q", "# thread dies with the parent", "t", ".", "daemon", "=", "True", "t", ".", "start", "(", ")", "if", "proc", ".", "stderr", ":", "q", "=", "queue", ".", "Queue", "(", ")", "t", "=", "threading", ".", "Thread", "(", "target", "=", "enqueue_lines", ",", "args", "=", "(", "proc", ".", "stderr", ",", "q", ")", ")", "proc", ".", "stderr", "=", "q", "t", ".", "daemon", "=", "True", "t", ".", "start", "(", ")", "return", "proc"], "docstring": "Open a subprocess without blocking. Return a process handle with any\n\toutput streams replaced by queues of lines from that stream.\n\n\tUsage::\n\n\t\tproc = Popen_nonblocking(..., stdout=subprocess.PIPE)\n\t\ttry:\n\t\t\tout_line = proc.stdout.get_nowait()\n\t\texcept queue.Empty:\n\t\t\t\"no output available\"\n\t\telse:\n\t\t\thandle_output(out_line)", "docstring_tokens": ["Open", "a", "subprocess", "without", "blocking", ".", "Return", "a", "process", "handle", "with", "any", "output", "streams", "replaced", "by", "queues", "of", "lines", "from", "that", "stream", "."], "sha": "f21071c64f165a5cf844db15e39356e1a47f4b02", "url": "https://github.com/jaraco/jaraco.util/blob/f21071c64f165a5cf844db15e39356e1a47f4b02/jaraco/util/subprocess.py#L17-L52", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/compiler/parser.py", "func_name": "_loc", "original_string": "def _loc(form: Union[LispForm, ISeq]) -> Optional[Tuple[int, int]]:\n    \"\"\"Fetch the location of the form in the original filename from the\n    input form, if it has metadata.\"\"\"\n    try:\n        meta = form.meta  # type: ignore\n        line = meta.get(reader.READER_LINE_KW)  # type: ignore\n        col = meta.get(reader.READER_COL_KW)  # type: ignore\n    except AttributeError:\n        return None\n    else:\n        assert isinstance(line, int) and isinstance(col, int)\n        return line, col", "language": "python", "code": "def _loc(form: Union[LispForm, ISeq]) -> Optional[Tuple[int, int]]:\n    \"\"\"Fetch the location of the form in the original filename from the\n    input form, if it has metadata.\"\"\"\n    try:\n        meta = form.meta  # type: ignore\n        line = meta.get(reader.READER_LINE_KW)  # type: ignore\n        col = meta.get(reader.READER_COL_KW)  # type: ignore\n    except AttributeError:\n        return None\n    else:\n        assert isinstance(line, int) and isinstance(col, int)\n        return line, col", "code_tokens": ["def", "_loc", "(", "form", ":", "Union", "[", "LispForm", ",", "ISeq", "]", ")", "->", "Optional", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "try", ":", "meta", "=", "form", ".", "meta", "# type: ignore", "line", "=", "meta", ".", "get", "(", "reader", ".", "READER_LINE_KW", ")", "# type: ignore", "col", "=", "meta", ".", "get", "(", "reader", ".", "READER_COL_KW", ")", "# type: ignore", "except", "AttributeError", ":", "return", "None", "else", ":", "assert", "isinstance", "(", "line", ",", "int", ")", "and", "isinstance", "(", "col", ",", "int", ")", "return", "line", ",", "col"], "docstring": "Fetch the location of the form in the original filename from the\n    input form, if it has metadata.", "docstring_tokens": ["Fetch", "the", "location", "of", "the", "form", "in", "the", "original", "filename", "from", "the", "input", "form", "if", "it", "has", "metadata", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/compiler/parser.py#L432-L443", "partition": "test"}
{"repo": "dry-python/dependencies", "path": "src/dependencies/contrib/_rest_framework.py", "func_name": "model_view_set", "original_string": "def model_view_set(injector):\n    \"\"\"Create DRF model view set from injector class.\"\"\"\n\n    handler = create_handler(ModelViewSet, injector)\n    apply_api_view_methods(handler, injector)\n    apply_generic_api_view_methods(handler, injector)\n    apply_model_view_set_methods(handler, injector)\n    return injector.let(as_viewset=lambda: handler)", "language": "python", "code": "def model_view_set(injector):\n    \"\"\"Create DRF model view set from injector class.\"\"\"\n\n    handler = create_handler(ModelViewSet, injector)\n    apply_api_view_methods(handler, injector)\n    apply_generic_api_view_methods(handler, injector)\n    apply_model_view_set_methods(handler, injector)\n    return injector.let(as_viewset=lambda: handler)", "code_tokens": ["def", "model_view_set", "(", "injector", ")", ":", "handler", "=", "create_handler", "(", "ModelViewSet", ",", "injector", ")", "apply_api_view_methods", "(", "handler", ",", "injector", ")", "apply_generic_api_view_methods", "(", "handler", ",", "injector", ")", "apply_model_view_set_methods", "(", "handler", ",", "injector", ")", "return", "injector", ".", "let", "(", "as_viewset", "=", "lambda", ":", "handler", ")"], "docstring": "Create DRF model view set from injector class.", "docstring_tokens": ["Create", "DRF", "model", "view", "set", "from", "injector", "class", "."], "sha": "297912cbc6482ba26b3104729645f3a2aba5facc", "url": "https://github.com/dry-python/dependencies/blob/297912cbc6482ba26b3104729645f3a2aba5facc/src/dependencies/contrib/_rest_framework.py#L31-L38", "partition": "test"}
{"repo": "Nekmo/amazon-dash", "path": "amazon_dash/listener.py", "func_name": "Listener.run", "original_string": "def run(self, root_allowed=False):\n        \"\"\"Start daemon mode\n\n        :param bool root_allowed: Only used for ExecuteCmd\n        :return: loop\n        \"\"\"\n        self.root_allowed = root_allowed\n        scan_devices(self.on_push, lambda d: d.src.lower() in self.devices, self.settings.get('interface'))", "language": "python", "code": "def run(self, root_allowed=False):\n        \"\"\"Start daemon mode\n\n        :param bool root_allowed: Only used for ExecuteCmd\n        :return: loop\n        \"\"\"\n        self.root_allowed = root_allowed\n        scan_devices(self.on_push, lambda d: d.src.lower() in self.devices, self.settings.get('interface'))", "code_tokens": ["def", "run", "(", "self", ",", "root_allowed", "=", "False", ")", ":", "self", ".", "root_allowed", "=", "root_allowed", "scan_devices", "(", "self", ".", "on_push", ",", "lambda", "d", ":", "d", ".", "src", ".", "lower", "(", ")", "in", "self", ".", "devices", ",", "self", ".", "settings", ".", "get", "(", "'interface'", ")", ")"], "docstring": "Start daemon mode\n\n        :param bool root_allowed: Only used for ExecuteCmd\n        :return: loop", "docstring_tokens": ["Start", "daemon", "mode"], "sha": "0e2bdc24ff8ea32cecb2f5f54f5cc1c0f99c197b", "url": "https://github.com/Nekmo/amazon-dash/blob/0e2bdc24ff8ea32cecb2f5f54f5cc1c0f99c197b/amazon_dash/listener.py#L152-L159", "partition": "test"}
{"repo": "dave-shawley/ietfparse", "path": "ietfparse/algorithms.py", "func_name": "_normalize_host", "original_string": "def _normalize_host(host, enable_long_host=False, encode_with_idna=None,\n                    scheme=None):\n    \"\"\"\n    Normalize a host for a URL.\n\n    :param str host: the host name to normalize\n\n    :keyword bool enable_long_host: if this keyword is specified\n        and it is :data:`True`, then the host name length restriction\n        from :rfc:`3986#section-3.2.2` is relaxed.\n    :keyword bool encode_with_idna: if this keyword is specified\n        and it is :data:`True`, then the ``host`` parameter will be\n        encoded using IDN.  If this value is provided as :data:`False`,\n        then the percent-encoding scheme is used instead.  If this\n        parameter is omitted or included with a different value, then\n        the ``host`` parameter is processed using :data:`IDNA_SCHEMES`.\n    :keyword str scheme: if this keyword is specified, then it is\n        used to determine whether to apply IDN rules or not.  This\n        parameter is ignored if `encode_with_idna` is not :data:`None`.\n\n    :return: the normalized and encoded string ready for inclusion\n        into a URL\n\n    \"\"\"\n    if encode_with_idna is not None:\n        enable_idna = encode_with_idna\n    else:\n        enable_idna = scheme.lower() in IDNA_SCHEMES if scheme else False\n    if enable_idna:\n        try:\n            host = '.'.join(segment.encode('idna').decode()\n                            for segment in host.split('.'))\n        except UnicodeError as exc:\n            raise ValueError('host is invalid - {0}'.format(exc))\n    else:\n        host = parse.quote(host.encode('utf-8'), safe=HOST_SAFE_CHARS)\n\n    if len(host) > 255 and not enable_long_host:\n        raise ValueError('host too long')\n\n    return host", "language": "python", "code": "def _normalize_host(host, enable_long_host=False, encode_with_idna=None,\n                    scheme=None):\n    \"\"\"\n    Normalize a host for a URL.\n\n    :param str host: the host name to normalize\n\n    :keyword bool enable_long_host: if this keyword is specified\n        and it is :data:`True`, then the host name length restriction\n        from :rfc:`3986#section-3.2.2` is relaxed.\n    :keyword bool encode_with_idna: if this keyword is specified\n        and it is :data:`True`, then the ``host`` parameter will be\n        encoded using IDN.  If this value is provided as :data:`False`,\n        then the percent-encoding scheme is used instead.  If this\n        parameter is omitted or included with a different value, then\n        the ``host`` parameter is processed using :data:`IDNA_SCHEMES`.\n    :keyword str scheme: if this keyword is specified, then it is\n        used to determine whether to apply IDN rules or not.  This\n        parameter is ignored if `encode_with_idna` is not :data:`None`.\n\n    :return: the normalized and encoded string ready for inclusion\n        into a URL\n\n    \"\"\"\n    if encode_with_idna is not None:\n        enable_idna = encode_with_idna\n    else:\n        enable_idna = scheme.lower() in IDNA_SCHEMES if scheme else False\n    if enable_idna:\n        try:\n            host = '.'.join(segment.encode('idna').decode()\n                            for segment in host.split('.'))\n        except UnicodeError as exc:\n            raise ValueError('host is invalid - {0}'.format(exc))\n    else:\n        host = parse.quote(host.encode('utf-8'), safe=HOST_SAFE_CHARS)\n\n    if len(host) > 255 and not enable_long_host:\n        raise ValueError('host too long')\n\n    return host", "code_tokens": ["def", "_normalize_host", "(", "host", ",", "enable_long_host", "=", "False", ",", "encode_with_idna", "=", "None", ",", "scheme", "=", "None", ")", ":", "if", "encode_with_idna", "is", "not", "None", ":", "enable_idna", "=", "encode_with_idna", "else", ":", "enable_idna", "=", "scheme", ".", "lower", "(", ")", "in", "IDNA_SCHEMES", "if", "scheme", "else", "False", "if", "enable_idna", ":", "try", ":", "host", "=", "'.'", ".", "join", "(", "segment", ".", "encode", "(", "'idna'", ")", ".", "decode", "(", ")", "for", "segment", "in", "host", ".", "split", "(", "'.'", ")", ")", "except", "UnicodeError", "as", "exc", ":", "raise", "ValueError", "(", "'host is invalid - {0}'", ".", "format", "(", "exc", ")", ")", "else", ":", "host", "=", "parse", ".", "quote", "(", "host", ".", "encode", "(", "'utf-8'", ")", ",", "safe", "=", "HOST_SAFE_CHARS", ")", "if", "len", "(", "host", ")", ">", "255", "and", "not", "enable_long_host", ":", "raise", "ValueError", "(", "'host too long'", ")", "return", "host"], "docstring": "Normalize a host for a URL.\n\n    :param str host: the host name to normalize\n\n    :keyword bool enable_long_host: if this keyword is specified\n        and it is :data:`True`, then the host name length restriction\n        from :rfc:`3986#section-3.2.2` is relaxed.\n    :keyword bool encode_with_idna: if this keyword is specified\n        and it is :data:`True`, then the ``host`` parameter will be\n        encoded using IDN.  If this value is provided as :data:`False`,\n        then the percent-encoding scheme is used instead.  If this\n        parameter is omitted or included with a different value, then\n        the ``host`` parameter is processed using :data:`IDNA_SCHEMES`.\n    :keyword str scheme: if this keyword is specified, then it is\n        used to determine whether to apply IDN rules or not.  This\n        parameter is ignored if `encode_with_idna` is not :data:`None`.\n\n    :return: the normalized and encoded string ready for inclusion\n        into a URL", "docstring_tokens": ["Normalize", "a", "host", "for", "a", "URL", "."], "sha": "d28f360941316e45c1596589fa59bc7d25aa20e0", "url": "https://github.com/dave-shawley/ietfparse/blob/d28f360941316e45c1596589fa59bc7d25aa20e0/ietfparse/algorithms.py#L384-L424", "partition": "test"}
{"repo": "juga0/dhcpcanon", "path": "dhcpcanon/dhcpcapfsm.py", "func_name": "DHCPCAPFSM.set_timeout", "original_string": "def set_timeout(self, state, function, newtimeout):\n        \"\"\"\n        Workaround to change timeout values in the ATMT.timeout class method.\n\n        self.timeout format is::\n\n            {'STATE': [\n                (TIMEOUT0, <function foo>),\n                (TIMEOUT1, <function bar>)),\n                (None, None)\n                ],\n            }\n\n\n        \"\"\"\n        state = STATES2NAMES[state]\n        for timeout_fn_t in self.timeout[state]:\n            # access the function name\n            if timeout_fn_t[1] is not None and \\\n               timeout_fn_t[1].atmt_condname == function.atmt_condname:\n                # convert list to tuple to make it mutable\n                timeout_l = list(timeout_fn_t)\n                # modify the timeout\n                timeout_l[0] = newtimeout\n                # set the new timeoute to self.timeout\n                i = self.timeout[state].index(timeout_fn_t)\n                self.timeout[state][i] = tuple(timeout_l)\n                logger.debug('Set state %s, function %s, to timeout %s',\n                             state, function.atmt_condname, newtimeout)", "language": "python", "code": "def set_timeout(self, state, function, newtimeout):\n        \"\"\"\n        Workaround to change timeout values in the ATMT.timeout class method.\n\n        self.timeout format is::\n\n            {'STATE': [\n                (TIMEOUT0, <function foo>),\n                (TIMEOUT1, <function bar>)),\n                (None, None)\n                ],\n            }\n\n\n        \"\"\"\n        state = STATES2NAMES[state]\n        for timeout_fn_t in self.timeout[state]:\n            # access the function name\n            if timeout_fn_t[1] is not None and \\\n               timeout_fn_t[1].atmt_condname == function.atmt_condname:\n                # convert list to tuple to make it mutable\n                timeout_l = list(timeout_fn_t)\n                # modify the timeout\n                timeout_l[0] = newtimeout\n                # set the new timeoute to self.timeout\n                i = self.timeout[state].index(timeout_fn_t)\n                self.timeout[state][i] = tuple(timeout_l)\n                logger.debug('Set state %s, function %s, to timeout %s',\n                             state, function.atmt_condname, newtimeout)", "code_tokens": ["def", "set_timeout", "(", "self", ",", "state", ",", "function", ",", "newtimeout", ")", ":", "state", "=", "STATES2NAMES", "[", "state", "]", "for", "timeout_fn_t", "in", "self", ".", "timeout", "[", "state", "]", ":", "# access the function name", "if", "timeout_fn_t", "[", "1", "]", "is", "not", "None", "and", "timeout_fn_t", "[", "1", "]", ".", "atmt_condname", "==", "function", ".", "atmt_condname", ":", "# convert list to tuple to make it mutable", "timeout_l", "=", "list", "(", "timeout_fn_t", ")", "# modify the timeout", "timeout_l", "[", "0", "]", "=", "newtimeout", "# set the new timeoute to self.timeout", "i", "=", "self", ".", "timeout", "[", "state", "]", ".", "index", "(", "timeout_fn_t", ")", "self", ".", "timeout", "[", "state", "]", "[", "i", "]", "=", "tuple", "(", "timeout_l", ")", "logger", ".", "debug", "(", "'Set state %s, function %s, to timeout %s'", ",", "state", ",", "function", ".", "atmt_condname", ",", "newtimeout", ")"], "docstring": "Workaround to change timeout values in the ATMT.timeout class method.\n\n        self.timeout format is::\n\n            {'STATE': [\n                (TIMEOUT0, <function foo>),\n                (TIMEOUT1, <function bar>)),\n                (None, None)\n                ],\n            }", "docstring_tokens": ["Workaround", "to", "change", "timeout", "values", "in", "the", "ATMT", ".", "timeout", "class", "method", "."], "sha": "9f51a29e57fe93dc93fb22bb0ed12fcfe9557e59", "url": "https://github.com/juga0/dhcpcanon/blob/9f51a29e57fe93dc93fb22bb0ed12fcfe9557e59/dhcpcanon/dhcpcapfsm.py#L118-L146", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/flask/helpers.py", "func_name": "_PackageBoundObject.jinja_loader", "original_string": "def jinja_loader(self):\n        \"\"\"The Jinja loader for this package bound object.\n\n        .. versionadded:: 0.5\n        \"\"\"\n        if self.template_folder is not None:\n            return FileSystemLoader(os.path.join(self.root_path,\n                                                 self.template_folder))", "language": "python", "code": "def jinja_loader(self):\n        \"\"\"The Jinja loader for this package bound object.\n\n        .. versionadded:: 0.5\n        \"\"\"\n        if self.template_folder is not None:\n            return FileSystemLoader(os.path.join(self.root_path,\n                                                 self.template_folder))", "code_tokens": ["def", "jinja_loader", "(", "self", ")", ":", "if", "self", ".", "template_folder", "is", "not", "None", ":", "return", "FileSystemLoader", "(", "os", ".", "path", ".", "join", "(", "self", ".", "root_path", ",", "self", ".", "template_folder", ")", ")"], "docstring": "The Jinja loader for this package bound object.\n\n        .. versionadded:: 0.5", "docstring_tokens": ["The", "Jinja", "loader", "for", "this", "package", "bound", "object", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/flask/helpers.py#L775-L782", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/pip/index.py", "func_name": "PackageFinder._find_all_versions", "original_string": "def _find_all_versions(self, project_name):\n        \"\"\"Find all available versions for project_name\n\n        This checks index_urls, find_links and dependency_links\n        All versions found are returned\n\n        See _link_package_versions for details on which files are accepted\n        \"\"\"\n        index_locations = self._get_index_urls_locations(project_name)\n        index_file_loc, index_url_loc = self._sort_locations(index_locations)\n        fl_file_loc, fl_url_loc = self._sort_locations(\n            self.find_links, expand_dir=True)\n        dep_file_loc, dep_url_loc = self._sort_locations(self.dependency_links)\n\n        file_locations = (\n            Link(url) for url in itertools.chain(\n                index_file_loc, fl_file_loc, dep_file_loc)\n        )\n\n        # We trust every url that the user has given us whether it was given\n        #   via --index-url or --find-links\n        # We explicitly do not trust links that came from dependency_links\n        # We want to filter out any thing which does not have a secure origin.\n        url_locations = [\n            link for link in itertools.chain(\n                (Link(url, trusted=True) for url in index_url_loc),\n                (Link(url, trusted=True) for url in fl_url_loc),\n                (Link(url) for url in dep_url_loc),\n            )\n            if self._validate_secure_origin(logger, link)\n        ]\n\n        logger.debug('%d location(s) to search for versions of %s:',\n                     len(url_locations), project_name)\n\n        for location in url_locations:\n            logger.debug('* %s', location)\n\n        canonical_name = pkg_resources.safe_name(project_name).lower()\n        formats = fmt_ctl_formats(self.format_control, canonical_name)\n        search = Search(project_name.lower(), canonical_name, formats)\n        find_links_versions = self._package_versions(\n            # We trust every directly linked archive in find_links\n            (Link(url, '-f', trusted=True) for url in self.find_links),\n            search\n        )\n\n        page_versions = []\n        for page in self._get_pages(url_locations, project_name):\n            logger.debug('Analyzing links from page %s', page.url)\n            with indent_log():\n                page_versions.extend(\n                    self._package_versions(page.links, search)\n                )\n\n        dependency_versions = self._package_versions(\n            (Link(url) for url in self.dependency_links), search\n        )\n        if dependency_versions:\n            logger.debug(\n                'dependency_links found: %s',\n                ', '.join([\n                    version.location.url for version in dependency_versions\n                ])\n            )\n\n        file_versions = self._package_versions(file_locations, search)\n        if file_versions:\n            file_versions.sort(reverse=True)\n            logger.debug(\n                'Local files found: %s',\n                ', '.join([\n                    url_to_path(candidate.location.url)\n                    for candidate in file_versions\n                ])\n            )\n\n        # This is an intentional priority ordering\n        return (\n            file_versions + find_links_versions + page_versions +\n            dependency_versions\n        )", "language": "python", "code": "def _find_all_versions(self, project_name):\n        \"\"\"Find all available versions for project_name\n\n        This checks index_urls, find_links and dependency_links\n        All versions found are returned\n\n        See _link_package_versions for details on which files are accepted\n        \"\"\"\n        index_locations = self._get_index_urls_locations(project_name)\n        index_file_loc, index_url_loc = self._sort_locations(index_locations)\n        fl_file_loc, fl_url_loc = self._sort_locations(\n            self.find_links, expand_dir=True)\n        dep_file_loc, dep_url_loc = self._sort_locations(self.dependency_links)\n\n        file_locations = (\n            Link(url) for url in itertools.chain(\n                index_file_loc, fl_file_loc, dep_file_loc)\n        )\n\n        # We trust every url that the user has given us whether it was given\n        #   via --index-url or --find-links\n        # We explicitly do not trust links that came from dependency_links\n        # We want to filter out any thing which does not have a secure origin.\n        url_locations = [\n            link for link in itertools.chain(\n                (Link(url, trusted=True) for url in index_url_loc),\n                (Link(url, trusted=True) for url in fl_url_loc),\n                (Link(url) for url in dep_url_loc),\n            )\n            if self._validate_secure_origin(logger, link)\n        ]\n\n        logger.debug('%d location(s) to search for versions of %s:',\n                     len(url_locations), project_name)\n\n        for location in url_locations:\n            logger.debug('* %s', location)\n\n        canonical_name = pkg_resources.safe_name(project_name).lower()\n        formats = fmt_ctl_formats(self.format_control, canonical_name)\n        search = Search(project_name.lower(), canonical_name, formats)\n        find_links_versions = self._package_versions(\n            # We trust every directly linked archive in find_links\n            (Link(url, '-f', trusted=True) for url in self.find_links),\n            search\n        )\n\n        page_versions = []\n        for page in self._get_pages(url_locations, project_name):\n            logger.debug('Analyzing links from page %s', page.url)\n            with indent_log():\n                page_versions.extend(\n                    self._package_versions(page.links, search)\n                )\n\n        dependency_versions = self._package_versions(\n            (Link(url) for url in self.dependency_links), search\n        )\n        if dependency_versions:\n            logger.debug(\n                'dependency_links found: %s',\n                ', '.join([\n                    version.location.url for version in dependency_versions\n                ])\n            )\n\n        file_versions = self._package_versions(file_locations, search)\n        if file_versions:\n            file_versions.sort(reverse=True)\n            logger.debug(\n                'Local files found: %s',\n                ', '.join([\n                    url_to_path(candidate.location.url)\n                    for candidate in file_versions\n                ])\n            )\n\n        # This is an intentional priority ordering\n        return (\n            file_versions + find_links_versions + page_versions +\n            dependency_versions\n        )", "code_tokens": ["def", "_find_all_versions", "(", "self", ",", "project_name", ")", ":", "index_locations", "=", "self", ".", "_get_index_urls_locations", "(", "project_name", ")", "index_file_loc", ",", "index_url_loc", "=", "self", ".", "_sort_locations", "(", "index_locations", ")", "fl_file_loc", ",", "fl_url_loc", "=", "self", ".", "_sort_locations", "(", "self", ".", "find_links", ",", "expand_dir", "=", "True", ")", "dep_file_loc", ",", "dep_url_loc", "=", "self", ".", "_sort_locations", "(", "self", ".", "dependency_links", ")", "file_locations", "=", "(", "Link", "(", "url", ")", "for", "url", "in", "itertools", ".", "chain", "(", "index_file_loc", ",", "fl_file_loc", ",", "dep_file_loc", ")", ")", "# We trust every url that the user has given us whether it was given", "#   via --index-url or --find-links", "# We explicitly do not trust links that came from dependency_links", "# We want to filter out any thing which does not have a secure origin.", "url_locations", "=", "[", "link", "for", "link", "in", "itertools", ".", "chain", "(", "(", "Link", "(", "url", ",", "trusted", "=", "True", ")", "for", "url", "in", "index_url_loc", ")", ",", "(", "Link", "(", "url", ",", "trusted", "=", "True", ")", "for", "url", "in", "fl_url_loc", ")", ",", "(", "Link", "(", "url", ")", "for", "url", "in", "dep_url_loc", ")", ",", ")", "if", "self", ".", "_validate_secure_origin", "(", "logger", ",", "link", ")", "]", "logger", ".", "debug", "(", "'%d location(s) to search for versions of %s:'", ",", "len", "(", "url_locations", ")", ",", "project_name", ")", "for", "location", "in", "url_locations", ":", "logger", ".", "debug", "(", "'* %s'", ",", "location", ")", "canonical_name", "=", "pkg_resources", ".", "safe_name", "(", "project_name", ")", ".", "lower", "(", ")", "formats", "=", "fmt_ctl_formats", "(", "self", ".", "format_control", ",", "canonical_name", ")", "search", "=", "Search", "(", "project_name", ".", "lower", "(", ")", ",", "canonical_name", ",", "formats", ")", "find_links_versions", "=", "self", ".", "_package_versions", "(", "# We trust every directly linked archive in find_links", "(", "Link", "(", "url", ",", "'-f'", ",", "trusted", "=", "True", ")", "for", "url", "in", "self", ".", "find_links", ")", ",", "search", ")", "page_versions", "=", "[", "]", "for", "page", "in", "self", ".", "_get_pages", "(", "url_locations", ",", "project_name", ")", ":", "logger", ".", "debug", "(", "'Analyzing links from page %s'", ",", "page", ".", "url", ")", "with", "indent_log", "(", ")", ":", "page_versions", ".", "extend", "(", "self", ".", "_package_versions", "(", "page", ".", "links", ",", "search", ")", ")", "dependency_versions", "=", "self", ".", "_package_versions", "(", "(", "Link", "(", "url", ")", "for", "url", "in", "self", ".", "dependency_links", ")", ",", "search", ")", "if", "dependency_versions", ":", "logger", ".", "debug", "(", "'dependency_links found: %s'", ",", "', '", ".", "join", "(", "[", "version", ".", "location", ".", "url", "for", "version", "in", "dependency_versions", "]", ")", ")", "file_versions", "=", "self", ".", "_package_versions", "(", "file_locations", ",", "search", ")", "if", "file_versions", ":", "file_versions", ".", "sort", "(", "reverse", "=", "True", ")", "logger", ".", "debug", "(", "'Local files found: %s'", ",", "', '", ".", "join", "(", "[", "url_to_path", "(", "candidate", ".", "location", ".", "url", ")", "for", "candidate", "in", "file_versions", "]", ")", ")", "# This is an intentional priority ordering", "return", "(", "file_versions", "+", "find_links_versions", "+", "page_versions", "+", "dependency_versions", ")"], "docstring": "Find all available versions for project_name\n\n        This checks index_urls, find_links and dependency_links\n        All versions found are returned\n\n        See _link_package_versions for details on which files are accepted", "docstring_tokens": ["Find", "all", "available", "versions", "for", "project_name"], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/pip/index.py#L396-L477", "partition": "test"}
{"repo": "LLNL/scraper", "path": "scripts/get_traffic.py", "func_name": "GitHub_Traffic.get_releases", "original_string": "def get_releases(self, url='', headers={}, repo_name=''):\n        \"\"\"\n        Retrieves the releases for the given repo in JSON.\n        \"\"\"\n        url_releases = (url + '/releases')\n        r = requests.get(url_releases, headers=headers)\n        self.releases_json[repo_name] = r.json()", "language": "python", "code": "def get_releases(self, url='', headers={}, repo_name=''):\n        \"\"\"\n        Retrieves the releases for the given repo in JSON.\n        \"\"\"\n        url_releases = (url + '/releases')\n        r = requests.get(url_releases, headers=headers)\n        self.releases_json[repo_name] = r.json()", "code_tokens": ["def", "get_releases", "(", "self", ",", "url", "=", "''", ",", "headers", "=", "{", "}", ",", "repo_name", "=", "''", ")", ":", "url_releases", "=", "(", "url", "+", "'/releases'", ")", "r", "=", "requests", ".", "get", "(", "url_releases", ",", "headers", "=", "headers", ")", "self", ".", "releases_json", "[", "repo_name", "]", "=", "r", ".", "json", "(", ")"], "docstring": "Retrieves the releases for the given repo in JSON.", "docstring_tokens": ["Retrieves", "the", "releases", "for", "the", "given", "repo", "in", "JSON", "."], "sha": "881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea", "url": "https://github.com/LLNL/scraper/blob/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea/scripts/get_traffic.py#L134-L140", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/_vendor/nvd3/NVD3Chart.py", "func_name": "NVD3Chart.create_y_axis", "original_string": "def create_y_axis(self, name, label=None, format=None, custom_format=False):\n        \"\"\"\n        Create Y-axis\n        \"\"\"\n        axis = {}\n\n        if custom_format and format:\n            axis['tickFormat'] = format\n        elif format:\n            axis['tickFormat'] = \"d3.format(',%s')\" % format\n\n        if label:\n            axis['axisLabel'] = \"'\" + label + \"'\"\n\n        # Add new axis to list of axis\n        self.axislist[name] = axis", "language": "python", "code": "def create_y_axis(self, name, label=None, format=None, custom_format=False):\n        \"\"\"\n        Create Y-axis\n        \"\"\"\n        axis = {}\n\n        if custom_format and format:\n            axis['tickFormat'] = format\n        elif format:\n            axis['tickFormat'] = \"d3.format(',%s')\" % format\n\n        if label:\n            axis['axisLabel'] = \"'\" + label + \"'\"\n\n        # Add new axis to list of axis\n        self.axislist[name] = axis", "code_tokens": ["def", "create_y_axis", "(", "self", ",", "name", ",", "label", "=", "None", ",", "format", "=", "None", ",", "custom_format", "=", "False", ")", ":", "axis", "=", "{", "}", "if", "custom_format", "and", "format", ":", "axis", "[", "'tickFormat'", "]", "=", "format", "elif", "format", ":", "axis", "[", "'tickFormat'", "]", "=", "\"d3.format(',%s')\"", "%", "format", "if", "label", ":", "axis", "[", "'axisLabel'", "]", "=", "\"'\"", "+", "label", "+", "\"'\"", "# Add new axis to list of axis", "self", ".", "axislist", "[", "name", "]", "=", "axis"], "docstring": "Create Y-axis", "docstring_tokens": ["Create", "Y", "-", "axis"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/_vendor/nvd3/NVD3Chart.py#L450-L465", "partition": "test"}
{"repo": "mseclab/PyJFuzz", "path": "gramfuzz/gramfuzz/__init__.py", "func_name": "GramFuzzer.gen", "original_string": "def gen(self, num, cat=None, cat_group=None, preferred=None, preferred_ratio=0.5, max_recursion=None, auto_process=True):\n        \"\"\"Generate ``num`` rules from category ``cat``, optionally specifying\n        preferred category groups ``preferred`` that should be preferred at\n        probability ``preferred_ratio`` over other randomly-chosen rule definitions.\n\n        :param int num: The number of rules to generate\n        :param str cat: The name of the category to generate ``num`` rules from\n        :param str cat_group: The category group (ie python file) to generate rules from. This\n            was added specifically to make it easier to generate data based on the name\n            of the file the grammar was defined in, and is intended to work with the\n            ``TOP_CAT`` values that may be defined in a loaded grammar file.\n        :param list preferred: A list of preferred category groups to generate rules from\n        :param float preferred_ratio: The percent probability that the preferred\n            groups will be chosen over randomly choosen rule definitions from category ``cat``.\n        :param int max_recursion: The maximum amount to allow references to recurse\n        :param bool auto_process: Whether rules should be automatically pruned and\n            shortest reference paths determined. See :any:`gramfuzz.GramFuzzer.preprocess_rules`\n            for what would automatically be done.\n        \"\"\"\n        import gramfuzz.fields\n        gramfuzz.fields.REF_LEVEL = 1\n\n        if cat is None and cat_group is None:\n            raise gramfuzz.errors.GramFuzzError(\"cat and cat_group are None, one must be set\")\n\n        if cat is None and cat_group is not None:\n            if cat_group not in self.cat_group_defaults:\n                raise gramfuzz.errors.GramFuzzError(\n                    \"cat_group {!r} did not define a TOP_CAT variable\"\n                )\n            cat = self.cat_group_defaults[cat_group]\n            if not isinstance(cat, basestring):\n                raise gramfuzz.errors.GramFuzzError(\n                    \"cat_group {!r}'s TOP_CAT variable was not a string\"\n                )\n\n        if auto_process and self._rules_processed == False:\n            self.preprocess_rules()\n\n        if max_recursion is not None:\n            self.set_max_recursion(max_recursion)\n\n        if preferred is None:\n            preferred = []\n\n        res = deque()\n        cat_defs = self.defs[cat]\n\n        # optimizations\n        _res_append = res.append\n        _res_extend = res.extend\n        _choice = rand.choice\n        _maybe = rand.maybe\n        _val = utils.val\n\n        keys = self.defs[cat].keys()\n\n        self._last_pref_keys = self._get_pref_keys(cat, preferred)\n        # be sure to set this *after* fetching the pref keys (above^)\n        self._last_prefs = preferred\n\n        total_errors = deque()\n        total_gend = 0\n        while total_gend < num:\n            # use a rule definition from one of the preferred category\n            # groups\n            if len(self._last_pref_keys) > 0 and _maybe(preferred_ratio):\n                rand_key = _choice(self._last_pref_keys)\n                if rand_key not in cat_defs:\n                    # TODO this means it was removed / pruned b/c it was unreachable??\n                    # TODO look into this more\n                    rand_key = _choice(list(keys))\n\n            # else just choose a key at random from the category\n            else:\n                rand_key = _choice(list(keys))\n\n            # pruning failed, this rule is not defined/reachable\n            if rand_key not in cat_defs:\n                continue\n\n            v = _choice(cat_defs[rand_key])\n\n            # not used directly by GramFuzzer, but could be useful\n            # to subclasses of GramFuzzer\n            info = {}\n\n            pre = deque()\n            self.pre_revert(info)\n            val_res = None\n\n            try:\n                val_res = _val(v, pre)\n            except errors.GramFuzzError as e:\n                raise\n                #total_errors.append(e)\n                #self.revert(info)\n                #continue\n            except RuntimeError as e:\n                print(\"RUNTIME ERROR\")\n                self.revert(info)\n                continue\n\n            if val_res is not None:\n                _res_extend(pre)\n                _res_append(val_res)\n\n                total_gend += 1\n                self.post_revert(cat, res, total_gend, num, info)\n\n        return res", "language": "python", "code": "def gen(self, num, cat=None, cat_group=None, preferred=None, preferred_ratio=0.5, max_recursion=None, auto_process=True):\n        \"\"\"Generate ``num`` rules from category ``cat``, optionally specifying\n        preferred category groups ``preferred`` that should be preferred at\n        probability ``preferred_ratio`` over other randomly-chosen rule definitions.\n\n        :param int num: The number of rules to generate\n        :param str cat: The name of the category to generate ``num`` rules from\n        :param str cat_group: The category group (ie python file) to generate rules from. This\n            was added specifically to make it easier to generate data based on the name\n            of the file the grammar was defined in, and is intended to work with the\n            ``TOP_CAT`` values that may be defined in a loaded grammar file.\n        :param list preferred: A list of preferred category groups to generate rules from\n        :param float preferred_ratio: The percent probability that the preferred\n            groups will be chosen over randomly choosen rule definitions from category ``cat``.\n        :param int max_recursion: The maximum amount to allow references to recurse\n        :param bool auto_process: Whether rules should be automatically pruned and\n            shortest reference paths determined. See :any:`gramfuzz.GramFuzzer.preprocess_rules`\n            for what would automatically be done.\n        \"\"\"\n        import gramfuzz.fields\n        gramfuzz.fields.REF_LEVEL = 1\n\n        if cat is None and cat_group is None:\n            raise gramfuzz.errors.GramFuzzError(\"cat and cat_group are None, one must be set\")\n\n        if cat is None and cat_group is not None:\n            if cat_group not in self.cat_group_defaults:\n                raise gramfuzz.errors.GramFuzzError(\n                    \"cat_group {!r} did not define a TOP_CAT variable\"\n                )\n            cat = self.cat_group_defaults[cat_group]\n            if not isinstance(cat, basestring):\n                raise gramfuzz.errors.GramFuzzError(\n                    \"cat_group {!r}'s TOP_CAT variable was not a string\"\n                )\n\n        if auto_process and self._rules_processed == False:\n            self.preprocess_rules()\n\n        if max_recursion is not None:\n            self.set_max_recursion(max_recursion)\n\n        if preferred is None:\n            preferred = []\n\n        res = deque()\n        cat_defs = self.defs[cat]\n\n        # optimizations\n        _res_append = res.append\n        _res_extend = res.extend\n        _choice = rand.choice\n        _maybe = rand.maybe\n        _val = utils.val\n\n        keys = self.defs[cat].keys()\n\n        self._last_pref_keys = self._get_pref_keys(cat, preferred)\n        # be sure to set this *after* fetching the pref keys (above^)\n        self._last_prefs = preferred\n\n        total_errors = deque()\n        total_gend = 0\n        while total_gend < num:\n            # use a rule definition from one of the preferred category\n            # groups\n            if len(self._last_pref_keys) > 0 and _maybe(preferred_ratio):\n                rand_key = _choice(self._last_pref_keys)\n                if rand_key not in cat_defs:\n                    # TODO this means it was removed / pruned b/c it was unreachable??\n                    # TODO look into this more\n                    rand_key = _choice(list(keys))\n\n            # else just choose a key at random from the category\n            else:\n                rand_key = _choice(list(keys))\n\n            # pruning failed, this rule is not defined/reachable\n            if rand_key not in cat_defs:\n                continue\n\n            v = _choice(cat_defs[rand_key])\n\n            # not used directly by GramFuzzer, but could be useful\n            # to subclasses of GramFuzzer\n            info = {}\n\n            pre = deque()\n            self.pre_revert(info)\n            val_res = None\n\n            try:\n                val_res = _val(v, pre)\n            except errors.GramFuzzError as e:\n                raise\n                #total_errors.append(e)\n                #self.revert(info)\n                #continue\n            except RuntimeError as e:\n                print(\"RUNTIME ERROR\")\n                self.revert(info)\n                continue\n\n            if val_res is not None:\n                _res_extend(pre)\n                _res_append(val_res)\n\n                total_gend += 1\n                self.post_revert(cat, res, total_gend, num, info)\n\n        return res", "code_tokens": ["def", "gen", "(", "self", ",", "num", ",", "cat", "=", "None", ",", "cat_group", "=", "None", ",", "preferred", "=", "None", ",", "preferred_ratio", "=", "0.5", ",", "max_recursion", "=", "None", ",", "auto_process", "=", "True", ")", ":", "import", "gramfuzz", ".", "fields", "gramfuzz", ".", "fields", ".", "REF_LEVEL", "=", "1", "if", "cat", "is", "None", "and", "cat_group", "is", "None", ":", "raise", "gramfuzz", ".", "errors", ".", "GramFuzzError", "(", "\"cat and cat_group are None, one must be set\"", ")", "if", "cat", "is", "None", "and", "cat_group", "is", "not", "None", ":", "if", "cat_group", "not", "in", "self", ".", "cat_group_defaults", ":", "raise", "gramfuzz", ".", "errors", ".", "GramFuzzError", "(", "\"cat_group {!r} did not define a TOP_CAT variable\"", ")", "cat", "=", "self", ".", "cat_group_defaults", "[", "cat_group", "]", "if", "not", "isinstance", "(", "cat", ",", "basestring", ")", ":", "raise", "gramfuzz", ".", "errors", ".", "GramFuzzError", "(", "\"cat_group {!r}'s TOP_CAT variable was not a string\"", ")", "if", "auto_process", "and", "self", ".", "_rules_processed", "==", "False", ":", "self", ".", "preprocess_rules", "(", ")", "if", "max_recursion", "is", "not", "None", ":", "self", ".", "set_max_recursion", "(", "max_recursion", ")", "if", "preferred", "is", "None", ":", "preferred", "=", "[", "]", "res", "=", "deque", "(", ")", "cat_defs", "=", "self", ".", "defs", "[", "cat", "]", "# optimizations", "_res_append", "=", "res", ".", "append", "_res_extend", "=", "res", ".", "extend", "_choice", "=", "rand", ".", "choice", "_maybe", "=", "rand", ".", "maybe", "_val", "=", "utils", ".", "val", "keys", "=", "self", ".", "defs", "[", "cat", "]", ".", "keys", "(", ")", "self", ".", "_last_pref_keys", "=", "self", ".", "_get_pref_keys", "(", "cat", ",", "preferred", ")", "# be sure to set this *after* fetching the pref keys (above^)", "self", ".", "_last_prefs", "=", "preferred", "total_errors", "=", "deque", "(", ")", "total_gend", "=", "0", "while", "total_gend", "<", "num", ":", "# use a rule definition from one of the preferred category", "# groups", "if", "len", "(", "self", ".", "_last_pref_keys", ")", ">", "0", "and", "_maybe", "(", "preferred_ratio", ")", ":", "rand_key", "=", "_choice", "(", "self", ".", "_last_pref_keys", ")", "if", "rand_key", "not", "in", "cat_defs", ":", "# TODO this means it was removed / pruned b/c it was unreachable??", "# TODO look into this more", "rand_key", "=", "_choice", "(", "list", "(", "keys", ")", ")", "# else just choose a key at random from the category", "else", ":", "rand_key", "=", "_choice", "(", "list", "(", "keys", ")", ")", "# pruning failed, this rule is not defined/reachable", "if", "rand_key", "not", "in", "cat_defs", ":", "continue", "v", "=", "_choice", "(", "cat_defs", "[", "rand_key", "]", ")", "# not used directly by GramFuzzer, but could be useful", "# to subclasses of GramFuzzer", "info", "=", "{", "}", "pre", "=", "deque", "(", ")", "self", ".", "pre_revert", "(", "info", ")", "val_res", "=", "None", "try", ":", "val_res", "=", "_val", "(", "v", ",", "pre", ")", "except", "errors", ".", "GramFuzzError", "as", "e", ":", "raise", "#total_errors.append(e)", "#self.revert(info)", "#continue", "except", "RuntimeError", "as", "e", ":", "print", "(", "\"RUNTIME ERROR\"", ")", "self", ".", "revert", "(", "info", ")", "continue", "if", "val_res", "is", "not", "None", ":", "_res_extend", "(", "pre", ")", "_res_append", "(", "val_res", ")", "total_gend", "+=", "1", "self", ".", "post_revert", "(", "cat", ",", "res", ",", "total_gend", ",", "num", ",", "info", ")", "return", "res"], "docstring": "Generate ``num`` rules from category ``cat``, optionally specifying\n        preferred category groups ``preferred`` that should be preferred at\n        probability ``preferred_ratio`` over other randomly-chosen rule definitions.\n\n        :param int num: The number of rules to generate\n        :param str cat: The name of the category to generate ``num`` rules from\n        :param str cat_group: The category group (ie python file) to generate rules from. This\n            was added specifically to make it easier to generate data based on the name\n            of the file the grammar was defined in, and is intended to work with the\n            ``TOP_CAT`` values that may be defined in a loaded grammar file.\n        :param list preferred: A list of preferred category groups to generate rules from\n        :param float preferred_ratio: The percent probability that the preferred\n            groups will be chosen over randomly choosen rule definitions from category ``cat``.\n        :param int max_recursion: The maximum amount to allow references to recurse\n        :param bool auto_process: Whether rules should be automatically pruned and\n            shortest reference paths determined. See :any:`gramfuzz.GramFuzzer.preprocess_rules`\n            for what would automatically be done.", "docstring_tokens": ["Generate", "num", "rules", "from", "category", "cat", "optionally", "specifying", "preferred", "category", "groups", "preferred", "that", "should", "be", "preferred", "at", "probability", "preferred_ratio", "over", "other", "randomly", "-", "chosen", "rule", "definitions", "."], "sha": "f777067076f62c9ab74ffea6e90fd54402b7a1b4", "url": "https://github.com/mseclab/PyJFuzz/blob/f777067076f62c9ab74ffea6e90fd54402b7a1b4/gramfuzz/gramfuzz/__init__.py#L382-L492", "partition": "test"}
{"repo": "respondcreate/django-versatileimagefield", "path": "versatileimagefield/forms.py", "func_name": "VersatileImageFormField.to_python", "original_string": "def to_python(self, data):\n        \"\"\"Ensure data is prepped properly before handing off to ImageField.\"\"\"\n        if data is not None:\n            if hasattr(data, 'open'):\n                data.open()\n            return super(VersatileImageFormField, self).to_python(data)", "language": "python", "code": "def to_python(self, data):\n        \"\"\"Ensure data is prepped properly before handing off to ImageField.\"\"\"\n        if data is not None:\n            if hasattr(data, 'open'):\n                data.open()\n            return super(VersatileImageFormField, self).to_python(data)", "code_tokens": ["def", "to_python", "(", "self", ",", "data", ")", ":", "if", "data", "is", "not", "None", ":", "if", "hasattr", "(", "data", ",", "'open'", ")", ":", "data", ".", "open", "(", ")", "return", "super", "(", "VersatileImageFormField", ",", "self", ")", ".", "to_python", "(", "data", ")"], "docstring": "Ensure data is prepped properly before handing off to ImageField.", "docstring_tokens": ["Ensure", "data", "is", "prepped", "properly", "before", "handing", "off", "to", "ImageField", "."], "sha": "d41e279c39cccffafbe876c67596184704ae8877", "url": "https://github.com/respondcreate/django-versatileimagefield/blob/d41e279c39cccffafbe876c67596184704ae8877/versatileimagefield/forms.py#L19-L24", "partition": "test"}
{"repo": "quantopian/pgcontents", "path": "pgcontents/utils/migrate.py", "func_name": "temp_alembic_ini", "original_string": "def temp_alembic_ini(alembic_dir_location, sqlalchemy_url):\n    \"\"\"\n    Temporarily write an alembic.ini file for use with alembic migration\n    scripts.\n    \"\"\"\n    with TemporaryDirectory() as tempdir:\n        alembic_ini_filename = join(tempdir, 'temp_alembic.ini')\n        with open(alembic_ini_filename, 'w') as f:\n            f.write(\n                ALEMBIC_INI_TEMPLATE.format(\n                    alembic_dir_location=alembic_dir_location,\n                    sqlalchemy_url=sqlalchemy_url,\n                )\n            )\n        yield alembic_ini_filename", "language": "python", "code": "def temp_alembic_ini(alembic_dir_location, sqlalchemy_url):\n    \"\"\"\n    Temporarily write an alembic.ini file for use with alembic migration\n    scripts.\n    \"\"\"\n    with TemporaryDirectory() as tempdir:\n        alembic_ini_filename = join(tempdir, 'temp_alembic.ini')\n        with open(alembic_ini_filename, 'w') as f:\n            f.write(\n                ALEMBIC_INI_TEMPLATE.format(\n                    alembic_dir_location=alembic_dir_location,\n                    sqlalchemy_url=sqlalchemy_url,\n                )\n            )\n        yield alembic_ini_filename", "code_tokens": ["def", "temp_alembic_ini", "(", "alembic_dir_location", ",", "sqlalchemy_url", ")", ":", "with", "TemporaryDirectory", "(", ")", "as", "tempdir", ":", "alembic_ini_filename", "=", "join", "(", "tempdir", ",", "'temp_alembic.ini'", ")", "with", "open", "(", "alembic_ini_filename", ",", "'w'", ")", "as", "f", ":", "f", ".", "write", "(", "ALEMBIC_INI_TEMPLATE", ".", "format", "(", "alembic_dir_location", "=", "alembic_dir_location", ",", "sqlalchemy_url", "=", "sqlalchemy_url", ",", ")", ")", "yield", "alembic_ini_filename"], "docstring": "Temporarily write an alembic.ini file for use with alembic migration\n    scripts.", "docstring_tokens": ["Temporarily", "write", "an", "alembic", ".", "ini", "file", "for", "use", "with", "alembic", "migration", "scripts", "."], "sha": "ed36268b7917332d16868208e1e565742a8753e1", "url": "https://github.com/quantopian/pgcontents/blob/ed36268b7917332d16868208e1e565742a8753e1/pgcontents/utils/migrate.py#L17-L31", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/utils/jsonutil.py", "func_name": "extract_dates", "original_string": "def extract_dates(obj):\n    \"\"\"extract ISO8601 dates from unpacked JSON\"\"\"\n    if isinstance(obj, dict):\n        obj = dict(obj) # don't clobber\n        for k,v in obj.iteritems():\n            obj[k] = extract_dates(v)\n    elif isinstance(obj, (list, tuple)):\n        obj = [ extract_dates(o) for o in obj ]\n    elif isinstance(obj, basestring):\n        if ISO8601_PAT.match(obj):\n            obj = datetime.strptime(obj, ISO8601)\n    return obj", "language": "python", "code": "def extract_dates(obj):\n    \"\"\"extract ISO8601 dates from unpacked JSON\"\"\"\n    if isinstance(obj, dict):\n        obj = dict(obj) # don't clobber\n        for k,v in obj.iteritems():\n            obj[k] = extract_dates(v)\n    elif isinstance(obj, (list, tuple)):\n        obj = [ extract_dates(o) for o in obj ]\n    elif isinstance(obj, basestring):\n        if ISO8601_PAT.match(obj):\n            obj = datetime.strptime(obj, ISO8601)\n    return obj", "code_tokens": ["def", "extract_dates", "(", "obj", ")", ":", "if", "isinstance", "(", "obj", ",", "dict", ")", ":", "obj", "=", "dict", "(", "obj", ")", "# don't clobber", "for", "k", ",", "v", "in", "obj", ".", "iteritems", "(", ")", ":", "obj", "[", "k", "]", "=", "extract_dates", "(", "v", ")", "elif", "isinstance", "(", "obj", ",", "(", "list", ",", "tuple", ")", ")", ":", "obj", "=", "[", "extract_dates", "(", "o", ")", "for", "o", "in", "obj", "]", "elif", "isinstance", "(", "obj", ",", "basestring", ")", ":", "if", "ISO8601_PAT", ".", "match", "(", "obj", ")", ":", "obj", "=", "datetime", ".", "strptime", "(", "obj", ",", "ISO8601", ")", "return", "obj"], "docstring": "extract ISO8601 dates from unpacked JSON", "docstring_tokens": ["extract", "ISO8601", "dates", "from", "unpacked", "JSON"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/utils/jsonutil.py#L61-L72", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/hdl/statements.py", "func_name": "HdlStatement._on_parent_event_dependent", "original_string": "def _on_parent_event_dependent(self):\n        \"\"\"\n        After parrent statement become event dependent\n        propagate event dependency flag to child statements\n        \"\"\"\n        if not self._is_completly_event_dependent:\n            self._is_completly_event_dependent = True\n            for stm in self._iter_stms():\n                stm._on_parent_event_dependent()", "language": "python", "code": "def _on_parent_event_dependent(self):\n        \"\"\"\n        After parrent statement become event dependent\n        propagate event dependency flag to child statements\n        \"\"\"\n        if not self._is_completly_event_dependent:\n            self._is_completly_event_dependent = True\n            for stm in self._iter_stms():\n                stm._on_parent_event_dependent()", "code_tokens": ["def", "_on_parent_event_dependent", "(", "self", ")", ":", "if", "not", "self", ".", "_is_completly_event_dependent", ":", "self", ".", "_is_completly_event_dependent", "=", "True", "for", "stm", "in", "self", ".", "_iter_stms", "(", ")", ":", "stm", ".", "_on_parent_event_dependent", "(", ")"], "docstring": "After parrent statement become event dependent\n        propagate event dependency flag to child statements", "docstring_tokens": ["After", "parrent", "statement", "become", "event", "dependent", "propagate", "event", "dependency", "flag", "to", "child", "statements"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/hdl/statements.py#L445-L453", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/collector.py", "func_name": "Collector.start", "original_string": "def start(self):\n        \"\"\"Start collecting trace information.\"\"\"\n        if self._collectors:\n            self._collectors[-1].pause()\n        self._collectors.append(self)\n        #print(\"Started: %r\" % self._collectors, file=sys.stderr)\n\n        # Check to see whether we had a fullcoverage tracer installed.\n        traces0 = []\n        if hasattr(sys, \"gettrace\"):\n            fn0 = sys.gettrace()\n            if fn0:\n                tracer0 = getattr(fn0, '__self__', None)\n                if tracer0:\n                    traces0 = getattr(tracer0, 'traces', [])\n\n        # Install the tracer on this thread.\n        fn = self._start_tracer()\n\n        for args in traces0:\n            (frame, event, arg), lineno = args\n            try:\n                fn(frame, event, arg, lineno=lineno)\n            except TypeError:\n                raise Exception(\n                    \"fullcoverage must be run with the C trace function.\"\n                )\n\n        # Install our installation tracer in threading, to jump start other\n        # threads.\n        threading.settrace(self._installation_trace)", "language": "python", "code": "def start(self):\n        \"\"\"Start collecting trace information.\"\"\"\n        if self._collectors:\n            self._collectors[-1].pause()\n        self._collectors.append(self)\n        #print(\"Started: %r\" % self._collectors, file=sys.stderr)\n\n        # Check to see whether we had a fullcoverage tracer installed.\n        traces0 = []\n        if hasattr(sys, \"gettrace\"):\n            fn0 = sys.gettrace()\n            if fn0:\n                tracer0 = getattr(fn0, '__self__', None)\n                if tracer0:\n                    traces0 = getattr(tracer0, 'traces', [])\n\n        # Install the tracer on this thread.\n        fn = self._start_tracer()\n\n        for args in traces0:\n            (frame, event, arg), lineno = args\n            try:\n                fn(frame, event, arg, lineno=lineno)\n            except TypeError:\n                raise Exception(\n                    \"fullcoverage must be run with the C trace function.\"\n                )\n\n        # Install our installation tracer in threading, to jump start other\n        # threads.\n        threading.settrace(self._installation_trace)", "code_tokens": ["def", "start", "(", "self", ")", ":", "if", "self", ".", "_collectors", ":", "self", ".", "_collectors", "[", "-", "1", "]", ".", "pause", "(", ")", "self", ".", "_collectors", ".", "append", "(", "self", ")", "#print(\"Started: %r\" % self._collectors, file=sys.stderr)", "# Check to see whether we had a fullcoverage tracer installed.", "traces0", "=", "[", "]", "if", "hasattr", "(", "sys", ",", "\"gettrace\"", ")", ":", "fn0", "=", "sys", ".", "gettrace", "(", ")", "if", "fn0", ":", "tracer0", "=", "getattr", "(", "fn0", ",", "'__self__'", ",", "None", ")", "if", "tracer0", ":", "traces0", "=", "getattr", "(", "tracer0", ",", "'traces'", ",", "[", "]", ")", "# Install the tracer on this thread.", "fn", "=", "self", ".", "_start_tracer", "(", ")", "for", "args", "in", "traces0", ":", "(", "frame", ",", "event", ",", "arg", ")", ",", "lineno", "=", "args", "try", ":", "fn", "(", "frame", ",", "event", ",", "arg", ",", "lineno", "=", "lineno", ")", "except", "TypeError", ":", "raise", "Exception", "(", "\"fullcoverage must be run with the C trace function.\"", ")", "# Install our installation tracer in threading, to jump start other", "# threads.", "threading", ".", "settrace", "(", "self", ".", "_installation_trace", ")"], "docstring": "Start collecting trace information.", "docstring_tokens": ["Start", "collecting", "trace", "information", "."], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/collector.py#L258-L288", "partition": "test"}
{"repo": "theduke/django-baseline", "path": "django_baseline/templatetags/helpers.py", "func_name": "sub", "original_string": "def sub(value, arg):\n    \"\"\"Subtract the arg from the value.\"\"\"\n    try:\n        return valid_numeric(value) - valid_numeric(arg)\n    except (ValueError, TypeError):\n        try:\n            return value - arg\n        except Exception:\n            return ''", "language": "python", "code": "def sub(value, arg):\n    \"\"\"Subtract the arg from the value.\"\"\"\n    try:\n        return valid_numeric(value) - valid_numeric(arg)\n    except (ValueError, TypeError):\n        try:\n            return value - arg\n        except Exception:\n            return ''", "code_tokens": ["def", "sub", "(", "value", ",", "arg", ")", ":", "try", ":", "return", "valid_numeric", "(", "value", ")", "-", "valid_numeric", "(", "arg", ")", "except", "(", "ValueError", ",", "TypeError", ")", ":", "try", ":", "return", "value", "-", "arg", "except", "Exception", ":", "return", "''"], "docstring": "Subtract the arg from the value.", "docstring_tokens": ["Subtract", "the", "arg", "from", "the", "value", "."], "sha": "7be8b956e53c70b35f34e1783a8fe8f716955afb", "url": "https://github.com/theduke/django-baseline/blob/7be8b956e53c70b35f34e1783a8fe8f716955afb/django_baseline/templatetags/helpers.py#L114-L122", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/core/constantq.py", "func_name": "pseudo_cqt", "original_string": "def pseudo_cqt(y, sr=22050, hop_length=512, fmin=None, n_bins=84,\n               bins_per_octave=12, tuning=0.0, filter_scale=1,\n               norm=1, sparsity=0.01, window='hann', scale=True,\n               pad_mode='reflect'):\n    '''Compute the pseudo constant-Q transform of an audio signal.\n\n    This uses a single fft size that is the smallest power of 2 that is greater\n    than or equal to the max of:\n\n        1. The longest CQT filter\n        2. 2x the hop_length\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,)]\n        audio time series\n\n    sr : number > 0 [scalar]\n        sampling rate of `y`\n\n    hop_length : int > 0 [scalar]\n        number of samples between successive CQT columns.\n\n    fmin : float > 0 [scalar]\n        Minimum frequency. Defaults to C1 ~= 32.70 Hz\n\n    n_bins : int > 0 [scalar]\n        Number of frequency bins, starting at `fmin`\n\n    bins_per_octave : int > 0 [scalar]\n        Number of bins per octave\n\n    tuning : None or float in `[-0.5, 0.5)`\n        Tuning offset in fractions of a bin (cents).\n\n        If `None`, tuning will be automatically estimated from the signal.\n\n    filter_scale : float > 0\n        Filter filter_scale factor. Larger values use longer windows.\n\n    sparsity : float in [0, 1)\n        Sparsify the CQT basis by discarding up to `sparsity`\n        fraction of the energy in each basis.\n\n        Set `sparsity=0` to disable sparsification.\n\n    window : str, tuple, number, or function\n        Window specification for the basis filters.\n        See `filters.get_window` for details.\n\n    pad_mode : string\n        Padding mode for centered frame analysis.\n\n        See also: `librosa.core.stft` and `np.pad`.\n\n    Returns\n    -------\n    CQT : np.ndarray [shape=(n_bins, t), dtype=np.float]\n        Pseudo Constant-Q energy for each frequency at each time.\n\n    Raises\n    ------\n    ParameterError\n        If `hop_length` is not an integer multiple of\n        `2**(n_bins / bins_per_octave)`\n\n        Or if `y` is too short to support the frequency range of the CQT.\n\n    Notes\n    -----\n    This function caches at level 20.\n\n    '''\n\n    if fmin is None:\n        # C1 by default\n        fmin = note_to_hz('C1')\n\n    if tuning is None:\n        tuning = estimate_tuning(y=y, sr=sr)\n\n    fft_basis, n_fft, _ = __cqt_filter_fft(sr, fmin, n_bins,\n                                           bins_per_octave,\n                                           tuning, filter_scale,\n                                           norm, sparsity,\n                                           hop_length=hop_length,\n                                           window=window)\n\n    fft_basis = np.abs(fft_basis)\n\n    # Compute the magnitude STFT with Hann window\n    D = np.abs(stft(y, n_fft=n_fft, hop_length=hop_length, pad_mode=pad_mode))\n\n    # Project onto the pseudo-cqt basis\n    C = fft_basis.dot(D)\n\n    if scale:\n        C /= np.sqrt(n_fft)\n    else:\n        lengths = filters.constant_q_lengths(sr, fmin,\n                                             n_bins=n_bins,\n                                             bins_per_octave=bins_per_octave,\n                                             tuning=tuning,\n                                             window=window,\n                                             filter_scale=filter_scale)\n\n        C *= np.sqrt(lengths[:, np.newaxis] / n_fft)\n\n    return C", "language": "python", "code": "def pseudo_cqt(y, sr=22050, hop_length=512, fmin=None, n_bins=84,\n               bins_per_octave=12, tuning=0.0, filter_scale=1,\n               norm=1, sparsity=0.01, window='hann', scale=True,\n               pad_mode='reflect'):\n    '''Compute the pseudo constant-Q transform of an audio signal.\n\n    This uses a single fft size that is the smallest power of 2 that is greater\n    than or equal to the max of:\n\n        1. The longest CQT filter\n        2. 2x the hop_length\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,)]\n        audio time series\n\n    sr : number > 0 [scalar]\n        sampling rate of `y`\n\n    hop_length : int > 0 [scalar]\n        number of samples between successive CQT columns.\n\n    fmin : float > 0 [scalar]\n        Minimum frequency. Defaults to C1 ~= 32.70 Hz\n\n    n_bins : int > 0 [scalar]\n        Number of frequency bins, starting at `fmin`\n\n    bins_per_octave : int > 0 [scalar]\n        Number of bins per octave\n\n    tuning : None or float in `[-0.5, 0.5)`\n        Tuning offset in fractions of a bin (cents).\n\n        If `None`, tuning will be automatically estimated from the signal.\n\n    filter_scale : float > 0\n        Filter filter_scale factor. Larger values use longer windows.\n\n    sparsity : float in [0, 1)\n        Sparsify the CQT basis by discarding up to `sparsity`\n        fraction of the energy in each basis.\n\n        Set `sparsity=0` to disable sparsification.\n\n    window : str, tuple, number, or function\n        Window specification for the basis filters.\n        See `filters.get_window` for details.\n\n    pad_mode : string\n        Padding mode for centered frame analysis.\n\n        See also: `librosa.core.stft` and `np.pad`.\n\n    Returns\n    -------\n    CQT : np.ndarray [shape=(n_bins, t), dtype=np.float]\n        Pseudo Constant-Q energy for each frequency at each time.\n\n    Raises\n    ------\n    ParameterError\n        If `hop_length` is not an integer multiple of\n        `2**(n_bins / bins_per_octave)`\n\n        Or if `y` is too short to support the frequency range of the CQT.\n\n    Notes\n    -----\n    This function caches at level 20.\n\n    '''\n\n    if fmin is None:\n        # C1 by default\n        fmin = note_to_hz('C1')\n\n    if tuning is None:\n        tuning = estimate_tuning(y=y, sr=sr)\n\n    fft_basis, n_fft, _ = __cqt_filter_fft(sr, fmin, n_bins,\n                                           bins_per_octave,\n                                           tuning, filter_scale,\n                                           norm, sparsity,\n                                           hop_length=hop_length,\n                                           window=window)\n\n    fft_basis = np.abs(fft_basis)\n\n    # Compute the magnitude STFT with Hann window\n    D = np.abs(stft(y, n_fft=n_fft, hop_length=hop_length, pad_mode=pad_mode))\n\n    # Project onto the pseudo-cqt basis\n    C = fft_basis.dot(D)\n\n    if scale:\n        C /= np.sqrt(n_fft)\n    else:\n        lengths = filters.constant_q_lengths(sr, fmin,\n                                             n_bins=n_bins,\n                                             bins_per_octave=bins_per_octave,\n                                             tuning=tuning,\n                                             window=window,\n                                             filter_scale=filter_scale)\n\n        C *= np.sqrt(lengths[:, np.newaxis] / n_fft)\n\n    return C", "code_tokens": ["def", "pseudo_cqt", "(", "y", ",", "sr", "=", "22050", ",", "hop_length", "=", "512", ",", "fmin", "=", "None", ",", "n_bins", "=", "84", ",", "bins_per_octave", "=", "12", ",", "tuning", "=", "0.0", ",", "filter_scale", "=", "1", ",", "norm", "=", "1", ",", "sparsity", "=", "0.01", ",", "window", "=", "'hann'", ",", "scale", "=", "True", ",", "pad_mode", "=", "'reflect'", ")", ":", "if", "fmin", "is", "None", ":", "# C1 by default", "fmin", "=", "note_to_hz", "(", "'C1'", ")", "if", "tuning", "is", "None", ":", "tuning", "=", "estimate_tuning", "(", "y", "=", "y", ",", "sr", "=", "sr", ")", "fft_basis", ",", "n_fft", ",", "_", "=", "__cqt_filter_fft", "(", "sr", ",", "fmin", ",", "n_bins", ",", "bins_per_octave", ",", "tuning", ",", "filter_scale", ",", "norm", ",", "sparsity", ",", "hop_length", "=", "hop_length", ",", "window", "=", "window", ")", "fft_basis", "=", "np", ".", "abs", "(", "fft_basis", ")", "# Compute the magnitude STFT with Hann window", "D", "=", "np", ".", "abs", "(", "stft", "(", "y", ",", "n_fft", "=", "n_fft", ",", "hop_length", "=", "hop_length", ",", "pad_mode", "=", "pad_mode", ")", ")", "# Project onto the pseudo-cqt basis", "C", "=", "fft_basis", ".", "dot", "(", "D", ")", "if", "scale", ":", "C", "/=", "np", ".", "sqrt", "(", "n_fft", ")", "else", ":", "lengths", "=", "filters", ".", "constant_q_lengths", "(", "sr", ",", "fmin", ",", "n_bins", "=", "n_bins", ",", "bins_per_octave", "=", "bins_per_octave", ",", "tuning", "=", "tuning", ",", "window", "=", "window", ",", "filter_scale", "=", "filter_scale", ")", "C", "*=", "np", ".", "sqrt", "(", "lengths", "[", ":", ",", "np", ".", "newaxis", "]", "/", "n_fft", ")", "return", "C"], "docstring": "Compute the pseudo constant-Q transform of an audio signal.\n\n    This uses a single fft size that is the smallest power of 2 that is greater\n    than or equal to the max of:\n\n        1. The longest CQT filter\n        2. 2x the hop_length\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,)]\n        audio time series\n\n    sr : number > 0 [scalar]\n        sampling rate of `y`\n\n    hop_length : int > 0 [scalar]\n        number of samples between successive CQT columns.\n\n    fmin : float > 0 [scalar]\n        Minimum frequency. Defaults to C1 ~= 32.70 Hz\n\n    n_bins : int > 0 [scalar]\n        Number of frequency bins, starting at `fmin`\n\n    bins_per_octave : int > 0 [scalar]\n        Number of bins per octave\n\n    tuning : None or float in `[-0.5, 0.5)`\n        Tuning offset in fractions of a bin (cents).\n\n        If `None`, tuning will be automatically estimated from the signal.\n\n    filter_scale : float > 0\n        Filter filter_scale factor. Larger values use longer windows.\n\n    sparsity : float in [0, 1)\n        Sparsify the CQT basis by discarding up to `sparsity`\n        fraction of the energy in each basis.\n\n        Set `sparsity=0` to disable sparsification.\n\n    window : str, tuple, number, or function\n        Window specification for the basis filters.\n        See `filters.get_window` for details.\n\n    pad_mode : string\n        Padding mode for centered frame analysis.\n\n        See also: `librosa.core.stft` and `np.pad`.\n\n    Returns\n    -------\n    CQT : np.ndarray [shape=(n_bins, t), dtype=np.float]\n        Pseudo Constant-Q energy for each frequency at each time.\n\n    Raises\n    ------\n    ParameterError\n        If `hop_length` is not an integer multiple of\n        `2**(n_bins / bins_per_octave)`\n\n        Or if `y` is too short to support the frequency range of the CQT.\n\n    Notes\n    -----\n    This function caches at level 20.", "docstring_tokens": ["Compute", "the", "pseudo", "constant", "-", "Q", "transform", "of", "an", "audio", "signal", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/core/constantq.py#L426-L534", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/numeric.py", "func_name": "soft_threshold", "original_string": "def soft_threshold(x, threshold, name=None):\n  \"\"\"Soft Thresholding operator.\n\n  This operator is defined by the equations\n\n  ```none\n                                { x[i] - gamma,  x[i] >   gamma\n  SoftThreshold(x, gamma)[i] =  { 0,             x[i] ==  gamma\n                                { x[i] + gamma,  x[i] <  -gamma\n  ```\n\n  In the context of proximal gradient methods, we have\n\n  ```none\n  SoftThreshold(x, gamma) = prox_{gamma L1}(x)\n  ```\n\n  where `prox` is the proximity operator.  Thus the soft thresholding operator\n  is used in proximal gradient descent for optimizing a smooth function with\n  (non-smooth) L1 regularization, as outlined below.\n\n  The proximity operator is defined as:\n\n  ```none\n  prox_r(x) = argmin{ r(z) + 0.5 ||x - z||_2**2 : z },\n  ```\n\n  where `r` is a (weakly) convex function, not necessarily differentiable.\n  Because the L2 norm is strictly convex, the above argmin is unique.\n\n  One important application of the proximity operator is as follows.  Let `L` be\n  a convex and differentiable function with Lipschitz-continuous gradient.  Let\n  `R` be a convex lower semicontinuous function which is possibly\n  nondifferentiable.  Let `gamma` be an arbitrary positive real.  Then\n\n  ```none\n  x_star = argmin{ L(x) + R(x) : x }\n  ```\n\n  if and only if the fixed-point equation is satisfied:\n\n  ```none\n  x_star = prox_{gamma R}(x_star - gamma grad L(x_star))\n  ```\n\n  Proximal gradient descent thus typically consists of choosing an initial value\n  `x^{(0)}` and repeatedly applying the update\n\n  ```none\n  x^{(k+1)} = prox_{gamma^{(k)} R}(x^{(k)} - gamma^{(k)} grad L(x^{(k)}))\n  ```\n\n  where `gamma` is allowed to vary from iteration to iteration.  Specializing to\n  the case where `R(x) = ||x||_1`, we minimize `L(x) + ||x||_1` by repeatedly\n  applying the update\n\n  ```\n  x^{(k+1)} = SoftThreshold(x - gamma grad L(x^{(k)}), gamma)\n  ```\n\n  (This idea can also be extended to second-order approximations, although the\n  multivariate case does not have a known closed form like above.)\n\n  Args:\n    x: `float` `Tensor` representing the input to the SoftThreshold function.\n    threshold: nonnegative scalar, `float` `Tensor` representing the radius of\n      the interval on which each coordinate of SoftThreshold takes the value\n      zero.  Denoted `gamma` above.\n    name: Python string indicating the name of the TensorFlow operation.\n      Default value: `'soft_threshold'`.\n\n  Returns:\n    softthreshold: `float` `Tensor` with the same shape and dtype as `x`,\n      representing the value of the SoftThreshold function.\n\n  #### References\n\n  [1]: Yu, Yao-Liang. The Proximity Operator.\n       https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf\n\n  [2]: Wikipedia Contributors. Proximal gradient methods for learning.\n       _Wikipedia, The Free Encyclopedia_, 2018.\n       https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning\n\n  \"\"\"\n  # https://math.stackexchange.com/questions/471339/derivation-of-soft-thresholding-operator\n  with tf.compat.v1.name_scope(name, 'soft_threshold', [x, threshold]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    threshold = tf.convert_to_tensor(\n        value=threshold, dtype=x.dtype, name='threshold')\n    return tf.sign(x) * tf.maximum(tf.abs(x) - threshold, 0.)", "language": "python", "code": "def soft_threshold(x, threshold, name=None):\n  \"\"\"Soft Thresholding operator.\n\n  This operator is defined by the equations\n\n  ```none\n                                { x[i] - gamma,  x[i] >   gamma\n  SoftThreshold(x, gamma)[i] =  { 0,             x[i] ==  gamma\n                                { x[i] + gamma,  x[i] <  -gamma\n  ```\n\n  In the context of proximal gradient methods, we have\n\n  ```none\n  SoftThreshold(x, gamma) = prox_{gamma L1}(x)\n  ```\n\n  where `prox` is the proximity operator.  Thus the soft thresholding operator\n  is used in proximal gradient descent for optimizing a smooth function with\n  (non-smooth) L1 regularization, as outlined below.\n\n  The proximity operator is defined as:\n\n  ```none\n  prox_r(x) = argmin{ r(z) + 0.5 ||x - z||_2**2 : z },\n  ```\n\n  where `r` is a (weakly) convex function, not necessarily differentiable.\n  Because the L2 norm is strictly convex, the above argmin is unique.\n\n  One important application of the proximity operator is as follows.  Let `L` be\n  a convex and differentiable function with Lipschitz-continuous gradient.  Let\n  `R` be a convex lower semicontinuous function which is possibly\n  nondifferentiable.  Let `gamma` be an arbitrary positive real.  Then\n\n  ```none\n  x_star = argmin{ L(x) + R(x) : x }\n  ```\n\n  if and only if the fixed-point equation is satisfied:\n\n  ```none\n  x_star = prox_{gamma R}(x_star - gamma grad L(x_star))\n  ```\n\n  Proximal gradient descent thus typically consists of choosing an initial value\n  `x^{(0)}` and repeatedly applying the update\n\n  ```none\n  x^{(k+1)} = prox_{gamma^{(k)} R}(x^{(k)} - gamma^{(k)} grad L(x^{(k)}))\n  ```\n\n  where `gamma` is allowed to vary from iteration to iteration.  Specializing to\n  the case where `R(x) = ||x||_1`, we minimize `L(x) + ||x||_1` by repeatedly\n  applying the update\n\n  ```\n  x^{(k+1)} = SoftThreshold(x - gamma grad L(x^{(k)}), gamma)\n  ```\n\n  (This idea can also be extended to second-order approximations, although the\n  multivariate case does not have a known closed form like above.)\n\n  Args:\n    x: `float` `Tensor` representing the input to the SoftThreshold function.\n    threshold: nonnegative scalar, `float` `Tensor` representing the radius of\n      the interval on which each coordinate of SoftThreshold takes the value\n      zero.  Denoted `gamma` above.\n    name: Python string indicating the name of the TensorFlow operation.\n      Default value: `'soft_threshold'`.\n\n  Returns:\n    softthreshold: `float` `Tensor` with the same shape and dtype as `x`,\n      representing the value of the SoftThreshold function.\n\n  #### References\n\n  [1]: Yu, Yao-Liang. The Proximity Operator.\n       https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf\n\n  [2]: Wikipedia Contributors. Proximal gradient methods for learning.\n       _Wikipedia, The Free Encyclopedia_, 2018.\n       https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning\n\n  \"\"\"\n  # https://math.stackexchange.com/questions/471339/derivation-of-soft-thresholding-operator\n  with tf.compat.v1.name_scope(name, 'soft_threshold', [x, threshold]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    threshold = tf.convert_to_tensor(\n        value=threshold, dtype=x.dtype, name='threshold')\n    return tf.sign(x) * tf.maximum(tf.abs(x) - threshold, 0.)", "code_tokens": ["def", "soft_threshold", "(", "x", ",", "threshold", ",", "name", "=", "None", ")", ":", "# https://math.stackexchange.com/questions/471339/derivation-of-soft-thresholding-operator", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'soft_threshold'", ",", "[", "x", ",", "threshold", "]", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ")", "threshold", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "threshold", ",", "dtype", "=", "x", ".", "dtype", ",", "name", "=", "'threshold'", ")", "return", "tf", ".", "sign", "(", "x", ")", "*", "tf", ".", "maximum", "(", "tf", ".", "abs", "(", "x", ")", "-", "threshold", ",", "0.", ")"], "docstring": "Soft Thresholding operator.\n\n  This operator is defined by the equations\n\n  ```none\n                                { x[i] - gamma,  x[i] >   gamma\n  SoftThreshold(x, gamma)[i] =  { 0,             x[i] ==  gamma\n                                { x[i] + gamma,  x[i] <  -gamma\n  ```\n\n  In the context of proximal gradient methods, we have\n\n  ```none\n  SoftThreshold(x, gamma) = prox_{gamma L1}(x)\n  ```\n\n  where `prox` is the proximity operator.  Thus the soft thresholding operator\n  is used in proximal gradient descent for optimizing a smooth function with\n  (non-smooth) L1 regularization, as outlined below.\n\n  The proximity operator is defined as:\n\n  ```none\n  prox_r(x) = argmin{ r(z) + 0.5 ||x - z||_2**2 : z },\n  ```\n\n  where `r` is a (weakly) convex function, not necessarily differentiable.\n  Because the L2 norm is strictly convex, the above argmin is unique.\n\n  One important application of the proximity operator is as follows.  Let `L` be\n  a convex and differentiable function with Lipschitz-continuous gradient.  Let\n  `R` be a convex lower semicontinuous function which is possibly\n  nondifferentiable.  Let `gamma` be an arbitrary positive real.  Then\n\n  ```none\n  x_star = argmin{ L(x) + R(x) : x }\n  ```\n\n  if and only if the fixed-point equation is satisfied:\n\n  ```none\n  x_star = prox_{gamma R}(x_star - gamma grad L(x_star))\n  ```\n\n  Proximal gradient descent thus typically consists of choosing an initial value\n  `x^{(0)}` and repeatedly applying the update\n\n  ```none\n  x^{(k+1)} = prox_{gamma^{(k)} R}(x^{(k)} - gamma^{(k)} grad L(x^{(k)}))\n  ```\n\n  where `gamma` is allowed to vary from iteration to iteration.  Specializing to\n  the case where `R(x) = ||x||_1`, we minimize `L(x) + ||x||_1` by repeatedly\n  applying the update\n\n  ```\n  x^{(k+1)} = SoftThreshold(x - gamma grad L(x^{(k)}), gamma)\n  ```\n\n  (This idea can also be extended to second-order approximations, although the\n  multivariate case does not have a known closed form like above.)\n\n  Args:\n    x: `float` `Tensor` representing the input to the SoftThreshold function.\n    threshold: nonnegative scalar, `float` `Tensor` representing the radius of\n      the interval on which each coordinate of SoftThreshold takes the value\n      zero.  Denoted `gamma` above.\n    name: Python string indicating the name of the TensorFlow operation.\n      Default value: `'soft_threshold'`.\n\n  Returns:\n    softthreshold: `float` `Tensor` with the same shape and dtype as `x`,\n      representing the value of the SoftThreshold function.\n\n  #### References\n\n  [1]: Yu, Yao-Liang. The Proximity Operator.\n       https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf\n\n  [2]: Wikipedia Contributors. Proximal gradient methods for learning.\n       _Wikipedia, The Free Encyclopedia_, 2018.\n       https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning", "docstring_tokens": ["Soft", "Thresholding", "operator", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/numeric.py#L59-L149", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/fitting.py", "func_name": "_build_trainable_posterior", "original_string": "def _build_trainable_posterior(param, initial_loc_fn):\n  \"\"\"Built a transformed-normal variational dist over a parameter's support.\"\"\"\n  loc = tf.compat.v1.get_variable(\n      param.name + '_loc',\n      initializer=lambda: initial_loc_fn(param),\n      dtype=param.prior.dtype,\n      use_resource=True)\n  scale = tf.nn.softplus(\n      tf.compat.v1.get_variable(\n          param.name + '_scale',\n          initializer=lambda: -4 * tf.ones_like(initial_loc_fn(param)),\n          dtype=param.prior.dtype,\n          use_resource=True))\n\n  q = tfd.Normal(loc=loc, scale=scale)\n\n  # Ensure the `event_shape` of the variational distribution matches the\n  # parameter.\n  if (param.prior.event_shape.ndims is None\n      or param.prior.event_shape.ndims > 0):\n    q = tfd.Independent(\n        q, reinterpreted_batch_ndims=param.prior.event_shape.ndims)\n\n  # Transform to constrained parameter space.\n  return tfd.TransformedDistribution(q, param.bijector)", "language": "python", "code": "def _build_trainable_posterior(param, initial_loc_fn):\n  \"\"\"Built a transformed-normal variational dist over a parameter's support.\"\"\"\n  loc = tf.compat.v1.get_variable(\n      param.name + '_loc',\n      initializer=lambda: initial_loc_fn(param),\n      dtype=param.prior.dtype,\n      use_resource=True)\n  scale = tf.nn.softplus(\n      tf.compat.v1.get_variable(\n          param.name + '_scale',\n          initializer=lambda: -4 * tf.ones_like(initial_loc_fn(param)),\n          dtype=param.prior.dtype,\n          use_resource=True))\n\n  q = tfd.Normal(loc=loc, scale=scale)\n\n  # Ensure the `event_shape` of the variational distribution matches the\n  # parameter.\n  if (param.prior.event_shape.ndims is None\n      or param.prior.event_shape.ndims > 0):\n    q = tfd.Independent(\n        q, reinterpreted_batch_ndims=param.prior.event_shape.ndims)\n\n  # Transform to constrained parameter space.\n  return tfd.TransformedDistribution(q, param.bijector)", "code_tokens": ["def", "_build_trainable_posterior", "(", "param", ",", "initial_loc_fn", ")", ":", "loc", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "param", ".", "name", "+", "'_loc'", ",", "initializer", "=", "lambda", ":", "initial_loc_fn", "(", "param", ")", ",", "dtype", "=", "param", ".", "prior", ".", "dtype", ",", "use_resource", "=", "True", ")", "scale", "=", "tf", ".", "nn", ".", "softplus", "(", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "param", ".", "name", "+", "'_scale'", ",", "initializer", "=", "lambda", ":", "-", "4", "*", "tf", ".", "ones_like", "(", "initial_loc_fn", "(", "param", ")", ")", ",", "dtype", "=", "param", ".", "prior", ".", "dtype", ",", "use_resource", "=", "True", ")", ")", "q", "=", "tfd", ".", "Normal", "(", "loc", "=", "loc", ",", "scale", "=", "scale", ")", "# Ensure the `event_shape` of the variational distribution matches the", "# parameter.", "if", "(", "param", ".", "prior", ".", "event_shape", ".", "ndims", "is", "None", "or", "param", ".", "prior", ".", "event_shape", ".", "ndims", ">", "0", ")", ":", "q", "=", "tfd", ".", "Independent", "(", "q", ",", "reinterpreted_batch_ndims", "=", "param", ".", "prior", ".", "event_shape", ".", "ndims", ")", "# Transform to constrained parameter space.", "return", "tfd", ".", "TransformedDistribution", "(", "q", ",", "param", ".", "bijector", ")"], "docstring": "Built a transformed-normal variational dist over a parameter's support.", "docstring_tokens": ["Built", "a", "transformed", "-", "normal", "variational", "dist", "over", "a", "parameter", "s", "support", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/fitting.py#L65-L89", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/completion_html.py", "func_name": "CompletionHtml.select_up", "original_string": "def select_up(self):\n        \"\"\"move cursor up\"\"\"\n        r, c = self._index\n        self._select_index(r-1, c)", "language": "python", "code": "def select_up(self):\n        \"\"\"move cursor up\"\"\"\n        r, c = self._index\n        self._select_index(r-1, c)", "code_tokens": ["def", "select_up", "(", "self", ")", ":", "r", ",", "c", "=", "self", ".", "_index", "self", ".", "_select_index", "(", "r", "-", "1", ",", "c", ")"], "docstring": "move cursor up", "docstring_tokens": ["move", "cursor", "up"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/completion_html.py#L282-L285", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/parse/panel.py", "func_name": "parse_genes", "original_string": "def parse_genes(gene_lines):\n    \"\"\"Parse a file with genes and return the hgnc ids\n\n    Args:\n        gene_lines(iterable(str)): Stream with genes\n\n    Returns:\n        genes(list(dict)): Dictionaries with relevant gene info\n    \"\"\"\n    genes = []\n    header = []\n    hgnc_identifiers = set()\n    delimiter = '\\t'\n    # This can be '\\t' or ';'\n    delimiters = ['\\t', ' ', ';']\n\n    # There are files that have '#' to indicate headers\n    # There are some files that start with a header line without\n    # any special symbol\n    for i,line in enumerate(gene_lines):\n        line = line.rstrip()\n        if not len(line) > 0:\n            continue\n        if line.startswith('#'):\n            if not line.startswith('##'):\n                # We need to try delimiters\n                # We prefer ';' or '\\t' byt should accept ' '\n                line_length = 0\n                delimiter = None\n                for alt in delimiters:\n                    head_line = line.split(alt)\n                    if len(head_line) > line_length:\n                        line_length = len(head_line)\n                        delimiter = alt\n\n                header = [word.lower() for word in line[1:].split(delimiter)]\n        else:\n            # If no header symbol(#) assume first line is header\n            if i == 0:\n                line_length = 0\n                for alt in delimiters:\n                    head_line = line.split(alt)\n                    if len(head_line) > line_length:\n                        line_length = len(head_line)\n                        delimiter = alt\n                \n                if ('hgnc' in line or 'HGNC' in line):\n                    header = [word.lower() for word in line.split(delimiter)]\n                    continue\n                # If first line is not a header try to sniff what the first\n                # columns holds\n                if line.split(delimiter)[0].isdigit():\n                    header = ['hgnc_id']\n                else:\n                    header = ['hgnc_symbol']\n\n            splitted_line = line.split(delimiter)\n            gene_info = dict(zip(header, splitted_line))\n\n            # There are cases when excel exports empty lines that looks like\n            # ;;;;;;;. This is a exception to handle these\n            info_found = False\n            for key in gene_info:\n                if gene_info[key]:\n                    info_found = True\n                    break\n            # If no info was found we skip that line\n            if not info_found:\n                continue\n\n            try:\n                gene = parse_gene(gene_info)\n            except Exception as e:\n                LOG.warning(e)\n                raise SyntaxError(\"Line {0} is malformed\".format(i + 1))\n\n            identifier = gene.pop('identifier')\n\n            if not identifier in hgnc_identifiers:\n                hgnc_identifiers.add(identifier)\n                genes.append(gene)\n\n    return genes", "language": "python", "code": "def parse_genes(gene_lines):\n    \"\"\"Parse a file with genes and return the hgnc ids\n\n    Args:\n        gene_lines(iterable(str)): Stream with genes\n\n    Returns:\n        genes(list(dict)): Dictionaries with relevant gene info\n    \"\"\"\n    genes = []\n    header = []\n    hgnc_identifiers = set()\n    delimiter = '\\t'\n    # This can be '\\t' or ';'\n    delimiters = ['\\t', ' ', ';']\n\n    # There are files that have '#' to indicate headers\n    # There are some files that start with a header line without\n    # any special symbol\n    for i,line in enumerate(gene_lines):\n        line = line.rstrip()\n        if not len(line) > 0:\n            continue\n        if line.startswith('#'):\n            if not line.startswith('##'):\n                # We need to try delimiters\n                # We prefer ';' or '\\t' byt should accept ' '\n                line_length = 0\n                delimiter = None\n                for alt in delimiters:\n                    head_line = line.split(alt)\n                    if len(head_line) > line_length:\n                        line_length = len(head_line)\n                        delimiter = alt\n\n                header = [word.lower() for word in line[1:].split(delimiter)]\n        else:\n            # If no header symbol(#) assume first line is header\n            if i == 0:\n                line_length = 0\n                for alt in delimiters:\n                    head_line = line.split(alt)\n                    if len(head_line) > line_length:\n                        line_length = len(head_line)\n                        delimiter = alt\n                \n                if ('hgnc' in line or 'HGNC' in line):\n                    header = [word.lower() for word in line.split(delimiter)]\n                    continue\n                # If first line is not a header try to sniff what the first\n                # columns holds\n                if line.split(delimiter)[0].isdigit():\n                    header = ['hgnc_id']\n                else:\n                    header = ['hgnc_symbol']\n\n            splitted_line = line.split(delimiter)\n            gene_info = dict(zip(header, splitted_line))\n\n            # There are cases when excel exports empty lines that looks like\n            # ;;;;;;;. This is a exception to handle these\n            info_found = False\n            for key in gene_info:\n                if gene_info[key]:\n                    info_found = True\n                    break\n            # If no info was found we skip that line\n            if not info_found:\n                continue\n\n            try:\n                gene = parse_gene(gene_info)\n            except Exception as e:\n                LOG.warning(e)\n                raise SyntaxError(\"Line {0} is malformed\".format(i + 1))\n\n            identifier = gene.pop('identifier')\n\n            if not identifier in hgnc_identifiers:\n                hgnc_identifiers.add(identifier)\n                genes.append(gene)\n\n    return genes", "code_tokens": ["def", "parse_genes", "(", "gene_lines", ")", ":", "genes", "=", "[", "]", "header", "=", "[", "]", "hgnc_identifiers", "=", "set", "(", ")", "delimiter", "=", "'\\t'", "# This can be '\\t' or ';'", "delimiters", "=", "[", "'\\t'", ",", "' '", ",", "';'", "]", "# There are files that have '#' to indicate headers", "# There are some files that start with a header line without", "# any special symbol", "for", "i", ",", "line", "in", "enumerate", "(", "gene_lines", ")", ":", "line", "=", "line", ".", "rstrip", "(", ")", "if", "not", "len", "(", "line", ")", ">", "0", ":", "continue", "if", "line", ".", "startswith", "(", "'#'", ")", ":", "if", "not", "line", ".", "startswith", "(", "'##'", ")", ":", "# We need to try delimiters", "# We prefer ';' or '\\t' byt should accept ' '", "line_length", "=", "0", "delimiter", "=", "None", "for", "alt", "in", "delimiters", ":", "head_line", "=", "line", ".", "split", "(", "alt", ")", "if", "len", "(", "head_line", ")", ">", "line_length", ":", "line_length", "=", "len", "(", "head_line", ")", "delimiter", "=", "alt", "header", "=", "[", "word", ".", "lower", "(", ")", "for", "word", "in", "line", "[", "1", ":", "]", ".", "split", "(", "delimiter", ")", "]", "else", ":", "# If no header symbol(#) assume first line is header", "if", "i", "==", "0", ":", "line_length", "=", "0", "for", "alt", "in", "delimiters", ":", "head_line", "=", "line", ".", "split", "(", "alt", ")", "if", "len", "(", "head_line", ")", ">", "line_length", ":", "line_length", "=", "len", "(", "head_line", ")", "delimiter", "=", "alt", "if", "(", "'hgnc'", "in", "line", "or", "'HGNC'", "in", "line", ")", ":", "header", "=", "[", "word", ".", "lower", "(", ")", "for", "word", "in", "line", ".", "split", "(", "delimiter", ")", "]", "continue", "# If first line is not a header try to sniff what the first", "# columns holds", "if", "line", ".", "split", "(", "delimiter", ")", "[", "0", "]", ".", "isdigit", "(", ")", ":", "header", "=", "[", "'hgnc_id'", "]", "else", ":", "header", "=", "[", "'hgnc_symbol'", "]", "splitted_line", "=", "line", ".", "split", "(", "delimiter", ")", "gene_info", "=", "dict", "(", "zip", "(", "header", ",", "splitted_line", ")", ")", "# There are cases when excel exports empty lines that looks like", "# ;;;;;;;. This is a exception to handle these", "info_found", "=", "False", "for", "key", "in", "gene_info", ":", "if", "gene_info", "[", "key", "]", ":", "info_found", "=", "True", "break", "# If no info was found we skip that line", "if", "not", "info_found", ":", "continue", "try", ":", "gene", "=", "parse_gene", "(", "gene_info", ")", "except", "Exception", "as", "e", ":", "LOG", ".", "warning", "(", "e", ")", "raise", "SyntaxError", "(", "\"Line {0} is malformed\"", ".", "format", "(", "i", "+", "1", ")", ")", "identifier", "=", "gene", ".", "pop", "(", "'identifier'", ")", "if", "not", "identifier", "in", "hgnc_identifiers", ":", "hgnc_identifiers", ".", "add", "(", "identifier", ")", "genes", ".", "append", "(", "gene", ")", "return", "genes"], "docstring": "Parse a file with genes and return the hgnc ids\n\n    Args:\n        gene_lines(iterable(str)): Stream with genes\n\n    Returns:\n        genes(list(dict)): Dictionaries with relevant gene info", "docstring_tokens": ["Parse", "a", "file", "with", "genes", "and", "return", "the", "hgnc", "ids"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/parse/panel.py#L175-L257", "partition": "test"}
{"repo": "datacamp/pythonwhat", "path": "pythonwhat/State.py", "func_name": "Dispatcher._getx", "original_string": "def _getx(self, Parser, ext_attr, tree):\n        \"\"\"getter for Parser outputs\"\"\"\n        # return cached output if possible\n        cache_key = Parser.__name__ + str(hash(tree))\n        if self._parser_cache.get(cache_key):\n            p = self._parser_cache[cache_key]\n        else:\n            # otherwise, run parser over tree\n            p = Parser()\n            # set mappings for parsers that inspect attribute access\n            if ext_attr != \"mappings\" and Parser in [\n                FunctionParser,\n                ObjectAccessParser,\n            ]:\n                p.mappings = self.context_mappings.copy()\n            # run parser\n            p.visit(tree)\n            # cache\n            self._parser_cache[cache_key] = p\n        return getattr(p, ext_attr)", "language": "python", "code": "def _getx(self, Parser, ext_attr, tree):\n        \"\"\"getter for Parser outputs\"\"\"\n        # return cached output if possible\n        cache_key = Parser.__name__ + str(hash(tree))\n        if self._parser_cache.get(cache_key):\n            p = self._parser_cache[cache_key]\n        else:\n            # otherwise, run parser over tree\n            p = Parser()\n            # set mappings for parsers that inspect attribute access\n            if ext_attr != \"mappings\" and Parser in [\n                FunctionParser,\n                ObjectAccessParser,\n            ]:\n                p.mappings = self.context_mappings.copy()\n            # run parser\n            p.visit(tree)\n            # cache\n            self._parser_cache[cache_key] = p\n        return getattr(p, ext_attr)", "code_tokens": ["def", "_getx", "(", "self", ",", "Parser", ",", "ext_attr", ",", "tree", ")", ":", "# return cached output if possible", "cache_key", "=", "Parser", ".", "__name__", "+", "str", "(", "hash", "(", "tree", ")", ")", "if", "self", ".", "_parser_cache", ".", "get", "(", "cache_key", ")", ":", "p", "=", "self", ".", "_parser_cache", "[", "cache_key", "]", "else", ":", "# otherwise, run parser over tree", "p", "=", "Parser", "(", ")", "# set mappings for parsers that inspect attribute access", "if", "ext_attr", "!=", "\"mappings\"", "and", "Parser", "in", "[", "FunctionParser", ",", "ObjectAccessParser", ",", "]", ":", "p", ".", "mappings", "=", "self", ".", "context_mappings", ".", "copy", "(", ")", "# run parser", "p", ".", "visit", "(", "tree", ")", "# cache", "self", ".", "_parser_cache", "[", "cache_key", "]", "=", "p", "return", "getattr", "(", "p", ",", "ext_attr", ")"], "docstring": "getter for Parser outputs", "docstring_tokens": ["getter", "for", "Parser", "outputs"], "sha": "ffbf7f8436a51f77c22f3bed75ba3bc37a5c666f", "url": "https://github.com/datacamp/pythonwhat/blob/ffbf7f8436a51f77c22f3bed75ba3bc37a5c666f/pythonwhat/State.py#L282-L301", "partition": "test"}
{"repo": "nkgilley/python-ecobee-api", "path": "pyecobee/__init__.py", "func_name": "Ecobee.request_pin", "original_string": "def request_pin(self):\n        ''' Method to request a PIN from ecobee for authorization '''\n        url = 'https://api.ecobee.com/authorize'\n        params = {'response_type': 'ecobeePin',\n                  'client_id': self.api_key, 'scope': 'smartWrite'}\n        try:\n            request = requests.get(url, params=params)\n        except RequestException:\n            logger.warn(\"Error connecting to Ecobee.  Possible connectivity outage.\"\n                        \"Could not request pin.\")\n            return\n        self.authorization_code = request.json()['code']\n        self.pin = request.json()['ecobeePin']\n        logger.error('Please authorize your ecobee developer app with PIN code '\n              + self.pin + '\\nGoto https://www.ecobee.com/consumerportal'\n              '/index.html, click\\nMy Apps, Add application, Enter Pin'\n              ' and click Authorize.\\nAfter authorizing, call request_'\n              'tokens() method.')", "language": "python", "code": "def request_pin(self):\n        ''' Method to request a PIN from ecobee for authorization '''\n        url = 'https://api.ecobee.com/authorize'\n        params = {'response_type': 'ecobeePin',\n                  'client_id': self.api_key, 'scope': 'smartWrite'}\n        try:\n            request = requests.get(url, params=params)\n        except RequestException:\n            logger.warn(\"Error connecting to Ecobee.  Possible connectivity outage.\"\n                        \"Could not request pin.\")\n            return\n        self.authorization_code = request.json()['code']\n        self.pin = request.json()['ecobeePin']\n        logger.error('Please authorize your ecobee developer app with PIN code '\n              + self.pin + '\\nGoto https://www.ecobee.com/consumerportal'\n              '/index.html, click\\nMy Apps, Add application, Enter Pin'\n              ' and click Authorize.\\nAfter authorizing, call request_'\n              'tokens() method.')", "code_tokens": ["def", "request_pin", "(", "self", ")", ":", "url", "=", "'https://api.ecobee.com/authorize'", "params", "=", "{", "'response_type'", ":", "'ecobeePin'", ",", "'client_id'", ":", "self", ".", "api_key", ",", "'scope'", ":", "'smartWrite'", "}", "try", ":", "request", "=", "requests", ".", "get", "(", "url", ",", "params", "=", "params", ")", "except", "RequestException", ":", "logger", ".", "warn", "(", "\"Error connecting to Ecobee.  Possible connectivity outage.\"", "\"Could not request pin.\"", ")", "return", "self", ".", "authorization_code", "=", "request", ".", "json", "(", ")", "[", "'code'", "]", "self", ".", "pin", "=", "request", ".", "json", "(", ")", "[", "'ecobeePin'", "]", "logger", ".", "error", "(", "'Please authorize your ecobee developer app with PIN code '", "+", "self", ".", "pin", "+", "'\\nGoto https://www.ecobee.com/consumerportal'", "'/index.html, click\\nMy Apps, Add application, Enter Pin'", "' and click Authorize.\\nAfter authorizing, call request_'", "'tokens() method.'", ")"], "docstring": "Method to request a PIN from ecobee for authorization", "docstring_tokens": ["Method", "to", "request", "a", "PIN", "from", "ecobee", "for", "authorization"], "sha": "cc8d90d20abcb9ef5b66ec9cb035bae2f06ba174", "url": "https://github.com/nkgilley/python-ecobee-api/blob/cc8d90d20abcb9ef5b66ec9cb035bae2f06ba174/pyecobee/__init__.py#L77-L94", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/git.py", "func_name": "GitRepository._update_ref", "original_string": "def _update_ref(self, ref, delete=False):\n        \"\"\"Update a reference.\"\"\"\n\n        cmd = ['git', 'update-ref']\n\n        if delete:\n            cmd.extend(['-d', ref.refname])\n            action = 'deleted'\n        else:\n            cmd.extend([ref.refname, ref.hash])\n            action = 'updated to %s' % ref.hash\n\n        try:\n            self._exec(cmd, cwd=self.dirpath, env=self.gitenv)\n        except RepositoryError as e:\n            logger.warning(\"Git %s ref could not be %s during sync process in %s (%s); skipped\",\n                           ref.refname, action, self.uri, self.dirpath)\n        else:\n            logger.debug(\"Git %s ref %s in %s (%s)\",\n                         ref.refname, action, self.uri, self.dirpath)", "language": "python", "code": "def _update_ref(self, ref, delete=False):\n        \"\"\"Update a reference.\"\"\"\n\n        cmd = ['git', 'update-ref']\n\n        if delete:\n            cmd.extend(['-d', ref.refname])\n            action = 'deleted'\n        else:\n            cmd.extend([ref.refname, ref.hash])\n            action = 'updated to %s' % ref.hash\n\n        try:\n            self._exec(cmd, cwd=self.dirpath, env=self.gitenv)\n        except RepositoryError as e:\n            logger.warning(\"Git %s ref could not be %s during sync process in %s (%s); skipped\",\n                           ref.refname, action, self.uri, self.dirpath)\n        else:\n            logger.debug(\"Git %s ref %s in %s (%s)\",\n                         ref.refname, action, self.uri, self.dirpath)", "code_tokens": ["def", "_update_ref", "(", "self", ",", "ref", ",", "delete", "=", "False", ")", ":", "cmd", "=", "[", "'git'", ",", "'update-ref'", "]", "if", "delete", ":", "cmd", ".", "extend", "(", "[", "'-d'", ",", "ref", ".", "refname", "]", ")", "action", "=", "'deleted'", "else", ":", "cmd", ".", "extend", "(", "[", "ref", ".", "refname", ",", "ref", ".", "hash", "]", ")", "action", "=", "'updated to %s'", "%", "ref", ".", "hash", "try", ":", "self", ".", "_exec", "(", "cmd", ",", "cwd", "=", "self", ".", "dirpath", ",", "env", "=", "self", ".", "gitenv", ")", "except", "RepositoryError", "as", "e", ":", "logger", ".", "warning", "(", "\"Git %s ref could not be %s during sync process in %s (%s); skipped\"", ",", "ref", ".", "refname", ",", "action", ",", "self", ".", "uri", ",", "self", ".", "dirpath", ")", "else", ":", "logger", ".", "debug", "(", "\"Git %s ref %s in %s (%s)\"", ",", "ref", ".", "refname", ",", "action", ",", "self", ".", "uri", ",", "self", ".", "dirpath", ")"], "docstring": "Update a reference.", "docstring_tokens": ["Update", "a", "reference", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/git.py#L1204-L1223", "partition": "test"}
{"repo": "piface/pifacecommon", "path": "pifacecommon/interrupts.py", "func_name": "PortEventListener.register", "original_string": "def register(self, pin_num, direction, callback,\n                 settle_time=DEFAULT_SETTLE_TIME):\n        \"\"\"Registers a pin number and direction to a callback function.\n\n        :param pin_num: The pin pin number.\n        :type pin_num: int\n        :param direction: The event direction\n            (use: IODIR_ON/IODIR_OFF/IODIR_BOTH)\n        :type direction: int\n        :param callback: The function to run when event is detected.\n        :type callback: function\n        :param settle_time: Time within which subsequent events are ignored.\n        :type settle_time: int\n        \"\"\"\n        self.pin_function_maps.append(\n            PinFunctionMap(pin_num, direction, callback, settle_time))", "language": "python", "code": "def register(self, pin_num, direction, callback,\n                 settle_time=DEFAULT_SETTLE_TIME):\n        \"\"\"Registers a pin number and direction to a callback function.\n\n        :param pin_num: The pin pin number.\n        :type pin_num: int\n        :param direction: The event direction\n            (use: IODIR_ON/IODIR_OFF/IODIR_BOTH)\n        :type direction: int\n        :param callback: The function to run when event is detected.\n        :type callback: function\n        :param settle_time: Time within which subsequent events are ignored.\n        :type settle_time: int\n        \"\"\"\n        self.pin_function_maps.append(\n            PinFunctionMap(pin_num, direction, callback, settle_time))", "code_tokens": ["def", "register", "(", "self", ",", "pin_num", ",", "direction", ",", "callback", ",", "settle_time", "=", "DEFAULT_SETTLE_TIME", ")", ":", "self", ".", "pin_function_maps", ".", "append", "(", "PinFunctionMap", "(", "pin_num", ",", "direction", ",", "callback", ",", "settle_time", ")", ")"], "docstring": "Registers a pin number and direction to a callback function.\n\n        :param pin_num: The pin pin number.\n        :type pin_num: int\n        :param direction: The event direction\n            (use: IODIR_ON/IODIR_OFF/IODIR_BOTH)\n        :type direction: int\n        :param callback: The function to run when event is detected.\n        :type callback: function\n        :param settle_time: Time within which subsequent events are ignored.\n        :type settle_time: int", "docstring_tokens": ["Registers", "a", "pin", "number", "and", "direction", "to", "a", "callback", "function", "."], "sha": "006bca14c18d43ba2d9eafaa84ef83b512c51cf6", "url": "https://github.com/piface/pifacecommon/blob/006bca14c18d43ba2d9eafaa84ef83b512c51cf6/pifacecommon/interrupts.py#L174-L189", "partition": "test"}
{"repo": "iotaledger/iota.lib.py", "path": "iota/bin/repl.py", "func_name": "IotaReplCommandLineApp._start_repl", "original_string": "def _start_repl(api):\n        # type: (Iota) -> None\n        \"\"\"\n        Starts the REPL.\n        \"\"\"\n        banner = (\n            'IOTA API client for {uri} ({testnet}) '\n            'initialized as variable `api`.\\n'\n            'Type `help(api)` for list of API commands.'.format(\n                testnet='testnet' if api.testnet else 'mainnet',\n                uri=api.adapter.get_uri(),\n            )\n        )\n\n        scope_vars = {'api': api}\n\n        try:\n            # noinspection PyUnresolvedReferences\n            import IPython\n        except ImportError:\n            # IPython not available; use regular Python REPL.\n            from code import InteractiveConsole\n            InteractiveConsole(locals=scope_vars).interact(banner, '')\n        else:\n            print(banner)\n            IPython.start_ipython(argv=[], user_ns=scope_vars)", "language": "python", "code": "def _start_repl(api):\n        # type: (Iota) -> None\n        \"\"\"\n        Starts the REPL.\n        \"\"\"\n        banner = (\n            'IOTA API client for {uri} ({testnet}) '\n            'initialized as variable `api`.\\n'\n            'Type `help(api)` for list of API commands.'.format(\n                testnet='testnet' if api.testnet else 'mainnet',\n                uri=api.adapter.get_uri(),\n            )\n        )\n\n        scope_vars = {'api': api}\n\n        try:\n            # noinspection PyUnresolvedReferences\n            import IPython\n        except ImportError:\n            # IPython not available; use regular Python REPL.\n            from code import InteractiveConsole\n            InteractiveConsole(locals=scope_vars).interact(banner, '')\n        else:\n            print(banner)\n            IPython.start_ipython(argv=[], user_ns=scope_vars)", "code_tokens": ["def", "_start_repl", "(", "api", ")", ":", "# type: (Iota) -> None", "banner", "=", "(", "'IOTA API client for {uri} ({testnet}) '", "'initialized as variable `api`.\\n'", "'Type `help(api)` for list of API commands.'", ".", "format", "(", "testnet", "=", "'testnet'", "if", "api", ".", "testnet", "else", "'mainnet'", ",", "uri", "=", "api", ".", "adapter", ".", "get_uri", "(", ")", ",", ")", ")", "scope_vars", "=", "{", "'api'", ":", "api", "}", "try", ":", "# noinspection PyUnresolvedReferences", "import", "IPython", "except", "ImportError", ":", "# IPython not available; use regular Python REPL.", "from", "code", "import", "InteractiveConsole", "InteractiveConsole", "(", "locals", "=", "scope_vars", ")", ".", "interact", "(", "banner", ",", "''", ")", "else", ":", "print", "(", "banner", ")", "IPython", ".", "start_ipython", "(", "argv", "=", "[", "]", ",", "user_ns", "=", "scope_vars", ")"], "docstring": "Starts the REPL.", "docstring_tokens": ["Starts", "the", "REPL", "."], "sha": "97cdd1e241498446b46157b79b2a1ea2ec6d387a", "url": "https://github.com/iotaledger/iota.lib.py/blob/97cdd1e241498446b46157b79b2a1ea2ec6d387a/iota/bin/repl.py#L86-L111", "partition": "test"}
{"repo": "tonyseek/flask-navigation", "path": "flask_navigation/api.py", "func_name": "Navigation.initialize_bars", "original_string": "def initialize_bars(self, sender=None, **kwargs):\n        \"\"\"Calls the initializers of all bound navigation bars.\"\"\"\n        for bar in self.bars.values():\n            for initializer in bar.initializers:\n                initializer(self)", "language": "python", "code": "def initialize_bars(self, sender=None, **kwargs):\n        \"\"\"Calls the initializers of all bound navigation bars.\"\"\"\n        for bar in self.bars.values():\n            for initializer in bar.initializers:\n                initializer(self)", "code_tokens": ["def", "initialize_bars", "(", "self", ",", "sender", "=", "None", ",", "*", "*", "kwargs", ")", ":", "for", "bar", "in", "self", ".", "bars", ".", "values", "(", ")", ":", "for", "initializer", "in", "bar", ".", "initializers", ":", "initializer", "(", "self", ")"], "docstring": "Calls the initializers of all bound navigation bars.", "docstring_tokens": ["Calls", "the", "initializers", "of", "all", "bound", "navigation", "bars", "."], "sha": "38fa83addcbe62f31516763fbe3c0bbdc793dc96", "url": "https://github.com/tonyseek/flask-navigation/blob/38fa83addcbe62f31516763fbe3c0bbdc793dc96/flask_navigation/api.py#L50-L54", "partition": "test"}
{"repo": "rupertford/melody", "path": "src/melody/inputs.py", "func_name": "Subsets._recurse", "original_string": "def _recurse(self, inputs, output, depth, max_depth):\n        '''We work out all combinations using this internal recursion method'''\n        if depth < max_depth:\n            for index, option in enumerate(inputs):\n                my_output = list(output)\n                my_output.append(option)\n                self._recurse(inputs[index + 1:], my_output, depth + 1,\n                              max_depth)\n        else:\n            self._options.append(output)", "language": "python", "code": "def _recurse(self, inputs, output, depth, max_depth):\n        '''We work out all combinations using this internal recursion method'''\n        if depth < max_depth:\n            for index, option in enumerate(inputs):\n                my_output = list(output)\n                my_output.append(option)\n                self._recurse(inputs[index + 1:], my_output, depth + 1,\n                              max_depth)\n        else:\n            self._options.append(output)", "code_tokens": ["def", "_recurse", "(", "self", ",", "inputs", ",", "output", ",", "depth", ",", "max_depth", ")", ":", "if", "depth", "<", "max_depth", ":", "for", "index", ",", "option", "in", "enumerate", "(", "inputs", ")", ":", "my_output", "=", "list", "(", "output", ")", "my_output", ".", "append", "(", "option", ")", "self", ".", "_recurse", "(", "inputs", "[", "index", "+", "1", ":", "]", ",", "my_output", ",", "depth", "+", "1", ",", "max_depth", ")", "else", ":", "self", ".", "_options", ".", "append", "(", "output", ")"], "docstring": "We work out all combinations using this internal recursion method", "docstring_tokens": ["We", "work", "out", "all", "combinations", "using", "this", "internal", "recursion", "method"], "sha": "d50459880a87fdd1802c6893f6e12b52d51b3b91", "url": "https://github.com/rupertford/melody/blob/d50459880a87fdd1802c6893f6e12b52d51b3b91/src/melody/inputs.py#L143-L152", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/sequence.py", "func_name": "transition_loop", "original_string": "def transition_loop(n_states, prob):\n    '''Construct a self-loop transition matrix over `n_states`.\n\n    The transition matrix will have the following properties:\n\n        - `transition[i, i] = p` for all i\n        - `transition[i, j] = (1 - p) / (n_states - 1)` for all `j != i`\n\n    This type of transition matrix is appropriate when states tend to be\n    locally stable, and there is no additional structure between different\n    states.  This is primarily useful for de-noising frame-wise predictions.\n\n    Parameters\n    ----------\n    n_states : int > 1\n        The number of states\n\n    prob : float in [0, 1] or iterable, length=n_states\n        If a scalar, this is the probability of a self-transition.\n\n        If a vector of length `n_states`, `p[i]` is the probability of state `i`'s self-transition.\n\n    Returns\n    -------\n    transition : np.ndarray [shape=(n_states, n_states)]\n        The transition matrix\n\n    Examples\n    --------\n    >>> librosa.sequence.transition_loop(3, 0.5)\n    array([[0.5 , 0.25, 0.25],\n           [0.25, 0.5 , 0.25],\n           [0.25, 0.25, 0.5 ]])\n\n    >>> librosa.sequence.transition_loop(3, [0.8, 0.5, 0.25])\n    array([[0.8  , 0.1  , 0.1  ],\n           [0.25 , 0.5  , 0.25 ],\n           [0.375, 0.375, 0.25 ]])\n    '''\n\n    if not isinstance(n_states, int) or n_states <= 1:\n        raise ParameterError('n_states={} must be a positive integer > 1')\n\n    transition = np.empty((n_states, n_states), dtype=np.float)\n\n    # if it's a float, make it a vector\n    prob = np.asarray(prob, dtype=np.float)\n\n    if prob.ndim == 0:\n        prob = np.tile(prob, n_states)\n\n    if prob.shape != (n_states,):\n        raise ParameterError('prob={} must have length equal to n_states={}'.format(prob, n_states))\n\n    if np.any(prob < 0) or np.any(prob > 1):\n        raise ParameterError('prob={} must have values in the range [0, 1]'.format(prob))\n\n    for i, prob_i in enumerate(prob):\n        transition[i] = (1. - prob_i) / (n_states - 1)\n        transition[i, i] = prob_i\n\n    return transition", "language": "python", "code": "def transition_loop(n_states, prob):\n    '''Construct a self-loop transition matrix over `n_states`.\n\n    The transition matrix will have the following properties:\n\n        - `transition[i, i] = p` for all i\n        - `transition[i, j] = (1 - p) / (n_states - 1)` for all `j != i`\n\n    This type of transition matrix is appropriate when states tend to be\n    locally stable, and there is no additional structure between different\n    states.  This is primarily useful for de-noising frame-wise predictions.\n\n    Parameters\n    ----------\n    n_states : int > 1\n        The number of states\n\n    prob : float in [0, 1] or iterable, length=n_states\n        If a scalar, this is the probability of a self-transition.\n\n        If a vector of length `n_states`, `p[i]` is the probability of state `i`'s self-transition.\n\n    Returns\n    -------\n    transition : np.ndarray [shape=(n_states, n_states)]\n        The transition matrix\n\n    Examples\n    --------\n    >>> librosa.sequence.transition_loop(3, 0.5)\n    array([[0.5 , 0.25, 0.25],\n           [0.25, 0.5 , 0.25],\n           [0.25, 0.25, 0.5 ]])\n\n    >>> librosa.sequence.transition_loop(3, [0.8, 0.5, 0.25])\n    array([[0.8  , 0.1  , 0.1  ],\n           [0.25 , 0.5  , 0.25 ],\n           [0.375, 0.375, 0.25 ]])\n    '''\n\n    if not isinstance(n_states, int) or n_states <= 1:\n        raise ParameterError('n_states={} must be a positive integer > 1')\n\n    transition = np.empty((n_states, n_states), dtype=np.float)\n\n    # if it's a float, make it a vector\n    prob = np.asarray(prob, dtype=np.float)\n\n    if prob.ndim == 0:\n        prob = np.tile(prob, n_states)\n\n    if prob.shape != (n_states,):\n        raise ParameterError('prob={} must have length equal to n_states={}'.format(prob, n_states))\n\n    if np.any(prob < 0) or np.any(prob > 1):\n        raise ParameterError('prob={} must have values in the range [0, 1]'.format(prob))\n\n    for i, prob_i in enumerate(prob):\n        transition[i] = (1. - prob_i) / (n_states - 1)\n        transition[i, i] = prob_i\n\n    return transition", "code_tokens": ["def", "transition_loop", "(", "n_states", ",", "prob", ")", ":", "if", "not", "isinstance", "(", "n_states", ",", "int", ")", "or", "n_states", "<=", "1", ":", "raise", "ParameterError", "(", "'n_states={} must be a positive integer > 1'", ")", "transition", "=", "np", ".", "empty", "(", "(", "n_states", ",", "n_states", ")", ",", "dtype", "=", "np", ".", "float", ")", "# if it's a float, make it a vector", "prob", "=", "np", ".", "asarray", "(", "prob", ",", "dtype", "=", "np", ".", "float", ")", "if", "prob", ".", "ndim", "==", "0", ":", "prob", "=", "np", ".", "tile", "(", "prob", ",", "n_states", ")", "if", "prob", ".", "shape", "!=", "(", "n_states", ",", ")", ":", "raise", "ParameterError", "(", "'prob={} must have length equal to n_states={}'", ".", "format", "(", "prob", ",", "n_states", ")", ")", "if", "np", ".", "any", "(", "prob", "<", "0", ")", "or", "np", ".", "any", "(", "prob", ">", "1", ")", ":", "raise", "ParameterError", "(", "'prob={} must have values in the range [0, 1]'", ".", "format", "(", "prob", ")", ")", "for", "i", ",", "prob_i", "in", "enumerate", "(", "prob", ")", ":", "transition", "[", "i", "]", "=", "(", "1.", "-", "prob_i", ")", "/", "(", "n_states", "-", "1", ")", "transition", "[", "i", ",", "i", "]", "=", "prob_i", "return", "transition"], "docstring": "Construct a self-loop transition matrix over `n_states`.\n\n    The transition matrix will have the following properties:\n\n        - `transition[i, i] = p` for all i\n        - `transition[i, j] = (1 - p) / (n_states - 1)` for all `j != i`\n\n    This type of transition matrix is appropriate when states tend to be\n    locally stable, and there is no additional structure between different\n    states.  This is primarily useful for de-noising frame-wise predictions.\n\n    Parameters\n    ----------\n    n_states : int > 1\n        The number of states\n\n    prob : float in [0, 1] or iterable, length=n_states\n        If a scalar, this is the probability of a self-transition.\n\n        If a vector of length `n_states`, `p[i]` is the probability of state `i`'s self-transition.\n\n    Returns\n    -------\n    transition : np.ndarray [shape=(n_states, n_states)]\n        The transition matrix\n\n    Examples\n    --------\n    >>> librosa.sequence.transition_loop(3, 0.5)\n    array([[0.5 , 0.25, 0.25],\n           [0.25, 0.5 , 0.25],\n           [0.25, 0.25, 0.5 ]])\n\n    >>> librosa.sequence.transition_loop(3, [0.8, 0.5, 0.25])\n    array([[0.8  , 0.1  , 0.1  ],\n           [0.25 , 0.5  , 0.25 ],\n           [0.375, 0.375, 0.25 ]])", "docstring_tokens": ["Construct", "a", "self", "-", "loop", "transition", "matrix", "over", "n_states", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/sequence.py#L897-L958", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/terminal/interactiveshell.py", "func_name": "TerminalInteractiveShell.exit", "original_string": "def exit(self):\n        \"\"\"Handle interactive exit.\n\n        This method calls the ask_exit callback.\"\"\"\n        if self.confirm_exit:\n            if self.ask_yes_no('Do you really want to exit ([y]/n)?','y'):\n                self.ask_exit()\n        else:\n            self.ask_exit()", "language": "python", "code": "def exit(self):\n        \"\"\"Handle interactive exit.\n\n        This method calls the ask_exit callback.\"\"\"\n        if self.confirm_exit:\n            if self.ask_yes_no('Do you really want to exit ([y]/n)?','y'):\n                self.ask_exit()\n        else:\n            self.ask_exit()", "code_tokens": ["def", "exit", "(", "self", ")", ":", "if", "self", ".", "confirm_exit", ":", "if", "self", ".", "ask_yes_no", "(", "'Do you really want to exit ([y]/n)?'", ",", "'y'", ")", ":", "self", ".", "ask_exit", "(", ")", "else", ":", "self", ".", "ask_exit", "(", ")"], "docstring": "Handle interactive exit.\n\n        This method calls the ask_exit callback.", "docstring_tokens": ["Handle", "interactive", "exit", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/terminal/interactiveshell.py#L703-L711", "partition": "test"}
{"repo": "trec-kba/streamcorpus-pipeline", "path": "streamcorpus_pipeline/convert_entities.py", "func_name": "html_entities_to_unicode", "original_string": "def html_entities_to_unicode(text, space_padding=False, safe_only=False):\n    '''\n    Convert any HTML, XML, or numeric entities in the attribute values.\n    For example '&amp;' becomes '&'.\n\n    This is adapted from BeautifulSoup, which should be able to do the\n    same thing when called like this --- but this fails to convert\n    everything for some bug.\n\n    text = unicode(BeautifulStoneSoup(text, convertEntities=BeautifulStoneSoup.XML_ENTITIES))\n    '''\n    def convert_entities(match):\n        '''\n        comes from BeautifulSoup.Tag._convertEntities\n        '''\n        x = match.group(1)\n\n        if safe_only and x not in ENTITIES_THAT_ARE_SAFE_TO_STRING_PAD:\n            return u'&%s;' % x\n\n        if x in name2codepoint:\n            ## handles most cases\n            return unichr(name2codepoint[x])\n\n        elif x in XML_ENTITIES_TO_SPECIAL_CHARS:\n            return XML_ENTITIES_TO_SPECIAL_CHARS[x]\n\n        elif len(x) > 0 and x[0] == '#':\n            # Handle numeric entities\n            if len(x) > 1 and x[1] == 'x':\n                return unichr(int(x[2:], 16))\n            else:\n                return unichr(int(x[1:]))\n        else:\n            ## uh oh, failed to anything\n            return u'&%s;' % x\n\n    def convert_to_padded_entitites(match):\n        converted_string = convert_entities(match)\n        num_spaces_needed = len(match.group(0)) - len(converted_string)\n        assert num_spaces_needed >= 0, \\\n            'len(%r) !<= len(%r)' % (converted_string, match.group(0))\n        ## Where to put the spaces?  Before, after, symmetric?\n        # Let's do symmetric.\n        ## cast to int in prep for python3\n        num_left = int(num_spaces_needed / 2)\n        num_right = num_spaces_needed - num_left\n        return (' ' * num_left) + converted_string + (' ' * num_right)\n\n    ## brute force regex through all the characters...\n    if space_padding:\n        return tags.sub(\n                      convert_to_padded_entitites,\n                      text)\n    else:\n        return tags.sub(\n                      convert_entities,\n                      text)", "language": "python", "code": "def html_entities_to_unicode(text, space_padding=False, safe_only=False):\n    '''\n    Convert any HTML, XML, or numeric entities in the attribute values.\n    For example '&amp;' becomes '&'.\n\n    This is adapted from BeautifulSoup, which should be able to do the\n    same thing when called like this --- but this fails to convert\n    everything for some bug.\n\n    text = unicode(BeautifulStoneSoup(text, convertEntities=BeautifulStoneSoup.XML_ENTITIES))\n    '''\n    def convert_entities(match):\n        '''\n        comes from BeautifulSoup.Tag._convertEntities\n        '''\n        x = match.group(1)\n\n        if safe_only and x not in ENTITIES_THAT_ARE_SAFE_TO_STRING_PAD:\n            return u'&%s;' % x\n\n        if x in name2codepoint:\n            ## handles most cases\n            return unichr(name2codepoint[x])\n\n        elif x in XML_ENTITIES_TO_SPECIAL_CHARS:\n            return XML_ENTITIES_TO_SPECIAL_CHARS[x]\n\n        elif len(x) > 0 and x[0] == '#':\n            # Handle numeric entities\n            if len(x) > 1 and x[1] == 'x':\n                return unichr(int(x[2:], 16))\n            else:\n                return unichr(int(x[1:]))\n        else:\n            ## uh oh, failed to anything\n            return u'&%s;' % x\n\n    def convert_to_padded_entitites(match):\n        converted_string = convert_entities(match)\n        num_spaces_needed = len(match.group(0)) - len(converted_string)\n        assert num_spaces_needed >= 0, \\\n            'len(%r) !<= len(%r)' % (converted_string, match.group(0))\n        ## Where to put the spaces?  Before, after, symmetric?\n        # Let's do symmetric.\n        ## cast to int in prep for python3\n        num_left = int(num_spaces_needed / 2)\n        num_right = num_spaces_needed - num_left\n        return (' ' * num_left) + converted_string + (' ' * num_right)\n\n    ## brute force regex through all the characters...\n    if space_padding:\n        return tags.sub(\n                      convert_to_padded_entitites,\n                      text)\n    else:\n        return tags.sub(\n                      convert_entities,\n                      text)", "code_tokens": ["def", "html_entities_to_unicode", "(", "text", ",", "space_padding", "=", "False", ",", "safe_only", "=", "False", ")", ":", "def", "convert_entities", "(", "match", ")", ":", "'''\n        comes from BeautifulSoup.Tag._convertEntities\n        '''", "x", "=", "match", ".", "group", "(", "1", ")", "if", "safe_only", "and", "x", "not", "in", "ENTITIES_THAT_ARE_SAFE_TO_STRING_PAD", ":", "return", "u'&%s;'", "%", "x", "if", "x", "in", "name2codepoint", ":", "## handles most cases", "return", "unichr", "(", "name2codepoint", "[", "x", "]", ")", "elif", "x", "in", "XML_ENTITIES_TO_SPECIAL_CHARS", ":", "return", "XML_ENTITIES_TO_SPECIAL_CHARS", "[", "x", "]", "elif", "len", "(", "x", ")", ">", "0", "and", "x", "[", "0", "]", "==", "'#'", ":", "# Handle numeric entities", "if", "len", "(", "x", ")", ">", "1", "and", "x", "[", "1", "]", "==", "'x'", ":", "return", "unichr", "(", "int", "(", "x", "[", "2", ":", "]", ",", "16", ")", ")", "else", ":", "return", "unichr", "(", "int", "(", "x", "[", "1", ":", "]", ")", ")", "else", ":", "## uh oh, failed to anything", "return", "u'&%s;'", "%", "x", "def", "convert_to_padded_entitites", "(", "match", ")", ":", "converted_string", "=", "convert_entities", "(", "match", ")", "num_spaces_needed", "=", "len", "(", "match", ".", "group", "(", "0", ")", ")", "-", "len", "(", "converted_string", ")", "assert", "num_spaces_needed", ">=", "0", ",", "'len(%r) !<= len(%r)'", "%", "(", "converted_string", ",", "match", ".", "group", "(", "0", ")", ")", "## Where to put the spaces?  Before, after, symmetric?", "# Let's do symmetric.", "## cast to int in prep for python3", "num_left", "=", "int", "(", "num_spaces_needed", "/", "2", ")", "num_right", "=", "num_spaces_needed", "-", "num_left", "return", "(", "' '", "*", "num_left", ")", "+", "converted_string", "+", "(", "' '", "*", "num_right", ")", "## brute force regex through all the characters...", "if", "space_padding", ":", "return", "tags", ".", "sub", "(", "convert_to_padded_entitites", ",", "text", ")", "else", ":", "return", "tags", ".", "sub", "(", "convert_entities", ",", "text", ")"], "docstring": "Convert any HTML, XML, or numeric entities in the attribute values.\n    For example '&amp;' becomes '&'.\n\n    This is adapted from BeautifulSoup, which should be able to do the\n    same thing when called like this --- but this fails to convert\n    everything for some bug.\n\n    text = unicode(BeautifulStoneSoup(text, convertEntities=BeautifulStoneSoup.XML_ENTITIES))", "docstring_tokens": ["Convert", "any", "HTML", "XML", "or", "numeric", "entities", "in", "the", "attribute", "values", ".", "For", "example", "&amp", ";", "becomes", "&", "."], "sha": "8bb82ea1beb83c6b40ed03fa1659df2897c2292a", "url": "https://github.com/trec-kba/streamcorpus-pipeline/blob/8bb82ea1beb83c6b40ed03fa1659df2897c2292a/streamcorpus_pipeline/convert_entities.py#L30-L87", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/parallel/client/magics.py", "func_name": "ParallelMagics._disable_autopx", "original_string": "def _disable_autopx(self):\n        \"\"\"Disable %autopx by restoring the original InteractiveShell.run_cell.\n        \"\"\"\n        if self._autopx:\n            self.shell.run_cell = self._original_run_cell\n            self._autopx = False\n            print \"%autopx disabled\"", "language": "python", "code": "def _disable_autopx(self):\n        \"\"\"Disable %autopx by restoring the original InteractiveShell.run_cell.\n        \"\"\"\n        if self._autopx:\n            self.shell.run_cell = self._original_run_cell\n            self._autopx = False\n            print \"%autopx disabled\"", "code_tokens": ["def", "_disable_autopx", "(", "self", ")", ":", "if", "self", ".", "_autopx", ":", "self", ".", "shell", ".", "run_cell", "=", "self", ".", "_original_run_cell", "self", ".", "_autopx", "=", "False", "print", "\"%autopx disabled\""], "docstring": "Disable %autopx by restoring the original InteractiveShell.run_cell.", "docstring_tokens": ["Disable", "%autopx", "by", "restoring", "the", "original", "InteractiveShell", ".", "run_cell", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/parallel/client/magics.py#L346-L352", "partition": "test"}
{"repo": "bloomreach/s4cmd", "path": "s4cmd.py", "func_name": "S3Handler.get_single_file", "original_string": "def get_single_file(self, pool, source, target):\n    '''Download a single file or a directory by adding a task into queue'''\n    if source[-1] == PATH_SEP:\n      if self.opt.recursive:\n        basepath = S3URL(source).path\n        for f in (f for f in self.s3walk(source) if not f['is_dir']):\n          pool.download(f['name'], os.path.join(target, os.path.relpath(S3URL(f['name']).path, basepath)))\n      else:\n        message('omitting directory \"%s\".' % source)\n    else:\n      pool.download(source, target)", "language": "python", "code": "def get_single_file(self, pool, source, target):\n    '''Download a single file or a directory by adding a task into queue'''\n    if source[-1] == PATH_SEP:\n      if self.opt.recursive:\n        basepath = S3URL(source).path\n        for f in (f for f in self.s3walk(source) if not f['is_dir']):\n          pool.download(f['name'], os.path.join(target, os.path.relpath(S3URL(f['name']).path, basepath)))\n      else:\n        message('omitting directory \"%s\".' % source)\n    else:\n      pool.download(source, target)", "code_tokens": ["def", "get_single_file", "(", "self", ",", "pool", ",", "source", ",", "target", ")", ":", "if", "source", "[", "-", "1", "]", "==", "PATH_SEP", ":", "if", "self", ".", "opt", ".", "recursive", ":", "basepath", "=", "S3URL", "(", "source", ")", ".", "path", "for", "f", "in", "(", "f", "for", "f", "in", "self", ".", "s3walk", "(", "source", ")", "if", "not", "f", "[", "'is_dir'", "]", ")", ":", "pool", ".", "download", "(", "f", "[", "'name'", "]", ",", "os", ".", "path", ".", "join", "(", "target", ",", "os", ".", "path", ".", "relpath", "(", "S3URL", "(", "f", "[", "'name'", "]", ")", ".", "path", ",", "basepath", ")", ")", ")", "else", ":", "message", "(", "'omitting directory \"%s\".'", "%", "source", ")", "else", ":", "pool", ".", "download", "(", "source", ",", "target", ")"], "docstring": "Download a single file or a directory by adding a task into queue", "docstring_tokens": ["Download", "a", "single", "file", "or", "a", "directory", "by", "adding", "a", "task", "into", "queue"], "sha": "bb51075bf43703e7cd95aa39288cf7732ec13a6d", "url": "https://github.com/bloomreach/s4cmd/blob/bb51075bf43703e7cd95aa39288cf7732ec13a6d/s4cmd.py#L854-L864", "partition": "test"}
{"repo": "kgiusti/pyngus", "path": "examples/recv.py", "func_name": "ReceiverEventHandler.receiver_failed", "original_string": "def receiver_failed(self, receiver_link, error):\n        \"\"\"Protocol error occurred.\"\"\"\n        LOG.warn(\"receiver_failed error=%s\", error)\n        receiver_link.close()\n        self.done = True", "language": "python", "code": "def receiver_failed(self, receiver_link, error):\n        \"\"\"Protocol error occurred.\"\"\"\n        LOG.warn(\"receiver_failed error=%s\", error)\n        receiver_link.close()\n        self.done = True", "code_tokens": ["def", "receiver_failed", "(", "self", ",", "receiver_link", ",", "error", ")", ":", "LOG", ".", "warn", "(", "\"receiver_failed error=%s\"", ",", "error", ")", "receiver_link", ".", "close", "(", ")", "self", ".", "done", "=", "True"], "docstring": "Protocol error occurred.", "docstring_tokens": ["Protocol", "error", "occurred", "."], "sha": "5392392046989f1bb84ba938c30e4d48311075f1", "url": "https://github.com/kgiusti/pyngus/blob/5392392046989f1bb84ba938c30e4d48311075f1/examples/recv.py#L60-L64", "partition": "test"}
{"repo": "pyca/pyopenssl", "path": "src/OpenSSL/_util.py", "func_name": "exception_from_error_queue", "original_string": "def exception_from_error_queue(exception_type):\n    \"\"\"\n    Convert an OpenSSL library failure into a Python exception.\n\n    When a call to the native OpenSSL library fails, this is usually signalled\n    by the return value, and an error code is stored in an error queue\n    associated with the current thread. The err library provides functions to\n    obtain these error codes and textual error messages.\n    \"\"\"\n    errors = []\n\n    while True:\n        error = lib.ERR_get_error()\n        if error == 0:\n            break\n        errors.append((\n            text(lib.ERR_lib_error_string(error)),\n            text(lib.ERR_func_error_string(error)),\n            text(lib.ERR_reason_error_string(error))))\n\n    raise exception_type(errors)", "language": "python", "code": "def exception_from_error_queue(exception_type):\n    \"\"\"\n    Convert an OpenSSL library failure into a Python exception.\n\n    When a call to the native OpenSSL library fails, this is usually signalled\n    by the return value, and an error code is stored in an error queue\n    associated with the current thread. The err library provides functions to\n    obtain these error codes and textual error messages.\n    \"\"\"\n    errors = []\n\n    while True:\n        error = lib.ERR_get_error()\n        if error == 0:\n            break\n        errors.append((\n            text(lib.ERR_lib_error_string(error)),\n            text(lib.ERR_func_error_string(error)),\n            text(lib.ERR_reason_error_string(error))))\n\n    raise exception_type(errors)", "code_tokens": ["def", "exception_from_error_queue", "(", "exception_type", ")", ":", "errors", "=", "[", "]", "while", "True", ":", "error", "=", "lib", ".", "ERR_get_error", "(", ")", "if", "error", "==", "0", ":", "break", "errors", ".", "append", "(", "(", "text", "(", "lib", ".", "ERR_lib_error_string", "(", "error", ")", ")", ",", "text", "(", "lib", ".", "ERR_func_error_string", "(", "error", ")", ")", ",", "text", "(", "lib", ".", "ERR_reason_error_string", "(", "error", ")", ")", ")", ")", "raise", "exception_type", "(", "errors", ")"], "docstring": "Convert an OpenSSL library failure into a Python exception.\n\n    When a call to the native OpenSSL library fails, this is usually signalled\n    by the return value, and an error code is stored in an error queue\n    associated with the current thread. The err library provides functions to\n    obtain these error codes and textual error messages.", "docstring_tokens": ["Convert", "an", "OpenSSL", "library", "failure", "into", "a", "Python", "exception", "."], "sha": "1fbe064c50fd030948141d7d630673761525b0d0", "url": "https://github.com/pyca/pyopenssl/blob/1fbe064c50fd030948141d7d630673761525b0d0/src/OpenSSL/_util.py#L34-L54", "partition": "test"}
{"repo": "appknox/google-chartwrapper", "path": "GChartWrapper/GChart.py", "func_name": "GChart.dataset", "original_string": "def dataset(self, data, series=''):\n        \"\"\"\n        Update the chart's dataset, can be two dimensional or contain string data\n        \"\"\"\n        self._dataset = data\n        self._series = series\n        return self", "language": "python", "code": "def dataset(self, data, series=''):\n        \"\"\"\n        Update the chart's dataset, can be two dimensional or contain string data\n        \"\"\"\n        self._dataset = data\n        self._series = series\n        return self", "code_tokens": ["def", "dataset", "(", "self", ",", "data", ",", "series", "=", "''", ")", ":", "self", ".", "_dataset", "=", "data", "self", ".", "_series", "=", "series", "return", "self"], "docstring": "Update the chart's dataset, can be two dimensional or contain string data", "docstring_tokens": ["Update", "the", "chart", "s", "dataset", "can", "be", "two", "dimensional", "or", "contain", "string", "data"], "sha": "3769aecbef6c83b6cd93ee72ece478ffe433ac57", "url": "https://github.com/appknox/google-chartwrapper/blob/3769aecbef6c83b6cd93ee72ece478ffe433ac57/GChartWrapper/GChart.py#L285-L291", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/message/message.py", "func_name": "Message.format", "original_string": "def format(self, template):\n        \"\"\"Format the message according to the given template.\n\n        The template format is the one of the format method :\n        cf. http://docs.python.org/2/library/string.html#formatstrings\n        \"\"\"\n        # For some reason, _asdict on derived namedtuples does not work with\n        # Python 3.4. Needs some investigation.\n        return template.format(**dict(zip(self._fields, self)))", "language": "python", "code": "def format(self, template):\n        \"\"\"Format the message according to the given template.\n\n        The template format is the one of the format method :\n        cf. http://docs.python.org/2/library/string.html#formatstrings\n        \"\"\"\n        # For some reason, _asdict on derived namedtuples does not work with\n        # Python 3.4. Needs some investigation.\n        return template.format(**dict(zip(self._fields, self)))", "code_tokens": ["def", "format", "(", "self", ",", "template", ")", ":", "# For some reason, _asdict on derived namedtuples does not work with", "# Python 3.4. Needs some investigation.", "return", "template", ".", "format", "(", "*", "*", "dict", "(", "zip", "(", "self", ".", "_fields", ",", "self", ")", ")", ")"], "docstring": "Format the message according to the given template.\n\n        The template format is the one of the format method :\n        cf. http://docs.python.org/2/library/string.html#formatstrings", "docstring_tokens": ["Format", "the", "message", "according", "to", "the", "given", "template", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/message/message.py#L45-L53", "partition": "test"}
{"repo": "jazzband/django-ddp", "path": "dddp/views.py", "func_name": "dict_merge", "original_string": "def dict_merge(lft, rgt):\n    \"\"\"\n    Recursive dict merge.\n\n    Recursively merges dict's. not just simple lft['key'] = rgt['key'], if\n    both lft and rgt have a key who's value is a dict then dict_merge is\n    called on both values and the result stored in the returned dictionary.\n    \"\"\"\n    if not isinstance(rgt, dict):\n        return rgt\n    result = deepcopy(lft)\n    for key, val in rgt.iteritems():\n        if key in result and isinstance(result[key], dict):\n            result[key] = dict_merge(result[key], val)\n        else:\n            result[key] = deepcopy(val)\n    return result", "language": "python", "code": "def dict_merge(lft, rgt):\n    \"\"\"\n    Recursive dict merge.\n\n    Recursively merges dict's. not just simple lft['key'] = rgt['key'], if\n    both lft and rgt have a key who's value is a dict then dict_merge is\n    called on both values and the result stored in the returned dictionary.\n    \"\"\"\n    if not isinstance(rgt, dict):\n        return rgt\n    result = deepcopy(lft)\n    for key, val in rgt.iteritems():\n        if key in result and isinstance(result[key], dict):\n            result[key] = dict_merge(result[key], val)\n        else:\n            result[key] = deepcopy(val)\n    return result", "code_tokens": ["def", "dict_merge", "(", "lft", ",", "rgt", ")", ":", "if", "not", "isinstance", "(", "rgt", ",", "dict", ")", ":", "return", "rgt", "result", "=", "deepcopy", "(", "lft", ")", "for", "key", ",", "val", "in", "rgt", ".", "iteritems", "(", ")", ":", "if", "key", "in", "result", "and", "isinstance", "(", "result", "[", "key", "]", ",", "dict", ")", ":", "result", "[", "key", "]", "=", "dict_merge", "(", "result", "[", "key", "]", ",", "val", ")", "else", ":", "result", "[", "key", "]", "=", "deepcopy", "(", "val", ")", "return", "result"], "docstring": "Recursive dict merge.\n\n    Recursively merges dict's. not just simple lft['key'] = rgt['key'], if\n    both lft and rgt have a key who's value is a dict then dict_merge is\n    called on both values and the result stored in the returned dictionary.", "docstring_tokens": ["Recursive", "dict", "merge", "."], "sha": "1e1954b06fe140346acea43582515991685e4e01", "url": "https://github.com/jazzband/django-ddp/blob/1e1954b06fe140346acea43582515991685e4e01/dddp/views.py#L18-L34", "partition": "test"}
{"repo": "amelchio/eternalegypt", "path": "eternalegypt/eternalegypt.py", "func_name": "autologin", "original_string": "def autologin(function, timeout=TIMEOUT):\n    \"\"\"Decorator that will try to login and redo an action before failing.\"\"\"\n    @wraps(function)\n    async def wrapper(self, *args, **kwargs):\n        \"\"\"Wrap a function with timeout.\"\"\"\n        try:\n            async with async_timeout.timeout(timeout):\n                return await function(self, *args, **kwargs)\n        except (asyncio.TimeoutError, ClientError, Error):\n            pass\n\n        _LOGGER.debug(\"autologin\")\n        try:\n            async with async_timeout.timeout(timeout):\n                await self.login()\n                return await function(self, *args, **kwargs)\n        except (asyncio.TimeoutError, ClientError, Error):\n            raise Error(str(function))\n\n    return wrapper", "language": "python", "code": "def autologin(function, timeout=TIMEOUT):\n    \"\"\"Decorator that will try to login and redo an action before failing.\"\"\"\n    @wraps(function)\n    async def wrapper(self, *args, **kwargs):\n        \"\"\"Wrap a function with timeout.\"\"\"\n        try:\n            async with async_timeout.timeout(timeout):\n                return await function(self, *args, **kwargs)\n        except (asyncio.TimeoutError, ClientError, Error):\n            pass\n\n        _LOGGER.debug(\"autologin\")\n        try:\n            async with async_timeout.timeout(timeout):\n                await self.login()\n                return await function(self, *args, **kwargs)\n        except (asyncio.TimeoutError, ClientError, Error):\n            raise Error(str(function))\n\n    return wrapper", "code_tokens": ["def", "autologin", "(", "function", ",", "timeout", "=", "TIMEOUT", ")", ":", "@", "wraps", "(", "function", ")", "async", "def", "wrapper", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "\"\"\"Wrap a function with timeout.\"\"\"", "try", ":", "async", "with", "async_timeout", ".", "timeout", "(", "timeout", ")", ":", "return", "await", "function", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", "except", "(", "asyncio", ".", "TimeoutError", ",", "ClientError", ",", "Error", ")", ":", "pass", "_LOGGER", ".", "debug", "(", "\"autologin\"", ")", "try", ":", "async", "with", "async_timeout", ".", "timeout", "(", "timeout", ")", ":", "await", "self", ".", "login", "(", ")", "return", "await", "function", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", "except", "(", "asyncio", ".", "TimeoutError", ",", "ClientError", ",", "Error", ")", ":", "raise", "Error", "(", "str", "(", "function", ")", ")", "return", "wrapper"], "docstring": "Decorator that will try to login and redo an action before failing.", "docstring_tokens": ["Decorator", "that", "will", "try", "to", "login", "and", "redo", "an", "action", "before", "failing", "."], "sha": "895e0b235ceaf7f61458c620237c3ad397780e98", "url": "https://github.com/amelchio/eternalegypt/blob/895e0b235ceaf7f61458c620237c3ad397780e98/eternalegypt/eternalegypt.py#L52-L71", "partition": "test"}
{"repo": "BD2KGenomics/toil-lib", "path": "src/toil_lib/validators.py", "func_name": "bam_quickcheck", "original_string": "def bam_quickcheck(bam_path):\n    \"\"\"\n    Perform a quick check on a BAM via `samtools quickcheck`.\n    This will detect obvious BAM errors such as truncation.\n\n    :param str bam_path: path to BAM file to checked\n\n    :rtype: boolean\n    :return: True if the BAM is valid, False is BAM is invalid or something related to the call went wrong\n    \"\"\"\n    directory, bam_name = os.path.split(bam_path)\n    exit_code = subprocess.call(['docker', 'run', '-v', directory + ':/data',\n                                 'quay.io/ucsc_cgl/samtools:1.3--256539928ea162949d8a65ca5c79a72ef557ce7c',\n                                 'quickcheck', '-vv', '/data/' + bam_name])\n    if exit_code != 0:\n        return False\n    return True", "language": "python", "code": "def bam_quickcheck(bam_path):\n    \"\"\"\n    Perform a quick check on a BAM via `samtools quickcheck`.\n    This will detect obvious BAM errors such as truncation.\n\n    :param str bam_path: path to BAM file to checked\n\n    :rtype: boolean\n    :return: True if the BAM is valid, False is BAM is invalid or something related to the call went wrong\n    \"\"\"\n    directory, bam_name = os.path.split(bam_path)\n    exit_code = subprocess.call(['docker', 'run', '-v', directory + ':/data',\n                                 'quay.io/ucsc_cgl/samtools:1.3--256539928ea162949d8a65ca5c79a72ef557ce7c',\n                                 'quickcheck', '-vv', '/data/' + bam_name])\n    if exit_code != 0:\n        return False\n    return True", "code_tokens": ["def", "bam_quickcheck", "(", "bam_path", ")", ":", "directory", ",", "bam_name", "=", "os", ".", "path", ".", "split", "(", "bam_path", ")", "exit_code", "=", "subprocess", ".", "call", "(", "[", "'docker'", ",", "'run'", ",", "'-v'", ",", "directory", "+", "':/data'", ",", "'quay.io/ucsc_cgl/samtools:1.3--256539928ea162949d8a65ca5c79a72ef557ce7c'", ",", "'quickcheck'", ",", "'-vv'", ",", "'/data/'", "+", "bam_name", "]", ")", "if", "exit_code", "!=", "0", ":", "return", "False", "return", "True"], "docstring": "Perform a quick check on a BAM via `samtools quickcheck`.\n    This will detect obvious BAM errors such as truncation.\n\n    :param str bam_path: path to BAM file to checked\n\n    :rtype: boolean\n    :return: True if the BAM is valid, False is BAM is invalid or something related to the call went wrong", "docstring_tokens": ["Perform", "a", "quick", "check", "on", "a", "BAM", "via", "samtools", "quickcheck", ".", "This", "will", "detect", "obvious", "BAM", "errors", "such", "as", "truncation", "."], "sha": "022a615fc3dc98fc1aaa7bfd232409962ca44fbd", "url": "https://github.com/BD2KGenomics/toil-lib/blob/022a615fc3dc98fc1aaa7bfd232409962ca44fbd/src/toil_lib/validators.py#L8-L24", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/message/message_definition.py", "func_name": "MessageDefinition.may_be_emitted", "original_string": "def may_be_emitted(self):\n        \"\"\"return True if message may be emitted using the current interpreter\"\"\"\n        if self.minversion is not None and self.minversion > sys.version_info:\n            return False\n        if self.maxversion is not None and self.maxversion <= sys.version_info:\n            return False\n        return True", "language": "python", "code": "def may_be_emitted(self):\n        \"\"\"return True if message may be emitted using the current interpreter\"\"\"\n        if self.minversion is not None and self.minversion > sys.version_info:\n            return False\n        if self.maxversion is not None and self.maxversion <= sys.version_info:\n            return False\n        return True", "code_tokens": ["def", "may_be_emitted", "(", "self", ")", ":", "if", "self", ".", "minversion", "is", "not", "None", "and", "self", ".", "minversion", ">", "sys", ".", "version_info", ":", "return", "False", "if", "self", ".", "maxversion", "is", "not", "None", "and", "self", ".", "maxversion", "<=", "sys", ".", "version_info", ":", "return", "False", "return", "True"], "docstring": "return True if message may be emitted using the current interpreter", "docstring_tokens": ["return", "True", "if", "message", "may", "be", "emitted", "using", "the", "current", "interpreter"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/message/message_definition.py#L43-L49", "partition": "test"}
{"repo": "mental32/spotify.py", "path": "spotify/http.py", "func_name": "HTTPClient.categories", "original_string": "def categories(self, limit=20, offset=0, country=None, locale=None):\n        \"\"\"Get a list of categories used to tag items in Spotify.\n\n        Parameters\n        ----------\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        offset : Optional[int]\n            The index of the first item to return. Default: 0\n        country : COUNTRY_TP\n            COUNTRY\n        locale : LOCALE_TP\n            LOCALE\n        \"\"\"\n        route = Route('GET', '/browse/categories')\n        payload = {'limit': limit, 'offset': offset}\n\n        if country:\n            payload['country'] = country\n\n        if locale:\n            payload['locale'] = locale\n\n        return self.request(route, params=payload)", "language": "python", "code": "def categories(self, limit=20, offset=0, country=None, locale=None):\n        \"\"\"Get a list of categories used to tag items in Spotify.\n\n        Parameters\n        ----------\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        offset : Optional[int]\n            The index of the first item to return. Default: 0\n        country : COUNTRY_TP\n            COUNTRY\n        locale : LOCALE_TP\n            LOCALE\n        \"\"\"\n        route = Route('GET', '/browse/categories')\n        payload = {'limit': limit, 'offset': offset}\n\n        if country:\n            payload['country'] = country\n\n        if locale:\n            payload['locale'] = locale\n\n        return self.request(route, params=payload)", "code_tokens": ["def", "categories", "(", "self", ",", "limit", "=", "20", ",", "offset", "=", "0", ",", "country", "=", "None", ",", "locale", "=", "None", ")", ":", "route", "=", "Route", "(", "'GET'", ",", "'/browse/categories'", ")", "payload", "=", "{", "'limit'", ":", "limit", ",", "'offset'", ":", "offset", "}", "if", "country", ":", "payload", "[", "'country'", "]", "=", "country", "if", "locale", ":", "payload", "[", "'locale'", "]", "=", "locale", "return", "self", ".", "request", "(", "route", ",", "params", "=", "payload", ")"], "docstring": "Get a list of categories used to tag items in Spotify.\n\n        Parameters\n        ----------\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        offset : Optional[int]\n            The index of the first item to return. Default: 0\n        country : COUNTRY_TP\n            COUNTRY\n        locale : LOCALE_TP\n            LOCALE", "docstring_tokens": ["Get", "a", "list", "of", "categories", "used", "to", "tag", "items", "in", "Spotify", "."], "sha": "bb296cac7c3dd289908906b7069bd80f43950515", "url": "https://github.com/mental32/spotify.py/blob/bb296cac7c3dd289908906b7069bd80f43950515/spotify/http.py#L354-L377", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/pip/index.py", "func_name": "PackageFinder.find_requirement", "original_string": "def find_requirement(self, req, upgrade):\n        \"\"\"Try to find an InstallationCandidate for req\n\n        Expects req, an InstallRequirement and upgrade, a boolean\n        Returns an InstallationCandidate or None\n        May raise DistributionNotFound or BestVersionAlreadyInstalled\n        \"\"\"\n        all_versions = self._find_all_versions(req.name)\n        # Filter out anything which doesn't match our specifier\n\n        _versions = set(\n            req.specifier.filter(\n                [x.version for x in all_versions],\n                prereleases=(\n                    self.allow_all_prereleases\n                    if self.allow_all_prereleases else None\n                ),\n            )\n        )\n        applicable_versions = [\n            x for x in all_versions if x.version in _versions\n        ]\n\n        if req.satisfied_by is not None:\n            # Finally add our existing versions to the front of our versions.\n            applicable_versions.insert(\n                0,\n                InstallationCandidate(\n                    req.name,\n                    req.satisfied_by.version,\n                    INSTALLED_VERSION,\n                )\n            )\n            existing_applicable = True\n        else:\n            existing_applicable = False\n\n        applicable_versions = self._sort_versions(applicable_versions)\n\n        if not upgrade and existing_applicable:\n            if applicable_versions[0].location is INSTALLED_VERSION:\n                logger.debug(\n                    'Existing installed version (%s) is most up-to-date and '\n                    'satisfies requirement',\n                    req.satisfied_by.version,\n                )\n            else:\n                logger.debug(\n                    'Existing installed version (%s) satisfies requirement '\n                    '(most up-to-date version is %s)',\n                    req.satisfied_by.version,\n                    applicable_versions[0][2],\n                )\n            return None\n\n        if not applicable_versions:\n            logger.critical(\n                'Could not find a version that satisfies the requirement %s '\n                '(from versions: %s)',\n                req,\n                ', '.join(\n                    sorted(\n                        set(str(i.version) for i in all_versions),\n                        key=parse_version,\n                    )\n                )\n            )\n\n            if self.need_warn_external:\n                logger.warning(\n                    \"Some externally hosted files were ignored as access to \"\n                    \"them may be unreliable (use --allow-external %s to \"\n                    \"allow).\",\n                    req.name,\n                )\n\n            if self.need_warn_unverified:\n                logger.warning(\n                    \"Some insecure and unverifiable files were ignored\"\n                    \" (use --allow-unverified %s to allow).\",\n                    req.name,\n                )\n\n            raise DistributionNotFound(\n                'No matching distribution found for %s' % req\n            )\n\n        if applicable_versions[0].location is INSTALLED_VERSION:\n            # We have an existing version, and its the best version\n            logger.debug(\n                'Installed version (%s) is most up-to-date (past versions: '\n                '%s)',\n                req.satisfied_by.version,\n                ', '.join(str(i.version) for i in applicable_versions[1:]) or\n                \"none\",\n            )\n            raise BestVersionAlreadyInstalled\n\n        if len(applicable_versions) > 1:\n            logger.debug(\n                'Using version %s (newest of versions: %s)',\n                applicable_versions[0].version,\n                ', '.join(str(i.version) for i in applicable_versions)\n            )\n\n        selected_version = applicable_versions[0].location\n\n        if (selected_version.verifiable is not None and not\n                selected_version.verifiable):\n            logger.warning(\n                \"%s is potentially insecure and unverifiable.\", req.name,\n            )\n\n        return selected_version", "language": "python", "code": "def find_requirement(self, req, upgrade):\n        \"\"\"Try to find an InstallationCandidate for req\n\n        Expects req, an InstallRequirement and upgrade, a boolean\n        Returns an InstallationCandidate or None\n        May raise DistributionNotFound or BestVersionAlreadyInstalled\n        \"\"\"\n        all_versions = self._find_all_versions(req.name)\n        # Filter out anything which doesn't match our specifier\n\n        _versions = set(\n            req.specifier.filter(\n                [x.version for x in all_versions],\n                prereleases=(\n                    self.allow_all_prereleases\n                    if self.allow_all_prereleases else None\n                ),\n            )\n        )\n        applicable_versions = [\n            x for x in all_versions if x.version in _versions\n        ]\n\n        if req.satisfied_by is not None:\n            # Finally add our existing versions to the front of our versions.\n            applicable_versions.insert(\n                0,\n                InstallationCandidate(\n                    req.name,\n                    req.satisfied_by.version,\n                    INSTALLED_VERSION,\n                )\n            )\n            existing_applicable = True\n        else:\n            existing_applicable = False\n\n        applicable_versions = self._sort_versions(applicable_versions)\n\n        if not upgrade and existing_applicable:\n            if applicable_versions[0].location is INSTALLED_VERSION:\n                logger.debug(\n                    'Existing installed version (%s) is most up-to-date and '\n                    'satisfies requirement',\n                    req.satisfied_by.version,\n                )\n            else:\n                logger.debug(\n                    'Existing installed version (%s) satisfies requirement '\n                    '(most up-to-date version is %s)',\n                    req.satisfied_by.version,\n                    applicable_versions[0][2],\n                )\n            return None\n\n        if not applicable_versions:\n            logger.critical(\n                'Could not find a version that satisfies the requirement %s '\n                '(from versions: %s)',\n                req,\n                ', '.join(\n                    sorted(\n                        set(str(i.version) for i in all_versions),\n                        key=parse_version,\n                    )\n                )\n            )\n\n            if self.need_warn_external:\n                logger.warning(\n                    \"Some externally hosted files were ignored as access to \"\n                    \"them may be unreliable (use --allow-external %s to \"\n                    \"allow).\",\n                    req.name,\n                )\n\n            if self.need_warn_unverified:\n                logger.warning(\n                    \"Some insecure and unverifiable files were ignored\"\n                    \" (use --allow-unverified %s to allow).\",\n                    req.name,\n                )\n\n            raise DistributionNotFound(\n                'No matching distribution found for %s' % req\n            )\n\n        if applicable_versions[0].location is INSTALLED_VERSION:\n            # We have an existing version, and its the best version\n            logger.debug(\n                'Installed version (%s) is most up-to-date (past versions: '\n                '%s)',\n                req.satisfied_by.version,\n                ', '.join(str(i.version) for i in applicable_versions[1:]) or\n                \"none\",\n            )\n            raise BestVersionAlreadyInstalled\n\n        if len(applicable_versions) > 1:\n            logger.debug(\n                'Using version %s (newest of versions: %s)',\n                applicable_versions[0].version,\n                ', '.join(str(i.version) for i in applicable_versions)\n            )\n\n        selected_version = applicable_versions[0].location\n\n        if (selected_version.verifiable is not None and not\n                selected_version.verifiable):\n            logger.warning(\n                \"%s is potentially insecure and unverifiable.\", req.name,\n            )\n\n        return selected_version", "code_tokens": ["def", "find_requirement", "(", "self", ",", "req", ",", "upgrade", ")", ":", "all_versions", "=", "self", ".", "_find_all_versions", "(", "req", ".", "name", ")", "# Filter out anything which doesn't match our specifier", "_versions", "=", "set", "(", "req", ".", "specifier", ".", "filter", "(", "[", "x", ".", "version", "for", "x", "in", "all_versions", "]", ",", "prereleases", "=", "(", "self", ".", "allow_all_prereleases", "if", "self", ".", "allow_all_prereleases", "else", "None", ")", ",", ")", ")", "applicable_versions", "=", "[", "x", "for", "x", "in", "all_versions", "if", "x", ".", "version", "in", "_versions", "]", "if", "req", ".", "satisfied_by", "is", "not", "None", ":", "# Finally add our existing versions to the front of our versions.", "applicable_versions", ".", "insert", "(", "0", ",", "InstallationCandidate", "(", "req", ".", "name", ",", "req", ".", "satisfied_by", ".", "version", ",", "INSTALLED_VERSION", ",", ")", ")", "existing_applicable", "=", "True", "else", ":", "existing_applicable", "=", "False", "applicable_versions", "=", "self", ".", "_sort_versions", "(", "applicable_versions", ")", "if", "not", "upgrade", "and", "existing_applicable", ":", "if", "applicable_versions", "[", "0", "]", ".", "location", "is", "INSTALLED_VERSION", ":", "logger", ".", "debug", "(", "'Existing installed version (%s) is most up-to-date and '", "'satisfies requirement'", ",", "req", ".", "satisfied_by", ".", "version", ",", ")", "else", ":", "logger", ".", "debug", "(", "'Existing installed version (%s) satisfies requirement '", "'(most up-to-date version is %s)'", ",", "req", ".", "satisfied_by", ".", "version", ",", "applicable_versions", "[", "0", "]", "[", "2", "]", ",", ")", "return", "None", "if", "not", "applicable_versions", ":", "logger", ".", "critical", "(", "'Could not find a version that satisfies the requirement %s '", "'(from versions: %s)'", ",", "req", ",", "', '", ".", "join", "(", "sorted", "(", "set", "(", "str", "(", "i", ".", "version", ")", "for", "i", "in", "all_versions", ")", ",", "key", "=", "parse_version", ",", ")", ")", ")", "if", "self", ".", "need_warn_external", ":", "logger", ".", "warning", "(", "\"Some externally hosted files were ignored as access to \"", "\"them may be unreliable (use --allow-external %s to \"", "\"allow).\"", ",", "req", ".", "name", ",", ")", "if", "self", ".", "need_warn_unverified", ":", "logger", ".", "warning", "(", "\"Some insecure and unverifiable files were ignored\"", "\" (use --allow-unverified %s to allow).\"", ",", "req", ".", "name", ",", ")", "raise", "DistributionNotFound", "(", "'No matching distribution found for %s'", "%", "req", ")", "if", "applicable_versions", "[", "0", "]", ".", "location", "is", "INSTALLED_VERSION", ":", "# We have an existing version, and its the best version", "logger", ".", "debug", "(", "'Installed version (%s) is most up-to-date (past versions: '", "'%s)'", ",", "req", ".", "satisfied_by", ".", "version", ",", "', '", ".", "join", "(", "str", "(", "i", ".", "version", ")", "for", "i", "in", "applicable_versions", "[", "1", ":", "]", ")", "or", "\"none\"", ",", ")", "raise", "BestVersionAlreadyInstalled", "if", "len", "(", "applicable_versions", ")", ">", "1", ":", "logger", ".", "debug", "(", "'Using version %s (newest of versions: %s)'", ",", "applicable_versions", "[", "0", "]", ".", "version", ",", "', '", ".", "join", "(", "str", "(", "i", ".", "version", ")", "for", "i", "in", "applicable_versions", ")", ")", "selected_version", "=", "applicable_versions", "[", "0", "]", ".", "location", "if", "(", "selected_version", ".", "verifiable", "is", "not", "None", "and", "not", "selected_version", ".", "verifiable", ")", ":", "logger", ".", "warning", "(", "\"%s is potentially insecure and unverifiable.\"", ",", "req", ".", "name", ",", ")", "return", "selected_version"], "docstring": "Try to find an InstallationCandidate for req\n\n        Expects req, an InstallRequirement and upgrade, a boolean\n        Returns an InstallationCandidate or None\n        May raise DistributionNotFound or BestVersionAlreadyInstalled", "docstring_tokens": ["Try", "to", "find", "an", "InstallationCandidate", "for", "req"], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/pip/index.py#L479-L592", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/phabricator.py", "func_name": "ConduitClient._call", "original_string": "def _call(self, method, params):\n        \"\"\"Call a method.\n\n        :param method: method to call\n        :param params: dict with the HTTP parameters needed to call\n            the given method\n\n        :raises ConduitError: when an error is returned by the server\n        \"\"\"\n        url = self.URL % {'base': self.base_url, 'method': method}\n\n        # Conduit and POST parameters\n        params['__conduit__'] = {'token': self.api_token}\n\n        data = {\n            'params': json.dumps(params, sort_keys=True),\n            'output': 'json',\n            '__conduit__': True\n        }\n\n        logger.debug(\"Phabricator Conduit client requests: %s params: %s\",\n                     method, str(data))\n\n        r = self.fetch(url, payload=data, method=HttpClient.POST, verify=False)\n\n        # Check for possible Conduit API errors\n        result = r.json()\n\n        if result['error_code']:\n            raise ConduitError(error=result['error_info'],\n                               code=result['error_code'])\n\n        return r.text", "language": "python", "code": "def _call(self, method, params):\n        \"\"\"Call a method.\n\n        :param method: method to call\n        :param params: dict with the HTTP parameters needed to call\n            the given method\n\n        :raises ConduitError: when an error is returned by the server\n        \"\"\"\n        url = self.URL % {'base': self.base_url, 'method': method}\n\n        # Conduit and POST parameters\n        params['__conduit__'] = {'token': self.api_token}\n\n        data = {\n            'params': json.dumps(params, sort_keys=True),\n            'output': 'json',\n            '__conduit__': True\n        }\n\n        logger.debug(\"Phabricator Conduit client requests: %s params: %s\",\n                     method, str(data))\n\n        r = self.fetch(url, payload=data, method=HttpClient.POST, verify=False)\n\n        # Check for possible Conduit API errors\n        result = r.json()\n\n        if result['error_code']:\n            raise ConduitError(error=result['error_info'],\n                               code=result['error_code'])\n\n        return r.text", "code_tokens": ["def", "_call", "(", "self", ",", "method", ",", "params", ")", ":", "url", "=", "self", ".", "URL", "%", "{", "'base'", ":", "self", ".", "base_url", ",", "'method'", ":", "method", "}", "# Conduit and POST parameters", "params", "[", "'__conduit__'", "]", "=", "{", "'token'", ":", "self", ".", "api_token", "}", "data", "=", "{", "'params'", ":", "json", ".", "dumps", "(", "params", ",", "sort_keys", "=", "True", ")", ",", "'output'", ":", "'json'", ",", "'__conduit__'", ":", "True", "}", "logger", ".", "debug", "(", "\"Phabricator Conduit client requests: %s params: %s\"", ",", "method", ",", "str", "(", "data", ")", ")", "r", "=", "self", ".", "fetch", "(", "url", ",", "payload", "=", "data", ",", "method", "=", "HttpClient", ".", "POST", ",", "verify", "=", "False", ")", "# Check for possible Conduit API errors", "result", "=", "r", ".", "json", "(", ")", "if", "result", "[", "'error_code'", "]", ":", "raise", "ConduitError", "(", "error", "=", "result", "[", "'error_info'", "]", ",", "code", "=", "result", "[", "'error_code'", "]", ")", "return", "r", ".", "text"], "docstring": "Call a method.\n\n        :param method: method to call\n        :param params: dict with the HTTP parameters needed to call\n            the given method\n\n        :raises ConduitError: when an error is returned by the server", "docstring_tokens": ["Call", "a", "method", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/phabricator.py#L575-L607", "partition": "test"}
{"repo": "intel-analytics/BigDL", "path": "pyspark/bigdl/models/lenet/utils.py", "func_name": "get_mnist", "original_string": "def get_mnist(sc, data_type=\"train\", location=\"/tmp/mnist\"):\n    \"\"\"\n    Get mnist dataset and parallelize into RDDs.\n    Data would be downloaded automatically if it doesn't present at the specific location.\n\n    :param sc: SparkContext.\n    :param data_type: \"train\" for training data and \"test\" for testing data.\n    :param location: Location to store mnist dataset.\n    :return: RDD of (features: ndarray, label: ndarray).\n    \"\"\"\n    (images, labels) = mnist.read_data_sets(location, data_type)\n    images = sc.parallelize(images)\n    labels = sc.parallelize(labels + 1)  # Target start from 1 in BigDL\n    record = images.zip(labels)\n    return record", "language": "python", "code": "def get_mnist(sc, data_type=\"train\", location=\"/tmp/mnist\"):\n    \"\"\"\n    Get mnist dataset and parallelize into RDDs.\n    Data would be downloaded automatically if it doesn't present at the specific location.\n\n    :param sc: SparkContext.\n    :param data_type: \"train\" for training data and \"test\" for testing data.\n    :param location: Location to store mnist dataset.\n    :return: RDD of (features: ndarray, label: ndarray).\n    \"\"\"\n    (images, labels) = mnist.read_data_sets(location, data_type)\n    images = sc.parallelize(images)\n    labels = sc.parallelize(labels + 1)  # Target start from 1 in BigDL\n    record = images.zip(labels)\n    return record", "code_tokens": ["def", "get_mnist", "(", "sc", ",", "data_type", "=", "\"train\"", ",", "location", "=", "\"/tmp/mnist\"", ")", ":", "(", "images", ",", "labels", ")", "=", "mnist", ".", "read_data_sets", "(", "location", ",", "data_type", ")", "images", "=", "sc", ".", "parallelize", "(", "images", ")", "labels", "=", "sc", ".", "parallelize", "(", "labels", "+", "1", ")", "# Target start from 1 in BigDL", "record", "=", "images", ".", "zip", "(", "labels", ")", "return", "record"], "docstring": "Get mnist dataset and parallelize into RDDs.\n    Data would be downloaded automatically if it doesn't present at the specific location.\n\n    :param sc: SparkContext.\n    :param data_type: \"train\" for training data and \"test\" for testing data.\n    :param location: Location to store mnist dataset.\n    :return: RDD of (features: ndarray, label: ndarray).", "docstring_tokens": ["Get", "mnist", "dataset", "and", "parallelize", "into", "RDDs", ".", "Data", "would", "be", "downloaded", "automatically", "if", "it", "doesn", "t", "present", "at", "the", "specific", "location", "."], "sha": "e9c19788285986ab789a2e2998f9a85d7524779f", "url": "https://github.com/intel-analytics/BigDL/blob/e9c19788285986ab789a2e2998f9a85d7524779f/pyspark/bigdl/models/lenet/utils.py#L22-L36", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/commands/wipe_database.py", "func_name": "wipe", "original_string": "def wipe(ctx):\n    \"\"\"Drop the mongo database given.\"\"\"\n    LOG.info(\"Running scout wipe\")\n    db_name = ctx.obj['mongodb']\n    LOG.info(\"Dropping database %s\", db_name)\n    try:\n        ctx.obj['client'].drop_database(db_name)\n    except Exception as err:\n        LOG.warning(err)\n        ctx.abort()\n    LOG.info(\"Dropped whole database\")", "language": "python", "code": "def wipe(ctx):\n    \"\"\"Drop the mongo database given.\"\"\"\n    LOG.info(\"Running scout wipe\")\n    db_name = ctx.obj['mongodb']\n    LOG.info(\"Dropping database %s\", db_name)\n    try:\n        ctx.obj['client'].drop_database(db_name)\n    except Exception as err:\n        LOG.warning(err)\n        ctx.abort()\n    LOG.info(\"Dropped whole database\")", "code_tokens": ["def", "wipe", "(", "ctx", ")", ":", "LOG", ".", "info", "(", "\"Running scout wipe\"", ")", "db_name", "=", "ctx", ".", "obj", "[", "'mongodb'", "]", "LOG", ".", "info", "(", "\"Dropping database %s\"", ",", "db_name", ")", "try", ":", "ctx", ".", "obj", "[", "'client'", "]", ".", "drop_database", "(", "db_name", ")", "except", "Exception", "as", "err", ":", "LOG", ".", "warning", "(", "err", ")", "ctx", ".", "abort", "(", ")", "LOG", ".", "info", "(", "\"Dropped whole database\"", ")"], "docstring": "Drop the mongo database given.", "docstring_tokens": ["Drop", "the", "mongo", "database", "given", "."], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/commands/wipe_database.py#L31-L41", "partition": "test"}
{"repo": "Julian/Ivoire", "path": "ivoire/result.py", "func_name": "FormatterMixin.statistics", "original_string": "def statistics(self, elapsed, result):\n        \"\"\"\n        Return output for the combined time and result summary statistics.\n\n        \"\"\"\n\n        return \"\\n\".join((self.timing(elapsed), self.result_summary(result)))", "language": "python", "code": "def statistics(self, elapsed, result):\n        \"\"\"\n        Return output for the combined time and result summary statistics.\n\n        \"\"\"\n\n        return \"\\n\".join((self.timing(elapsed), self.result_summary(result)))", "code_tokens": ["def", "statistics", "(", "self", ",", "elapsed", ",", "result", ")", ":", "return", "\"\\n\"", ".", "join", "(", "(", "self", ".", "timing", "(", "elapsed", ")", ",", "self", ".", "result_summary", "(", "result", ")", ")", ")"], "docstring": "Return output for the combined time and result summary statistics.", "docstring_tokens": ["Return", "output", "for", "the", "combined", "time", "and", "result", "summary", "statistics", "."], "sha": "5b8218cffa409ed733cf850a6fde16fafb8fc2af", "url": "https://github.com/Julian/Ivoire/blob/5b8218cffa409ed733cf850a6fde16fafb8fc2af/ivoire/result.py#L77-L83", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/reader.py", "func_name": "_read_regex", "original_string": "def _read_regex(ctx: ReaderContext) -> Pattern:\n    \"\"\"Read a regex reader macro from the input stream.\"\"\"\n    s = _read_str(ctx, allow_arbitrary_escapes=True)\n    try:\n        return langutil.regex_from_str(s)\n    except re.error:\n        raise SyntaxError(f\"Unrecognized regex pattern syntax: {s}\")", "language": "python", "code": "def _read_regex(ctx: ReaderContext) -> Pattern:\n    \"\"\"Read a regex reader macro from the input stream.\"\"\"\n    s = _read_str(ctx, allow_arbitrary_escapes=True)\n    try:\n        return langutil.regex_from_str(s)\n    except re.error:\n        raise SyntaxError(f\"Unrecognized regex pattern syntax: {s}\")", "code_tokens": ["def", "_read_regex", "(", "ctx", ":", "ReaderContext", ")", "->", "Pattern", ":", "s", "=", "_read_str", "(", "ctx", ",", "allow_arbitrary_escapes", "=", "True", ")", "try", ":", "return", "langutil", ".", "regex_from_str", "(", "s", ")", "except", "re", ".", "error", ":", "raise", "SyntaxError", "(", "f\"Unrecognized regex pattern syntax: {s}\"", ")"], "docstring": "Read a regex reader macro from the input stream.", "docstring_tokens": ["Read", "a", "regex", "reader", "macro", "from", "the", "input", "stream", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/reader.py#L916-L922", "partition": "test"}
{"repo": "napalm-automation/napalm-yang", "path": "napalm_yang/utils.py", "func_name": "model_to_dict", "original_string": "def model_to_dict(model, mode=\"\", show_defaults=False):\n    \"\"\"\n    Given a model, return a representation of the model in a dict.\n\n    This is mostly useful to have a quick visual represenation of the model.\n\n    Args:\n\n        model (PybindBase): Model to transform.\n        mode (string): Whether to print config, state or all elements (\"\" for all)\n\n    Returns:\n\n        dict: A dictionary representing the model.\n\n    Examples:\n\n\n        >>> config = napalm_yang.base.Root()\n        >>>\n        >>> # Adding models to the object\n        >>> config.add_model(napalm_yang.models.openconfig_interfaces())\n        >>> config.add_model(napalm_yang.models.openconfig_vlan())\n        >>> # Printing the model in a human readable format\n        >>> pretty_print(napalm_yang.utils.model_to_dict(config))\n        >>> {\n        >>>     \"openconfig-interfaces:interfaces [rw]\": {\n        >>>         \"interface [rw]\": {\n        >>>             \"config [rw]\": {\n        >>>                 \"description [rw]\": \"string\",\n        >>>                 \"enabled [rw]\": \"boolean\",\n        >>>                 \"mtu [rw]\": \"uint16\",\n        >>>                 \"name [rw]\": \"string\",\n        >>>                 \"type [rw]\": \"identityref\"\n        >>>             },\n        >>>             \"hold_time [rw]\": {\n        >>>                 \"config [rw]\": {\n        >>>                     \"down [rw]\": \"uint32\",\n        >>>                     \"up [rw]\": \"uint32\"\n            (trimmed for clarity)\n    \"\"\"\n\n    def is_mode(obj, mode):\n        if mode == \"\":\n            return True\n        elif mode == \"config\":\n            return obj._yang_name == \"config\" or obj._is_config\n        elif mode == \"state\":\n            return obj._yang_name == \"state\" or not obj._is_config\n        else:\n            raise ValueError(\n                \"mode can only be config, state or ''. Passed: {}\".format(mode)\n            )\n\n    def get_key(key, model, parent_defining_module, show_defaults):\n        if not show_defaults:\n            # No need to display rw/ro when showing the defaults.\n            key = \"{} {}\".format(key, \"[rw]\" if model._is_config else \"[ro]\")\n        if parent_defining_module != model._defining_module:\n            key = \"{}:{}\".format(model._defining_module, key)\n        return key\n\n    if model._yang_type in (\"container\", \"list\"):\n        cls = model if model._yang_type in (\"container\",) else model._contained_class()\n        result = {}\n        for k, v in cls:\n            r = model_to_dict(v, mode=mode, show_defaults=show_defaults)\n            if r:\n                result[get_key(k, v, model._defining_module, show_defaults)] = r\n        return result\n    else:\n        if show_defaults:\n            if model._default is False:\n                if model._yang_type != \"boolean\":\n                    # Unless the datatype is bool, when the _default attribute\n                    # is False, it means there is not default value defined in\n                    # the YANG model.\n                    return None\n            return model._default\n        return model._yang_type if is_mode(model, mode) else None", "language": "python", "code": "def model_to_dict(model, mode=\"\", show_defaults=False):\n    \"\"\"\n    Given a model, return a representation of the model in a dict.\n\n    This is mostly useful to have a quick visual represenation of the model.\n\n    Args:\n\n        model (PybindBase): Model to transform.\n        mode (string): Whether to print config, state or all elements (\"\" for all)\n\n    Returns:\n\n        dict: A dictionary representing the model.\n\n    Examples:\n\n\n        >>> config = napalm_yang.base.Root()\n        >>>\n        >>> # Adding models to the object\n        >>> config.add_model(napalm_yang.models.openconfig_interfaces())\n        >>> config.add_model(napalm_yang.models.openconfig_vlan())\n        >>> # Printing the model in a human readable format\n        >>> pretty_print(napalm_yang.utils.model_to_dict(config))\n        >>> {\n        >>>     \"openconfig-interfaces:interfaces [rw]\": {\n        >>>         \"interface [rw]\": {\n        >>>             \"config [rw]\": {\n        >>>                 \"description [rw]\": \"string\",\n        >>>                 \"enabled [rw]\": \"boolean\",\n        >>>                 \"mtu [rw]\": \"uint16\",\n        >>>                 \"name [rw]\": \"string\",\n        >>>                 \"type [rw]\": \"identityref\"\n        >>>             },\n        >>>             \"hold_time [rw]\": {\n        >>>                 \"config [rw]\": {\n        >>>                     \"down [rw]\": \"uint32\",\n        >>>                     \"up [rw]\": \"uint32\"\n            (trimmed for clarity)\n    \"\"\"\n\n    def is_mode(obj, mode):\n        if mode == \"\":\n            return True\n        elif mode == \"config\":\n            return obj._yang_name == \"config\" or obj._is_config\n        elif mode == \"state\":\n            return obj._yang_name == \"state\" or not obj._is_config\n        else:\n            raise ValueError(\n                \"mode can only be config, state or ''. Passed: {}\".format(mode)\n            )\n\n    def get_key(key, model, parent_defining_module, show_defaults):\n        if not show_defaults:\n            # No need to display rw/ro when showing the defaults.\n            key = \"{} {}\".format(key, \"[rw]\" if model._is_config else \"[ro]\")\n        if parent_defining_module != model._defining_module:\n            key = \"{}:{}\".format(model._defining_module, key)\n        return key\n\n    if model._yang_type in (\"container\", \"list\"):\n        cls = model if model._yang_type in (\"container\",) else model._contained_class()\n        result = {}\n        for k, v in cls:\n            r = model_to_dict(v, mode=mode, show_defaults=show_defaults)\n            if r:\n                result[get_key(k, v, model._defining_module, show_defaults)] = r\n        return result\n    else:\n        if show_defaults:\n            if model._default is False:\n                if model._yang_type != \"boolean\":\n                    # Unless the datatype is bool, when the _default attribute\n                    # is False, it means there is not default value defined in\n                    # the YANG model.\n                    return None\n            return model._default\n        return model._yang_type if is_mode(model, mode) else None", "code_tokens": ["def", "model_to_dict", "(", "model", ",", "mode", "=", "\"\"", ",", "show_defaults", "=", "False", ")", ":", "def", "is_mode", "(", "obj", ",", "mode", ")", ":", "if", "mode", "==", "\"\"", ":", "return", "True", "elif", "mode", "==", "\"config\"", ":", "return", "obj", ".", "_yang_name", "==", "\"config\"", "or", "obj", ".", "_is_config", "elif", "mode", "==", "\"state\"", ":", "return", "obj", ".", "_yang_name", "==", "\"state\"", "or", "not", "obj", ".", "_is_config", "else", ":", "raise", "ValueError", "(", "\"mode can only be config, state or ''. Passed: {}\"", ".", "format", "(", "mode", ")", ")", "def", "get_key", "(", "key", ",", "model", ",", "parent_defining_module", ",", "show_defaults", ")", ":", "if", "not", "show_defaults", ":", "# No need to display rw/ro when showing the defaults.", "key", "=", "\"{} {}\"", ".", "format", "(", "key", ",", "\"[rw]\"", "if", "model", ".", "_is_config", "else", "\"[ro]\"", ")", "if", "parent_defining_module", "!=", "model", ".", "_defining_module", ":", "key", "=", "\"{}:{}\"", ".", "format", "(", "model", ".", "_defining_module", ",", "key", ")", "return", "key", "if", "model", ".", "_yang_type", "in", "(", "\"container\"", ",", "\"list\"", ")", ":", "cls", "=", "model", "if", "model", ".", "_yang_type", "in", "(", "\"container\"", ",", ")", "else", "model", ".", "_contained_class", "(", ")", "result", "=", "{", "}", "for", "k", ",", "v", "in", "cls", ":", "r", "=", "model_to_dict", "(", "v", ",", "mode", "=", "mode", ",", "show_defaults", "=", "show_defaults", ")", "if", "r", ":", "result", "[", "get_key", "(", "k", ",", "v", ",", "model", ".", "_defining_module", ",", "show_defaults", ")", "]", "=", "r", "return", "result", "else", ":", "if", "show_defaults", ":", "if", "model", ".", "_default", "is", "False", ":", "if", "model", ".", "_yang_type", "!=", "\"boolean\"", ":", "# Unless the datatype is bool, when the _default attribute", "# is False, it means there is not default value defined in", "# the YANG model.", "return", "None", "return", "model", ".", "_default", "return", "model", ".", "_yang_type", "if", "is_mode", "(", "model", ",", "mode", ")", "else", "None"], "docstring": "Given a model, return a representation of the model in a dict.\n\n    This is mostly useful to have a quick visual represenation of the model.\n\n    Args:\n\n        model (PybindBase): Model to transform.\n        mode (string): Whether to print config, state or all elements (\"\" for all)\n\n    Returns:\n\n        dict: A dictionary representing the model.\n\n    Examples:\n\n\n        >>> config = napalm_yang.base.Root()\n        >>>\n        >>> # Adding models to the object\n        >>> config.add_model(napalm_yang.models.openconfig_interfaces())\n        >>> config.add_model(napalm_yang.models.openconfig_vlan())\n        >>> # Printing the model in a human readable format\n        >>> pretty_print(napalm_yang.utils.model_to_dict(config))\n        >>> {\n        >>>     \"openconfig-interfaces:interfaces [rw]\": {\n        >>>         \"interface [rw]\": {\n        >>>             \"config [rw]\": {\n        >>>                 \"description [rw]\": \"string\",\n        >>>                 \"enabled [rw]\": \"boolean\",\n        >>>                 \"mtu [rw]\": \"uint16\",\n        >>>                 \"name [rw]\": \"string\",\n        >>>                 \"type [rw]\": \"identityref\"\n        >>>             },\n        >>>             \"hold_time [rw]\": {\n        >>>                 \"config [rw]\": {\n        >>>                     \"down [rw]\": \"uint32\",\n        >>>                     \"up [rw]\": \"uint32\"\n            (trimmed for clarity)", "docstring_tokens": ["Given", "a", "model", "return", "a", "representation", "of", "the", "model", "in", "a", "dict", "."], "sha": "998e8a933171d010b8544bcc5dc448e2b68051e2", "url": "https://github.com/napalm-automation/napalm-yang/blob/998e8a933171d010b8544bcc5dc448e2b68051e2/napalm_yang/utils.py#L4-L83", "partition": "test"}
{"repo": "oscarbranson/latools", "path": "latools/latools.py", "func_name": "analyse.bkg_calc_interp1d", "original_string": "def bkg_calc_interp1d(self, analytes=None, kind=1, n_min=10, n_max=None, cstep=None,\n                          bkg_filter=False, f_win=7, f_n_lim=3, focus_stage='despiked'):\n        \"\"\"\n        Background calculation using a 1D interpolation.\n\n        scipy.interpolate.interp1D is used for interpolation.\n\n        Parameters\n        ----------\n        analytes : str or iterable\n            Which analyte or analytes to calculate.\n        kind : str or int\n            Integer specifying the order of the spline interpolation\n            used, or string specifying a type of interpolation.\n            Passed to `scipy.interpolate.interp1D`\n        n_min : int\n            Background regions with fewer than n_min points\n            will not be included in the fit.\n        cstep : float or None\n            The interval between calculated background points.\n        filter : bool\n            If true, apply a rolling filter to the isolated background regions\n            to exclude regions with anomalously high values. If True, two parameters\n            alter the filter's behaviour:\n        f_win : int\n            The size of the rolling window\n        f_n_lim : float\n            The number of standard deviations above the rolling mean\n            to set the threshold.\n        focus_stage : str\n            Which stage of analysis to apply processing to. \n            Defaults to 'despiked' if present, or 'rawdata' if not. \n            Can be one of:\n            * 'rawdata': raw data, loaded from csv file.\n            * 'despiked': despiked data.\n            * 'signal'/'background': isolated signal and background data.\n              Created by self.separate, after signal and background\n              regions have been identified by self.autorange.\n            * 'bkgsub': background subtracted data, created by \n              self.bkg_correct\n            * 'ratios': element ratio data, created by self.ratio.\n            * 'calibrated': ratio data calibrated to standards, created by self.calibrate.            \n        \"\"\"\n        if analytes is None:\n            analytes = self.analytes\n            self.bkg = Bunch()\n        elif isinstance(analytes, str):\n            analytes = [analytes]\n\n        self.get_background(n_min=n_min, n_max=n_max,\n                            bkg_filter=bkg_filter,\n                            f_win=f_win, f_n_lim=f_n_lim, focus_stage=focus_stage)\n\n        def pad(a, lo=None, hi=None):\n            if lo is None:\n                lo = [a[0]]\n            if hi is None:\n                hi = [a[-1]]\n            return np.concatenate((lo, a, hi))\n\n        if 'calc' not in self.bkg.keys():\n            # create time points to calculate background\n            # if cstep is None:\n                # cstep = self.bkg['raw']['uTime'].ptp() / 100\n            # bkg_t = np.arange(self.bkg['summary']['uTime']['mean'].min(),\n            #                   self.bkg['summary']['uTime']['mean'].max(),\n            #                   cstep)\n\n            bkg_t = pad(self.bkg['summary'].loc[:, ('uTime', 'mean')], [0], [self.max_time])\n\n            self.bkg['calc'] = Bunch()\n            self.bkg['calc']['uTime'] = bkg_t\n\n        d = self.bkg['summary']\n        with self.pbar.set(total=len(analytes), desc='Calculating Analyte Backgrounds') as prog:\n            for a in analytes:\n                self.bkg['calc'][a] = {'mean': pad(d.loc[:, (a, 'mean')].values),\n                                    'std': pad(d.loc[:, (a, 'std')].values),\n                                    'stderr': pad(d.loc[:, (a, 'stderr')].values)}\n                prog.update()\n\n        self.bkg['calc']\n\n        return", "language": "python", "code": "def bkg_calc_interp1d(self, analytes=None, kind=1, n_min=10, n_max=None, cstep=None,\n                          bkg_filter=False, f_win=7, f_n_lim=3, focus_stage='despiked'):\n        \"\"\"\n        Background calculation using a 1D interpolation.\n\n        scipy.interpolate.interp1D is used for interpolation.\n\n        Parameters\n        ----------\n        analytes : str or iterable\n            Which analyte or analytes to calculate.\n        kind : str or int\n            Integer specifying the order of the spline interpolation\n            used, or string specifying a type of interpolation.\n            Passed to `scipy.interpolate.interp1D`\n        n_min : int\n            Background regions with fewer than n_min points\n            will not be included in the fit.\n        cstep : float or None\n            The interval between calculated background points.\n        filter : bool\n            If true, apply a rolling filter to the isolated background regions\n            to exclude regions with anomalously high values. If True, two parameters\n            alter the filter's behaviour:\n        f_win : int\n            The size of the rolling window\n        f_n_lim : float\n            The number of standard deviations above the rolling mean\n            to set the threshold.\n        focus_stage : str\n            Which stage of analysis to apply processing to. \n            Defaults to 'despiked' if present, or 'rawdata' if not. \n            Can be one of:\n            * 'rawdata': raw data, loaded from csv file.\n            * 'despiked': despiked data.\n            * 'signal'/'background': isolated signal and background data.\n              Created by self.separate, after signal and background\n              regions have been identified by self.autorange.\n            * 'bkgsub': background subtracted data, created by \n              self.bkg_correct\n            * 'ratios': element ratio data, created by self.ratio.\n            * 'calibrated': ratio data calibrated to standards, created by self.calibrate.            \n        \"\"\"\n        if analytes is None:\n            analytes = self.analytes\n            self.bkg = Bunch()\n        elif isinstance(analytes, str):\n            analytes = [analytes]\n\n        self.get_background(n_min=n_min, n_max=n_max,\n                            bkg_filter=bkg_filter,\n                            f_win=f_win, f_n_lim=f_n_lim, focus_stage=focus_stage)\n\n        def pad(a, lo=None, hi=None):\n            if lo is None:\n                lo = [a[0]]\n            if hi is None:\n                hi = [a[-1]]\n            return np.concatenate((lo, a, hi))\n\n        if 'calc' not in self.bkg.keys():\n            # create time points to calculate background\n            # if cstep is None:\n                # cstep = self.bkg['raw']['uTime'].ptp() / 100\n            # bkg_t = np.arange(self.bkg['summary']['uTime']['mean'].min(),\n            #                   self.bkg['summary']['uTime']['mean'].max(),\n            #                   cstep)\n\n            bkg_t = pad(self.bkg['summary'].loc[:, ('uTime', 'mean')], [0], [self.max_time])\n\n            self.bkg['calc'] = Bunch()\n            self.bkg['calc']['uTime'] = bkg_t\n\n        d = self.bkg['summary']\n        with self.pbar.set(total=len(analytes), desc='Calculating Analyte Backgrounds') as prog:\n            for a in analytes:\n                self.bkg['calc'][a] = {'mean': pad(d.loc[:, (a, 'mean')].values),\n                                    'std': pad(d.loc[:, (a, 'std')].values),\n                                    'stderr': pad(d.loc[:, (a, 'stderr')].values)}\n                prog.update()\n\n        self.bkg['calc']\n\n        return", "code_tokens": ["def", "bkg_calc_interp1d", "(", "self", ",", "analytes", "=", "None", ",", "kind", "=", "1", ",", "n_min", "=", "10", ",", "n_max", "=", "None", ",", "cstep", "=", "None", ",", "bkg_filter", "=", "False", ",", "f_win", "=", "7", ",", "f_n_lim", "=", "3", ",", "focus_stage", "=", "'despiked'", ")", ":", "if", "analytes", "is", "None", ":", "analytes", "=", "self", ".", "analytes", "self", ".", "bkg", "=", "Bunch", "(", ")", "elif", "isinstance", "(", "analytes", ",", "str", ")", ":", "analytes", "=", "[", "analytes", "]", "self", ".", "get_background", "(", "n_min", "=", "n_min", ",", "n_max", "=", "n_max", ",", "bkg_filter", "=", "bkg_filter", ",", "f_win", "=", "f_win", ",", "f_n_lim", "=", "f_n_lim", ",", "focus_stage", "=", "focus_stage", ")", "def", "pad", "(", "a", ",", "lo", "=", "None", ",", "hi", "=", "None", ")", ":", "if", "lo", "is", "None", ":", "lo", "=", "[", "a", "[", "0", "]", "]", "if", "hi", "is", "None", ":", "hi", "=", "[", "a", "[", "-", "1", "]", "]", "return", "np", ".", "concatenate", "(", "(", "lo", ",", "a", ",", "hi", ")", ")", "if", "'calc'", "not", "in", "self", ".", "bkg", ".", "keys", "(", ")", ":", "# create time points to calculate background", "# if cstep is None:", "# cstep = self.bkg['raw']['uTime'].ptp() / 100", "# bkg_t = np.arange(self.bkg['summary']['uTime']['mean'].min(),", "#                   self.bkg['summary']['uTime']['mean'].max(),", "#                   cstep)", "bkg_t", "=", "pad", "(", "self", ".", "bkg", "[", "'summary'", "]", ".", "loc", "[", ":", ",", "(", "'uTime'", ",", "'mean'", ")", "]", ",", "[", "0", "]", ",", "[", "self", ".", "max_time", "]", ")", "self", ".", "bkg", "[", "'calc'", "]", "=", "Bunch", "(", ")", "self", ".", "bkg", "[", "'calc'", "]", "[", "'uTime'", "]", "=", "bkg_t", "d", "=", "self", ".", "bkg", "[", "'summary'", "]", "with", "self", ".", "pbar", ".", "set", "(", "total", "=", "len", "(", "analytes", ")", ",", "desc", "=", "'Calculating Analyte Backgrounds'", ")", "as", "prog", ":", "for", "a", "in", "analytes", ":", "self", ".", "bkg", "[", "'calc'", "]", "[", "a", "]", "=", "{", "'mean'", ":", "pad", "(", "d", ".", "loc", "[", ":", ",", "(", "a", ",", "'mean'", ")", "]", ".", "values", ")", ",", "'std'", ":", "pad", "(", "d", ".", "loc", "[", ":", ",", "(", "a", ",", "'std'", ")", "]", ".", "values", ")", ",", "'stderr'", ":", "pad", "(", "d", ".", "loc", "[", ":", ",", "(", "a", ",", "'stderr'", ")", "]", ".", "values", ")", "}", "prog", ".", "update", "(", ")", "self", ".", "bkg", "[", "'calc'", "]", "return"], "docstring": "Background calculation using a 1D interpolation.\n\n        scipy.interpolate.interp1D is used for interpolation.\n\n        Parameters\n        ----------\n        analytes : str or iterable\n            Which analyte or analytes to calculate.\n        kind : str or int\n            Integer specifying the order of the spline interpolation\n            used, or string specifying a type of interpolation.\n            Passed to `scipy.interpolate.interp1D`\n        n_min : int\n            Background regions with fewer than n_min points\n            will not be included in the fit.\n        cstep : float or None\n            The interval between calculated background points.\n        filter : bool\n            If true, apply a rolling filter to the isolated background regions\n            to exclude regions with anomalously high values. If True, two parameters\n            alter the filter's behaviour:\n        f_win : int\n            The size of the rolling window\n        f_n_lim : float\n            The number of standard deviations above the rolling mean\n            to set the threshold.\n        focus_stage : str\n            Which stage of analysis to apply processing to. \n            Defaults to 'despiked' if present, or 'rawdata' if not. \n            Can be one of:\n            * 'rawdata': raw data, loaded from csv file.\n            * 'despiked': despiked data.\n            * 'signal'/'background': isolated signal and background data.\n              Created by self.separate, after signal and background\n              regions have been identified by self.autorange.\n            * 'bkgsub': background subtracted data, created by \n              self.bkg_correct\n            * 'ratios': element ratio data, created by self.ratio.\n            * 'calibrated': ratio data calibrated to standards, created by self.calibrate.", "docstring_tokens": ["Background", "calculation", "using", "a", "1D", "interpolation", "."], "sha": "cd25a650cfee318152f234d992708511f7047fbe", "url": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/latools.py#L878-L961", "partition": "test"}
{"repo": "gtaylor/python-route53", "path": "route53/hosted_zone.py", "func_name": "HostedZone.create_mx_record", "original_string": "def create_mx_record(self, name, values, ttl=60):\n        \"\"\"\n        Creates a MX record attached to this hosted zone.\n\n        :param str name: The fully qualified name of the record to add.\n        :param list values: A list of value strings for the record.\n        :keyword int ttl: The time-to-live of the record (in seconds).\n        :rtype: tuple\n        :returns: A tuple in the form of ``(rrset, change_info)``, where\n            ``rrset`` is the newly created MXResourceRecordSet instance.\n        \"\"\"\n\n        self._halt_if_already_deleted()\n\n        # Grab the params/kwargs here for brevity's sake.\n        values = locals()\n        del values['self']\n\n        return self._add_record(MXResourceRecordSet, **values)", "language": "python", "code": "def create_mx_record(self, name, values, ttl=60):\n        \"\"\"\n        Creates a MX record attached to this hosted zone.\n\n        :param str name: The fully qualified name of the record to add.\n        :param list values: A list of value strings for the record.\n        :keyword int ttl: The time-to-live of the record (in seconds).\n        :rtype: tuple\n        :returns: A tuple in the form of ``(rrset, change_info)``, where\n            ``rrset`` is the newly created MXResourceRecordSet instance.\n        \"\"\"\n\n        self._halt_if_already_deleted()\n\n        # Grab the params/kwargs here for brevity's sake.\n        values = locals()\n        del values['self']\n\n        return self._add_record(MXResourceRecordSet, **values)", "code_tokens": ["def", "create_mx_record", "(", "self", ",", "name", ",", "values", ",", "ttl", "=", "60", ")", ":", "self", ".", "_halt_if_already_deleted", "(", ")", "# Grab the params/kwargs here for brevity's sake.", "values", "=", "locals", "(", ")", "del", "values", "[", "'self'", "]", "return", "self", ".", "_add_record", "(", "MXResourceRecordSet", ",", "*", "*", "values", ")"], "docstring": "Creates a MX record attached to this hosted zone.\n\n        :param str name: The fully qualified name of the record to add.\n        :param list values: A list of value strings for the record.\n        :keyword int ttl: The time-to-live of the record (in seconds).\n        :rtype: tuple\n        :returns: A tuple in the form of ``(rrset, change_info)``, where\n            ``rrset`` is the newly created MXResourceRecordSet instance.", "docstring_tokens": ["Creates", "a", "MX", "record", "attached", "to", "this", "hosted", "zone", "."], "sha": "b9fc7e258a79551c9ed61e4a71668b7f06f9e774", "url": "https://github.com/gtaylor/python-route53/blob/b9fc7e258a79551c9ed61e4a71668b7f06f9e774/route53/hosted_zone.py#L284-L302", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/flask/app.py", "func_name": "Flask.handle_http_exception", "original_string": "def handle_http_exception(self, e):\n        \"\"\"Handles an HTTP exception.  By default this will invoke the\n        registered error handlers and fall back to returning the\n        exception as response.\n\n        .. versionadded:: 0.3\n        \"\"\"\n        handlers = self.error_handler_spec.get(request.blueprint)\n        # Proxy exceptions don't have error codes.  We want to always return\n        # those unchanged as errors\n        if e.code is None:\n            return e\n        if handlers and e.code in handlers:\n            handler = handlers[e.code]\n        else:\n            handler = self.error_handler_spec[None].get(e.code)\n        if handler is None:\n            return e\n        return handler(e)", "language": "python", "code": "def handle_http_exception(self, e):\n        \"\"\"Handles an HTTP exception.  By default this will invoke the\n        registered error handlers and fall back to returning the\n        exception as response.\n\n        .. versionadded:: 0.3\n        \"\"\"\n        handlers = self.error_handler_spec.get(request.blueprint)\n        # Proxy exceptions don't have error codes.  We want to always return\n        # those unchanged as errors\n        if e.code is None:\n            return e\n        if handlers and e.code in handlers:\n            handler = handlers[e.code]\n        else:\n            handler = self.error_handler_spec[None].get(e.code)\n        if handler is None:\n            return e\n        return handler(e)", "code_tokens": ["def", "handle_http_exception", "(", "self", ",", "e", ")", ":", "handlers", "=", "self", ".", "error_handler_spec", ".", "get", "(", "request", ".", "blueprint", ")", "# Proxy exceptions don't have error codes.  We want to always return", "# those unchanged as errors", "if", "e", ".", "code", "is", "None", ":", "return", "e", "if", "handlers", "and", "e", ".", "code", "in", "handlers", ":", "handler", "=", "handlers", "[", "e", ".", "code", "]", "else", ":", "handler", "=", "self", ".", "error_handler_spec", "[", "None", "]", ".", "get", "(", "e", ".", "code", ")", "if", "handler", "is", "None", ":", "return", "e", "return", "handler", "(", "e", ")"], "docstring": "Handles an HTTP exception.  By default this will invoke the\n        registered error handlers and fall back to returning the\n        exception as response.\n\n        .. versionadded:: 0.3", "docstring_tokens": ["Handles", "an", "HTTP", "exception", ".", "By", "default", "this", "will", "invoke", "the", "registered", "error", "handlers", "and", "fall", "back", "to", "returning", "the", "exception", "as", "response", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/flask/app.py#L1312-L1330", "partition": "test"}
{"repo": "Azure/Azure-MachineLearning-ClientLibrary-Python", "path": "azureml/__init__.py", "func_name": "SourceDataset._update_from_dataframe", "original_string": "def _update_from_dataframe(self, dataframe, data_type_id=None, name=None,\n                              description=None):\n        \"\"\"\n        Serialize the specified DataFrame and replace the existing dataset.\n\n        Parameters\n        ----------\n        dataframe : pandas.DataFrame\n            Data to serialize.\n        data_type_id : str, optional\n            Format to serialize to.\n            If None, the existing format is preserved.\n            Supported formats are:\n                'PlainText'\n                'GenericCSV'\n                'GenericTSV'\n                'GenericCSVNoHeader'\n                'GenericTSVNoHeader'\n            See the azureml.DataTypeIds class for constants.\n        name : str, optional\n            Name for the dataset.\n            If None, the name of the existing dataset is used.\n        description : str, optional\n            Description for the dataset.\n            If None, the name of the existing dataset is used.\n        \"\"\"\n        _not_none('dataframe', dataframe)\n\n        if data_type_id is None:\n            data_type_id = self.data_type_id\n        if name is None:\n            name = self.name\n        if description is None:\n            description = self.description\n\n        try:\n            output = BytesIO()\n            serialize_dataframe(output, data_type_id, dataframe)\n            raw_data = output.getvalue()\n        finally:\n            output.close()\n\n        self._upload_and_refresh(raw_data, data_type_id, name, description)", "language": "python", "code": "def _update_from_dataframe(self, dataframe, data_type_id=None, name=None,\n                              description=None):\n        \"\"\"\n        Serialize the specified DataFrame and replace the existing dataset.\n\n        Parameters\n        ----------\n        dataframe : pandas.DataFrame\n            Data to serialize.\n        data_type_id : str, optional\n            Format to serialize to.\n            If None, the existing format is preserved.\n            Supported formats are:\n                'PlainText'\n                'GenericCSV'\n                'GenericTSV'\n                'GenericCSVNoHeader'\n                'GenericTSVNoHeader'\n            See the azureml.DataTypeIds class for constants.\n        name : str, optional\n            Name for the dataset.\n            If None, the name of the existing dataset is used.\n        description : str, optional\n            Description for the dataset.\n            If None, the name of the existing dataset is used.\n        \"\"\"\n        _not_none('dataframe', dataframe)\n\n        if data_type_id is None:\n            data_type_id = self.data_type_id\n        if name is None:\n            name = self.name\n        if description is None:\n            description = self.description\n\n        try:\n            output = BytesIO()\n            serialize_dataframe(output, data_type_id, dataframe)\n            raw_data = output.getvalue()\n        finally:\n            output.close()\n\n        self._upload_and_refresh(raw_data, data_type_id, name, description)", "code_tokens": ["def", "_update_from_dataframe", "(", "self", ",", "dataframe", ",", "data_type_id", "=", "None", ",", "name", "=", "None", ",", "description", "=", "None", ")", ":", "_not_none", "(", "'dataframe'", ",", "dataframe", ")", "if", "data_type_id", "is", "None", ":", "data_type_id", "=", "self", ".", "data_type_id", "if", "name", "is", "None", ":", "name", "=", "self", ".", "name", "if", "description", "is", "None", ":", "description", "=", "self", ".", "description", "try", ":", "output", "=", "BytesIO", "(", ")", "serialize_dataframe", "(", "output", ",", "data_type_id", ",", "dataframe", ")", "raw_data", "=", "output", ".", "getvalue", "(", ")", "finally", ":", "output", ".", "close", "(", ")", "self", ".", "_upload_and_refresh", "(", "raw_data", ",", "data_type_id", ",", "name", ",", "description", ")"], "docstring": "Serialize the specified DataFrame and replace the existing dataset.\n\n        Parameters\n        ----------\n        dataframe : pandas.DataFrame\n            Data to serialize.\n        data_type_id : str, optional\n            Format to serialize to.\n            If None, the existing format is preserved.\n            Supported formats are:\n                'PlainText'\n                'GenericCSV'\n                'GenericTSV'\n                'GenericCSVNoHeader'\n                'GenericTSVNoHeader'\n            See the azureml.DataTypeIds class for constants.\n        name : str, optional\n            Name for the dataset.\n            If None, the name of the existing dataset is used.\n        description : str, optional\n            Description for the dataset.\n            If None, the name of the existing dataset is used.", "docstring_tokens": ["Serialize", "the", "specified", "DataFrame", "and", "replace", "the", "existing", "dataset", "."], "sha": "d1211b289747671898eb063013e0dc53d3c80acd", "url": "https://github.com/Azure/Azure-MachineLearning-ClientLibrary-Python/blob/d1211b289747671898eb063013e0dc53d3c80acd/azureml/__init__.py#L134-L176", "partition": "test"}
{"repo": "iotaledger/iota.lib.py", "path": "iota/transaction/creation.py", "func_name": "ProposedBundle.sign_input_at", "original_string": "def sign_input_at(self, start_index, private_key):\n        # type: (int, PrivateKey) -> None\n        \"\"\"\n        Signs the input at the specified index.\n\n        :param start_index:\n            The index of the first input transaction.\n\n            If necessary, the resulting signature will be split across\n            multiple transactions automatically (i.e., if an input has\n            ``security_level=2``, you still only need to call\n            :py:meth:`sign_input_at` once).\n\n        :param private_key:\n            The private key that will be used to generate the signature.\n\n            .. important::\n                Be sure that the private key was generated using the\n                correct seed, or the resulting signature will be\n                invalid!\n        \"\"\"\n        if not self.hash:\n            raise RuntimeError('Cannot sign inputs until bundle is finalized.')\n\n        private_key.sign_input_transactions(self, start_index)", "language": "python", "code": "def sign_input_at(self, start_index, private_key):\n        # type: (int, PrivateKey) -> None\n        \"\"\"\n        Signs the input at the specified index.\n\n        :param start_index:\n            The index of the first input transaction.\n\n            If necessary, the resulting signature will be split across\n            multiple transactions automatically (i.e., if an input has\n            ``security_level=2``, you still only need to call\n            :py:meth:`sign_input_at` once).\n\n        :param private_key:\n            The private key that will be used to generate the signature.\n\n            .. important::\n                Be sure that the private key was generated using the\n                correct seed, or the resulting signature will be\n                invalid!\n        \"\"\"\n        if not self.hash:\n            raise RuntimeError('Cannot sign inputs until bundle is finalized.')\n\n        private_key.sign_input_transactions(self, start_index)", "code_tokens": ["def", "sign_input_at", "(", "self", ",", "start_index", ",", "private_key", ")", ":", "# type: (int, PrivateKey) -> None", "if", "not", "self", ".", "hash", ":", "raise", "RuntimeError", "(", "'Cannot sign inputs until bundle is finalized.'", ")", "private_key", ".", "sign_input_transactions", "(", "self", ",", "start_index", ")"], "docstring": "Signs the input at the specified index.\n\n        :param start_index:\n            The index of the first input transaction.\n\n            If necessary, the resulting signature will be split across\n            multiple transactions automatically (i.e., if an input has\n            ``security_level=2``, you still only need to call\n            :py:meth:`sign_input_at` once).\n\n        :param private_key:\n            The private key that will be used to generate the signature.\n\n            .. important::\n                Be sure that the private key was generated using the\n                correct seed, or the resulting signature will be\n                invalid!", "docstring_tokens": ["Signs", "the", "input", "at", "the", "specified", "index", "."], "sha": "97cdd1e241498446b46157b79b2a1ea2ec6d387a", "url": "https://github.com/iotaledger/iota.lib.py/blob/97cdd1e241498446b46157b79b2a1ea2ec6d387a/iota/transaction/creation.py#L440-L464", "partition": "test"}
{"repo": "assemblerflow/flowcraft", "path": "flowcraft/templates/assembly_report.py", "func_name": "Assembly._get_window_labels", "original_string": "def _get_window_labels(self, window):\n        \"\"\"Returns the mapping between sliding window points and their contigs,\n        and the x-axis position of contig\n\n        Parameters\n        ----------\n        window : int\n            Size of the window.\n\n        Returns\n        -------\n        xbars : list\n            The x-axis position of the ending for each contig.\n        labels : list\n            The x-axis labels for each data point in the sliding window\n\n        \"\"\"\n\n        # Get summary stats, if they have not yet been triggered\n        if not self.summary_info:\n            self.get_summary_stats()\n\n        # Get contig boundary positon\n        c = 0\n        xbars = []\n        for contig, seq in self.contigs.items():\n            contig_id = self._get_contig_id(contig)\n            self.contig_boundaries[contig_id] = [c, c + len(seq)]\n            c += len(seq)\n            xbars.append((contig_id, c, contig))\n\n        return xbars", "language": "python", "code": "def _get_window_labels(self, window):\n        \"\"\"Returns the mapping between sliding window points and their contigs,\n        and the x-axis position of contig\n\n        Parameters\n        ----------\n        window : int\n            Size of the window.\n\n        Returns\n        -------\n        xbars : list\n            The x-axis position of the ending for each contig.\n        labels : list\n            The x-axis labels for each data point in the sliding window\n\n        \"\"\"\n\n        # Get summary stats, if they have not yet been triggered\n        if not self.summary_info:\n            self.get_summary_stats()\n\n        # Get contig boundary positon\n        c = 0\n        xbars = []\n        for contig, seq in self.contigs.items():\n            contig_id = self._get_contig_id(contig)\n            self.contig_boundaries[contig_id] = [c, c + len(seq)]\n            c += len(seq)\n            xbars.append((contig_id, c, contig))\n\n        return xbars", "code_tokens": ["def", "_get_window_labels", "(", "self", ",", "window", ")", ":", "# Get summary stats, if they have not yet been triggered", "if", "not", "self", ".", "summary_info", ":", "self", ".", "get_summary_stats", "(", ")", "# Get contig boundary positon", "c", "=", "0", "xbars", "=", "[", "]", "for", "contig", ",", "seq", "in", "self", ".", "contigs", ".", "items", "(", ")", ":", "contig_id", "=", "self", ".", "_get_contig_id", "(", "contig", ")", "self", ".", "contig_boundaries", "[", "contig_id", "]", "=", "[", "c", ",", "c", "+", "len", "(", "seq", ")", "]", "c", "+=", "len", "(", "seq", ")", "xbars", ".", "append", "(", "(", "contig_id", ",", "c", ",", "contig", ")", ")", "return", "xbars"], "docstring": "Returns the mapping between sliding window points and their contigs,\n        and the x-axis position of contig\n\n        Parameters\n        ----------\n        window : int\n            Size of the window.\n\n        Returns\n        -------\n        xbars : list\n            The x-axis position of the ending for each contig.\n        labels : list\n            The x-axis labels for each data point in the sliding window", "docstring_tokens": ["Returns", "the", "mapping", "between", "sliding", "window", "points", "and", "their", "contigs", "and", "the", "x", "-", "axis", "position", "of", "contig"], "sha": "fc3f4bddded1efc76006600016dc71a06dd908c0", "url": "https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/templates/assembly_report.py#L284-L315", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/parser.py", "func_name": "ByteParser._block_stack_repr", "original_string": "def _block_stack_repr(self, block_stack):\n        \"\"\"Get a string version of `block_stack`, for debugging.\"\"\"\n        blocks = \", \".join(\n            [\"(%s, %r)\" % (dis.opname[b[0]], b[1]) for b in block_stack]\n        )\n        return \"[\" + blocks + \"]\"", "language": "python", "code": "def _block_stack_repr(self, block_stack):\n        \"\"\"Get a string version of `block_stack`, for debugging.\"\"\"\n        blocks = \", \".join(\n            [\"(%s, %r)\" % (dis.opname[b[0]], b[1]) for b in block_stack]\n        )\n        return \"[\" + blocks + \"]\"", "code_tokens": ["def", "_block_stack_repr", "(", "self", ",", "block_stack", ")", ":", "blocks", "=", "\", \"", ".", "join", "(", "[", "\"(%s, %r)\"", "%", "(", "dis", ".", "opname", "[", "b", "[", "0", "]", "]", ",", "b", "[", "1", "]", ")", "for", "b", "in", "block_stack", "]", ")", "return", "\"[\"", "+", "blocks", "+", "\"]\""], "docstring": "Get a string version of `block_stack`, for debugging.", "docstring_tokens": ["Get", "a", "string", "version", "of", "block_stack", "for", "debugging", "."], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/parser.py#L415-L420", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/special_math.py", "func_name": "_ndtr", "original_string": "def _ndtr(x):\n  \"\"\"Implements ndtr core logic.\"\"\"\n  half_sqrt_2 = tf.constant(\n      0.5 * np.sqrt(2.), dtype=x.dtype, name=\"half_sqrt_2\")\n  w = x * half_sqrt_2\n  z = tf.abs(w)\n  y = tf.where(\n      tf.less(z, half_sqrt_2), 1. + tf.math.erf(w),\n      tf.where(tf.greater(w, 0.), 2. - tf.math.erfc(z), tf.math.erfc(z)))\n  return 0.5 * y", "language": "python", "code": "def _ndtr(x):\n  \"\"\"Implements ndtr core logic.\"\"\"\n  half_sqrt_2 = tf.constant(\n      0.5 * np.sqrt(2.), dtype=x.dtype, name=\"half_sqrt_2\")\n  w = x * half_sqrt_2\n  z = tf.abs(w)\n  y = tf.where(\n      tf.less(z, half_sqrt_2), 1. + tf.math.erf(w),\n      tf.where(tf.greater(w, 0.), 2. - tf.math.erfc(z), tf.math.erfc(z)))\n  return 0.5 * y", "code_tokens": ["def", "_ndtr", "(", "x", ")", ":", "half_sqrt_2", "=", "tf", ".", "constant", "(", "0.5", "*", "np", ".", "sqrt", "(", "2.", ")", ",", "dtype", "=", "x", ".", "dtype", ",", "name", "=", "\"half_sqrt_2\"", ")", "w", "=", "x", "*", "half_sqrt_2", "z", "=", "tf", ".", "abs", "(", "w", ")", "y", "=", "tf", ".", "where", "(", "tf", ".", "less", "(", "z", ",", "half_sqrt_2", ")", ",", "1.", "+", "tf", ".", "math", ".", "erf", "(", "w", ")", ",", "tf", ".", "where", "(", "tf", ".", "greater", "(", "w", ",", "0.", ")", ",", "2.", "-", "tf", ".", "math", ".", "erfc", "(", "z", ")", ",", "tf", ".", "math", ".", "erfc", "(", "z", ")", ")", ")", "return", "0.5", "*", "y"], "docstring": "Implements ndtr core logic.", "docstring_tokens": ["Implements", "ndtr", "core", "logic", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/special_math.py#L145-L154", "partition": "test"}
{"repo": "ellmetha/neojsonrpc", "path": "neojsonrpc/client.py", "func_name": "Client.get_block_sys_fee", "original_string": "def get_block_sys_fee(self, block_index, **kwargs):\n        \"\"\" Returns the system fees associated with a specific block index.\n\n        :param block_index: a block index (block height)\n        :type block_index: int\n        :return: system fees of the block, expressed in NeoGas units\n        :rtype: str\n\n        \"\"\"\n        return self._call(JSONRPCMethods.GET_BLOCK_SYS_FEE.value, [block_index, ], **kwargs)", "language": "python", "code": "def get_block_sys_fee(self, block_index, **kwargs):\n        \"\"\" Returns the system fees associated with a specific block index.\n\n        :param block_index: a block index (block height)\n        :type block_index: int\n        :return: system fees of the block, expressed in NeoGas units\n        :rtype: str\n\n        \"\"\"\n        return self._call(JSONRPCMethods.GET_BLOCK_SYS_FEE.value, [block_index, ], **kwargs)", "code_tokens": ["def", "get_block_sys_fee", "(", "self", ",", "block_index", ",", "*", "*", "kwargs", ")", ":", "return", "self", ".", "_call", "(", "JSONRPCMethods", ".", "GET_BLOCK_SYS_FEE", ".", "value", ",", "[", "block_index", ",", "]", ",", "*", "*", "kwargs", ")"], "docstring": "Returns the system fees associated with a specific block index.\n\n        :param block_index: a block index (block height)\n        :type block_index: int\n        :return: system fees of the block, expressed in NeoGas units\n        :rtype: str", "docstring_tokens": ["Returns", "the", "system", "fees", "associated", "with", "a", "specific", "block", "index", "."], "sha": "e369b633a727482d5f9e310f0c3337ae5f7265db", "url": "https://github.com/ellmetha/neojsonrpc/blob/e369b633a727482d5f9e310f0c3337ae5f7265db/neojsonrpc/client.py#L146-L155", "partition": "test"}
{"repo": "pmacosta/peng", "path": "peng/wave_functions.py", "func_name": "_running_area", "original_string": "def _running_area(indep_vector, dep_vector):\n    \"\"\"Calculate running area under curve.\"\"\"\n    rect_height = np.minimum(dep_vector[:-1], dep_vector[1:])\n    rect_base = np.diff(indep_vector)\n    rect_area = np.multiply(rect_height, rect_base)\n    triang_height = np.abs(np.diff(dep_vector))\n    triang_area = 0.5 * np.multiply(triang_height, rect_base)\n    return np.cumsum(np.concatenate((np.array([0.0]), triang_area + rect_area)))", "language": "python", "code": "def _running_area(indep_vector, dep_vector):\n    \"\"\"Calculate running area under curve.\"\"\"\n    rect_height = np.minimum(dep_vector[:-1], dep_vector[1:])\n    rect_base = np.diff(indep_vector)\n    rect_area = np.multiply(rect_height, rect_base)\n    triang_height = np.abs(np.diff(dep_vector))\n    triang_area = 0.5 * np.multiply(triang_height, rect_base)\n    return np.cumsum(np.concatenate((np.array([0.0]), triang_area + rect_area)))", "code_tokens": ["def", "_running_area", "(", "indep_vector", ",", "dep_vector", ")", ":", "rect_height", "=", "np", ".", "minimum", "(", "dep_vector", "[", ":", "-", "1", "]", ",", "dep_vector", "[", "1", ":", "]", ")", "rect_base", "=", "np", ".", "diff", "(", "indep_vector", ")", "rect_area", "=", "np", ".", "multiply", "(", "rect_height", ",", "rect_base", ")", "triang_height", "=", "np", ".", "abs", "(", "np", ".", "diff", "(", "dep_vector", ")", ")", "triang_area", "=", "0.5", "*", "np", ".", "multiply", "(", "triang_height", ",", "rect_base", ")", "return", "np", ".", "cumsum", "(", "np", ".", "concatenate", "(", "(", "np", ".", "array", "(", "[", "0.0", "]", ")", ",", "triang_area", "+", "rect_area", ")", ")", ")"], "docstring": "Calculate running area under curve.", "docstring_tokens": ["Calculate", "running", "area", "under", "curve", "."], "sha": "976935377adaa3de26fc5677aceb2cdfbd6f93a7", "url": "https://github.com/pmacosta/peng/blob/976935377adaa3de26fc5677aceb2cdfbd6f93a7/peng/wave_functions.py#L91-L98", "partition": "test"}
{"repo": "getgauge/gauge-python", "path": "getgauge/parser_redbaron.py", "func_name": "RedbaronPythonFile._step_decorator_args", "original_string": "def _step_decorator_args(self, decorator):\n        \"\"\"\n        Get arguments passed to step decorators converted to python objects.\n        \"\"\"\n        args = decorator.call.value\n        step = None\n        if len(args) == 1:\n            try:\n                step = args[0].value.to_python()\n            except (ValueError, SyntaxError):\n                pass\n            if isinstance(step, six.string_types + (list,)):\n                return step\n            logging.error(\"Decorator step accepts either a string or a list of \\\n                strings - %s\",\n                          self.file_path)\n        else:\n            logging.error(\"Decorator step accepts only one argument - %s\",\n                          self.file_path)", "language": "python", "code": "def _step_decorator_args(self, decorator):\n        \"\"\"\n        Get arguments passed to step decorators converted to python objects.\n        \"\"\"\n        args = decorator.call.value\n        step = None\n        if len(args) == 1:\n            try:\n                step = args[0].value.to_python()\n            except (ValueError, SyntaxError):\n                pass\n            if isinstance(step, six.string_types + (list,)):\n                return step\n            logging.error(\"Decorator step accepts either a string or a list of \\\n                strings - %s\",\n                          self.file_path)\n        else:\n            logging.error(\"Decorator step accepts only one argument - %s\",\n                          self.file_path)", "code_tokens": ["def", "_step_decorator_args", "(", "self", ",", "decorator", ")", ":", "args", "=", "decorator", ".", "call", ".", "value", "step", "=", "None", "if", "len", "(", "args", ")", "==", "1", ":", "try", ":", "step", "=", "args", "[", "0", "]", ".", "value", ".", "to_python", "(", ")", "except", "(", "ValueError", ",", "SyntaxError", ")", ":", "pass", "if", "isinstance", "(", "step", ",", "six", ".", "string_types", "+", "(", "list", ",", ")", ")", ":", "return", "step", "logging", ".", "error", "(", "\"Decorator step accepts either a string or a list of \\\n                strings - %s\"", ",", "self", ".", "file_path", ")", "else", ":", "logging", ".", "error", "(", "\"Decorator step accepts only one argument - %s\"", ",", "self", ".", "file_path", ")"], "docstring": "Get arguments passed to step decorators converted to python objects.", "docstring_tokens": ["Get", "arguments", "passed", "to", "step", "decorators", "converted", "to", "python", "objects", "."], "sha": "90f3547dcfd2d16d51f116cdd4e53527eeab1a57", "url": "https://github.com/getgauge/gauge-python/blob/90f3547dcfd2d16d51f116cdd4e53527eeab1a57/getgauge/parser_redbaron.py#L62-L80", "partition": "test"}
{"repo": "LuminosoInsight/luminoso-api-client-python", "path": "luminoso_api/v4_client.py", "func_name": "LuminosoClient._json_request", "original_string": "def _json_request(self, req_type, url, **kwargs):\n        \"\"\"\n        Make a request of the specified type and expect a JSON object in\n        response.\n\n        If the result has an 'error' value, raise a LuminosoAPIError with\n        its contents. Otherwise, return the contents of the 'result' value.\n        \"\"\"\n        response = self._request(req_type, url, **kwargs)\n        try:\n            json_response = response.json()\n        except ValueError:\n            logger.error(\"Received response with no JSON: %s %s\" %\n                         (response, response.content))\n            raise LuminosoError('Response body contained no JSON. '\n                                'Perhaps you meant to use get_raw?')\n        if json_response.get('error'):\n            raise LuminosoAPIError(json_response.get('error'))\n        return json_response['result']", "language": "python", "code": "def _json_request(self, req_type, url, **kwargs):\n        \"\"\"\n        Make a request of the specified type and expect a JSON object in\n        response.\n\n        If the result has an 'error' value, raise a LuminosoAPIError with\n        its contents. Otherwise, return the contents of the 'result' value.\n        \"\"\"\n        response = self._request(req_type, url, **kwargs)\n        try:\n            json_response = response.json()\n        except ValueError:\n            logger.error(\"Received response with no JSON: %s %s\" %\n                         (response, response.content))\n            raise LuminosoError('Response body contained no JSON. '\n                                'Perhaps you meant to use get_raw?')\n        if json_response.get('error'):\n            raise LuminosoAPIError(json_response.get('error'))\n        return json_response['result']", "code_tokens": ["def", "_json_request", "(", "self", ",", "req_type", ",", "url", ",", "*", "*", "kwargs", ")", ":", "response", "=", "self", ".", "_request", "(", "req_type", ",", "url", ",", "*", "*", "kwargs", ")", "try", ":", "json_response", "=", "response", ".", "json", "(", ")", "except", "ValueError", ":", "logger", ".", "error", "(", "\"Received response with no JSON: %s %s\"", "%", "(", "response", ",", "response", ".", "content", ")", ")", "raise", "LuminosoError", "(", "'Response body contained no JSON. '", "'Perhaps you meant to use get_raw?'", ")", "if", "json_response", ".", "get", "(", "'error'", ")", ":", "raise", "LuminosoAPIError", "(", "json_response", ".", "get", "(", "'error'", ")", ")", "return", "json_response", "[", "'result'", "]"], "docstring": "Make a request of the specified type and expect a JSON object in\n        response.\n\n        If the result has an 'error' value, raise a LuminosoAPIError with\n        its contents. Otherwise, return the contents of the 'result' value.", "docstring_tokens": ["Make", "a", "request", "of", "the", "specified", "type", "and", "expect", "a", "JSON", "object", "in", "response", "."], "sha": "3bedf2a454aee39214c11fbf556ead3eecc27881", "url": "https://github.com/LuminosoInsight/luminoso-api-client-python/blob/3bedf2a454aee39214c11fbf556ead3eecc27881/luminoso_api/v4_client.py#L196-L214", "partition": "test"}
{"repo": "reingart/gui2py", "path": "gui/tools/toolbox.py", "func_name": "set_drop_target", "original_string": "def set_drop_target(obj, root, designer, inspector):\n    \"Recursively create and set the drop target for obj and childs\"\n    if obj._meta.container:\n        dt = ToolBoxDropTarget(obj, root, designer=designer, \n                                          inspector=inspector)\n        obj.drop_target = dt\n    for child in obj:\n        set_drop_target(child, root, designer, inspector)", "language": "python", "code": "def set_drop_target(obj, root, designer, inspector):\n    \"Recursively create and set the drop target for obj and childs\"\n    if obj._meta.container:\n        dt = ToolBoxDropTarget(obj, root, designer=designer, \n                                          inspector=inspector)\n        obj.drop_target = dt\n    for child in obj:\n        set_drop_target(child, root, designer, inspector)", "code_tokens": ["def", "set_drop_target", "(", "obj", ",", "root", ",", "designer", ",", "inspector", ")", ":", "if", "obj", ".", "_meta", ".", "container", ":", "dt", "=", "ToolBoxDropTarget", "(", "obj", ",", "root", ",", "designer", "=", "designer", ",", "inspector", "=", "inspector", ")", "obj", ".", "drop_target", "=", "dt", "for", "child", "in", "obj", ":", "set_drop_target", "(", "child", ",", "root", ",", "designer", ",", "inspector", ")"], "docstring": "Recursively create and set the drop target for obj and childs", "docstring_tokens": ["Recursively", "create", "and", "set", "the", "drop", "target", "for", "obj", "and", "childs"], "sha": "aca0a05f6fcde55c94ad7cc058671a06608b01a4", "url": "https://github.com/reingart/gui2py/blob/aca0a05f6fcde55c94ad7cc058671a06608b01a4/gui/tools/toolbox.py#L227-L234", "partition": "test"}
{"repo": "elliterate/capybara.py", "path": "capybara/node/document_matchers.py", "func_name": "DocumentMatchersMixin.assert_no_title", "original_string": "def assert_no_title(self, title, **kwargs):\n        \"\"\"\n        Asserts that the page doesn't have the given title.\n\n        Args:\n            title (str | RegexObject): The string that the title should include.\n            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.\n        \"\"\"\n\n        query = TitleQuery(title, **kwargs)\n\n        @self.synchronize(wait=query.wait)\n        def assert_no_title():\n            if query.resolves_for(self):\n                raise ExpectationNotMet(query.negative_failure_message)\n\n            return True\n\n        return assert_no_title()", "language": "python", "code": "def assert_no_title(self, title, **kwargs):\n        \"\"\"\n        Asserts that the page doesn't have the given title.\n\n        Args:\n            title (str | RegexObject): The string that the title should include.\n            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.\n        \"\"\"\n\n        query = TitleQuery(title, **kwargs)\n\n        @self.synchronize(wait=query.wait)\n        def assert_no_title():\n            if query.resolves_for(self):\n                raise ExpectationNotMet(query.negative_failure_message)\n\n            return True\n\n        return assert_no_title()", "code_tokens": ["def", "assert_no_title", "(", "self", ",", "title", ",", "*", "*", "kwargs", ")", ":", "query", "=", "TitleQuery", "(", "title", ",", "*", "*", "kwargs", ")", "@", "self", ".", "synchronize", "(", "wait", "=", "query", ".", "wait", ")", "def", "assert_no_title", "(", ")", ":", "if", "query", ".", "resolves_for", "(", "self", ")", ":", "raise", "ExpectationNotMet", "(", "query", ".", "negative_failure_message", ")", "return", "True", "return", "assert_no_title", "(", ")"], "docstring": "Asserts that the page doesn't have the given title.\n\n        Args:\n            title (str | RegexObject): The string that the title should include.\n            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.", "docstring_tokens": ["Asserts", "that", "the", "page", "doesn", "t", "have", "the", "given", "title", "."], "sha": "0c6ae449cc37e4445ec3cd6af95674533beedc6c", "url": "https://github.com/elliterate/capybara.py/blob/0c6ae449cc37e4445ec3cd6af95674533beedc6c/capybara/node/document_matchers.py#L32-L56", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/code.py", "func_name": "log2ceil", "original_string": "def log2ceil(x):\n    \"\"\"\n    Returns no of bits required to store x-1\n    for example x=8 returns 3\n    \"\"\"\n\n    if not isinstance(x, (int, float)):\n        x = int(x)\n\n    if x == 0 or x == 1:\n        res = 1\n    else:\n        res = math.ceil(math.log2(x))\n\n    return hInt(res)", "language": "python", "code": "def log2ceil(x):\n    \"\"\"\n    Returns no of bits required to store x-1\n    for example x=8 returns 3\n    \"\"\"\n\n    if not isinstance(x, (int, float)):\n        x = int(x)\n\n    if x == 0 or x == 1:\n        res = 1\n    else:\n        res = math.ceil(math.log2(x))\n\n    return hInt(res)", "code_tokens": ["def", "log2ceil", "(", "x", ")", ":", "if", "not", "isinstance", "(", "x", ",", "(", "int", ",", "float", ")", ")", ":", "x", "=", "int", "(", "x", ")", "if", "x", "==", "0", "or", "x", "==", "1", ":", "res", "=", "1", "else", ":", "res", "=", "math", ".", "ceil", "(", "math", ".", "log2", "(", "x", ")", ")", "return", "hInt", "(", "res", ")"], "docstring": "Returns no of bits required to store x-1\n    for example x=8 returns 3", "docstring_tokens": ["Returns", "no", "of", "bits", "required", "to", "store", "x", "-", "1", "for", "example", "x", "=", "8", "returns", "3"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/code.py#L366-L380", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/groupsio.py", "func_name": "GroupsioClient.__find_group_id", "original_string": "def __find_group_id(self):\n        \"\"\"Find the id of a group given its name by iterating on the list of subscriptions\"\"\"\n\n        group_subscriptions = self.subscriptions(self.auth)\n\n        for subscriptions in group_subscriptions:\n            for sub in subscriptions:\n                if sub['group_name'] == self.group_name:\n                    return sub['group_id']\n\n        msg = \"Group id not found for group name %s\" % self.group_name\n        raise BackendError(cause=msg)", "language": "python", "code": "def __find_group_id(self):\n        \"\"\"Find the id of a group given its name by iterating on the list of subscriptions\"\"\"\n\n        group_subscriptions = self.subscriptions(self.auth)\n\n        for subscriptions in group_subscriptions:\n            for sub in subscriptions:\n                if sub['group_name'] == self.group_name:\n                    return sub['group_id']\n\n        msg = \"Group id not found for group name %s\" % self.group_name\n        raise BackendError(cause=msg)", "code_tokens": ["def", "__find_group_id", "(", "self", ")", ":", "group_subscriptions", "=", "self", ".", "subscriptions", "(", "self", ".", "auth", ")", "for", "subscriptions", "in", "group_subscriptions", ":", "for", "sub", "in", "subscriptions", ":", "if", "sub", "[", "'group_name'", "]", "==", "self", ".", "group_name", ":", "return", "sub", "[", "'group_id'", "]", "msg", "=", "\"Group id not found for group name %s\"", "%", "self", ".", "group_name", "raise", "BackendError", "(", "cause", "=", "msg", ")"], "docstring": "Find the id of a group given its name by iterating on the list of subscriptions", "docstring_tokens": ["Find", "the", "id", "of", "a", "group", "given", "its", "name", "by", "iterating", "on", "the", "list", "of", "subscriptions"], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/groupsio.py#L229-L240", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/aws_glue_catalog_hook.py", "func_name": "AwsGlueCatalogHook.get_table_location", "original_string": "def get_table_location(self, database_name, table_name):\n        \"\"\"\n        Get the physical location of the table\n\n        :param database_name: Name of hive database (schema) @table belongs to\n        :type database_name: str\n        :param table_name: Name of hive table\n        :type table_name: str\n        :return: str\n        \"\"\"\n\n        table = self.get_table(database_name, table_name)\n\n        return table['StorageDescriptor']['Location']", "language": "python", "code": "def get_table_location(self, database_name, table_name):\n        \"\"\"\n        Get the physical location of the table\n\n        :param database_name: Name of hive database (schema) @table belongs to\n        :type database_name: str\n        :param table_name: Name of hive table\n        :type table_name: str\n        :return: str\n        \"\"\"\n\n        table = self.get_table(database_name, table_name)\n\n        return table['StorageDescriptor']['Location']", "code_tokens": ["def", "get_table_location", "(", "self", ",", "database_name", ",", "table_name", ")", ":", "table", "=", "self", ".", "get_table", "(", "database_name", ",", "table_name", ")", "return", "table", "[", "'StorageDescriptor'", "]", "[", "'Location'", "]"], "docstring": "Get the physical location of the table\n\n        :param database_name: Name of hive database (schema) @table belongs to\n        :type database_name: str\n        :param table_name: Name of hive table\n        :type table_name: str\n        :return: str", "docstring_tokens": ["Get", "the", "physical", "location", "of", "the", "table"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_glue_catalog_hook.py#L139-L152", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/serializer/resourceAnalyzer/utils.py", "func_name": "ResourceContext.finalize", "original_string": "def finalize(self):\n        \"\"\"\n        Resolve ports of discovered memories\n        \"\"\"\n        ff_to_remove = 0\n        res = self.resources\n        for m, addrDict in self.memories.items():\n            rwSyncPorts, rSyncPorts, wSyncPorts = 0, 0, 0\n            rwAsyncPorts, rAsyncPorts, wAsyncPorts = 0, 0, 0\n            rSync_wAsyncPorts, rAsync_wSyncPorts = 0, 0\n\n            for _, (rSync, wSync, rAsync, wAsync) in addrDict.items():\n                if rSync:\n                    ff_to_remove += rSync * m._dtype.elmType.bit_length()\n\n                # resolve port count for this addr signal\n                rwSync = min(rSync, wSync)\n                rSync -= rwSync\n                wSync -= rwSync\n\n                rwAsync = min(rAsync, wAsync)\n                rAsync -= rwAsync\n                wAsync -= rwAsync\n\n                rSync_wAsync = min(rSync, wAsync)\n                rSync -= rSync_wAsync\n                wAsync -= rSync_wAsync\n\n                rAsync_wSync = min(rAsync, wSync)\n                rAsync -= rAsync_wSync\n                wSync -= rAsync_wSync\n\n                # update port counts for mem\n                rwSyncPorts += rwSync\n                rSyncPorts += rSync\n                wSyncPorts += wSync\n                rwAsyncPorts += rwAsync\n                rAsyncPorts += rAsync\n                wAsyncPorts += wAsync\n\n                rSync_wAsyncPorts += rSync_wAsync\n                rAsync_wSyncPorts += rAsync_wSync\n            k = ResourceRAM(m._dtype.elmType.bit_length(),\n                            int(m._dtype.size),\n                            rwSyncPorts, rSyncPorts, wSyncPorts,\n                            rSync_wAsyncPorts,\n                            rwAsyncPorts, rAsyncPorts, wAsyncPorts,\n                            rAsync_wSyncPorts)\n            res[k] = res.get(k, 0) + 1\n\n        self.memories.clear()\n\n        # remove register on read ports which will be merged into ram\n        if ff_to_remove:\n            ff_cnt = res[ResourceFF]\n            ff_cnt -= ff_to_remove\n            if ff_cnt:\n                res[ResourceFF] = ff_cnt\n            else:\n                del res[ResourceFF]", "language": "python", "code": "def finalize(self):\n        \"\"\"\n        Resolve ports of discovered memories\n        \"\"\"\n        ff_to_remove = 0\n        res = self.resources\n        for m, addrDict in self.memories.items():\n            rwSyncPorts, rSyncPorts, wSyncPorts = 0, 0, 0\n            rwAsyncPorts, rAsyncPorts, wAsyncPorts = 0, 0, 0\n            rSync_wAsyncPorts, rAsync_wSyncPorts = 0, 0\n\n            for _, (rSync, wSync, rAsync, wAsync) in addrDict.items():\n                if rSync:\n                    ff_to_remove += rSync * m._dtype.elmType.bit_length()\n\n                # resolve port count for this addr signal\n                rwSync = min(rSync, wSync)\n                rSync -= rwSync\n                wSync -= rwSync\n\n                rwAsync = min(rAsync, wAsync)\n                rAsync -= rwAsync\n                wAsync -= rwAsync\n\n                rSync_wAsync = min(rSync, wAsync)\n                rSync -= rSync_wAsync\n                wAsync -= rSync_wAsync\n\n                rAsync_wSync = min(rAsync, wSync)\n                rAsync -= rAsync_wSync\n                wSync -= rAsync_wSync\n\n                # update port counts for mem\n                rwSyncPorts += rwSync\n                rSyncPorts += rSync\n                wSyncPorts += wSync\n                rwAsyncPorts += rwAsync\n                rAsyncPorts += rAsync\n                wAsyncPorts += wAsync\n\n                rSync_wAsyncPorts += rSync_wAsync\n                rAsync_wSyncPorts += rAsync_wSync\n            k = ResourceRAM(m._dtype.elmType.bit_length(),\n                            int(m._dtype.size),\n                            rwSyncPorts, rSyncPorts, wSyncPorts,\n                            rSync_wAsyncPorts,\n                            rwAsyncPorts, rAsyncPorts, wAsyncPorts,\n                            rAsync_wSyncPorts)\n            res[k] = res.get(k, 0) + 1\n\n        self.memories.clear()\n\n        # remove register on read ports which will be merged into ram\n        if ff_to_remove:\n            ff_cnt = res[ResourceFF]\n            ff_cnt -= ff_to_remove\n            if ff_cnt:\n                res[ResourceFF] = ff_cnt\n            else:\n                del res[ResourceFF]", "code_tokens": ["def", "finalize", "(", "self", ")", ":", "ff_to_remove", "=", "0", "res", "=", "self", ".", "resources", "for", "m", ",", "addrDict", "in", "self", ".", "memories", ".", "items", "(", ")", ":", "rwSyncPorts", ",", "rSyncPorts", ",", "wSyncPorts", "=", "0", ",", "0", ",", "0", "rwAsyncPorts", ",", "rAsyncPorts", ",", "wAsyncPorts", "=", "0", ",", "0", ",", "0", "rSync_wAsyncPorts", ",", "rAsync_wSyncPorts", "=", "0", ",", "0", "for", "_", ",", "(", "rSync", ",", "wSync", ",", "rAsync", ",", "wAsync", ")", "in", "addrDict", ".", "items", "(", ")", ":", "if", "rSync", ":", "ff_to_remove", "+=", "rSync", "*", "m", ".", "_dtype", ".", "elmType", ".", "bit_length", "(", ")", "# resolve port count for this addr signal", "rwSync", "=", "min", "(", "rSync", ",", "wSync", ")", "rSync", "-=", "rwSync", "wSync", "-=", "rwSync", "rwAsync", "=", "min", "(", "rAsync", ",", "wAsync", ")", "rAsync", "-=", "rwAsync", "wAsync", "-=", "rwAsync", "rSync_wAsync", "=", "min", "(", "rSync", ",", "wAsync", ")", "rSync", "-=", "rSync_wAsync", "wAsync", "-=", "rSync_wAsync", "rAsync_wSync", "=", "min", "(", "rAsync", ",", "wSync", ")", "rAsync", "-=", "rAsync_wSync", "wSync", "-=", "rAsync_wSync", "# update port counts for mem", "rwSyncPorts", "+=", "rwSync", "rSyncPorts", "+=", "rSync", "wSyncPorts", "+=", "wSync", "rwAsyncPorts", "+=", "rwAsync", "rAsyncPorts", "+=", "rAsync", "wAsyncPorts", "+=", "wAsync", "rSync_wAsyncPorts", "+=", "rSync_wAsync", "rAsync_wSyncPorts", "+=", "rAsync_wSync", "k", "=", "ResourceRAM", "(", "m", ".", "_dtype", ".", "elmType", ".", "bit_length", "(", ")", ",", "int", "(", "m", ".", "_dtype", ".", "size", ")", ",", "rwSyncPorts", ",", "rSyncPorts", ",", "wSyncPorts", ",", "rSync_wAsyncPorts", ",", "rwAsyncPorts", ",", "rAsyncPorts", ",", "wAsyncPorts", ",", "rAsync_wSyncPorts", ")", "res", "[", "k", "]", "=", "res", ".", "get", "(", "k", ",", "0", ")", "+", "1", "self", ".", "memories", ".", "clear", "(", ")", "# remove register on read ports which will be merged into ram", "if", "ff_to_remove", ":", "ff_cnt", "=", "res", "[", "ResourceFF", "]", "ff_cnt", "-=", "ff_to_remove", "if", "ff_cnt", ":", "res", "[", "ResourceFF", "]", "=", "ff_cnt", "else", ":", "del", "res", "[", "ResourceFF", "]"], "docstring": "Resolve ports of discovered memories", "docstring_tokens": ["Resolve", "ports", "of", "discovered", "memories"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/serializer/resourceAnalyzer/utils.py#L98-L157", "partition": "test"}
{"repo": "jaraco/svg.charts", "path": "svg/charts/plot.py", "func_name": "Plot.get_single_axis_values", "original_string": "def get_single_axis_values(self, axis, dataset):\n\t\t\"\"\"\n\t\tReturn all the values for a single axis of the data.\n\t\t\"\"\"\n\t\tdata_index = getattr(self, '%s_data_index' % axis)\n\t\treturn [p[data_index] for p in dataset['data']]", "language": "python", "code": "def get_single_axis_values(self, axis, dataset):\n\t\t\"\"\"\n\t\tReturn all the values for a single axis of the data.\n\t\t\"\"\"\n\t\tdata_index = getattr(self, '%s_data_index' % axis)\n\t\treturn [p[data_index] for p in dataset['data']]", "code_tokens": ["def", "get_single_axis_values", "(", "self", ",", "axis", ",", "dataset", ")", ":", "data_index", "=", "getattr", "(", "self", ",", "'%s_data_index'", "%", "axis", ")", "return", "[", "p", "[", "data_index", "]", "for", "p", "in", "dataset", "[", "'data'", "]", "]"], "docstring": "Return all the values for a single axis of the data.", "docstring_tokens": ["Return", "all", "the", "values", "for", "a", "single", "axis", "of", "the", "data", "."], "sha": "23053497b3f1af4e760f355050107ae3bc05909d", "url": "https://github.com/jaraco/svg.charts/blob/23053497b3f1af4e760f355050107ae3bc05909d/svg/charts/plot.py#L195-L200", "partition": "test"}
{"repo": "zqfang/GSEApy", "path": "gseapy/gsea.py", "func_name": "GSEAbase._save_results", "original_string": "def _save_results(self, zipdata, outdir, module, gmt, rank_metric, permutation_type):\n        \"\"\"reformat gsea results, and save to txt\"\"\"\n\n        res = OrderedDict()\n        for gs, gseale, ind, RES in zipdata:\n            rdict = OrderedDict()\n            rdict['es'] = gseale[0]\n            rdict['nes'] = gseale[1]\n            rdict['pval'] = gseale[2]\n            rdict['fdr'] = gseale[3]\n            rdict['geneset_size'] = len(gmt[gs])\n            rdict['matched_size'] = len(ind)\n            #reformat gene list.\n            _genes = rank_metric.index.values[ind]\n            rdict['genes'] = \";\".join([ str(g).strip() for g in _genes ])\n            \n            if self.module != 'ssgsea':\n                # extract leading edge genes\n                if rdict['es'] > 0:\n                    # RES -> ndarray, ind -> list\n                    idx = RES.argmax()\n                    ldg_pos = list(filter(lambda x: x<= idx, ind))\n                elif rdict['es'] < 0:\n                    idx = RES.argmin()\n                    ldg_pos = list(filter(lambda x: x >= idx, ind))\n                else:\n                    ldg_pos = ind # es == 0 ?\n                rdict['ledge_genes'] = ';'.join(list(map(str,rank_metric.iloc[ldg_pos].index)))\n                \n            rdict['RES'] = RES\n            rdict['hits_indices'] = ind\n            # save to one odict\n            res[gs] = rdict\n        # save\n        self.results  = res\n        # save to dataframe\n        res_df = pd.DataFrame.from_dict(res, orient='index')\n        res_df.index.name = 'Term'\n        res_df.drop(['RES','hits_indices'], axis=1, inplace=True)\n        res_df.sort_values(by=['fdr','pval'], inplace=True)\n        self.res2d = res_df\n\n        if self._outdir is None: return\n        out = os.path.join(outdir,'gseapy.{b}.{c}.report.csv'.format(b=module, c=permutation_type))\n        if self.module == 'ssgsea':\n            out = out.replace(\".csv\",\".txt\")\n            with open(out, 'a') as f:\n                f.write('# normalize enrichment scores by random permutation procedure (GSEA method)\\n')\n                f.write(\"# might not proper for publication\\n\")\n                res_df.to_csv(f, sep='\\t')\n        else:\n            res_df.to_csv(out)\n\n        return", "language": "python", "code": "def _save_results(self, zipdata, outdir, module, gmt, rank_metric, permutation_type):\n        \"\"\"reformat gsea results, and save to txt\"\"\"\n\n        res = OrderedDict()\n        for gs, gseale, ind, RES in zipdata:\n            rdict = OrderedDict()\n            rdict['es'] = gseale[0]\n            rdict['nes'] = gseale[1]\n            rdict['pval'] = gseale[2]\n            rdict['fdr'] = gseale[3]\n            rdict['geneset_size'] = len(gmt[gs])\n            rdict['matched_size'] = len(ind)\n            #reformat gene list.\n            _genes = rank_metric.index.values[ind]\n            rdict['genes'] = \";\".join([ str(g).strip() for g in _genes ])\n            \n            if self.module != 'ssgsea':\n                # extract leading edge genes\n                if rdict['es'] > 0:\n                    # RES -> ndarray, ind -> list\n                    idx = RES.argmax()\n                    ldg_pos = list(filter(lambda x: x<= idx, ind))\n                elif rdict['es'] < 0:\n                    idx = RES.argmin()\n                    ldg_pos = list(filter(lambda x: x >= idx, ind))\n                else:\n                    ldg_pos = ind # es == 0 ?\n                rdict['ledge_genes'] = ';'.join(list(map(str,rank_metric.iloc[ldg_pos].index)))\n                \n            rdict['RES'] = RES\n            rdict['hits_indices'] = ind\n            # save to one odict\n            res[gs] = rdict\n        # save\n        self.results  = res\n        # save to dataframe\n        res_df = pd.DataFrame.from_dict(res, orient='index')\n        res_df.index.name = 'Term'\n        res_df.drop(['RES','hits_indices'], axis=1, inplace=True)\n        res_df.sort_values(by=['fdr','pval'], inplace=True)\n        self.res2d = res_df\n\n        if self._outdir is None: return\n        out = os.path.join(outdir,'gseapy.{b}.{c}.report.csv'.format(b=module, c=permutation_type))\n        if self.module == 'ssgsea':\n            out = out.replace(\".csv\",\".txt\")\n            with open(out, 'a') as f:\n                f.write('# normalize enrichment scores by random permutation procedure (GSEA method)\\n')\n                f.write(\"# might not proper for publication\\n\")\n                res_df.to_csv(f, sep='\\t')\n        else:\n            res_df.to_csv(out)\n\n        return", "code_tokens": ["def", "_save_results", "(", "self", ",", "zipdata", ",", "outdir", ",", "module", ",", "gmt", ",", "rank_metric", ",", "permutation_type", ")", ":", "res", "=", "OrderedDict", "(", ")", "for", "gs", ",", "gseale", ",", "ind", ",", "RES", "in", "zipdata", ":", "rdict", "=", "OrderedDict", "(", ")", "rdict", "[", "'es'", "]", "=", "gseale", "[", "0", "]", "rdict", "[", "'nes'", "]", "=", "gseale", "[", "1", "]", "rdict", "[", "'pval'", "]", "=", "gseale", "[", "2", "]", "rdict", "[", "'fdr'", "]", "=", "gseale", "[", "3", "]", "rdict", "[", "'geneset_size'", "]", "=", "len", "(", "gmt", "[", "gs", "]", ")", "rdict", "[", "'matched_size'", "]", "=", "len", "(", "ind", ")", "#reformat gene list.", "_genes", "=", "rank_metric", ".", "index", ".", "values", "[", "ind", "]", "rdict", "[", "'genes'", "]", "=", "\";\"", ".", "join", "(", "[", "str", "(", "g", ")", ".", "strip", "(", ")", "for", "g", "in", "_genes", "]", ")", "if", "self", ".", "module", "!=", "'ssgsea'", ":", "# extract leading edge genes", "if", "rdict", "[", "'es'", "]", ">", "0", ":", "# RES -> ndarray, ind -> list", "idx", "=", "RES", ".", "argmax", "(", ")", "ldg_pos", "=", "list", "(", "filter", "(", "lambda", "x", ":", "x", "<=", "idx", ",", "ind", ")", ")", "elif", "rdict", "[", "'es'", "]", "<", "0", ":", "idx", "=", "RES", ".", "argmin", "(", ")", "ldg_pos", "=", "list", "(", "filter", "(", "lambda", "x", ":", "x", ">=", "idx", ",", "ind", ")", ")", "else", ":", "ldg_pos", "=", "ind", "# es == 0 ?", "rdict", "[", "'ledge_genes'", "]", "=", "';'", ".", "join", "(", "list", "(", "map", "(", "str", ",", "rank_metric", ".", "iloc", "[", "ldg_pos", "]", ".", "index", ")", ")", ")", "rdict", "[", "'RES'", "]", "=", "RES", "rdict", "[", "'hits_indices'", "]", "=", "ind", "# save to one odict", "res", "[", "gs", "]", "=", "rdict", "# save", "self", ".", "results", "=", "res", "# save to dataframe", "res_df", "=", "pd", ".", "DataFrame", ".", "from_dict", "(", "res", ",", "orient", "=", "'index'", ")", "res_df", ".", "index", ".", "name", "=", "'Term'", "res_df", ".", "drop", "(", "[", "'RES'", ",", "'hits_indices'", "]", ",", "axis", "=", "1", ",", "inplace", "=", "True", ")", "res_df", ".", "sort_values", "(", "by", "=", "[", "'fdr'", ",", "'pval'", "]", ",", "inplace", "=", "True", ")", "self", ".", "res2d", "=", "res_df", "if", "self", ".", "_outdir", "is", "None", ":", "return", "out", "=", "os", ".", "path", ".", "join", "(", "outdir", ",", "'gseapy.{b}.{c}.report.csv'", ".", "format", "(", "b", "=", "module", ",", "c", "=", "permutation_type", ")", ")", "if", "self", ".", "module", "==", "'ssgsea'", ":", "out", "=", "out", ".", "replace", "(", "\".csv\"", ",", "\".txt\"", ")", "with", "open", "(", "out", ",", "'a'", ")", "as", "f", ":", "f", ".", "write", "(", "'# normalize enrichment scores by random permutation procedure (GSEA method)\\n'", ")", "f", ".", "write", "(", "\"# might not proper for publication\\n\"", ")", "res_df", ".", "to_csv", "(", "f", ",", "sep", "=", "'\\t'", ")", "else", ":", "res_df", ".", "to_csv", "(", "out", ")", "return"], "docstring": "reformat gsea results, and save to txt", "docstring_tokens": ["reformat", "gsea", "results", "and", "save", "to", "txt"], "sha": "673e9ec1391e3b14d3e8a4353117151fd2cb9345", "url": "https://github.com/zqfang/GSEApy/blob/673e9ec1391e3b14d3e8a4353117151fd2cb9345/gseapy/gsea.py#L259-L312", "partition": "test"}
{"repo": "StaticCube/python-synology", "path": "SynologyDSM/SynologyDSM.py", "func_name": "SynoStorage.volume_disk_temp_max", "original_string": "def volume_disk_temp_max(self, volume):\r\n        \"\"\"Maximum temperature of all disks making up the volume\"\"\"\r\n        volume = self._get_volume(volume)\r\n        if volume is not None:\r\n            vol_disks = volume[\"disks\"]\r\n            if vol_disks is not None:\r\n                max_temp = 0\r\n\r\n                for vol_disk in vol_disks:\r\n                    disk_temp = self.disk_temp(vol_disk)\r\n                    if disk_temp is not None and disk_temp > max_temp:\r\n                        max_temp = disk_temp\r\n\r\n                return max_temp", "language": "python", "code": "def volume_disk_temp_max(self, volume):\r\n        \"\"\"Maximum temperature of all disks making up the volume\"\"\"\r\n        volume = self._get_volume(volume)\r\n        if volume is not None:\r\n            vol_disks = volume[\"disks\"]\r\n            if vol_disks is not None:\r\n                max_temp = 0\r\n\r\n                for vol_disk in vol_disks:\r\n                    disk_temp = self.disk_temp(vol_disk)\r\n                    if disk_temp is not None and disk_temp > max_temp:\r\n                        max_temp = disk_temp\r\n\r\n                return max_temp", "code_tokens": ["def", "volume_disk_temp_max", "(", "self", ",", "volume", ")", ":", "volume", "=", "self", ".", "_get_volume", "(", "volume", ")", "if", "volume", "is", "not", "None", ":", "vol_disks", "=", "volume", "[", "\"disks\"", "]", "if", "vol_disks", "is", "not", "None", ":", "max_temp", "=", "0", "for", "vol_disk", "in", "vol_disks", ":", "disk_temp", "=", "self", ".", "disk_temp", "(", "vol_disk", ")", "if", "disk_temp", "is", "not", "None", "and", "disk_temp", ">", "max_temp", ":", "max_temp", "=", "disk_temp", "return", "max_temp"], "docstring": "Maximum temperature of all disks making up the volume", "docstring_tokens": ["Maximum", "temperature", "of", "all", "disks", "making", "up", "the", "volume"], "sha": "a5446a052fc91a38f7589803dc7a654180db2566", "url": "https://github.com/StaticCube/python-synology/blob/a5446a052fc91a38f7589803dc7a654180db2566/SynologyDSM/SynologyDSM.py#L298-L311", "partition": "test"}
{"repo": "jazzband/django-ddp", "path": "dddp/accounts/ddp.py", "func_name": "Auth.do_login", "original_string": "def do_login(self, user):\n        \"\"\"Login a user.\"\"\"\n        this.user_id = user.pk\n        this.user_ddp_id = get_meteor_id(user)\n        # silent subscription (sans sub/nosub msg) to LoggedInUser pub\n        this.user_sub_id = meteor_random_id()\n        API.do_sub(this.user_sub_id, 'LoggedInUser', silent=True)\n        self.update_subs(user.pk)\n        user_logged_in.send(\n            sender=user.__class__, request=this.request, user=user,\n        )", "language": "python", "code": "def do_login(self, user):\n        \"\"\"Login a user.\"\"\"\n        this.user_id = user.pk\n        this.user_ddp_id = get_meteor_id(user)\n        # silent subscription (sans sub/nosub msg) to LoggedInUser pub\n        this.user_sub_id = meteor_random_id()\n        API.do_sub(this.user_sub_id, 'LoggedInUser', silent=True)\n        self.update_subs(user.pk)\n        user_logged_in.send(\n            sender=user.__class__, request=this.request, user=user,\n        )", "code_tokens": ["def", "do_login", "(", "self", ",", "user", ")", ":", "this", ".", "user_id", "=", "user", ".", "pk", "this", ".", "user_ddp_id", "=", "get_meteor_id", "(", "user", ")", "# silent subscription (sans sub/nosub msg) to LoggedInUser pub", "this", ".", "user_sub_id", "=", "meteor_random_id", "(", ")", "API", ".", "do_sub", "(", "this", ".", "user_sub_id", ",", "'LoggedInUser'", ",", "silent", "=", "True", ")", "self", ".", "update_subs", "(", "user", ".", "pk", ")", "user_logged_in", ".", "send", "(", "sender", "=", "user", ".", "__class__", ",", "request", "=", "this", ".", "request", ",", "user", "=", "user", ",", ")"], "docstring": "Login a user.", "docstring_tokens": ["Login", "a", "user", "."], "sha": "1e1954b06fe140346acea43582515991685e4e01", "url": "https://github.com/jazzband/django-ddp/blob/1e1954b06fe140346acea43582515991685e4e01/dddp/accounts/ddp.py#L426-L436", "partition": "test"}
{"repo": "solvebio/solvebio-python", "path": "solvebio/resource/object.py", "func_name": "Object.validate_full_path", "original_string": "def validate_full_path(cls, full_path, **kwargs):\n        \"\"\"Helper method to parse a full or partial path and\n        return a full path as well as a dict containing path parts.\n\n        Uses the following rules when processing the path:\n\n            * If no domain, uses the current user's account domain\n            * If no vault, uses the current user's personal vault.\n            * If no path, uses '/' (vault root)\n\n        Returns a tuple containing:\n\n            * The validated full_path\n            * A dictionary with the components:\n                * domain: the domain of the vault\n                * vault: the name of the vault, without domain\n                * vault_full_path: domain:vault\n                * path: the object path within the vault\n                * parent_path: the parent path to the object\n                * filename: the object's filename (if any)\n                * full_path: the validated full path\n\n        The following components may be overridden using kwargs:\n\n            * vault\n            * path\n\n        Object paths (also known as \"paths\") must begin with a forward slash.\n\n        The following path formats are supported:\n\n            domain:vault:/path -> object \"path\" in the root of \"domain:vault\"\n            domain:vault/path  -> object \"path\" in the root of \"domain:vault\"\n            vault:/path        -> object \"path\" in the root of \"vault\"\n            vault/path         -> object \"path\" in the root of \"vault\"\n            ~/path             -> object \"path\" in the root of personal vault\n            vault/             -> root of \"vault\"\n            ~/                 -> root of your personal vault\n\n        The following two formats are not supported:\n\n            path               -> invalid/ambiguous path (exception)\n            vault:path         -> invalid/ambiguous path (exception)\n            vault:path/path    -> unsupported, interpreted as domain:vault/path\n\n        \"\"\"\n        from solvebio.resource.vault import Vault\n\n        _client = kwargs.pop('client', None) or cls._client or client\n\n        if not full_path:\n            raise Exception(\n                'Invalid path: ',\n                'Full path must be in one of the following formats: '\n                '\"vault:/path\", \"domain:vault:/path\", or \"~/path\"')\n\n        # Parse the vault's full_path, using overrides if any\n        input_vault = kwargs.get('vault') or full_path\n        try:\n            vault_full_path, path_dict = \\\n                Vault.validate_full_path(input_vault, client=_client)\n        except Exception as err:\n            raise Exception('Could not determine vault from \"{0}\": {1}'\n                            .format(input_vault, err))\n\n        if kwargs.get('path'):\n            # Allow override of the object_path.\n            full_path = '{0}:/{1}'.format(vault_full_path, kwargs['path'])\n\n        match = cls.PATH_RE.match(full_path)\n        if match:\n            object_path = match.groupdict()['path']\n        else:\n            raise Exception(\n                'Cannot find a valid object path in \"{0}\". '\n                'Full path must be in one of the following formats: '\n                '\"vault:/path\", \"domain:vault:/path\", or \"~/path\"'\n                .format(full_path))\n\n        # Remove double slashes\n        object_path = re.sub('//+', '/', object_path)\n        if object_path != '/':\n            # Remove trailing slash\n            object_path = object_path.rstrip('/')\n\n        path_dict['path'] = object_path\n        # TODO: parent_path and filename\n        full_path = '{domain}:{vault}:{path}'.format(**path_dict)\n        path_dict['full_path'] = full_path\n        return full_path, path_dict", "language": "python", "code": "def validate_full_path(cls, full_path, **kwargs):\n        \"\"\"Helper method to parse a full or partial path and\n        return a full path as well as a dict containing path parts.\n\n        Uses the following rules when processing the path:\n\n            * If no domain, uses the current user's account domain\n            * If no vault, uses the current user's personal vault.\n            * If no path, uses '/' (vault root)\n\n        Returns a tuple containing:\n\n            * The validated full_path\n            * A dictionary with the components:\n                * domain: the domain of the vault\n                * vault: the name of the vault, without domain\n                * vault_full_path: domain:vault\n                * path: the object path within the vault\n                * parent_path: the parent path to the object\n                * filename: the object's filename (if any)\n                * full_path: the validated full path\n\n        The following components may be overridden using kwargs:\n\n            * vault\n            * path\n\n        Object paths (also known as \"paths\") must begin with a forward slash.\n\n        The following path formats are supported:\n\n            domain:vault:/path -> object \"path\" in the root of \"domain:vault\"\n            domain:vault/path  -> object \"path\" in the root of \"domain:vault\"\n            vault:/path        -> object \"path\" in the root of \"vault\"\n            vault/path         -> object \"path\" in the root of \"vault\"\n            ~/path             -> object \"path\" in the root of personal vault\n            vault/             -> root of \"vault\"\n            ~/                 -> root of your personal vault\n\n        The following two formats are not supported:\n\n            path               -> invalid/ambiguous path (exception)\n            vault:path         -> invalid/ambiguous path (exception)\n            vault:path/path    -> unsupported, interpreted as domain:vault/path\n\n        \"\"\"\n        from solvebio.resource.vault import Vault\n\n        _client = kwargs.pop('client', None) or cls._client or client\n\n        if not full_path:\n            raise Exception(\n                'Invalid path: ',\n                'Full path must be in one of the following formats: '\n                '\"vault:/path\", \"domain:vault:/path\", or \"~/path\"')\n\n        # Parse the vault's full_path, using overrides if any\n        input_vault = kwargs.get('vault') or full_path\n        try:\n            vault_full_path, path_dict = \\\n                Vault.validate_full_path(input_vault, client=_client)\n        except Exception as err:\n            raise Exception('Could not determine vault from \"{0}\": {1}'\n                            .format(input_vault, err))\n\n        if kwargs.get('path'):\n            # Allow override of the object_path.\n            full_path = '{0}:/{1}'.format(vault_full_path, kwargs['path'])\n\n        match = cls.PATH_RE.match(full_path)\n        if match:\n            object_path = match.groupdict()['path']\n        else:\n            raise Exception(\n                'Cannot find a valid object path in \"{0}\". '\n                'Full path must be in one of the following formats: '\n                '\"vault:/path\", \"domain:vault:/path\", or \"~/path\"'\n                .format(full_path))\n\n        # Remove double slashes\n        object_path = re.sub('//+', '/', object_path)\n        if object_path != '/':\n            # Remove trailing slash\n            object_path = object_path.rstrip('/')\n\n        path_dict['path'] = object_path\n        # TODO: parent_path and filename\n        full_path = '{domain}:{vault}:{path}'.format(**path_dict)\n        path_dict['full_path'] = full_path\n        return full_path, path_dict", "code_tokens": ["def", "validate_full_path", "(", "cls", ",", "full_path", ",", "*", "*", "kwargs", ")", ":", "from", "solvebio", ".", "resource", ".", "vault", "import", "Vault", "_client", "=", "kwargs", ".", "pop", "(", "'client'", ",", "None", ")", "or", "cls", ".", "_client", "or", "client", "if", "not", "full_path", ":", "raise", "Exception", "(", "'Invalid path: '", ",", "'Full path must be in one of the following formats: '", "'\"vault:/path\", \"domain:vault:/path\", or \"~/path\"'", ")", "# Parse the vault's full_path, using overrides if any", "input_vault", "=", "kwargs", ".", "get", "(", "'vault'", ")", "or", "full_path", "try", ":", "vault_full_path", ",", "path_dict", "=", "Vault", ".", "validate_full_path", "(", "input_vault", ",", "client", "=", "_client", ")", "except", "Exception", "as", "err", ":", "raise", "Exception", "(", "'Could not determine vault from \"{0}\": {1}'", ".", "format", "(", "input_vault", ",", "err", ")", ")", "if", "kwargs", ".", "get", "(", "'path'", ")", ":", "# Allow override of the object_path.", "full_path", "=", "'{0}:/{1}'", ".", "format", "(", "vault_full_path", ",", "kwargs", "[", "'path'", "]", ")", "match", "=", "cls", ".", "PATH_RE", ".", "match", "(", "full_path", ")", "if", "match", ":", "object_path", "=", "match", ".", "groupdict", "(", ")", "[", "'path'", "]", "else", ":", "raise", "Exception", "(", "'Cannot find a valid object path in \"{0}\". '", "'Full path must be in one of the following formats: '", "'\"vault:/path\", \"domain:vault:/path\", or \"~/path\"'", ".", "format", "(", "full_path", ")", ")", "# Remove double slashes", "object_path", "=", "re", ".", "sub", "(", "'//+'", ",", "'/'", ",", "object_path", ")", "if", "object_path", "!=", "'/'", ":", "# Remove trailing slash", "object_path", "=", "object_path", ".", "rstrip", "(", "'/'", ")", "path_dict", "[", "'path'", "]", "=", "object_path", "# TODO: parent_path and filename", "full_path", "=", "'{domain}:{vault}:{path}'", ".", "format", "(", "*", "*", "path_dict", ")", "path_dict", "[", "'full_path'", "]", "=", "full_path", "return", "full_path", ",", "path_dict"], "docstring": "Helper method to parse a full or partial path and\n        return a full path as well as a dict containing path parts.\n\n        Uses the following rules when processing the path:\n\n            * If no domain, uses the current user's account domain\n            * If no vault, uses the current user's personal vault.\n            * If no path, uses '/' (vault root)\n\n        Returns a tuple containing:\n\n            * The validated full_path\n            * A dictionary with the components:\n                * domain: the domain of the vault\n                * vault: the name of the vault, without domain\n                * vault_full_path: domain:vault\n                * path: the object path within the vault\n                * parent_path: the parent path to the object\n                * filename: the object's filename (if any)\n                * full_path: the validated full path\n\n        The following components may be overridden using kwargs:\n\n            * vault\n            * path\n\n        Object paths (also known as \"paths\") must begin with a forward slash.\n\n        The following path formats are supported:\n\n            domain:vault:/path -> object \"path\" in the root of \"domain:vault\"\n            domain:vault/path  -> object \"path\" in the root of \"domain:vault\"\n            vault:/path        -> object \"path\" in the root of \"vault\"\n            vault/path         -> object \"path\" in the root of \"vault\"\n            ~/path             -> object \"path\" in the root of personal vault\n            vault/             -> root of \"vault\"\n            ~/                 -> root of your personal vault\n\n        The following two formats are not supported:\n\n            path               -> invalid/ambiguous path (exception)\n            vault:path         -> invalid/ambiguous path (exception)\n            vault:path/path    -> unsupported, interpreted as domain:vault/path", "docstring_tokens": ["Helper", "method", "to", "parse", "a", "full", "or", "partial", "path", "and", "return", "a", "full", "path", "as", "well", "as", "a", "dict", "containing", "path", "parts", "."], "sha": "b29614643043afd19c1d8074e8f25c6700d51a73", "url": "https://github.com/solvebio/solvebio-python/blob/b29614643043afd19c1d8074e8f25c6700d51a73/solvebio/resource/object.py#L46-L135", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/debug.py", "func_name": "DebugControl.write", "original_string": "def write(self, msg):\n        \"\"\"Write a line of debug output.\"\"\"\n        if self.should('pid'):\n            msg = \"pid %5d: %s\" % (os.getpid(), msg)\n        self.output.write(msg+\"\\n\")\n        self.output.flush()", "language": "python", "code": "def write(self, msg):\n        \"\"\"Write a line of debug output.\"\"\"\n        if self.should('pid'):\n            msg = \"pid %5d: %s\" % (os.getpid(), msg)\n        self.output.write(msg+\"\\n\")\n        self.output.flush()", "code_tokens": ["def", "write", "(", "self", ",", "msg", ")", ":", "if", "self", ".", "should", "(", "'pid'", ")", ":", "msg", "=", "\"pid %5d: %s\"", "%", "(", "os", ".", "getpid", "(", ")", ",", "msg", ")", "self", ".", "output", ".", "write", "(", "msg", "+", "\"\\n\"", ")", "self", ".", "output", ".", "flush", "(", ")"], "docstring": "Write a line of debug output.", "docstring_tokens": ["Write", "a", "line", "of", "debug", "output", "."], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/debug.py#L24-L29", "partition": "test"}
{"repo": "open-mmlab/mmcv", "path": "mmcv/image/transforms/geometry.py", "func_name": "impad_to_multiple", "original_string": "def impad_to_multiple(img, divisor, pad_val=0):\n    \"\"\"Pad an image to ensure each edge to be multiple to some number.\n\n    Args:\n        img (ndarray): Image to be padded.\n        divisor (int): Padded image edges will be multiple to divisor.\n        pad_val (number or sequence): Same as :func:`impad`.\n\n    Returns:\n        ndarray: The padded image.\n    \"\"\"\n    pad_h = int(np.ceil(img.shape[0] / divisor)) * divisor\n    pad_w = int(np.ceil(img.shape[1] / divisor)) * divisor\n    return impad(img, (pad_h, pad_w), pad_val)", "language": "python", "code": "def impad_to_multiple(img, divisor, pad_val=0):\n    \"\"\"Pad an image to ensure each edge to be multiple to some number.\n\n    Args:\n        img (ndarray): Image to be padded.\n        divisor (int): Padded image edges will be multiple to divisor.\n        pad_val (number or sequence): Same as :func:`impad`.\n\n    Returns:\n        ndarray: The padded image.\n    \"\"\"\n    pad_h = int(np.ceil(img.shape[0] / divisor)) * divisor\n    pad_w = int(np.ceil(img.shape[1] / divisor)) * divisor\n    return impad(img, (pad_h, pad_w), pad_val)", "code_tokens": ["def", "impad_to_multiple", "(", "img", ",", "divisor", ",", "pad_val", "=", "0", ")", ":", "pad_h", "=", "int", "(", "np", ".", "ceil", "(", "img", ".", "shape", "[", "0", "]", "/", "divisor", ")", ")", "*", "divisor", "pad_w", "=", "int", "(", "np", ".", "ceil", "(", "img", ".", "shape", "[", "1", "]", "/", "divisor", ")", ")", "*", "divisor", "return", "impad", "(", "img", ",", "(", "pad_h", ",", "pad_w", ")", ",", "pad_val", ")"], "docstring": "Pad an image to ensure each edge to be multiple to some number.\n\n    Args:\n        img (ndarray): Image to be padded.\n        divisor (int): Padded image edges will be multiple to divisor.\n        pad_val (number or sequence): Same as :func:`impad`.\n\n    Returns:\n        ndarray: The padded image.", "docstring_tokens": ["Pad", "an", "image", "to", "ensure", "each", "edge", "to", "be", "multiple", "to", "some", "number", "."], "sha": "0d77f61450aab4dde8b8585a577cc496acb95d7f", "url": "https://github.com/open-mmlab/mmcv/blob/0d77f61450aab4dde8b8585a577cc496acb95d7f/mmcv/image/transforms/geometry.py#L190-L203", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/backend/connection.py", "func_name": "H2OConnection._print", "original_string": "def _print(self, msg, flush=False, end=\"\\n\"):\n        \"\"\"Helper function to print connection status messages when in verbose mode.\"\"\"\n        if self._verbose:\n            print2(msg, end=end, flush=flush)", "language": "python", "code": "def _print(self, msg, flush=False, end=\"\\n\"):\n        \"\"\"Helper function to print connection status messages when in verbose mode.\"\"\"\n        if self._verbose:\n            print2(msg, end=end, flush=flush)", "code_tokens": ["def", "_print", "(", "self", ",", "msg", ",", "flush", "=", "False", ",", "end", "=", "\"\\n\"", ")", ":", "if", "self", ".", "_verbose", ":", "print2", "(", "msg", ",", "end", "=", "end", ",", "flush", "=", "flush", ")"], "docstring": "Helper function to print connection status messages when in verbose mode.", "docstring_tokens": ["Helper", "function", "to", "print", "connection", "status", "messages", "when", "in", "verbose", "mode", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/backend/connection.py#L758-L761", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicefabric/azure/servicefabric/service_fabric_client_ap_is.py", "func_name": "ServiceFabricClientAPIs.unprovision_application_type", "original_string": "def unprovision_application_type(\n            self, application_type_name, application_type_version, timeout=60, async_parameter=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Removes or unregisters a Service Fabric application type from the\n        cluster.\n\n        This operation can only be performed if all application instances of\n        the application type have been deleted. Once the application type is\n        unregistered, no new application instances can be created for this\n        particular application type.\n\n        :param application_type_name: The name of the application type.\n        :type application_type_name: str\n        :param application_type_version: The version of the application type\n         as defined in the application manifest.\n        :type application_type_version: str\n        :param timeout: The server timeout for performing the operation in\n         seconds. This timeout specifies the time duration that the client is\n         willing to wait for the requested operation to complete. The default\n         value for this parameter is 60 seconds.\n        :type timeout: long\n        :param async_parameter: The flag indicating whether or not unprovision\n         should occur asynchronously. When set to true, the unprovision\n         operation returns when the request is accepted by the system, and the\n         unprovision operation continues without any timeout limit. The default\n         value is false. However, we recommend setting it to true for large\n         application packages that were provisioned.\n        :type async_parameter: bool\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`\n        \"\"\"\n        unprovision_application_type_description_info = models.UnprovisionApplicationTypeDescriptionInfo(application_type_version=application_type_version, async_property=async_parameter)\n\n        api_version = \"6.0\"\n\n        # Construct URL\n        url = self.unprovision_application_type.metadata['url']\n        path_format_arguments = {\n            'applicationTypeName': self._serialize.url(\"application_type_name\", application_type_name, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n        if timeout is not None:\n            query_parameters['timeout'] = self._serialize.query(\"timeout\", timeout, 'long', maximum=4294967295, minimum=1)\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(unprovision_application_type_description_info, 'UnprovisionApplicationTypeDescriptionInfo')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200, 202]:\n            raise models.FabricErrorException(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response", "language": "python", "code": "def unprovision_application_type(\n            self, application_type_name, application_type_version, timeout=60, async_parameter=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Removes or unregisters a Service Fabric application type from the\n        cluster.\n\n        This operation can only be performed if all application instances of\n        the application type have been deleted. Once the application type is\n        unregistered, no new application instances can be created for this\n        particular application type.\n\n        :param application_type_name: The name of the application type.\n        :type application_type_name: str\n        :param application_type_version: The version of the application type\n         as defined in the application manifest.\n        :type application_type_version: str\n        :param timeout: The server timeout for performing the operation in\n         seconds. This timeout specifies the time duration that the client is\n         willing to wait for the requested operation to complete. The default\n         value for this parameter is 60 seconds.\n        :type timeout: long\n        :param async_parameter: The flag indicating whether or not unprovision\n         should occur asynchronously. When set to true, the unprovision\n         operation returns when the request is accepted by the system, and the\n         unprovision operation continues without any timeout limit. The default\n         value is false. However, we recommend setting it to true for large\n         application packages that were provisioned.\n        :type async_parameter: bool\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`\n        \"\"\"\n        unprovision_application_type_description_info = models.UnprovisionApplicationTypeDescriptionInfo(application_type_version=application_type_version, async_property=async_parameter)\n\n        api_version = \"6.0\"\n\n        # Construct URL\n        url = self.unprovision_application_type.metadata['url']\n        path_format_arguments = {\n            'applicationTypeName': self._serialize.url(\"application_type_name\", application_type_name, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n        if timeout is not None:\n            query_parameters['timeout'] = self._serialize.query(\"timeout\", timeout, 'long', maximum=4294967295, minimum=1)\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(unprovision_application_type_description_info, 'UnprovisionApplicationTypeDescriptionInfo')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200, 202]:\n            raise models.FabricErrorException(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response", "code_tokens": ["def", "unprovision_application_type", "(", "self", ",", "application_type_name", ",", "application_type_version", ",", "timeout", "=", "60", ",", "async_parameter", "=", "None", ",", "custom_headers", "=", "None", ",", "raw", "=", "False", ",", "*", "*", "operation_config", ")", ":", "unprovision_application_type_description_info", "=", "models", ".", "UnprovisionApplicationTypeDescriptionInfo", "(", "application_type_version", "=", "application_type_version", ",", "async_property", "=", "async_parameter", ")", "api_version", "=", "\"6.0\"", "# Construct URL", "url", "=", "self", ".", "unprovision_application_type", ".", "metadata", "[", "'url'", "]", "path_format_arguments", "=", "{", "'applicationTypeName'", ":", "self", ".", "_serialize", ".", "url", "(", "\"application_type_name\"", ",", "application_type_name", ",", "'str'", ")", "}", "url", "=", "self", ".", "_client", ".", "format_url", "(", "url", ",", "*", "*", "path_format_arguments", ")", "# Construct parameters", "query_parameters", "=", "{", "}", "query_parameters", "[", "'api-version'", "]", "=", "self", ".", "_serialize", ".", "query", "(", "\"api_version\"", ",", "api_version", ",", "'str'", ")", "if", "timeout", "is", "not", "None", ":", "query_parameters", "[", "'timeout'", "]", "=", "self", ".", "_serialize", ".", "query", "(", "\"timeout\"", ",", "timeout", ",", "'long'", ",", "maximum", "=", "4294967295", ",", "minimum", "=", "1", ")", "# Construct headers", "header_parameters", "=", "{", "}", "header_parameters", "[", "'Content-Type'", "]", "=", "'application/json; charset=utf-8'", "if", "custom_headers", ":", "header_parameters", ".", "update", "(", "custom_headers", ")", "# Construct body", "body_content", "=", "self", ".", "_serialize", ".", "body", "(", "unprovision_application_type_description_info", ",", "'UnprovisionApplicationTypeDescriptionInfo'", ")", "# Construct and send request", "request", "=", "self", ".", "_client", ".", "post", "(", "url", ",", "query_parameters", ",", "header_parameters", ",", "body_content", ")", "response", "=", "self", ".", "_client", ".", "send", "(", "request", ",", "stream", "=", "False", ",", "*", "*", "operation_config", ")", "if", "response", ".", "status_code", "not", "in", "[", "200", ",", "202", "]", ":", "raise", "models", ".", "FabricErrorException", "(", "self", ".", "_deserialize", ",", "response", ")", "if", "raw", ":", "client_raw_response", "=", "ClientRawResponse", "(", "None", ",", "response", ")", "return", "client_raw_response"], "docstring": "Removes or unregisters a Service Fabric application type from the\n        cluster.\n\n        This operation can only be performed if all application instances of\n        the application type have been deleted. Once the application type is\n        unregistered, no new application instances can be created for this\n        particular application type.\n\n        :param application_type_name: The name of the application type.\n        :type application_type_name: str\n        :param application_type_version: The version of the application type\n         as defined in the application manifest.\n        :type application_type_version: str\n        :param timeout: The server timeout for performing the operation in\n         seconds. This timeout specifies the time duration that the client is\n         willing to wait for the requested operation to complete. The default\n         value for this parameter is 60 seconds.\n        :type timeout: long\n        :param async_parameter: The flag indicating whether or not unprovision\n         should occur asynchronously. When set to true, the unprovision\n         operation returns when the request is accepted by the system, and the\n         unprovision operation continues without any timeout limit. The default\n         value is false. However, we recommend setting it to true for large\n         application packages that were provisioned.\n        :type async_parameter: bool\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`", "docstring_tokens": ["Removes", "or", "unregisters", "a", "Service", "Fabric", "application", "type", "from", "the", "cluster", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicefabric/azure/servicefabric/service_fabric_client_ap_is.py#L2842-L2914", "partition": "test"}
{"repo": "inveniosoftware/invenio-migrator", "path": "invenio_migrator/legacy/users.py", "func_name": "_get_users_invenio2", "original_string": "def _get_users_invenio2(*args, **kwargs):\n    \"\"\"Get user accounts from Invenio 2.\"\"\"\n    from invenio.modules.accounts.models import User\n    q = User.query\n    return q.count(), q.all()", "language": "python", "code": "def _get_users_invenio2(*args, **kwargs):\n    \"\"\"Get user accounts from Invenio 2.\"\"\"\n    from invenio.modules.accounts.models import User\n    q = User.query\n    return q.count(), q.all()", "code_tokens": ["def", "_get_users_invenio2", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "from", "invenio", ".", "modules", ".", "accounts", ".", "models", "import", "User", "q", "=", "User", ".", "query", "return", "q", ".", "count", "(", ")", ",", "q", ".", "all", "(", ")"], "docstring": "Get user accounts from Invenio 2.", "docstring_tokens": ["Get", "user", "accounts", "from", "Invenio", "2", "."], "sha": "6902c6968a39b747d15e32363f43b7dffe2622c2", "url": "https://github.com/inveniosoftware/invenio-migrator/blob/6902c6968a39b747d15e32363f43b7dffe2622c2/invenio_migrator/legacy/users.py#L58-L62", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/utils/debugging.py", "func_name": "_get_args_str", "original_string": "def _get_args_str(func, highlight=None):\n    \"\"\"\n    Return function's declared arguments as a string.\n\n    For example for this function it returns \"func, highlight=None\"; for the ``_wrap`` function it returns\n    \"text, wrap_at=120, indent=4\". This should usually coincide with the function's declaration (the part\n    which is inside the parentheses).\n    \"\"\"\n    if not func: return \"\"\n    s = str(inspect.signature(func))[1:-1]\n    if highlight:\n        s = re.sub(r\"\\b%s\\b\" % highlight, Style.BRIGHT + Fore.WHITE + highlight + Fore.LIGHTBLACK_EX + Style.NORMAL, s)\n    return s", "language": "python", "code": "def _get_args_str(func, highlight=None):\n    \"\"\"\n    Return function's declared arguments as a string.\n\n    For example for this function it returns \"func, highlight=None\"; for the ``_wrap`` function it returns\n    \"text, wrap_at=120, indent=4\". This should usually coincide with the function's declaration (the part\n    which is inside the parentheses).\n    \"\"\"\n    if not func: return \"\"\n    s = str(inspect.signature(func))[1:-1]\n    if highlight:\n        s = re.sub(r\"\\b%s\\b\" % highlight, Style.BRIGHT + Fore.WHITE + highlight + Fore.LIGHTBLACK_EX + Style.NORMAL, s)\n    return s", "code_tokens": ["def", "_get_args_str", "(", "func", ",", "highlight", "=", "None", ")", ":", "if", "not", "func", ":", "return", "\"\"", "s", "=", "str", "(", "inspect", ".", "signature", "(", "func", ")", ")", "[", "1", ":", "-", "1", "]", "if", "highlight", ":", "s", "=", "re", ".", "sub", "(", "r\"\\b%s\\b\"", "%", "highlight", ",", "Style", ".", "BRIGHT", "+", "Fore", ".", "WHITE", "+", "highlight", "+", "Fore", ".", "LIGHTBLACK_EX", "+", "Style", ".", "NORMAL", ",", "s", ")", "return", "s"], "docstring": "Return function's declared arguments as a string.\n\n    For example for this function it returns \"func, highlight=None\"; for the ``_wrap`` function it returns\n    \"text, wrap_at=120, indent=4\". This should usually coincide with the function's declaration (the part\n    which is inside the parentheses).", "docstring_tokens": ["Return", "function", "s", "declared", "arguments", "as", "a", "string", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/utils/debugging.py#L307-L319", "partition": "test"}
{"repo": "andreikop/python-ws-discovery", "path": "wsdiscovery/daemon.py", "func_name": "WSDiscovery.setRemoteServiceHelloCallback", "original_string": "def setRemoteServiceHelloCallback(self, cb, types=None, scopes=None):\n        \"\"\"Set callback, which will be called when new service appeared online\n        and sent Hi message\n\n        typesFilter and scopesFilter might be list of types and scopes.\n        If filter is set, callback is called only for Hello messages,\n        which match filter\n\n        Set None to disable callback\n        \"\"\"\n        self._remoteServiceHelloCallback = cb\n        self._remoteServiceHelloCallbackTypesFilter = types\n        self._remoteServiceHelloCallbackScopesFilter = scopes", "language": "python", "code": "def setRemoteServiceHelloCallback(self, cb, types=None, scopes=None):\n        \"\"\"Set callback, which will be called when new service appeared online\n        and sent Hi message\n\n        typesFilter and scopesFilter might be list of types and scopes.\n        If filter is set, callback is called only for Hello messages,\n        which match filter\n\n        Set None to disable callback\n        \"\"\"\n        self._remoteServiceHelloCallback = cb\n        self._remoteServiceHelloCallbackTypesFilter = types\n        self._remoteServiceHelloCallbackScopesFilter = scopes", "code_tokens": ["def", "setRemoteServiceHelloCallback", "(", "self", ",", "cb", ",", "types", "=", "None", ",", "scopes", "=", "None", ")", ":", "self", ".", "_remoteServiceHelloCallback", "=", "cb", "self", ".", "_remoteServiceHelloCallbackTypesFilter", "=", "types", "self", ".", "_remoteServiceHelloCallbackScopesFilter", "=", "scopes"], "docstring": "Set callback, which will be called when new service appeared online\n        and sent Hi message\n\n        typesFilter and scopesFilter might be list of types and scopes.\n        If filter is set, callback is called only for Hello messages,\n        which match filter\n\n        Set None to disable callback", "docstring_tokens": ["Set", "callback", "which", "will", "be", "called", "when", "new", "service", "appeared", "online", "and", "sent", "Hi", "message"], "sha": "a7b852cf43115c6f986e509b1870d6963e76687f", "url": "https://github.com/andreikop/python-ws-discovery/blob/a7b852cf43115c6f986e509b1870d6963e76687f/wsdiscovery/daemon.py#L291-L303", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/cli.py", "func_name": "run", "original_string": "def run(  # pylint: disable=too-many-arguments\n    file_or_code,\n    code,\n    in_ns,\n    use_var_indirection,\n    warn_on_shadowed_name,\n    warn_on_shadowed_var,\n    warn_on_var_indirection,\n):\n    \"\"\"Run a Basilisp script or a line of code, if it is provided.\"\"\"\n    basilisp.init()\n    ctx = compiler.CompilerContext(\n        filename=CLI_INPUT_FILE_PATH\n        if code\n        else (\n            STDIN_INPUT_FILE_PATH if file_or_code == STDIN_FILE_NAME else file_or_code\n        ),\n        opts={\n            compiler.WARN_ON_SHADOWED_NAME: warn_on_shadowed_name,\n            compiler.WARN_ON_SHADOWED_VAR: warn_on_shadowed_var,\n            compiler.USE_VAR_INDIRECTION: use_var_indirection,\n            compiler.WARN_ON_VAR_INDIRECTION: warn_on_var_indirection,\n        },\n    )\n    eof = object()\n\n    with runtime.ns_bindings(in_ns) as ns:\n        if code:\n            print(runtime.lrepr(eval_str(file_or_code, ctx, ns.module, eof)))\n        elif file_or_code == STDIN_FILE_NAME:\n            print(\n                runtime.lrepr(\n                    eval_stream(click.get_text_stream(\"stdin\"), ctx, ns.module)\n                )\n            )\n        else:\n            print(runtime.lrepr(eval_file(file_or_code, ctx, ns.module)))", "language": "python", "code": "def run(  # pylint: disable=too-many-arguments\n    file_or_code,\n    code,\n    in_ns,\n    use_var_indirection,\n    warn_on_shadowed_name,\n    warn_on_shadowed_var,\n    warn_on_var_indirection,\n):\n    \"\"\"Run a Basilisp script or a line of code, if it is provided.\"\"\"\n    basilisp.init()\n    ctx = compiler.CompilerContext(\n        filename=CLI_INPUT_FILE_PATH\n        if code\n        else (\n            STDIN_INPUT_FILE_PATH if file_or_code == STDIN_FILE_NAME else file_or_code\n        ),\n        opts={\n            compiler.WARN_ON_SHADOWED_NAME: warn_on_shadowed_name,\n            compiler.WARN_ON_SHADOWED_VAR: warn_on_shadowed_var,\n            compiler.USE_VAR_INDIRECTION: use_var_indirection,\n            compiler.WARN_ON_VAR_INDIRECTION: warn_on_var_indirection,\n        },\n    )\n    eof = object()\n\n    with runtime.ns_bindings(in_ns) as ns:\n        if code:\n            print(runtime.lrepr(eval_str(file_or_code, ctx, ns.module, eof)))\n        elif file_or_code == STDIN_FILE_NAME:\n            print(\n                runtime.lrepr(\n                    eval_stream(click.get_text_stream(\"stdin\"), ctx, ns.module)\n                )\n            )\n        else:\n            print(runtime.lrepr(eval_file(file_or_code, ctx, ns.module)))", "code_tokens": ["def", "run", "(", "# pylint: disable=too-many-arguments", "file_or_code", ",", "code", ",", "in_ns", ",", "use_var_indirection", ",", "warn_on_shadowed_name", ",", "warn_on_shadowed_var", ",", "warn_on_var_indirection", ",", ")", ":", "basilisp", ".", "init", "(", ")", "ctx", "=", "compiler", ".", "CompilerContext", "(", "filename", "=", "CLI_INPUT_FILE_PATH", "if", "code", "else", "(", "STDIN_INPUT_FILE_PATH", "if", "file_or_code", "==", "STDIN_FILE_NAME", "else", "file_or_code", ")", ",", "opts", "=", "{", "compiler", ".", "WARN_ON_SHADOWED_NAME", ":", "warn_on_shadowed_name", ",", "compiler", ".", "WARN_ON_SHADOWED_VAR", ":", "warn_on_shadowed_var", ",", "compiler", ".", "USE_VAR_INDIRECTION", ":", "use_var_indirection", ",", "compiler", ".", "WARN_ON_VAR_INDIRECTION", ":", "warn_on_var_indirection", ",", "}", ",", ")", "eof", "=", "object", "(", ")", "with", "runtime", ".", "ns_bindings", "(", "in_ns", ")", "as", "ns", ":", "if", "code", ":", "print", "(", "runtime", ".", "lrepr", "(", "eval_str", "(", "file_or_code", ",", "ctx", ",", "ns", ".", "module", ",", "eof", ")", ")", ")", "elif", "file_or_code", "==", "STDIN_FILE_NAME", ":", "print", "(", "runtime", ".", "lrepr", "(", "eval_stream", "(", "click", ".", "get_text_stream", "(", "\"stdin\"", ")", ",", "ctx", ",", "ns", ".", "module", ")", ")", ")", "else", ":", "print", "(", "runtime", ".", "lrepr", "(", "eval_file", "(", "file_or_code", ",", "ctx", ",", "ns", ".", "module", ")", ")", ")"], "docstring": "Run a Basilisp script or a line of code, if it is provided.", "docstring_tokens": ["Run", "a", "Basilisp", "script", "or", "a", "line", "of", "code", "if", "it", "is", "provided", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/cli.py#L214-L250", "partition": "test"}
{"repo": "solvebio/solvebio-python", "path": "solvebio/resource/apiresource.py", "func_name": "SingletonAPIResource.class_url", "original_string": "def class_url(cls):\n        \"\"\"\n        Returns a versioned URI string for this class,\n        and don't pluralize the class name.\n        \"\"\"\n        base = 'v{0}'.format(getattr(cls, 'RESOURCE_VERSION', '1'))\n        return \"/{0}/{1}\".format(base, class_to_api_name(\n            cls.class_name(), pluralize=False))", "language": "python", "code": "def class_url(cls):\n        \"\"\"\n        Returns a versioned URI string for this class,\n        and don't pluralize the class name.\n        \"\"\"\n        base = 'v{0}'.format(getattr(cls, 'RESOURCE_VERSION', '1'))\n        return \"/{0}/{1}\".format(base, class_to_api_name(\n            cls.class_name(), pluralize=False))", "code_tokens": ["def", "class_url", "(", "cls", ")", ":", "base", "=", "'v{0}'", ".", "format", "(", "getattr", "(", "cls", ",", "'RESOURCE_VERSION'", ",", "'1'", ")", ")", "return", "\"/{0}/{1}\"", ".", "format", "(", "base", ",", "class_to_api_name", "(", "cls", ".", "class_name", "(", ")", ",", "pluralize", "=", "False", ")", ")"], "docstring": "Returns a versioned URI string for this class,\n        and don't pluralize the class name.", "docstring_tokens": ["Returns", "a", "versioned", "URI", "string", "for", "this", "class", "and", "don", "t", "pluralize", "the", "class", "name", "."], "sha": "b29614643043afd19c1d8074e8f25c6700d51a73", "url": "https://github.com/solvebio/solvebio-python/blob/b29614643043afd19c1d8074e8f25c6700d51a73/solvebio/resource/apiresource.py#L146-L153", "partition": "test"}
{"repo": "chaoss/grimoirelab-kingarthur", "path": "arthur/jobs.py", "func_name": "PercevalJob.initialize_archive_manager", "original_string": "def initialize_archive_manager(self, archive_path):\n        \"\"\"Initialize the archive manager.\n\n        :param archive_path: path where the archive manager is located\n        \"\"\"\n        if archive_path == \"\":\n            raise ValueError(\"Archive manager path cannot be empty\")\n\n        if archive_path:\n            self.archive_manager = perceval.archive.ArchiveManager(archive_path)", "language": "python", "code": "def initialize_archive_manager(self, archive_path):\n        \"\"\"Initialize the archive manager.\n\n        :param archive_path: path where the archive manager is located\n        \"\"\"\n        if archive_path == \"\":\n            raise ValueError(\"Archive manager path cannot be empty\")\n\n        if archive_path:\n            self.archive_manager = perceval.archive.ArchiveManager(archive_path)", "code_tokens": ["def", "initialize_archive_manager", "(", "self", ",", "archive_path", ")", ":", "if", "archive_path", "==", "\"\"", ":", "raise", "ValueError", "(", "\"Archive manager path cannot be empty\"", ")", "if", "archive_path", ":", "self", ".", "archive_manager", "=", "perceval", ".", "archive", ".", "ArchiveManager", "(", "archive_path", ")"], "docstring": "Initialize the archive manager.\n\n        :param archive_path: path where the archive manager is located", "docstring_tokens": ["Initialize", "the", "archive", "manager", "."], "sha": "9d6a638bee68d5e5c511f045eeebf06340fd3252", "url": "https://github.com/chaoss/grimoirelab-kingarthur/blob/9d6a638bee68d5e5c511f045eeebf06340fd3252/arthur/jobs.py#L135-L144", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "maybe_get_static_value", "original_string": "def maybe_get_static_value(x, dtype=None):\n  \"\"\"Helper which tries to return a static value.\n\n  Given `x`, extract it's value statically, optionally casting to a specific\n  dtype. If this is not possible, None is returned.\n\n  Args:\n    x: `Tensor` for which to extract a value statically.\n    dtype: Optional dtype to cast to.\n\n  Returns:\n    Statically inferred value if possible, otherwise None.\n  \"\"\"\n  if x is None:\n    return x\n  try:\n    # This returns an np.ndarray.\n    x_ = tf.get_static_value(x)\n  except TypeError:\n    x_ = x\n  if x_ is None or dtype is None:\n    return x_\n  return np.array(x_, dtype)", "language": "python", "code": "def maybe_get_static_value(x, dtype=None):\n  \"\"\"Helper which tries to return a static value.\n\n  Given `x`, extract it's value statically, optionally casting to a specific\n  dtype. If this is not possible, None is returned.\n\n  Args:\n    x: `Tensor` for which to extract a value statically.\n    dtype: Optional dtype to cast to.\n\n  Returns:\n    Statically inferred value if possible, otherwise None.\n  \"\"\"\n  if x is None:\n    return x\n  try:\n    # This returns an np.ndarray.\n    x_ = tf.get_static_value(x)\n  except TypeError:\n    x_ = x\n  if x_ is None or dtype is None:\n    return x_\n  return np.array(x_, dtype)", "code_tokens": ["def", "maybe_get_static_value", "(", "x", ",", "dtype", "=", "None", ")", ":", "if", "x", "is", "None", ":", "return", "x", "try", ":", "# This returns an np.ndarray.", "x_", "=", "tf", ".", "get_static_value", "(", "x", ")", "except", "TypeError", ":", "x_", "=", "x", "if", "x_", "is", "None", "or", "dtype", "is", "None", ":", "return", "x_", "return", "np", ".", "array", "(", "x_", ",", "dtype", ")"], "docstring": "Helper which tries to return a static value.\n\n  Given `x`, extract it's value statically, optionally casting to a specific\n  dtype. If this is not possible, None is returned.\n\n  Args:\n    x: `Tensor` for which to extract a value statically.\n    dtype: Optional dtype to cast to.\n\n  Returns:\n    Statically inferred value if possible, otherwise None.", "docstring_tokens": ["Helper", "which", "tries", "to", "return", "a", "static", "value", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L740-L762", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/quantum_info/operators/channel/transformations.py", "func_name": "_transform_from_pauli", "original_string": "def _transform_from_pauli(data, num_qubits):\n    \"\"\"Change of basis of bipartite matrix represenation.\"\"\"\n    # Change basis: sum_{i=0}^3 =|\\sigma_i>><i|\n    basis_mat = np.array(\n        [[1, 0, 0, 1], [0, 1, 1j, 0], [0, 1, -1j, 0], [1, 0j, 0, -1]],\n        dtype=complex)\n    # Note that we manually renormalized after change of basis\n    # to avoid rounding errors from square-roots of 2.\n    cob = basis_mat\n    for _ in range(num_qubits - 1):\n        dim = int(np.sqrt(len(cob)))\n        cob = np.reshape(\n            np.transpose(\n                np.reshape(\n                    np.kron(basis_mat, cob), (2, 2, dim, dim, 4, dim * dim)),\n                (0, 2, 1, 3, 4, 5)), (4 * dim * dim, 4 * dim * dim))\n    return np.dot(np.dot(cob, data), cob.conj().T) / 2**num_qubits", "language": "python", "code": "def _transform_from_pauli(data, num_qubits):\n    \"\"\"Change of basis of bipartite matrix represenation.\"\"\"\n    # Change basis: sum_{i=0}^3 =|\\sigma_i>><i|\n    basis_mat = np.array(\n        [[1, 0, 0, 1], [0, 1, 1j, 0], [0, 1, -1j, 0], [1, 0j, 0, -1]],\n        dtype=complex)\n    # Note that we manually renormalized after change of basis\n    # to avoid rounding errors from square-roots of 2.\n    cob = basis_mat\n    for _ in range(num_qubits - 1):\n        dim = int(np.sqrt(len(cob)))\n        cob = np.reshape(\n            np.transpose(\n                np.reshape(\n                    np.kron(basis_mat, cob), (2, 2, dim, dim, 4, dim * dim)),\n                (0, 2, 1, 3, 4, 5)), (4 * dim * dim, 4 * dim * dim))\n    return np.dot(np.dot(cob, data), cob.conj().T) / 2**num_qubits", "code_tokens": ["def", "_transform_from_pauli", "(", "data", ",", "num_qubits", ")", ":", "# Change basis: sum_{i=0}^3 =|\\sigma_i>><i|", "basis_mat", "=", "np", ".", "array", "(", "[", "[", "1", ",", "0", ",", "0", ",", "1", "]", ",", "[", "0", ",", "1", ",", "1j", ",", "0", "]", ",", "[", "0", ",", "1", ",", "-", "1j", ",", "0", "]", ",", "[", "1", ",", "0j", ",", "0", ",", "-", "1", "]", "]", ",", "dtype", "=", "complex", ")", "# Note that we manually renormalized after change of basis", "# to avoid rounding errors from square-roots of 2.", "cob", "=", "basis_mat", "for", "_", "in", "range", "(", "num_qubits", "-", "1", ")", ":", "dim", "=", "int", "(", "np", ".", "sqrt", "(", "len", "(", "cob", ")", ")", ")", "cob", "=", "np", ".", "reshape", "(", "np", ".", "transpose", "(", "np", ".", "reshape", "(", "np", ".", "kron", "(", "basis_mat", ",", "cob", ")", ",", "(", "2", ",", "2", ",", "dim", ",", "dim", ",", "4", ",", "dim", "*", "dim", ")", ")", ",", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ",", "5", ")", ")", ",", "(", "4", "*", "dim", "*", "dim", ",", "4", "*", "dim", "*", "dim", ")", ")", "return", "np", ".", "dot", "(", "np", ".", "dot", "(", "cob", ",", "data", ")", ",", "cob", ".", "conj", "(", ")", ".", "T", ")", "/", "2", "**", "num_qubits"], "docstring": "Change of basis of bipartite matrix represenation.", "docstring_tokens": ["Change", "of", "basis", "of", "bipartite", "matrix", "represenation", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/quantum_info/operators/channel/transformations.py#L400-L416", "partition": "test"}
{"repo": "CiscoDevNet/webexteamssdk", "path": "webexteamssdk/utils.py", "func_name": "json_dict", "original_string": "def json_dict(json_data):\n    \"\"\"Given a dictionary or JSON string; return a dictionary.\n\n    Args:\n        json_data(dict, str): Input JSON object.\n\n    Returns:\n        A Python dictionary with the contents of the JSON object.\n\n    Raises:\n        TypeError: If the input object is not a dictionary or string.\n\n    \"\"\"\n    if isinstance(json_data, dict):\n        return json_data\n    elif isinstance(json_data, basestring):\n        return json.loads(json_data, object_hook=OrderedDict)\n    else:\n        raise TypeError(\n            \"'json_data' must be a dictionary or valid JSON string; \"\n            \"received: {!r}\".format(json_data)\n        )", "language": "python", "code": "def json_dict(json_data):\n    \"\"\"Given a dictionary or JSON string; return a dictionary.\n\n    Args:\n        json_data(dict, str): Input JSON object.\n\n    Returns:\n        A Python dictionary with the contents of the JSON object.\n\n    Raises:\n        TypeError: If the input object is not a dictionary or string.\n\n    \"\"\"\n    if isinstance(json_data, dict):\n        return json_data\n    elif isinstance(json_data, basestring):\n        return json.loads(json_data, object_hook=OrderedDict)\n    else:\n        raise TypeError(\n            \"'json_data' must be a dictionary or valid JSON string; \"\n            \"received: {!r}\".format(json_data)\n        )", "code_tokens": ["def", "json_dict", "(", "json_data", ")", ":", "if", "isinstance", "(", "json_data", ",", "dict", ")", ":", "return", "json_data", "elif", "isinstance", "(", "json_data", ",", "basestring", ")", ":", "return", "json", ".", "loads", "(", "json_data", ",", "object_hook", "=", "OrderedDict", ")", "else", ":", "raise", "TypeError", "(", "\"'json_data' must be a dictionary or valid JSON string; \"", "\"received: {!r}\"", ".", "format", "(", "json_data", ")", ")"], "docstring": "Given a dictionary or JSON string; return a dictionary.\n\n    Args:\n        json_data(dict, str): Input JSON object.\n\n    Returns:\n        A Python dictionary with the contents of the JSON object.\n\n    Raises:\n        TypeError: If the input object is not a dictionary or string.", "docstring_tokens": ["Given", "a", "dictionary", "or", "JSON", "string", ";", "return", "a", "dictionary", "."], "sha": "6fc2cc3557e080ba4b2a380664cb2a0532ae45cd", "url": "https://github.com/CiscoDevNet/webexteamssdk/blob/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd/webexteamssdk/utils.py#L234-L255", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/storageservice.py", "func_name": "HDF5StorageService._all_get_node_by_name", "original_string": "def _all_get_node_by_name(self, name):\n        \"\"\"Returns an HDF5 node by the path specified in `name`\"\"\"\n        path_name = name.replace('.', '/')\n        where = '/%s/%s' % (self._trajectory_name, path_name)\n        return self._hdf5file.get_node(where=where)", "language": "python", "code": "def _all_get_node_by_name(self, name):\n        \"\"\"Returns an HDF5 node by the path specified in `name`\"\"\"\n        path_name = name.replace('.', '/')\n        where = '/%s/%s' % (self._trajectory_name, path_name)\n        return self._hdf5file.get_node(where=where)", "code_tokens": ["def", "_all_get_node_by_name", "(", "self", ",", "name", ")", ":", "path_name", "=", "name", ".", "replace", "(", "'.'", ",", "'/'", ")", "where", "=", "'/%s/%s'", "%", "(", "self", ".", "_trajectory_name", ",", "path_name", ")", "return", "self", ".", "_hdf5file", ".", "get_node", "(", "where", "=", "where", ")"], "docstring": "Returns an HDF5 node by the path specified in `name`", "docstring_tokens": ["Returns", "an", "HDF5", "node", "by", "the", "path", "specified", "in", "name"], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/storageservice.py#L3017-L3021", "partition": "test"}
{"repo": "bloomreach/s4cmd", "path": "s4cmd.py", "func_name": "ThreadUtil.s3walk", "original_string": "def s3walk(self, s3url, s3dir, filter_path, result):\n    '''Thread worker for s3walk.\n       Recursively walk into all subdirectories if they still match the filter\n       path partially.\n    '''\n\n    paginator = self.s3.get_paginator('list_objects')\n    filter_path_level = filter_path.count(PATH_SEP)\n\n    for page in paginator.paginate(Bucket=s3url.bucket, Prefix=s3dir, Delimiter=PATH_SEP, PaginationConfig={'PageSize': 1000}):\n      # Get subdirectories first.\n      for obj in page.get('CommonPrefixes') or []:\n        obj_name = obj['Prefix']\n\n        if not self.partial_match(obj_name, filter_path):\n          continue\n\n        if self.opt.recursive or (obj_name.count(PATH_SEP) != filter_path_level + 1):\n          self.pool.s3walk(s3url, obj_name, filter_path, result)\n        else:\n          self.conditional(result, {\n            'name': S3URL.combine(s3url.proto, s3url.bucket, obj_name),\n            'is_dir': True,\n            'size': 0,\n            'last_modified': None\n          })\n\n      # Then get all items in this folder.\n      for obj in page.get('Contents') or []:\n        obj_name = obj['Key']\n        if not self.partial_match(obj_name, filter_path):\n          continue\n\n        if self.opt.recursive or obj_name.count(PATH_SEP) == filter_path_level:\n          self.conditional(result, {\n            'name': S3URL.combine(s3url.proto, s3url.bucket, obj_name),\n            'is_dir': False,\n            'size': obj['Size'],\n            'last_modified': obj['LastModified']\n          })", "language": "python", "code": "def s3walk(self, s3url, s3dir, filter_path, result):\n    '''Thread worker for s3walk.\n       Recursively walk into all subdirectories if they still match the filter\n       path partially.\n    '''\n\n    paginator = self.s3.get_paginator('list_objects')\n    filter_path_level = filter_path.count(PATH_SEP)\n\n    for page in paginator.paginate(Bucket=s3url.bucket, Prefix=s3dir, Delimiter=PATH_SEP, PaginationConfig={'PageSize': 1000}):\n      # Get subdirectories first.\n      for obj in page.get('CommonPrefixes') or []:\n        obj_name = obj['Prefix']\n\n        if not self.partial_match(obj_name, filter_path):\n          continue\n\n        if self.opt.recursive or (obj_name.count(PATH_SEP) != filter_path_level + 1):\n          self.pool.s3walk(s3url, obj_name, filter_path, result)\n        else:\n          self.conditional(result, {\n            'name': S3URL.combine(s3url.proto, s3url.bucket, obj_name),\n            'is_dir': True,\n            'size': 0,\n            'last_modified': None\n          })\n\n      # Then get all items in this folder.\n      for obj in page.get('Contents') or []:\n        obj_name = obj['Key']\n        if not self.partial_match(obj_name, filter_path):\n          continue\n\n        if self.opt.recursive or obj_name.count(PATH_SEP) == filter_path_level:\n          self.conditional(result, {\n            'name': S3URL.combine(s3url.proto, s3url.bucket, obj_name),\n            'is_dir': False,\n            'size': obj['Size'],\n            'last_modified': obj['LastModified']\n          })", "code_tokens": ["def", "s3walk", "(", "self", ",", "s3url", ",", "s3dir", ",", "filter_path", ",", "result", ")", ":", "paginator", "=", "self", ".", "s3", ".", "get_paginator", "(", "'list_objects'", ")", "filter_path_level", "=", "filter_path", ".", "count", "(", "PATH_SEP", ")", "for", "page", "in", "paginator", ".", "paginate", "(", "Bucket", "=", "s3url", ".", "bucket", ",", "Prefix", "=", "s3dir", ",", "Delimiter", "=", "PATH_SEP", ",", "PaginationConfig", "=", "{", "'PageSize'", ":", "1000", "}", ")", ":", "# Get subdirectories first.", "for", "obj", "in", "page", ".", "get", "(", "'CommonPrefixes'", ")", "or", "[", "]", ":", "obj_name", "=", "obj", "[", "'Prefix'", "]", "if", "not", "self", ".", "partial_match", "(", "obj_name", ",", "filter_path", ")", ":", "continue", "if", "self", ".", "opt", ".", "recursive", "or", "(", "obj_name", ".", "count", "(", "PATH_SEP", ")", "!=", "filter_path_level", "+", "1", ")", ":", "self", ".", "pool", ".", "s3walk", "(", "s3url", ",", "obj_name", ",", "filter_path", ",", "result", ")", "else", ":", "self", ".", "conditional", "(", "result", ",", "{", "'name'", ":", "S3URL", ".", "combine", "(", "s3url", ".", "proto", ",", "s3url", ".", "bucket", ",", "obj_name", ")", ",", "'is_dir'", ":", "True", ",", "'size'", ":", "0", ",", "'last_modified'", ":", "None", "}", ")", "# Then get all items in this folder.", "for", "obj", "in", "page", ".", "get", "(", "'Contents'", ")", "or", "[", "]", ":", "obj_name", "=", "obj", "[", "'Key'", "]", "if", "not", "self", ".", "partial_match", "(", "obj_name", ",", "filter_path", ")", ":", "continue", "if", "self", ".", "opt", ".", "recursive", "or", "obj_name", ".", "count", "(", "PATH_SEP", ")", "==", "filter_path_level", ":", "self", ".", "conditional", "(", "result", ",", "{", "'name'", ":", "S3URL", ".", "combine", "(", "s3url", ".", "proto", ",", "s3url", ".", "bucket", ",", "obj_name", ")", ",", "'is_dir'", ":", "False", ",", "'size'", ":", "obj", "[", "'Size'", "]", ",", "'last_modified'", ":", "obj", "[", "'LastModified'", "]", "}", ")"], "docstring": "Thread worker for s3walk.\n       Recursively walk into all subdirectories if they still match the filter\n       path partially.", "docstring_tokens": ["Thread", "worker", "for", "s3walk", ".", "Recursively", "walk", "into", "all", "subdirectories", "if", "they", "still", "match", "the", "filter", "path", "partially", "."], "sha": "bb51075bf43703e7cd95aa39288cf7732ec13a6d", "url": "https://github.com/bloomreach/s4cmd/blob/bb51075bf43703e7cd95aa39288cf7732ec13a6d/s4cmd.py#L1167-L1206", "partition": "test"}
{"repo": "mental32/spotify.py", "path": "spotify/utils.py", "func_name": "assert_hasattr", "original_string": "def assert_hasattr(attr: str, msg: str, tp: BaseException = SpotifyException) -> Callable:\n    \"\"\"decorator to assert an object has an attribute when run.\"\"\"\n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        def decorated(self, *args, **kwargs):\n            if not hasattr(self, attr):\n                raise tp(msg)\n            return func(self, *args, **kwargs)\n\n        if inspect.iscoroutinefunction(func):\n            @functools.wraps(func)\n            async def decorated(*args, **kwargs):\n                return await decorated(*args, **kwargs)\n\n        return decorated\n    return decorator", "language": "python", "code": "def assert_hasattr(attr: str, msg: str, tp: BaseException = SpotifyException) -> Callable:\n    \"\"\"decorator to assert an object has an attribute when run.\"\"\"\n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        def decorated(self, *args, **kwargs):\n            if not hasattr(self, attr):\n                raise tp(msg)\n            return func(self, *args, **kwargs)\n\n        if inspect.iscoroutinefunction(func):\n            @functools.wraps(func)\n            async def decorated(*args, **kwargs):\n                return await decorated(*args, **kwargs)\n\n        return decorated\n    return decorator", "code_tokens": ["def", "assert_hasattr", "(", "attr", ":", "str", ",", "msg", ":", "str", ",", "tp", ":", "BaseException", "=", "SpotifyException", ")", "->", "Callable", ":", "def", "decorator", "(", "func", ":", "Callable", ")", "->", "Callable", ":", "@", "functools", ".", "wraps", "(", "func", ")", "def", "decorated", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "if", "not", "hasattr", "(", "self", ",", "attr", ")", ":", "raise", "tp", "(", "msg", ")", "return", "func", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", "if", "inspect", ".", "iscoroutinefunction", "(", "func", ")", ":", "@", "functools", ".", "wraps", "(", "func", ")", "async", "def", "decorated", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "return", "await", "decorated", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "decorated", "return", "decorator"], "docstring": "decorator to assert an object has an attribute when run.", "docstring_tokens": ["decorator", "to", "assert", "an", "object", "has", "an", "attribute", "when", "run", "."], "sha": "bb296cac7c3dd289908906b7069bd80f43950515", "url": "https://github.com/mental32/spotify.py/blob/bb296cac7c3dd289908906b7069bd80f43950515/spotify/utils.py#L49-L64", "partition": "test"}
{"repo": "hustlzp/Flask-Boost", "path": "flask_boost/project/manage.py", "func_name": "live", "original_string": "def live():\n    \"\"\"Run livereload server\"\"\"\n    from livereload import Server\n\n    server = Server(app)\n\n    map(server.watch, glob2.glob(\"application/pages/**/*.*\"))  # pages\n    map(server.watch, glob2.glob(\"application/macros/**/*.html\"))  # macros\n    map(server.watch, glob2.glob(\"application/static/**/*.*\"))  # public assets\n\n    server.serve(port=PORT)", "language": "python", "code": "def live():\n    \"\"\"Run livereload server\"\"\"\n    from livereload import Server\n\n    server = Server(app)\n\n    map(server.watch, glob2.glob(\"application/pages/**/*.*\"))  # pages\n    map(server.watch, glob2.glob(\"application/macros/**/*.html\"))  # macros\n    map(server.watch, glob2.glob(\"application/static/**/*.*\"))  # public assets\n\n    server.serve(port=PORT)", "code_tokens": ["def", "live", "(", ")", ":", "from", "livereload", "import", "Server", "server", "=", "Server", "(", "app", ")", "map", "(", "server", ".", "watch", ",", "glob2", ".", "glob", "(", "\"application/pages/**/*.*\"", ")", ")", "# pages", "map", "(", "server", ".", "watch", ",", "glob2", ".", "glob", "(", "\"application/macros/**/*.html\"", ")", ")", "# macros", "map", "(", "server", ".", "watch", ",", "glob2", ".", "glob", "(", "\"application/static/**/*.*\"", ")", ")", "# public assets", "server", ".", "serve", "(", "port", "=", "PORT", ")"], "docstring": "Run livereload server", "docstring_tokens": ["Run", "livereload", "server"], "sha": "d0308408ebb248dd752b77123b845f8ec637fab2", "url": "https://github.com/hustlzp/Flask-Boost/blob/d0308408ebb248dd752b77123b845f8ec637fab2/flask_boost/project/manage.py#L27-L37", "partition": "test"}
{"repo": "Numergy/yoda", "path": "yoda/subcommand/show.py", "func_name": "Show.show_workspace", "original_string": "def show_workspace(self, name):\n        \"\"\"Show specific workspace.\"\"\"\n        if not self.workspace.exists(name):\n            raise ValueError(\"Workspace `%s` doesn't exists.\" % name)\n\n        color = Color()\n        workspaces = self.workspace.list()\n\n        self.logger.info(\"<== %s workspace ==>\" % color.colored(name, \"green\"))\n        self.logger.info(\"\\tPath: %s\" % workspaces[name][\"path\"])\n        self.logger.info(\"\\tNumber of repositories: %s\"\n                         % color.colored(\n                             len(workspaces[name][\"repositories\"]),\n                             \"yellow\"))\n\n        repo_colored = color.colored(\"Repositories\", \"blue\")\n        path_colored = color.colored(\"Path\", \"blue\")\n        trepositories = PrettyTable(\n            [repo_colored, path_colored, color.colored(\"+\", \"blue\")])\n        trepositories.align[repo_colored] = \"l\"\n        trepositories.align[path_colored] = \"l\"\n\n        for repo_name in workspaces[name][\"repositories\"]:\n            fullname = \"%s/%s\" % (name, repo_name)\n            fullpath = find_path(fullname, self.config)[fullname]\n            try:\n                repo = Repository(fullpath)\n                repo_scm = repo.get_scm()\n            except RepositoryAdapterNotFound:\n                repo_scm = None\n            trepositories.add_row(\n                [color.colored(repo_name, \"cyan\"), fullpath, repo_scm])\n\n        self.logger.info(trepositories)", "language": "python", "code": "def show_workspace(self, name):\n        \"\"\"Show specific workspace.\"\"\"\n        if not self.workspace.exists(name):\n            raise ValueError(\"Workspace `%s` doesn't exists.\" % name)\n\n        color = Color()\n        workspaces = self.workspace.list()\n\n        self.logger.info(\"<== %s workspace ==>\" % color.colored(name, \"green\"))\n        self.logger.info(\"\\tPath: %s\" % workspaces[name][\"path\"])\n        self.logger.info(\"\\tNumber of repositories: %s\"\n                         % color.colored(\n                             len(workspaces[name][\"repositories\"]),\n                             \"yellow\"))\n\n        repo_colored = color.colored(\"Repositories\", \"blue\")\n        path_colored = color.colored(\"Path\", \"blue\")\n        trepositories = PrettyTable(\n            [repo_colored, path_colored, color.colored(\"+\", \"blue\")])\n        trepositories.align[repo_colored] = \"l\"\n        trepositories.align[path_colored] = \"l\"\n\n        for repo_name in workspaces[name][\"repositories\"]:\n            fullname = \"%s/%s\" % (name, repo_name)\n            fullpath = find_path(fullname, self.config)[fullname]\n            try:\n                repo = Repository(fullpath)\n                repo_scm = repo.get_scm()\n            except RepositoryAdapterNotFound:\n                repo_scm = None\n            trepositories.add_row(\n                [color.colored(repo_name, \"cyan\"), fullpath, repo_scm])\n\n        self.logger.info(trepositories)", "code_tokens": ["def", "show_workspace", "(", "self", ",", "name", ")", ":", "if", "not", "self", ".", "workspace", ".", "exists", "(", "name", ")", ":", "raise", "ValueError", "(", "\"Workspace `%s` doesn't exists.\"", "%", "name", ")", "color", "=", "Color", "(", ")", "workspaces", "=", "self", ".", "workspace", ".", "list", "(", ")", "self", ".", "logger", ".", "info", "(", "\"<== %s workspace ==>\"", "%", "color", ".", "colored", "(", "name", ",", "\"green\"", ")", ")", "self", ".", "logger", ".", "info", "(", "\"\\tPath: %s\"", "%", "workspaces", "[", "name", "]", "[", "\"path\"", "]", ")", "self", ".", "logger", ".", "info", "(", "\"\\tNumber of repositories: %s\"", "%", "color", ".", "colored", "(", "len", "(", "workspaces", "[", "name", "]", "[", "\"repositories\"", "]", ")", ",", "\"yellow\"", ")", ")", "repo_colored", "=", "color", ".", "colored", "(", "\"Repositories\"", ",", "\"blue\"", ")", "path_colored", "=", "color", ".", "colored", "(", "\"Path\"", ",", "\"blue\"", ")", "trepositories", "=", "PrettyTable", "(", "[", "repo_colored", ",", "path_colored", ",", "color", ".", "colored", "(", "\"+\"", ",", "\"blue\"", ")", "]", ")", "trepositories", ".", "align", "[", "repo_colored", "]", "=", "\"l\"", "trepositories", ".", "align", "[", "path_colored", "]", "=", "\"l\"", "for", "repo_name", "in", "workspaces", "[", "name", "]", "[", "\"repositories\"", "]", ":", "fullname", "=", "\"%s/%s\"", "%", "(", "name", ",", "repo_name", ")", "fullpath", "=", "find_path", "(", "fullname", ",", "self", ".", "config", ")", "[", "fullname", "]", "try", ":", "repo", "=", "Repository", "(", "fullpath", ")", "repo_scm", "=", "repo", ".", "get_scm", "(", ")", "except", "RepositoryAdapterNotFound", ":", "repo_scm", "=", "None", "trepositories", ".", "add_row", "(", "[", "color", ".", "colored", "(", "repo_name", ",", "\"cyan\"", ")", ",", "fullpath", ",", "repo_scm", "]", ")", "self", ".", "logger", ".", "info", "(", "trepositories", ")"], "docstring": "Show specific workspace.", "docstring_tokens": ["Show", "specific", "workspace", "."], "sha": "109f0e9441130488b0155f05883ef6531cf46ee9", "url": "https://github.com/Numergy/yoda/blob/109f0e9441130488b0155f05883ef6531cf46ee9/yoda/subcommand/show.py#L58-L91", "partition": "test"}
{"repo": "tomMoral/loky", "path": "loky/backend/context.py", "func_name": "cpu_count", "original_string": "def cpu_count():\n    \"\"\"Return the number of CPUs the current process can use.\n\n    The returned number of CPUs accounts for:\n     * the number of CPUs in the system, as given by\n       ``multiprocessing.cpu_count``;\n     * the CPU affinity settings of the current process\n       (available with Python 3.4+ on some Unix systems);\n     * CFS scheduler CPU bandwidth limit (available on Linux only, typically\n       set by docker and similar container orchestration systems);\n     * the value of the LOKY_MAX_CPU_COUNT environment variable if defined.\n    and is given as the minimum of these constraints.\n    It is also always larger or equal to 1.\n    \"\"\"\n    import math\n\n    try:\n        cpu_count_mp = mp.cpu_count()\n    except NotImplementedError:\n        cpu_count_mp = 1\n\n    # Number of available CPUs given affinity settings\n    cpu_count_affinity = cpu_count_mp\n    if hasattr(os, 'sched_getaffinity'):\n        try:\n            cpu_count_affinity = len(os.sched_getaffinity(0))\n        except NotImplementedError:\n            pass\n\n    # CFS scheduler CPU bandwidth limit\n    # available in Linux since 2.6 kernel\n    cpu_count_cfs = cpu_count_mp\n    cfs_quota_fname = \"/sys/fs/cgroup/cpu/cpu.cfs_quota_us\"\n    cfs_period_fname = \"/sys/fs/cgroup/cpu/cpu.cfs_period_us\"\n    if os.path.exists(cfs_quota_fname) and os.path.exists(cfs_period_fname):\n        with open(cfs_quota_fname, 'r') as fh:\n            cfs_quota_us = int(fh.read())\n        with open(cfs_period_fname, 'r') as fh:\n            cfs_period_us = int(fh.read())\n\n        if cfs_quota_us > 0 and cfs_period_us > 0:\n            # Make sure this quantity is an int as math.ceil returns a\n            # float in python2.7. (See issue #165)\n            cpu_count_cfs = int(math.ceil(cfs_quota_us / cfs_period_us))\n\n    # User defined soft-limit passed as an loky specific environment variable.\n    cpu_count_loky = int(os.environ.get('LOKY_MAX_CPU_COUNT', cpu_count_mp))\n    aggregate_cpu_count = min(cpu_count_mp, cpu_count_affinity, cpu_count_cfs,\n                              cpu_count_loky)\n    return max(aggregate_cpu_count, 1)", "language": "python", "code": "def cpu_count():\n    \"\"\"Return the number of CPUs the current process can use.\n\n    The returned number of CPUs accounts for:\n     * the number of CPUs in the system, as given by\n       ``multiprocessing.cpu_count``;\n     * the CPU affinity settings of the current process\n       (available with Python 3.4+ on some Unix systems);\n     * CFS scheduler CPU bandwidth limit (available on Linux only, typically\n       set by docker and similar container orchestration systems);\n     * the value of the LOKY_MAX_CPU_COUNT environment variable if defined.\n    and is given as the minimum of these constraints.\n    It is also always larger or equal to 1.\n    \"\"\"\n    import math\n\n    try:\n        cpu_count_mp = mp.cpu_count()\n    except NotImplementedError:\n        cpu_count_mp = 1\n\n    # Number of available CPUs given affinity settings\n    cpu_count_affinity = cpu_count_mp\n    if hasattr(os, 'sched_getaffinity'):\n        try:\n            cpu_count_affinity = len(os.sched_getaffinity(0))\n        except NotImplementedError:\n            pass\n\n    # CFS scheduler CPU bandwidth limit\n    # available in Linux since 2.6 kernel\n    cpu_count_cfs = cpu_count_mp\n    cfs_quota_fname = \"/sys/fs/cgroup/cpu/cpu.cfs_quota_us\"\n    cfs_period_fname = \"/sys/fs/cgroup/cpu/cpu.cfs_period_us\"\n    if os.path.exists(cfs_quota_fname) and os.path.exists(cfs_period_fname):\n        with open(cfs_quota_fname, 'r') as fh:\n            cfs_quota_us = int(fh.read())\n        with open(cfs_period_fname, 'r') as fh:\n            cfs_period_us = int(fh.read())\n\n        if cfs_quota_us > 0 and cfs_period_us > 0:\n            # Make sure this quantity is an int as math.ceil returns a\n            # float in python2.7. (See issue #165)\n            cpu_count_cfs = int(math.ceil(cfs_quota_us / cfs_period_us))\n\n    # User defined soft-limit passed as an loky specific environment variable.\n    cpu_count_loky = int(os.environ.get('LOKY_MAX_CPU_COUNT', cpu_count_mp))\n    aggregate_cpu_count = min(cpu_count_mp, cpu_count_affinity, cpu_count_cfs,\n                              cpu_count_loky)\n    return max(aggregate_cpu_count, 1)", "code_tokens": ["def", "cpu_count", "(", ")", ":", "import", "math", "try", ":", "cpu_count_mp", "=", "mp", ".", "cpu_count", "(", ")", "except", "NotImplementedError", ":", "cpu_count_mp", "=", "1", "# Number of available CPUs given affinity settings", "cpu_count_affinity", "=", "cpu_count_mp", "if", "hasattr", "(", "os", ",", "'sched_getaffinity'", ")", ":", "try", ":", "cpu_count_affinity", "=", "len", "(", "os", ".", "sched_getaffinity", "(", "0", ")", ")", "except", "NotImplementedError", ":", "pass", "# CFS scheduler CPU bandwidth limit", "# available in Linux since 2.6 kernel", "cpu_count_cfs", "=", "cpu_count_mp", "cfs_quota_fname", "=", "\"/sys/fs/cgroup/cpu/cpu.cfs_quota_us\"", "cfs_period_fname", "=", "\"/sys/fs/cgroup/cpu/cpu.cfs_period_us\"", "if", "os", ".", "path", ".", "exists", "(", "cfs_quota_fname", ")", "and", "os", ".", "path", ".", "exists", "(", "cfs_period_fname", ")", ":", "with", "open", "(", "cfs_quota_fname", ",", "'r'", ")", "as", "fh", ":", "cfs_quota_us", "=", "int", "(", "fh", ".", "read", "(", ")", ")", "with", "open", "(", "cfs_period_fname", ",", "'r'", ")", "as", "fh", ":", "cfs_period_us", "=", "int", "(", "fh", ".", "read", "(", ")", ")", "if", "cfs_quota_us", ">", "0", "and", "cfs_period_us", ">", "0", ":", "# Make sure this quantity is an int as math.ceil returns a", "# float in python2.7. (See issue #165)", "cpu_count_cfs", "=", "int", "(", "math", ".", "ceil", "(", "cfs_quota_us", "/", "cfs_period_us", ")", ")", "# User defined soft-limit passed as an loky specific environment variable.", "cpu_count_loky", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'LOKY_MAX_CPU_COUNT'", ",", "cpu_count_mp", ")", ")", "aggregate_cpu_count", "=", "min", "(", "cpu_count_mp", ",", "cpu_count_affinity", ",", "cpu_count_cfs", ",", "cpu_count_loky", ")", "return", "max", "(", "aggregate_cpu_count", ",", "1", ")"], "docstring": "Return the number of CPUs the current process can use.\n\n    The returned number of CPUs accounts for:\n     * the number of CPUs in the system, as given by\n       ``multiprocessing.cpu_count``;\n     * the CPU affinity settings of the current process\n       (available with Python 3.4+ on some Unix systems);\n     * CFS scheduler CPU bandwidth limit (available on Linux only, typically\n       set by docker and similar container orchestration systems);\n     * the value of the LOKY_MAX_CPU_COUNT environment variable if defined.\n    and is given as the minimum of these constraints.\n    It is also always larger or equal to 1.", "docstring_tokens": ["Return", "the", "number", "of", "CPUs", "the", "current", "process", "can", "use", "."], "sha": "dc2d941d8285a96f3a5b666a4bd04875b0b25984", "url": "https://github.com/tomMoral/loky/blob/dc2d941d8285a96f3a5b666a4bd04875b0b25984/loky/backend/context.py#L104-L153", "partition": "test"}
{"repo": "ndrlslz/ternya", "path": "ternya/annotation.py", "func_name": "cinder", "original_string": "def cinder(*arg):\n    \"\"\"\n    Cinder annotation for adding function to process cinder notification.\n\n    if event_type include wildcard, will put {pattern: function} into process_wildcard dict\n    else will put {event_type: function} into process dict\n\n    :param arg: event_type of notification\n    \"\"\"\n    check_event_type(Openstack.Cinder, *arg)\n    event_type = arg[0]\n\n    def decorator(func):\n        if event_type.find(\"*\") != -1:\n            event_type_pattern = pre_compile(event_type)\n            cinder_customer_process_wildcard[event_type_pattern] = func\n        else:\n            cinder_customer_process[event_type] = func\n        log.info(\"add function {0} to process event_type:{1}\".format(func.__name__, event_type))\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator", "language": "python", "code": "def cinder(*arg):\n    \"\"\"\n    Cinder annotation for adding function to process cinder notification.\n\n    if event_type include wildcard, will put {pattern: function} into process_wildcard dict\n    else will put {event_type: function} into process dict\n\n    :param arg: event_type of notification\n    \"\"\"\n    check_event_type(Openstack.Cinder, *arg)\n    event_type = arg[0]\n\n    def decorator(func):\n        if event_type.find(\"*\") != -1:\n            event_type_pattern = pre_compile(event_type)\n            cinder_customer_process_wildcard[event_type_pattern] = func\n        else:\n            cinder_customer_process[event_type] = func\n        log.info(\"add function {0} to process event_type:{1}\".format(func.__name__, event_type))\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator", "code_tokens": ["def", "cinder", "(", "*", "arg", ")", ":", "check_event_type", "(", "Openstack", ".", "Cinder", ",", "*", "arg", ")", "event_type", "=", "arg", "[", "0", "]", "def", "decorator", "(", "func", ")", ":", "if", "event_type", ".", "find", "(", "\"*\"", ")", "!=", "-", "1", ":", "event_type_pattern", "=", "pre_compile", "(", "event_type", ")", "cinder_customer_process_wildcard", "[", "event_type_pattern", "]", "=", "func", "else", ":", "cinder_customer_process", "[", "event_type", "]", "=", "func", "log", ".", "info", "(", "\"add function {0} to process event_type:{1}\"", ".", "format", "(", "func", ".", "__name__", ",", "event_type", ")", ")", "@", "functools", ".", "wraps", "(", "func", ")", "def", "wrapper", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "func", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "wrapper", "return", "decorator"], "docstring": "Cinder annotation for adding function to process cinder notification.\n\n    if event_type include wildcard, will put {pattern: function} into process_wildcard dict\n    else will put {event_type: function} into process dict\n\n    :param arg: event_type of notification", "docstring_tokens": ["Cinder", "annotation", "for", "adding", "function", "to", "process", "cinder", "notification", "."], "sha": "c05aec10029e645d63ff04313dbcf2644743481f", "url": "https://github.com/ndrlslz/ternya/blob/c05aec10029e645d63ff04313dbcf2644743481f/ternya/annotation.py#L82-L108", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/pip/download.py", "func_name": "is_url", "original_string": "def is_url(name):\n    \"\"\"Returns true if the name looks like a URL\"\"\"\n    if ':' not in name:\n        return False\n    scheme = name.split(':', 1)[0].lower()\n    return scheme in ['http', 'https', 'file', 'ftp'] + vcs.all_schemes", "language": "python", "code": "def is_url(name):\n    \"\"\"Returns true if the name looks like a URL\"\"\"\n    if ':' not in name:\n        return False\n    scheme = name.split(':', 1)[0].lower()\n    return scheme in ['http', 'https', 'file', 'ftp'] + vcs.all_schemes", "code_tokens": ["def", "is_url", "(", "name", ")", ":", "if", "':'", "not", "in", "name", ":", "return", "False", "scheme", "=", "name", ".", "split", "(", "':'", ",", "1", ")", "[", "0", "]", ".", "lower", "(", ")", "return", "scheme", "in", "[", "'http'", ",", "'https'", ",", "'file'", ",", "'ftp'", "]", "+", "vcs", ".", "all_schemes"], "docstring": "Returns true if the name looks like a URL", "docstring_tokens": ["Returns", "true", "if", "the", "name", "looks", "like", "a", "URL"], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/pip/download.py#L425-L430", "partition": "test"}
{"repo": "mdgoldberg/sportsref", "path": "sportsref/nfl/boxscores.py", "func_name": "BoxScore.winner", "original_string": "def winner(self):\n        \"\"\"Returns the team ID of the winning team. Returns NaN if a tie.\"\"\"\n        hmScore = self.home_score()\n        awScore = self.away_score()\n        if hmScore > awScore:\n            return self.home()\n        elif hmScore < awScore:\n            return self.away()\n        else:\n            return None", "language": "python", "code": "def winner(self):\n        \"\"\"Returns the team ID of the winning team. Returns NaN if a tie.\"\"\"\n        hmScore = self.home_score()\n        awScore = self.away_score()\n        if hmScore > awScore:\n            return self.home()\n        elif hmScore < awScore:\n            return self.away()\n        else:\n            return None", "code_tokens": ["def", "winner", "(", "self", ")", ":", "hmScore", "=", "self", ".", "home_score", "(", ")", "awScore", "=", "self", ".", "away_score", "(", ")", "if", "hmScore", ">", "awScore", ":", "return", "self", ".", "home", "(", ")", "elif", "hmScore", "<", "awScore", ":", "return", "self", ".", "away", "(", ")", "else", ":", "return", "None"], "docstring": "Returns the team ID of the winning team. Returns NaN if a tie.", "docstring_tokens": ["Returns", "the", "team", "ID", "of", "the", "winning", "team", ".", "Returns", "NaN", "if", "a", "tie", "."], "sha": "09f11ac856a23c96d666d1d510bb35d6f050b5c3", "url": "https://github.com/mdgoldberg/sportsref/blob/09f11ac856a23c96d666d1d510bb35d6f050b5c3/sportsref/nfl/boxscores.py#L115-L124", "partition": "test"}
{"repo": "rwl/godot", "path": "godot/ui/graph_editor.py", "func_name": "GraphNode.when_label_changed", "original_string": "def when_label_changed ( self, object, listener, remove ):\n        \"\"\" Sets up or removes a listener for the label being changed on a\n            specified object.\n        \"\"\"\n        label = self.label\n        if label[:1] != '=':\n            object.on_trait_change( listener, label, remove = remove,\n                                    dispatch = 'ui' )", "language": "python", "code": "def when_label_changed ( self, object, listener, remove ):\n        \"\"\" Sets up or removes a listener for the label being changed on a\n            specified object.\n        \"\"\"\n        label = self.label\n        if label[:1] != '=':\n            object.on_trait_change( listener, label, remove = remove,\n                                    dispatch = 'ui' )", "code_tokens": ["def", "when_label_changed", "(", "self", ",", "object", ",", "listener", ",", "remove", ")", ":", "label", "=", "self", ".", "label", "if", "label", "[", ":", "1", "]", "!=", "'='", ":", "object", ".", "on_trait_change", "(", "listener", ",", "label", ",", "remove", "=", "remove", ",", "dispatch", "=", "'ui'", ")"], "docstring": "Sets up or removes a listener for the label being changed on a\n            specified object.", "docstring_tokens": ["Sets", "up", "or", "removes", "a", "listener", "for", "the", "label", "being", "changed", "on", "a", "specified", "object", "."], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/ui/graph_editor.py#L133-L140", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py", "func_name": "ServiceManagementService.check_hosted_service_name_availability", "original_string": "def check_hosted_service_name_availability(self, service_name):\n        '''\n        Checks to see if the specified hosted service name is available, or if\n        it has already been taken.\n\n        service_name:\n            Name of the hosted service.\n        '''\n        _validate_not_none('service_name', service_name)\n        return self._perform_get(\n            '/' + self.subscription_id +\n            '/services/hostedservices/operations/isavailable/' +\n            _str(service_name) + '',\n            AvailabilityResponse)", "language": "python", "code": "def check_hosted_service_name_availability(self, service_name):\n        '''\n        Checks to see if the specified hosted service name is available, or if\n        it has already been taken.\n\n        service_name:\n            Name of the hosted service.\n        '''\n        _validate_not_none('service_name', service_name)\n        return self._perform_get(\n            '/' + self.subscription_id +\n            '/services/hostedservices/operations/isavailable/' +\n            _str(service_name) + '',\n            AvailabilityResponse)", "code_tokens": ["def", "check_hosted_service_name_availability", "(", "self", ",", "service_name", ")", ":", "_validate_not_none", "(", "'service_name'", ",", "service_name", ")", "return", "self", ".", "_perform_get", "(", "'/'", "+", "self", ".", "subscription_id", "+", "'/services/hostedservices/operations/isavailable/'", "+", "_str", "(", "service_name", ")", "+", "''", ",", "AvailabilityResponse", ")"], "docstring": "Checks to see if the specified hosted service name is available, or if\n        it has already been taken.\n\n        service_name:\n            Name of the hosted service.", "docstring_tokens": ["Checks", "to", "see", "if", "the", "specified", "hosted", "service", "name", "is", "available", "or", "if", "it", "has", "already", "been", "taken", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py#L874-L887", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/util/utils.py", "func_name": "tiny", "original_string": "def tiny(x):\n    '''Compute the tiny-value corresponding to an input's data type.\n\n    This is the smallest \"usable\" number representable in `x`'s\n    data type (e.g., float32).\n\n    This is primarily useful for determining a threshold for\n    numerical underflow in division or multiplication operations.\n\n    Parameters\n    ----------\n    x : number or np.ndarray\n        The array to compute the tiny-value for.\n        All that matters here is `x.dtype`.\n\n    Returns\n    -------\n    tiny_value : float\n        The smallest positive usable number for the type of `x`.\n        If `x` is integer-typed, then the tiny value for `np.float32`\n        is returned instead.\n\n    See Also\n    --------\n    numpy.finfo\n\n    Examples\n    --------\n\n    For a standard double-precision floating point number:\n\n    >>> librosa.util.tiny(1.0)\n    2.2250738585072014e-308\n\n    Or explicitly as double-precision\n\n    >>> librosa.util.tiny(np.asarray(1e-5, dtype=np.float64))\n    2.2250738585072014e-308\n\n    Or complex numbers\n\n    >>> librosa.util.tiny(1j)\n    2.2250738585072014e-308\n\n    Single-precision floating point:\n\n    >>> librosa.util.tiny(np.asarray(1e-5, dtype=np.float32))\n    1.1754944e-38\n\n    Integer\n\n    >>> librosa.util.tiny(5)\n    1.1754944e-38\n    '''\n\n    # Make sure we have an array view\n    x = np.asarray(x)\n\n    # Only floating types generate a tiny\n    if np.issubdtype(x.dtype, np.floating) or np.issubdtype(x.dtype, np.complexfloating):\n        dtype = x.dtype\n    else:\n        dtype = np.float32\n\n    return np.finfo(dtype).tiny", "language": "python", "code": "def tiny(x):\n    '''Compute the tiny-value corresponding to an input's data type.\n\n    This is the smallest \"usable\" number representable in `x`'s\n    data type (e.g., float32).\n\n    This is primarily useful for determining a threshold for\n    numerical underflow in division or multiplication operations.\n\n    Parameters\n    ----------\n    x : number or np.ndarray\n        The array to compute the tiny-value for.\n        All that matters here is `x.dtype`.\n\n    Returns\n    -------\n    tiny_value : float\n        The smallest positive usable number for the type of `x`.\n        If `x` is integer-typed, then the tiny value for `np.float32`\n        is returned instead.\n\n    See Also\n    --------\n    numpy.finfo\n\n    Examples\n    --------\n\n    For a standard double-precision floating point number:\n\n    >>> librosa.util.tiny(1.0)\n    2.2250738585072014e-308\n\n    Or explicitly as double-precision\n\n    >>> librosa.util.tiny(np.asarray(1e-5, dtype=np.float64))\n    2.2250738585072014e-308\n\n    Or complex numbers\n\n    >>> librosa.util.tiny(1j)\n    2.2250738585072014e-308\n\n    Single-precision floating point:\n\n    >>> librosa.util.tiny(np.asarray(1e-5, dtype=np.float32))\n    1.1754944e-38\n\n    Integer\n\n    >>> librosa.util.tiny(5)\n    1.1754944e-38\n    '''\n\n    # Make sure we have an array view\n    x = np.asarray(x)\n\n    # Only floating types generate a tiny\n    if np.issubdtype(x.dtype, np.floating) or np.issubdtype(x.dtype, np.complexfloating):\n        dtype = x.dtype\n    else:\n        dtype = np.float32\n\n    return np.finfo(dtype).tiny", "code_tokens": ["def", "tiny", "(", "x", ")", ":", "# Make sure we have an array view", "x", "=", "np", ".", "asarray", "(", "x", ")", "# Only floating types generate a tiny", "if", "np", ".", "issubdtype", "(", "x", ".", "dtype", ",", "np", ".", "floating", ")", "or", "np", ".", "issubdtype", "(", "x", ".", "dtype", ",", "np", ".", "complexfloating", ")", ":", "dtype", "=", "x", ".", "dtype", "else", ":", "dtype", "=", "np", ".", "float32", "return", "np", ".", "finfo", "(", "dtype", ")", ".", "tiny"], "docstring": "Compute the tiny-value corresponding to an input's data type.\n\n    This is the smallest \"usable\" number representable in `x`'s\n    data type (e.g., float32).\n\n    This is primarily useful for determining a threshold for\n    numerical underflow in division or multiplication operations.\n\n    Parameters\n    ----------\n    x : number or np.ndarray\n        The array to compute the tiny-value for.\n        All that matters here is `x.dtype`.\n\n    Returns\n    -------\n    tiny_value : float\n        The smallest positive usable number for the type of `x`.\n        If `x` is integer-typed, then the tiny value for `np.float32`\n        is returned instead.\n\n    See Also\n    --------\n    numpy.finfo\n\n    Examples\n    --------\n\n    For a standard double-precision floating point number:\n\n    >>> librosa.util.tiny(1.0)\n    2.2250738585072014e-308\n\n    Or explicitly as double-precision\n\n    >>> librosa.util.tiny(np.asarray(1e-5, dtype=np.float64))\n    2.2250738585072014e-308\n\n    Or complex numbers\n\n    >>> librosa.util.tiny(1j)\n    2.2250738585072014e-308\n\n    Single-precision floating point:\n\n    >>> librosa.util.tiny(np.asarray(1e-5, dtype=np.float32))\n    1.1754944e-38\n\n    Integer\n\n    >>> librosa.util.tiny(5)\n    1.1754944e-38", "docstring_tokens": ["Compute", "the", "tiny", "-", "value", "corresponding", "to", "an", "input", "s", "data", "type", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/util/utils.py#L1510-L1574", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/frame.py", "func_name": "H2OFrame.set_name", "original_string": "def set_name(self, col=None, name=None):\n        \"\"\"\n        Set a new name for a column.\n\n        :param col: index or name of the column whose name is to be set; may be skipped for 1-column frames\n        :param name: the new name of the column\n        \"\"\"\n        assert_is_type(col, None, int, str)\n        assert_is_type(name, str)\n        ncols = self.ncols\n\n        col_index = None\n        if is_type(col, int):\n            if not(-ncols <= col < ncols):\n                raise H2OValueError(\"Index %d is out of bounds for a frame with %d columns\" % (col, ncols))\n            col_index = (col + ncols) % ncols  # handle negative indices\n        elif is_type(col, str):\n            if col not in self.names:\n                raise H2OValueError(\"Column %s doesn't exist in the frame.\" % col)\n            col_index = self.names.index(col)  # lookup the name\n        else:\n            assert col is None\n            if ncols != 1:\n                raise H2OValueError(\"The frame has %d columns; please specify which one to rename\" % ncols)\n            col_index = 0\n        if name != self.names[col_index] and name in self.types:\n            raise H2OValueError(\"Column '%s' already exists in the frame\" % name)\n\n        oldname = self.names[col_index]\n        old_cache = self._ex._cache\n        self._ex = ExprNode(\"colnames=\", self, col_index, name)  # Update-in-place, but still lazy\n        self._ex._cache.fill_from(old_cache)\n        if self.names is None:\n            self._frame()._ex._cache.fill()\n        else:\n            self._ex._cache._names = self.names[:col_index] + [name] + self.names[col_index + 1:]\n            self._ex._cache._types[name] = self._ex._cache._types.pop(oldname)\n        return", "language": "python", "code": "def set_name(self, col=None, name=None):\n        \"\"\"\n        Set a new name for a column.\n\n        :param col: index or name of the column whose name is to be set; may be skipped for 1-column frames\n        :param name: the new name of the column\n        \"\"\"\n        assert_is_type(col, None, int, str)\n        assert_is_type(name, str)\n        ncols = self.ncols\n\n        col_index = None\n        if is_type(col, int):\n            if not(-ncols <= col < ncols):\n                raise H2OValueError(\"Index %d is out of bounds for a frame with %d columns\" % (col, ncols))\n            col_index = (col + ncols) % ncols  # handle negative indices\n        elif is_type(col, str):\n            if col not in self.names:\n                raise H2OValueError(\"Column %s doesn't exist in the frame.\" % col)\n            col_index = self.names.index(col)  # lookup the name\n        else:\n            assert col is None\n            if ncols != 1:\n                raise H2OValueError(\"The frame has %d columns; please specify which one to rename\" % ncols)\n            col_index = 0\n        if name != self.names[col_index] and name in self.types:\n            raise H2OValueError(\"Column '%s' already exists in the frame\" % name)\n\n        oldname = self.names[col_index]\n        old_cache = self._ex._cache\n        self._ex = ExprNode(\"colnames=\", self, col_index, name)  # Update-in-place, but still lazy\n        self._ex._cache.fill_from(old_cache)\n        if self.names is None:\n            self._frame()._ex._cache.fill()\n        else:\n            self._ex._cache._names = self.names[:col_index] + [name] + self.names[col_index + 1:]\n            self._ex._cache._types[name] = self._ex._cache._types.pop(oldname)\n        return", "code_tokens": ["def", "set_name", "(", "self", ",", "col", "=", "None", ",", "name", "=", "None", ")", ":", "assert_is_type", "(", "col", ",", "None", ",", "int", ",", "str", ")", "assert_is_type", "(", "name", ",", "str", ")", "ncols", "=", "self", ".", "ncols", "col_index", "=", "None", "if", "is_type", "(", "col", ",", "int", ")", ":", "if", "not", "(", "-", "ncols", "<=", "col", "<", "ncols", ")", ":", "raise", "H2OValueError", "(", "\"Index %d is out of bounds for a frame with %d columns\"", "%", "(", "col", ",", "ncols", ")", ")", "col_index", "=", "(", "col", "+", "ncols", ")", "%", "ncols", "# handle negative indices", "elif", "is_type", "(", "col", ",", "str", ")", ":", "if", "col", "not", "in", "self", ".", "names", ":", "raise", "H2OValueError", "(", "\"Column %s doesn't exist in the frame.\"", "%", "col", ")", "col_index", "=", "self", ".", "names", ".", "index", "(", "col", ")", "# lookup the name", "else", ":", "assert", "col", "is", "None", "if", "ncols", "!=", "1", ":", "raise", "H2OValueError", "(", "\"The frame has %d columns; please specify which one to rename\"", "%", "ncols", ")", "col_index", "=", "0", "if", "name", "!=", "self", ".", "names", "[", "col_index", "]", "and", "name", "in", "self", ".", "types", ":", "raise", "H2OValueError", "(", "\"Column '%s' already exists in the frame\"", "%", "name", ")", "oldname", "=", "self", ".", "names", "[", "col_index", "]", "old_cache", "=", "self", ".", "_ex", ".", "_cache", "self", ".", "_ex", "=", "ExprNode", "(", "\"colnames=\"", ",", "self", ",", "col_index", ",", "name", ")", "# Update-in-place, but still lazy", "self", ".", "_ex", ".", "_cache", ".", "fill_from", "(", "old_cache", ")", "if", "self", ".", "names", "is", "None", ":", "self", ".", "_frame", "(", ")", ".", "_ex", ".", "_cache", ".", "fill", "(", ")", "else", ":", "self", ".", "_ex", ".", "_cache", ".", "_names", "=", "self", ".", "names", "[", ":", "col_index", "]", "+", "[", "name", "]", "+", "self", ".", "names", "[", "col_index", "+", "1", ":", "]", "self", ".", "_ex", ".", "_cache", ".", "_types", "[", "name", "]", "=", "self", ".", "_ex", ".", "_cache", ".", "_types", ".", "pop", "(", "oldname", ")", "return"], "docstring": "Set a new name for a column.\n\n        :param col: index or name of the column whose name is to be set; may be skipped for 1-column frames\n        :param name: the new name of the column", "docstring_tokens": ["Set", "a", "new", "name", "for", "a", "column", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/frame.py#L1073-L1110", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/control.py", "func_name": "coverage._should_trace_with_reason", "original_string": "def _should_trace_with_reason(self, filename, frame):\n        \"\"\"Decide whether to trace execution in `filename`, with a reason.\n\n        This function is called from the trace function.  As each new file name\n        is encountered, this function determines whether it is traced or not.\n\n        Returns a pair of values:  the first indicates whether the file should\n        be traced: it's a canonicalized filename if it should be traced, None\n        if it should not.  The second value is a string, the resason for the\n        decision.\n\n        \"\"\"\n        if not filename:\n            # Empty string is pretty useless\n            return None, \"empty string isn't a filename\"\n\n        if filename.startswith('<'):\n            # Lots of non-file execution is represented with artificial\n            # filenames like \"<string>\", \"<doctest readme.txt[0]>\", or\n            # \"<exec_function>\".  Don't ever trace these executions, since we\n            # can't do anything with the data later anyway.\n            return None, \"not a real filename\"\n\n        self._check_for_packages()\n\n        # Compiled Python files have two filenames: frame.f_code.co_filename is\n        # the filename at the time the .pyc was compiled.  The second name is\n        # __file__, which is where the .pyc was actually loaded from.  Since\n        # .pyc files can be moved after compilation (for example, by being\n        # installed), we look for __file__ in the frame and prefer it to the\n        # co_filename value.\n        dunder_file = frame.f_globals.get('__file__')\n        if dunder_file:\n            filename = self._source_for_file(dunder_file)\n\n        # Jython reports the .class file to the tracer, use the source file.\n        if filename.endswith(\"$py.class\"):\n            filename = filename[:-9] + \".py\"\n\n        canonical = self.file_locator.canonical_filename(filename)\n\n        # If the user specified source or include, then that's authoritative\n        # about the outer bound of what to measure and we don't have to apply\n        # any canned exclusions. If they didn't, then we have to exclude the\n        # stdlib and coverage.py directories.\n        if self.source_match:\n            if not self.source_match.match(canonical):\n                return None, \"falls outside the --source trees\"\n        elif self.include_match:\n            if not self.include_match.match(canonical):\n                return None, \"falls outside the --include trees\"\n        else:\n            # If we aren't supposed to trace installed code, then check if this\n            # is near the Python standard library and skip it if so.\n            if self.pylib_match and self.pylib_match.match(canonical):\n                return None, \"is in the stdlib\"\n\n            # We exclude the coverage code itself, since a little of it will be\n            # measured otherwise.\n            if self.cover_match and self.cover_match.match(canonical):\n                return None, \"is part of coverage.py\"\n\n        # Check the file against the omit pattern.\n        if self.omit_match and self.omit_match.match(canonical):\n            return None, \"is inside an --omit pattern\"\n\n        return canonical, \"because we love you\"", "language": "python", "code": "def _should_trace_with_reason(self, filename, frame):\n        \"\"\"Decide whether to trace execution in `filename`, with a reason.\n\n        This function is called from the trace function.  As each new file name\n        is encountered, this function determines whether it is traced or not.\n\n        Returns a pair of values:  the first indicates whether the file should\n        be traced: it's a canonicalized filename if it should be traced, None\n        if it should not.  The second value is a string, the resason for the\n        decision.\n\n        \"\"\"\n        if not filename:\n            # Empty string is pretty useless\n            return None, \"empty string isn't a filename\"\n\n        if filename.startswith('<'):\n            # Lots of non-file execution is represented with artificial\n            # filenames like \"<string>\", \"<doctest readme.txt[0]>\", or\n            # \"<exec_function>\".  Don't ever trace these executions, since we\n            # can't do anything with the data later anyway.\n            return None, \"not a real filename\"\n\n        self._check_for_packages()\n\n        # Compiled Python files have two filenames: frame.f_code.co_filename is\n        # the filename at the time the .pyc was compiled.  The second name is\n        # __file__, which is where the .pyc was actually loaded from.  Since\n        # .pyc files can be moved after compilation (for example, by being\n        # installed), we look for __file__ in the frame and prefer it to the\n        # co_filename value.\n        dunder_file = frame.f_globals.get('__file__')\n        if dunder_file:\n            filename = self._source_for_file(dunder_file)\n\n        # Jython reports the .class file to the tracer, use the source file.\n        if filename.endswith(\"$py.class\"):\n            filename = filename[:-9] + \".py\"\n\n        canonical = self.file_locator.canonical_filename(filename)\n\n        # If the user specified source or include, then that's authoritative\n        # about the outer bound of what to measure and we don't have to apply\n        # any canned exclusions. If they didn't, then we have to exclude the\n        # stdlib and coverage.py directories.\n        if self.source_match:\n            if not self.source_match.match(canonical):\n                return None, \"falls outside the --source trees\"\n        elif self.include_match:\n            if not self.include_match.match(canonical):\n                return None, \"falls outside the --include trees\"\n        else:\n            # If we aren't supposed to trace installed code, then check if this\n            # is near the Python standard library and skip it if so.\n            if self.pylib_match and self.pylib_match.match(canonical):\n                return None, \"is in the stdlib\"\n\n            # We exclude the coverage code itself, since a little of it will be\n            # measured otherwise.\n            if self.cover_match and self.cover_match.match(canonical):\n                return None, \"is part of coverage.py\"\n\n        # Check the file against the omit pattern.\n        if self.omit_match and self.omit_match.match(canonical):\n            return None, \"is inside an --omit pattern\"\n\n        return canonical, \"because we love you\"", "code_tokens": ["def", "_should_trace_with_reason", "(", "self", ",", "filename", ",", "frame", ")", ":", "if", "not", "filename", ":", "# Empty string is pretty useless", "return", "None", ",", "\"empty string isn't a filename\"", "if", "filename", ".", "startswith", "(", "'<'", ")", ":", "# Lots of non-file execution is represented with artificial", "# filenames like \"<string>\", \"<doctest readme.txt[0]>\", or", "# \"<exec_function>\".  Don't ever trace these executions, since we", "# can't do anything with the data later anyway.", "return", "None", ",", "\"not a real filename\"", "self", ".", "_check_for_packages", "(", ")", "# Compiled Python files have two filenames: frame.f_code.co_filename is", "# the filename at the time the .pyc was compiled.  The second name is", "# __file__, which is where the .pyc was actually loaded from.  Since", "# .pyc files can be moved after compilation (for example, by being", "# installed), we look for __file__ in the frame and prefer it to the", "# co_filename value.", "dunder_file", "=", "frame", ".", "f_globals", ".", "get", "(", "'__file__'", ")", "if", "dunder_file", ":", "filename", "=", "self", ".", "_source_for_file", "(", "dunder_file", ")", "# Jython reports the .class file to the tracer, use the source file.", "if", "filename", ".", "endswith", "(", "\"$py.class\"", ")", ":", "filename", "=", "filename", "[", ":", "-", "9", "]", "+", "\".py\"", "canonical", "=", "self", ".", "file_locator", ".", "canonical_filename", "(", "filename", ")", "# If the user specified source or include, then that's authoritative", "# about the outer bound of what to measure and we don't have to apply", "# any canned exclusions. If they didn't, then we have to exclude the", "# stdlib and coverage.py directories.", "if", "self", ".", "source_match", ":", "if", "not", "self", ".", "source_match", ".", "match", "(", "canonical", ")", ":", "return", "None", ",", "\"falls outside the --source trees\"", "elif", "self", ".", "include_match", ":", "if", "not", "self", ".", "include_match", ".", "match", "(", "canonical", ")", ":", "return", "None", ",", "\"falls outside the --include trees\"", "else", ":", "# If we aren't supposed to trace installed code, then check if this", "# is near the Python standard library and skip it if so.", "if", "self", ".", "pylib_match", "and", "self", ".", "pylib_match", ".", "match", "(", "canonical", ")", ":", "return", "None", ",", "\"is in the stdlib\"", "# We exclude the coverage code itself, since a little of it will be", "# measured otherwise.", "if", "self", ".", "cover_match", "and", "self", ".", "cover_match", ".", "match", "(", "canonical", ")", ":", "return", "None", ",", "\"is part of coverage.py\"", "# Check the file against the omit pattern.", "if", "self", ".", "omit_match", "and", "self", ".", "omit_match", ".", "match", "(", "canonical", ")", ":", "return", "None", ",", "\"is inside an --omit pattern\"", "return", "canonical", ",", "\"because we love you\""], "docstring": "Decide whether to trace execution in `filename`, with a reason.\n\n        This function is called from the trace function.  As each new file name\n        is encountered, this function determines whether it is traced or not.\n\n        Returns a pair of values:  the first indicates whether the file should\n        be traced: it's a canonicalized filename if it should be traced, None\n        if it should not.  The second value is a string, the resason for the\n        decision.", "docstring_tokens": ["Decide", "whether", "to", "trace", "execution", "in", "filename", "with", "a", "reason", "."], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/control.py#L222-L288", "partition": "test"}
{"repo": "neurosynth/neurosynth", "path": "neurosynth/analysis/classify.py", "func_name": "get_studies_by_regions", "original_string": "def get_studies_by_regions(dataset, masks, threshold=0.08, remove_overlap=True,\n                           studies=None, features=None,\n                           regularization=\"scale\"):\n    \"\"\" Set up data for a classification task given a set of masks\n\n        Given a set of masks, this function retrieves studies associated with\n        each mask at the specified threshold, optionally removes overlap and\n        filters by studies and features, and returns studies by feature matrix\n        (X) and class labels (y)\n\n        Args:\n            dataset: a Neurosynth dataset\n            maks: a list of paths to Nifti masks\n            threshold: percentage of voxels active within the mask for study\n                to be included\n            remove_overlap: A boolean indicating if studies studies that\n                appear in more than one mask should be excluded\n            studies: An optional list of study names used to constrain the set\n                used in classification. If None, will use all features in the\n                dataset.\n            features: An optional list of feature names used to constrain the\n                set used in classification. If None, will use all features in\n                the dataset.\n            regularize: Optional boolean indicating if X should be regularized\n\n        Returns:\n            A tuple (X, y) of np arrays.\n            X is a feature by studies matrix and y is a vector of class labels\n    \"\"\"\n\n    import nibabel as nib\n    import os\n\n    # Load masks using NiBabel\n\n    try:\n        loaded_masks = [nib.load(os.path.relpath(m)) for m in masks]\n    except OSError:\n        print('Error loading masks. Check the path')\n\n    # Get a list of studies that activate for each mask file--i.e.,  a list of\n    # lists\n\n    grouped_ids = [dataset.get_studies(mask=m, activation_threshold=threshold)\n                   for m in loaded_masks]\n\n    # Flattened ids\n\n    flat_ids = reduce(lambda a, b: a + b, grouped_ids)\n\n    # Remove duplicates\n\n    if remove_overlap:\n        import collections\n        flat_ids = [id for (id, count) in\n                    collections.Counter(flat_ids).items() if count == 1]\n        grouped_ids = [[x for x in m if x in flat_ids] for m in\n                       grouped_ids]  # Remove\n\n    # Create class label(y)\n    y = [[idx] * len(ids) for (idx, ids) in enumerate(grouped_ids)]\n    y = reduce(lambda a, b: a + b, y)  # Flatten\n    y = np.array(y)\n\n    # Extract feature set for each class separately\n    X = [dataset.get_feature_data(ids=group_ids, features=features)\n         for group_ids in grouped_ids]\n\n    X = np.vstack(tuple(X))\n\n    if regularization:\n        X = regularize(X, method=regularization)\n\n    return (X, y)", "language": "python", "code": "def get_studies_by_regions(dataset, masks, threshold=0.08, remove_overlap=True,\n                           studies=None, features=None,\n                           regularization=\"scale\"):\n    \"\"\" Set up data for a classification task given a set of masks\n\n        Given a set of masks, this function retrieves studies associated with\n        each mask at the specified threshold, optionally removes overlap and\n        filters by studies and features, and returns studies by feature matrix\n        (X) and class labels (y)\n\n        Args:\n            dataset: a Neurosynth dataset\n            maks: a list of paths to Nifti masks\n            threshold: percentage of voxels active within the mask for study\n                to be included\n            remove_overlap: A boolean indicating if studies studies that\n                appear in more than one mask should be excluded\n            studies: An optional list of study names used to constrain the set\n                used in classification. If None, will use all features in the\n                dataset.\n            features: An optional list of feature names used to constrain the\n                set used in classification. If None, will use all features in\n                the dataset.\n            regularize: Optional boolean indicating if X should be regularized\n\n        Returns:\n            A tuple (X, y) of np arrays.\n            X is a feature by studies matrix and y is a vector of class labels\n    \"\"\"\n\n    import nibabel as nib\n    import os\n\n    # Load masks using NiBabel\n\n    try:\n        loaded_masks = [nib.load(os.path.relpath(m)) for m in masks]\n    except OSError:\n        print('Error loading masks. Check the path')\n\n    # Get a list of studies that activate for each mask file--i.e.,  a list of\n    # lists\n\n    grouped_ids = [dataset.get_studies(mask=m, activation_threshold=threshold)\n                   for m in loaded_masks]\n\n    # Flattened ids\n\n    flat_ids = reduce(lambda a, b: a + b, grouped_ids)\n\n    # Remove duplicates\n\n    if remove_overlap:\n        import collections\n        flat_ids = [id for (id, count) in\n                    collections.Counter(flat_ids).items() if count == 1]\n        grouped_ids = [[x for x in m if x in flat_ids] for m in\n                       grouped_ids]  # Remove\n\n    # Create class label(y)\n    y = [[idx] * len(ids) for (idx, ids) in enumerate(grouped_ids)]\n    y = reduce(lambda a, b: a + b, y)  # Flatten\n    y = np.array(y)\n\n    # Extract feature set for each class separately\n    X = [dataset.get_feature_data(ids=group_ids, features=features)\n         for group_ids in grouped_ids]\n\n    X = np.vstack(tuple(X))\n\n    if regularization:\n        X = regularize(X, method=regularization)\n\n    return (X, y)", "code_tokens": ["def", "get_studies_by_regions", "(", "dataset", ",", "masks", ",", "threshold", "=", "0.08", ",", "remove_overlap", "=", "True", ",", "studies", "=", "None", ",", "features", "=", "None", ",", "regularization", "=", "\"scale\"", ")", ":", "import", "nibabel", "as", "nib", "import", "os", "# Load masks using NiBabel", "try", ":", "loaded_masks", "=", "[", "nib", ".", "load", "(", "os", ".", "path", ".", "relpath", "(", "m", ")", ")", "for", "m", "in", "masks", "]", "except", "OSError", ":", "print", "(", "'Error loading masks. Check the path'", ")", "# Get a list of studies that activate for each mask file--i.e.,  a list of", "# lists", "grouped_ids", "=", "[", "dataset", ".", "get_studies", "(", "mask", "=", "m", ",", "activation_threshold", "=", "threshold", ")", "for", "m", "in", "loaded_masks", "]", "# Flattened ids", "flat_ids", "=", "reduce", "(", "lambda", "a", ",", "b", ":", "a", "+", "b", ",", "grouped_ids", ")", "# Remove duplicates", "if", "remove_overlap", ":", "import", "collections", "flat_ids", "=", "[", "id", "for", "(", "id", ",", "count", ")", "in", "collections", ".", "Counter", "(", "flat_ids", ")", ".", "items", "(", ")", "if", "count", "==", "1", "]", "grouped_ids", "=", "[", "[", "x", "for", "x", "in", "m", "if", "x", "in", "flat_ids", "]", "for", "m", "in", "grouped_ids", "]", "# Remove", "# Create class label(y)", "y", "=", "[", "[", "idx", "]", "*", "len", "(", "ids", ")", "for", "(", "idx", ",", "ids", ")", "in", "enumerate", "(", "grouped_ids", ")", "]", "y", "=", "reduce", "(", "lambda", "a", ",", "b", ":", "a", "+", "b", ",", "y", ")", "# Flatten", "y", "=", "np", ".", "array", "(", "y", ")", "# Extract feature set for each class separately", "X", "=", "[", "dataset", ".", "get_feature_data", "(", "ids", "=", "group_ids", ",", "features", "=", "features", ")", "for", "group_ids", "in", "grouped_ids", "]", "X", "=", "np", ".", "vstack", "(", "tuple", "(", "X", ")", ")", "if", "regularization", ":", "X", "=", "regularize", "(", "X", ",", "method", "=", "regularization", ")", "return", "(", "X", ",", "y", ")"], "docstring": "Set up data for a classification task given a set of masks\n\n        Given a set of masks, this function retrieves studies associated with\n        each mask at the specified threshold, optionally removes overlap and\n        filters by studies and features, and returns studies by feature matrix\n        (X) and class labels (y)\n\n        Args:\n            dataset: a Neurosynth dataset\n            maks: a list of paths to Nifti masks\n            threshold: percentage of voxels active within the mask for study\n                to be included\n            remove_overlap: A boolean indicating if studies studies that\n                appear in more than one mask should be excluded\n            studies: An optional list of study names used to constrain the set\n                used in classification. If None, will use all features in the\n                dataset.\n            features: An optional list of feature names used to constrain the\n                set used in classification. If None, will use all features in\n                the dataset.\n            regularize: Optional boolean indicating if X should be regularized\n\n        Returns:\n            A tuple (X, y) of np arrays.\n            X is a feature by studies matrix and y is a vector of class labels", "docstring_tokens": ["Set", "up", "data", "for", "a", "classification", "task", "given", "a", "set", "of", "masks"], "sha": "948ce7edce15d7df693446e76834e0c23bfe8f11", "url": "https://github.com/neurosynth/neurosynth/blob/948ce7edce15d7df693446e76834e0c23bfe8f11/neurosynth/analysis/classify.py#L64-L137", "partition": "test"}
{"repo": "dopefishh/pympi", "path": "pympi/Elan.py", "func_name": "Eaf.generate_annotation_id", "original_string": "def generate_annotation_id(self):\n        \"\"\"Generate the next annotation id, this function is mainly used\n        internally.\n        \"\"\"\n        if not self.maxaid:\n            valid_anns = [int(''.join(filter(str.isdigit, a)))\n                          for a in self.timeslots]\n            self.maxaid = max(valid_anns + [1])+1\n        else:\n            self.maxaid += 1\n        return 'a{:d}'.format(self.maxaid)", "language": "python", "code": "def generate_annotation_id(self):\n        \"\"\"Generate the next annotation id, this function is mainly used\n        internally.\n        \"\"\"\n        if not self.maxaid:\n            valid_anns = [int(''.join(filter(str.isdigit, a)))\n                          for a in self.timeslots]\n            self.maxaid = max(valid_anns + [1])+1\n        else:\n            self.maxaid += 1\n        return 'a{:d}'.format(self.maxaid)", "code_tokens": ["def", "generate_annotation_id", "(", "self", ")", ":", "if", "not", "self", ".", "maxaid", ":", "valid_anns", "=", "[", "int", "(", "''", ".", "join", "(", "filter", "(", "str", ".", "isdigit", ",", "a", ")", ")", ")", "for", "a", "in", "self", ".", "timeslots", "]", "self", ".", "maxaid", "=", "max", "(", "valid_anns", "+", "[", "1", "]", ")", "+", "1", "else", ":", "self", ".", "maxaid", "+=", "1", "return", "'a{:d}'", ".", "format", "(", "self", ".", "maxaid", ")"], "docstring": "Generate the next annotation id, this function is mainly used\n        internally.", "docstring_tokens": ["Generate", "the", "next", "annotation", "id", "this", "function", "is", "mainly", "used", "internally", "."], "sha": "79c747cde45b5ba203ed93154d8c123ac9c3ef56", "url": "https://github.com/dopefishh/pympi/blob/79c747cde45b5ba203ed93154d8c123ac9c3ef56/pympi/Elan.py#L514-L524", "partition": "test"}
{"repo": "elliterate/capybara.py", "path": "capybara/helpers.py", "func_name": "normalize_whitespace", "original_string": "def normalize_whitespace(text):\n    \"\"\"\n    Returns the given text with outer whitespace removed and inner whitespace collapsed.\n\n    Args:\n        text (str): The text to normalize.\n\n    Returns:\n        str: The normalized text.\n    \"\"\"\n\n    return re.sub(r\"\\s+\", \" \", text, flags=re.UNICODE).strip()", "language": "python", "code": "def normalize_whitespace(text):\n    \"\"\"\n    Returns the given text with outer whitespace removed and inner whitespace collapsed.\n\n    Args:\n        text (str): The text to normalize.\n\n    Returns:\n        str: The normalized text.\n    \"\"\"\n\n    return re.sub(r\"\\s+\", \" \", text, flags=re.UNICODE).strip()", "code_tokens": ["def", "normalize_whitespace", "(", "text", ")", ":", "return", "re", ".", "sub", "(", "r\"\\s+\"", ",", "\" \"", ",", "text", ",", "flags", "=", "re", ".", "UNICODE", ")", ".", "strip", "(", ")"], "docstring": "Returns the given text with outer whitespace removed and inner whitespace collapsed.\n\n    Args:\n        text (str): The text to normalize.\n\n    Returns:\n        str: The normalized text.", "docstring_tokens": ["Returns", "the", "given", "text", "with", "outer", "whitespace", "removed", "and", "inner", "whitespace", "collapsed", "."], "sha": "0c6ae449cc37e4445ec3cd6af95674533beedc6c", "url": "https://github.com/elliterate/capybara.py/blob/0c6ae449cc37e4445ec3cd6af95674533beedc6c/capybara/helpers.py#L165-L176", "partition": "test"}
{"repo": "oscarbranson/latools", "path": "latools/latools.py", "func_name": "analyse._minimal_export_traces", "original_string": "def _minimal_export_traces(self, outdir=None, analytes=None,\n                               samples=None, subset='All_Analyses'):\n        \"\"\"\n        Used for exporting minimal dataset. DON'T USE.\n        \"\"\"\n        if analytes is None:\n            analytes = self.analytes\n        elif isinstance(analytes, str):\n            analytes = [analytes]\n\n        if samples is not None:\n            subset = self.make_subset(samples)\n\n        samples = self._get_samples(subset)\n\n        focus_stage = 'rawdata'\n        # ud = 'counts'\n\n        if not os.path.isdir(outdir):\n            os.mkdir(outdir)\n\n        for s in samples:\n            d = self.data[s].data[focus_stage]\n            out = Bunch()\n\n            for a in analytes:\n                out[a] = d[a]\n\n            out = pd.DataFrame(out, index=self.data[s].Time)\n            out.index.name = 'Time'\n\n            d = dateutil.parser.parse(self.data[s].meta['date'])\n            header = ['# Minimal Reproduction Dataset Exported from LATOOLS on %s' %\n                      (time.strftime('%Y:%m:%d %H:%M:%S')),\n                      \"# Analysis described in '../analysis.lalog'\",\n                      '# Run latools.reproduce to import analysis.',\n                      '#',\n                      '# Sample: %s' % (s),\n                      '# Analysis Time: ' + d.strftime('%Y-%m-%d %H:%M:%S')]\n\n            header = '\\n'.join(header) + '\\n'\n\n            csv = out.to_csv()\n\n            with open('%s/%s.csv' % (outdir, s), 'w') as f:\n                f.write(header)\n                f.write(csv)\n        return", "language": "python", "code": "def _minimal_export_traces(self, outdir=None, analytes=None,\n                               samples=None, subset='All_Analyses'):\n        \"\"\"\n        Used for exporting minimal dataset. DON'T USE.\n        \"\"\"\n        if analytes is None:\n            analytes = self.analytes\n        elif isinstance(analytes, str):\n            analytes = [analytes]\n\n        if samples is not None:\n            subset = self.make_subset(samples)\n\n        samples = self._get_samples(subset)\n\n        focus_stage = 'rawdata'\n        # ud = 'counts'\n\n        if not os.path.isdir(outdir):\n            os.mkdir(outdir)\n\n        for s in samples:\n            d = self.data[s].data[focus_stage]\n            out = Bunch()\n\n            for a in analytes:\n                out[a] = d[a]\n\n            out = pd.DataFrame(out, index=self.data[s].Time)\n            out.index.name = 'Time'\n\n            d = dateutil.parser.parse(self.data[s].meta['date'])\n            header = ['# Minimal Reproduction Dataset Exported from LATOOLS on %s' %\n                      (time.strftime('%Y:%m:%d %H:%M:%S')),\n                      \"# Analysis described in '../analysis.lalog'\",\n                      '# Run latools.reproduce to import analysis.',\n                      '#',\n                      '# Sample: %s' % (s),\n                      '# Analysis Time: ' + d.strftime('%Y-%m-%d %H:%M:%S')]\n\n            header = '\\n'.join(header) + '\\n'\n\n            csv = out.to_csv()\n\n            with open('%s/%s.csv' % (outdir, s), 'w') as f:\n                f.write(header)\n                f.write(csv)\n        return", "code_tokens": ["def", "_minimal_export_traces", "(", "self", ",", "outdir", "=", "None", ",", "analytes", "=", "None", ",", "samples", "=", "None", ",", "subset", "=", "'All_Analyses'", ")", ":", "if", "analytes", "is", "None", ":", "analytes", "=", "self", ".", "analytes", "elif", "isinstance", "(", "analytes", ",", "str", ")", ":", "analytes", "=", "[", "analytes", "]", "if", "samples", "is", "not", "None", ":", "subset", "=", "self", ".", "make_subset", "(", "samples", ")", "samples", "=", "self", ".", "_get_samples", "(", "subset", ")", "focus_stage", "=", "'rawdata'", "# ud = 'counts'", "if", "not", "os", ".", "path", ".", "isdir", "(", "outdir", ")", ":", "os", ".", "mkdir", "(", "outdir", ")", "for", "s", "in", "samples", ":", "d", "=", "self", ".", "data", "[", "s", "]", ".", "data", "[", "focus_stage", "]", "out", "=", "Bunch", "(", ")", "for", "a", "in", "analytes", ":", "out", "[", "a", "]", "=", "d", "[", "a", "]", "out", "=", "pd", ".", "DataFrame", "(", "out", ",", "index", "=", "self", ".", "data", "[", "s", "]", ".", "Time", ")", "out", ".", "index", ".", "name", "=", "'Time'", "d", "=", "dateutil", ".", "parser", ".", "parse", "(", "self", ".", "data", "[", "s", "]", ".", "meta", "[", "'date'", "]", ")", "header", "=", "[", "'# Minimal Reproduction Dataset Exported from LATOOLS on %s'", "%", "(", "time", ".", "strftime", "(", "'%Y:%m:%d %H:%M:%S'", ")", ")", ",", "\"# Analysis described in '../analysis.lalog'\"", ",", "'# Run latools.reproduce to import analysis.'", ",", "'#'", ",", "'# Sample: %s'", "%", "(", "s", ")", ",", "'# Analysis Time: '", "+", "d", ".", "strftime", "(", "'%Y-%m-%d %H:%M:%S'", ")", "]", "header", "=", "'\\n'", ".", "join", "(", "header", ")", "+", "'\\n'", "csv", "=", "out", ".", "to_csv", "(", ")", "with", "open", "(", "'%s/%s.csv'", "%", "(", "outdir", ",", "s", ")", ",", "'w'", ")", "as", "f", ":", "f", ".", "write", "(", "header", ")", "f", ".", "write", "(", "csv", ")", "return"], "docstring": "Used for exporting minimal dataset. DON'T USE.", "docstring_tokens": ["Used", "for", "exporting", "minimal", "dataset", ".", "DON", "T", "USE", "."], "sha": "cd25a650cfee318152f234d992708511f7047fbe", "url": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/latools.py#L3772-L3819", "partition": "test"}
{"repo": "funilrys/PyFunceble", "path": "PyFunceble/prints.py", "func_name": "Prints.header", "original_string": "def header(\n        self, do_not_print=False\n    ):  # pragma: no cover pylint: disable=too-many-branches\n        \"\"\"\n        Management and creation of templates of header.\n        Please consider as \"header\" the title of each columns.\n\n        :param do_not_print:\n            Tell us if we have to print the header or not.\n        :type do_not_print: bool\n        \"\"\"\n\n        if (\n            not PyFunceble.CONFIGURATION[\"header_printed\"]\n            or self.template == \"Percentage\"\n            or do_not_print\n        ):\n            # * The header has not been already printed.\n            # or\n            # * The template is the `Percentage template`.\n            # or\n            # * We are authorized to print something.\n\n            if (\n                self.template.lower() in PyFunceble.STATUS[\"list\"][\"generic\"]\n                or self.template == \"Generic_File\"\n            ):\n                # * The template is into the list of generic status.\n                # or\n                # * The template is equal to `Generic_File`.\n\n                # The data to print is the Generic header.\n                to_print = self.headers[\"Generic\"]\n\n                if (\n                    self.template.lower() in PyFunceble.STATUS[\"list\"][\"generic\"]\n                    and PyFunceble.HTTP_CODE[\"active\"]\n                ):\n                    # * The template is in the list of generic status.\n                    # and\n                    # * the http status code extraction is activated.\n\n                    # We remove the Analyze Date colomn from the data to print.\n                    to_print = Dict(to_print).remove_key(\"Analyze Date\")\n            elif self.template.lower() in PyFunceble.STATUS[\"list\"][\"up\"]:\n                # The template is in the list of up status.\n\n                # We informations to print is the up header.\n                to_print = self.headers[PyFunceble.STATUS[\"official\"][\"up\"]]\n            elif self.template.lower() in PyFunceble.STATUS[\"list\"][\"valid\"]:\n                # The template is in the list of valid status.\n\n                # We informations to print is the valid header.\n                to_print = self.headers[PyFunceble.STATUS[\"official\"][\"valid\"]]\n            elif self.template.lower() in PyFunceble.STATUS[\"list\"][\"down\"]:\n                # The template is in the list of down status.\n\n                # We informations to print is the down header.\n                to_print = self.headers[PyFunceble.STATUS[\"official\"][\"down\"]]\n            elif self.template.lower() in PyFunceble.STATUS[\"list\"][\"invalid\"]:\n                # The template is in the list of invalid status.\n\n                # We informations to print is the invalid header.\n                to_print = self.headers[PyFunceble.STATUS[\"official\"][\"invalid\"]]\n            elif (\n                self.template == \"Less\"\n                or self.template == \"Percentage\"\n                or self.template == \"HTTP\"\n            ):  # pylint: disable=line-too-long\n                # * The template is equal to `Less`.\n                # or\n                # * The template is equal to `Percentage`.\n                # or\n                # * The template is equal to `HTTP`.\n\n                # We get the header with the help of the template name.\n                to_print = self.headers[self.template]\n\n                if self.template == \"Less\" and not PyFunceble.HTTP_CODE[\"active\"]:\n                    # * The template is equal to `Less`.\n                    # and\n                    # * The http status code extraction is deactivated.\n\n                    # We append the source index to the header.\n                    to_print[\"Source\"] = 10\n\n            if not PyFunceble.HTTP_CODE[\"active\"]:\n                # * The http status code extraction is deactivated.\n\n                # We remove the HTTP Code index from the data to print.\n                to_print = Dict(to_print).remove_key(\"HTTP Code\")\n\n            # We update the currently used header.\n            self.currently_used_header = to_print\n\n            if not do_not_print:\n                # We are not authorized to print anything.\n\n                # We generate the before header.\n                self._before_header()\n\n                for formatted_template in self._header_constructor(to_print):\n                    # We loop through the formatted template.\n\n                    if not self.only_on_file:\n                        # We do not have to print only on file.\n\n                        # We print on screen the formatted header template.\n                        print(formatted_template)\n\n                    if not PyFunceble.CONFIGURATION[\"no_files\"] and self.output:\n                        # An output destination is given.\n\n                        # We write the file with the formatted header template.\n                        File(self.output).write(formatted_template + \"\\n\")", "language": "python", "code": "def header(\n        self, do_not_print=False\n    ):  # pragma: no cover pylint: disable=too-many-branches\n        \"\"\"\n        Management and creation of templates of header.\n        Please consider as \"header\" the title of each columns.\n\n        :param do_not_print:\n            Tell us if we have to print the header or not.\n        :type do_not_print: bool\n        \"\"\"\n\n        if (\n            not PyFunceble.CONFIGURATION[\"header_printed\"]\n            or self.template == \"Percentage\"\n            or do_not_print\n        ):\n            # * The header has not been already printed.\n            # or\n            # * The template is the `Percentage template`.\n            # or\n            # * We are authorized to print something.\n\n            if (\n                self.template.lower() in PyFunceble.STATUS[\"list\"][\"generic\"]\n                or self.template == \"Generic_File\"\n            ):\n                # * The template is into the list of generic status.\n                # or\n                # * The template is equal to `Generic_File`.\n\n                # The data to print is the Generic header.\n                to_print = self.headers[\"Generic\"]\n\n                if (\n                    self.template.lower() in PyFunceble.STATUS[\"list\"][\"generic\"]\n                    and PyFunceble.HTTP_CODE[\"active\"]\n                ):\n                    # * The template is in the list of generic status.\n                    # and\n                    # * the http status code extraction is activated.\n\n                    # We remove the Analyze Date colomn from the data to print.\n                    to_print = Dict(to_print).remove_key(\"Analyze Date\")\n            elif self.template.lower() in PyFunceble.STATUS[\"list\"][\"up\"]:\n                # The template is in the list of up status.\n\n                # We informations to print is the up header.\n                to_print = self.headers[PyFunceble.STATUS[\"official\"][\"up\"]]\n            elif self.template.lower() in PyFunceble.STATUS[\"list\"][\"valid\"]:\n                # The template is in the list of valid status.\n\n                # We informations to print is the valid header.\n                to_print = self.headers[PyFunceble.STATUS[\"official\"][\"valid\"]]\n            elif self.template.lower() in PyFunceble.STATUS[\"list\"][\"down\"]:\n                # The template is in the list of down status.\n\n                # We informations to print is the down header.\n                to_print = self.headers[PyFunceble.STATUS[\"official\"][\"down\"]]\n            elif self.template.lower() in PyFunceble.STATUS[\"list\"][\"invalid\"]:\n                # The template is in the list of invalid status.\n\n                # We informations to print is the invalid header.\n                to_print = self.headers[PyFunceble.STATUS[\"official\"][\"invalid\"]]\n            elif (\n                self.template == \"Less\"\n                or self.template == \"Percentage\"\n                or self.template == \"HTTP\"\n            ):  # pylint: disable=line-too-long\n                # * The template is equal to `Less`.\n                # or\n                # * The template is equal to `Percentage`.\n                # or\n                # * The template is equal to `HTTP`.\n\n                # We get the header with the help of the template name.\n                to_print = self.headers[self.template]\n\n                if self.template == \"Less\" and not PyFunceble.HTTP_CODE[\"active\"]:\n                    # * The template is equal to `Less`.\n                    # and\n                    # * The http status code extraction is deactivated.\n\n                    # We append the source index to the header.\n                    to_print[\"Source\"] = 10\n\n            if not PyFunceble.HTTP_CODE[\"active\"]:\n                # * The http status code extraction is deactivated.\n\n                # We remove the HTTP Code index from the data to print.\n                to_print = Dict(to_print).remove_key(\"HTTP Code\")\n\n            # We update the currently used header.\n            self.currently_used_header = to_print\n\n            if not do_not_print:\n                # We are not authorized to print anything.\n\n                # We generate the before header.\n                self._before_header()\n\n                for formatted_template in self._header_constructor(to_print):\n                    # We loop through the formatted template.\n\n                    if not self.only_on_file:\n                        # We do not have to print only on file.\n\n                        # We print on screen the formatted header template.\n                        print(formatted_template)\n\n                    if not PyFunceble.CONFIGURATION[\"no_files\"] and self.output:\n                        # An output destination is given.\n\n                        # We write the file with the formatted header template.\n                        File(self.output).write(formatted_template + \"\\n\")", "code_tokens": ["def", "header", "(", "self", ",", "do_not_print", "=", "False", ")", ":", "# pragma: no cover pylint: disable=too-many-branches", "if", "(", "not", "PyFunceble", ".", "CONFIGURATION", "[", "\"header_printed\"", "]", "or", "self", ".", "template", "==", "\"Percentage\"", "or", "do_not_print", ")", ":", "# * The header has not been already printed.", "# or", "# * The template is the `Percentage template`.", "# or", "# * We are authorized to print something.", "if", "(", "self", ".", "template", ".", "lower", "(", ")", "in", "PyFunceble", ".", "STATUS", "[", "\"list\"", "]", "[", "\"generic\"", "]", "or", "self", ".", "template", "==", "\"Generic_File\"", ")", ":", "# * The template is into the list of generic status.", "# or", "# * The template is equal to `Generic_File`.", "# The data to print is the Generic header.", "to_print", "=", "self", ".", "headers", "[", "\"Generic\"", "]", "if", "(", "self", ".", "template", ".", "lower", "(", ")", "in", "PyFunceble", ".", "STATUS", "[", "\"list\"", "]", "[", "\"generic\"", "]", "and", "PyFunceble", ".", "HTTP_CODE", "[", "\"active\"", "]", ")", ":", "# * The template is in the list of generic status.", "# and", "# * the http status code extraction is activated.", "# We remove the Analyze Date colomn from the data to print.", "to_print", "=", "Dict", "(", "to_print", ")", ".", "remove_key", "(", "\"Analyze Date\"", ")", "elif", "self", ".", "template", ".", "lower", "(", ")", "in", "PyFunceble", ".", "STATUS", "[", "\"list\"", "]", "[", "\"up\"", "]", ":", "# The template is in the list of up status.", "# We informations to print is the up header.", "to_print", "=", "self", ".", "headers", "[", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"up\"", "]", "]", "elif", "self", ".", "template", ".", "lower", "(", ")", "in", "PyFunceble", ".", "STATUS", "[", "\"list\"", "]", "[", "\"valid\"", "]", ":", "# The template is in the list of valid status.", "# We informations to print is the valid header.", "to_print", "=", "self", ".", "headers", "[", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"valid\"", "]", "]", "elif", "self", ".", "template", ".", "lower", "(", ")", "in", "PyFunceble", ".", "STATUS", "[", "\"list\"", "]", "[", "\"down\"", "]", ":", "# The template is in the list of down status.", "# We informations to print is the down header.", "to_print", "=", "self", ".", "headers", "[", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"down\"", "]", "]", "elif", "self", ".", "template", ".", "lower", "(", ")", "in", "PyFunceble", ".", "STATUS", "[", "\"list\"", "]", "[", "\"invalid\"", "]", ":", "# The template is in the list of invalid status.", "# We informations to print is the invalid header.", "to_print", "=", "self", ".", "headers", "[", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"invalid\"", "]", "]", "elif", "(", "self", ".", "template", "==", "\"Less\"", "or", "self", ".", "template", "==", "\"Percentage\"", "or", "self", ".", "template", "==", "\"HTTP\"", ")", ":", "# pylint: disable=line-too-long", "# * The template is equal to `Less`.", "# or", "# * The template is equal to `Percentage`.", "# or", "# * The template is equal to `HTTP`.", "# We get the header with the help of the template name.", "to_print", "=", "self", ".", "headers", "[", "self", ".", "template", "]", "if", "self", ".", "template", "==", "\"Less\"", "and", "not", "PyFunceble", ".", "HTTP_CODE", "[", "\"active\"", "]", ":", "# * The template is equal to `Less`.", "# and", "# * The http status code extraction is deactivated.", "# We append the source index to the header.", "to_print", "[", "\"Source\"", "]", "=", "10", "if", "not", "PyFunceble", ".", "HTTP_CODE", "[", "\"active\"", "]", ":", "# * The http status code extraction is deactivated.", "# We remove the HTTP Code index from the data to print.", "to_print", "=", "Dict", "(", "to_print", ")", ".", "remove_key", "(", "\"HTTP Code\"", ")", "# We update the currently used header.", "self", ".", "currently_used_header", "=", "to_print", "if", "not", "do_not_print", ":", "# We are not authorized to print anything.", "# We generate the before header.", "self", ".", "_before_header", "(", ")", "for", "formatted_template", "in", "self", ".", "_header_constructor", "(", "to_print", ")", ":", "# We loop through the formatted template.", "if", "not", "self", ".", "only_on_file", ":", "# We do not have to print only on file.", "# We print on screen the formatted header template.", "print", "(", "formatted_template", ")", "if", "not", "PyFunceble", ".", "CONFIGURATION", "[", "\"no_files\"", "]", "and", "self", ".", "output", ":", "# An output destination is given.", "# We write the file with the formatted header template.", "File", "(", "self", ".", "output", ")", ".", "write", "(", "formatted_template", "+", "\"\\n\"", ")"], "docstring": "Management and creation of templates of header.\n        Please consider as \"header\" the title of each columns.\n\n        :param do_not_print:\n            Tell us if we have to print the header or not.\n        :type do_not_print: bool", "docstring_tokens": ["Management", "and", "creation", "of", "templates", "of", "header", ".", "Please", "consider", "as", "header", "the", "title", "of", "each", "columns", "."], "sha": "cdf69cbde120199171f7158e1c33635753e6e2f5", "url": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/prints.py#L327-L441", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/parallel/apps/launcher.py", "func_name": "BaseLauncher.on_stop", "original_string": "def on_stop(self, f):\n        \"\"\"Register a callback to be called with this Launcher's stop_data\n        when the process actually finishes.\n        \"\"\"\n        if self.state=='after':\n            return f(self.stop_data)\n        else:\n            self.stop_callbacks.append(f)", "language": "python", "code": "def on_stop(self, f):\n        \"\"\"Register a callback to be called with this Launcher's stop_data\n        when the process actually finishes.\n        \"\"\"\n        if self.state=='after':\n            return f(self.stop_data)\n        else:\n            self.stop_callbacks.append(f)", "code_tokens": ["def", "on_stop", "(", "self", ",", "f", ")", ":", "if", "self", ".", "state", "==", "'after'", ":", "return", "f", "(", "self", ".", "stop_data", ")", "else", ":", "self", ".", "stop_callbacks", ".", "append", "(", "f", ")"], "docstring": "Register a callback to be called with this Launcher's stop_data\n        when the process actually finishes.", "docstring_tokens": ["Register", "a", "callback", "to", "be", "called", "with", "this", "Launcher", "s", "stop_data", "when", "the", "process", "actually", "finishes", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/parallel/apps/launcher.py#L169-L176", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/hooks/webhdfs_hook.py", "func_name": "WebHDFSHook.check_for_path", "original_string": "def check_for_path(self, hdfs_path):\n        \"\"\"\n        Check for the existence of a path in HDFS by querying FileStatus.\n\n        :param hdfs_path: The path to check.\n        :type hdfs_path: str\n        :return: True if the path exists and False if not.\n        :rtype: bool\n        \"\"\"\n        conn = self.get_conn()\n\n        status = conn.status(hdfs_path, strict=False)\n        return bool(status)", "language": "python", "code": "def check_for_path(self, hdfs_path):\n        \"\"\"\n        Check for the existence of a path in HDFS by querying FileStatus.\n\n        :param hdfs_path: The path to check.\n        :type hdfs_path: str\n        :return: True if the path exists and False if not.\n        :rtype: bool\n        \"\"\"\n        conn = self.get_conn()\n\n        status = conn.status(hdfs_path, strict=False)\n        return bool(status)", "code_tokens": ["def", "check_for_path", "(", "self", ",", "hdfs_path", ")", ":", "conn", "=", "self", ".", "get_conn", "(", ")", "status", "=", "conn", ".", "status", "(", "hdfs_path", ",", "strict", "=", "False", ")", "return", "bool", "(", "status", ")"], "docstring": "Check for the existence of a path in HDFS by querying FileStatus.\n\n        :param hdfs_path: The path to check.\n        :type hdfs_path: str\n        :return: True if the path exists and False if not.\n        :rtype: bool", "docstring_tokens": ["Check", "for", "the", "existence", "of", "a", "path", "in", "HDFS", "by", "querying", "FileStatus", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/webhdfs_hook.py#L92-L104", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/adapter/mongo/variant_events.py", "func_name": "VariantEventHandler.update_dismiss_variant", "original_string": "def update_dismiss_variant(self, institute, case, user, link, variant,\n                               dismiss_variant):\n        \"\"\"Create an event for updating the manual dismiss variant entry\n\n          This function will create a event and update the dismiss variant\n          field of the variant.\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            variant (dict): A variant object\n            dismiss_variant (list): The new dismiss variant list\n\n        Return:\n            updated_variant\n\n        \"\"\"\n        LOG.info(\"Creating event for updating dismiss variant for \"\n                    \"variant {0}\".format(variant['display_name']))\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='variant',\n            verb='dismiss_variant',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n\n        if dismiss_variant:\n            LOG.info(\"Setting dismiss variant to {0} for variant {1}\"\n                        .format(dismiss_variant, variant['display_name']))\n            action = '$set'\n        else:\n            LOG.info(\"Reset dismiss variant from {0} for variant {1}\"\n                        .format(variant['dismiss_variant'], variant['display_name']))\n            action = '$unset'\n\n        updated_variant = self.variant_collection.find_one_and_update(\n            {'_id': variant['_id']},\n            {action: {'dismiss_variant': dismiss_variant}},\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n        LOG.debug(\"Variant updated\")\n        return updated_variant", "language": "python", "code": "def update_dismiss_variant(self, institute, case, user, link, variant,\n                               dismiss_variant):\n        \"\"\"Create an event for updating the manual dismiss variant entry\n\n          This function will create a event and update the dismiss variant\n          field of the variant.\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            variant (dict): A variant object\n            dismiss_variant (list): The new dismiss variant list\n\n        Return:\n            updated_variant\n\n        \"\"\"\n        LOG.info(\"Creating event for updating dismiss variant for \"\n                    \"variant {0}\".format(variant['display_name']))\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='variant',\n            verb='dismiss_variant',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n\n        if dismiss_variant:\n            LOG.info(\"Setting dismiss variant to {0} for variant {1}\"\n                        .format(dismiss_variant, variant['display_name']))\n            action = '$set'\n        else:\n            LOG.info(\"Reset dismiss variant from {0} for variant {1}\"\n                        .format(variant['dismiss_variant'], variant['display_name']))\n            action = '$unset'\n\n        updated_variant = self.variant_collection.find_one_and_update(\n            {'_id': variant['_id']},\n            {action: {'dismiss_variant': dismiss_variant}},\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n        LOG.debug(\"Variant updated\")\n        return updated_variant", "code_tokens": ["def", "update_dismiss_variant", "(", "self", ",", "institute", ",", "case", ",", "user", ",", "link", ",", "variant", ",", "dismiss_variant", ")", ":", "LOG", ".", "info", "(", "\"Creating event for updating dismiss variant for \"", "\"variant {0}\"", ".", "format", "(", "variant", "[", "'display_name'", "]", ")", ")", "self", ".", "create_event", "(", "institute", "=", "institute", ",", "case", "=", "case", ",", "user", "=", "user", ",", "link", "=", "link", ",", "category", "=", "'variant'", ",", "verb", "=", "'dismiss_variant'", ",", "variant", "=", "variant", ",", "subject", "=", "variant", "[", "'display_name'", "]", ",", ")", "if", "dismiss_variant", ":", "LOG", ".", "info", "(", "\"Setting dismiss variant to {0} for variant {1}\"", ".", "format", "(", "dismiss_variant", ",", "variant", "[", "'display_name'", "]", ")", ")", "action", "=", "'$set'", "else", ":", "LOG", ".", "info", "(", "\"Reset dismiss variant from {0} for variant {1}\"", ".", "format", "(", "variant", "[", "'dismiss_variant'", "]", ",", "variant", "[", "'display_name'", "]", ")", ")", "action", "=", "'$unset'", "updated_variant", "=", "self", ".", "variant_collection", ".", "find_one_and_update", "(", "{", "'_id'", ":", "variant", "[", "'_id'", "]", "}", ",", "{", "action", ":", "{", "'dismiss_variant'", ":", "dismiss_variant", "}", "}", ",", "return_document", "=", "pymongo", ".", "ReturnDocument", ".", "AFTER", ")", "LOG", ".", "debug", "(", "\"Variant updated\"", ")", "return", "updated_variant"], "docstring": "Create an event for updating the manual dismiss variant entry\n\n          This function will create a event and update the dismiss variant\n          field of the variant.\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            variant (dict): A variant object\n            dismiss_variant (list): The new dismiss variant list\n\n        Return:\n            updated_variant", "docstring_tokens": ["Create", "an", "event", "for", "updating", "the", "manual", "dismiss", "variant", "entry"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/adapter/mongo/variant_events.py#L434-L482", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/adapter/mongo/case.py", "func_name": "CaseHandler.update_caseid", "original_string": "def update_caseid(self, case_obj, family_id):\n        \"\"\"Update case id for a case across the database.\n\n        This function is used when a case is a rerun or updated for another reason.\n\n        Args:\n            case_obj(dict)\n            family_id(str): The new family id\n\n        Returns:\n            new_case(dict): The updated case object\n\n        \"\"\"\n        new_case = deepcopy(case_obj)\n        new_case['_id'] = family_id\n\n        # update suspects and causatives\n        for case_variants in ['suspects', 'causatives']:\n            new_variantids = []\n            for variant_id in case_obj.get(case_variants, []):\n                case_variant = self.variant(variant_id)\n                if not case_variant:\n                    continue\n                new_variantid = get_variantid(case_variant, family_id)\n                new_variantids.append(new_variantid)\n            new_case[case_variants] = new_variantids\n\n        # update ACMG\n        for acmg_obj in self.acmg_collection.find({'case_id': case_obj['_id']}):\n            LOG.info(\"update ACMG classification: %s\", acmg_obj['classification'])\n            acmg_variant = self.variant(acmg_obj['variant_specific'])\n            new_specific_id = get_variantid(acmg_variant, family_id)\n            self.acmg_collection.find_one_and_update(\n                {'_id': acmg_obj['_id']},\n                {'$set': {'case_id': family_id, 'variant_specific': new_specific_id}},\n            )\n\n        # update events\n        institute_obj = self.institute(case_obj['owner'])\n        for event_obj in self.events(institute_obj, case=case_obj):\n            LOG.info(\"update event: %s\", event_obj['verb'])\n            self.event_collection.find_one_and_update(\n                {'_id': event_obj['_id']},\n                {'$set': {'case': family_id}},\n            )\n\n        # insert the updated case\n        self.case_collection.insert_one(new_case)\n        # delete the old case\n        self.case_collection.find_one_and_delete({'_id': case_obj['_id']})\n        return new_case", "language": "python", "code": "def update_caseid(self, case_obj, family_id):\n        \"\"\"Update case id for a case across the database.\n\n        This function is used when a case is a rerun or updated for another reason.\n\n        Args:\n            case_obj(dict)\n            family_id(str): The new family id\n\n        Returns:\n            new_case(dict): The updated case object\n\n        \"\"\"\n        new_case = deepcopy(case_obj)\n        new_case['_id'] = family_id\n\n        # update suspects and causatives\n        for case_variants in ['suspects', 'causatives']:\n            new_variantids = []\n            for variant_id in case_obj.get(case_variants, []):\n                case_variant = self.variant(variant_id)\n                if not case_variant:\n                    continue\n                new_variantid = get_variantid(case_variant, family_id)\n                new_variantids.append(new_variantid)\n            new_case[case_variants] = new_variantids\n\n        # update ACMG\n        for acmg_obj in self.acmg_collection.find({'case_id': case_obj['_id']}):\n            LOG.info(\"update ACMG classification: %s\", acmg_obj['classification'])\n            acmg_variant = self.variant(acmg_obj['variant_specific'])\n            new_specific_id = get_variantid(acmg_variant, family_id)\n            self.acmg_collection.find_one_and_update(\n                {'_id': acmg_obj['_id']},\n                {'$set': {'case_id': family_id, 'variant_specific': new_specific_id}},\n            )\n\n        # update events\n        institute_obj = self.institute(case_obj['owner'])\n        for event_obj in self.events(institute_obj, case=case_obj):\n            LOG.info(\"update event: %s\", event_obj['verb'])\n            self.event_collection.find_one_and_update(\n                {'_id': event_obj['_id']},\n                {'$set': {'case': family_id}},\n            )\n\n        # insert the updated case\n        self.case_collection.insert_one(new_case)\n        # delete the old case\n        self.case_collection.find_one_and_delete({'_id': case_obj['_id']})\n        return new_case", "code_tokens": ["def", "update_caseid", "(", "self", ",", "case_obj", ",", "family_id", ")", ":", "new_case", "=", "deepcopy", "(", "case_obj", ")", "new_case", "[", "'_id'", "]", "=", "family_id", "# update suspects and causatives", "for", "case_variants", "in", "[", "'suspects'", ",", "'causatives'", "]", ":", "new_variantids", "=", "[", "]", "for", "variant_id", "in", "case_obj", ".", "get", "(", "case_variants", ",", "[", "]", ")", ":", "case_variant", "=", "self", ".", "variant", "(", "variant_id", ")", "if", "not", "case_variant", ":", "continue", "new_variantid", "=", "get_variantid", "(", "case_variant", ",", "family_id", ")", "new_variantids", ".", "append", "(", "new_variantid", ")", "new_case", "[", "case_variants", "]", "=", "new_variantids", "# update ACMG", "for", "acmg_obj", "in", "self", ".", "acmg_collection", ".", "find", "(", "{", "'case_id'", ":", "case_obj", "[", "'_id'", "]", "}", ")", ":", "LOG", ".", "info", "(", "\"update ACMG classification: %s\"", ",", "acmg_obj", "[", "'classification'", "]", ")", "acmg_variant", "=", "self", ".", "variant", "(", "acmg_obj", "[", "'variant_specific'", "]", ")", "new_specific_id", "=", "get_variantid", "(", "acmg_variant", ",", "family_id", ")", "self", ".", "acmg_collection", ".", "find_one_and_update", "(", "{", "'_id'", ":", "acmg_obj", "[", "'_id'", "]", "}", ",", "{", "'$set'", ":", "{", "'case_id'", ":", "family_id", ",", "'variant_specific'", ":", "new_specific_id", "}", "}", ",", ")", "# update events", "institute_obj", "=", "self", ".", "institute", "(", "case_obj", "[", "'owner'", "]", ")", "for", "event_obj", "in", "self", ".", "events", "(", "institute_obj", ",", "case", "=", "case_obj", ")", ":", "LOG", ".", "info", "(", "\"update event: %s\"", ",", "event_obj", "[", "'verb'", "]", ")", "self", ".", "event_collection", ".", "find_one_and_update", "(", "{", "'_id'", ":", "event_obj", "[", "'_id'", "]", "}", ",", "{", "'$set'", ":", "{", "'case'", ":", "family_id", "}", "}", ",", ")", "# insert the updated case", "self", ".", "case_collection", ".", "insert_one", "(", "new_case", ")", "# delete the old case", "self", ".", "case_collection", ".", "find_one_and_delete", "(", "{", "'_id'", ":", "case_obj", "[", "'_id'", "]", "}", ")", "return", "new_case"], "docstring": "Update case id for a case across the database.\n\n        This function is used when a case is a rerun or updated for another reason.\n\n        Args:\n            case_obj(dict)\n            family_id(str): The new family id\n\n        Returns:\n            new_case(dict): The updated case object", "docstring_tokens": ["Update", "case", "id", "for", "a", "case", "across", "the", "database", "."], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/adapter/mongo/case.py#L459-L509", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/bijectors/real_nvp.py", "func_name": "real_nvp_default_template", "original_string": "def real_nvp_default_template(hidden_layers,\n                              shift_only=False,\n                              activation=tf.nn.relu,\n                              name=None,\n                              *args,  # pylint: disable=keyword-arg-before-vararg\n                              **kwargs):\n  \"\"\"Build a scale-and-shift function using a multi-layer neural network.\n\n  This will be wrapped in a make_template to ensure the variables are only\n  created once. It takes the `d`-dimensional input x[0:d] and returns the `D-d`\n  dimensional outputs `loc` (\"mu\") and `log_scale` (\"alpha\").\n\n  The default template does not support conditioning and will raise an\n  exception if `condition_kwargs` are passed to it. To use conditioning in\n  real nvp bijector, implement a conditioned shift/scale template that\n  handles the `condition_kwargs`.\n\n  Arguments:\n    hidden_layers: Python `list`-like of non-negative integer, scalars\n      indicating the number of units in each hidden layer. Default: `[512, 512].\n    shift_only: Python `bool` indicating if only the `shift` term shall be\n      computed (i.e. NICE bijector). Default: `False`.\n    activation: Activation function (callable). Explicitly setting to `None`\n      implies a linear activation.\n    name: A name for ops managed by this function. Default:\n      \"real_nvp_default_template\".\n    *args: `tf.layers.dense` arguments.\n    **kwargs: `tf.layers.dense` keyword arguments.\n\n  Returns:\n    shift: `Float`-like `Tensor` of shift terms (\"mu\" in\n      [Papamakarios et al.  (2016)][1]).\n    log_scale: `Float`-like `Tensor` of log(scale) terms (\"alpha\" in\n      [Papamakarios et al. (2016)][1]).\n\n  Raises:\n    NotImplementedError: if rightmost dimension of `inputs` is unknown prior to\n      graph execution, or if `condition_kwargs` is not empty.\n\n  #### References\n\n  [1]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked\n       Autoregressive Flow for Density Estimation. In _Neural Information\n       Processing Systems_, 2017. https://arxiv.org/abs/1705.07057\n  \"\"\"\n\n  with tf.compat.v2.name_scope(name or \"real_nvp_default_template\"):\n\n    def _fn(x, output_units, **condition_kwargs):\n      \"\"\"Fully connected MLP parameterized via `real_nvp_template`.\"\"\"\n      if condition_kwargs:\n        raise NotImplementedError(\n            \"Conditioning not implemented in the default template.\")\n\n      if tensorshape_util.rank(x.shape) == 1:\n        x = x[tf.newaxis, ...]\n        reshape_output = lambda x: x[0]\n      else:\n        reshape_output = lambda x: x\n      for units in hidden_layers:\n        x = tf.compat.v1.layers.dense(\n            inputs=x,\n            units=units,\n            activation=activation,\n            *args,  # pylint: disable=keyword-arg-before-vararg\n            **kwargs)\n      x = tf.compat.v1.layers.dense(\n          inputs=x,\n          units=(1 if shift_only else 2) * output_units,\n          activation=None,\n          *args,  # pylint: disable=keyword-arg-before-vararg\n          **kwargs)\n      if shift_only:\n        return reshape_output(x), None\n      shift, log_scale = tf.split(x, 2, axis=-1)\n      return reshape_output(shift), reshape_output(log_scale)\n\n    return tf.compat.v1.make_template(\"real_nvp_default_template\", _fn)", "language": "python", "code": "def real_nvp_default_template(hidden_layers,\n                              shift_only=False,\n                              activation=tf.nn.relu,\n                              name=None,\n                              *args,  # pylint: disable=keyword-arg-before-vararg\n                              **kwargs):\n  \"\"\"Build a scale-and-shift function using a multi-layer neural network.\n\n  This will be wrapped in a make_template to ensure the variables are only\n  created once. It takes the `d`-dimensional input x[0:d] and returns the `D-d`\n  dimensional outputs `loc` (\"mu\") and `log_scale` (\"alpha\").\n\n  The default template does not support conditioning and will raise an\n  exception if `condition_kwargs` are passed to it. To use conditioning in\n  real nvp bijector, implement a conditioned shift/scale template that\n  handles the `condition_kwargs`.\n\n  Arguments:\n    hidden_layers: Python `list`-like of non-negative integer, scalars\n      indicating the number of units in each hidden layer. Default: `[512, 512].\n    shift_only: Python `bool` indicating if only the `shift` term shall be\n      computed (i.e. NICE bijector). Default: `False`.\n    activation: Activation function (callable). Explicitly setting to `None`\n      implies a linear activation.\n    name: A name for ops managed by this function. Default:\n      \"real_nvp_default_template\".\n    *args: `tf.layers.dense` arguments.\n    **kwargs: `tf.layers.dense` keyword arguments.\n\n  Returns:\n    shift: `Float`-like `Tensor` of shift terms (\"mu\" in\n      [Papamakarios et al.  (2016)][1]).\n    log_scale: `Float`-like `Tensor` of log(scale) terms (\"alpha\" in\n      [Papamakarios et al. (2016)][1]).\n\n  Raises:\n    NotImplementedError: if rightmost dimension of `inputs` is unknown prior to\n      graph execution, or if `condition_kwargs` is not empty.\n\n  #### References\n\n  [1]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked\n       Autoregressive Flow for Density Estimation. In _Neural Information\n       Processing Systems_, 2017. https://arxiv.org/abs/1705.07057\n  \"\"\"\n\n  with tf.compat.v2.name_scope(name or \"real_nvp_default_template\"):\n\n    def _fn(x, output_units, **condition_kwargs):\n      \"\"\"Fully connected MLP parameterized via `real_nvp_template`.\"\"\"\n      if condition_kwargs:\n        raise NotImplementedError(\n            \"Conditioning not implemented in the default template.\")\n\n      if tensorshape_util.rank(x.shape) == 1:\n        x = x[tf.newaxis, ...]\n        reshape_output = lambda x: x[0]\n      else:\n        reshape_output = lambda x: x\n      for units in hidden_layers:\n        x = tf.compat.v1.layers.dense(\n            inputs=x,\n            units=units,\n            activation=activation,\n            *args,  # pylint: disable=keyword-arg-before-vararg\n            **kwargs)\n      x = tf.compat.v1.layers.dense(\n          inputs=x,\n          units=(1 if shift_only else 2) * output_units,\n          activation=None,\n          *args,  # pylint: disable=keyword-arg-before-vararg\n          **kwargs)\n      if shift_only:\n        return reshape_output(x), None\n      shift, log_scale = tf.split(x, 2, axis=-1)\n      return reshape_output(shift), reshape_output(log_scale)\n\n    return tf.compat.v1.make_template(\"real_nvp_default_template\", _fn)", "code_tokens": ["def", "real_nvp_default_template", "(", "hidden_layers", ",", "shift_only", "=", "False", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "name", "=", "None", ",", "*", "args", ",", "# pylint: disable=keyword-arg-before-vararg", "*", "*", "kwargs", ")", ":", "with", "tf", ".", "compat", ".", "v2", ".", "name_scope", "(", "name", "or", "\"real_nvp_default_template\"", ")", ":", "def", "_fn", "(", "x", ",", "output_units", ",", "*", "*", "condition_kwargs", ")", ":", "\"\"\"Fully connected MLP parameterized via `real_nvp_template`.\"\"\"", "if", "condition_kwargs", ":", "raise", "NotImplementedError", "(", "\"Conditioning not implemented in the default template.\"", ")", "if", "tensorshape_util", ".", "rank", "(", "x", ".", "shape", ")", "==", "1", ":", "x", "=", "x", "[", "tf", ".", "newaxis", ",", "...", "]", "reshape_output", "=", "lambda", "x", ":", "x", "[", "0", "]", "else", ":", "reshape_output", "=", "lambda", "x", ":", "x", "for", "units", "in", "hidden_layers", ":", "x", "=", "tf", ".", "compat", ".", "v1", ".", "layers", ".", "dense", "(", "inputs", "=", "x", ",", "units", "=", "units", ",", "activation", "=", "activation", ",", "*", "args", ",", "# pylint: disable=keyword-arg-before-vararg", "*", "*", "kwargs", ")", "x", "=", "tf", ".", "compat", ".", "v1", ".", "layers", ".", "dense", "(", "inputs", "=", "x", ",", "units", "=", "(", "1", "if", "shift_only", "else", "2", ")", "*", "output_units", ",", "activation", "=", "None", ",", "*", "args", ",", "# pylint: disable=keyword-arg-before-vararg", "*", "*", "kwargs", ")", "if", "shift_only", ":", "return", "reshape_output", "(", "x", ")", ",", "None", "shift", ",", "log_scale", "=", "tf", ".", "split", "(", "x", ",", "2", ",", "axis", "=", "-", "1", ")", "return", "reshape_output", "(", "shift", ")", ",", "reshape_output", "(", "log_scale", ")", "return", "tf", ".", "compat", ".", "v1", ".", "make_template", "(", "\"real_nvp_default_template\"", ",", "_fn", ")"], "docstring": "Build a scale-and-shift function using a multi-layer neural network.\n\n  This will be wrapped in a make_template to ensure the variables are only\n  created once. It takes the `d`-dimensional input x[0:d] and returns the `D-d`\n  dimensional outputs `loc` (\"mu\") and `log_scale` (\"alpha\").\n\n  The default template does not support conditioning and will raise an\n  exception if `condition_kwargs` are passed to it. To use conditioning in\n  real nvp bijector, implement a conditioned shift/scale template that\n  handles the `condition_kwargs`.\n\n  Arguments:\n    hidden_layers: Python `list`-like of non-negative integer, scalars\n      indicating the number of units in each hidden layer. Default: `[512, 512].\n    shift_only: Python `bool` indicating if only the `shift` term shall be\n      computed (i.e. NICE bijector). Default: `False`.\n    activation: Activation function (callable). Explicitly setting to `None`\n      implies a linear activation.\n    name: A name for ops managed by this function. Default:\n      \"real_nvp_default_template\".\n    *args: `tf.layers.dense` arguments.\n    **kwargs: `tf.layers.dense` keyword arguments.\n\n  Returns:\n    shift: `Float`-like `Tensor` of shift terms (\"mu\" in\n      [Papamakarios et al.  (2016)][1]).\n    log_scale: `Float`-like `Tensor` of log(scale) terms (\"alpha\" in\n      [Papamakarios et al. (2016)][1]).\n\n  Raises:\n    NotImplementedError: if rightmost dimension of `inputs` is unknown prior to\n      graph execution, or if `condition_kwargs` is not empty.\n\n  #### References\n\n  [1]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked\n       Autoregressive Flow for Density Estimation. In _Neural Information\n       Processing Systems_, 2017. https://arxiv.org/abs/1705.07057", "docstring_tokens": ["Build", "a", "scale", "-", "and", "-", "shift", "function", "using", "a", "multi", "-", "layer", "neural", "network", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/bijectors/real_nvp.py#L228-L305", "partition": "test"}
{"repo": "versae/ipython-cypher", "path": "src/cypher/run.py", "func_name": "ResultSet.csv", "original_string": "def csv(self, filename=None, **format_params):\n        \"\"\"Generates results in comma-separated form.  Write to ``filename``\n        if given. Any other parameter will be passed on to ``csv.writer``.\n\n        :param filename: if given, the CSV will be written to filename.\n\n        Any additional keyword arguments will be passsed\n        through to ``csv.writer``.\n        \"\"\"\n        if not self.pretty:\n            return None  # no results\n        if filename:\n            outfile = open(filename, 'w')\n        else:\n            outfile = StringIO()\n        writer = UnicodeWriter(outfile, **format_params)\n        writer.writerow(self.field_names)\n        for row in self:\n            writer.writerow(row)\n        if filename:\n            outfile.close()\n            return CsvResultDescriptor(filename)\n        else:\n            return outfile.getvalue()", "language": "python", "code": "def csv(self, filename=None, **format_params):\n        \"\"\"Generates results in comma-separated form.  Write to ``filename``\n        if given. Any other parameter will be passed on to ``csv.writer``.\n\n        :param filename: if given, the CSV will be written to filename.\n\n        Any additional keyword arguments will be passsed\n        through to ``csv.writer``.\n        \"\"\"\n        if not self.pretty:\n            return None  # no results\n        if filename:\n            outfile = open(filename, 'w')\n        else:\n            outfile = StringIO()\n        writer = UnicodeWriter(outfile, **format_params)\n        writer.writerow(self.field_names)\n        for row in self:\n            writer.writerow(row)\n        if filename:\n            outfile.close()\n            return CsvResultDescriptor(filename)\n        else:\n            return outfile.getvalue()", "code_tokens": ["def", "csv", "(", "self", ",", "filename", "=", "None", ",", "*", "*", "format_params", ")", ":", "if", "not", "self", ".", "pretty", ":", "return", "None", "# no results", "if", "filename", ":", "outfile", "=", "open", "(", "filename", ",", "'w'", ")", "else", ":", "outfile", "=", "StringIO", "(", ")", "writer", "=", "UnicodeWriter", "(", "outfile", ",", "*", "*", "format_params", ")", "writer", ".", "writerow", "(", "self", ".", "field_names", ")", "for", "row", "in", "self", ":", "writer", ".", "writerow", "(", "row", ")", "if", "filename", ":", "outfile", ".", "close", "(", ")", "return", "CsvResultDescriptor", "(", "filename", ")", "else", ":", "return", "outfile", ".", "getvalue", "(", ")"], "docstring": "Generates results in comma-separated form.  Write to ``filename``\n        if given. Any other parameter will be passed on to ``csv.writer``.\n\n        :param filename: if given, the CSV will be written to filename.\n\n        Any additional keyword arguments will be passsed\n        through to ``csv.writer``.", "docstring_tokens": ["Generates", "results", "in", "comma", "-", "separated", "form", ".", "Write", "to", "filename", "if", "given", ".", "Any", "other", "parameter", "will", "be", "passed", "on", "to", "csv", ".", "writer", "."], "sha": "1e88bd8227743e70b78af42e0e713ae8803485e1", "url": "https://github.com/versae/ipython-cypher/blob/1e88bd8227743e70b78af42e0e713ae8803485e1/src/cypher/run.py#L424-L447", "partition": "test"}
{"repo": "instagrambot/instabot", "path": "instabot/bot/bot_comment.py", "func_name": "comment_user", "original_string": "def comment_user(self, user_id, amount=None):\n    \"\"\" Comments last user_id's medias \"\"\"\n    if not self.check_user(user_id, filter_closed_acc=True):\n        return False\n    self.logger.info(\"Going to comment user_%s's feed:\" % user_id)\n    user_id = self.convert_to_user_id(user_id)\n    medias = self.get_user_medias(user_id, is_comment=True)\n    if not medias:\n        self.logger.info(\n            \"None medias received: account is closed or medias have been filtered.\")\n        return False\n    return self.comment_medias(medias[:amount])", "language": "python", "code": "def comment_user(self, user_id, amount=None):\n    \"\"\" Comments last user_id's medias \"\"\"\n    if not self.check_user(user_id, filter_closed_acc=True):\n        return False\n    self.logger.info(\"Going to comment user_%s's feed:\" % user_id)\n    user_id = self.convert_to_user_id(user_id)\n    medias = self.get_user_medias(user_id, is_comment=True)\n    if not medias:\n        self.logger.info(\n            \"None medias received: account is closed or medias have been filtered.\")\n        return False\n    return self.comment_medias(medias[:amount])", "code_tokens": ["def", "comment_user", "(", "self", ",", "user_id", ",", "amount", "=", "None", ")", ":", "if", "not", "self", ".", "check_user", "(", "user_id", ",", "filter_closed_acc", "=", "True", ")", ":", "return", "False", "self", ".", "logger", ".", "info", "(", "\"Going to comment user_%s's feed:\"", "%", "user_id", ")", "user_id", "=", "self", ".", "convert_to_user_id", "(", "user_id", ")", "medias", "=", "self", ".", "get_user_medias", "(", "user_id", ",", "is_comment", "=", "True", ")", "if", "not", "medias", ":", "self", ".", "logger", ".", "info", "(", "\"None medias received: account is closed or medias have been filtered.\"", ")", "return", "False", "return", "self", ".", "comment_medias", "(", "medias", "[", ":", "amount", "]", ")"], "docstring": "Comments last user_id's medias", "docstring_tokens": ["Comments", "last", "user_id", "s", "medias"], "sha": "d734f892ac4cc35d22746a4f2680425ffaff0927", "url": "https://github.com/instagrambot/instabot/blob/d734f892ac4cc35d22746a4f2680425ffaff0927/instabot/bot/bot_comment.py#L91-L102", "partition": "test"}
{"repo": "UCBerkeleySETI/blimpy", "path": "blimpy/filterbank.py", "func_name": "Filterbank._setup_freqs", "original_string": "def _setup_freqs(self, f_start=None, f_stop=None):\n        \"\"\" Setup frequency axis \"\"\"\n        ## Setup frequency axis\n        f0 = self.header[b'fch1']\n        f_delt = self.header[b'foff']\n\n        i_start, i_stop = 0, self.header[b'nchans']\n        if f_start:\n            i_start = int((f_start - f0) / f_delt)\n        if f_stop:\n            i_stop  = int((f_stop - f0)  / f_delt)\n\n        #calculate closest true index value\n        chan_start_idx = np.int(i_start)\n        chan_stop_idx  = np.int(i_stop)\n\n        #create freq array\n        if i_start < i_stop:\n            i_vals = np.arange(chan_start_idx, chan_stop_idx)\n        else:\n            i_vals = np.arange(chan_stop_idx, chan_start_idx)\n\n        self.freqs = f_delt * i_vals + f0\n\n#         if f_delt < 0:\n#             self.freqs = self.freqs[::-1]\n\n        if chan_stop_idx < chan_start_idx:\n            chan_stop_idx, chan_start_idx = chan_start_idx,chan_stop_idx\n\n        return i_start, i_stop, chan_start_idx, chan_stop_idx", "language": "python", "code": "def _setup_freqs(self, f_start=None, f_stop=None):\n        \"\"\" Setup frequency axis \"\"\"\n        ## Setup frequency axis\n        f0 = self.header[b'fch1']\n        f_delt = self.header[b'foff']\n\n        i_start, i_stop = 0, self.header[b'nchans']\n        if f_start:\n            i_start = int((f_start - f0) / f_delt)\n        if f_stop:\n            i_stop  = int((f_stop - f0)  / f_delt)\n\n        #calculate closest true index value\n        chan_start_idx = np.int(i_start)\n        chan_stop_idx  = np.int(i_stop)\n\n        #create freq array\n        if i_start < i_stop:\n            i_vals = np.arange(chan_start_idx, chan_stop_idx)\n        else:\n            i_vals = np.arange(chan_stop_idx, chan_start_idx)\n\n        self.freqs = f_delt * i_vals + f0\n\n#         if f_delt < 0:\n#             self.freqs = self.freqs[::-1]\n\n        if chan_stop_idx < chan_start_idx:\n            chan_stop_idx, chan_start_idx = chan_start_idx,chan_stop_idx\n\n        return i_start, i_stop, chan_start_idx, chan_stop_idx", "code_tokens": ["def", "_setup_freqs", "(", "self", ",", "f_start", "=", "None", ",", "f_stop", "=", "None", ")", ":", "## Setup frequency axis", "f0", "=", "self", ".", "header", "[", "b'fch1'", "]", "f_delt", "=", "self", ".", "header", "[", "b'foff'", "]", "i_start", ",", "i_stop", "=", "0", ",", "self", ".", "header", "[", "b'nchans'", "]", "if", "f_start", ":", "i_start", "=", "int", "(", "(", "f_start", "-", "f0", ")", "/", "f_delt", ")", "if", "f_stop", ":", "i_stop", "=", "int", "(", "(", "f_stop", "-", "f0", ")", "/", "f_delt", ")", "#calculate closest true index value", "chan_start_idx", "=", "np", ".", "int", "(", "i_start", ")", "chan_stop_idx", "=", "np", ".", "int", "(", "i_stop", ")", "#create freq array", "if", "i_start", "<", "i_stop", ":", "i_vals", "=", "np", ".", "arange", "(", "chan_start_idx", ",", "chan_stop_idx", ")", "else", ":", "i_vals", "=", "np", ".", "arange", "(", "chan_stop_idx", ",", "chan_start_idx", ")", "self", ".", "freqs", "=", "f_delt", "*", "i_vals", "+", "f0", "#         if f_delt < 0:", "#             self.freqs = self.freqs[::-1]", "if", "chan_stop_idx", "<", "chan_start_idx", ":", "chan_stop_idx", ",", "chan_start_idx", "=", "chan_start_idx", ",", "chan_stop_idx", "return", "i_start", ",", "i_stop", ",", "chan_start_idx", ",", "chan_stop_idx"], "docstring": "Setup frequency axis", "docstring_tokens": ["Setup", "frequency", "axis"], "sha": "b8822d3e3e911944370d84371a91fa0c29e9772e", "url": "https://github.com/UCBerkeleySETI/blimpy/blob/b8822d3e3e911944370d84371a91fa0c29e9772e/blimpy/filterbank.py#L186-L216", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/mainwindow.py", "func_name": "MainWindow.add_menu_action", "original_string": "def add_menu_action(self, menu, action, defer_shortcut=False):\n        \"\"\"Add action to menu as well as self\n        \n        So that when the menu bar is invisible, its actions are still available.\n        \n        If defer_shortcut is True, set the shortcut context to widget-only,\n        where it will avoid conflict with shortcuts already bound to the\n        widgets themselves.\n        \"\"\"\n        menu.addAction(action)\n        self.addAction(action)\n\n        if defer_shortcut:\n            action.setShortcutContext(QtCore.Qt.WidgetShortcut)", "language": "python", "code": "def add_menu_action(self, menu, action, defer_shortcut=False):\n        \"\"\"Add action to menu as well as self\n        \n        So that when the menu bar is invisible, its actions are still available.\n        \n        If defer_shortcut is True, set the shortcut context to widget-only,\n        where it will avoid conflict with shortcuts already bound to the\n        widgets themselves.\n        \"\"\"\n        menu.addAction(action)\n        self.addAction(action)\n\n        if defer_shortcut:\n            action.setShortcutContext(QtCore.Qt.WidgetShortcut)", "code_tokens": ["def", "add_menu_action", "(", "self", ",", "menu", ",", "action", ",", "defer_shortcut", "=", "False", ")", ":", "menu", ".", "addAction", "(", "action", ")", "self", ".", "addAction", "(", "action", ")", "if", "defer_shortcut", ":", "action", ".", "setShortcutContext", "(", "QtCore", ".", "Qt", ".", "WidgetShortcut", ")"], "docstring": "Add action to menu as well as self\n        \n        So that when the menu bar is invisible, its actions are still available.\n        \n        If defer_shortcut is True, set the shortcut context to widget-only,\n        where it will avoid conflict with shortcuts already bound to the\n        widgets themselves.", "docstring_tokens": ["Add", "action", "to", "menu", "as", "well", "as", "self", "So", "that", "when", "the", "menu", "bar", "is", "invisible", "its", "actions", "are", "still", "available", ".", "If", "defer_shortcut", "is", "True", "set", "the", "shortcut", "context", "to", "widget", "-", "only", "where", "it", "will", "avoid", "conflict", "with", "shortcuts", "already", "bound", "to", "the", "widgets", "themselves", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/mainwindow.py#L334-L347", "partition": "test"}
{"repo": "Azure/Azure-MachineLearning-ClientLibrary-Python", "path": "azureml/__init__.py", "func_name": "SourceDataset.contents_url", "original_string": "def contents_url(self):\n        \"\"\"Full URL to the dataset contents.\"\"\"\n        loc = self.download_location\n        return loc.base_uri + loc.location + loc.access_credential", "language": "python", "code": "def contents_url(self):\n        \"\"\"Full URL to the dataset contents.\"\"\"\n        loc = self.download_location\n        return loc.base_uri + loc.location + loc.access_credential", "code_tokens": ["def", "contents_url", "(", "self", ")", ":", "loc", "=", "self", ".", "download_location", "return", "loc", ".", "base_uri", "+", "loc", ".", "location", "+", "loc", ".", "access_credential"], "docstring": "Full URL to the dataset contents.", "docstring_tokens": ["Full", "URL", "to", "the", "dataset", "contents", "."], "sha": "d1211b289747671898eb063013e0dc53d3c80acd", "url": "https://github.com/Azure/Azure-MachineLearning-ClientLibrary-Python/blob/d1211b289747671898eb063013e0dc53d3c80acd/azureml/__init__.py#L411-L414", "partition": "test"}
{"repo": "its-rigs/Trolly", "path": "trolly/card.py", "func_name": "Card.add_checklist", "original_string": "def add_checklist(self, query_params=None):\n        '''\n        Add a checklist to this card. Returns a Checklist object.\n        '''\n        checklist_json = self.fetch_json(\n            uri_path=self.base_uri + '/checklists',\n            http_method='POST',\n            query_params=query_params or {}\n        )\n\n        return self.create_checklist(checklist_json)", "language": "python", "code": "def add_checklist(self, query_params=None):\n        '''\n        Add a checklist to this card. Returns a Checklist object.\n        '''\n        checklist_json = self.fetch_json(\n            uri_path=self.base_uri + '/checklists',\n            http_method='POST',\n            query_params=query_params or {}\n        )\n\n        return self.create_checklist(checklist_json)", "code_tokens": ["def", "add_checklist", "(", "self", ",", "query_params", "=", "None", ")", ":", "checklist_json", "=", "self", ".", "fetch_json", "(", "uri_path", "=", "self", ".", "base_uri", "+", "'/checklists'", ",", "http_method", "=", "'POST'", ",", "query_params", "=", "query_params", "or", "{", "}", ")", "return", "self", ".", "create_checklist", "(", "checklist_json", ")"], "docstring": "Add a checklist to this card. Returns a Checklist object.", "docstring_tokens": ["Add", "a", "checklist", "to", "this", "card", ".", "Returns", "a", "Checklist", "object", "."], "sha": "483dc94c352df40dc05ead31820b059b2545cf82", "url": "https://github.com/its-rigs/Trolly/blob/483dc94c352df40dc05ead31820b059b2545cf82/trolly/card.py#L130-L140", "partition": "test"}
{"repo": "Terrance/SkPy", "path": "skpy/conn.py", "func_name": "SkypeLiveAuthProvider.checkUser", "original_string": "def checkUser(self, user):\n        \"\"\"\n        Query a username or email address to see if a corresponding Microsoft account exists.\n\n        Args:\n            user (str): username or email address of an account\n\n        Returns:\n            bool: whether the account exists\n        \"\"\"\n        return not self.conn(\"POST\", \"{0}/GetCredentialType.srf\".format(SkypeConnection.API_MSACC),\n                             json={\"username\": user}).json().get(\"IfExistsResult\")", "language": "python", "code": "def checkUser(self, user):\n        \"\"\"\n        Query a username or email address to see if a corresponding Microsoft account exists.\n\n        Args:\n            user (str): username or email address of an account\n\n        Returns:\n            bool: whether the account exists\n        \"\"\"\n        return not self.conn(\"POST\", \"{0}/GetCredentialType.srf\".format(SkypeConnection.API_MSACC),\n                             json={\"username\": user}).json().get(\"IfExistsResult\")", "code_tokens": ["def", "checkUser", "(", "self", ",", "user", ")", ":", "return", "not", "self", ".", "conn", "(", "\"POST\"", ",", "\"{0}/GetCredentialType.srf\"", ".", "format", "(", "SkypeConnection", ".", "API_MSACC", ")", ",", "json", "=", "{", "\"username\"", ":", "user", "}", ")", ".", "json", "(", ")", ".", "get", "(", "\"IfExistsResult\"", ")"], "docstring": "Query a username or email address to see if a corresponding Microsoft account exists.\n\n        Args:\n            user (str): username or email address of an account\n\n        Returns:\n            bool: whether the account exists", "docstring_tokens": ["Query", "a", "username", "or", "email", "address", "to", "see", "if", "a", "corresponding", "Microsoft", "account", "exists", "."], "sha": "0f9489c94e8ec4d3effab4314497428872a80ad1", "url": "https://github.com/Terrance/SkPy/blob/0f9489c94e8ec4d3effab4314497428872a80ad1/skpy/conn.py#L507-L518", "partition": "test"}
{"repo": "NarrativeScience/lsi", "path": "src/lsi/utils/term.py", "func_name": "get_color_hash", "original_string": "def get_color_hash(string, _min=MIN_COLOR_BRIGHT, _max=MAX_COLOR_BRIGHT):\n    \"\"\"\n    Hashes a string and returns a number between ``min`` and ``max``.\n    \"\"\"\n    hash_num = int(hashlib.sha1(string.encode('utf-8')).hexdigest()[:6], 16)\n    _range = _max - _min\n    num_in_range = hash_num % _range\n    return color(_min + num_in_range)", "language": "python", "code": "def get_color_hash(string, _min=MIN_COLOR_BRIGHT, _max=MAX_COLOR_BRIGHT):\n    \"\"\"\n    Hashes a string and returns a number between ``min`` and ``max``.\n    \"\"\"\n    hash_num = int(hashlib.sha1(string.encode('utf-8')).hexdigest()[:6], 16)\n    _range = _max - _min\n    num_in_range = hash_num % _range\n    return color(_min + num_in_range)", "code_tokens": ["def", "get_color_hash", "(", "string", ",", "_min", "=", "MIN_COLOR_BRIGHT", ",", "_max", "=", "MAX_COLOR_BRIGHT", ")", ":", "hash_num", "=", "int", "(", "hashlib", ".", "sha1", "(", "string", ".", "encode", "(", "'utf-8'", ")", ")", ".", "hexdigest", "(", ")", "[", ":", "6", "]", ",", "16", ")", "_range", "=", "_max", "-", "_min", "num_in_range", "=", "hash_num", "%", "_range", "return", "color", "(", "_min", "+", "num_in_range", ")"], "docstring": "Hashes a string and returns a number between ``min`` and ``max``.", "docstring_tokens": ["Hashes", "a", "string", "and", "returns", "a", "number", "between", "min", "and", "max", "."], "sha": "7d901b03fdb1a34ef795e5412bfe9685d948e32d", "url": "https://github.com/NarrativeScience/lsi/blob/7d901b03fdb1a34ef795e5412bfe9685d948e32d/src/lsi/utils/term.py#L76-L83", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/parallel/error.py", "func_name": "CompositeError.render_traceback", "original_string": "def render_traceback(self, excid=None):\n        \"\"\"render one or all of my tracebacks to a list of lines\"\"\"\n        lines = []\n        if excid is None:\n            for (en,ev,etb,ei) in self.elist:\n                lines.append(self._get_engine_str(ei))\n                lines.extend((etb or 'No traceback available').splitlines())\n                lines.append('')\n        else:\n            try:\n                en,ev,etb,ei = self.elist[excid]\n            except:\n                raise IndexError(\"an exception with index %i does not exist\"%excid)\n            else:\n                lines.append(self._get_engine_str(ei))\n                lines.extend((etb or 'No traceback available').splitlines())\n        \n        return lines", "language": "python", "code": "def render_traceback(self, excid=None):\n        \"\"\"render one or all of my tracebacks to a list of lines\"\"\"\n        lines = []\n        if excid is None:\n            for (en,ev,etb,ei) in self.elist:\n                lines.append(self._get_engine_str(ei))\n                lines.extend((etb or 'No traceback available').splitlines())\n                lines.append('')\n        else:\n            try:\n                en,ev,etb,ei = self.elist[excid]\n            except:\n                raise IndexError(\"an exception with index %i does not exist\"%excid)\n            else:\n                lines.append(self._get_engine_str(ei))\n                lines.extend((etb or 'No traceback available').splitlines())\n        \n        return lines", "code_tokens": ["def", "render_traceback", "(", "self", ",", "excid", "=", "None", ")", ":", "lines", "=", "[", "]", "if", "excid", "is", "None", ":", "for", "(", "en", ",", "ev", ",", "etb", ",", "ei", ")", "in", "self", ".", "elist", ":", "lines", ".", "append", "(", "self", ".", "_get_engine_str", "(", "ei", ")", ")", "lines", ".", "extend", "(", "(", "etb", "or", "'No traceback available'", ")", ".", "splitlines", "(", ")", ")", "lines", ".", "append", "(", "''", ")", "else", ":", "try", ":", "en", ",", "ev", ",", "etb", ",", "ei", "=", "self", ".", "elist", "[", "excid", "]", "except", ":", "raise", "IndexError", "(", "\"an exception with index %i does not exist\"", "%", "excid", ")", "else", ":", "lines", ".", "append", "(", "self", ".", "_get_engine_str", "(", "ei", ")", ")", "lines", ".", "extend", "(", "(", "etb", "or", "'No traceback available'", ")", ".", "splitlines", "(", ")", ")", "return", "lines"], "docstring": "render one or all of my tracebacks to a list of lines", "docstring_tokens": ["render", "one", "or", "all", "of", "my", "tracebacks", "to", "a", "list", "of", "lines"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/parallel/error.py#L262-L279", "partition": "test"}
{"repo": "alexprengere/currencyconverter", "path": "currency_converter/currency_converter.py", "func_name": "CurrencyConverter.convert", "original_string": "def convert(self, amount, currency, new_currency='EUR', date=None):\n        \"\"\"Convert amount from a currency to another one.\n\n        :param float amount: The amount of `currency` to convert.\n        :param str currency: The currency to convert from.\n        :param str new_currency: The currency to convert to.\n        :param datetime.date date: Use the conversion rate of this date. If this\n            is not given, the most recent rate is used.\n\n        :return: The value of `amount` in `new_currency`.\n        :rtype: float\n\n        >>> from datetime import date\n        >>> c = CurrencyConverter()\n        >>> c.convert(100, 'EUR', 'USD', date=date(2014, 3, 28))\n        137.5...\n        >>> c.convert(100, 'USD', date=date(2014, 3, 28))\n        72.67...\n        >>> c.convert(100, 'BGN', date=date(2010, 11, 21))\n        Traceback (most recent call last):\n        RateNotFoundError: BGN has no rate for 2010-11-21\n        \"\"\"\n        for c in currency, new_currency:\n            if c not in self.currencies:\n                raise ValueError('{0} is not a supported currency'.format(c))\n\n        if date is None:\n            date = self.bounds[currency].last_date\n        else:\n            try:\n                date = date.date()  # fallback if input was a datetime object\n            except AttributeError:\n                pass\n\n        r0 = self._get_rate(currency, date)\n        r1 = self._get_rate(new_currency, date)\n\n        return float(amount) / r0 * r1", "language": "python", "code": "def convert(self, amount, currency, new_currency='EUR', date=None):\n        \"\"\"Convert amount from a currency to another one.\n\n        :param float amount: The amount of `currency` to convert.\n        :param str currency: The currency to convert from.\n        :param str new_currency: The currency to convert to.\n        :param datetime.date date: Use the conversion rate of this date. If this\n            is not given, the most recent rate is used.\n\n        :return: The value of `amount` in `new_currency`.\n        :rtype: float\n\n        >>> from datetime import date\n        >>> c = CurrencyConverter()\n        >>> c.convert(100, 'EUR', 'USD', date=date(2014, 3, 28))\n        137.5...\n        >>> c.convert(100, 'USD', date=date(2014, 3, 28))\n        72.67...\n        >>> c.convert(100, 'BGN', date=date(2010, 11, 21))\n        Traceback (most recent call last):\n        RateNotFoundError: BGN has no rate for 2010-11-21\n        \"\"\"\n        for c in currency, new_currency:\n            if c not in self.currencies:\n                raise ValueError('{0} is not a supported currency'.format(c))\n\n        if date is None:\n            date = self.bounds[currency].last_date\n        else:\n            try:\n                date = date.date()  # fallback if input was a datetime object\n            except AttributeError:\n                pass\n\n        r0 = self._get_rate(currency, date)\n        r1 = self._get_rate(new_currency, date)\n\n        return float(amount) / r0 * r1", "code_tokens": ["def", "convert", "(", "self", ",", "amount", ",", "currency", ",", "new_currency", "=", "'EUR'", ",", "date", "=", "None", ")", ":", "for", "c", "in", "currency", ",", "new_currency", ":", "if", "c", "not", "in", "self", ".", "currencies", ":", "raise", "ValueError", "(", "'{0} is not a supported currency'", ".", "format", "(", "c", ")", ")", "if", "date", "is", "None", ":", "date", "=", "self", ".", "bounds", "[", "currency", "]", ".", "last_date", "else", ":", "try", ":", "date", "=", "date", ".", "date", "(", ")", "# fallback if input was a datetime object", "except", "AttributeError", ":", "pass", "r0", "=", "self", ".", "_get_rate", "(", "currency", ",", "date", ")", "r1", "=", "self", ".", "_get_rate", "(", "new_currency", ",", "date", ")", "return", "float", "(", "amount", ")", "/", "r0", "*", "r1"], "docstring": "Convert amount from a currency to another one.\n\n        :param float amount: The amount of `currency` to convert.\n        :param str currency: The currency to convert from.\n        :param str new_currency: The currency to convert to.\n        :param datetime.date date: Use the conversion rate of this date. If this\n            is not given, the most recent rate is used.\n\n        :return: The value of `amount` in `new_currency`.\n        :rtype: float\n\n        >>> from datetime import date\n        >>> c = CurrencyConverter()\n        >>> c.convert(100, 'EUR', 'USD', date=date(2014, 3, 28))\n        137.5...\n        >>> c.convert(100, 'USD', date=date(2014, 3, 28))\n        72.67...\n        >>> c.convert(100, 'BGN', date=date(2010, 11, 21))\n        Traceback (most recent call last):\n        RateNotFoundError: BGN has no rate for 2010-11-21", "docstring_tokens": ["Convert", "amount", "from", "a", "currency", "to", "another", "one", "."], "sha": "e3cb0d693819c0c824214225b23a47e9380f71df", "url": "https://github.com/alexprengere/currencyconverter/blob/e3cb0d693819c0c824214225b23a47e9380f71df/currency_converter/currency_converter.py#L286-L323", "partition": "test"}
{"repo": "jonfaustman/django-frontend", "path": "djfrontend/templatetags/djfrontend.py", "func_name": "djfrontend_jquery_formset", "original_string": "def djfrontend_jquery_formset(version=None):\n    \"\"\"\n    Returns the jQuery Dynamic Formset plugin file according to version number.\n    TEMPLATE_DEBUG returns full file, otherwise returns minified file.\n    \"\"\"\n    if version is None:\n        version = getattr(settings, 'DJFRONTEND_JQUERY_FORMSET', DJFRONTEND_JQUERY_FORMSET_DEFAULT)\n\n    if getattr(settings, 'TEMPLATE_DEBUG', False):\n        template = '<script src=\"{static}djfrontend/js/jquery/jquery.formset/{v}/jquery.formset.js\"></script>'\n    else:\n        template = (\n            '<script src=\"//cdnjs.cloudflare.com/ajax/libs/jquery.formset/{v}/jquery.formset.min.js\"></script>\\n'\n            '<script>window.jQuery.fn.formset || document.write(\\'<script src=\"{static}djfrontend/js/jquery/jquery.formset/{v}/jquery.formset.min.js\"><\\/script>\\')</script>')\n    return format_html(template, static=_static_url, v=version)", "language": "python", "code": "def djfrontend_jquery_formset(version=None):\n    \"\"\"\n    Returns the jQuery Dynamic Formset plugin file according to version number.\n    TEMPLATE_DEBUG returns full file, otherwise returns minified file.\n    \"\"\"\n    if version is None:\n        version = getattr(settings, 'DJFRONTEND_JQUERY_FORMSET', DJFRONTEND_JQUERY_FORMSET_DEFAULT)\n\n    if getattr(settings, 'TEMPLATE_DEBUG', False):\n        template = '<script src=\"{static}djfrontend/js/jquery/jquery.formset/{v}/jquery.formset.js\"></script>'\n    else:\n        template = (\n            '<script src=\"//cdnjs.cloudflare.com/ajax/libs/jquery.formset/{v}/jquery.formset.min.js\"></script>\\n'\n            '<script>window.jQuery.fn.formset || document.write(\\'<script src=\"{static}djfrontend/js/jquery/jquery.formset/{v}/jquery.formset.min.js\"><\\/script>\\')</script>')\n    return format_html(template, static=_static_url, v=version)", "code_tokens": ["def", "djfrontend_jquery_formset", "(", "version", "=", "None", ")", ":", "if", "version", "is", "None", ":", "version", "=", "getattr", "(", "settings", ",", "'DJFRONTEND_JQUERY_FORMSET'", ",", "DJFRONTEND_JQUERY_FORMSET_DEFAULT", ")", "if", "getattr", "(", "settings", ",", "'TEMPLATE_DEBUG'", ",", "False", ")", ":", "template", "=", "'<script src=\"{static}djfrontend/js/jquery/jquery.formset/{v}/jquery.formset.js\"></script>'", "else", ":", "template", "=", "(", "'<script src=\"//cdnjs.cloudflare.com/ajax/libs/jquery.formset/{v}/jquery.formset.min.js\"></script>\\n'", "'<script>window.jQuery.fn.formset || document.write(\\'<script src=\"{static}djfrontend/js/jquery/jquery.formset/{v}/jquery.formset.min.js\"><\\/script>\\')</script>'", ")", "return", "format_html", "(", "template", ",", "static", "=", "_static_url", ",", "v", "=", "version", ")"], "docstring": "Returns the jQuery Dynamic Formset plugin file according to version number.\n    TEMPLATE_DEBUG returns full file, otherwise returns minified file.", "docstring_tokens": ["Returns", "the", "jQuery", "Dynamic", "Formset", "plugin", "file", "according", "to", "version", "number", ".", "TEMPLATE_DEBUG", "returns", "full", "file", "otherwise", "returns", "minified", "file", "."], "sha": "897934d593fade0eb1998f8fadd18c91a89e5b9a", "url": "https://github.com/jonfaustman/django-frontend/blob/897934d593fade0eb1998f8fadd18c91a89e5b9a/djfrontend/templatetags/djfrontend.py#L186-L200", "partition": "test"}
{"repo": "rwl/godot", "path": "godot/ui/graph_view_model.py", "func_name": "GraphViewModel.on_exit", "original_string": "def on_exit(self, info):\n        \"\"\" Handles the user attempting to exit Godot.\n        \"\"\"\n        if self.prompt_on_exit:# and (not is_ok):\n            retval = confirm(parent  = info.ui.control,\n                             message = \"Exit Godot?\",\n                             title   = \"Confirm exit\",\n                             default = YES)\n            if retval == YES:\n                self._on_close( info )\n        else:\n            self._on_close( info )", "language": "python", "code": "def on_exit(self, info):\n        \"\"\" Handles the user attempting to exit Godot.\n        \"\"\"\n        if self.prompt_on_exit:# and (not is_ok):\n            retval = confirm(parent  = info.ui.control,\n                             message = \"Exit Godot?\",\n                             title   = \"Confirm exit\",\n                             default = YES)\n            if retval == YES:\n                self._on_close( info )\n        else:\n            self._on_close( info )", "code_tokens": ["def", "on_exit", "(", "self", ",", "info", ")", ":", "if", "self", ".", "prompt_on_exit", ":", "# and (not is_ok):", "retval", "=", "confirm", "(", "parent", "=", "info", ".", "ui", ".", "control", ",", "message", "=", "\"Exit Godot?\"", ",", "title", "=", "\"Confirm exit\"", ",", "default", "=", "YES", ")", "if", "retval", "==", "YES", ":", "self", ".", "_on_close", "(", "info", ")", "else", ":", "self", ".", "_on_close", "(", "info", ")"], "docstring": "Handles the user attempting to exit Godot.", "docstring_tokens": ["Handles", "the", "user", "attempting", "to", "exit", "Godot", "."], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/ui/graph_view_model.py#L474-L485", "partition": "test"}
{"repo": "rwl/godot", "path": "godot/component/polygon.py", "func_name": "Polygon.is_in", "original_string": "def is_in(self, point_x, point_y):\n        \"\"\" Test if a point is within this polygonal region \"\"\"\n\n        point_array = array(((point_x, point_y),))\n        vertices = array(self.points)\n        winding = self.inside_rule == \"winding\"\n        result = points_in_polygon(point_array, vertices, winding)\n        return result[0]", "language": "python", "code": "def is_in(self, point_x, point_y):\n        \"\"\" Test if a point is within this polygonal region \"\"\"\n\n        point_array = array(((point_x, point_y),))\n        vertices = array(self.points)\n        winding = self.inside_rule == \"winding\"\n        result = points_in_polygon(point_array, vertices, winding)\n        return result[0]", "code_tokens": ["def", "is_in", "(", "self", ",", "point_x", ",", "point_y", ")", ":", "point_array", "=", "array", "(", "(", "(", "point_x", ",", "point_y", ")", ",", ")", ")", "vertices", "=", "array", "(", "self", ".", "points", ")", "winding", "=", "self", ".", "inside_rule", "==", "\"winding\"", "result", "=", "points_in_polygon", "(", "point_array", ",", "vertices", ",", "winding", ")", "return", "result", "[", "0", "]"], "docstring": "Test if a point is within this polygonal region", "docstring_tokens": ["Test", "if", "a", "point", "is", "within", "this", "polygonal", "region"], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/component/polygon.py#L122-L129", "partition": "test"}
{"repo": "zqfang/GSEApy", "path": "gseapy/gsea.py", "func_name": "prerank", "original_string": "def prerank(rnk, gene_sets, outdir='GSEA_Prerank', pheno_pos='Pos', pheno_neg='Neg',\n            min_size=15, max_size=500, permutation_num=1000, weighted_score_type=1,\n            ascending=False, processes=1, figsize=(6.5,6), format='pdf',\n            graph_num=20, no_plot=False, seed=None, verbose=False):\n    \"\"\" Run Gene Set Enrichment Analysis with pre-ranked correlation defined by user.\n\n    :param rnk: pre-ranked correlation table or pandas DataFrame. Same input with ``GSEA`` .rnk file.\n    :param gene_sets: Enrichr Library name or .gmt gene sets file or dict of gene sets. Same input with GSEA.\n    :param outdir: results output directory.\n    :param int permutation_num: Number of permutations for significance computation. Default: 1000.\n    :param int min_size: Minimum allowed number of genes from gene set also the data set. Default: 15.\n    :param int max_size: Maximum allowed number of genes from gene set also the data set. Defaults: 500.\n    :param str weighted_score_type: Refer to :func:`algorithm.enrichment_score`. Default:1.\n    :param bool ascending: Sorting order of rankings. Default: False.\n    :param int processes: Number of Processes you are going to use. Default: 1.\n    :param list figsize: Matplotlib figsize, accept a tuple or list, e.g. [width,height]. Default: [6.5,6].\n    :param str format: Matplotlib figure format. Default: 'pdf'.\n    :param int graph_num: Plot graphs for top sets of each phenotype.\n    :param bool no_plot: If equals to True, no figure will be drawn. Default: False.\n    :param seed: Random seed. expect an integer. Default:None.\n    :param bool verbose: Bool, increase output verbosity, print out progress of your job, Default: False.\n\n    :return: Return a Prerank obj. All results store to  a dictionary, obj.results,\n             where contains::\n\n                 | {es: enrichment score,\n                 |  nes: normalized enrichment score,\n                 |  p: P-value,\n                 |  fdr: FDR,\n                 |  size: gene set size,\n                 |  matched_size: genes matched to the data,\n                 |  genes: gene names from the data set\n                 |  ledge_genes: leading edge genes}\n\n\n    \"\"\"\n    pre = Prerank(rnk, gene_sets, outdir, pheno_pos, pheno_neg,\n                  min_size, max_size, permutation_num, weighted_score_type,\n                  ascending, processes, figsize, format, graph_num, no_plot, seed, verbose)\n    pre.run()\n    return pre", "language": "python", "code": "def prerank(rnk, gene_sets, outdir='GSEA_Prerank', pheno_pos='Pos', pheno_neg='Neg',\n            min_size=15, max_size=500, permutation_num=1000, weighted_score_type=1,\n            ascending=False, processes=1, figsize=(6.5,6), format='pdf',\n            graph_num=20, no_plot=False, seed=None, verbose=False):\n    \"\"\" Run Gene Set Enrichment Analysis with pre-ranked correlation defined by user.\n\n    :param rnk: pre-ranked correlation table or pandas DataFrame. Same input with ``GSEA`` .rnk file.\n    :param gene_sets: Enrichr Library name or .gmt gene sets file or dict of gene sets. Same input with GSEA.\n    :param outdir: results output directory.\n    :param int permutation_num: Number of permutations for significance computation. Default: 1000.\n    :param int min_size: Minimum allowed number of genes from gene set also the data set. Default: 15.\n    :param int max_size: Maximum allowed number of genes from gene set also the data set. Defaults: 500.\n    :param str weighted_score_type: Refer to :func:`algorithm.enrichment_score`. Default:1.\n    :param bool ascending: Sorting order of rankings. Default: False.\n    :param int processes: Number of Processes you are going to use. Default: 1.\n    :param list figsize: Matplotlib figsize, accept a tuple or list, e.g. [width,height]. Default: [6.5,6].\n    :param str format: Matplotlib figure format. Default: 'pdf'.\n    :param int graph_num: Plot graphs for top sets of each phenotype.\n    :param bool no_plot: If equals to True, no figure will be drawn. Default: False.\n    :param seed: Random seed. expect an integer. Default:None.\n    :param bool verbose: Bool, increase output verbosity, print out progress of your job, Default: False.\n\n    :return: Return a Prerank obj. All results store to  a dictionary, obj.results,\n             where contains::\n\n                 | {es: enrichment score,\n                 |  nes: normalized enrichment score,\n                 |  p: P-value,\n                 |  fdr: FDR,\n                 |  size: gene set size,\n                 |  matched_size: genes matched to the data,\n                 |  genes: gene names from the data set\n                 |  ledge_genes: leading edge genes}\n\n\n    \"\"\"\n    pre = Prerank(rnk, gene_sets, outdir, pheno_pos, pheno_neg,\n                  min_size, max_size, permutation_num, weighted_score_type,\n                  ascending, processes, figsize, format, graph_num, no_plot, seed, verbose)\n    pre.run()\n    return pre", "code_tokens": ["def", "prerank", "(", "rnk", ",", "gene_sets", ",", "outdir", "=", "'GSEA_Prerank'", ",", "pheno_pos", "=", "'Pos'", ",", "pheno_neg", "=", "'Neg'", ",", "min_size", "=", "15", ",", "max_size", "=", "500", ",", "permutation_num", "=", "1000", ",", "weighted_score_type", "=", "1", ",", "ascending", "=", "False", ",", "processes", "=", "1", ",", "figsize", "=", "(", "6.5", ",", "6", ")", ",", "format", "=", "'pdf'", ",", "graph_num", "=", "20", ",", "no_plot", "=", "False", ",", "seed", "=", "None", ",", "verbose", "=", "False", ")", ":", "pre", "=", "Prerank", "(", "rnk", ",", "gene_sets", ",", "outdir", ",", "pheno_pos", ",", "pheno_neg", ",", "min_size", ",", "max_size", ",", "permutation_num", ",", "weighted_score_type", ",", "ascending", ",", "processes", ",", "figsize", ",", "format", ",", "graph_num", ",", "no_plot", ",", "seed", ",", "verbose", ")", "pre", ".", "run", "(", ")", "return", "pre"], "docstring": "Run Gene Set Enrichment Analysis with pre-ranked correlation defined by user.\n\n    :param rnk: pre-ranked correlation table or pandas DataFrame. Same input with ``GSEA`` .rnk file.\n    :param gene_sets: Enrichr Library name or .gmt gene sets file or dict of gene sets. Same input with GSEA.\n    :param outdir: results output directory.\n    :param int permutation_num: Number of permutations for significance computation. Default: 1000.\n    :param int min_size: Minimum allowed number of genes from gene set also the data set. Default: 15.\n    :param int max_size: Maximum allowed number of genes from gene set also the data set. Defaults: 500.\n    :param str weighted_score_type: Refer to :func:`algorithm.enrichment_score`. Default:1.\n    :param bool ascending: Sorting order of rankings. Default: False.\n    :param int processes: Number of Processes you are going to use. Default: 1.\n    :param list figsize: Matplotlib figsize, accept a tuple or list, e.g. [width,height]. Default: [6.5,6].\n    :param str format: Matplotlib figure format. Default: 'pdf'.\n    :param int graph_num: Plot graphs for top sets of each phenotype.\n    :param bool no_plot: If equals to True, no figure will be drawn. Default: False.\n    :param seed: Random seed. expect an integer. Default:None.\n    :param bool verbose: Bool, increase output verbosity, print out progress of your job, Default: False.\n\n    :return: Return a Prerank obj. All results store to  a dictionary, obj.results,\n             where contains::\n\n                 | {es: enrichment score,\n                 |  nes: normalized enrichment score,\n                 |  p: P-value,\n                 |  fdr: FDR,\n                 |  size: gene set size,\n                 |  matched_size: genes matched to the data,\n                 |  genes: gene names from the data set\n                 |  ledge_genes: leading edge genes}", "docstring_tokens": ["Run", "Gene", "Set", "Enrichment", "Analysis", "with", "pre", "-", "ranked", "correlation", "defined", "by", "user", "."], "sha": "673e9ec1391e3b14d3e8a4353117151fd2cb9345", "url": "https://github.com/zqfang/GSEApy/blob/673e9ec1391e3b14d3e8a4353117151fd2cb9345/gseapy/gsea.py#L991-L1031", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/server/blueprints/cases/views.py", "func_name": "case_diagnosis", "original_string": "def case_diagnosis(institute_id, case_name):\n    \"\"\"Add or remove a diagnosis for a case.\"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    user_obj = store.user(current_user.email)\n    link = url_for('.case', institute_id=institute_id, case_name=case_name)\n    level = 'phenotype' if 'phenotype' in request.form else 'gene'\n    omim_id = request.form['omim_id']\n    remove = True if request.args.get('remove') == 'yes' else False\n    store.diagnose(institute_obj, case_obj, user_obj, link, level=level,\n                   omim_id=omim_id, remove=remove)\n    return redirect(request.referrer)", "language": "python", "code": "def case_diagnosis(institute_id, case_name):\n    \"\"\"Add or remove a diagnosis for a case.\"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    user_obj = store.user(current_user.email)\n    link = url_for('.case', institute_id=institute_id, case_name=case_name)\n    level = 'phenotype' if 'phenotype' in request.form else 'gene'\n    omim_id = request.form['omim_id']\n    remove = True if request.args.get('remove') == 'yes' else False\n    store.diagnose(institute_obj, case_obj, user_obj, link, level=level,\n                   omim_id=omim_id, remove=remove)\n    return redirect(request.referrer)", "code_tokens": ["def", "case_diagnosis", "(", "institute_id", ",", "case_name", ")", ":", "institute_obj", ",", "case_obj", "=", "institute_and_case", "(", "store", ",", "institute_id", ",", "case_name", ")", "user_obj", "=", "store", ".", "user", "(", "current_user", ".", "email", ")", "link", "=", "url_for", "(", "'.case'", ",", "institute_id", "=", "institute_id", ",", "case_name", "=", "case_name", ")", "level", "=", "'phenotype'", "if", "'phenotype'", "in", "request", ".", "form", "else", "'gene'", "omim_id", "=", "request", ".", "form", "[", "'omim_id'", "]", "remove", "=", "True", "if", "request", ".", "args", ".", "get", "(", "'remove'", ")", "==", "'yes'", "else", "False", "store", ".", "diagnose", "(", "institute_obj", ",", "case_obj", ",", "user_obj", ",", "link", ",", "level", "=", "level", ",", "omim_id", "=", "omim_id", ",", "remove", "=", "remove", ")", "return", "redirect", "(", "request", ".", "referrer", ")"], "docstring": "Add or remove a diagnosis for a case.", "docstring_tokens": ["Add", "or", "remove", "a", "diagnosis", "for", "a", "case", "."], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/server/blueprints/cases/views.py#L466-L476", "partition": "test"}
{"repo": "vaexio/vaex", "path": "packages/vaex-core/vaex/dataframe.py", "func_name": "DataFrame.describe", "original_string": "def describe(self, strings=True, virtual=True, selection=None):\n        \"\"\"Give a description of the DataFrame.\n\n        >>> import vaex\n        >>> df = vaex.example()[['x', 'y', 'z']]\n        >>> df.describe()\n                         x          y          z\n        dtype      float64    float64    float64\n        count       330000     330000     330000\n        missing          0          0          0\n        mean    -0.0671315 -0.0535899  0.0169582\n        std        7.31746    7.78605    5.05521\n        min       -128.294   -71.5524   -44.3342\n        max        271.366    146.466    50.7185\n        >>> df.describe(selection=df.x > 0)\n                           x         y          z\n        dtype        float64   float64    float64\n        count         164060    164060     164060\n        missing       165940    165940     165940\n        mean         5.13572 -0.486786 -0.0868073\n        std          5.18701   7.61621    5.02831\n        min      1.51635e-05  -71.5524   -44.3342\n        max          271.366   78.0724    40.2191\n\n        :param bool strings: Describe string columns or not\n        :param bool virtual: Describe virtual columns or not\n        :param selection: Optional selection to use.\n        :return: Pandas dataframe\n\n        \"\"\"\n        import pandas as pd\n        N = len(self)\n        columns = {}\n        for feature in self.get_column_names(strings=strings, virtual=virtual)[:]:\n            dtype = str(self.dtype(feature)) if self.dtype(feature) != str else 'str'\n            if self.dtype(feature) == str_type or self.dtype(feature).kind in ['S', 'U', 'O']:\n                count = self.count(feature, selection=selection, delay=True)\n                self.execute()\n                count = count.get()\n                columns[feature] = ((dtype, count, N-count, '--', '--', '--', '--'))\n            else:\n                count = self.count(feature, selection=selection, delay=True)\n                mean = self.mean(feature, selection=selection, delay=True)\n                std = self.std(feature, selection=selection, delay=True)\n                minmax = self.minmax(feature, selection=selection, delay=True)\n                self.execute()\n                count, mean, std, minmax = count.get(), mean.get(), std.get(), minmax.get()\n                count = int(count)\n                columns[feature] = ((dtype, count, N-count, mean, std, minmax[0], minmax[1]))\n        return pd.DataFrame(data=columns, index=['dtype', 'count', 'missing', 'mean', 'std', 'min', 'max'])", "language": "python", "code": "def describe(self, strings=True, virtual=True, selection=None):\n        \"\"\"Give a description of the DataFrame.\n\n        >>> import vaex\n        >>> df = vaex.example()[['x', 'y', 'z']]\n        >>> df.describe()\n                         x          y          z\n        dtype      float64    float64    float64\n        count       330000     330000     330000\n        missing          0          0          0\n        mean    -0.0671315 -0.0535899  0.0169582\n        std        7.31746    7.78605    5.05521\n        min       -128.294   -71.5524   -44.3342\n        max        271.366    146.466    50.7185\n        >>> df.describe(selection=df.x > 0)\n                           x         y          z\n        dtype        float64   float64    float64\n        count         164060    164060     164060\n        missing       165940    165940     165940\n        mean         5.13572 -0.486786 -0.0868073\n        std          5.18701   7.61621    5.02831\n        min      1.51635e-05  -71.5524   -44.3342\n        max          271.366   78.0724    40.2191\n\n        :param bool strings: Describe string columns or not\n        :param bool virtual: Describe virtual columns or not\n        :param selection: Optional selection to use.\n        :return: Pandas dataframe\n\n        \"\"\"\n        import pandas as pd\n        N = len(self)\n        columns = {}\n        for feature in self.get_column_names(strings=strings, virtual=virtual)[:]:\n            dtype = str(self.dtype(feature)) if self.dtype(feature) != str else 'str'\n            if self.dtype(feature) == str_type or self.dtype(feature).kind in ['S', 'U', 'O']:\n                count = self.count(feature, selection=selection, delay=True)\n                self.execute()\n                count = count.get()\n                columns[feature] = ((dtype, count, N-count, '--', '--', '--', '--'))\n            else:\n                count = self.count(feature, selection=selection, delay=True)\n                mean = self.mean(feature, selection=selection, delay=True)\n                std = self.std(feature, selection=selection, delay=True)\n                minmax = self.minmax(feature, selection=selection, delay=True)\n                self.execute()\n                count, mean, std, minmax = count.get(), mean.get(), std.get(), minmax.get()\n                count = int(count)\n                columns[feature] = ((dtype, count, N-count, mean, std, minmax[0], minmax[1]))\n        return pd.DataFrame(data=columns, index=['dtype', 'count', 'missing', 'mean', 'std', 'min', 'max'])", "code_tokens": ["def", "describe", "(", "self", ",", "strings", "=", "True", ",", "virtual", "=", "True", ",", "selection", "=", "None", ")", ":", "import", "pandas", "as", "pd", "N", "=", "len", "(", "self", ")", "columns", "=", "{", "}", "for", "feature", "in", "self", ".", "get_column_names", "(", "strings", "=", "strings", ",", "virtual", "=", "virtual", ")", "[", ":", "]", ":", "dtype", "=", "str", "(", "self", ".", "dtype", "(", "feature", ")", ")", "if", "self", ".", "dtype", "(", "feature", ")", "!=", "str", "else", "'str'", "if", "self", ".", "dtype", "(", "feature", ")", "==", "str_type", "or", "self", ".", "dtype", "(", "feature", ")", ".", "kind", "in", "[", "'S'", ",", "'U'", ",", "'O'", "]", ":", "count", "=", "self", ".", "count", "(", "feature", ",", "selection", "=", "selection", ",", "delay", "=", "True", ")", "self", ".", "execute", "(", ")", "count", "=", "count", ".", "get", "(", ")", "columns", "[", "feature", "]", "=", "(", "(", "dtype", ",", "count", ",", "N", "-", "count", ",", "'--'", ",", "'--'", ",", "'--'", ",", "'--'", ")", ")", "else", ":", "count", "=", "self", ".", "count", "(", "feature", ",", "selection", "=", "selection", ",", "delay", "=", "True", ")", "mean", "=", "self", ".", "mean", "(", "feature", ",", "selection", "=", "selection", ",", "delay", "=", "True", ")", "std", "=", "self", ".", "std", "(", "feature", ",", "selection", "=", "selection", ",", "delay", "=", "True", ")", "minmax", "=", "self", ".", "minmax", "(", "feature", ",", "selection", "=", "selection", ",", "delay", "=", "True", ")", "self", ".", "execute", "(", ")", "count", ",", "mean", ",", "std", ",", "minmax", "=", "count", ".", "get", "(", ")", ",", "mean", ".", "get", "(", ")", ",", "std", ".", "get", "(", ")", ",", "minmax", ".", "get", "(", ")", "count", "=", "int", "(", "count", ")", "columns", "[", "feature", "]", "=", "(", "(", "dtype", ",", "count", ",", "N", "-", "count", ",", "mean", ",", "std", ",", "minmax", "[", "0", "]", ",", "minmax", "[", "1", "]", ")", ")", "return", "pd", ".", "DataFrame", "(", "data", "=", "columns", ",", "index", "=", "[", "'dtype'", ",", "'count'", ",", "'missing'", ",", "'mean'", ",", "'std'", ",", "'min'", ",", "'max'", "]", ")"], "docstring": "Give a description of the DataFrame.\n\n        >>> import vaex\n        >>> df = vaex.example()[['x', 'y', 'z']]\n        >>> df.describe()\n                         x          y          z\n        dtype      float64    float64    float64\n        count       330000     330000     330000\n        missing          0          0          0\n        mean    -0.0671315 -0.0535899  0.0169582\n        std        7.31746    7.78605    5.05521\n        min       -128.294   -71.5524   -44.3342\n        max        271.366    146.466    50.7185\n        >>> df.describe(selection=df.x > 0)\n                           x         y          z\n        dtype        float64   float64    float64\n        count         164060    164060     164060\n        missing       165940    165940     165940\n        mean         5.13572 -0.486786 -0.0868073\n        std          5.18701   7.61621    5.02831\n        min      1.51635e-05  -71.5524   -44.3342\n        max          271.366   78.0724    40.2191\n\n        :param bool strings: Describe string columns or not\n        :param bool virtual: Describe virtual columns or not\n        :param selection: Optional selection to use.\n        :return: Pandas dataframe", "docstring_tokens": ["Give", "a", "description", "of", "the", "DataFrame", "."], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/dataframe.py#L3415-L3464", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/itsdangerous.py", "func_name": "TimestampSigner.validate", "original_string": "def validate(self, signed_value, max_age=None):\n        \"\"\"Just validates the given signed value.  Returns `True` if the\n        signature exists and is valid, `False` otherwise.\"\"\"\n        try:\n            self.unsign(signed_value, max_age=max_age)\n            return True\n        except BadSignature:\n            return False", "language": "python", "code": "def validate(self, signed_value, max_age=None):\n        \"\"\"Just validates the given signed value.  Returns `True` if the\n        signature exists and is valid, `False` otherwise.\"\"\"\n        try:\n            self.unsign(signed_value, max_age=max_age)\n            return True\n        except BadSignature:\n            return False", "code_tokens": ["def", "validate", "(", "self", ",", "signed_value", ",", "max_age", "=", "None", ")", ":", "try", ":", "self", ".", "unsign", "(", "signed_value", ",", "max_age", "=", "max_age", ")", "return", "True", "except", "BadSignature", ":", "return", "False"], "docstring": "Just validates the given signed value.  Returns `True` if the\n        signature exists and is valid, `False` otherwise.", "docstring_tokens": ["Just", "validates", "the", "given", "signed", "value", ".", "Returns", "True", "if", "the", "signature", "exists", "and", "is", "valid", "False", "otherwise", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/itsdangerous.py#L469-L476", "partition": "test"}
{"repo": "pmacosta/peng", "path": "peng/wave_functions.py", "func_name": "_bound_waveform", "original_string": "def _bound_waveform(wave, indep_min, indep_max):\n    \"\"\"Add independent variable vector bounds if they are not in vector.\"\"\"\n    indep_min, indep_max = _validate_min_max(wave, indep_min, indep_max)\n    indep_vector = copy.copy(wave._indep_vector)\n    if (\n        isinstance(indep_min, float) or isinstance(indep_max, float)\n    ) and indep_vector.dtype.name.startswith(\"int\"):\n        indep_vector = indep_vector.astype(float)\n    min_pos = np.searchsorted(indep_vector, indep_min)\n    if not np.isclose(indep_min, indep_vector[min_pos], FP_RTOL, FP_ATOL):\n        indep_vector = np.insert(indep_vector, min_pos, indep_min)\n    max_pos = np.searchsorted(indep_vector, indep_max)\n    if not np.isclose(indep_max, indep_vector[max_pos], FP_RTOL, FP_ATOL):\n        indep_vector = np.insert(indep_vector, max_pos, indep_max)\n    dep_vector = _interp_dep_vector(wave, indep_vector)\n    wave._indep_vector = indep_vector[min_pos : max_pos + 1]\n    wave._dep_vector = dep_vector[min_pos : max_pos + 1]", "language": "python", "code": "def _bound_waveform(wave, indep_min, indep_max):\n    \"\"\"Add independent variable vector bounds if they are not in vector.\"\"\"\n    indep_min, indep_max = _validate_min_max(wave, indep_min, indep_max)\n    indep_vector = copy.copy(wave._indep_vector)\n    if (\n        isinstance(indep_min, float) or isinstance(indep_max, float)\n    ) and indep_vector.dtype.name.startswith(\"int\"):\n        indep_vector = indep_vector.astype(float)\n    min_pos = np.searchsorted(indep_vector, indep_min)\n    if not np.isclose(indep_min, indep_vector[min_pos], FP_RTOL, FP_ATOL):\n        indep_vector = np.insert(indep_vector, min_pos, indep_min)\n    max_pos = np.searchsorted(indep_vector, indep_max)\n    if not np.isclose(indep_max, indep_vector[max_pos], FP_RTOL, FP_ATOL):\n        indep_vector = np.insert(indep_vector, max_pos, indep_max)\n    dep_vector = _interp_dep_vector(wave, indep_vector)\n    wave._indep_vector = indep_vector[min_pos : max_pos + 1]\n    wave._dep_vector = dep_vector[min_pos : max_pos + 1]", "code_tokens": ["def", "_bound_waveform", "(", "wave", ",", "indep_min", ",", "indep_max", ")", ":", "indep_min", ",", "indep_max", "=", "_validate_min_max", "(", "wave", ",", "indep_min", ",", "indep_max", ")", "indep_vector", "=", "copy", ".", "copy", "(", "wave", ".", "_indep_vector", ")", "if", "(", "isinstance", "(", "indep_min", ",", "float", ")", "or", "isinstance", "(", "indep_max", ",", "float", ")", ")", "and", "indep_vector", ".", "dtype", ".", "name", ".", "startswith", "(", "\"int\"", ")", ":", "indep_vector", "=", "indep_vector", ".", "astype", "(", "float", ")", "min_pos", "=", "np", ".", "searchsorted", "(", "indep_vector", ",", "indep_min", ")", "if", "not", "np", ".", "isclose", "(", "indep_min", ",", "indep_vector", "[", "min_pos", "]", ",", "FP_RTOL", ",", "FP_ATOL", ")", ":", "indep_vector", "=", "np", ".", "insert", "(", "indep_vector", ",", "min_pos", ",", "indep_min", ")", "max_pos", "=", "np", ".", "searchsorted", "(", "indep_vector", ",", "indep_max", ")", "if", "not", "np", ".", "isclose", "(", "indep_max", ",", "indep_vector", "[", "max_pos", "]", ",", "FP_RTOL", ",", "FP_ATOL", ")", ":", "indep_vector", "=", "np", ".", "insert", "(", "indep_vector", ",", "max_pos", ",", "indep_max", ")", "dep_vector", "=", "_interp_dep_vector", "(", "wave", ",", "indep_vector", ")", "wave", ".", "_indep_vector", "=", "indep_vector", "[", "min_pos", ":", "max_pos", "+", "1", "]", "wave", ".", "_dep_vector", "=", "dep_vector", "[", "min_pos", ":", "max_pos", "+", "1", "]"], "docstring": "Add independent variable vector bounds if they are not in vector.", "docstring_tokens": ["Add", "independent", "variable", "vector", "bounds", "if", "they", "are", "not", "in", "vector", "."], "sha": "976935377adaa3de26fc5677aceb2cdfbd6f93a7", "url": "https://github.com/pmacosta/peng/blob/976935377adaa3de26fc5677aceb2cdfbd6f93a7/peng/wave_functions.py#L48-L64", "partition": "test"}
{"repo": "bjoernricks/python-quilt", "path": "quilt/push.py", "func_name": "Push.apply_next_patch", "original_string": "def apply_next_patch(self, force=False, quiet=False):\n        \"\"\" Apply next patch in series file \"\"\"\n        self._check()\n        top = self.db.top_patch()\n        if not top:\n            patch = self.series.first_patch()\n        else:\n            patch = self.series.patch_after(top)\n\n        if not patch:\n            raise AllPatchesApplied(self.series, top)\n\n        self.applying(patch)\n\n        self._apply_patch(patch, force, quiet)\n\n        self.db.save()\n\n        self.applied(self.db.top_patch())", "language": "python", "code": "def apply_next_patch(self, force=False, quiet=False):\n        \"\"\" Apply next patch in series file \"\"\"\n        self._check()\n        top = self.db.top_patch()\n        if not top:\n            patch = self.series.first_patch()\n        else:\n            patch = self.series.patch_after(top)\n\n        if not patch:\n            raise AllPatchesApplied(self.series, top)\n\n        self.applying(patch)\n\n        self._apply_patch(patch, force, quiet)\n\n        self.db.save()\n\n        self.applied(self.db.top_patch())", "code_tokens": ["def", "apply_next_patch", "(", "self", ",", "force", "=", "False", ",", "quiet", "=", "False", ")", ":", "self", ".", "_check", "(", ")", "top", "=", "self", ".", "db", ".", "top_patch", "(", ")", "if", "not", "top", ":", "patch", "=", "self", ".", "series", ".", "first_patch", "(", ")", "else", ":", "patch", "=", "self", ".", "series", ".", "patch_after", "(", "top", ")", "if", "not", "patch", ":", "raise", "AllPatchesApplied", "(", "self", ".", "series", ",", "top", ")", "self", ".", "applying", "(", "patch", ")", "self", ".", "_apply_patch", "(", "patch", ",", "force", ",", "quiet", ")", "self", ".", "db", ".", "save", "(", ")", "self", ".", "applied", "(", "self", ".", "db", ".", "top_patch", "(", ")", ")"], "docstring": "Apply next patch in series file", "docstring_tokens": ["Apply", "next", "patch", "in", "series", "file"], "sha": "fae88237f601848cc34d073584d9dcb409f01777", "url": "https://github.com/bjoernricks/python-quilt/blob/fae88237f601848cc34d073584d9dcb409f01777/quilt/push.py#L111-L129", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/importer.py", "func_name": "BasilispImporter._exec_module", "original_string": "def _exec_module(\n        self,\n        fullname: str,\n        loader_state: Mapping[str, str],\n        path_stats: Mapping[str, int],\n        module: types.ModuleType,\n    ):\n        \"\"\"Load and execute a non-cached Basilisp module.\"\"\"\n        filename = loader_state[\"filename\"]\n        cache_filename = loader_state[\"cache_filename\"]\n\n        with timed(\n            lambda duration: logger.debug(\n                f\"Loaded Basilisp module '{fullname}' in {duration / 1000000}ms\"\n            )\n        ):\n            # During compilation, bytecode objects are added to the list via the closure\n            # add_bytecode below, which is passed to the compiler. The collected bytecodes\n            # will be used to generate an .lpyc file for caching the compiled file.\n            all_bytecode = []\n\n            def add_bytecode(bytecode: types.CodeType):\n                all_bytecode.append(bytecode)\n\n            logger.debug(f\"Reading and compiling Basilisp module '{fullname}'\")\n            forms = reader.read_file(filename, resolver=runtime.resolve_alias)\n            compiler.compile_module(  # pylint: disable=unexpected-keyword-arg\n                forms,\n                compiler.CompilerContext(filename=filename),\n                module,\n                collect_bytecode=add_bytecode,\n            )\n\n        # Cache the bytecode that was collected through the compilation run.\n        cache_file_bytes = _basilisp_bytecode(\n            path_stats[\"mtime\"], path_stats[\"size\"], all_bytecode\n        )\n        self._cache_bytecode(filename, cache_filename, cache_file_bytes)", "language": "python", "code": "def _exec_module(\n        self,\n        fullname: str,\n        loader_state: Mapping[str, str],\n        path_stats: Mapping[str, int],\n        module: types.ModuleType,\n    ):\n        \"\"\"Load and execute a non-cached Basilisp module.\"\"\"\n        filename = loader_state[\"filename\"]\n        cache_filename = loader_state[\"cache_filename\"]\n\n        with timed(\n            lambda duration: logger.debug(\n                f\"Loaded Basilisp module '{fullname}' in {duration / 1000000}ms\"\n            )\n        ):\n            # During compilation, bytecode objects are added to the list via the closure\n            # add_bytecode below, which is passed to the compiler. The collected bytecodes\n            # will be used to generate an .lpyc file for caching the compiled file.\n            all_bytecode = []\n\n            def add_bytecode(bytecode: types.CodeType):\n                all_bytecode.append(bytecode)\n\n            logger.debug(f\"Reading and compiling Basilisp module '{fullname}'\")\n            forms = reader.read_file(filename, resolver=runtime.resolve_alias)\n            compiler.compile_module(  # pylint: disable=unexpected-keyword-arg\n                forms,\n                compiler.CompilerContext(filename=filename),\n                module,\n                collect_bytecode=add_bytecode,\n            )\n\n        # Cache the bytecode that was collected through the compilation run.\n        cache_file_bytes = _basilisp_bytecode(\n            path_stats[\"mtime\"], path_stats[\"size\"], all_bytecode\n        )\n        self._cache_bytecode(filename, cache_filename, cache_file_bytes)", "code_tokens": ["def", "_exec_module", "(", "self", ",", "fullname", ":", "str", ",", "loader_state", ":", "Mapping", "[", "str", ",", "str", "]", ",", "path_stats", ":", "Mapping", "[", "str", ",", "int", "]", ",", "module", ":", "types", ".", "ModuleType", ",", ")", ":", "filename", "=", "loader_state", "[", "\"filename\"", "]", "cache_filename", "=", "loader_state", "[", "\"cache_filename\"", "]", "with", "timed", "(", "lambda", "duration", ":", "logger", ".", "debug", "(", "f\"Loaded Basilisp module '{fullname}' in {duration / 1000000}ms\"", ")", ")", ":", "# During compilation, bytecode objects are added to the list via the closure", "# add_bytecode below, which is passed to the compiler. The collected bytecodes", "# will be used to generate an .lpyc file for caching the compiled file.", "all_bytecode", "=", "[", "]", "def", "add_bytecode", "(", "bytecode", ":", "types", ".", "CodeType", ")", ":", "all_bytecode", ".", "append", "(", "bytecode", ")", "logger", ".", "debug", "(", "f\"Reading and compiling Basilisp module '{fullname}'\"", ")", "forms", "=", "reader", ".", "read_file", "(", "filename", ",", "resolver", "=", "runtime", ".", "resolve_alias", ")", "compiler", ".", "compile_module", "(", "# pylint: disable=unexpected-keyword-arg", "forms", ",", "compiler", ".", "CompilerContext", "(", "filename", "=", "filename", ")", ",", "module", ",", "collect_bytecode", "=", "add_bytecode", ",", ")", "# Cache the bytecode that was collected through the compilation run.", "cache_file_bytes", "=", "_basilisp_bytecode", "(", "path_stats", "[", "\"mtime\"", "]", ",", "path_stats", "[", "\"size\"", "]", ",", "all_bytecode", ")", "self", ".", "_cache_bytecode", "(", "filename", ",", "cache_filename", ",", "cache_file_bytes", ")"], "docstring": "Load and execute a non-cached Basilisp module.", "docstring_tokens": ["Load", "and", "execute", "a", "non", "-", "cached", "Basilisp", "module", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/importer.py#L203-L240", "partition": "test"}
{"repo": "solvebio/solvebio-python", "path": "solvebio/resource/apiresource.py", "func_name": "DownloadableAPIResource.download", "original_string": "def download(self, path=None, **kwargs):\n        \"\"\"\n        Download the file to the specified directory or file path.\n        Downloads to a temporary directory if no path is specified.\n\n        Returns the absolute path to the file.\n        \"\"\"\n        download_url = self.download_url(**kwargs)\n        try:\n            # For vault objects, use the object's filename\n            # as the fallback if none is specified.\n            filename = self.filename\n        except AttributeError:\n            # If the object has no filename attribute,\n            # extract one from the download URL.\n            filename = download_url.split('%3B%20filename%3D')[1]\n            # Remove additional URL params from the name and \"unquote\" it.\n            filename = unquote(filename.split('&')[0])\n\n        if path:\n            path = os.path.expanduser(path)\n            # If the path is a dir, use the extracted filename\n            if os.path.isdir(path):\n                path = os.path.join(path, filename)\n        else:\n            # Create a temporary directory for the file\n            path = os.path.join(tempfile.gettempdir(), filename)\n\n        try:\n            response = requests.request(method='get', url=download_url)\n        except Exception as e:\n            _handle_request_error(e)\n\n        if not (200 <= response.status_code < 400):\n            _handle_api_error(response)\n\n        with open(path, 'wb') as fileobj:\n            fileobj.write(response._content)\n\n        return path", "language": "python", "code": "def download(self, path=None, **kwargs):\n        \"\"\"\n        Download the file to the specified directory or file path.\n        Downloads to a temporary directory if no path is specified.\n\n        Returns the absolute path to the file.\n        \"\"\"\n        download_url = self.download_url(**kwargs)\n        try:\n            # For vault objects, use the object's filename\n            # as the fallback if none is specified.\n            filename = self.filename\n        except AttributeError:\n            # If the object has no filename attribute,\n            # extract one from the download URL.\n            filename = download_url.split('%3B%20filename%3D')[1]\n            # Remove additional URL params from the name and \"unquote\" it.\n            filename = unquote(filename.split('&')[0])\n\n        if path:\n            path = os.path.expanduser(path)\n            # If the path is a dir, use the extracted filename\n            if os.path.isdir(path):\n                path = os.path.join(path, filename)\n        else:\n            # Create a temporary directory for the file\n            path = os.path.join(tempfile.gettempdir(), filename)\n\n        try:\n            response = requests.request(method='get', url=download_url)\n        except Exception as e:\n            _handle_request_error(e)\n\n        if not (200 <= response.status_code < 400):\n            _handle_api_error(response)\n\n        with open(path, 'wb') as fileobj:\n            fileobj.write(response._content)\n\n        return path", "code_tokens": ["def", "download", "(", "self", ",", "path", "=", "None", ",", "*", "*", "kwargs", ")", ":", "download_url", "=", "self", ".", "download_url", "(", "*", "*", "kwargs", ")", "try", ":", "# For vault objects, use the object's filename", "# as the fallback if none is specified.", "filename", "=", "self", ".", "filename", "except", "AttributeError", ":", "# If the object has no filename attribute,", "# extract one from the download URL.", "filename", "=", "download_url", ".", "split", "(", "'%3B%20filename%3D'", ")", "[", "1", "]", "# Remove additional URL params from the name and \"unquote\" it.", "filename", "=", "unquote", "(", "filename", ".", "split", "(", "'&'", ")", "[", "0", "]", ")", "if", "path", ":", "path", "=", "os", ".", "path", ".", "expanduser", "(", "path", ")", "# If the path is a dir, use the extracted filename", "if", "os", ".", "path", ".", "isdir", "(", "path", ")", ":", "path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "filename", ")", "else", ":", "# Create a temporary directory for the file", "path", "=", "os", ".", "path", ".", "join", "(", "tempfile", ".", "gettempdir", "(", ")", ",", "filename", ")", "try", ":", "response", "=", "requests", ".", "request", "(", "method", "=", "'get'", ",", "url", "=", "download_url", ")", "except", "Exception", "as", "e", ":", "_handle_request_error", "(", "e", ")", "if", "not", "(", "200", "<=", "response", ".", "status_code", "<", "400", ")", ":", "_handle_api_error", "(", "response", ")", "with", "open", "(", "path", ",", "'wb'", ")", "as", "fileobj", ":", "fileobj", ".", "write", "(", "response", ".", "_content", ")", "return", "path"], "docstring": "Download the file to the specified directory or file path.\n        Downloads to a temporary directory if no path is specified.\n\n        Returns the absolute path to the file.", "docstring_tokens": ["Download", "the", "file", "to", "the", "specified", "directory", "or", "file", "path", ".", "Downloads", "to", "a", "temporary", "directory", "if", "no", "path", "is", "specified", "."], "sha": "b29614643043afd19c1d8074e8f25c6700d51a73", "url": "https://github.com/solvebio/solvebio-python/blob/b29614643043afd19c1d8074e8f25c6700d51a73/solvebio/resource/apiresource.py#L187-L226", "partition": "test"}
{"repo": "solvebio/solvebio-python", "path": "solvebio/cli/main.py", "func_name": "main", "original_string": "def main(argv=sys.argv[1:]):\n    \"\"\" Main entry point for SolveBio CLI \"\"\"\n    parser = SolveArgumentParser()\n    args = parser.parse_solvebio_args(argv)\n\n    if args.api_host:\n        solvebio.api_host = args.api_host\n\n    if args.api_key:\n        solvebio.api_key = args.api_key\n\n    if not solvebio.api_key:\n        # If nothing is set (via command line or environment)\n        # look in local credentials\n        try:\n            from .credentials import get_credentials\n            solvebio.api_key = get_credentials()\n        except:\n            pass\n\n    # Update the client host and token\n    client.set_host()\n    client.set_token()\n\n    return args.func(args)", "language": "python", "code": "def main(argv=sys.argv[1:]):\n    \"\"\" Main entry point for SolveBio CLI \"\"\"\n    parser = SolveArgumentParser()\n    args = parser.parse_solvebio_args(argv)\n\n    if args.api_host:\n        solvebio.api_host = args.api_host\n\n    if args.api_key:\n        solvebio.api_key = args.api_key\n\n    if not solvebio.api_key:\n        # If nothing is set (via command line or environment)\n        # look in local credentials\n        try:\n            from .credentials import get_credentials\n            solvebio.api_key = get_credentials()\n        except:\n            pass\n\n    # Update the client host and token\n    client.set_host()\n    client.set_token()\n\n    return args.func(args)", "code_tokens": ["def", "main", "(", "argv", "=", "sys", ".", "argv", "[", "1", ":", "]", ")", ":", "parser", "=", "SolveArgumentParser", "(", ")", "args", "=", "parser", ".", "parse_solvebio_args", "(", "argv", ")", "if", "args", ".", "api_host", ":", "solvebio", ".", "api_host", "=", "args", ".", "api_host", "if", "args", ".", "api_key", ":", "solvebio", ".", "api_key", "=", "args", ".", "api_key", "if", "not", "solvebio", ".", "api_key", ":", "# If nothing is set (via command line or environment)", "# look in local credentials", "try", ":", "from", ".", "credentials", "import", "get_credentials", "solvebio", ".", "api_key", "=", "get_credentials", "(", ")", "except", ":", "pass", "# Update the client host and token", "client", ".", "set_host", "(", ")", "client", ".", "set_token", "(", ")", "return", "args", ".", "func", "(", "args", ")"], "docstring": "Main entry point for SolveBio CLI", "docstring_tokens": ["Main", "entry", "point", "for", "SolveBio", "CLI"], "sha": "b29614643043afd19c1d8074e8f25c6700d51a73", "url": "https://github.com/solvebio/solvebio-python/blob/b29614643043afd19c1d8074e8f25c6700d51a73/solvebio/cli/main.py#L285-L309", "partition": "test"}
{"repo": "agabrown/PyGaia", "path": "examples/plotPhotometricErrorsSkyAvg.py", "func_name": "makePlot", "original_string": "def makePlot(args):\n  \"\"\"\n  Make the plot with photometry performance predictions.\n\n  :argument args: command line arguments\n  \"\"\"\n  gmag=np.linspace(3.0,20.0,171)\n\n  vmini = args['vmini']\n  \n  vmag=gmag-gminvFromVmini(vmini)\n  \n  if args['eom']:\n      sigmaG = gMagnitudeErrorEoM(gmag)\n      sigmaGBp = bpMagnitudeErrorEoM(gmag, vmini)\n      sigmaGRp = rpMagnitudeErrorEoM(gmag, vmini)\n      yminmax = (1.0-4,0.1)\n  else:\n      sigmaG = gMagnitudeError(gmag)\n      sigmaGBp = bpMagnitudeError(gmag, vmini)\n      sigmaGRp = rpMagnitudeError(gmag, vmini)\n      yminmax = (1.0-4,1)\n\n  fig=plt.figure(figsize=(10,6.5))\n  \n  if (args['vmagAbscissa']):\n    plt.semilogy(vmag, sigmaG, 'k', label='$\\\\sigma_G$')\n    plt.semilogy(vmag, sigmaGBp, 'b', label='$\\\\sigma_{G_\\\\mathrm{BP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.semilogy(vmag, sigmaGRp, 'r', label='$\\\\sigma_{G_\\\\mathrm{RP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.xlim((6,20))\n    #plt.ylim(yminmax)\n    plt.legend(loc=0)\n    plt.xlabel('$V$ [mag]')\n  else:\n    ax=fig.add_subplot(111)\n    plt.semilogy(gmag, sigmaG, 'k', label='$\\\\sigma_G$')\n    plt.semilogy(gmag, sigmaGBp, 'b', label='$\\\\sigma_{G_\\\\mathrm{BP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.semilogy(gmag, sigmaGRp, 'r', label='$\\\\sigma_{G_\\\\mathrm{RP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.xlim((6,20))\n    #plt.ylim(yminmax)\n    plt.legend(loc=0)\n    plt.xlabel('$G$ [mag]')\n  \n  plt.xticks(np.arange(6,20,2))\n  ax = plt.gca().yaxis \n  #ax.set_major_formatter(matplotlib.ticker.ScalarFormatter())\n  #plt.ticklabel_format(axis='y',style='plain')\n  plt.grid(which='both')\n  plt.ylabel('Photometric error [mag]')\n  if args['eom']:\n      plt.title('End-of-mission mean photometry: sky averaged errors for $(V-I)={0}$'.format(vmini), fontsize=14)\n  else:\n      plt.title('Single-FoV-transit photometry: sky averaged errors for $(V-I)={0}$'.format(vmini), fontsize=14)\n  \n  basename = 'PhotometricErrors'\n  if (args['pdfOutput']):\n    plt.savefig(basename+'.pdf')\n  elif (args['pngOutput']):\n    plt.savefig(basename+'.png')\n  else:\n    plt.show()", "language": "python", "code": "def makePlot(args):\n  \"\"\"\n  Make the plot with photometry performance predictions.\n\n  :argument args: command line arguments\n  \"\"\"\n  gmag=np.linspace(3.0,20.0,171)\n\n  vmini = args['vmini']\n  \n  vmag=gmag-gminvFromVmini(vmini)\n  \n  if args['eom']:\n      sigmaG = gMagnitudeErrorEoM(gmag)\n      sigmaGBp = bpMagnitudeErrorEoM(gmag, vmini)\n      sigmaGRp = rpMagnitudeErrorEoM(gmag, vmini)\n      yminmax = (1.0-4,0.1)\n  else:\n      sigmaG = gMagnitudeError(gmag)\n      sigmaGBp = bpMagnitudeError(gmag, vmini)\n      sigmaGRp = rpMagnitudeError(gmag, vmini)\n      yminmax = (1.0-4,1)\n\n  fig=plt.figure(figsize=(10,6.5))\n  \n  if (args['vmagAbscissa']):\n    plt.semilogy(vmag, sigmaG, 'k', label='$\\\\sigma_G$')\n    plt.semilogy(vmag, sigmaGBp, 'b', label='$\\\\sigma_{G_\\\\mathrm{BP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.semilogy(vmag, sigmaGRp, 'r', label='$\\\\sigma_{G_\\\\mathrm{RP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.xlim((6,20))\n    #plt.ylim(yminmax)\n    plt.legend(loc=0)\n    plt.xlabel('$V$ [mag]')\n  else:\n    ax=fig.add_subplot(111)\n    plt.semilogy(gmag, sigmaG, 'k', label='$\\\\sigma_G$')\n    plt.semilogy(gmag, sigmaGBp, 'b', label='$\\\\sigma_{G_\\\\mathrm{BP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.semilogy(gmag, sigmaGRp, 'r', label='$\\\\sigma_{G_\\\\mathrm{RP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.xlim((6,20))\n    #plt.ylim(yminmax)\n    plt.legend(loc=0)\n    plt.xlabel('$G$ [mag]')\n  \n  plt.xticks(np.arange(6,20,2))\n  ax = plt.gca().yaxis \n  #ax.set_major_formatter(matplotlib.ticker.ScalarFormatter())\n  #plt.ticklabel_format(axis='y',style='plain')\n  plt.grid(which='both')\n  plt.ylabel('Photometric error [mag]')\n  if args['eom']:\n      plt.title('End-of-mission mean photometry: sky averaged errors for $(V-I)={0}$'.format(vmini), fontsize=14)\n  else:\n      plt.title('Single-FoV-transit photometry: sky averaged errors for $(V-I)={0}$'.format(vmini), fontsize=14)\n  \n  basename = 'PhotometricErrors'\n  if (args['pdfOutput']):\n    plt.savefig(basename+'.pdf')\n  elif (args['pngOutput']):\n    plt.savefig(basename+'.png')\n  else:\n    plt.show()", "code_tokens": ["def", "makePlot", "(", "args", ")", ":", "gmag", "=", "np", ".", "linspace", "(", "3.0", ",", "20.0", ",", "171", ")", "vmini", "=", "args", "[", "'vmini'", "]", "vmag", "=", "gmag", "-", "gminvFromVmini", "(", "vmini", ")", "if", "args", "[", "'eom'", "]", ":", "sigmaG", "=", "gMagnitudeErrorEoM", "(", "gmag", ")", "sigmaGBp", "=", "bpMagnitudeErrorEoM", "(", "gmag", ",", "vmini", ")", "sigmaGRp", "=", "rpMagnitudeErrorEoM", "(", "gmag", ",", "vmini", ")", "yminmax", "=", "(", "1.0", "-", "4", ",", "0.1", ")", "else", ":", "sigmaG", "=", "gMagnitudeError", "(", "gmag", ")", "sigmaGBp", "=", "bpMagnitudeError", "(", "gmag", ",", "vmini", ")", "sigmaGRp", "=", "rpMagnitudeError", "(", "gmag", ",", "vmini", ")", "yminmax", "=", "(", "1.0", "-", "4", ",", "1", ")", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "10", ",", "6.5", ")", ")", "if", "(", "args", "[", "'vmagAbscissa'", "]", ")", ":", "plt", ".", "semilogy", "(", "vmag", ",", "sigmaG", ",", "'k'", ",", "label", "=", "'$\\\\sigma_G$'", ")", "plt", ".", "semilogy", "(", "vmag", ",", "sigmaGBp", ",", "'b'", ",", "label", "=", "'$\\\\sigma_{G_\\\\mathrm{BP}}$'", "+", "' for $(V-I)={0}$'", ".", "format", "(", "vmini", ")", ")", "plt", ".", "semilogy", "(", "vmag", ",", "sigmaGRp", ",", "'r'", ",", "label", "=", "'$\\\\sigma_{G_\\\\mathrm{RP}}$'", "+", "' for $(V-I)={0}$'", ".", "format", "(", "vmini", ")", ")", "plt", ".", "xlim", "(", "(", "6", ",", "20", ")", ")", "#plt.ylim(yminmax)", "plt", ".", "legend", "(", "loc", "=", "0", ")", "plt", ".", "xlabel", "(", "'$V$ [mag]'", ")", "else", ":", "ax", "=", "fig", ".", "add_subplot", "(", "111", ")", "plt", ".", "semilogy", "(", "gmag", ",", "sigmaG", ",", "'k'", ",", "label", "=", "'$\\\\sigma_G$'", ")", "plt", ".", "semilogy", "(", "gmag", ",", "sigmaGBp", ",", "'b'", ",", "label", "=", "'$\\\\sigma_{G_\\\\mathrm{BP}}$'", "+", "' for $(V-I)={0}$'", ".", "format", "(", "vmini", ")", ")", "plt", ".", "semilogy", "(", "gmag", ",", "sigmaGRp", ",", "'r'", ",", "label", "=", "'$\\\\sigma_{G_\\\\mathrm{RP}}$'", "+", "' for $(V-I)={0}$'", ".", "format", "(", "vmini", ")", ")", "plt", ".", "xlim", "(", "(", "6", ",", "20", ")", ")", "#plt.ylim(yminmax)", "plt", ".", "legend", "(", "loc", "=", "0", ")", "plt", ".", "xlabel", "(", "'$G$ [mag]'", ")", "plt", ".", "xticks", "(", "np", ".", "arange", "(", "6", ",", "20", ",", "2", ")", ")", "ax", "=", "plt", ".", "gca", "(", ")", ".", "yaxis", "#ax.set_major_formatter(matplotlib.ticker.ScalarFormatter())", "#plt.ticklabel_format(axis='y',style='plain')", "plt", ".", "grid", "(", "which", "=", "'both'", ")", "plt", ".", "ylabel", "(", "'Photometric error [mag]'", ")", "if", "args", "[", "'eom'", "]", ":", "plt", ".", "title", "(", "'End-of-mission mean photometry: sky averaged errors for $(V-I)={0}$'", ".", "format", "(", "vmini", ")", ",", "fontsize", "=", "14", ")", "else", ":", "plt", ".", "title", "(", "'Single-FoV-transit photometry: sky averaged errors for $(V-I)={0}$'", ".", "format", "(", "vmini", ")", ",", "fontsize", "=", "14", ")", "basename", "=", "'PhotometricErrors'", "if", "(", "args", "[", "'pdfOutput'", "]", ")", ":", "plt", ".", "savefig", "(", "basename", "+", "'.pdf'", ")", "elif", "(", "args", "[", "'pngOutput'", "]", ")", ":", "plt", ".", "savefig", "(", "basename", "+", "'.png'", ")", "else", ":", "plt", ".", "show", "(", ")"], "docstring": "Make the plot with photometry performance predictions.\n\n  :argument args: command line arguments", "docstring_tokens": ["Make", "the", "plot", "with", "photometry", "performance", "predictions", "."], "sha": "ae972b0622a15f713ffae471f925eac25ccdae47", "url": "https://github.com/agabrown/PyGaia/blob/ae972b0622a15f713ffae471f925eac25ccdae47/examples/plotPhotometricErrorsSkyAvg.py#L31-L91", "partition": "test"}
{"repo": "celiao/tmdbsimple", "path": "tmdbsimple/tv.py", "func_name": "TV_Seasons.external_ids", "original_string": "def external_ids(self, **kwargs):\n        \"\"\"\n        Get the external ids that we have stored for a TV season by season\n        number.\n\n        Args:\n            language: (optional) ISO 639 code.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_series_id_season_number_path('external_ids')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response", "language": "python", "code": "def external_ids(self, **kwargs):\n        \"\"\"\n        Get the external ids that we have stored for a TV season by season\n        number.\n\n        Args:\n            language: (optional) ISO 639 code.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_series_id_season_number_path('external_ids')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response", "code_tokens": ["def", "external_ids", "(", "self", ",", "*", "*", "kwargs", ")", ":", "path", "=", "self", ".", "_get_series_id_season_number_path", "(", "'external_ids'", ")", "response", "=", "self", ".", "_GET", "(", "path", ",", "kwargs", ")", "self", ".", "_set_attrs_to_values", "(", "response", ")", "return", "response"], "docstring": "Get the external ids that we have stored for a TV season by season\n        number.\n\n        Args:\n            language: (optional) ISO 639 code.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.", "docstring_tokens": ["Get", "the", "external", "ids", "that", "we", "have", "stored", "for", "a", "TV", "season", "by", "season", "number", "."], "sha": "ff17893110c99771d6398a62c35d36dd9735f4b9", "url": "https://github.com/celiao/tmdbsimple/blob/ff17893110c99771d6398a62c35d36dd9735f4b9/tmdbsimple/tv.py#L386-L401", "partition": "test"}
{"repo": "HumanBrainProject/hbp-service-client", "path": "hbp_service_client/storage_service/api.py", "func_name": "ApiClient.get_metadata", "original_string": "def get_metadata(self, entity_type, entity_id):\n        '''Get metadata of an entity.\n\n        Args:\n            entity_type (str): Type of the entity. Admitted values: ['project',\n                'folder', 'file'].\n            entity_id (str): The UUID of the entity to be modified.\n\n        Returns:\n            A dictionary of the metadata::\n\n                {\n                    u'bar': u'200',\n                    u'foo': u'100'\n                }\n\n        Raises:\n            StorageArgumentException: Invalid arguments\n            StorageForbiddenException: Server response code 403\n            StorageNotFoundException: Server response code 404\n            StorageException: other 400-600 error codes\n        '''\n        if not is_valid_uuid(entity_id):\n            raise StorageArgumentException(\n                'Invalid UUID for entity_id: {0}'.format(entity_id))\n\n        return self._authenticated_request \\\n            .to_endpoint('{}/{}/metadata/'.format(entity_type, entity_id)) \\\n            .return_body() \\\n            .get()", "language": "python", "code": "def get_metadata(self, entity_type, entity_id):\n        '''Get metadata of an entity.\n\n        Args:\n            entity_type (str): Type of the entity. Admitted values: ['project',\n                'folder', 'file'].\n            entity_id (str): The UUID of the entity to be modified.\n\n        Returns:\n            A dictionary of the metadata::\n\n                {\n                    u'bar': u'200',\n                    u'foo': u'100'\n                }\n\n        Raises:\n            StorageArgumentException: Invalid arguments\n            StorageForbiddenException: Server response code 403\n            StorageNotFoundException: Server response code 404\n            StorageException: other 400-600 error codes\n        '''\n        if not is_valid_uuid(entity_id):\n            raise StorageArgumentException(\n                'Invalid UUID for entity_id: {0}'.format(entity_id))\n\n        return self._authenticated_request \\\n            .to_endpoint('{}/{}/metadata/'.format(entity_type, entity_id)) \\\n            .return_body() \\\n            .get()", "code_tokens": ["def", "get_metadata", "(", "self", ",", "entity_type", ",", "entity_id", ")", ":", "if", "not", "is_valid_uuid", "(", "entity_id", ")", ":", "raise", "StorageArgumentException", "(", "'Invalid UUID for entity_id: {0}'", ".", "format", "(", "entity_id", ")", ")", "return", "self", ".", "_authenticated_request", ".", "to_endpoint", "(", "'{}/{}/metadata/'", ".", "format", "(", "entity_type", ",", "entity_id", ")", ")", ".", "return_body", "(", ")", ".", "get", "(", ")"], "docstring": "Get metadata of an entity.\n\n        Args:\n            entity_type (str): Type of the entity. Admitted values: ['project',\n                'folder', 'file'].\n            entity_id (str): The UUID of the entity to be modified.\n\n        Returns:\n            A dictionary of the metadata::\n\n                {\n                    u'bar': u'200',\n                    u'foo': u'100'\n                }\n\n        Raises:\n            StorageArgumentException: Invalid arguments\n            StorageForbiddenException: Server response code 403\n            StorageNotFoundException: Server response code 404\n            StorageException: other 400-600 error codes", "docstring_tokens": ["Get", "metadata", "of", "an", "entity", "."], "sha": "b338fb41a7f0e7b9d654ff28fcf13a56d03bff4d", "url": "https://github.com/HumanBrainProject/hbp-service-client/blob/b338fb41a7f0e7b9d654ff28fcf13a56d03bff4d/hbp_service_client/storage_service/api.py#L271-L300", "partition": "test"}
{"repo": "mohabusama/pyguacamole", "path": "guacamole/client.py", "func_name": "GuacamoleClient.close", "original_string": "def close(self):\n        \"\"\"\n        Terminate connection with Guacamole guacd server.\n        \"\"\"\n        self.client.close()\n        self._client = None\n        self.connected = False\n        self.logger.debug('Connection closed.')", "language": "python", "code": "def close(self):\n        \"\"\"\n        Terminate connection with Guacamole guacd server.\n        \"\"\"\n        self.client.close()\n        self._client = None\n        self.connected = False\n        self.logger.debug('Connection closed.')", "code_tokens": ["def", "close", "(", "self", ")", ":", "self", ".", "client", ".", "close", "(", ")", "self", ".", "_client", "=", "None", "self", ".", "connected", "=", "False", "self", ".", "logger", ".", "debug", "(", "'Connection closed.'", ")"], "docstring": "Terminate connection with Guacamole guacd server.", "docstring_tokens": ["Terminate", "connection", "with", "Guacamole", "guacd", "server", "."], "sha": "344dccc6cb3a9a045afeaf337677e5d0001aa83a", "url": "https://github.com/mohabusama/pyguacamole/blob/344dccc6cb3a9a045afeaf337677e5d0001aa83a/guacamole/client.py#L82-L89", "partition": "test"}
{"repo": "yandex/yandex-tank", "path": "yandextank/api/apiworker.py", "func_name": "ApiWorker.__add_user_options", "original_string": "def __add_user_options(self):\n        \"\"\" override config options with user specified options\"\"\"\n        if self.options.get('user_options', None):\n            self.core.apply_shorthand_options(self.options['user_options'])", "language": "python", "code": "def __add_user_options(self):\n        \"\"\" override config options with user specified options\"\"\"\n        if self.options.get('user_options', None):\n            self.core.apply_shorthand_options(self.options['user_options'])", "code_tokens": ["def", "__add_user_options", "(", "self", ")", ":", "if", "self", ".", "options", ".", "get", "(", "'user_options'", ",", "None", ")", ":", "self", ".", "core", ".", "apply_shorthand_options", "(", "self", ".", "options", "[", "'user_options'", "]", ")"], "docstring": "override config options with user specified options", "docstring_tokens": ["override", "config", "options", "with", "user", "specified", "options"], "sha": "d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b", "url": "https://github.com/yandex/yandex-tank/blob/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b/yandextank/api/apiworker.py#L60-L63", "partition": "test"}
{"repo": "praekelt/django-analytics", "path": "analytics/geckoboard_views.py", "func_name": "geckoboard_line_chart", "original_string": "def geckoboard_line_chart(request):\n    \"\"\"\n    Returns the data for a line chart for the specified metric.\n    \"\"\"\n\n    params = get_gecko_params(request, cumulative=False, days_back=7)\n    metric = Metric.objects.get(uid=params['uid'])\n\n    start_date = datetime.now()-timedelta(days=params['days_back'])\n    stats = [s for s in metric.statistics.filter(frequency=params['frequency'],\n        date_time__gte=start_date).order_by('date_time')]\n\n    if len(stats) == 0:\n        raise Exception, _(\"No statistics for metric %(metric)s.\") % {'metric': params['uid']}\n\n    dates = [stats[0].date_time]\n\n    # get up to 3 dates from the stats\n    if len(stats) >= 3:\n        mid = len(stats)/2\n        if not mid:\n            mid = 1\n        dates.extend([stats[mid].date_time, stats[-1].date_time])\n    elif len(stats) == 2:\n        dates.extend([stats[-1].date_time])\n\n    return (\n        [s.count for s in stats],\n        dates,\n        metric.title,\n    )", "language": "python", "code": "def geckoboard_line_chart(request):\n    \"\"\"\n    Returns the data for a line chart for the specified metric.\n    \"\"\"\n\n    params = get_gecko_params(request, cumulative=False, days_back=7)\n    metric = Metric.objects.get(uid=params['uid'])\n\n    start_date = datetime.now()-timedelta(days=params['days_back'])\n    stats = [s for s in metric.statistics.filter(frequency=params['frequency'],\n        date_time__gte=start_date).order_by('date_time')]\n\n    if len(stats) == 0:\n        raise Exception, _(\"No statistics for metric %(metric)s.\") % {'metric': params['uid']}\n\n    dates = [stats[0].date_time]\n\n    # get up to 3 dates from the stats\n    if len(stats) >= 3:\n        mid = len(stats)/2\n        if not mid:\n            mid = 1\n        dates.extend([stats[mid].date_time, stats[-1].date_time])\n    elif len(stats) == 2:\n        dates.extend([stats[-1].date_time])\n\n    return (\n        [s.count for s in stats],\n        dates,\n        metric.title,\n    )", "code_tokens": ["def", "geckoboard_line_chart", "(", "request", ")", ":", "params", "=", "get_gecko_params", "(", "request", ",", "cumulative", "=", "False", ",", "days_back", "=", "7", ")", "metric", "=", "Metric", ".", "objects", ".", "get", "(", "uid", "=", "params", "[", "'uid'", "]", ")", "start_date", "=", "datetime", ".", "now", "(", ")", "-", "timedelta", "(", "days", "=", "params", "[", "'days_back'", "]", ")", "stats", "=", "[", "s", "for", "s", "in", "metric", ".", "statistics", ".", "filter", "(", "frequency", "=", "params", "[", "'frequency'", "]", ",", "date_time__gte", "=", "start_date", ")", ".", "order_by", "(", "'date_time'", ")", "]", "if", "len", "(", "stats", ")", "==", "0", ":", "raise", "Exception", ",", "_", "(", "\"No statistics for metric %(metric)s.\"", ")", "%", "{", "'metric'", ":", "params", "[", "'uid'", "]", "}", "dates", "=", "[", "stats", "[", "0", "]", ".", "date_time", "]", "# get up to 3 dates from the stats", "if", "len", "(", "stats", ")", ">=", "3", ":", "mid", "=", "len", "(", "stats", ")", "/", "2", "if", "not", "mid", ":", "mid", "=", "1", "dates", ".", "extend", "(", "[", "stats", "[", "mid", "]", ".", "date_time", ",", "stats", "[", "-", "1", "]", ".", "date_time", "]", ")", "elif", "len", "(", "stats", ")", "==", "2", ":", "dates", ".", "extend", "(", "[", "stats", "[", "-", "1", "]", ".", "date_time", "]", ")", "return", "(", "[", "s", ".", "count", "for", "s", "in", "stats", "]", ",", "dates", ",", "metric", ".", "title", ",", ")"], "docstring": "Returns the data for a line chart for the specified metric.", "docstring_tokens": ["Returns", "the", "data", "for", "a", "line", "chart", "for", "the", "specified", "metric", "."], "sha": "29c22d03374ccc0ec451650e2c2886d324f6e5c6", "url": "https://github.com/praekelt/django-analytics/blob/29c22d03374ccc0ec451650e2c2886d324f6e5c6/analytics/geckoboard_views.py#L149-L179", "partition": "test"}
{"repo": "scheibler/khard", "path": "khard/carddav_object.py", "func_name": "CarddavObject.new_contact", "original_string": "def new_contact(cls, address_book, supported_private_objects, version,\n            localize_dates):\n        \"\"\"Use this to create a new and empty contact.\"\"\"\n        return cls(address_book, None, supported_private_objects, version,\n                localize_dates)", "language": "python", "code": "def new_contact(cls, address_book, supported_private_objects, version,\n            localize_dates):\n        \"\"\"Use this to create a new and empty contact.\"\"\"\n        return cls(address_book, None, supported_private_objects, version,\n                localize_dates)", "code_tokens": ["def", "new_contact", "(", "cls", ",", "address_book", ",", "supported_private_objects", ",", "version", ",", "localize_dates", ")", ":", "return", "cls", "(", "address_book", ",", "None", ",", "supported_private_objects", ",", "version", ",", "localize_dates", ")"], "docstring": "Use this to create a new and empty contact.", "docstring_tokens": ["Use", "this", "to", "create", "a", "new", "and", "empty", "contact", "."], "sha": "0f69430c2680f1ff5f073a977a3c5b753b96cc17", "url": "https://github.com/scheibler/khard/blob/0f69430c2680f1ff5f073a977a3c5b753b96cc17/khard/carddav_object.py#L91-L95", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/reporters/ureports/nodes.py", "func_name": "BaseLayout.append", "original_string": "def append(self, child):\n        \"\"\"overridden to detect problems easily\"\"\"\n        assert child not in self.parents()\n        VNode.append(self, child)", "language": "python", "code": "def append(self, child):\n        \"\"\"overridden to detect problems easily\"\"\"\n        assert child not in self.parents()\n        VNode.append(self, child)", "code_tokens": ["def", "append", "(", "self", ",", "child", ")", ":", "assert", "child", "not", "in", "self", ".", "parents", "(", ")", "VNode", ".", "append", "(", "self", ",", "child", ")"], "docstring": "overridden to detect problems easily", "docstring_tokens": ["overridden", "to", "detect", "problems", "easily"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/reporters/ureports/nodes.py#L72-L75", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/tools/qi/qi.py", "func_name": "vectorize", "original_string": "def vectorize(density_matrix, method='col'):\n    \"\"\"Flatten an operator to a vector in a specified basis.\n\n    Args:\n        density_matrix (ndarray): a density matrix.\n        method (str): the method of vectorization. Allowed values are\n            - 'col' (default) flattens to column-major vector.\n            - 'row' flattens to row-major vector.\n            - 'pauli'flattens in the n-qubit Pauli basis.\n            - 'pauli-weights': flattens in the n-qubit Pauli basis ordered by\n               weight.\n\n    Returns:\n        ndarray: the resulting vector.\n    Raises:\n        Exception: if input state is not a n-qubit state\n    \"\"\"\n    density_matrix = np.array(density_matrix)\n    if method == 'col':\n        return density_matrix.flatten(order='F')\n    elif method == 'row':\n        return density_matrix.flatten(order='C')\n    elif method in ['pauli', 'pauli_weights']:\n        num = int(np.log2(len(density_matrix)))  # number of qubits\n        if len(density_matrix) != 2**num:\n            raise Exception('Input state must be n-qubit state')\n        if method == 'pauli_weights':\n            pgroup = pauli_group(num, case='weight')\n        else:\n            pgroup = pauli_group(num, case='tensor')\n        vals = [np.trace(np.dot(p.to_matrix(), density_matrix))\n                for p in pgroup]\n        return np.array(vals)\n    return None", "language": "python", "code": "def vectorize(density_matrix, method='col'):\n    \"\"\"Flatten an operator to a vector in a specified basis.\n\n    Args:\n        density_matrix (ndarray): a density matrix.\n        method (str): the method of vectorization. Allowed values are\n            - 'col' (default) flattens to column-major vector.\n            - 'row' flattens to row-major vector.\n            - 'pauli'flattens in the n-qubit Pauli basis.\n            - 'pauli-weights': flattens in the n-qubit Pauli basis ordered by\n               weight.\n\n    Returns:\n        ndarray: the resulting vector.\n    Raises:\n        Exception: if input state is not a n-qubit state\n    \"\"\"\n    density_matrix = np.array(density_matrix)\n    if method == 'col':\n        return density_matrix.flatten(order='F')\n    elif method == 'row':\n        return density_matrix.flatten(order='C')\n    elif method in ['pauli', 'pauli_weights']:\n        num = int(np.log2(len(density_matrix)))  # number of qubits\n        if len(density_matrix) != 2**num:\n            raise Exception('Input state must be n-qubit state')\n        if method == 'pauli_weights':\n            pgroup = pauli_group(num, case='weight')\n        else:\n            pgroup = pauli_group(num, case='tensor')\n        vals = [np.trace(np.dot(p.to_matrix(), density_matrix))\n                for p in pgroup]\n        return np.array(vals)\n    return None", "code_tokens": ["def", "vectorize", "(", "density_matrix", ",", "method", "=", "'col'", ")", ":", "density_matrix", "=", "np", ".", "array", "(", "density_matrix", ")", "if", "method", "==", "'col'", ":", "return", "density_matrix", ".", "flatten", "(", "order", "=", "'F'", ")", "elif", "method", "==", "'row'", ":", "return", "density_matrix", ".", "flatten", "(", "order", "=", "'C'", ")", "elif", "method", "in", "[", "'pauli'", ",", "'pauli_weights'", "]", ":", "num", "=", "int", "(", "np", ".", "log2", "(", "len", "(", "density_matrix", ")", ")", ")", "# number of qubits", "if", "len", "(", "density_matrix", ")", "!=", "2", "**", "num", ":", "raise", "Exception", "(", "'Input state must be n-qubit state'", ")", "if", "method", "==", "'pauli_weights'", ":", "pgroup", "=", "pauli_group", "(", "num", ",", "case", "=", "'weight'", ")", "else", ":", "pgroup", "=", "pauli_group", "(", "num", ",", "case", "=", "'tensor'", ")", "vals", "=", "[", "np", ".", "trace", "(", "np", ".", "dot", "(", "p", ".", "to_matrix", "(", ")", ",", "density_matrix", ")", ")", "for", "p", "in", "pgroup", "]", "return", "np", ".", "array", "(", "vals", ")", "return", "None"], "docstring": "Flatten an operator to a vector in a specified basis.\n\n    Args:\n        density_matrix (ndarray): a density matrix.\n        method (str): the method of vectorization. Allowed values are\n            - 'col' (default) flattens to column-major vector.\n            - 'row' flattens to row-major vector.\n            - 'pauli'flattens in the n-qubit Pauli basis.\n            - 'pauli-weights': flattens in the n-qubit Pauli basis ordered by\n               weight.\n\n    Returns:\n        ndarray: the resulting vector.\n    Raises:\n        Exception: if input state is not a n-qubit state", "docstring_tokens": ["Flatten", "an", "operator", "to", "a", "vector", "in", "a", "specified", "basis", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/tools/qi/qi.py#L174-L207", "partition": "test"}
{"repo": "elliterate/capybara.py", "path": "capybara/session.py", "func_name": "Session.switch_to_frame", "original_string": "def switch_to_frame(self, frame):\n        \"\"\"\n        Switch to the given frame.\n\n        If you use this method you are responsible for making sure you switch back to the parent\n        frame when done in the frame changed to. :meth:`frame` is preferred over this method and\n        should be used when possible. May not be supported by all drivers.\n\n        Args:\n            frame (Element | str): The iframe/frame element to switch to.\n        \"\"\"\n\n        if isinstance(frame, Element):\n            self.driver.switch_to_frame(frame)\n            self._scopes.append(\"frame\")\n        elif frame == \"parent\":\n            if self._scopes[-1] != \"frame\":\n                raise ScopeError(\"`switch_to_frame(\\\"parent\\\")` cannot be called \"\n                                 \"from inside a descendant frame's `scope` context.\")\n            self._scopes.pop()\n            self.driver.switch_to_frame(\"parent\")\n        elif frame == \"top\":\n            if \"frame\" in self._scopes:\n                idx = self._scopes.index(\"frame\")\n                if any([scope not in [\"frame\", None] for scope in self._scopes[idx:]]):\n                    raise ScopeError(\"`switch_to_frame(\\\"top\\\")` cannot be called \"\n                                     \"from inside a descendant frame's `scope` context.\")\n                self._scopes = self._scopes[:idx]\n                self.driver.switch_to_frame(\"top\")\n        else:\n            raise ValueError(\n                \"You must provide a frame element, \\\"parent\\\", or \\\"top\\\" \"\n                \"when calling switch_to_frame\")", "language": "python", "code": "def switch_to_frame(self, frame):\n        \"\"\"\n        Switch to the given frame.\n\n        If you use this method you are responsible for making sure you switch back to the parent\n        frame when done in the frame changed to. :meth:`frame` is preferred over this method and\n        should be used when possible. May not be supported by all drivers.\n\n        Args:\n            frame (Element | str): The iframe/frame element to switch to.\n        \"\"\"\n\n        if isinstance(frame, Element):\n            self.driver.switch_to_frame(frame)\n            self._scopes.append(\"frame\")\n        elif frame == \"parent\":\n            if self._scopes[-1] != \"frame\":\n                raise ScopeError(\"`switch_to_frame(\\\"parent\\\")` cannot be called \"\n                                 \"from inside a descendant frame's `scope` context.\")\n            self._scopes.pop()\n            self.driver.switch_to_frame(\"parent\")\n        elif frame == \"top\":\n            if \"frame\" in self._scopes:\n                idx = self._scopes.index(\"frame\")\n                if any([scope not in [\"frame\", None] for scope in self._scopes[idx:]]):\n                    raise ScopeError(\"`switch_to_frame(\\\"top\\\")` cannot be called \"\n                                     \"from inside a descendant frame's `scope` context.\")\n                self._scopes = self._scopes[:idx]\n                self.driver.switch_to_frame(\"top\")\n        else:\n            raise ValueError(\n                \"You must provide a frame element, \\\"parent\\\", or \\\"top\\\" \"\n                \"when calling switch_to_frame\")", "code_tokens": ["def", "switch_to_frame", "(", "self", ",", "frame", ")", ":", "if", "isinstance", "(", "frame", ",", "Element", ")", ":", "self", ".", "driver", ".", "switch_to_frame", "(", "frame", ")", "self", ".", "_scopes", ".", "append", "(", "\"frame\"", ")", "elif", "frame", "==", "\"parent\"", ":", "if", "self", ".", "_scopes", "[", "-", "1", "]", "!=", "\"frame\"", ":", "raise", "ScopeError", "(", "\"`switch_to_frame(\\\"parent\\\")` cannot be called \"", "\"from inside a descendant frame's `scope` context.\"", ")", "self", ".", "_scopes", ".", "pop", "(", ")", "self", ".", "driver", ".", "switch_to_frame", "(", "\"parent\"", ")", "elif", "frame", "==", "\"top\"", ":", "if", "\"frame\"", "in", "self", ".", "_scopes", ":", "idx", "=", "self", ".", "_scopes", ".", "index", "(", "\"frame\"", ")", "if", "any", "(", "[", "scope", "not", "in", "[", "\"frame\"", ",", "None", "]", "for", "scope", "in", "self", ".", "_scopes", "[", "idx", ":", "]", "]", ")", ":", "raise", "ScopeError", "(", "\"`switch_to_frame(\\\"top\\\")` cannot be called \"", "\"from inside a descendant frame's `scope` context.\"", ")", "self", ".", "_scopes", "=", "self", ".", "_scopes", "[", ":", "idx", "]", "self", ".", "driver", ".", "switch_to_frame", "(", "\"top\"", ")", "else", ":", "raise", "ValueError", "(", "\"You must provide a frame element, \\\"parent\\\", or \\\"top\\\" \"", "\"when calling switch_to_frame\"", ")"], "docstring": "Switch to the given frame.\n\n        If you use this method you are responsible for making sure you switch back to the parent\n        frame when done in the frame changed to. :meth:`frame` is preferred over this method and\n        should be used when possible. May not be supported by all drivers.\n\n        Args:\n            frame (Element | str): The iframe/frame element to switch to.", "docstring_tokens": ["Switch", "to", "the", "given", "frame", "."], "sha": "0c6ae449cc37e4445ec3cd6af95674533beedc6c", "url": "https://github.com/elliterate/capybara.py/blob/0c6ae449cc37e4445ec3cd6af95674533beedc6c/capybara/session.py#L308-L340", "partition": "test"}
{"repo": "streamlink/streamlink", "path": "src/streamlink/plugin/api/validate.py", "func_name": "xml_findall", "original_string": "def xml_findall(xpath):\n    \"\"\"Find a list of XML elements via xpath.\"\"\"\n    def xpath_findall(value):\n        validate(ET.iselement, value)\n        return value.findall(xpath)\n\n    return transform(xpath_findall)", "language": "python", "code": "def xml_findall(xpath):\n    \"\"\"Find a list of XML elements via xpath.\"\"\"\n    def xpath_findall(value):\n        validate(ET.iselement, value)\n        return value.findall(xpath)\n\n    return transform(xpath_findall)", "code_tokens": ["def", "xml_findall", "(", "xpath", ")", ":", "def", "xpath_findall", "(", "value", ")", ":", "validate", "(", "ET", ".", "iselement", ",", "value", ")", "return", "value", ".", "findall", "(", "xpath", ")", "return", "transform", "(", "xpath_findall", ")"], "docstring": "Find a list of XML elements via xpath.", "docstring_tokens": ["Find", "a", "list", "of", "XML", "elements", "via", "xpath", "."], "sha": "c8ed1daff14ac03195870238b9b900c1109dd5c1", "url": "https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink/plugin/api/validate.py#L287-L293", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/pie/factory.py", "func_name": "ObjectFactory.convert", "original_string": "def convert(self, obj):\n        \"\"\"\n        Convert a Pie object into a core secret object and vice versa.\n\n        Args:\n            obj (various): A Pie or core secret object to convert into the\n                opposite object space. Required.\n\n        Raises:\n            TypeError: if the object type is unrecognized or unsupported.\n        \"\"\"\n        if isinstance(obj, pobjects.SymmetricKey):\n            return self._build_core_key(obj, secrets.SymmetricKey)\n        elif isinstance(obj, secrets.SymmetricKey):\n            return self._build_pie_key(obj, pobjects.SymmetricKey)\n        elif isinstance(obj, pobjects.PublicKey):\n            return self._build_core_key(obj, secrets.PublicKey)\n        elif isinstance(obj, secrets.PublicKey):\n            return self._build_pie_key(obj, pobjects.PublicKey)\n        elif isinstance(obj, pobjects.PrivateKey):\n            return self._build_core_key(obj, secrets.PrivateKey)\n        elif isinstance(obj, secrets.PrivateKey):\n            return self._build_pie_key(obj, pobjects.PrivateKey)\n        elif isinstance(obj, pobjects.Certificate):\n            return self._build_core_certificate(obj)\n        elif isinstance(obj, secrets.Certificate):\n            return self._build_pie_certificate(obj)\n        elif isinstance(obj, pobjects.SecretData):\n            return self._build_core_secret_data(obj)\n        elif isinstance(obj, secrets.SecretData):\n            return self._build_pie_secret_data(obj)\n        elif isinstance(obj, pobjects.OpaqueObject):\n            return self._build_core_opaque_object(obj)\n        elif isinstance(obj, secrets.OpaqueObject):\n            return self._build_pie_opaque_object(obj)\n        else:\n            raise TypeError(\"object type unsupported and cannot be converted\")", "language": "python", "code": "def convert(self, obj):\n        \"\"\"\n        Convert a Pie object into a core secret object and vice versa.\n\n        Args:\n            obj (various): A Pie or core secret object to convert into the\n                opposite object space. Required.\n\n        Raises:\n            TypeError: if the object type is unrecognized or unsupported.\n        \"\"\"\n        if isinstance(obj, pobjects.SymmetricKey):\n            return self._build_core_key(obj, secrets.SymmetricKey)\n        elif isinstance(obj, secrets.SymmetricKey):\n            return self._build_pie_key(obj, pobjects.SymmetricKey)\n        elif isinstance(obj, pobjects.PublicKey):\n            return self._build_core_key(obj, secrets.PublicKey)\n        elif isinstance(obj, secrets.PublicKey):\n            return self._build_pie_key(obj, pobjects.PublicKey)\n        elif isinstance(obj, pobjects.PrivateKey):\n            return self._build_core_key(obj, secrets.PrivateKey)\n        elif isinstance(obj, secrets.PrivateKey):\n            return self._build_pie_key(obj, pobjects.PrivateKey)\n        elif isinstance(obj, pobjects.Certificate):\n            return self._build_core_certificate(obj)\n        elif isinstance(obj, secrets.Certificate):\n            return self._build_pie_certificate(obj)\n        elif isinstance(obj, pobjects.SecretData):\n            return self._build_core_secret_data(obj)\n        elif isinstance(obj, secrets.SecretData):\n            return self._build_pie_secret_data(obj)\n        elif isinstance(obj, pobjects.OpaqueObject):\n            return self._build_core_opaque_object(obj)\n        elif isinstance(obj, secrets.OpaqueObject):\n            return self._build_pie_opaque_object(obj)\n        else:\n            raise TypeError(\"object type unsupported and cannot be converted\")", "code_tokens": ["def", "convert", "(", "self", ",", "obj", ")", ":", "if", "isinstance", "(", "obj", ",", "pobjects", ".", "SymmetricKey", ")", ":", "return", "self", ".", "_build_core_key", "(", "obj", ",", "secrets", ".", "SymmetricKey", ")", "elif", "isinstance", "(", "obj", ",", "secrets", ".", "SymmetricKey", ")", ":", "return", "self", ".", "_build_pie_key", "(", "obj", ",", "pobjects", ".", "SymmetricKey", ")", "elif", "isinstance", "(", "obj", ",", "pobjects", ".", "PublicKey", ")", ":", "return", "self", ".", "_build_core_key", "(", "obj", ",", "secrets", ".", "PublicKey", ")", "elif", "isinstance", "(", "obj", ",", "secrets", ".", "PublicKey", ")", ":", "return", "self", ".", "_build_pie_key", "(", "obj", ",", "pobjects", ".", "PublicKey", ")", "elif", "isinstance", "(", "obj", ",", "pobjects", ".", "PrivateKey", ")", ":", "return", "self", ".", "_build_core_key", "(", "obj", ",", "secrets", ".", "PrivateKey", ")", "elif", "isinstance", "(", "obj", ",", "secrets", ".", "PrivateKey", ")", ":", "return", "self", ".", "_build_pie_key", "(", "obj", ",", "pobjects", ".", "PrivateKey", ")", "elif", "isinstance", "(", "obj", ",", "pobjects", ".", "Certificate", ")", ":", "return", "self", ".", "_build_core_certificate", "(", "obj", ")", "elif", "isinstance", "(", "obj", ",", "secrets", ".", "Certificate", ")", ":", "return", "self", ".", "_build_pie_certificate", "(", "obj", ")", "elif", "isinstance", "(", "obj", ",", "pobjects", ".", "SecretData", ")", ":", "return", "self", ".", "_build_core_secret_data", "(", "obj", ")", "elif", "isinstance", "(", "obj", ",", "secrets", ".", "SecretData", ")", ":", "return", "self", ".", "_build_pie_secret_data", "(", "obj", ")", "elif", "isinstance", "(", "obj", ",", "pobjects", ".", "OpaqueObject", ")", ":", "return", "self", ".", "_build_core_opaque_object", "(", "obj", ")", "elif", "isinstance", "(", "obj", ",", "secrets", ".", "OpaqueObject", ")", ":", "return", "self", ".", "_build_pie_opaque_object", "(", "obj", ")", "else", ":", "raise", "TypeError", "(", "\"object type unsupported and cannot be converted\"", ")"], "docstring": "Convert a Pie object into a core secret object and vice versa.\n\n        Args:\n            obj (various): A Pie or core secret object to convert into the\n                opposite object space. Required.\n\n        Raises:\n            TypeError: if the object type is unrecognized or unsupported.", "docstring_tokens": ["Convert", "a", "Pie", "object", "into", "a", "core", "secret", "object", "and", "vice", "versa", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/pie/factory.py#L36-L72", "partition": "test"}
{"repo": "rocky/python3-trepan", "path": "trepan/lib/stack.py", "func_name": "print_stack_trace", "original_string": "def print_stack_trace(proc_obj, count=None, color='plain', opts={}):\n    \"Print count entries of the stack trace\"\n    if count is None:\n        n=len(proc_obj.stack)\n    else:\n        n=min(len(proc_obj.stack), count)\n    try:\n        for i in range(n):\n            print_stack_entry(proc_obj, i, color=color, opts=opts)\n    except KeyboardInterrupt:\n        pass\n    return", "language": "python", "code": "def print_stack_trace(proc_obj, count=None, color='plain', opts={}):\n    \"Print count entries of the stack trace\"\n    if count is None:\n        n=len(proc_obj.stack)\n    else:\n        n=min(len(proc_obj.stack), count)\n    try:\n        for i in range(n):\n            print_stack_entry(proc_obj, i, color=color, opts=opts)\n    except KeyboardInterrupt:\n        pass\n    return", "code_tokens": ["def", "print_stack_trace", "(", "proc_obj", ",", "count", "=", "None", ",", "color", "=", "'plain'", ",", "opts", "=", "{", "}", ")", ":", "if", "count", "is", "None", ":", "n", "=", "len", "(", "proc_obj", ".", "stack", ")", "else", ":", "n", "=", "min", "(", "len", "(", "proc_obj", ".", "stack", ")", ",", "count", ")", "try", ":", "for", "i", "in", "range", "(", "n", ")", ":", "print_stack_entry", "(", "proc_obj", ",", "i", ",", "color", "=", "color", ",", "opts", "=", "opts", ")", "except", "KeyboardInterrupt", ":", "pass", "return"], "docstring": "Print count entries of the stack trace", "docstring_tokens": ["Print", "count", "entries", "of", "the", "stack", "trace"], "sha": "14e91bc0acce090d67be145b1ac040cab92ac5f3", "url": "https://github.com/rocky/python3-trepan/blob/14e91bc0acce090d67be145b1ac040cab92ac5f3/trepan/lib/stack.py#L282-L293", "partition": "test"}
{"repo": "sbneto/s3conf", "path": "s3conf/client.py", "func_name": "upload", "original_string": "def upload(remote_path, local_path):\n    \"\"\"\n    Upload a file or folder to the S3-like service.\n\n    If LOCAL_PATH is a folder, the files and subfolder structure in LOCAL_PATH are copied to REMOTE_PATH.\n\n    If LOCAL_PATH is a file, the REMOTE_PATH file is created with the same contents.\n    \"\"\"\n    storage = STORAGES['s3']()\n    conf = s3conf.S3Conf(storage=storage)\n    conf.upload(local_path, remote_path)", "language": "python", "code": "def upload(remote_path, local_path):\n    \"\"\"\n    Upload a file or folder to the S3-like service.\n\n    If LOCAL_PATH is a folder, the files and subfolder structure in LOCAL_PATH are copied to REMOTE_PATH.\n\n    If LOCAL_PATH is a file, the REMOTE_PATH file is created with the same contents.\n    \"\"\"\n    storage = STORAGES['s3']()\n    conf = s3conf.S3Conf(storage=storage)\n    conf.upload(local_path, remote_path)", "code_tokens": ["def", "upload", "(", "remote_path", ",", "local_path", ")", ":", "storage", "=", "STORAGES", "[", "'s3'", "]", "(", ")", "conf", "=", "s3conf", ".", "S3Conf", "(", "storage", "=", "storage", ")", "conf", ".", "upload", "(", "local_path", ",", "remote_path", ")"], "docstring": "Upload a file or folder to the S3-like service.\n\n    If LOCAL_PATH is a folder, the files and subfolder structure in LOCAL_PATH are copied to REMOTE_PATH.\n\n    If LOCAL_PATH is a file, the REMOTE_PATH file is created with the same contents.", "docstring_tokens": ["Upload", "a", "file", "or", "folder", "to", "the", "S3", "-", "like", "service", "."], "sha": "92fd2973beccc85bb21d3157ff227929e62ed695", "url": "https://github.com/sbneto/s3conf/blob/92fd2973beccc85bb21d3157ff227929e62ed695/s3conf/client.py#L196-L206", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/trajectory.py", "func_name": "Trajectory._is_completed", "original_string": "def _is_completed(self, name_or_id=None):\n        \"\"\"Private function such that it can still be called by the environment during\n        a single run\"\"\"\n\n        if name_or_id is None:\n            return all(\n                (runinfo['completed'] for runinfo in self._run_information.values()))\n        else:\n            return self.f_get_run_information(name_or_id, copy=False)['completed']", "language": "python", "code": "def _is_completed(self, name_or_id=None):\n        \"\"\"Private function such that it can still be called by the environment during\n        a single run\"\"\"\n\n        if name_or_id is None:\n            return all(\n                (runinfo['completed'] for runinfo in self._run_information.values()))\n        else:\n            return self.f_get_run_information(name_or_id, copy=False)['completed']", "code_tokens": ["def", "_is_completed", "(", "self", ",", "name_or_id", "=", "None", ")", ":", "if", "name_or_id", "is", "None", ":", "return", "all", "(", "(", "runinfo", "[", "'completed'", "]", "for", "runinfo", "in", "self", ".", "_run_information", ".", "values", "(", ")", ")", ")", "else", ":", "return", "self", ".", "f_get_run_information", "(", "name_or_id", ",", "copy", "=", "False", ")", "[", "'completed'", "]"], "docstring": "Private function such that it can still be called by the environment during\n        a single run", "docstring_tokens": ["Private", "function", "such", "that", "it", "can", "still", "be", "called", "by", "the", "environment", "during", "a", "single", "run"], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/trajectory.py#L957-L965", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/edward2/random_variable.py", "func_name": "RandomVariable.eval", "original_string": "def eval(self, session=None, feed_dict=None):\n    \"\"\"In a session, computes and returns the value of this random variable.\n\n    This is not a graph construction method, it does not add ops to the graph.\n\n    This convenience method requires a session where the graph\n    containing this variable has been launched. If no session is\n    passed, the default session is used.\n\n    Args:\n      session: tf.BaseSession.\n        The `tf.Session` to use to evaluate this random variable. If\n        none, the default session is used.\n      feed_dict: dict.\n        A dictionary that maps `tf.Tensor` objects to feed values. See\n        `tf.Session.run()` for a description of the valid feed values.\n\n    Returns:\n      Value of the random variable.\n\n    #### Examples\n\n    ```python\n    x = Normal(0.0, 1.0)\n    with tf.Session() as sess:\n      # Usage passing the session explicitly.\n      print(x.eval(sess))\n      # Usage with the default session.  The 'with' block\n      # above makes 'sess' the default session.\n      print(x.eval())\n    ```\n    \"\"\"\n    return self.value.eval(session=session, feed_dict=feed_dict)", "language": "python", "code": "def eval(self, session=None, feed_dict=None):\n    \"\"\"In a session, computes and returns the value of this random variable.\n\n    This is not a graph construction method, it does not add ops to the graph.\n\n    This convenience method requires a session where the graph\n    containing this variable has been launched. If no session is\n    passed, the default session is used.\n\n    Args:\n      session: tf.BaseSession.\n        The `tf.Session` to use to evaluate this random variable. If\n        none, the default session is used.\n      feed_dict: dict.\n        A dictionary that maps `tf.Tensor` objects to feed values. See\n        `tf.Session.run()` for a description of the valid feed values.\n\n    Returns:\n      Value of the random variable.\n\n    #### Examples\n\n    ```python\n    x = Normal(0.0, 1.0)\n    with tf.Session() as sess:\n      # Usage passing the session explicitly.\n      print(x.eval(sess))\n      # Usage with the default session.  The 'with' block\n      # above makes 'sess' the default session.\n      print(x.eval())\n    ```\n    \"\"\"\n    return self.value.eval(session=session, feed_dict=feed_dict)", "code_tokens": ["def", "eval", "(", "self", ",", "session", "=", "None", ",", "feed_dict", "=", "None", ")", ":", "return", "self", ".", "value", ".", "eval", "(", "session", "=", "session", ",", "feed_dict", "=", "feed_dict", ")"], "docstring": "In a session, computes and returns the value of this random variable.\n\n    This is not a graph construction method, it does not add ops to the graph.\n\n    This convenience method requires a session where the graph\n    containing this variable has been launched. If no session is\n    passed, the default session is used.\n\n    Args:\n      session: tf.BaseSession.\n        The `tf.Session` to use to evaluate this random variable. If\n        none, the default session is used.\n      feed_dict: dict.\n        A dictionary that maps `tf.Tensor` objects to feed values. See\n        `tf.Session.run()` for a description of the valid feed values.\n\n    Returns:\n      Value of the random variable.\n\n    #### Examples\n\n    ```python\n    x = Normal(0.0, 1.0)\n    with tf.Session() as sess:\n      # Usage passing the session explicitly.\n      print(x.eval(sess))\n      # Usage with the default session.  The 'with' block\n      # above makes 'sess' the default session.\n      print(x.eval())\n    ```", "docstring_tokens": ["In", "a", "session", "computes", "and", "returns", "the", "value", "of", "this", "random", "variable", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/random_variable.py#L236-L268", "partition": "test"}
{"repo": "zinic/pynsive", "path": "pynsive/plugin/loader.py", "func_name": "ModuleLoader.load_module", "original_string": "def load_module(self, module_name):\n        \"\"\"\n        Loads a module's code and sets the module's expected hidden\n        variables. For more information on these variables and what they\n        are for, please see PEP302.\n\n        :param module_name: the full name of the module to load\n        \"\"\"\n        if module_name != self.module_name:\n            raise LoaderError(\n                'Requesting a module that the loader is unaware of.')\n\n        if module_name in sys.modules:\n            return sys.modules[module_name]\n\n        module = self.load_module_py_path(module_name, self.load_target)\n        if self.is_pkg:\n            module.__path__ = [self.module_path]\n            module.__package__ = module_name\n        else:\n            module.__package__ = module_name.rpartition('.')[0]\n\n        sys.modules[module_name] = module\n        return module", "language": "python", "code": "def load_module(self, module_name):\n        \"\"\"\n        Loads a module's code and sets the module's expected hidden\n        variables. For more information on these variables and what they\n        are for, please see PEP302.\n\n        :param module_name: the full name of the module to load\n        \"\"\"\n        if module_name != self.module_name:\n            raise LoaderError(\n                'Requesting a module that the loader is unaware of.')\n\n        if module_name in sys.modules:\n            return sys.modules[module_name]\n\n        module = self.load_module_py_path(module_name, self.load_target)\n        if self.is_pkg:\n            module.__path__ = [self.module_path]\n            module.__package__ = module_name\n        else:\n            module.__package__ = module_name.rpartition('.')[0]\n\n        sys.modules[module_name] = module\n        return module", "code_tokens": ["def", "load_module", "(", "self", ",", "module_name", ")", ":", "if", "module_name", "!=", "self", ".", "module_name", ":", "raise", "LoaderError", "(", "'Requesting a module that the loader is unaware of.'", ")", "if", "module_name", "in", "sys", ".", "modules", ":", "return", "sys", ".", "modules", "[", "module_name", "]", "module", "=", "self", ".", "load_module_py_path", "(", "module_name", ",", "self", ".", "load_target", ")", "if", "self", ".", "is_pkg", ":", "module", ".", "__path__", "=", "[", "self", ".", "module_path", "]", "module", ".", "__package__", "=", "module_name", "else", ":", "module", ".", "__package__", "=", "module_name", ".", "rpartition", "(", "'.'", ")", "[", "0", "]", "sys", ".", "modules", "[", "module_name", "]", "=", "module", "return", "module"], "docstring": "Loads a module's code and sets the module's expected hidden\n        variables. For more information on these variables and what they\n        are for, please see PEP302.\n\n        :param module_name: the full name of the module to load", "docstring_tokens": ["Loads", "a", "module", "s", "code", "and", "sets", "the", "module", "s", "expected", "hidden", "variables", ".", "For", "more", "information", "on", "these", "variables", "and", "what", "they", "are", "for", "please", "see", "PEP302", "."], "sha": "15bc8b35a91be5817979eb327427b6235b1b411e", "url": "https://github.com/zinic/pynsive/blob/15bc8b35a91be5817979eb327427b6235b1b411e/pynsive/plugin/loader.py#L52-L75", "partition": "test"}
{"repo": "gears/gears", "path": "gears/asset_attributes.py", "func_name": "AssetAttributes.mimetype", "original_string": "def mimetype(self):\n        \"\"\"MIME type of the asset.\"\"\"\n        return (self.environment.mimetypes.get(self.format_extension) or\n                self.compiler_mimetype or 'application/octet-stream')", "language": "python", "code": "def mimetype(self):\n        \"\"\"MIME type of the asset.\"\"\"\n        return (self.environment.mimetypes.get(self.format_extension) or\n                self.compiler_mimetype or 'application/octet-stream')", "code_tokens": ["def", "mimetype", "(", "self", ")", ":", "return", "(", "self", ".", "environment", ".", "mimetypes", ".", "get", "(", "self", ".", "format_extension", ")", "or", "self", ".", "compiler_mimetype", "or", "'application/octet-stream'", ")"], "docstring": "MIME type of the asset.", "docstring_tokens": ["MIME", "type", "of", "the", "asset", "."], "sha": "5729c2525a8c04c185e998bd9a86233708972921", "url": "https://github.com/gears/gears/blob/5729c2525a8c04c185e998bd9a86233708972921/gears/asset_attributes.py#L191-L194", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/environment.py", "func_name": "_scoop_single_run", "original_string": "def _scoop_single_run(kwargs):\n    \"\"\"Wrapper function for scoop, that does not configure logging\"\"\"\n    try:\n        try:\n            is_origin = scoop.IS_ORIGIN\n        except AttributeError:\n            # scoop is not properly started, i.e. with `python -m scoop...`\n            # in this case scoop uses default `map` function, i.e.\n            # the main process\n            is_origin = True\n        if not is_origin:\n            # configure logging and niceness if not the main process:\n            _configure_niceness(kwargs)\n            _configure_logging(kwargs)\n        return _single_run(kwargs)\n    except Exception:\n        scoop.logger.exception('ERROR occurred during a single run!')\n        raise", "language": "python", "code": "def _scoop_single_run(kwargs):\n    \"\"\"Wrapper function for scoop, that does not configure logging\"\"\"\n    try:\n        try:\n            is_origin = scoop.IS_ORIGIN\n        except AttributeError:\n            # scoop is not properly started, i.e. with `python -m scoop...`\n            # in this case scoop uses default `map` function, i.e.\n            # the main process\n            is_origin = True\n        if not is_origin:\n            # configure logging and niceness if not the main process:\n            _configure_niceness(kwargs)\n            _configure_logging(kwargs)\n        return _single_run(kwargs)\n    except Exception:\n        scoop.logger.exception('ERROR occurred during a single run!')\n        raise", "code_tokens": ["def", "_scoop_single_run", "(", "kwargs", ")", ":", "try", ":", "try", ":", "is_origin", "=", "scoop", ".", "IS_ORIGIN", "except", "AttributeError", ":", "# scoop is not properly started, i.e. with `python -m scoop...`", "# in this case scoop uses default `map` function, i.e.", "# the main process", "is_origin", "=", "True", "if", "not", "is_origin", ":", "# configure logging and niceness if not the main process:", "_configure_niceness", "(", "kwargs", ")", "_configure_logging", "(", "kwargs", ")", "return", "_single_run", "(", "kwargs", ")", "except", "Exception", ":", "scoop", ".", "logger", ".", "exception", "(", "'ERROR occurred during a single run!'", ")", "raise"], "docstring": "Wrapper function for scoop, that does not configure logging", "docstring_tokens": ["Wrapper", "function", "for", "scoop", "that", "does", "not", "configure", "logging"], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/environment.py#L190-L207", "partition": "test"}
{"repo": "vaexio/vaex", "path": "packages/vaex-core/vaex/dataframe.py", "func_name": "DataFrame.head_and_tail_print", "original_string": "def head_and_tail_print(self, n=5):\n        \"\"\"Display the first and last n elements of a DataFrame.\"\"\"\n        from IPython import display\n        display.display(display.HTML(self._head_and_tail_table(n)))", "language": "python", "code": "def head_and_tail_print(self, n=5):\n        \"\"\"Display the first and last n elements of a DataFrame.\"\"\"\n        from IPython import display\n        display.display(display.HTML(self._head_and_tail_table(n)))", "code_tokens": ["def", "head_and_tail_print", "(", "self", ",", "n", "=", "5", ")", ":", "from", "IPython", "import", "display", "display", ".", "display", "(", "display", ".", "HTML", "(", "self", ".", "_head_and_tail_table", "(", "n", ")", ")", ")"], "docstring": "Display the first and last n elements of a DataFrame.", "docstring_tokens": ["Display", "the", "first", "and", "last", "n", "elements", "of", "a", "DataFrame", "."], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/dataframe.py#L3410-L3413", "partition": "test"}
{"repo": "tmontaigu/pylas", "path": "pylas/headers/rawheader.py", "func_name": "RawHeader1_1.scales", "original_string": "def scales(self):\n        \"\"\" Returns the scaling values of x, y, z as a numpy array\n        \"\"\"\n        return np.array([self.x_scale, self.y_scale, self.z_scale])", "language": "python", "code": "def scales(self):\n        \"\"\" Returns the scaling values of x, y, z as a numpy array\n        \"\"\"\n        return np.array([self.x_scale, self.y_scale, self.z_scale])", "code_tokens": ["def", "scales", "(", "self", ")", ":", "return", "np", ".", "array", "(", "[", "self", ".", "x_scale", ",", "self", ".", "y_scale", ",", "self", ".", "z_scale", "]", ")"], "docstring": "Returns the scaling values of x, y, z as a numpy array", "docstring_tokens": ["Returns", "the", "scaling", "values", "of", "x", "y", "z", "as", "a", "numpy", "array"], "sha": "8335a1a7d7677f0e4bc391bb6fa3c75b42ed5b06", "url": "https://github.com/tmontaigu/pylas/blob/8335a1a7d7677f0e4bc391bb6fa3c75b42ed5b06/pylas/headers/rawheader.py#L224-L227", "partition": "test"}
{"repo": "juga0/dhcpcanon", "path": "dhcpcanon/dhcpcapfsm.py", "func_name": "DHCPCAPFSM.receive_nak_rebinding", "original_string": "def receive_nak_rebinding(self, pkt):\n        \"\"\"Receive NAK in REBINDING state.\"\"\"\n        logger.debug(\"C3.1. Received NAK?, in RENEWING state.\")\n        if self.process_received_nak(pkt):\n            logger.debug(\"C3.1: T. Received NAK, in RENEWING state, \"\n                         \"raise INIT.\")\n            raise self.INIT()", "language": "python", "code": "def receive_nak_rebinding(self, pkt):\n        \"\"\"Receive NAK in REBINDING state.\"\"\"\n        logger.debug(\"C3.1. Received NAK?, in RENEWING state.\")\n        if self.process_received_nak(pkt):\n            logger.debug(\"C3.1: T. Received NAK, in RENEWING state, \"\n                         \"raise INIT.\")\n            raise self.INIT()", "code_tokens": ["def", "receive_nak_rebinding", "(", "self", ",", "pkt", ")", ":", "logger", ".", "debug", "(", "\"C3.1. Received NAK?, in RENEWING state.\"", ")", "if", "self", ".", "process_received_nak", "(", "pkt", ")", ":", "logger", ".", "debug", "(", "\"C3.1: T. Received NAK, in RENEWING state, \"", "\"raise INIT.\"", ")", "raise", "self", ".", "INIT", "(", ")"], "docstring": "Receive NAK in REBINDING state.", "docstring_tokens": ["Receive", "NAK", "in", "REBINDING", "state", "."], "sha": "9f51a29e57fe93dc93fb22bb0ed12fcfe9557e59", "url": "https://github.com/juga0/dhcpcanon/blob/9f51a29e57fe93dc93fb22bb0ed12fcfe9557e59/dhcpcanon/dhcpcapfsm.py#L626-L632", "partition": "test"}
{"repo": "boto/s3transfer", "path": "s3transfer/futures.py", "func_name": "TransferCoordinator.cancel", "original_string": "def cancel(self, msg='', exc_type=CancelledError):\n        \"\"\"Cancels the TransferFuture\n\n        :param msg: The message to attach to the cancellation\n        :param exc_type: The type of exception to set for the cancellation\n        \"\"\"\n        with self._lock:\n            if not self.done():\n                should_announce_done = False\n                logger.debug('%s cancel(%s) called', self, msg)\n                self._exception = exc_type(msg)\n                if self._status == 'not-started':\n                    should_announce_done = True\n                self._status = 'cancelled'\n                if should_announce_done:\n                    self.announce_done()", "language": "python", "code": "def cancel(self, msg='', exc_type=CancelledError):\n        \"\"\"Cancels the TransferFuture\n\n        :param msg: The message to attach to the cancellation\n        :param exc_type: The type of exception to set for the cancellation\n        \"\"\"\n        with self._lock:\n            if not self.done():\n                should_announce_done = False\n                logger.debug('%s cancel(%s) called', self, msg)\n                self._exception = exc_type(msg)\n                if self._status == 'not-started':\n                    should_announce_done = True\n                self._status = 'cancelled'\n                if should_announce_done:\n                    self.announce_done()", "code_tokens": ["def", "cancel", "(", "self", ",", "msg", "=", "''", ",", "exc_type", "=", "CancelledError", ")", ":", "with", "self", ".", "_lock", ":", "if", "not", "self", ".", "done", "(", ")", ":", "should_announce_done", "=", "False", "logger", ".", "debug", "(", "'%s cancel(%s) called'", ",", "self", ",", "msg", ")", "self", ".", "_exception", "=", "exc_type", "(", "msg", ")", "if", "self", ".", "_status", "==", "'not-started'", ":", "should_announce_done", "=", "True", "self", ".", "_status", "=", "'cancelled'", "if", "should_announce_done", ":", "self", ".", "announce_done", "(", ")"], "docstring": "Cancels the TransferFuture\n\n        :param msg: The message to attach to the cancellation\n        :param exc_type: The type of exception to set for the cancellation", "docstring_tokens": ["Cancels", "the", "TransferFuture"], "sha": "2aead638c8385d8ae0b1756b2de17e8fad45fffa", "url": "https://github.com/boto/s3transfer/blob/2aead638c8385d8ae0b1756b2de17e8fad45fffa/s3transfer/futures.py#L268-L283", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/hdl/types/hdlType.py", "func_name": "HdlType.reinterpret_cast", "original_string": "def reinterpret_cast(self, sigOrVal, toType):\n        \"\"\"\n        Cast value or signal of this type to another type of same size.\n\n        :param sigOrVal: instance of signal or value to cast\n        :param toType: instance of HdlType to cast into\n        \"\"\"\n        try:\n            return self.auto_cast(sigOrVal, toType)\n        except TypeConversionErr:\n            pass\n\n        try:\n            r = self._reinterpret_cast_fn\n        except AttributeError:\n            r = self.get_reinterpret_cast_fn()\n            self._reinterpret_cast_fn = r\n\n        return r(self, sigOrVal, toType)", "language": "python", "code": "def reinterpret_cast(self, sigOrVal, toType):\n        \"\"\"\n        Cast value or signal of this type to another type of same size.\n\n        :param sigOrVal: instance of signal or value to cast\n        :param toType: instance of HdlType to cast into\n        \"\"\"\n        try:\n            return self.auto_cast(sigOrVal, toType)\n        except TypeConversionErr:\n            pass\n\n        try:\n            r = self._reinterpret_cast_fn\n        except AttributeError:\n            r = self.get_reinterpret_cast_fn()\n            self._reinterpret_cast_fn = r\n\n        return r(self, sigOrVal, toType)", "code_tokens": ["def", "reinterpret_cast", "(", "self", ",", "sigOrVal", ",", "toType", ")", ":", "try", ":", "return", "self", ".", "auto_cast", "(", "sigOrVal", ",", "toType", ")", "except", "TypeConversionErr", ":", "pass", "try", ":", "r", "=", "self", ".", "_reinterpret_cast_fn", "except", "AttributeError", ":", "r", "=", "self", ".", "get_reinterpret_cast_fn", "(", ")", "self", ".", "_reinterpret_cast_fn", "=", "r", "return", "r", "(", "self", ",", "sigOrVal", ",", "toType", ")"], "docstring": "Cast value or signal of this type to another type of same size.\n\n        :param sigOrVal: instance of signal or value to cast\n        :param toType: instance of HdlType to cast into", "docstring_tokens": ["Cast", "value", "or", "signal", "of", "this", "type", "to", "another", "type", "of", "same", "size", "."], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/hdl/types/hdlType.py#L53-L71", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_speech_to_text_hook.py", "func_name": "GCPSpeechToTextHook.recognize_speech", "original_string": "def recognize_speech(self, config, audio, retry=None, timeout=None):\n        \"\"\"\n        Recognizes audio input\n\n        :param config: information to the recognizer that specifies how to process the request.\n            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionConfig\n        :type config: dict or google.cloud.speech_v1.types.RecognitionConfig\n        :param audio: audio data to be recognized\n            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionAudio\n        :type audio: dict or google.cloud.speech_v1.types.RecognitionAudio\n        :param retry: (Optional) A retry object used to retry requests. If None is specified,\n            requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.\n            Note that if retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        \"\"\"\n        client = self.get_conn()\n        response = client.recognize(config=config, audio=audio, retry=retry, timeout=timeout)\n        self.log.info(\"Recognised speech: %s\" % response)\n        return response", "language": "python", "code": "def recognize_speech(self, config, audio, retry=None, timeout=None):\n        \"\"\"\n        Recognizes audio input\n\n        :param config: information to the recognizer that specifies how to process the request.\n            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionConfig\n        :type config: dict or google.cloud.speech_v1.types.RecognitionConfig\n        :param audio: audio data to be recognized\n            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionAudio\n        :type audio: dict or google.cloud.speech_v1.types.RecognitionAudio\n        :param retry: (Optional) A retry object used to retry requests. If None is specified,\n            requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.\n            Note that if retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        \"\"\"\n        client = self.get_conn()\n        response = client.recognize(config=config, audio=audio, retry=retry, timeout=timeout)\n        self.log.info(\"Recognised speech: %s\" % response)\n        return response", "code_tokens": ["def", "recognize_speech", "(", "self", ",", "config", ",", "audio", ",", "retry", "=", "None", ",", "timeout", "=", "None", ")", ":", "client", "=", "self", ".", "get_conn", "(", ")", "response", "=", "client", ".", "recognize", "(", "config", "=", "config", ",", "audio", "=", "audio", ",", "retry", "=", "retry", ",", "timeout", "=", "timeout", ")", "self", ".", "log", ".", "info", "(", "\"Recognised speech: %s\"", "%", "response", ")", "return", "response"], "docstring": "Recognizes audio input\n\n        :param config: information to the recognizer that specifies how to process the request.\n            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionConfig\n        :type config: dict or google.cloud.speech_v1.types.RecognitionConfig\n        :param audio: audio data to be recognized\n            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionAudio\n        :type audio: dict or google.cloud.speech_v1.types.RecognitionAudio\n        :param retry: (Optional) A retry object used to retry requests. If None is specified,\n            requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.\n            Note that if retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float", "docstring_tokens": ["Recognizes", "audio", "input"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_speech_to_text_hook.py#L53-L73", "partition": "test"}
{"repo": "mdgoldberg/sportsref", "path": "sportsref/nba/boxscores.py", "func_name": "BoxScore._get_player_stats", "original_string": "def _get_player_stats(self, table_id_fmt):\n        \"\"\"Returns a DataFrame of player stats from the game (either basic or\n        advanced, depending on the argument.\n\n        :param table_id_fmt: Format string for str.format with a placeholder\n            for the team ID (e.g. 'box_{}_basic')\n        :returns: DataFrame of player stats\n        \"\"\"\n\n        # get data\n        doc = self.get_main_doc()\n        tms = self.away(), self.home()\n        tm_ids = [table_id_fmt.format(tm) for tm in tms]\n        tables = [doc('table#{}'.format(tm_id).lower()) for tm_id in tm_ids]\n        dfs = [sportsref.utils.parse_table(table) for table in tables]\n\n        # clean data and add features\n        for i, (tm, df) in enumerate(zip(tms, dfs)):\n            no_time = df['mp'] == 0\n            stat_cols = [col for col, dtype in df.dtypes.items()\n                         if dtype != 'object']\n            df.loc[no_time, stat_cols] = 0\n            df['team_id'] = tm\n            df['is_home'] = i == 1\n            df['is_starter'] = [p < 5 for p in range(df.shape[0])]\n            df.drop_duplicates(subset='player_id', keep='first', inplace=True)\n\n        return pd.concat(dfs)", "language": "python", "code": "def _get_player_stats(self, table_id_fmt):\n        \"\"\"Returns a DataFrame of player stats from the game (either basic or\n        advanced, depending on the argument.\n\n        :param table_id_fmt: Format string for str.format with a placeholder\n            for the team ID (e.g. 'box_{}_basic')\n        :returns: DataFrame of player stats\n        \"\"\"\n\n        # get data\n        doc = self.get_main_doc()\n        tms = self.away(), self.home()\n        tm_ids = [table_id_fmt.format(tm) for tm in tms]\n        tables = [doc('table#{}'.format(tm_id).lower()) for tm_id in tm_ids]\n        dfs = [sportsref.utils.parse_table(table) for table in tables]\n\n        # clean data and add features\n        for i, (tm, df) in enumerate(zip(tms, dfs)):\n            no_time = df['mp'] == 0\n            stat_cols = [col for col, dtype in df.dtypes.items()\n                         if dtype != 'object']\n            df.loc[no_time, stat_cols] = 0\n            df['team_id'] = tm\n            df['is_home'] = i == 1\n            df['is_starter'] = [p < 5 for p in range(df.shape[0])]\n            df.drop_duplicates(subset='player_id', keep='first', inplace=True)\n\n        return pd.concat(dfs)", "code_tokens": ["def", "_get_player_stats", "(", "self", ",", "table_id_fmt", ")", ":", "# get data", "doc", "=", "self", ".", "get_main_doc", "(", ")", "tms", "=", "self", ".", "away", "(", ")", ",", "self", ".", "home", "(", ")", "tm_ids", "=", "[", "table_id_fmt", ".", "format", "(", "tm", ")", "for", "tm", "in", "tms", "]", "tables", "=", "[", "doc", "(", "'table#{}'", ".", "format", "(", "tm_id", ")", ".", "lower", "(", ")", ")", "for", "tm_id", "in", "tm_ids", "]", "dfs", "=", "[", "sportsref", ".", "utils", ".", "parse_table", "(", "table", ")", "for", "table", "in", "tables", "]", "# clean data and add features", "for", "i", ",", "(", "tm", ",", "df", ")", "in", "enumerate", "(", "zip", "(", "tms", ",", "dfs", ")", ")", ":", "no_time", "=", "df", "[", "'mp'", "]", "==", "0", "stat_cols", "=", "[", "col", "for", "col", ",", "dtype", "in", "df", ".", "dtypes", ".", "items", "(", ")", "if", "dtype", "!=", "'object'", "]", "df", ".", "loc", "[", "no_time", ",", "stat_cols", "]", "=", "0", "df", "[", "'team_id'", "]", "=", "tm", "df", "[", "'is_home'", "]", "=", "i", "==", "1", "df", "[", "'is_starter'", "]", "=", "[", "p", "<", "5", "for", "p", "in", "range", "(", "df", ".", "shape", "[", "0", "]", ")", "]", "df", ".", "drop_duplicates", "(", "subset", "=", "'player_id'", ",", "keep", "=", "'first'", ",", "inplace", "=", "True", ")", "return", "pd", ".", "concat", "(", "dfs", ")"], "docstring": "Returns a DataFrame of player stats from the game (either basic or\n        advanced, depending on the argument.\n\n        :param table_id_fmt: Format string for str.format with a placeholder\n            for the team ID (e.g. 'box_{}_basic')\n        :returns: DataFrame of player stats", "docstring_tokens": ["Returns", "a", "DataFrame", "of", "player", "stats", "from", "the", "game", "(", "either", "basic", "or", "advanced", "depending", "on", "the", "argument", "."], "sha": "09f11ac856a23c96d666d1d510bb35d6f050b5c3", "url": "https://github.com/mdgoldberg/sportsref/blob/09f11ac856a23c96d666d1d510bb35d6f050b5c3/sportsref/nba/boxscores.py#L138-L165", "partition": "test"}
{"repo": "pytorch/vision", "path": "torchvision/transforms/functional.py", "func_name": "perspective", "original_string": "def perspective(img, startpoints, endpoints, interpolation=Image.BICUBIC):\n    \"\"\"Perform perspective transform of the given PIL Image.\n\n    Args:\n        img (PIL Image): Image to be transformed.\n        coeffs (tuple) : 8-tuple (a, b, c, d, e, f, g, h) which contains the coefficients.\n                            for a perspective transform.\n        interpolation: Default- Image.BICUBIC\n    Returns:\n        PIL Image:  Perspectively transformed Image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    coeffs = _get_perspective_coeffs(startpoints, endpoints)\n    return img.transform(img.size, Image.PERSPECTIVE, coeffs, interpolation)", "language": "python", "code": "def perspective(img, startpoints, endpoints, interpolation=Image.BICUBIC):\n    \"\"\"Perform perspective transform of the given PIL Image.\n\n    Args:\n        img (PIL Image): Image to be transformed.\n        coeffs (tuple) : 8-tuple (a, b, c, d, e, f, g, h) which contains the coefficients.\n                            for a perspective transform.\n        interpolation: Default- Image.BICUBIC\n    Returns:\n        PIL Image:  Perspectively transformed Image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    coeffs = _get_perspective_coeffs(startpoints, endpoints)\n    return img.transform(img.size, Image.PERSPECTIVE, coeffs, interpolation)", "code_tokens": ["def", "perspective", "(", "img", ",", "startpoints", ",", "endpoints", ",", "interpolation", "=", "Image", ".", "BICUBIC", ")", ":", "if", "not", "_is_pil_image", "(", "img", ")", ":", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "coeffs", "=", "_get_perspective_coeffs", "(", "startpoints", ",", "endpoints", ")", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "PERSPECTIVE", ",", "coeffs", ",", "interpolation", ")"], "docstring": "Perform perspective transform of the given PIL Image.\n\n    Args:\n        img (PIL Image): Image to be transformed.\n        coeffs (tuple) : 8-tuple (a, b, c, d, e, f, g, h) which contains the coefficients.\n                            for a perspective transform.\n        interpolation: Default- Image.BICUBIC\n    Returns:\n        PIL Image:  Perspectively transformed Image.", "docstring_tokens": ["Perform", "perspective", "transform", "of", "the", "given", "PIL", "Image", "."], "sha": "3afcf3cd49661c466c75ea536b0b2a7ff57f9a05", "url": "https://github.com/pytorch/vision/blob/3afcf3cd49661c466c75ea536b0b2a7ff57f9a05/torchvision/transforms/functional.py#L435-L450", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/pie/objects.py", "func_name": "OpaqueObject.validate", "original_string": "def validate(self):\n        \"\"\"\n        Verify that the contents of the OpaqueObject are valid.\n\n        Raises:\n            TypeError: if the types of any OpaqueObject attributes are invalid.\n        \"\"\"\n        if not isinstance(self.value, bytes):\n            raise TypeError(\"opaque value must be bytes\")\n        elif not isinstance(self.opaque_type, enums.OpaqueDataType):\n            raise TypeError(\"opaque data type must be an OpaqueDataType \"\n                            \"enumeration\")\n\n        name_count = len(self.names)\n        for i in range(name_count):\n            name = self.names[i]\n            if not isinstance(name, six.string_types):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\"opaque data name {0} must be a string\".format(\n                    position))", "language": "python", "code": "def validate(self):\n        \"\"\"\n        Verify that the contents of the OpaqueObject are valid.\n\n        Raises:\n            TypeError: if the types of any OpaqueObject attributes are invalid.\n        \"\"\"\n        if not isinstance(self.value, bytes):\n            raise TypeError(\"opaque value must be bytes\")\n        elif not isinstance(self.opaque_type, enums.OpaqueDataType):\n            raise TypeError(\"opaque data type must be an OpaqueDataType \"\n                            \"enumeration\")\n\n        name_count = len(self.names)\n        for i in range(name_count):\n            name = self.names[i]\n            if not isinstance(name, six.string_types):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\"opaque data name {0} must be a string\".format(\n                    position))", "code_tokens": ["def", "validate", "(", "self", ")", ":", "if", "not", "isinstance", "(", "self", ".", "value", ",", "bytes", ")", ":", "raise", "TypeError", "(", "\"opaque value must be bytes\"", ")", "elif", "not", "isinstance", "(", "self", ".", "opaque_type", ",", "enums", ".", "OpaqueDataType", ")", ":", "raise", "TypeError", "(", "\"opaque data type must be an OpaqueDataType \"", "\"enumeration\"", ")", "name_count", "=", "len", "(", "self", ".", "names", ")", "for", "i", "in", "range", "(", "name_count", ")", ":", "name", "=", "self", ".", "names", "[", "i", "]", "if", "not", "isinstance", "(", "name", ",", "six", ".", "string_types", ")", ":", "position", "=", "\"({0} in list)\"", ".", "format", "(", "i", ")", "raise", "TypeError", "(", "\"opaque data name {0} must be a string\"", ".", "format", "(", "position", ")", ")"], "docstring": "Verify that the contents of the OpaqueObject are valid.\n\n        Raises:\n            TypeError: if the types of any OpaqueObject attributes are invalid.", "docstring_tokens": ["Verify", "that", "the", "contents", "of", "the", "OpaqueObject", "are", "valid", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/pie/objects.py#L1407-L1426", "partition": "test"}
{"repo": "philipsoutham/py-mysql2pgsql", "path": "mysql2pgsql/lib/postgres_db_writer.py", "func_name": "PostgresDbWriter.write_contents", "original_string": "def write_contents(self, table, reader):\n        \"\"\"Write the contents of `table`\n\n        :Parameters:\n          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.\n          - `reader`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader` object that allows reading from the data source.\n\n        Returns None\n        \"\"\"\n        f = self.FileObjFaker(table, reader.read(table), self.process_row, self.verbose)\n        self.copy_from(f, '\"%s\"' % table.name, ['\"%s\"' % c['name'] for c in table.columns])", "language": "python", "code": "def write_contents(self, table, reader):\n        \"\"\"Write the contents of `table`\n\n        :Parameters:\n          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.\n          - `reader`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader` object that allows reading from the data source.\n\n        Returns None\n        \"\"\"\n        f = self.FileObjFaker(table, reader.read(table), self.process_row, self.verbose)\n        self.copy_from(f, '\"%s\"' % table.name, ['\"%s\"' % c['name'] for c in table.columns])", "code_tokens": ["def", "write_contents", "(", "self", ",", "table", ",", "reader", ")", ":", "f", "=", "self", ".", "FileObjFaker", "(", "table", ",", "reader", ".", "read", "(", "table", ")", ",", "self", ".", "process_row", ",", "self", ".", "verbose", ")", "self", ".", "copy_from", "(", "f", ",", "'\"%s\"'", "%", "table", ".", "name", ",", "[", "'\"%s\"'", "%", "c", "[", "'name'", "]", "for", "c", "in", "table", ".", "columns", "]", ")"], "docstring": "Write the contents of `table`\n\n        :Parameters:\n          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.\n          - `reader`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader` object that allows reading from the data source.\n\n        Returns None", "docstring_tokens": ["Write", "the", "contents", "of", "table"], "sha": "66dc2a3a3119263b3fe77300fb636346509787ef", "url": "https://github.com/philipsoutham/py-mysql2pgsql/blob/66dc2a3a3119263b3fe77300fb636346509787ef/mysql2pgsql/lib/postgres_db_writer.py#L196-L206", "partition": "test"}
{"repo": "PythonSanSebastian/docstamp", "path": "docstamp/pdflatex.py", "func_name": "tex2pdf", "original_string": "def tex2pdf(tex_file, output_file=None, output_format='pdf'):\n    \"\"\" Call PDFLatex to convert TeX files to PDF.\n\n    Parameters\n    ----------\n    tex_file: str\n        Path to the input LateX file.\n\n    output_file: str\n        Path to the output PDF file.\n        If None, will use the same output directory as the tex_file.\n\n    output_format: str\n        Output file format. Choices: 'pdf' or 'dvi'. Default: 'pdf'\n\n    Returns\n    -------\n    return_value\n        PDFLatex command call return value.\n    \"\"\"\n    if not os.path.exists(tex_file):\n        raise IOError('Could not find file {}.'.format(tex_file))\n\n    if output_format != 'pdf' and output_format != 'dvi':\n        raise ValueError(\"Invalid output format given {}. Can only accept 'pdf' or 'dvi'.\".format(output_format))\n\n    cmd_name = 'pdflatex'\n    check_command(cmd_name)\n\n    args_strings = [cmd_name]\n    if output_file is not None:\n        args_strings += ['-output-directory=\"{}\" '.format(os.path.abspath(os.path.dirname(output_file)))]\n\n    result_dir = os.path.dirname(output_file) if output_file else os.path.dirname(tex_file)\n\n    args_strings += ['-output-format=\"{}\"'.format(output_format)]\n    args_strings += ['\"' + tex_file + '\"']\n\n    log.debug('Calling command {} with args: {}.'.format(cmd_name, args_strings))\n    ret = simple_call(args_strings)\n\n    result_file = os.path.join(result_dir, remove_ext(os.path.basename(tex_file)) + '.' + output_format)\n    if os.path.exists(result_file):\n        shutil.move(result_file, output_file)\n    else:\n        raise IOError('Could not find PDFLatex result file.')\n\n    log.debug('Cleaning *.aux and *.log files from folder {}.'.format(result_dir))\n    cleanup(result_dir, 'aux')\n    cleanup(result_dir, 'log')\n\n    return ret", "language": "python", "code": "def tex2pdf(tex_file, output_file=None, output_format='pdf'):\n    \"\"\" Call PDFLatex to convert TeX files to PDF.\n\n    Parameters\n    ----------\n    tex_file: str\n        Path to the input LateX file.\n\n    output_file: str\n        Path to the output PDF file.\n        If None, will use the same output directory as the tex_file.\n\n    output_format: str\n        Output file format. Choices: 'pdf' or 'dvi'. Default: 'pdf'\n\n    Returns\n    -------\n    return_value\n        PDFLatex command call return value.\n    \"\"\"\n    if not os.path.exists(tex_file):\n        raise IOError('Could not find file {}.'.format(tex_file))\n\n    if output_format != 'pdf' and output_format != 'dvi':\n        raise ValueError(\"Invalid output format given {}. Can only accept 'pdf' or 'dvi'.\".format(output_format))\n\n    cmd_name = 'pdflatex'\n    check_command(cmd_name)\n\n    args_strings = [cmd_name]\n    if output_file is not None:\n        args_strings += ['-output-directory=\"{}\" '.format(os.path.abspath(os.path.dirname(output_file)))]\n\n    result_dir = os.path.dirname(output_file) if output_file else os.path.dirname(tex_file)\n\n    args_strings += ['-output-format=\"{}\"'.format(output_format)]\n    args_strings += ['\"' + tex_file + '\"']\n\n    log.debug('Calling command {} with args: {}.'.format(cmd_name, args_strings))\n    ret = simple_call(args_strings)\n\n    result_file = os.path.join(result_dir, remove_ext(os.path.basename(tex_file)) + '.' + output_format)\n    if os.path.exists(result_file):\n        shutil.move(result_file, output_file)\n    else:\n        raise IOError('Could not find PDFLatex result file.')\n\n    log.debug('Cleaning *.aux and *.log files from folder {}.'.format(result_dir))\n    cleanup(result_dir, 'aux')\n    cleanup(result_dir, 'log')\n\n    return ret", "code_tokens": ["def", "tex2pdf", "(", "tex_file", ",", "output_file", "=", "None", ",", "output_format", "=", "'pdf'", ")", ":", "if", "not", "os", ".", "path", ".", "exists", "(", "tex_file", ")", ":", "raise", "IOError", "(", "'Could not find file {}.'", ".", "format", "(", "tex_file", ")", ")", "if", "output_format", "!=", "'pdf'", "and", "output_format", "!=", "'dvi'", ":", "raise", "ValueError", "(", "\"Invalid output format given {}. Can only accept 'pdf' or 'dvi'.\"", ".", "format", "(", "output_format", ")", ")", "cmd_name", "=", "'pdflatex'", "check_command", "(", "cmd_name", ")", "args_strings", "=", "[", "cmd_name", "]", "if", "output_file", "is", "not", "None", ":", "args_strings", "+=", "[", "'-output-directory=\"{}\" '", ".", "format", "(", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "dirname", "(", "output_file", ")", ")", ")", "]", "result_dir", "=", "os", ".", "path", ".", "dirname", "(", "output_file", ")", "if", "output_file", "else", "os", ".", "path", ".", "dirname", "(", "tex_file", ")", "args_strings", "+=", "[", "'-output-format=\"{}\"'", ".", "format", "(", "output_format", ")", "]", "args_strings", "+=", "[", "'\"'", "+", "tex_file", "+", "'\"'", "]", "log", ".", "debug", "(", "'Calling command {} with args: {}.'", ".", "format", "(", "cmd_name", ",", "args_strings", ")", ")", "ret", "=", "simple_call", "(", "args_strings", ")", "result_file", "=", "os", ".", "path", ".", "join", "(", "result_dir", ",", "remove_ext", "(", "os", ".", "path", ".", "basename", "(", "tex_file", ")", ")", "+", "'.'", "+", "output_format", ")", "if", "os", ".", "path", ".", "exists", "(", "result_file", ")", ":", "shutil", ".", "move", "(", "result_file", ",", "output_file", ")", "else", ":", "raise", "IOError", "(", "'Could not find PDFLatex result file.'", ")", "log", ".", "debug", "(", "'Cleaning *.aux and *.log files from folder {}.'", ".", "format", "(", "result_dir", ")", ")", "cleanup", "(", "result_dir", ",", "'aux'", ")", "cleanup", "(", "result_dir", ",", "'log'", ")", "return", "ret"], "docstring": "Call PDFLatex to convert TeX files to PDF.\n\n    Parameters\n    ----------\n    tex_file: str\n        Path to the input LateX file.\n\n    output_file: str\n        Path to the output PDF file.\n        If None, will use the same output directory as the tex_file.\n\n    output_format: str\n        Output file format. Choices: 'pdf' or 'dvi'. Default: 'pdf'\n\n    Returns\n    -------\n    return_value\n        PDFLatex command call return value.", "docstring_tokens": ["Call", "PDFLatex", "to", "convert", "TeX", "files", "to", "PDF", "."], "sha": "b43808f2e15351b0b2f0b7eade9c7ef319c9e646", "url": "https://github.com/PythonSanSebastian/docstamp/blob/b43808f2e15351b0b2f0b7eade9c7ef319c9e646/docstamp/pdflatex.py#L21-L72", "partition": "test"}
{"repo": "iotile/typedargs", "path": "typedargs/typeinfo.py", "func_name": "TypeSystem.is_known_type", "original_string": "def is_known_type(self, type_name):\n        \"\"\"Check if type is known to the type system.\n\n        Returns:\n            bool: True if the type is a known instantiated simple type, False otherwise\n        \"\"\"\n\n        type_name = str(type_name)\n        if type_name in self.known_types:\n            return True\n\n        return False", "language": "python", "code": "def is_known_type(self, type_name):\n        \"\"\"Check if type is known to the type system.\n\n        Returns:\n            bool: True if the type is a known instantiated simple type, False otherwise\n        \"\"\"\n\n        type_name = str(type_name)\n        if type_name in self.known_types:\n            return True\n\n        return False", "code_tokens": ["def", "is_known_type", "(", "self", ",", "type_name", ")", ":", "type_name", "=", "str", "(", "type_name", ")", "if", "type_name", "in", "self", ".", "known_types", ":", "return", "True", "return", "False"], "docstring": "Check if type is known to the type system.\n\n        Returns:\n            bool: True if the type is a known instantiated simple type, False otherwise", "docstring_tokens": ["Check", "if", "type", "is", "known", "to", "the", "type", "system", "."], "sha": "0a5091a664b9b4d836e091e9ba583e944f438fd8", "url": "https://github.com/iotile/typedargs/blob/0a5091a664b9b4d836e091e9ba583e944f438fd8/typedargs/typeinfo.py#L174-L185", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/backend/connection.py", "func_name": "H2OConnection.start_logging", "original_string": "def start_logging(self, dest=None):\n        \"\"\"\n        Start logging all API requests to the provided destination.\n\n        :param dest: Where to write the log: either a filename (str), or an open file handle (file). If not given,\n            then a new temporary file will be created.\n        \"\"\"\n        assert_is_type(dest, None, str, type(sys.stdout))\n        if dest is None:\n            dest = os.path.join(tempfile.mkdtemp(), \"h2o-connection.log\")\n        self._print(\"Now logging all API requests to file %r\" % dest)\n        self._is_logging = True\n        self._logging_dest = dest", "language": "python", "code": "def start_logging(self, dest=None):\n        \"\"\"\n        Start logging all API requests to the provided destination.\n\n        :param dest: Where to write the log: either a filename (str), or an open file handle (file). If not given,\n            then a new temporary file will be created.\n        \"\"\"\n        assert_is_type(dest, None, str, type(sys.stdout))\n        if dest is None:\n            dest = os.path.join(tempfile.mkdtemp(), \"h2o-connection.log\")\n        self._print(\"Now logging all API requests to file %r\" % dest)\n        self._is_logging = True\n        self._logging_dest = dest", "code_tokens": ["def", "start_logging", "(", "self", ",", "dest", "=", "None", ")", ":", "assert_is_type", "(", "dest", ",", "None", ",", "str", ",", "type", "(", "sys", ".", "stdout", ")", ")", "if", "dest", "is", "None", ":", "dest", "=", "os", ".", "path", ".", "join", "(", "tempfile", ".", "mkdtemp", "(", ")", ",", "\"h2o-connection.log\"", ")", "self", ".", "_print", "(", "\"Now logging all API requests to file %r\"", "%", "dest", ")", "self", ".", "_is_logging", "=", "True", "self", ".", "_logging_dest", "=", "dest"], "docstring": "Start logging all API requests to the provided destination.\n\n        :param dest: Where to write the log: either a filename (str), or an open file handle (file). If not given,\n            then a new temporary file will be created.", "docstring_tokens": ["Start", "logging", "all", "API", "requests", "to", "the", "provided", "destination", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/backend/connection.py#L503-L515", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py", "func_name": "ServiceManagementService.create_virtual_machine_deployment", "original_string": "def create_virtual_machine_deployment(self, service_name, deployment_name,\n                                          deployment_slot, label, role_name,\n                                          system_config, os_virtual_hard_disk,\n                                          network_config=None,\n                                          availability_set_name=None,\n                                          data_virtual_hard_disks=None,\n                                          role_size=None,\n                                          role_type='PersistentVMRole',\n                                          virtual_network_name=None,\n                                          resource_extension_references=None,\n                                          provision_guest_agent=None,\n                                          vm_image_name=None,\n                                          media_location=None,\n                                          dns_servers=None,\n                                          reserved_ip_name=None):\n        '''\n        Provisions a virtual machine based on the supplied configuration.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name for the deployment. The deployment name must be unique\n            among other deployments for the hosted service.\n        deployment_slot:\n            The environment to which the hosted service is deployed. Valid\n            values are: staging, production\n        label:\n            Specifies an identifier for the deployment. The label can be up to\n            100 characters long. The label can be used for tracking purposes.\n        role_name:\n            The name of the role.\n        system_config:\n            Contains the metadata required to provision a virtual machine from\n            a Windows or Linux OS image.  Use an instance of\n            WindowsConfigurationSet or LinuxConfigurationSet.\n        os_virtual_hard_disk:\n            Contains the parameters Windows Azure uses to create the operating\n            system disk for the virtual machine. If you are creating a Virtual\n            Machine by using a VM Image, this parameter is not used.\n        network_config:\n            Encapsulates the metadata required to create the virtual network\n            configuration for a virtual machine. If you do not include a\n            network configuration set you will not be able to access the VM\n            through VIPs over the internet. If your virtual machine belongs to\n            a virtual network you can not specify which subnet address space\n            it resides under. Use an instance of ConfigurationSet.\n        availability_set_name:\n            Specifies the name of an availability set to which to add the\n            virtual machine. This value controls the virtual machine\n            allocation in the Windows Azure environment. Virtual machines\n            specified in the same availability set are allocated to different\n            nodes to maximize availability.\n        data_virtual_hard_disks:\n            Contains the parameters Windows Azure uses to create a data disk\n            for a virtual machine.\n        role_size:\n            The size of the virtual machine to allocate. The default value is\n            Small. Possible values are: ExtraSmall,Small,Medium,Large,\n            ExtraLarge,A5,A6,A7,A8,A9,Basic_A0,Basic_A1,Basic_A2,Basic_A3,\n            Basic_A4,Standard_D1,Standard_D2,Standard_D3,Standard_D4,\n            Standard_D11,Standard_D12,Standard_D13,Standard_D14,Standard_G1,\n            Standard_G2,Sandard_G3,Standard_G4,Standard_G5. The specified\n            value must be compatible with the disk selected in the \n            OSVirtualHardDisk values.\n        role_type:\n            The type of the role for the virtual machine. The only supported\n            value is PersistentVMRole.\n        virtual_network_name:\n            Specifies the name of an existing virtual network to which the\n            deployment will belong.\n        resource_extension_references:\n            Optional. Contains a collection of resource extensions that are to\n            be installed on the Virtual Machine. This element is used if\n            provision_guest_agent is set to True. Use an iterable of instances\n            of ResourceExtensionReference.\n        provision_guest_agent:\n            Optional. Indicates whether the VM Agent is installed on the\n            Virtual Machine. To run a resource extension in a Virtual Machine,\n            this service must be installed.\n        vm_image_name:\n            Optional. Specifies the name of the VM Image that is to be used to\n            create the Virtual Machine. If this is specified, the\n            system_config and network_config parameters are not used.\n        media_location:\n            Optional. Required if the Virtual Machine is being created from a\n            published VM Image. Specifies the location of the VHD file that is\n            created when VMImageName specifies a published VM Image.\n        dns_servers:\n            Optional. List of DNS servers (use DnsServer class) to associate\n            with the Virtual Machine.\n        reserved_ip_name:\n            Optional. Specifies the name of a reserved IP address that is to be\n            assigned to the deployment. You must run create_reserved_ip_address\n            before you can assign the address to the deployment using this\n            element.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('deployment_slot', deployment_slot)\n        _validate_not_none('label', label)\n        _validate_not_none('role_name', role_name)\n        return self._perform_post(\n            self._get_deployment_path_using_name(service_name),\n            _XmlSerializer.virtual_machine_deployment_to_xml(\n                deployment_name,\n                deployment_slot,\n                label,\n                role_name,\n                system_config,\n                os_virtual_hard_disk,\n                role_type,\n                network_config,\n                availability_set_name,\n                data_virtual_hard_disks,\n                role_size,\n                virtual_network_name,\n                resource_extension_references,\n                provision_guest_agent,\n                vm_image_name,\n                media_location,\n                dns_servers,\n                reserved_ip_name),\n            as_async=True)", "language": "python", "code": "def create_virtual_machine_deployment(self, service_name, deployment_name,\n                                          deployment_slot, label, role_name,\n                                          system_config, os_virtual_hard_disk,\n                                          network_config=None,\n                                          availability_set_name=None,\n                                          data_virtual_hard_disks=None,\n                                          role_size=None,\n                                          role_type='PersistentVMRole',\n                                          virtual_network_name=None,\n                                          resource_extension_references=None,\n                                          provision_guest_agent=None,\n                                          vm_image_name=None,\n                                          media_location=None,\n                                          dns_servers=None,\n                                          reserved_ip_name=None):\n        '''\n        Provisions a virtual machine based on the supplied configuration.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name for the deployment. The deployment name must be unique\n            among other deployments for the hosted service.\n        deployment_slot:\n            The environment to which the hosted service is deployed. Valid\n            values are: staging, production\n        label:\n            Specifies an identifier for the deployment. The label can be up to\n            100 characters long. The label can be used for tracking purposes.\n        role_name:\n            The name of the role.\n        system_config:\n            Contains the metadata required to provision a virtual machine from\n            a Windows or Linux OS image.  Use an instance of\n            WindowsConfigurationSet or LinuxConfigurationSet.\n        os_virtual_hard_disk:\n            Contains the parameters Windows Azure uses to create the operating\n            system disk for the virtual machine. If you are creating a Virtual\n            Machine by using a VM Image, this parameter is not used.\n        network_config:\n            Encapsulates the metadata required to create the virtual network\n            configuration for a virtual machine. If you do not include a\n            network configuration set you will not be able to access the VM\n            through VIPs over the internet. If your virtual machine belongs to\n            a virtual network you can not specify which subnet address space\n            it resides under. Use an instance of ConfigurationSet.\n        availability_set_name:\n            Specifies the name of an availability set to which to add the\n            virtual machine. This value controls the virtual machine\n            allocation in the Windows Azure environment. Virtual machines\n            specified in the same availability set are allocated to different\n            nodes to maximize availability.\n        data_virtual_hard_disks:\n            Contains the parameters Windows Azure uses to create a data disk\n            for a virtual machine.\n        role_size:\n            The size of the virtual machine to allocate. The default value is\n            Small. Possible values are: ExtraSmall,Small,Medium,Large,\n            ExtraLarge,A5,A6,A7,A8,A9,Basic_A0,Basic_A1,Basic_A2,Basic_A3,\n            Basic_A4,Standard_D1,Standard_D2,Standard_D3,Standard_D4,\n            Standard_D11,Standard_D12,Standard_D13,Standard_D14,Standard_G1,\n            Standard_G2,Sandard_G3,Standard_G4,Standard_G5. The specified\n            value must be compatible with the disk selected in the \n            OSVirtualHardDisk values.\n        role_type:\n            The type of the role for the virtual machine. The only supported\n            value is PersistentVMRole.\n        virtual_network_name:\n            Specifies the name of an existing virtual network to which the\n            deployment will belong.\n        resource_extension_references:\n            Optional. Contains a collection of resource extensions that are to\n            be installed on the Virtual Machine. This element is used if\n            provision_guest_agent is set to True. Use an iterable of instances\n            of ResourceExtensionReference.\n        provision_guest_agent:\n            Optional. Indicates whether the VM Agent is installed on the\n            Virtual Machine. To run a resource extension in a Virtual Machine,\n            this service must be installed.\n        vm_image_name:\n            Optional. Specifies the name of the VM Image that is to be used to\n            create the Virtual Machine. If this is specified, the\n            system_config and network_config parameters are not used.\n        media_location:\n            Optional. Required if the Virtual Machine is being created from a\n            published VM Image. Specifies the location of the VHD file that is\n            created when VMImageName specifies a published VM Image.\n        dns_servers:\n            Optional. List of DNS servers (use DnsServer class) to associate\n            with the Virtual Machine.\n        reserved_ip_name:\n            Optional. Specifies the name of a reserved IP address that is to be\n            assigned to the deployment. You must run create_reserved_ip_address\n            before you can assign the address to the deployment using this\n            element.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('deployment_slot', deployment_slot)\n        _validate_not_none('label', label)\n        _validate_not_none('role_name', role_name)\n        return self._perform_post(\n            self._get_deployment_path_using_name(service_name),\n            _XmlSerializer.virtual_machine_deployment_to_xml(\n                deployment_name,\n                deployment_slot,\n                label,\n                role_name,\n                system_config,\n                os_virtual_hard_disk,\n                role_type,\n                network_config,\n                availability_set_name,\n                data_virtual_hard_disks,\n                role_size,\n                virtual_network_name,\n                resource_extension_references,\n                provision_guest_agent,\n                vm_image_name,\n                media_location,\n                dns_servers,\n                reserved_ip_name),\n            as_async=True)", "code_tokens": ["def", "create_virtual_machine_deployment", "(", "self", ",", "service_name", ",", "deployment_name", ",", "deployment_slot", ",", "label", ",", "role_name", ",", "system_config", ",", "os_virtual_hard_disk", ",", "network_config", "=", "None", ",", "availability_set_name", "=", "None", ",", "data_virtual_hard_disks", "=", "None", ",", "role_size", "=", "None", ",", "role_type", "=", "'PersistentVMRole'", ",", "virtual_network_name", "=", "None", ",", "resource_extension_references", "=", "None", ",", "provision_guest_agent", "=", "None", ",", "vm_image_name", "=", "None", ",", "media_location", "=", "None", ",", "dns_servers", "=", "None", ",", "reserved_ip_name", "=", "None", ")", ":", "_validate_not_none", "(", "'service_name'", ",", "service_name", ")", "_validate_not_none", "(", "'deployment_name'", ",", "deployment_name", ")", "_validate_not_none", "(", "'deployment_slot'", ",", "deployment_slot", ")", "_validate_not_none", "(", "'label'", ",", "label", ")", "_validate_not_none", "(", "'role_name'", ",", "role_name", ")", "return", "self", ".", "_perform_post", "(", "self", ".", "_get_deployment_path_using_name", "(", "service_name", ")", ",", "_XmlSerializer", ".", "virtual_machine_deployment_to_xml", "(", "deployment_name", ",", "deployment_slot", ",", "label", ",", "role_name", ",", "system_config", ",", "os_virtual_hard_disk", ",", "role_type", ",", "network_config", ",", "availability_set_name", ",", "data_virtual_hard_disks", ",", "role_size", ",", "virtual_network_name", ",", "resource_extension_references", ",", "provision_guest_agent", ",", "vm_image_name", ",", "media_location", ",", "dns_servers", ",", "reserved_ip_name", ")", ",", "as_async", "=", "True", ")"], "docstring": "Provisions a virtual machine based on the supplied configuration.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name for the deployment. The deployment name must be unique\n            among other deployments for the hosted service.\n        deployment_slot:\n            The environment to which the hosted service is deployed. Valid\n            values are: staging, production\n        label:\n            Specifies an identifier for the deployment. The label can be up to\n            100 characters long. The label can be used for tracking purposes.\n        role_name:\n            The name of the role.\n        system_config:\n            Contains the metadata required to provision a virtual machine from\n            a Windows or Linux OS image.  Use an instance of\n            WindowsConfigurationSet or LinuxConfigurationSet.\n        os_virtual_hard_disk:\n            Contains the parameters Windows Azure uses to create the operating\n            system disk for the virtual machine. If you are creating a Virtual\n            Machine by using a VM Image, this parameter is not used.\n        network_config:\n            Encapsulates the metadata required to create the virtual network\n            configuration for a virtual machine. If you do not include a\n            network configuration set you will not be able to access the VM\n            through VIPs over the internet. If your virtual machine belongs to\n            a virtual network you can not specify which subnet address space\n            it resides under. Use an instance of ConfigurationSet.\n        availability_set_name:\n            Specifies the name of an availability set to which to add the\n            virtual machine. This value controls the virtual machine\n            allocation in the Windows Azure environment. Virtual machines\n            specified in the same availability set are allocated to different\n            nodes to maximize availability.\n        data_virtual_hard_disks:\n            Contains the parameters Windows Azure uses to create a data disk\n            for a virtual machine.\n        role_size:\n            The size of the virtual machine to allocate. The default value is\n            Small. Possible values are: ExtraSmall,Small,Medium,Large,\n            ExtraLarge,A5,A6,A7,A8,A9,Basic_A0,Basic_A1,Basic_A2,Basic_A3,\n            Basic_A4,Standard_D1,Standard_D2,Standard_D3,Standard_D4,\n            Standard_D11,Standard_D12,Standard_D13,Standard_D14,Standard_G1,\n            Standard_G2,Sandard_G3,Standard_G4,Standard_G5. The specified\n            value must be compatible with the disk selected in the \n            OSVirtualHardDisk values.\n        role_type:\n            The type of the role for the virtual machine. The only supported\n            value is PersistentVMRole.\n        virtual_network_name:\n            Specifies the name of an existing virtual network to which the\n            deployment will belong.\n        resource_extension_references:\n            Optional. Contains a collection of resource extensions that are to\n            be installed on the Virtual Machine. This element is used if\n            provision_guest_agent is set to True. Use an iterable of instances\n            of ResourceExtensionReference.\n        provision_guest_agent:\n            Optional. Indicates whether the VM Agent is installed on the\n            Virtual Machine. To run a resource extension in a Virtual Machine,\n            this service must be installed.\n        vm_image_name:\n            Optional. Specifies the name of the VM Image that is to be used to\n            create the Virtual Machine. If this is specified, the\n            system_config and network_config parameters are not used.\n        media_location:\n            Optional. Required if the Virtual Machine is being created from a\n            published VM Image. Specifies the location of the VHD file that is\n            created when VMImageName specifies a published VM Image.\n        dns_servers:\n            Optional. List of DNS servers (use DnsServer class) to associate\n            with the Virtual Machine.\n        reserved_ip_name:\n            Optional. Specifies the name of a reserved IP address that is to be\n            assigned to the deployment. You must run create_reserved_ip_address\n            before you can assign the address to the deployment using this\n            element.", "docstring_tokens": ["Provisions", "a", "virtual", "machine", "based", "on", "the", "supplied", "configuration", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py#L1322-L1444", "partition": "test"}
{"repo": "zinic/pynsive", "path": "pynsive/reflection.py", "func_name": "rdiscover_modules", "original_string": "def rdiscover_modules(directory):\n    \"\"\"\n    Attempts to list all of the modules and submodules found within a given\n    directory tree. This function recursively searches the directory tree\n    for potential python modules and returns a list of candidate names.\n\n    **Note:** This function returns a list of strings representing\n    discovered module names, not the actual, loaded modules.\n\n    :param directory: the directory to search for modules.\n    \"\"\"\n    found = list()\n\n    if os.path.isdir(directory):\n        for entry in os.listdir(directory):\n            next_dir = os.path.join(directory, entry)\n\n            # Scan only if there's an __init__.py file\n            if os.path.isfile(os.path.join(next_dir, MODULE_INIT_FILE)):\n                modules = _search_for_modules(next_dir, True, entry)\n                found.extend(modules)\n\n    return found", "language": "python", "code": "def rdiscover_modules(directory):\n    \"\"\"\n    Attempts to list all of the modules and submodules found within a given\n    directory tree. This function recursively searches the directory tree\n    for potential python modules and returns a list of candidate names.\n\n    **Note:** This function returns a list of strings representing\n    discovered module names, not the actual, loaded modules.\n\n    :param directory: the directory to search for modules.\n    \"\"\"\n    found = list()\n\n    if os.path.isdir(directory):\n        for entry in os.listdir(directory):\n            next_dir = os.path.join(directory, entry)\n\n            # Scan only if there's an __init__.py file\n            if os.path.isfile(os.path.join(next_dir, MODULE_INIT_FILE)):\n                modules = _search_for_modules(next_dir, True, entry)\n                found.extend(modules)\n\n    return found", "code_tokens": ["def", "rdiscover_modules", "(", "directory", ")", ":", "found", "=", "list", "(", ")", "if", "os", ".", "path", ".", "isdir", "(", "directory", ")", ":", "for", "entry", "in", "os", ".", "listdir", "(", "directory", ")", ":", "next_dir", "=", "os", ".", "path", ".", "join", "(", "directory", ",", "entry", ")", "# Scan only if there's an __init__.py file", "if", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "next_dir", ",", "MODULE_INIT_FILE", ")", ")", ":", "modules", "=", "_search_for_modules", "(", "next_dir", ",", "True", ",", "entry", ")", "found", ".", "extend", "(", "modules", ")", "return", "found"], "docstring": "Attempts to list all of the modules and submodules found within a given\n    directory tree. This function recursively searches the directory tree\n    for potential python modules and returns a list of candidate names.\n\n    **Note:** This function returns a list of strings representing\n    discovered module names, not the actual, loaded modules.\n\n    :param directory: the directory to search for modules.", "docstring_tokens": ["Attempts", "to", "list", "all", "of", "the", "modules", "and", "submodules", "found", "within", "a", "given", "directory", "tree", ".", "This", "function", "recursively", "searches", "the", "directory", "tree", "for", "potential", "python", "modules", "and", "returns", "a", "list", "of", "candidate", "names", "."], "sha": "15bc8b35a91be5817979eb327427b6235b1b411e", "url": "https://github.com/zinic/pynsive/blob/15bc8b35a91be5817979eb327427b6235b1b411e/pynsive/reflection.py#L114-L136", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/ultratb.py", "func_name": "ListTB._format_list", "original_string": "def _format_list(self, extracted_list):\n        \"\"\"Format a list of traceback entry tuples for printing.\n\n        Given a list of tuples as returned by extract_tb() or\n        extract_stack(), return a list of strings ready for printing.\n        Each string in the resulting list corresponds to the item with the\n        same index in the argument list.  Each string ends in a newline;\n        the strings may contain internal newlines as well, for those items\n        whose source text line is not None.\n\n        Lifted almost verbatim from traceback.py\n        \"\"\"\n\n        Colors = self.Colors\n        list = []\n        for filename, lineno, name, line in extracted_list[:-1]:\n            item = '  File %s\"%s\"%s, line %s%d%s, in %s%s%s\\n' % \\\n                    (Colors.filename, filename, Colors.Normal,\n                     Colors.lineno, lineno, Colors.Normal,\n                     Colors.name, name, Colors.Normal)\n            if line:\n                item += '    %s\\n' % line.strip()\n            list.append(item)\n        # Emphasize the last entry\n        filename, lineno, name, line = extracted_list[-1]\n        item = '%s  File %s\"%s\"%s, line %s%d%s, in %s%s%s%s\\n' % \\\n                (Colors.normalEm,\n                 Colors.filenameEm, filename, Colors.normalEm,\n                 Colors.linenoEm, lineno, Colors.normalEm,\n                 Colors.nameEm, name, Colors.normalEm,\n                 Colors.Normal)\n        if line:\n            item += '%s    %s%s\\n' % (Colors.line, line.strip(),\n                                            Colors.Normal)\n        list.append(item)\n        #from pprint import pformat; print 'LISTTB', pformat(list) # dbg\n        return list", "language": "python", "code": "def _format_list(self, extracted_list):\n        \"\"\"Format a list of traceback entry tuples for printing.\n\n        Given a list of tuples as returned by extract_tb() or\n        extract_stack(), return a list of strings ready for printing.\n        Each string in the resulting list corresponds to the item with the\n        same index in the argument list.  Each string ends in a newline;\n        the strings may contain internal newlines as well, for those items\n        whose source text line is not None.\n\n        Lifted almost verbatim from traceback.py\n        \"\"\"\n\n        Colors = self.Colors\n        list = []\n        for filename, lineno, name, line in extracted_list[:-1]:\n            item = '  File %s\"%s\"%s, line %s%d%s, in %s%s%s\\n' % \\\n                    (Colors.filename, filename, Colors.Normal,\n                     Colors.lineno, lineno, Colors.Normal,\n                     Colors.name, name, Colors.Normal)\n            if line:\n                item += '    %s\\n' % line.strip()\n            list.append(item)\n        # Emphasize the last entry\n        filename, lineno, name, line = extracted_list[-1]\n        item = '%s  File %s\"%s\"%s, line %s%d%s, in %s%s%s%s\\n' % \\\n                (Colors.normalEm,\n                 Colors.filenameEm, filename, Colors.normalEm,\n                 Colors.linenoEm, lineno, Colors.normalEm,\n                 Colors.nameEm, name, Colors.normalEm,\n                 Colors.Normal)\n        if line:\n            item += '%s    %s%s\\n' % (Colors.line, line.strip(),\n                                            Colors.Normal)\n        list.append(item)\n        #from pprint import pformat; print 'LISTTB', pformat(list) # dbg\n        return list", "code_tokens": ["def", "_format_list", "(", "self", ",", "extracted_list", ")", ":", "Colors", "=", "self", ".", "Colors", "list", "=", "[", "]", "for", "filename", ",", "lineno", ",", "name", ",", "line", "in", "extracted_list", "[", ":", "-", "1", "]", ":", "item", "=", "'  File %s\"%s\"%s, line %s%d%s, in %s%s%s\\n'", "%", "(", "Colors", ".", "filename", ",", "filename", ",", "Colors", ".", "Normal", ",", "Colors", ".", "lineno", ",", "lineno", ",", "Colors", ".", "Normal", ",", "Colors", ".", "name", ",", "name", ",", "Colors", ".", "Normal", ")", "if", "line", ":", "item", "+=", "'    %s\\n'", "%", "line", ".", "strip", "(", ")", "list", ".", "append", "(", "item", ")", "# Emphasize the last entry", "filename", ",", "lineno", ",", "name", ",", "line", "=", "extracted_list", "[", "-", "1", "]", "item", "=", "'%s  File %s\"%s\"%s, line %s%d%s, in %s%s%s%s\\n'", "%", "(", "Colors", ".", "normalEm", ",", "Colors", ".", "filenameEm", ",", "filename", ",", "Colors", ".", "normalEm", ",", "Colors", ".", "linenoEm", ",", "lineno", ",", "Colors", ".", "normalEm", ",", "Colors", ".", "nameEm", ",", "name", ",", "Colors", ".", "normalEm", ",", "Colors", ".", "Normal", ")", "if", "line", ":", "item", "+=", "'%s    %s%s\\n'", "%", "(", "Colors", ".", "line", ",", "line", ".", "strip", "(", ")", ",", "Colors", ".", "Normal", ")", "list", ".", "append", "(", "item", ")", "#from pprint import pformat; print 'LISTTB', pformat(list) # dbg", "return", "list"], "docstring": "Format a list of traceback entry tuples for printing.\n\n        Given a list of tuples as returned by extract_tb() or\n        extract_stack(), return a list of strings ready for printing.\n        Each string in the resulting list corresponds to the item with the\n        same index in the argument list.  Each string ends in a newline;\n        the strings may contain internal newlines as well, for those items\n        whose source text line is not None.\n\n        Lifted almost verbatim from traceback.py", "docstring_tokens": ["Format", "a", "list", "of", "traceback", "entry", "tuples", "for", "printing", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/ultratb.py#L506-L542", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/jinja2/compiler.py", "func_name": "Identifiers.is_declared", "original_string": "def is_declared(self, name):\n        \"\"\"Check if a name is declared in this or an outer scope.\"\"\"\n        if name in self.declared_locally or name in self.declared_parameter:\n            return True\n        return name in self.declared", "language": "python", "code": "def is_declared(self, name):\n        \"\"\"Check if a name is declared in this or an outer scope.\"\"\"\n        if name in self.declared_locally or name in self.declared_parameter:\n            return True\n        return name in self.declared", "code_tokens": ["def", "is_declared", "(", "self", ",", "name", ")", ":", "if", "name", "in", "self", ".", "declared_locally", "or", "name", "in", "self", ".", "declared_parameter", ":", "return", "True", "return", "name", "in", "self", ".", "declared"], "docstring": "Check if a name is declared in this or an outer scope.", "docstring_tokens": ["Check", "if", "a", "name", "is", "declared", "in", "this", "or", "an", "outer", "scope", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/jinja2/compiler.py#L128-L132", "partition": "test"}
{"repo": "IdentityPython/fedoidcmsg", "path": "src/fedoidcmsg/operator.py", "func_name": "Operator.extend_with_ms", "original_string": "def extend_with_ms(self, req, sms_dict):\n        \"\"\"\n        Add signed metadata statements to a request\n\n        :param req: The request \n        :param sms_dict: A dictionary with FO IDs as keys and signed metadata\n            statements (sms) or uris pointing to sms as values.\n        :return: The updated request\n        \"\"\"\n        _ms_uri = {}\n        _ms = {}\n        for fo, sms in sms_dict.items():\n            if sms.startswith('http://') or sms.startswith('https://'):\n                _ms_uri[fo] = sms\n            else:\n                _ms[fo] = sms\n\n        if _ms:\n            req['metadata_statements'] = Message(**_ms)\n        if _ms_uri:\n            req['metadata_statement_uris'] = Message(**_ms_uri)\n        return req", "language": "python", "code": "def extend_with_ms(self, req, sms_dict):\n        \"\"\"\n        Add signed metadata statements to a request\n\n        :param req: The request \n        :param sms_dict: A dictionary with FO IDs as keys and signed metadata\n            statements (sms) or uris pointing to sms as values.\n        :return: The updated request\n        \"\"\"\n        _ms_uri = {}\n        _ms = {}\n        for fo, sms in sms_dict.items():\n            if sms.startswith('http://') or sms.startswith('https://'):\n                _ms_uri[fo] = sms\n            else:\n                _ms[fo] = sms\n\n        if _ms:\n            req['metadata_statements'] = Message(**_ms)\n        if _ms_uri:\n            req['metadata_statement_uris'] = Message(**_ms_uri)\n        return req", "code_tokens": ["def", "extend_with_ms", "(", "self", ",", "req", ",", "sms_dict", ")", ":", "_ms_uri", "=", "{", "}", "_ms", "=", "{", "}", "for", "fo", ",", "sms", "in", "sms_dict", ".", "items", "(", ")", ":", "if", "sms", ".", "startswith", "(", "'http://'", ")", "or", "sms", ".", "startswith", "(", "'https://'", ")", ":", "_ms_uri", "[", "fo", "]", "=", "sms", "else", ":", "_ms", "[", "fo", "]", "=", "sms", "if", "_ms", ":", "req", "[", "'metadata_statements'", "]", "=", "Message", "(", "*", "*", "_ms", ")", "if", "_ms_uri", ":", "req", "[", "'metadata_statement_uris'", "]", "=", "Message", "(", "*", "*", "_ms_uri", ")", "return", "req"], "docstring": "Add signed metadata statements to a request\n\n        :param req: The request \n        :param sms_dict: A dictionary with FO IDs as keys and signed metadata\n            statements (sms) or uris pointing to sms as values.\n        :return: The updated request", "docstring_tokens": ["Add", "signed", "metadata", "statements", "to", "a", "request"], "sha": "d30107be02521fa6cdfe285da3b6b0cdd153c8cc", "url": "https://github.com/IdentityPython/fedoidcmsg/blob/d30107be02521fa6cdfe285da3b6b0cdd153c8cc/src/fedoidcmsg/operator.py#L461-L482", "partition": "test"}
{"repo": "LordSputnik/mutagen", "path": "mutagen/id3.py", "func_name": "ID3.update_to_v24", "original_string": "def update_to_v24(self):\n        \"\"\"Convert older tags into an ID3v2.4 tag.\n\n        This updates old ID3v2 frames to ID3v2.4 ones (e.g. TYER to\n        TDRC). If you intend to save tags, you must call this function\n        at some point; it is called by default when loading the tag.\n        \"\"\"\n\n        self.__update_common()\n\n        if self.__unknown_version == self._V23:\n            # convert unknown 2.3 frames (flags/size) to 2.4\n            converted = []\n            for frame in self.unknown_frames:\n                try:\n                    name, size, flags = unpack('>4sLH', frame[:10])\n                    frame = BinaryFrame.fromData(self, flags, frame[10:])\n                except (struct.error, error):\n                    continue\n                name = name.decode('ascii')\n                converted.append(self.__save_frame(frame, name=name))\n            self.unknown_frames[:] = converted\n            self.__unknown_version = self._V24\n\n        # TDAT, TYER, and TIME have been turned into TDRC.\n        try:\n            date = text_type(self.get(\"TYER\", \"\"))\n            if date.strip(u\"\\x00\"):\n                self.pop(\"TYER\")\n                dat = text_type(self.get(\"TDAT\", \"\"))\n                if dat.strip(\"\\x00\"):\n                    self.pop(\"TDAT\")\n                    date = \"%s-%s-%s\" % (date, dat[2:], dat[:2])\n                    time = text_type(self.get(\"TIME\", \"\"))\n                    if time.strip(\"\\x00\"):\n                        self.pop(\"TIME\")\n                        date += \"T%s:%s:00\" % (time[:2], time[2:])\n                if \"TDRC\" not in self:\n                    self.add(TDRC(encoding=0, text=date))\n        except UnicodeDecodeError:\n            # Old ID3 tags have *lots* of Unicode problems, so if TYER\n            # is bad, just chuck the frames.\n            pass\n\n        # TORY can be the first part of a TDOR.\n        if \"TORY\" in self:\n            f = self.pop(\"TORY\")\n            if \"TDOR\" not in self:\n                try:\n                    self.add(TDOR(encoding=0, text=str(f)))\n                except UnicodeDecodeError:\n                    pass\n\n        # IPLS is now TIPL.\n        if \"IPLS\" in self:\n            f = self.pop(\"IPLS\")\n            if \"TIPL\" not in self:\n                self.add(TIPL(encoding=f.encoding, people=f.people))\n\n        # These can't be trivially translated to any ID3v2.4 tags, or\n        # should have been removed already.\n        for key in [\"RVAD\", \"EQUA\", \"TRDA\", \"TSIZ\", \"TDAT\", \"TIME\", \"CRM\"]:\n            if key in self:\n                del(self[key])", "language": "python", "code": "def update_to_v24(self):\n        \"\"\"Convert older tags into an ID3v2.4 tag.\n\n        This updates old ID3v2 frames to ID3v2.4 ones (e.g. TYER to\n        TDRC). If you intend to save tags, you must call this function\n        at some point; it is called by default when loading the tag.\n        \"\"\"\n\n        self.__update_common()\n\n        if self.__unknown_version == self._V23:\n            # convert unknown 2.3 frames (flags/size) to 2.4\n            converted = []\n            for frame in self.unknown_frames:\n                try:\n                    name, size, flags = unpack('>4sLH', frame[:10])\n                    frame = BinaryFrame.fromData(self, flags, frame[10:])\n                except (struct.error, error):\n                    continue\n                name = name.decode('ascii')\n                converted.append(self.__save_frame(frame, name=name))\n            self.unknown_frames[:] = converted\n            self.__unknown_version = self._V24\n\n        # TDAT, TYER, and TIME have been turned into TDRC.\n        try:\n            date = text_type(self.get(\"TYER\", \"\"))\n            if date.strip(u\"\\x00\"):\n                self.pop(\"TYER\")\n                dat = text_type(self.get(\"TDAT\", \"\"))\n                if dat.strip(\"\\x00\"):\n                    self.pop(\"TDAT\")\n                    date = \"%s-%s-%s\" % (date, dat[2:], dat[:2])\n                    time = text_type(self.get(\"TIME\", \"\"))\n                    if time.strip(\"\\x00\"):\n                        self.pop(\"TIME\")\n                        date += \"T%s:%s:00\" % (time[:2], time[2:])\n                if \"TDRC\" not in self:\n                    self.add(TDRC(encoding=0, text=date))\n        except UnicodeDecodeError:\n            # Old ID3 tags have *lots* of Unicode problems, so if TYER\n            # is bad, just chuck the frames.\n            pass\n\n        # TORY can be the first part of a TDOR.\n        if \"TORY\" in self:\n            f = self.pop(\"TORY\")\n            if \"TDOR\" not in self:\n                try:\n                    self.add(TDOR(encoding=0, text=str(f)))\n                except UnicodeDecodeError:\n                    pass\n\n        # IPLS is now TIPL.\n        if \"IPLS\" in self:\n            f = self.pop(\"IPLS\")\n            if \"TIPL\" not in self:\n                self.add(TIPL(encoding=f.encoding, people=f.people))\n\n        # These can't be trivially translated to any ID3v2.4 tags, or\n        # should have been removed already.\n        for key in [\"RVAD\", \"EQUA\", \"TRDA\", \"TSIZ\", \"TDAT\", \"TIME\", \"CRM\"]:\n            if key in self:\n                del(self[key])", "code_tokens": ["def", "update_to_v24", "(", "self", ")", ":", "self", ".", "__update_common", "(", ")", "if", "self", ".", "__unknown_version", "==", "self", ".", "_V23", ":", "# convert unknown 2.3 frames (flags/size) to 2.4", "converted", "=", "[", "]", "for", "frame", "in", "self", ".", "unknown_frames", ":", "try", ":", "name", ",", "size", ",", "flags", "=", "unpack", "(", "'>4sLH'", ",", "frame", "[", ":", "10", "]", ")", "frame", "=", "BinaryFrame", ".", "fromData", "(", "self", ",", "flags", ",", "frame", "[", "10", ":", "]", ")", "except", "(", "struct", ".", "error", ",", "error", ")", ":", "continue", "name", "=", "name", ".", "decode", "(", "'ascii'", ")", "converted", ".", "append", "(", "self", ".", "__save_frame", "(", "frame", ",", "name", "=", "name", ")", ")", "self", ".", "unknown_frames", "[", ":", "]", "=", "converted", "self", ".", "__unknown_version", "=", "self", ".", "_V24", "# TDAT, TYER, and TIME have been turned into TDRC.", "try", ":", "date", "=", "text_type", "(", "self", ".", "get", "(", "\"TYER\"", ",", "\"\"", ")", ")", "if", "date", ".", "strip", "(", "u\"\\x00\"", ")", ":", "self", ".", "pop", "(", "\"TYER\"", ")", "dat", "=", "text_type", "(", "self", ".", "get", "(", "\"TDAT\"", ",", "\"\"", ")", ")", "if", "dat", ".", "strip", "(", "\"\\x00\"", ")", ":", "self", ".", "pop", "(", "\"TDAT\"", ")", "date", "=", "\"%s-%s-%s\"", "%", "(", "date", ",", "dat", "[", "2", ":", "]", ",", "dat", "[", ":", "2", "]", ")", "time", "=", "text_type", "(", "self", ".", "get", "(", "\"TIME\"", ",", "\"\"", ")", ")", "if", "time", ".", "strip", "(", "\"\\x00\"", ")", ":", "self", ".", "pop", "(", "\"TIME\"", ")", "date", "+=", "\"T%s:%s:00\"", "%", "(", "time", "[", ":", "2", "]", ",", "time", "[", "2", ":", "]", ")", "if", "\"TDRC\"", "not", "in", "self", ":", "self", ".", "add", "(", "TDRC", "(", "encoding", "=", "0", ",", "text", "=", "date", ")", ")", "except", "UnicodeDecodeError", ":", "# Old ID3 tags have *lots* of Unicode problems, so if TYER", "# is bad, just chuck the frames.", "pass", "# TORY can be the first part of a TDOR.", "if", "\"TORY\"", "in", "self", ":", "f", "=", "self", ".", "pop", "(", "\"TORY\"", ")", "if", "\"TDOR\"", "not", "in", "self", ":", "try", ":", "self", ".", "add", "(", "TDOR", "(", "encoding", "=", "0", ",", "text", "=", "str", "(", "f", ")", ")", ")", "except", "UnicodeDecodeError", ":", "pass", "# IPLS is now TIPL.", "if", "\"IPLS\"", "in", "self", ":", "f", "=", "self", ".", "pop", "(", "\"IPLS\"", ")", "if", "\"TIPL\"", "not", "in", "self", ":", "self", ".", "add", "(", "TIPL", "(", "encoding", "=", "f", ".", "encoding", ",", "people", "=", "f", ".", "people", ")", ")", "# These can't be trivially translated to any ID3v2.4 tags, or", "# should have been removed already.", "for", "key", "in", "[", "\"RVAD\"", ",", "\"EQUA\"", ",", "\"TRDA\"", ",", "\"TSIZ\"", ",", "\"TDAT\"", ",", "\"TIME\"", ",", "\"CRM\"", "]", ":", "if", "key", "in", "self", ":", "del", "(", "self", "[", "key", "]", ")"], "docstring": "Convert older tags into an ID3v2.4 tag.\n\n        This updates old ID3v2 frames to ID3v2.4 ones (e.g. TYER to\n        TDRC). If you intend to save tags, you must call this function\n        at some point; it is called by default when loading the tag.", "docstring_tokens": ["Convert", "older", "tags", "into", "an", "ID3v2", ".", "4", "tag", "."], "sha": "38e62c8dc35c72b16554f5dbe7c0fde91acc3411", "url": "https://github.com/LordSputnik/mutagen/blob/38e62c8dc35c72b16554f5dbe7c0fde91acc3411/mutagen/id3.py#L611-L674", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/pip/basecommand.py", "func_name": "RequirementCommand.populate_requirement_set", "original_string": "def populate_requirement_set(requirement_set, args, options, finder,\n                                 session, name, wheel_cache):\n        \"\"\"\n        Marshal cmd line args into a requirement set.\n        \"\"\"\n        for req in args:\n            requirement_set.add_requirement(\n                InstallRequirement.from_line(\n                    req, None, isolated=options.isolated_mode,\n                    wheel_cache=wheel_cache\n                )\n            )\n\n        for req in options.editables:\n            requirement_set.add_requirement(\n                InstallRequirement.from_editable(\n                    req,\n                    default_vcs=options.default_vcs,\n                    isolated=options.isolated_mode,\n                    wheel_cache=wheel_cache\n                )\n            )\n\n        found_req_in_file = False\n        for filename in options.requirements:\n            for req in parse_requirements(\n                    filename,\n                    finder=finder, options=options, session=session,\n                    wheel_cache=wheel_cache):\n                found_req_in_file = True\n                requirement_set.add_requirement(req)\n\n        if not (args or options.editables or found_req_in_file):\n            opts = {'name': name}\n            if options.find_links:\n                msg = ('You must give at least one requirement to '\n                       '%(name)s (maybe you meant \"pip %(name)s '\n                       '%(links)s\"?)' %\n                       dict(opts, links=' '.join(options.find_links)))\n            else:\n                msg = ('You must give at least one requirement '\n                       'to %(name)s (see \"pip help %(name)s\")' % opts)\n            logger.warning(msg)", "language": "python", "code": "def populate_requirement_set(requirement_set, args, options, finder,\n                                 session, name, wheel_cache):\n        \"\"\"\n        Marshal cmd line args into a requirement set.\n        \"\"\"\n        for req in args:\n            requirement_set.add_requirement(\n                InstallRequirement.from_line(\n                    req, None, isolated=options.isolated_mode,\n                    wheel_cache=wheel_cache\n                )\n            )\n\n        for req in options.editables:\n            requirement_set.add_requirement(\n                InstallRequirement.from_editable(\n                    req,\n                    default_vcs=options.default_vcs,\n                    isolated=options.isolated_mode,\n                    wheel_cache=wheel_cache\n                )\n            )\n\n        found_req_in_file = False\n        for filename in options.requirements:\n            for req in parse_requirements(\n                    filename,\n                    finder=finder, options=options, session=session,\n                    wheel_cache=wheel_cache):\n                found_req_in_file = True\n                requirement_set.add_requirement(req)\n\n        if not (args or options.editables or found_req_in_file):\n            opts = {'name': name}\n            if options.find_links:\n                msg = ('You must give at least one requirement to '\n                       '%(name)s (maybe you meant \"pip %(name)s '\n                       '%(links)s\"?)' %\n                       dict(opts, links=' '.join(options.find_links)))\n            else:\n                msg = ('You must give at least one requirement '\n                       'to %(name)s (see \"pip help %(name)s\")' % opts)\n            logger.warning(msg)", "code_tokens": ["def", "populate_requirement_set", "(", "requirement_set", ",", "args", ",", "options", ",", "finder", ",", "session", ",", "name", ",", "wheel_cache", ")", ":", "for", "req", "in", "args", ":", "requirement_set", ".", "add_requirement", "(", "InstallRequirement", ".", "from_line", "(", "req", ",", "None", ",", "isolated", "=", "options", ".", "isolated_mode", ",", "wheel_cache", "=", "wheel_cache", ")", ")", "for", "req", "in", "options", ".", "editables", ":", "requirement_set", ".", "add_requirement", "(", "InstallRequirement", ".", "from_editable", "(", "req", ",", "default_vcs", "=", "options", ".", "default_vcs", ",", "isolated", "=", "options", ".", "isolated_mode", ",", "wheel_cache", "=", "wheel_cache", ")", ")", "found_req_in_file", "=", "False", "for", "filename", "in", "options", ".", "requirements", ":", "for", "req", "in", "parse_requirements", "(", "filename", ",", "finder", "=", "finder", ",", "options", "=", "options", ",", "session", "=", "session", ",", "wheel_cache", "=", "wheel_cache", ")", ":", "found_req_in_file", "=", "True", "requirement_set", ".", "add_requirement", "(", "req", ")", "if", "not", "(", "args", "or", "options", ".", "editables", "or", "found_req_in_file", ")", ":", "opts", "=", "{", "'name'", ":", "name", "}", "if", "options", ".", "find_links", ":", "msg", "=", "(", "'You must give at least one requirement to '", "'%(name)s (maybe you meant \"pip %(name)s '", "'%(links)s\"?)'", "%", "dict", "(", "opts", ",", "links", "=", "' '", ".", "join", "(", "options", ".", "find_links", ")", ")", ")", "else", ":", "msg", "=", "(", "'You must give at least one requirement '", "'to %(name)s (see \"pip help %(name)s\")'", "%", "opts", ")", "logger", ".", "warning", "(", "msg", ")"], "docstring": "Marshal cmd line args into a requirement set.", "docstring_tokens": ["Marshal", "cmd", "line", "args", "into", "a", "requirement", "set", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/pip/basecommand.py#L259-L301", "partition": "test"}
{"repo": "josiahcarlson/rom", "path": "rom/model.py", "func_name": "Model.delete", "original_string": "def delete(self, **kwargs):\n        '''\n        Deletes the entity immediately. Also performs any on_delete operations\n        specified as part of column definitions.\n        '''\n        if kwargs.get('skip_on_delete_i_really_mean_it') is not SKIP_ON_DELETE:\n            # handle the pre-commit hook\n            self._before_delete()\n            # handle any foreign key references + cascade options\n            _on_delete(self)\n\n        session.forget(self)\n        self._apply_changes(self._last, {}, delete=True, _conn=kwargs.get('_conn'))\n        self._modified = True\n        self._deleted = True\n        # handle the post-commit hooks\n        if kwargs.get('skip_on_delete_i_really_mean_it') is not SKIP_ON_DELETE:\n            self._after_delete()", "language": "python", "code": "def delete(self, **kwargs):\n        '''\n        Deletes the entity immediately. Also performs any on_delete operations\n        specified as part of column definitions.\n        '''\n        if kwargs.get('skip_on_delete_i_really_mean_it') is not SKIP_ON_DELETE:\n            # handle the pre-commit hook\n            self._before_delete()\n            # handle any foreign key references + cascade options\n            _on_delete(self)\n\n        session.forget(self)\n        self._apply_changes(self._last, {}, delete=True, _conn=kwargs.get('_conn'))\n        self._modified = True\n        self._deleted = True\n        # handle the post-commit hooks\n        if kwargs.get('skip_on_delete_i_really_mean_it') is not SKIP_ON_DELETE:\n            self._after_delete()", "code_tokens": ["def", "delete", "(", "self", ",", "*", "*", "kwargs", ")", ":", "if", "kwargs", ".", "get", "(", "'skip_on_delete_i_really_mean_it'", ")", "is", "not", "SKIP_ON_DELETE", ":", "# handle the pre-commit hook", "self", ".", "_before_delete", "(", ")", "# handle any foreign key references + cascade options", "_on_delete", "(", "self", ")", "session", ".", "forget", "(", "self", ")", "self", ".", "_apply_changes", "(", "self", ".", "_last", ",", "{", "}", ",", "delete", "=", "True", ",", "_conn", "=", "kwargs", ".", "get", "(", "'_conn'", ")", ")", "self", ".", "_modified", "=", "True", "self", ".", "_deleted", "=", "True", "# handle the post-commit hooks", "if", "kwargs", ".", "get", "(", "'skip_on_delete_i_really_mean_it'", ")", "is", "not", "SKIP_ON_DELETE", ":", "self", ".", "_after_delete", "(", ")"], "docstring": "Deletes the entity immediately. Also performs any on_delete operations\n        specified as part of column definitions.", "docstring_tokens": ["Deletes", "the", "entity", "immediately", ".", "Also", "performs", "any", "on_delete", "operations", "specified", "as", "part", "of", "column", "definitions", "."], "sha": "8b5607a856341df85df33422accc30ba9294dbdb", "url": "https://github.com/josiahcarlson/rom/blob/8b5607a856341df85df33422accc30ba9294dbdb/rom/model.py#L504-L521", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/flask/ctx.py", "func_name": "copy_current_request_context", "original_string": "def copy_current_request_context(f):\n    \"\"\"A helper function that decorates a function to retain the current\n    request context.  This is useful when working with greenlets.  The moment\n    the function is decorated a copy of the request context is created and\n    then pushed when the function is called.\n\n    Example::\n\n        import gevent\n        from flask import copy_current_request_context\n\n        @app.route('/')\n        def index():\n            @copy_current_request_context\n            def do_some_work():\n                # do some work here, it can access flask.request like you\n                # would otherwise in the view function.\n                ...\n            gevent.spawn(do_some_work)\n            return 'Regular response'\n\n    .. versionadded:: 0.10\n    \"\"\"\n    top = _request_ctx_stack.top\n    if top is None:\n        raise RuntimeError('This decorator can only be used at local scopes '\n            'when a request context is on the stack.  For instance within '\n            'view functions.')\n    reqctx = top.copy()\n    def wrapper(*args, **kwargs):\n        with reqctx:\n            return f(*args, **kwargs)\n    return update_wrapper(wrapper, f)", "language": "python", "code": "def copy_current_request_context(f):\n    \"\"\"A helper function that decorates a function to retain the current\n    request context.  This is useful when working with greenlets.  The moment\n    the function is decorated a copy of the request context is created and\n    then pushed when the function is called.\n\n    Example::\n\n        import gevent\n        from flask import copy_current_request_context\n\n        @app.route('/')\n        def index():\n            @copy_current_request_context\n            def do_some_work():\n                # do some work here, it can access flask.request like you\n                # would otherwise in the view function.\n                ...\n            gevent.spawn(do_some_work)\n            return 'Regular response'\n\n    .. versionadded:: 0.10\n    \"\"\"\n    top = _request_ctx_stack.top\n    if top is None:\n        raise RuntimeError('This decorator can only be used at local scopes '\n            'when a request context is on the stack.  For instance within '\n            'view functions.')\n    reqctx = top.copy()\n    def wrapper(*args, **kwargs):\n        with reqctx:\n            return f(*args, **kwargs)\n    return update_wrapper(wrapper, f)", "code_tokens": ["def", "copy_current_request_context", "(", "f", ")", ":", "top", "=", "_request_ctx_stack", ".", "top", "if", "top", "is", "None", ":", "raise", "RuntimeError", "(", "'This decorator can only be used at local scopes '", "'when a request context is on the stack.  For instance within '", "'view functions.'", ")", "reqctx", "=", "top", ".", "copy", "(", ")", "def", "wrapper", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "with", "reqctx", ":", "return", "f", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "update_wrapper", "(", "wrapper", ",", "f", ")"], "docstring": "A helper function that decorates a function to retain the current\n    request context.  This is useful when working with greenlets.  The moment\n    the function is decorated a copy of the request context is created and\n    then pushed when the function is called.\n\n    Example::\n\n        import gevent\n        from flask import copy_current_request_context\n\n        @app.route('/')\n        def index():\n            @copy_current_request_context\n            def do_some_work():\n                # do some work here, it can access flask.request like you\n                # would otherwise in the view function.\n                ...\n            gevent.spawn(do_some_work)\n            return 'Regular response'\n\n    .. versionadded:: 0.10", "docstring_tokens": ["A", "helper", "function", "that", "decorates", "a", "function", "to", "retain", "the", "current", "request", "context", ".", "This", "is", "useful", "when", "working", "with", "greenlets", ".", "The", "moment", "the", "function", "is", "decorated", "a", "copy", "of", "the", "request", "context", "is", "created", "and", "then", "pushed", "when", "the", "function", "is", "called", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/flask/ctx.py#L68-L100", "partition": "test"}
{"repo": "codelv/enaml-web", "path": "web/impl/lxml_raw.py", "func_name": "RawComponent.set_source", "original_string": "def set_source(self, source):\n        \"\"\" Set the source by parsing the source and inserting it into the \n        component. \n        \"\"\"\n        self.widget.clear()\n        html = etree.HTML(source)\n        self.widget.extend(html[0])\n\n        # Clear removes everything so it must be reinitialized\n        super(RawComponent, self).init_widget()", "language": "python", "code": "def set_source(self, source):\n        \"\"\" Set the source by parsing the source and inserting it into the \n        component. \n        \"\"\"\n        self.widget.clear()\n        html = etree.HTML(source)\n        self.widget.extend(html[0])\n\n        # Clear removes everything so it must be reinitialized\n        super(RawComponent, self).init_widget()", "code_tokens": ["def", "set_source", "(", "self", ",", "source", ")", ":", "self", ".", "widget", ".", "clear", "(", ")", "html", "=", "etree", ".", "HTML", "(", "source", ")", "self", ".", "widget", ".", "extend", "(", "html", "[", "0", "]", ")", "# Clear removes everything so it must be reinitialized", "super", "(", "RawComponent", ",", "self", ")", ".", "init_widget", "(", ")"], "docstring": "Set the source by parsing the source and inserting it into the \n        component.", "docstring_tokens": ["Set", "the", "source", "by", "parsing", "the", "source", "and", "inserting", "it", "into", "the", "component", "."], "sha": "88f1131a7b3ba9e83467b4f44bc3bab6f0de7559", "url": "https://github.com/codelv/enaml-web/blob/88f1131a7b3ba9e83467b4f44bc3bab6f0de7559/web/impl/lxml_raw.py#L29-L38", "partition": "test"}
{"repo": "mk-fg/python-onedrive", "path": "onedrive/api_v5.py", "func_name": "OneDriveAuth.auth_get_token", "original_string": "def auth_get_token(self, check_scope=True):\n\t\t'Refresh or acquire access_token.'\n\t\tres = self.auth_access_data_raw = self._auth_token_request()\n\t\treturn self._auth_token_process(res, check_scope=check_scope)", "language": "python", "code": "def auth_get_token(self, check_scope=True):\n\t\t'Refresh or acquire access_token.'\n\t\tres = self.auth_access_data_raw = self._auth_token_request()\n\t\treturn self._auth_token_process(res, check_scope=check_scope)", "code_tokens": ["def", "auth_get_token", "(", "self", ",", "check_scope", "=", "True", ")", ":", "res", "=", "self", ".", "auth_access_data_raw", "=", "self", ".", "_auth_token_request", "(", ")", "return", "self", ".", "_auth_token_process", "(", "res", ",", "check_scope", "=", "check_scope", ")"], "docstring": "Refresh or acquire access_token.", "docstring_tokens": ["Refresh", "or", "acquire", "access_token", "."], "sha": "74d3f6605b0e8a9031a2aab8092f551293ffb533", "url": "https://github.com/mk-fg/python-onedrive/blob/74d3f6605b0e8a9031a2aab8092f551293ffb533/onedrive/api_v5.py#L271-L274", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-mgmt-iothubprovisioningservices/azure/mgmt/iothubprovisioningservices/operations/dps_certificate_operations.py", "func_name": "DpsCertificateOperations.delete", "original_string": "def delete(\n            self, resource_group_name, if_match, provisioning_service_name, certificate_name, certificatename=None, certificateraw_bytes=None, certificateis_verified=None, certificatepurpose=None, certificatecreated=None, certificatelast_updated=None, certificatehas_private_key=None, certificatenonce=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Delete the Provisioning Service Certificate.\n\n        Deletes the specified certificate assosciated with the Provisioning\n        Service.\n\n        :param resource_group_name: Resource group identifier.\n        :type resource_group_name: str\n        :param if_match: ETag of the certificate\n        :type if_match: str\n        :param provisioning_service_name: The name of the provisioning\n         service.\n        :type provisioning_service_name: str\n        :param certificate_name: This is a mandatory field, and is the logical\n         name of the certificate that the provisioning service will access by.\n        :type certificate_name: str\n        :param certificatename: This is optional, and it is the Common Name of\n         the certificate.\n        :type certificatename: str\n        :param certificateraw_bytes: Raw data within the certificate.\n        :type certificateraw_bytes: bytearray\n        :param certificateis_verified: Indicates if certificate has been\n         verified by owner of the private key.\n        :type certificateis_verified: bool\n        :param certificatepurpose: A description that mentions the purpose of\n         the certificate. Possible values include: 'clientAuthentication',\n         'serverAuthentication'\n        :type certificatepurpose: str or\n         ~azure.mgmt.iothubprovisioningservices.models.CertificatePurpose\n        :param certificatecreated: Time the certificate is created.\n        :type certificatecreated: datetime\n        :param certificatelast_updated: Time the certificate is last updated.\n        :type certificatelast_updated: datetime\n        :param certificatehas_private_key: Indicates if the certificate\n         contains a private key.\n        :type certificatehas_private_key: bool\n        :param certificatenonce: Random number generated to indicate Proof of\n         Possession.\n        :type certificatenonce: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`ErrorDetailsException<azure.mgmt.iothubprovisioningservices.models.ErrorDetailsException>`\n        \"\"\"\n        # Construct URL\n        url = self.delete.metadata['url']\n        path_format_arguments = {\n            'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str'),\n            'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n            'provisioningServiceName': self._serialize.url(\"provisioning_service_name\", provisioning_service_name, 'str'),\n            'certificateName': self._serialize.url(\"certificate_name\", certificate_name, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        if certificatename is not None:\n            query_parameters['certificate.name'] = self._serialize.query(\"certificatename\", certificatename, 'str')\n        if certificateraw_bytes is not None:\n            query_parameters['certificate.rawBytes'] = self._serialize.query(\"certificateraw_bytes\", certificateraw_bytes, 'bytearray')\n        if certificateis_verified is not None:\n            query_parameters['certificate.isVerified'] = self._serialize.query(\"certificateis_verified\", certificateis_verified, 'bool')\n        if certificatepurpose is not None:\n            query_parameters['certificate.purpose'] = self._serialize.query(\"certificatepurpose\", certificatepurpose, 'str')\n        if certificatecreated is not None:\n            query_parameters['certificate.created'] = self._serialize.query(\"certificatecreated\", certificatecreated, 'iso-8601')\n        if certificatelast_updated is not None:\n            query_parameters['certificate.lastUpdated'] = self._serialize.query(\"certificatelast_updated\", certificatelast_updated, 'iso-8601')\n        if certificatehas_private_key is not None:\n            query_parameters['certificate.hasPrivateKey'] = self._serialize.query(\"certificatehas_private_key\", certificatehas_private_key, 'bool')\n        if certificatenonce is not None:\n            query_parameters['certificate.nonce'] = self._serialize.query(\"certificatenonce\", certificatenonce, 'str')\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        header_parameters['If-Match'] = self._serialize.header(\"if_match\", if_match, 'str')\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        # Construct and send request\n        request = self._client.delete(url, query_parameters)\n        response = self._client.send(request, header_parameters, stream=False, **operation_config)\n\n        if response.status_code not in [200, 204]:\n            raise models.ErrorDetailsException(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response", "language": "python", "code": "def delete(\n            self, resource_group_name, if_match, provisioning_service_name, certificate_name, certificatename=None, certificateraw_bytes=None, certificateis_verified=None, certificatepurpose=None, certificatecreated=None, certificatelast_updated=None, certificatehas_private_key=None, certificatenonce=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Delete the Provisioning Service Certificate.\n\n        Deletes the specified certificate assosciated with the Provisioning\n        Service.\n\n        :param resource_group_name: Resource group identifier.\n        :type resource_group_name: str\n        :param if_match: ETag of the certificate\n        :type if_match: str\n        :param provisioning_service_name: The name of the provisioning\n         service.\n        :type provisioning_service_name: str\n        :param certificate_name: This is a mandatory field, and is the logical\n         name of the certificate that the provisioning service will access by.\n        :type certificate_name: str\n        :param certificatename: This is optional, and it is the Common Name of\n         the certificate.\n        :type certificatename: str\n        :param certificateraw_bytes: Raw data within the certificate.\n        :type certificateraw_bytes: bytearray\n        :param certificateis_verified: Indicates if certificate has been\n         verified by owner of the private key.\n        :type certificateis_verified: bool\n        :param certificatepurpose: A description that mentions the purpose of\n         the certificate. Possible values include: 'clientAuthentication',\n         'serverAuthentication'\n        :type certificatepurpose: str or\n         ~azure.mgmt.iothubprovisioningservices.models.CertificatePurpose\n        :param certificatecreated: Time the certificate is created.\n        :type certificatecreated: datetime\n        :param certificatelast_updated: Time the certificate is last updated.\n        :type certificatelast_updated: datetime\n        :param certificatehas_private_key: Indicates if the certificate\n         contains a private key.\n        :type certificatehas_private_key: bool\n        :param certificatenonce: Random number generated to indicate Proof of\n         Possession.\n        :type certificatenonce: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`ErrorDetailsException<azure.mgmt.iothubprovisioningservices.models.ErrorDetailsException>`\n        \"\"\"\n        # Construct URL\n        url = self.delete.metadata['url']\n        path_format_arguments = {\n            'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str'),\n            'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n            'provisioningServiceName': self._serialize.url(\"provisioning_service_name\", provisioning_service_name, 'str'),\n            'certificateName': self._serialize.url(\"certificate_name\", certificate_name, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        if certificatename is not None:\n            query_parameters['certificate.name'] = self._serialize.query(\"certificatename\", certificatename, 'str')\n        if certificateraw_bytes is not None:\n            query_parameters['certificate.rawBytes'] = self._serialize.query(\"certificateraw_bytes\", certificateraw_bytes, 'bytearray')\n        if certificateis_verified is not None:\n            query_parameters['certificate.isVerified'] = self._serialize.query(\"certificateis_verified\", certificateis_verified, 'bool')\n        if certificatepurpose is not None:\n            query_parameters['certificate.purpose'] = self._serialize.query(\"certificatepurpose\", certificatepurpose, 'str')\n        if certificatecreated is not None:\n            query_parameters['certificate.created'] = self._serialize.query(\"certificatecreated\", certificatecreated, 'iso-8601')\n        if certificatelast_updated is not None:\n            query_parameters['certificate.lastUpdated'] = self._serialize.query(\"certificatelast_updated\", certificatelast_updated, 'iso-8601')\n        if certificatehas_private_key is not None:\n            query_parameters['certificate.hasPrivateKey'] = self._serialize.query(\"certificatehas_private_key\", certificatehas_private_key, 'bool')\n        if certificatenonce is not None:\n            query_parameters['certificate.nonce'] = self._serialize.query(\"certificatenonce\", certificatenonce, 'str')\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        header_parameters['If-Match'] = self._serialize.header(\"if_match\", if_match, 'str')\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        # Construct and send request\n        request = self._client.delete(url, query_parameters)\n        response = self._client.send(request, header_parameters, stream=False, **operation_config)\n\n        if response.status_code not in [200, 204]:\n            raise models.ErrorDetailsException(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response", "code_tokens": ["def", "delete", "(", "self", ",", "resource_group_name", ",", "if_match", ",", "provisioning_service_name", ",", "certificate_name", ",", "certificatename", "=", "None", ",", "certificateraw_bytes", "=", "None", ",", "certificateis_verified", "=", "None", ",", "certificatepurpose", "=", "None", ",", "certificatecreated", "=", "None", ",", "certificatelast_updated", "=", "None", ",", "certificatehas_private_key", "=", "None", ",", "certificatenonce", "=", "None", ",", "custom_headers", "=", "None", ",", "raw", "=", "False", ",", "*", "*", "operation_config", ")", ":", "# Construct URL", "url", "=", "self", ".", "delete", ".", "metadata", "[", "'url'", "]", "path_format_arguments", "=", "{", "'subscriptionId'", ":", "self", ".", "_serialize", ".", "url", "(", "\"self.config.subscription_id\"", ",", "self", ".", "config", ".", "subscription_id", ",", "'str'", ")", ",", "'resourceGroupName'", ":", "self", ".", "_serialize", ".", "url", "(", "\"resource_group_name\"", ",", "resource_group_name", ",", "'str'", ")", ",", "'provisioningServiceName'", ":", "self", ".", "_serialize", ".", "url", "(", "\"provisioning_service_name\"", ",", "provisioning_service_name", ",", "'str'", ")", ",", "'certificateName'", ":", "self", ".", "_serialize", ".", "url", "(", "\"certificate_name\"", ",", "certificate_name", ",", "'str'", ")", "}", "url", "=", "self", ".", "_client", ".", "format_url", "(", "url", ",", "*", "*", "path_format_arguments", ")", "# Construct parameters", "query_parameters", "=", "{", "}", "if", "certificatename", "is", "not", "None", ":", "query_parameters", "[", "'certificate.name'", "]", "=", "self", ".", "_serialize", ".", "query", "(", "\"certificatename\"", ",", "certificatename", ",", "'str'", ")", "if", "certificateraw_bytes", "is", "not", "None", ":", "query_parameters", "[", "'certificate.rawBytes'", "]", "=", "self", ".", "_serialize", ".", "query", "(", "\"certificateraw_bytes\"", ",", "certificateraw_bytes", ",", "'bytearray'", ")", "if", "certificateis_verified", "is", "not", "None", ":", "query_parameters", "[", "'certificate.isVerified'", "]", "=", "self", ".", "_serialize", ".", "query", "(", "\"certificateis_verified\"", ",", "certificateis_verified", ",", "'bool'", ")", "if", "certificatepurpose", "is", "not", "None", ":", "query_parameters", "[", "'certificate.purpose'", "]", "=", "self", ".", "_serialize", ".", "query", "(", "\"certificatepurpose\"", ",", "certificatepurpose", ",", "'str'", ")", "if", "certificatecreated", "is", "not", "None", ":", "query_parameters", "[", "'certificate.created'", "]", "=", "self", ".", "_serialize", ".", "query", "(", "\"certificatecreated\"", ",", "certificatecreated", ",", "'iso-8601'", ")", "if", "certificatelast_updated", "is", "not", "None", ":", "query_parameters", "[", "'certificate.lastUpdated'", "]", "=", "self", ".", "_serialize", ".", "query", "(", "\"certificatelast_updated\"", ",", "certificatelast_updated", ",", "'iso-8601'", ")", "if", "certificatehas_private_key", "is", "not", "None", ":", "query_parameters", "[", "'certificate.hasPrivateKey'", "]", "=", "self", ".", "_serialize", ".", "query", "(", "\"certificatehas_private_key\"", ",", "certificatehas_private_key", ",", "'bool'", ")", "if", "certificatenonce", "is", "not", "None", ":", "query_parameters", "[", "'certificate.nonce'", "]", "=", "self", ".", "_serialize", ".", "query", "(", "\"certificatenonce\"", ",", "certificatenonce", ",", "'str'", ")", "query_parameters", "[", "'api-version'", "]", "=", "self", ".", "_serialize", ".", "query", "(", "\"self.api_version\"", ",", "self", ".", "api_version", ",", "'str'", ")", "# Construct headers", "header_parameters", "=", "{", "}", "header_parameters", "[", "'Content-Type'", "]", "=", "'application/json; charset=utf-8'", "if", "self", ".", "config", ".", "generate_client_request_id", ":", "header_parameters", "[", "'x-ms-client-request-id'", "]", "=", "str", "(", "uuid", ".", "uuid1", "(", ")", ")", "if", "custom_headers", ":", "header_parameters", ".", "update", "(", "custom_headers", ")", "header_parameters", "[", "'If-Match'", "]", "=", "self", ".", "_serialize", ".", "header", "(", "\"if_match\"", ",", "if_match", ",", "'str'", ")", "if", "self", ".", "config", ".", "accept_language", "is", "not", "None", ":", "header_parameters", "[", "'accept-language'", "]", "=", "self", ".", "_serialize", ".", "header", "(", "\"self.config.accept_language\"", ",", "self", ".", "config", ".", "accept_language", ",", "'str'", ")", "# Construct and send request", "request", "=", "self", ".", "_client", ".", "delete", "(", "url", ",", "query_parameters", ")", "response", "=", "self", ".", "_client", ".", "send", "(", "request", ",", "header_parameters", ",", "stream", "=", "False", ",", "*", "*", "operation_config", ")", "if", "response", ".", "status_code", "not", "in", "[", "200", ",", "204", "]", ":", "raise", "models", ".", "ErrorDetailsException", "(", "self", ".", "_deserialize", ",", "response", ")", "if", "raw", ":", "client_raw_response", "=", "ClientRawResponse", "(", "None", ",", "response", ")", "return", "client_raw_response"], "docstring": "Delete the Provisioning Service Certificate.\n\n        Deletes the specified certificate assosciated with the Provisioning\n        Service.\n\n        :param resource_group_name: Resource group identifier.\n        :type resource_group_name: str\n        :param if_match: ETag of the certificate\n        :type if_match: str\n        :param provisioning_service_name: The name of the provisioning\n         service.\n        :type provisioning_service_name: str\n        :param certificate_name: This is a mandatory field, and is the logical\n         name of the certificate that the provisioning service will access by.\n        :type certificate_name: str\n        :param certificatename: This is optional, and it is the Common Name of\n         the certificate.\n        :type certificatename: str\n        :param certificateraw_bytes: Raw data within the certificate.\n        :type certificateraw_bytes: bytearray\n        :param certificateis_verified: Indicates if certificate has been\n         verified by owner of the private key.\n        :type certificateis_verified: bool\n        :param certificatepurpose: A description that mentions the purpose of\n         the certificate. Possible values include: 'clientAuthentication',\n         'serverAuthentication'\n        :type certificatepurpose: str or\n         ~azure.mgmt.iothubprovisioningservices.models.CertificatePurpose\n        :param certificatecreated: Time the certificate is created.\n        :type certificatecreated: datetime\n        :param certificatelast_updated: Time the certificate is last updated.\n        :type certificatelast_updated: datetime\n        :param certificatehas_private_key: Indicates if the certificate\n         contains a private key.\n        :type certificatehas_private_key: bool\n        :param certificatenonce: Random number generated to indicate Proof of\n         Possession.\n        :type certificatenonce: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`ErrorDetailsException<azure.mgmt.iothubprovisioningservices.models.ErrorDetailsException>`", "docstring_tokens": ["Delete", "the", "Provisioning", "Service", "Certificate", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-mgmt-iothubprovisioningservices/azure/mgmt/iothubprovisioningservices/operations/dps_certificate_operations.py#L192-L292", "partition": "test"}
{"repo": "GeoffAtHome/lightwave", "path": "lightwave/lightwave.py", "func_name": "LWLink._send_reliable_message", "original_string": "def _send_reliable_message(self, msg):\n        \"\"\"Send msg to LightwaveRF hub.\"\"\"\n        result = False\n        max_retries = 15\n        trans_id = next(LWLink.transaction_id)\n        msg = \"%d,%s\" % (trans_id, msg)\n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) \\\n                    as write_sock, \\\n                    socket.socket(socket.AF_INET, socket.SOCK_DGRAM) \\\n                    as read_sock:\n                write_sock.setsockopt(\n                    socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n                read_sock.setsockopt(socket.SOL_SOCKET,\n                                     socket.SO_BROADCAST, 1)\n                read_sock.settimeout(self.SOCKET_TIMEOUT)\n                read_sock.bind(('0.0.0.0', self.RX_PORT))\n                while max_retries:\n                    max_retries -= 1\n                    write_sock.sendto(msg.encode(\n                        'UTF-8'), (LWLink.link_ip, self.TX_PORT))\n                    result = False\n                    while True:\n                        response, dummy = read_sock.recvfrom(1024)\n                        response = response.decode('UTF-8')\n                        if \"Not yet registered.\" in response:\n                            _LOGGER.error(\"Not yet registered\")\n                            self.register()\n                            result = True\n                            break\n\n                        if response.startswith(\"%d,OK\" % trans_id):\n                            result = True\n                            break\n                        if response.startswith(\"%d,ERR\" % trans_id):\n                            _LOGGER.error(response)\n                            break\n\n                        _LOGGER.info(response)\n\n                    if result:\n                        break\n\n                    time.sleep(0.25)\n\n        except socket.timeout:\n            _LOGGER.error(\"LW broker timeout!\")\n            return result\n\n        except Exception as ex:\n            _LOGGER.error(ex)\n            raise\n\n        if result:\n            _LOGGER.info(\"LW broker OK!\")\n        else:\n            _LOGGER.error(\"LW broker fail!\")\n        return result", "language": "python", "code": "def _send_reliable_message(self, msg):\n        \"\"\"Send msg to LightwaveRF hub.\"\"\"\n        result = False\n        max_retries = 15\n        trans_id = next(LWLink.transaction_id)\n        msg = \"%d,%s\" % (trans_id, msg)\n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) \\\n                    as write_sock, \\\n                    socket.socket(socket.AF_INET, socket.SOCK_DGRAM) \\\n                    as read_sock:\n                write_sock.setsockopt(\n                    socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n                read_sock.setsockopt(socket.SOL_SOCKET,\n                                     socket.SO_BROADCAST, 1)\n                read_sock.settimeout(self.SOCKET_TIMEOUT)\n                read_sock.bind(('0.0.0.0', self.RX_PORT))\n                while max_retries:\n                    max_retries -= 1\n                    write_sock.sendto(msg.encode(\n                        'UTF-8'), (LWLink.link_ip, self.TX_PORT))\n                    result = False\n                    while True:\n                        response, dummy = read_sock.recvfrom(1024)\n                        response = response.decode('UTF-8')\n                        if \"Not yet registered.\" in response:\n                            _LOGGER.error(\"Not yet registered\")\n                            self.register()\n                            result = True\n                            break\n\n                        if response.startswith(\"%d,OK\" % trans_id):\n                            result = True\n                            break\n                        if response.startswith(\"%d,ERR\" % trans_id):\n                            _LOGGER.error(response)\n                            break\n\n                        _LOGGER.info(response)\n\n                    if result:\n                        break\n\n                    time.sleep(0.25)\n\n        except socket.timeout:\n            _LOGGER.error(\"LW broker timeout!\")\n            return result\n\n        except Exception as ex:\n            _LOGGER.error(ex)\n            raise\n\n        if result:\n            _LOGGER.info(\"LW broker OK!\")\n        else:\n            _LOGGER.error(\"LW broker fail!\")\n        return result", "code_tokens": ["def", "_send_reliable_message", "(", "self", ",", "msg", ")", ":", "result", "=", "False", "max_retries", "=", "15", "trans_id", "=", "next", "(", "LWLink", ".", "transaction_id", ")", "msg", "=", "\"%d,%s\"", "%", "(", "trans_id", ",", "msg", ")", "try", ":", "with", "socket", ".", "socket", "(", "socket", ".", "AF_INET", ",", "socket", ".", "SOCK_DGRAM", ")", "as", "write_sock", ",", "socket", ".", "socket", "(", "socket", ".", "AF_INET", ",", "socket", ".", "SOCK_DGRAM", ")", "as", "read_sock", ":", "write_sock", ".", "setsockopt", "(", "socket", ".", "SOL_SOCKET", ",", "socket", ".", "SO_REUSEADDR", ",", "1", ")", "read_sock", ".", "setsockopt", "(", "socket", ".", "SOL_SOCKET", ",", "socket", ".", "SO_BROADCAST", ",", "1", ")", "read_sock", ".", "settimeout", "(", "self", ".", "SOCKET_TIMEOUT", ")", "read_sock", ".", "bind", "(", "(", "'0.0.0.0'", ",", "self", ".", "RX_PORT", ")", ")", "while", "max_retries", ":", "max_retries", "-=", "1", "write_sock", ".", "sendto", "(", "msg", ".", "encode", "(", "'UTF-8'", ")", ",", "(", "LWLink", ".", "link_ip", ",", "self", ".", "TX_PORT", ")", ")", "result", "=", "False", "while", "True", ":", "response", ",", "dummy", "=", "read_sock", ".", "recvfrom", "(", "1024", ")", "response", "=", "response", ".", "decode", "(", "'UTF-8'", ")", "if", "\"Not yet registered.\"", "in", "response", ":", "_LOGGER", ".", "error", "(", "\"Not yet registered\"", ")", "self", ".", "register", "(", ")", "result", "=", "True", "break", "if", "response", ".", "startswith", "(", "\"%d,OK\"", "%", "trans_id", ")", ":", "result", "=", "True", "break", "if", "response", ".", "startswith", "(", "\"%d,ERR\"", "%", "trans_id", ")", ":", "_LOGGER", ".", "error", "(", "response", ")", "break", "_LOGGER", ".", "info", "(", "response", ")", "if", "result", ":", "break", "time", ".", "sleep", "(", "0.25", ")", "except", "socket", ".", "timeout", ":", "_LOGGER", ".", "error", "(", "\"LW broker timeout!\"", ")", "return", "result", "except", "Exception", "as", "ex", ":", "_LOGGER", ".", "error", "(", "ex", ")", "raise", "if", "result", ":", "_LOGGER", ".", "info", "(", "\"LW broker OK!\"", ")", "else", ":", "_LOGGER", ".", "error", "(", "\"LW broker fail!\"", ")", "return", "result"], "docstring": "Send msg to LightwaveRF hub.", "docstring_tokens": ["Send", "msg", "to", "LightwaveRF", "hub", "."], "sha": "2fab4ee8c9f14dd97dffd4b8cd70b217e884e581", "url": "https://github.com/GeoffAtHome/lightwave/blob/2fab4ee8c9f14dd97dffd4b8cd70b217e884e581/lightwave/lightwave.py#L75-L132", "partition": "test"}
{"repo": "ankitmathur3193/song-cli", "path": "song/commands/SearchEngineParser/GoogleParser.py", "func_name": "GoogleParser.google_url", "original_string": "def google_url(self,song_name,website):\n\t\t''' It will return the google url to be searched'''\n\t\tname='+'.join(song_name)\n\t\tprefix='https://www.google.co.in/search?q='\t\n\t\twebsite=website.split(\" \")\n\t\tsuffix='+'.join(website)\n\t\turl=prefix+name+suffix\n\t\t#print url\n\t\treturn url", "language": "python", "code": "def google_url(self,song_name,website):\n\t\t''' It will return the google url to be searched'''\n\t\tname='+'.join(song_name)\n\t\tprefix='https://www.google.co.in/search?q='\t\n\t\twebsite=website.split(\" \")\n\t\tsuffix='+'.join(website)\n\t\turl=prefix+name+suffix\n\t\t#print url\n\t\treturn url", "code_tokens": ["def", "google_url", "(", "self", ",", "song_name", ",", "website", ")", ":", "name", "=", "'+'", ".", "join", "(", "song_name", ")", "prefix", "=", "'https://www.google.co.in/search?q='", "website", "=", "website", ".", "split", "(", "\" \"", ")", "suffix", "=", "'+'", ".", "join", "(", "website", ")", "url", "=", "prefix", "+", "name", "+", "suffix", "#print url", "return", "url"], "docstring": "It will return the google url to be searched", "docstring_tokens": ["It", "will", "return", "the", "google", "url", "to", "be", "searched"], "sha": "ca8ccfe547e9d702313ff6d14e81ae4355989a67", "url": "https://github.com/ankitmathur3193/song-cli/blob/ca8ccfe547e9d702313ff6d14e81ae4355989a67/song/commands/SearchEngineParser/GoogleParser.py#L10-L18", "partition": "test"}
{"repo": "bloomreach/s4cmd", "path": "s4cmd.py", "func_name": "log_calls", "original_string": "def log_calls(func):\n  '''Decorator to log function calls.'''\n  def wrapper(*args, **kargs):\n    callStr = \"%s(%s)\" % (func.__name__, \", \".join([repr(p) for p in args] + [\"%s=%s\" % (k, repr(v)) for (k, v) in list(kargs.items())]))\n    debug(\">> %s\", callStr)\n    ret = func(*args, **kargs)\n    debug(\"<< %s: %s\", callStr, repr(ret))\n    return ret\n  return wrapper", "language": "python", "code": "def log_calls(func):\n  '''Decorator to log function calls.'''\n  def wrapper(*args, **kargs):\n    callStr = \"%s(%s)\" % (func.__name__, \", \".join([repr(p) for p in args] + [\"%s=%s\" % (k, repr(v)) for (k, v) in list(kargs.items())]))\n    debug(\">> %s\", callStr)\n    ret = func(*args, **kargs)\n    debug(\"<< %s: %s\", callStr, repr(ret))\n    return ret\n  return wrapper", "code_tokens": ["def", "log_calls", "(", "func", ")", ":", "def", "wrapper", "(", "*", "args", ",", "*", "*", "kargs", ")", ":", "callStr", "=", "\"%s(%s)\"", "%", "(", "func", ".", "__name__", ",", "\", \"", ".", "join", "(", "[", "repr", "(", "p", ")", "for", "p", "in", "args", "]", "+", "[", "\"%s=%s\"", "%", "(", "k", ",", "repr", "(", "v", ")", ")", "for", "(", "k", ",", "v", ")", "in", "list", "(", "kargs", ".", "items", "(", ")", ")", "]", ")", ")", "debug", "(", "\">> %s\"", ",", "callStr", ")", "ret", "=", "func", "(", "*", "args", ",", "*", "*", "kargs", ")", "debug", "(", "\"<< %s: %s\"", ",", "callStr", ",", "repr", "(", "ret", ")", ")", "return", "ret", "return", "wrapper"], "docstring": "Decorator to log function calls.", "docstring_tokens": ["Decorator", "to", "log", "function", "calls", "."], "sha": "bb51075bf43703e7cd95aa39288cf7732ec13a6d", "url": "https://github.com/bloomreach/s4cmd/blob/bb51075bf43703e7cd95aa39288cf7732ec13a6d/s4cmd.py#L124-L132", "partition": "test"}
{"repo": "gholt/swiftly", "path": "swiftly/cli/put.py", "func_name": "cli_put_account", "original_string": "def cli_put_account(context):\n    \"\"\"\n    Performs a PUT on the account.\n\n    See :py:mod:`swiftly.cli.put` for context usage information.\n\n    See :py:class:`CLIPut` for more information.\n    \"\"\"\n    body = None\n    if context.input_:\n        if context.input_ == '-':\n            body = context.io_manager.get_stdin()\n        else:\n            body = open(context.input_, 'rb')\n    with context.client_manager.with_client() as client:\n        status, reason, headers, contents = client.put_account(\n            headers=context.headers, query=context.query, cdn=context.cdn,\n            body=body)\n        if hasattr(contents, 'read'):\n            contents.read()\n    if status // 100 != 2:\n        raise ReturnCode('putting account: %s %s' % (status, reason))", "language": "python", "code": "def cli_put_account(context):\n    \"\"\"\n    Performs a PUT on the account.\n\n    See :py:mod:`swiftly.cli.put` for context usage information.\n\n    See :py:class:`CLIPut` for more information.\n    \"\"\"\n    body = None\n    if context.input_:\n        if context.input_ == '-':\n            body = context.io_manager.get_stdin()\n        else:\n            body = open(context.input_, 'rb')\n    with context.client_manager.with_client() as client:\n        status, reason, headers, contents = client.put_account(\n            headers=context.headers, query=context.query, cdn=context.cdn,\n            body=body)\n        if hasattr(contents, 'read'):\n            contents.read()\n    if status // 100 != 2:\n        raise ReturnCode('putting account: %s %s' % (status, reason))", "code_tokens": ["def", "cli_put_account", "(", "context", ")", ":", "body", "=", "None", "if", "context", ".", "input_", ":", "if", "context", ".", "input_", "==", "'-'", ":", "body", "=", "context", ".", "io_manager", ".", "get_stdin", "(", ")", "else", ":", "body", "=", "open", "(", "context", ".", "input_", ",", "'rb'", ")", "with", "context", ".", "client_manager", ".", "with_client", "(", ")", "as", "client", ":", "status", ",", "reason", ",", "headers", ",", "contents", "=", "client", ".", "put_account", "(", "headers", "=", "context", ".", "headers", ",", "query", "=", "context", ".", "query", ",", "cdn", "=", "context", ".", "cdn", ",", "body", "=", "body", ")", "if", "hasattr", "(", "contents", ",", "'read'", ")", ":", "contents", ".", "read", "(", ")", "if", "status", "//", "100", "!=", "2", ":", "raise", "ReturnCode", "(", "'putting account: %s %s'", "%", "(", "status", ",", "reason", ")", ")"], "docstring": "Performs a PUT on the account.\n\n    See :py:mod:`swiftly.cli.put` for context usage information.\n\n    See :py:class:`CLIPut` for more information.", "docstring_tokens": ["Performs", "a", "PUT", "on", "the", "account", "."], "sha": "5bcc1c65323b1caf1f85adbefd9fc4988c072149", "url": "https://github.com/gholt/swiftly/blob/5bcc1c65323b1caf1f85adbefd9fc4988c072149/swiftly/cli/put.py#L133-L154", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-mgmt-web/azure/mgmt/web/operations/top_level_domains_operations.py", "func_name": "TopLevelDomainsOperations.list_agreements", "original_string": "def list_agreements(\n            self, name, include_privacy=None, for_transfer=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Gets all legal agreements that user needs to accept before purchasing a\n        domain.\n\n        Gets all legal agreements that user needs to accept before purchasing a\n        domain.\n\n        :param name: Name of the top-level domain.\n        :type name: str\n        :param include_privacy: If <code>true</code>, then the list of\n         agreements will include agreements for domain privacy as well;\n         otherwise, <code>false</code>.\n        :type include_privacy: bool\n        :param for_transfer: If <code>true</code>, then the list of agreements\n         will include agreements for domain transfer as well; otherwise,\n         <code>false</code>.\n        :type for_transfer: bool\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: An iterator like instance of TldLegalAgreement\n        :rtype:\n         ~azure.mgmt.web.models.TldLegalAgreementPaged[~azure.mgmt.web.models.TldLegalAgreement]\n        :raises:\n         :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`\n        \"\"\"\n        agreement_option = models.TopLevelDomainAgreementOption(include_privacy=include_privacy, for_transfer=for_transfer)\n\n        def internal_paging(next_link=None, raw=False):\n\n            if not next_link:\n                # Construct URL\n                url = self.list_agreements.metadata['url']\n                path_format_arguments = {\n                    'name': self._serialize.url(\"name\", name, 'str'),\n                    'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str')\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n\n                # Construct parameters\n                query_parameters = {}\n                query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n            else:\n                url = next_link\n                query_parameters = {}\n\n            # Construct headers\n            header_parameters = {}\n            header_parameters['Accept'] = 'application/json'\n            header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n            if self.config.generate_client_request_id:\n                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n            if custom_headers:\n                header_parameters.update(custom_headers)\n            if self.config.accept_language is not None:\n                header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n            # Construct body\n            body_content = self._serialize.body(agreement_option, 'TopLevelDomainAgreementOption')\n\n            # Construct and send request\n            request = self._client.post(url, query_parameters, header_parameters, body_content)\n            response = self._client.send(request, stream=False, **operation_config)\n\n            if response.status_code not in [200]:\n                raise models.DefaultErrorResponseException(self._deserialize, response)\n\n            return response\n\n        # Deserialize response\n        deserialized = models.TldLegalAgreementPaged(internal_paging, self._deserialize.dependencies)\n\n        if raw:\n            header_dict = {}\n            client_raw_response = models.TldLegalAgreementPaged(internal_paging, self._deserialize.dependencies, header_dict)\n            return client_raw_response\n\n        return deserialized", "language": "python", "code": "def list_agreements(\n            self, name, include_privacy=None, for_transfer=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Gets all legal agreements that user needs to accept before purchasing a\n        domain.\n\n        Gets all legal agreements that user needs to accept before purchasing a\n        domain.\n\n        :param name: Name of the top-level domain.\n        :type name: str\n        :param include_privacy: If <code>true</code>, then the list of\n         agreements will include agreements for domain privacy as well;\n         otherwise, <code>false</code>.\n        :type include_privacy: bool\n        :param for_transfer: If <code>true</code>, then the list of agreements\n         will include agreements for domain transfer as well; otherwise,\n         <code>false</code>.\n        :type for_transfer: bool\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: An iterator like instance of TldLegalAgreement\n        :rtype:\n         ~azure.mgmt.web.models.TldLegalAgreementPaged[~azure.mgmt.web.models.TldLegalAgreement]\n        :raises:\n         :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`\n        \"\"\"\n        agreement_option = models.TopLevelDomainAgreementOption(include_privacy=include_privacy, for_transfer=for_transfer)\n\n        def internal_paging(next_link=None, raw=False):\n\n            if not next_link:\n                # Construct URL\n                url = self.list_agreements.metadata['url']\n                path_format_arguments = {\n                    'name': self._serialize.url(\"name\", name, 'str'),\n                    'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str')\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n\n                # Construct parameters\n                query_parameters = {}\n                query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n            else:\n                url = next_link\n                query_parameters = {}\n\n            # Construct headers\n            header_parameters = {}\n            header_parameters['Accept'] = 'application/json'\n            header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n            if self.config.generate_client_request_id:\n                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n            if custom_headers:\n                header_parameters.update(custom_headers)\n            if self.config.accept_language is not None:\n                header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n            # Construct body\n            body_content = self._serialize.body(agreement_option, 'TopLevelDomainAgreementOption')\n\n            # Construct and send request\n            request = self._client.post(url, query_parameters, header_parameters, body_content)\n            response = self._client.send(request, stream=False, **operation_config)\n\n            if response.status_code not in [200]:\n                raise models.DefaultErrorResponseException(self._deserialize, response)\n\n            return response\n\n        # Deserialize response\n        deserialized = models.TldLegalAgreementPaged(internal_paging, self._deserialize.dependencies)\n\n        if raw:\n            header_dict = {}\n            client_raw_response = models.TldLegalAgreementPaged(internal_paging, self._deserialize.dependencies, header_dict)\n            return client_raw_response\n\n        return deserialized", "code_tokens": ["def", "list_agreements", "(", "self", ",", "name", ",", "include_privacy", "=", "None", ",", "for_transfer", "=", "None", ",", "custom_headers", "=", "None", ",", "raw", "=", "False", ",", "*", "*", "operation_config", ")", ":", "agreement_option", "=", "models", ".", "TopLevelDomainAgreementOption", "(", "include_privacy", "=", "include_privacy", ",", "for_transfer", "=", "for_transfer", ")", "def", "internal_paging", "(", "next_link", "=", "None", ",", "raw", "=", "False", ")", ":", "if", "not", "next_link", ":", "# Construct URL", "url", "=", "self", ".", "list_agreements", ".", "metadata", "[", "'url'", "]", "path_format_arguments", "=", "{", "'name'", ":", "self", ".", "_serialize", ".", "url", "(", "\"name\"", ",", "name", ",", "'str'", ")", ",", "'subscriptionId'", ":", "self", ".", "_serialize", ".", "url", "(", "\"self.config.subscription_id\"", ",", "self", ".", "config", ".", "subscription_id", ",", "'str'", ")", "}", "url", "=", "self", ".", "_client", ".", "format_url", "(", "url", ",", "*", "*", "path_format_arguments", ")", "# Construct parameters", "query_parameters", "=", "{", "}", "query_parameters", "[", "'api-version'", "]", "=", "self", ".", "_serialize", ".", "query", "(", "\"self.api_version\"", ",", "self", ".", "api_version", ",", "'str'", ")", "else", ":", "url", "=", "next_link", "query_parameters", "=", "{", "}", "# Construct headers", "header_parameters", "=", "{", "}", "header_parameters", "[", "'Accept'", "]", "=", "'application/json'", "header_parameters", "[", "'Content-Type'", "]", "=", "'application/json; charset=utf-8'", "if", "self", ".", "config", ".", "generate_client_request_id", ":", "header_parameters", "[", "'x-ms-client-request-id'", "]", "=", "str", "(", "uuid", ".", "uuid1", "(", ")", ")", "if", "custom_headers", ":", "header_parameters", ".", "update", "(", "custom_headers", ")", "if", "self", ".", "config", ".", "accept_language", "is", "not", "None", ":", "header_parameters", "[", "'accept-language'", "]", "=", "self", ".", "_serialize", ".", "header", "(", "\"self.config.accept_language\"", ",", "self", ".", "config", ".", "accept_language", ",", "'str'", ")", "# Construct body", "body_content", "=", "self", ".", "_serialize", ".", "body", "(", "agreement_option", ",", "'TopLevelDomainAgreementOption'", ")", "# Construct and send request", "request", "=", "self", ".", "_client", ".", "post", "(", "url", ",", "query_parameters", ",", "header_parameters", ",", "body_content", ")", "response", "=", "self", ".", "_client", ".", "send", "(", "request", ",", "stream", "=", "False", ",", "*", "*", "operation_config", ")", "if", "response", ".", "status_code", "not", "in", "[", "200", "]", ":", "raise", "models", ".", "DefaultErrorResponseException", "(", "self", ".", "_deserialize", ",", "response", ")", "return", "response", "# Deserialize response", "deserialized", "=", "models", ".", "TldLegalAgreementPaged", "(", "internal_paging", ",", "self", ".", "_deserialize", ".", "dependencies", ")", "if", "raw", ":", "header_dict", "=", "{", "}", "client_raw_response", "=", "models", ".", "TldLegalAgreementPaged", "(", "internal_paging", ",", "self", ".", "_deserialize", ".", "dependencies", ",", "header_dict", ")", "return", "client_raw_response", "return", "deserialized"], "docstring": "Gets all legal agreements that user needs to accept before purchasing a\n        domain.\n\n        Gets all legal agreements that user needs to accept before purchasing a\n        domain.\n\n        :param name: Name of the top-level domain.\n        :type name: str\n        :param include_privacy: If <code>true</code>, then the list of\n         agreements will include agreements for domain privacy as well;\n         otherwise, <code>false</code>.\n        :type include_privacy: bool\n        :param for_transfer: If <code>true</code>, then the list of agreements\n         will include agreements for domain transfer as well; otherwise,\n         <code>false</code>.\n        :type for_transfer: bool\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: An iterator like instance of TldLegalAgreement\n        :rtype:\n         ~azure.mgmt.web.models.TldLegalAgreementPaged[~azure.mgmt.web.models.TldLegalAgreement]\n        :raises:\n         :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`", "docstring_tokens": ["Gets", "all", "legal", "agreements", "that", "user", "needs", "to", "accept", "before", "purchasing", "a", "domain", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-mgmt-web/azure/mgmt/web/operations/top_level_domains_operations.py#L164-L245", "partition": "test"}
{"repo": "zenodo/zenodo-accessrequests", "path": "zenodo_accessrequests/receivers.py", "func_name": "send_accept_notification", "original_string": "def send_accept_notification(request, message=None, expires_at=None):\n    \"\"\"Receiver for request-accepted signal to send email notification.\"\"\"\n    pid, record = get_record(request.recid)\n    _send_notification(\n        request.sender_email,\n        _(\"Access request accepted\"),\n        \"zenodo_accessrequests/emails/accepted.tpl\",\n        request=request,\n        record=record,\n        pid=pid,\n        record_link=request.link.get_absolute_url('invenio_records_ui.recid'),\n        message=message,\n        expires_at=expires_at,\n    )", "language": "python", "code": "def send_accept_notification(request, message=None, expires_at=None):\n    \"\"\"Receiver for request-accepted signal to send email notification.\"\"\"\n    pid, record = get_record(request.recid)\n    _send_notification(\n        request.sender_email,\n        _(\"Access request accepted\"),\n        \"zenodo_accessrequests/emails/accepted.tpl\",\n        request=request,\n        record=record,\n        pid=pid,\n        record_link=request.link.get_absolute_url('invenio_records_ui.recid'),\n        message=message,\n        expires_at=expires_at,\n    )", "code_tokens": ["def", "send_accept_notification", "(", "request", ",", "message", "=", "None", ",", "expires_at", "=", "None", ")", ":", "pid", ",", "record", "=", "get_record", "(", "request", ".", "recid", ")", "_send_notification", "(", "request", ".", "sender_email", ",", "_", "(", "\"Access request accepted\"", ")", ",", "\"zenodo_accessrequests/emails/accepted.tpl\"", ",", "request", "=", "request", ",", "record", "=", "record", ",", "pid", "=", "pid", ",", "record_link", "=", "request", ".", "link", ".", "get_absolute_url", "(", "'invenio_records_ui.recid'", ")", ",", "message", "=", "message", ",", "expires_at", "=", "expires_at", ",", ")"], "docstring": "Receiver for request-accepted signal to send email notification.", "docstring_tokens": ["Receiver", "for", "request", "-", "accepted", "signal", "to", "send", "email", "notification", "."], "sha": "ce2cf3f1425d02ba4f3ad3202cfca43a1892558a", "url": "https://github.com/zenodo/zenodo-accessrequests/blob/ce2cf3f1425d02ba4f3ad3202cfca43a1892558a/zenodo_accessrequests/receivers.py#L73-L86", "partition": "test"}
{"repo": "ellmetha/neojsonrpc", "path": "neojsonrpc/client.py", "func_name": "Client.get_block_hash", "original_string": "def get_block_hash(self, block_index, **kwargs):\n        \"\"\" Returns the hash value associated with a specific block index.\n\n        :param block_index: a block index (block height)\n        :type block_index: int\n        :return: hash of the block associated with the considered index\n        :rtype: str\n\n        \"\"\"\n        return self._call(JSONRPCMethods.GET_BLOCK_HASH.value, [block_index, ], **kwargs)", "language": "python", "code": "def get_block_hash(self, block_index, **kwargs):\n        \"\"\" Returns the hash value associated with a specific block index.\n\n        :param block_index: a block index (block height)\n        :type block_index: int\n        :return: hash of the block associated with the considered index\n        :rtype: str\n\n        \"\"\"\n        return self._call(JSONRPCMethods.GET_BLOCK_HASH.value, [block_index, ], **kwargs)", "code_tokens": ["def", "get_block_hash", "(", "self", ",", "block_index", ",", "*", "*", "kwargs", ")", ":", "return", "self", ".", "_call", "(", "JSONRPCMethods", ".", "GET_BLOCK_HASH", ".", "value", ",", "[", "block_index", ",", "]", ",", "*", "*", "kwargs", ")"], "docstring": "Returns the hash value associated with a specific block index.\n\n        :param block_index: a block index (block height)\n        :type block_index: int\n        :return: hash of the block associated with the considered index\n        :rtype: str", "docstring_tokens": ["Returns", "the", "hash", "value", "associated", "with", "a", "specific", "block", "index", "."], "sha": "e369b633a727482d5f9e310f0c3337ae5f7265db", "url": "https://github.com/ellmetha/neojsonrpc/blob/e369b633a727482d5f9e310f0c3337ae5f7265db/neojsonrpc/client.py#L135-L144", "partition": "test"}
{"repo": "edx/web-fragments", "path": "web_fragments/fragment.py", "func_name": "Fragment.resource_to_html", "original_string": "def resource_to_html(resource):\n        \"\"\"\n        Returns `resource` wrapped in the appropriate html tag for it's mimetype.\n        \"\"\"\n        if resource.mimetype == \"text/css\":\n            if resource.kind == \"text\":\n                return u\"<style type='text/css'>\\n%s\\n</style>\" % resource.data\n            elif resource.kind == \"url\":\n                return u\"<link rel='stylesheet' href='%s' type='text/css'>\" % resource.data\n            else:\n                raise Exception(\"Unrecognized resource kind %r\" % resource.kind)\n\n        elif resource.mimetype == \"application/javascript\":\n            if resource.kind == \"text\":\n                return u\"<script>\\n%s\\n</script>\" % resource.data\n            elif resource.kind == \"url\":\n                return u\"<script src='%s' type='application/javascript'></script>\" % resource.data\n            else:\n                raise Exception(\"Unrecognized resource kind %r\" % resource.kind)\n\n        elif resource.mimetype == \"text/html\":\n            assert resource.kind == \"text\"\n            return resource.data\n\n        else:\n            raise Exception(\"Unrecognized mimetype %r\" % resource.mimetype)", "language": "python", "code": "def resource_to_html(resource):\n        \"\"\"\n        Returns `resource` wrapped in the appropriate html tag for it's mimetype.\n        \"\"\"\n        if resource.mimetype == \"text/css\":\n            if resource.kind == \"text\":\n                return u\"<style type='text/css'>\\n%s\\n</style>\" % resource.data\n            elif resource.kind == \"url\":\n                return u\"<link rel='stylesheet' href='%s' type='text/css'>\" % resource.data\n            else:\n                raise Exception(\"Unrecognized resource kind %r\" % resource.kind)\n\n        elif resource.mimetype == \"application/javascript\":\n            if resource.kind == \"text\":\n                return u\"<script>\\n%s\\n</script>\" % resource.data\n            elif resource.kind == \"url\":\n                return u\"<script src='%s' type='application/javascript'></script>\" % resource.data\n            else:\n                raise Exception(\"Unrecognized resource kind %r\" % resource.kind)\n\n        elif resource.mimetype == \"text/html\":\n            assert resource.kind == \"text\"\n            return resource.data\n\n        else:\n            raise Exception(\"Unrecognized mimetype %r\" % resource.mimetype)", "code_tokens": ["def", "resource_to_html", "(", "resource", ")", ":", "if", "resource", ".", "mimetype", "==", "\"text/css\"", ":", "if", "resource", ".", "kind", "==", "\"text\"", ":", "return", "u\"<style type='text/css'>\\n%s\\n</style>\"", "%", "resource", ".", "data", "elif", "resource", ".", "kind", "==", "\"url\"", ":", "return", "u\"<link rel='stylesheet' href='%s' type='text/css'>\"", "%", "resource", ".", "data", "else", ":", "raise", "Exception", "(", "\"Unrecognized resource kind %r\"", "%", "resource", ".", "kind", ")", "elif", "resource", ".", "mimetype", "==", "\"application/javascript\"", ":", "if", "resource", ".", "kind", "==", "\"text\"", ":", "return", "u\"<script>\\n%s\\n</script>\"", "%", "resource", ".", "data", "elif", "resource", ".", "kind", "==", "\"url\"", ":", "return", "u\"<script src='%s' type='application/javascript'></script>\"", "%", "resource", ".", "data", "else", ":", "raise", "Exception", "(", "\"Unrecognized resource kind %r\"", "%", "resource", ".", "kind", ")", "elif", "resource", ".", "mimetype", "==", "\"text/html\"", ":", "assert", "resource", ".", "kind", "==", "\"text\"", "return", "resource", ".", "data", "else", ":", "raise", "Exception", "(", "\"Unrecognized mimetype %r\"", "%", "resource", ".", "mimetype", ")"], "docstring": "Returns `resource` wrapped in the appropriate html tag for it's mimetype.", "docstring_tokens": ["Returns", "resource", "wrapped", "in", "the", "appropriate", "html", "tag", "for", "it", "s", "mimetype", "."], "sha": "42d760d700d70465e4e573b7b41442d8802ccd3c", "url": "https://github.com/edx/web-fragments/blob/42d760d700d70465e4e573b7b41442d8802ccd3c/web_fragments/fragment.py#L248-L273", "partition": "test"}
{"repo": "farzadghanei/statsd-metrics", "path": "statsdmetrics/client/timing.py", "func_name": "Chronometer.time_callable", "original_string": "def time_callable(self, name, target, rate=None, args=(), kwargs={}):\n        # type: (str, Callable, float, Tuple, Dict) -> Chronometer\n        \"\"\"Send a Timer metric calculating duration of execution of the provided callable\"\"\"\n        assert callable(target)\n        if rate is None:\n            rate = self._rate\n        else:\n            assert_sample_rate(rate)\n        start_time = time()  # type: float\n        result = target(*args, **kwargs)\n        self.since(name, start_time, rate)\n        return result", "language": "python", "code": "def time_callable(self, name, target, rate=None, args=(), kwargs={}):\n        # type: (str, Callable, float, Tuple, Dict) -> Chronometer\n        \"\"\"Send a Timer metric calculating duration of execution of the provided callable\"\"\"\n        assert callable(target)\n        if rate is None:\n            rate = self._rate\n        else:\n            assert_sample_rate(rate)\n        start_time = time()  # type: float\n        result = target(*args, **kwargs)\n        self.since(name, start_time, rate)\n        return result", "code_tokens": ["def", "time_callable", "(", "self", ",", "name", ",", "target", ",", "rate", "=", "None", ",", "args", "=", "(", ")", ",", "kwargs", "=", "{", "}", ")", ":", "# type: (str, Callable, float, Tuple, Dict) -> Chronometer", "assert", "callable", "(", "target", ")", "if", "rate", "is", "None", ":", "rate", "=", "self", ".", "_rate", "else", ":", "assert_sample_rate", "(", "rate", ")", "start_time", "=", "time", "(", ")", "# type: float", "result", "=", "target", "(", "*", "args", ",", "*", "*", "kwargs", ")", "self", ".", "since", "(", "name", ",", "start_time", ",", "rate", ")", "return", "result"], "docstring": "Send a Timer metric calculating duration of execution of the provided callable", "docstring_tokens": ["Send", "a", "Timer", "metric", "calculating", "duration", "of", "execution", "of", "the", "provided", "callable"], "sha": "153ff37b79777f208e49bb9d3fb737ba52b99f98", "url": "https://github.com/farzadghanei/statsd-metrics/blob/153ff37b79777f208e49bb9d3fb737ba52b99f98/statsdmetrics/client/timing.py#L77-L88", "partition": "test"}
{"repo": "3DLIRIOUS/MeshLabXML", "path": "meshlabxml/sampling.py", "func_name": "mesh_element", "original_string": "def mesh_element(script, sample_num=1000, element='VERT'):\n    \"\"\" Create a new layer populated with a point sampling of the current mesh,\n        at most one sample for each element of the mesh is created.\n\n    Samples are taking in a uniform way, one for each element\n    (vertex/edge/face); all the elements have the same probabilty of being\n    choosen.\n\n    Args:\n        script: the FilterScript object or script filename to write\n            the filter to.\n        sample_num (int): The desired number of elements that must be chosen.\n            Being a subsampling of the original elements if this number should\n            not be larger than the number of elements of the original mesh.\n        element (enum in ['VERT', 'EDGE', 'FACE']): Choose what mesh element\n            will be used for the subsampling. At most one point sample will\n            be added for each one of the chosen elements\n\n    Layer stack:\n        Creates new layer 'Sampled Mesh'. Current layer is changed to the new\n            layer.\n\n    MeshLab versions:\n        2016.12\n        1.3.4BETA\n    \"\"\"\n    if element.lower() == 'vert':\n        element_num = 0\n    elif element.lower() == 'edge':\n        element_num = 1\n    elif element.lower() == 'face':\n        element_num = 2\n    filter_xml = ''.join([\n        '  <filter name=\"Mesh Element Subsampling\">\\n',\n        '    <Param name=\"Sampling\" ',\n        'value=\"{:d}\" '.format(element_num),\n        'description=\"Element to sample:\" ',\n        'enum_val0=\"Vertex\" ',\n        'enum_val1=\"Edge\" ',\n        'enum_val2=\"Face\" ',\n        'enum_cardinality=\"3\" ',\n        'type=\"RichEnum\" ',\n        '/>\\n',\n        '    <Param name=\"SampleNum\" ',\n        'value=\"{:d}\" '.format(sample_num),\n        'description=\"Number of samples\" ',\n        'type=\"RichInt\" ',\n        '/>\\n',\n        '  </filter>\\n'])\n    util.write_filter(script, filter_xml)\n    if isinstance(script, FilterScript):\n        script.add_layer('Sampled Mesh')\n    return None", "language": "python", "code": "def mesh_element(script, sample_num=1000, element='VERT'):\n    \"\"\" Create a new layer populated with a point sampling of the current mesh,\n        at most one sample for each element of the mesh is created.\n\n    Samples are taking in a uniform way, one for each element\n    (vertex/edge/face); all the elements have the same probabilty of being\n    choosen.\n\n    Args:\n        script: the FilterScript object or script filename to write\n            the filter to.\n        sample_num (int): The desired number of elements that must be chosen.\n            Being a subsampling of the original elements if this number should\n            not be larger than the number of elements of the original mesh.\n        element (enum in ['VERT', 'EDGE', 'FACE']): Choose what mesh element\n            will be used for the subsampling. At most one point sample will\n            be added for each one of the chosen elements\n\n    Layer stack:\n        Creates new layer 'Sampled Mesh'. Current layer is changed to the new\n            layer.\n\n    MeshLab versions:\n        2016.12\n        1.3.4BETA\n    \"\"\"\n    if element.lower() == 'vert':\n        element_num = 0\n    elif element.lower() == 'edge':\n        element_num = 1\n    elif element.lower() == 'face':\n        element_num = 2\n    filter_xml = ''.join([\n        '  <filter name=\"Mesh Element Subsampling\">\\n',\n        '    <Param name=\"Sampling\" ',\n        'value=\"{:d}\" '.format(element_num),\n        'description=\"Element to sample:\" ',\n        'enum_val0=\"Vertex\" ',\n        'enum_val1=\"Edge\" ',\n        'enum_val2=\"Face\" ',\n        'enum_cardinality=\"3\" ',\n        'type=\"RichEnum\" ',\n        '/>\\n',\n        '    <Param name=\"SampleNum\" ',\n        'value=\"{:d}\" '.format(sample_num),\n        'description=\"Number of samples\" ',\n        'type=\"RichInt\" ',\n        '/>\\n',\n        '  </filter>\\n'])\n    util.write_filter(script, filter_xml)\n    if isinstance(script, FilterScript):\n        script.add_layer('Sampled Mesh')\n    return None", "code_tokens": ["def", "mesh_element", "(", "script", ",", "sample_num", "=", "1000", ",", "element", "=", "'VERT'", ")", ":", "if", "element", ".", "lower", "(", ")", "==", "'vert'", ":", "element_num", "=", "0", "elif", "element", ".", "lower", "(", ")", "==", "'edge'", ":", "element_num", "=", "1", "elif", "element", ".", "lower", "(", ")", "==", "'face'", ":", "element_num", "=", "2", "filter_xml", "=", "''", ".", "join", "(", "[", "'  <filter name=\"Mesh Element Subsampling\">\\n'", ",", "'    <Param name=\"Sampling\" '", ",", "'value=\"{:d}\" '", ".", "format", "(", "element_num", ")", ",", "'description=\"Element to sample:\" '", ",", "'enum_val0=\"Vertex\" '", ",", "'enum_val1=\"Edge\" '", ",", "'enum_val2=\"Face\" '", ",", "'enum_cardinality=\"3\" '", ",", "'type=\"RichEnum\" '", ",", "'/>\\n'", ",", "'    <Param name=\"SampleNum\" '", ",", "'value=\"{:d}\" '", ".", "format", "(", "sample_num", ")", ",", "'description=\"Number of samples\" '", ",", "'type=\"RichInt\" '", ",", "'/>\\n'", ",", "'  </filter>\\n'", "]", ")", "util", ".", "write_filter", "(", "script", ",", "filter_xml", ")", "if", "isinstance", "(", "script", ",", "FilterScript", ")", ":", "script", ".", "add_layer", "(", "'Sampled Mesh'", ")", "return", "None"], "docstring": "Create a new layer populated with a point sampling of the current mesh,\n        at most one sample for each element of the mesh is created.\n\n    Samples are taking in a uniform way, one for each element\n    (vertex/edge/face); all the elements have the same probabilty of being\n    choosen.\n\n    Args:\n        script: the FilterScript object or script filename to write\n            the filter to.\n        sample_num (int): The desired number of elements that must be chosen.\n            Being a subsampling of the original elements if this number should\n            not be larger than the number of elements of the original mesh.\n        element (enum in ['VERT', 'EDGE', 'FACE']): Choose what mesh element\n            will be used for the subsampling. At most one point sample will\n            be added for each one of the chosen elements\n\n    Layer stack:\n        Creates new layer 'Sampled Mesh'. Current layer is changed to the new\n            layer.\n\n    MeshLab versions:\n        2016.12\n        1.3.4BETA", "docstring_tokens": ["Create", "a", "new", "layer", "populated", "with", "a", "point", "sampling", "of", "the", "current", "mesh", "at", "most", "one", "sample", "for", "each", "element", "of", "the", "mesh", "is", "created", "."], "sha": "177cce21e92baca500f56a932d66bd9a33257af8", "url": "https://github.com/3DLIRIOUS/MeshLabXML/blob/177cce21e92baca500f56a932d66bd9a33257af8/meshlabxml/sampling.py#L263-L315", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/compiler/parser.py", "func_name": "_with_loc", "original_string": "def _with_loc(f: ParseFunction):\n    \"\"\"Attach any available location information from the input form to\n    the node environment returned from the parsing function.\"\"\"\n\n    @wraps(f)\n    def _parse_form(ctx: ParserContext, form: Union[LispForm, ISeq]) -> Node:\n        form_loc = _loc(form)\n        if form_loc is None:\n            return f(ctx, form)\n        else:\n            return f(ctx, form).fix_missing_locations(form_loc)\n\n    return _parse_form", "language": "python", "code": "def _with_loc(f: ParseFunction):\n    \"\"\"Attach any available location information from the input form to\n    the node environment returned from the parsing function.\"\"\"\n\n    @wraps(f)\n    def _parse_form(ctx: ParserContext, form: Union[LispForm, ISeq]) -> Node:\n        form_loc = _loc(form)\n        if form_loc is None:\n            return f(ctx, form)\n        else:\n            return f(ctx, form).fix_missing_locations(form_loc)\n\n    return _parse_form", "code_tokens": ["def", "_with_loc", "(", "f", ":", "ParseFunction", ")", ":", "@", "wraps", "(", "f", ")", "def", "_parse_form", "(", "ctx", ":", "ParserContext", ",", "form", ":", "Union", "[", "LispForm", ",", "ISeq", "]", ")", "->", "Node", ":", "form_loc", "=", "_loc", "(", "form", ")", "if", "form_loc", "is", "None", ":", "return", "f", "(", "ctx", ",", "form", ")", "else", ":", "return", "f", "(", "ctx", ",", "form", ")", ".", "fix_missing_locations", "(", "form_loc", ")", "return", "_parse_form"], "docstring": "Attach any available location information from the input form to\n    the node environment returned from the parsing function.", "docstring_tokens": ["Attach", "any", "available", "location", "information", "from", "the", "input", "form", "to", "the", "node", "environment", "returned", "from", "the", "parsing", "function", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/compiler/parser.py#L446-L458", "partition": "test"}
{"repo": "chrisjrn/registrasion", "path": "registrasion/controllers/invoice.py", "func_name": "InvoiceController.email", "original_string": "def email(cls, invoice, kind):\n        ''' Sends out an e-mail notifying the user about something to do\n        with that invoice. '''\n\n        context = {\n            \"invoice\": invoice,\n        }\n\n        send_email([invoice.user.email], kind, context=context)", "language": "python", "code": "def email(cls, invoice, kind):\n        ''' Sends out an e-mail notifying the user about something to do\n        with that invoice. '''\n\n        context = {\n            \"invoice\": invoice,\n        }\n\n        send_email([invoice.user.email], kind, context=context)", "code_tokens": ["def", "email", "(", "cls", ",", "invoice", ",", "kind", ")", ":", "context", "=", "{", "\"invoice\"", ":", "invoice", ",", "}", "send_email", "(", "[", "invoice", ".", "user", ".", "email", "]", ",", "kind", ",", "context", "=", "context", ")"], "docstring": "Sends out an e-mail notifying the user about something to do\n        with that invoice.", "docstring_tokens": ["Sends", "out", "an", "e", "-", "mail", "notifying", "the", "user", "about", "something", "to", "do", "with", "that", "invoice", "."], "sha": "461d5846c6f9f3b7099322a94f5d9911564448e4", "url": "https://github.com/chrisjrn/registrasion/blob/461d5846c6f9f3b7099322a94f5d9911564448e4/registrasion/controllers/invoice.py#L415-L423", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/segment.py", "func_name": "timelag_filter", "original_string": "def timelag_filter(function, pad=True, index=0):\n    '''Filtering in the time-lag domain.\n\n    This is primarily useful for adapting image filters to operate on\n    `recurrence_to_lag` output.\n\n    Using `timelag_filter` is equivalent to the following sequence of\n    operations:\n\n    >>> data_tl = librosa.segment.recurrence_to_lag(data)\n    >>> data_filtered_tl = function(data_tl)\n    >>> data_filtered = librosa.segment.lag_to_recurrence(data_filtered_tl)\n\n    Parameters\n    ----------\n    function : callable\n        The filtering function to wrap, e.g., `scipy.ndimage.median_filter`\n\n    pad : bool\n        Whether to zero-pad the structure feature matrix\n\n    index : int >= 0\n        If `function` accepts input data as a positional argument, it should be\n        indexed by `index`\n\n\n    Returns\n    -------\n    wrapped_function : callable\n        A new filter function which applies in time-lag space rather than\n        time-time space.\n\n\n    Examples\n    --------\n\n    Apply a 5-bin median filter to the diagonal of a recurrence matrix\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n    >>> rec = librosa.segment.recurrence_matrix(chroma)\n    >>> from scipy.ndimage import median_filter\n    >>> diagonal_median = librosa.segment.timelag_filter(median_filter)\n    >>> rec_filtered = diagonal_median(rec, size=(1, 3), mode='mirror')\n\n    Or with affinity weights\n\n    >>> rec_aff = librosa.segment.recurrence_matrix(chroma, mode='affinity')\n    >>> rec_aff_fil = diagonal_median(rec_aff, size=(1, 3), mode='mirror')\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8,8))\n    >>> plt.subplot(2, 2, 1)\n    >>> librosa.display.specshow(rec, y_axis='time')\n    >>> plt.title('Raw recurrence matrix')\n    >>> plt.subplot(2, 2, 2)\n    >>> librosa.display.specshow(rec_filtered)\n    >>> plt.title('Filtered recurrence matrix')\n    >>> plt.subplot(2, 2, 3)\n    >>> librosa.display.specshow(rec_aff, x_axis='time', y_axis='time',\n    ...                          cmap='magma_r')\n    >>> plt.title('Raw affinity matrix')\n    >>> plt.subplot(2, 2, 4)\n    >>> librosa.display.specshow(rec_aff_fil, x_axis='time',\n    ...                          cmap='magma_r')\n    >>> plt.title('Filtered affinity matrix')\n    >>> plt.tight_layout()\n    '''\n\n    def __my_filter(wrapped_f, *args, **kwargs):\n        '''Decorator to wrap the filter'''\n        # Map the input data into time-lag space\n        args = list(args)\n\n        args[index] = recurrence_to_lag(args[index], pad=pad)\n\n        # Apply the filtering function\n        result = wrapped_f(*args, **kwargs)\n\n        # Map back into time-time and return\n        return lag_to_recurrence(result)\n\n    return decorator(__my_filter, function)", "language": "python", "code": "def timelag_filter(function, pad=True, index=0):\n    '''Filtering in the time-lag domain.\n\n    This is primarily useful for adapting image filters to operate on\n    `recurrence_to_lag` output.\n\n    Using `timelag_filter` is equivalent to the following sequence of\n    operations:\n\n    >>> data_tl = librosa.segment.recurrence_to_lag(data)\n    >>> data_filtered_tl = function(data_tl)\n    >>> data_filtered = librosa.segment.lag_to_recurrence(data_filtered_tl)\n\n    Parameters\n    ----------\n    function : callable\n        The filtering function to wrap, e.g., `scipy.ndimage.median_filter`\n\n    pad : bool\n        Whether to zero-pad the structure feature matrix\n\n    index : int >= 0\n        If `function` accepts input data as a positional argument, it should be\n        indexed by `index`\n\n\n    Returns\n    -------\n    wrapped_function : callable\n        A new filter function which applies in time-lag space rather than\n        time-time space.\n\n\n    Examples\n    --------\n\n    Apply a 5-bin median filter to the diagonal of a recurrence matrix\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n    >>> rec = librosa.segment.recurrence_matrix(chroma)\n    >>> from scipy.ndimage import median_filter\n    >>> diagonal_median = librosa.segment.timelag_filter(median_filter)\n    >>> rec_filtered = diagonal_median(rec, size=(1, 3), mode='mirror')\n\n    Or with affinity weights\n\n    >>> rec_aff = librosa.segment.recurrence_matrix(chroma, mode='affinity')\n    >>> rec_aff_fil = diagonal_median(rec_aff, size=(1, 3), mode='mirror')\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8,8))\n    >>> plt.subplot(2, 2, 1)\n    >>> librosa.display.specshow(rec, y_axis='time')\n    >>> plt.title('Raw recurrence matrix')\n    >>> plt.subplot(2, 2, 2)\n    >>> librosa.display.specshow(rec_filtered)\n    >>> plt.title('Filtered recurrence matrix')\n    >>> plt.subplot(2, 2, 3)\n    >>> librosa.display.specshow(rec_aff, x_axis='time', y_axis='time',\n    ...                          cmap='magma_r')\n    >>> plt.title('Raw affinity matrix')\n    >>> plt.subplot(2, 2, 4)\n    >>> librosa.display.specshow(rec_aff_fil, x_axis='time',\n    ...                          cmap='magma_r')\n    >>> plt.title('Filtered affinity matrix')\n    >>> plt.tight_layout()\n    '''\n\n    def __my_filter(wrapped_f, *args, **kwargs):\n        '''Decorator to wrap the filter'''\n        # Map the input data into time-lag space\n        args = list(args)\n\n        args[index] = recurrence_to_lag(args[index], pad=pad)\n\n        # Apply the filtering function\n        result = wrapped_f(*args, **kwargs)\n\n        # Map back into time-time and return\n        return lag_to_recurrence(result)\n\n    return decorator(__my_filter, function)", "code_tokens": ["def", "timelag_filter", "(", "function", ",", "pad", "=", "True", ",", "index", "=", "0", ")", ":", "def", "__my_filter", "(", "wrapped_f", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "'''Decorator to wrap the filter'''", "# Map the input data into time-lag space", "args", "=", "list", "(", "args", ")", "args", "[", "index", "]", "=", "recurrence_to_lag", "(", "args", "[", "index", "]", ",", "pad", "=", "pad", ")", "# Apply the filtering function", "result", "=", "wrapped_f", "(", "*", "args", ",", "*", "*", "kwargs", ")", "# Map back into time-time and return", "return", "lag_to_recurrence", "(", "result", ")", "return", "decorator", "(", "__my_filter", ",", "function", ")"], "docstring": "Filtering in the time-lag domain.\n\n    This is primarily useful for adapting image filters to operate on\n    `recurrence_to_lag` output.\n\n    Using `timelag_filter` is equivalent to the following sequence of\n    operations:\n\n    >>> data_tl = librosa.segment.recurrence_to_lag(data)\n    >>> data_filtered_tl = function(data_tl)\n    >>> data_filtered = librosa.segment.lag_to_recurrence(data_filtered_tl)\n\n    Parameters\n    ----------\n    function : callable\n        The filtering function to wrap, e.g., `scipy.ndimage.median_filter`\n\n    pad : bool\n        Whether to zero-pad the structure feature matrix\n\n    index : int >= 0\n        If `function` accepts input data as a positional argument, it should be\n        indexed by `index`\n\n\n    Returns\n    -------\n    wrapped_function : callable\n        A new filter function which applies in time-lag space rather than\n        time-time space.\n\n\n    Examples\n    --------\n\n    Apply a 5-bin median filter to the diagonal of a recurrence matrix\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n    >>> rec = librosa.segment.recurrence_matrix(chroma)\n    >>> from scipy.ndimage import median_filter\n    >>> diagonal_median = librosa.segment.timelag_filter(median_filter)\n    >>> rec_filtered = diagonal_median(rec, size=(1, 3), mode='mirror')\n\n    Or with affinity weights\n\n    >>> rec_aff = librosa.segment.recurrence_matrix(chroma, mode='affinity')\n    >>> rec_aff_fil = diagonal_median(rec_aff, size=(1, 3), mode='mirror')\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8,8))\n    >>> plt.subplot(2, 2, 1)\n    >>> librosa.display.specshow(rec, y_axis='time')\n    >>> plt.title('Raw recurrence matrix')\n    >>> plt.subplot(2, 2, 2)\n    >>> librosa.display.specshow(rec_filtered)\n    >>> plt.title('Filtered recurrence matrix')\n    >>> plt.subplot(2, 2, 3)\n    >>> librosa.display.specshow(rec_aff, x_axis='time', y_axis='time',\n    ...                          cmap='magma_r')\n    >>> plt.title('Raw affinity matrix')\n    >>> plt.subplot(2, 2, 4)\n    >>> librosa.display.specshow(rec_aff_fil, x_axis='time',\n    ...                          cmap='magma_r')\n    >>> plt.title('Filtered affinity matrix')\n    >>> plt.tight_layout()", "docstring_tokens": ["Filtering", "in", "the", "time", "-", "lag", "domain", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/segment.py#L477-L559", "partition": "test"}
{"repo": "google/brotli", "path": "research/brotlidump.py", "func_name": "WithExtra.explanation", "original_string": "def explanation(self, index, extra=None):\n        \"\"\"Expanded version of Code.explanation supporting extra bits.\n        If you don't supply extra, it is not mentioned.\n        \"\"\"\n        extraBits = 0 if extra is None else self.extraBits(index)\n        if not hasattr(self, 'extraTable'):\n            formatString = '{0}{3}'\n            lo = hi = value = self.value(index, extra)\n        elif extraBits==0:\n            formatString = '{0}{2}: {3}'\n            lo, hi = self.span(index)\n            value = lo\n        else:\n            formatString = '{0}{1} {2}: {3}-{4}; {3}+{5}={6}'\n            lo, hi = self.span(index)\n            value = lo+extra\n        return formatString.format(\n            self.description and self.description+': ',\n            'x'*extraBits,\n            self.bitPattern(index),\n            lo, hi,\n            extra,\n            value,\n            )", "language": "python", "code": "def explanation(self, index, extra=None):\n        \"\"\"Expanded version of Code.explanation supporting extra bits.\n        If you don't supply extra, it is not mentioned.\n        \"\"\"\n        extraBits = 0 if extra is None else self.extraBits(index)\n        if not hasattr(self, 'extraTable'):\n            formatString = '{0}{3}'\n            lo = hi = value = self.value(index, extra)\n        elif extraBits==0:\n            formatString = '{0}{2}: {3}'\n            lo, hi = self.span(index)\n            value = lo\n        else:\n            formatString = '{0}{1} {2}: {3}-{4}; {3}+{5}={6}'\n            lo, hi = self.span(index)\n            value = lo+extra\n        return formatString.format(\n            self.description and self.description+': ',\n            'x'*extraBits,\n            self.bitPattern(index),\n            lo, hi,\n            extra,\n            value,\n            )", "code_tokens": ["def", "explanation", "(", "self", ",", "index", ",", "extra", "=", "None", ")", ":", "extraBits", "=", "0", "if", "extra", "is", "None", "else", "self", ".", "extraBits", "(", "index", ")", "if", "not", "hasattr", "(", "self", ",", "'extraTable'", ")", ":", "formatString", "=", "'{0}{3}'", "lo", "=", "hi", "=", "value", "=", "self", ".", "value", "(", "index", ",", "extra", ")", "elif", "extraBits", "==", "0", ":", "formatString", "=", "'{0}{2}: {3}'", "lo", ",", "hi", "=", "self", ".", "span", "(", "index", ")", "value", "=", "lo", "else", ":", "formatString", "=", "'{0}{1} {2}: {3}-{4}; {3}+{5}={6}'", "lo", ",", "hi", "=", "self", ".", "span", "(", "index", ")", "value", "=", "lo", "+", "extra", "return", "formatString", ".", "format", "(", "self", ".", "description", "and", "self", ".", "description", "+", "': '", ",", "'x'", "*", "extraBits", ",", "self", ".", "bitPattern", "(", "index", ")", ",", "lo", ",", "hi", ",", "extra", ",", "value", ",", ")"], "docstring": "Expanded version of Code.explanation supporting extra bits.\n        If you don't supply extra, it is not mentioned.", "docstring_tokens": ["Expanded", "version", "of", "Code", ".", "explanation", "supporting", "extra", "bits", ".", "If", "you", "don", "t", "supply", "extra", "it", "is", "not", "mentioned", "."], "sha": "4b2b2d4f83ffeaac7708e44409fe34896a01a278", "url": "https://github.com/google/brotli/blob/4b2b2d4f83ffeaac7708e44409fe34896a01a278/research/brotlidump.py#L478-L501", "partition": "test"}
{"repo": "codelv/enaml-web", "path": "web/impl/lxml_toolkit_object.py", "func_name": "WebComponent.set_attribute", "original_string": "def set_attribute(self, name, value):\n        \"\"\" Default handler for those not explicitly defined \"\"\"\n        if value is True:\n            self.widget.set(name, name)\n        elif value is False:\n            del self.widget.attrib[name]\n        else:\n            self.widget.set(name, str(value))", "language": "python", "code": "def set_attribute(self, name, value):\n        \"\"\" Default handler for those not explicitly defined \"\"\"\n        if value is True:\n            self.widget.set(name, name)\n        elif value is False:\n            del self.widget.attrib[name]\n        else:\n            self.widget.set(name, str(value))", "code_tokens": ["def", "set_attribute", "(", "self", ",", "name", ",", "value", ")", ":", "if", "value", "is", "True", ":", "self", ".", "widget", ".", "set", "(", "name", ",", "name", ")", "elif", "value", "is", "False", ":", "del", "self", ".", "widget", ".", "attrib", "[", "name", "]", "else", ":", "self", ".", "widget", ".", "set", "(", "name", ",", "str", "(", "value", ")", ")"], "docstring": "Default handler for those not explicitly defined", "docstring_tokens": ["Default", "handler", "for", "those", "not", "explicitly", "defined"], "sha": "88f1131a7b3ba9e83467b4f44bc3bab6f0de7559", "url": "https://github.com/codelv/enaml-web/blob/88f1131a7b3ba9e83467b4f44bc3bab6f0de7559/web/impl/lxml_toolkit_object.py#L263-L270", "partition": "test"}
{"repo": "LuminosoInsight/luminoso-api-client-python", "path": "luminoso_api/v4_json_stream.py", "func_name": "stream_json_lines", "original_string": "def stream_json_lines(file):\n    \"\"\"\n    Load a JSON stream and return a generator, yielding one object at a time.\n    \"\"\"\n    if isinstance(file, string_type):\n        file = open(file, 'rb')\n    for line in file:\n        line = line.strip()\n        if line:\n            if isinstance(line, bytes):\n                line = line.decode('utf-8')\n            yield json.loads(line)", "language": "python", "code": "def stream_json_lines(file):\n    \"\"\"\n    Load a JSON stream and return a generator, yielding one object at a time.\n    \"\"\"\n    if isinstance(file, string_type):\n        file = open(file, 'rb')\n    for line in file:\n        line = line.strip()\n        if line:\n            if isinstance(line, bytes):\n                line = line.decode('utf-8')\n            yield json.loads(line)", "code_tokens": ["def", "stream_json_lines", "(", "file", ")", ":", "if", "isinstance", "(", "file", ",", "string_type", ")", ":", "file", "=", "open", "(", "file", ",", "'rb'", ")", "for", "line", "in", "file", ":", "line", "=", "line", ".", "strip", "(", ")", "if", "line", ":", "if", "isinstance", "(", "line", ",", "bytes", ")", ":", "line", "=", "line", ".", "decode", "(", "'utf-8'", ")", "yield", "json", ".", "loads", "(", "line", ")"], "docstring": "Load a JSON stream and return a generator, yielding one object at a time.", "docstring_tokens": ["Load", "a", "JSON", "stream", "and", "return", "a", "generator", "yielding", "one", "object", "at", "a", "time", "."], "sha": "3bedf2a454aee39214c11fbf556ead3eecc27881", "url": "https://github.com/LuminosoInsight/luminoso-api-client-python/blob/3bedf2a454aee39214c11fbf556ead3eecc27881/luminoso_api/v4_json_stream.py#L175-L186", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/commands/view/hpo.py", "func_name": "hpo", "original_string": "def hpo(context, term, description):\n    \"\"\"Show all hpo terms in the database\"\"\"\n    LOG.info(\"Running scout view hpo\")\n    adapter = context.obj['adapter']\n    if term:\n        term = term.upper()\n        if not term.startswith('HP:'):\n            while len(term) < 7:\n                term = '0' + term\n            term = 'HP:' + term\n        LOG.info(\"Searching for term %s\", term)\n        hpo_terms = adapter.hpo_terms(hpo_term=term)\n    elif description:\n        sorted_terms = sorted(adapter.hpo_terms(query=description), key=itemgetter('hpo_number'))\n        for term in sorted_terms:\n            term.pop('genes')\n            print(\"name: {} | {} | {}\".format(term['_id'], term['description'], term['hpo_number']))\n        # pp(hpo_terms)\n        context.abort()\n    else:\n        hpo_terms = adapter.hpo_terms()\n    if hpo_terms.count() == 0:\n        LOG.warning(\"No matching terms found\")\n        return\n\n    click.echo(\"hpo_id\\tdescription\\tnr_genes\")\n    for hpo_obj in hpo_terms:\n        click.echo(\"{0}\\t{1}\\t{2}\".format(\n            hpo_obj['hpo_id'],\n            hpo_obj['description'],\n            len(hpo_obj.get('genes',[]))\n        ))", "language": "python", "code": "def hpo(context, term, description):\n    \"\"\"Show all hpo terms in the database\"\"\"\n    LOG.info(\"Running scout view hpo\")\n    adapter = context.obj['adapter']\n    if term:\n        term = term.upper()\n        if not term.startswith('HP:'):\n            while len(term) < 7:\n                term = '0' + term\n            term = 'HP:' + term\n        LOG.info(\"Searching for term %s\", term)\n        hpo_terms = adapter.hpo_terms(hpo_term=term)\n    elif description:\n        sorted_terms = sorted(adapter.hpo_terms(query=description), key=itemgetter('hpo_number'))\n        for term in sorted_terms:\n            term.pop('genes')\n            print(\"name: {} | {} | {}\".format(term['_id'], term['description'], term['hpo_number']))\n        # pp(hpo_terms)\n        context.abort()\n    else:\n        hpo_terms = adapter.hpo_terms()\n    if hpo_terms.count() == 0:\n        LOG.warning(\"No matching terms found\")\n        return\n\n    click.echo(\"hpo_id\\tdescription\\tnr_genes\")\n    for hpo_obj in hpo_terms:\n        click.echo(\"{0}\\t{1}\\t{2}\".format(\n            hpo_obj['hpo_id'],\n            hpo_obj['description'],\n            len(hpo_obj.get('genes',[]))\n        ))", "code_tokens": ["def", "hpo", "(", "context", ",", "term", ",", "description", ")", ":", "LOG", ".", "info", "(", "\"Running scout view hpo\"", ")", "adapter", "=", "context", ".", "obj", "[", "'adapter'", "]", "if", "term", ":", "term", "=", "term", ".", "upper", "(", ")", "if", "not", "term", ".", "startswith", "(", "'HP:'", ")", ":", "while", "len", "(", "term", ")", "<", "7", ":", "term", "=", "'0'", "+", "term", "term", "=", "'HP:'", "+", "term", "LOG", ".", "info", "(", "\"Searching for term %s\"", ",", "term", ")", "hpo_terms", "=", "adapter", ".", "hpo_terms", "(", "hpo_term", "=", "term", ")", "elif", "description", ":", "sorted_terms", "=", "sorted", "(", "adapter", ".", "hpo_terms", "(", "query", "=", "description", ")", ",", "key", "=", "itemgetter", "(", "'hpo_number'", ")", ")", "for", "term", "in", "sorted_terms", ":", "term", ".", "pop", "(", "'genes'", ")", "print", "(", "\"name: {} | {} | {}\"", ".", "format", "(", "term", "[", "'_id'", "]", ",", "term", "[", "'description'", "]", ",", "term", "[", "'hpo_number'", "]", ")", ")", "# pp(hpo_terms)", "context", ".", "abort", "(", ")", "else", ":", "hpo_terms", "=", "adapter", ".", "hpo_terms", "(", ")", "if", "hpo_terms", ".", "count", "(", ")", "==", "0", ":", "LOG", ".", "warning", "(", "\"No matching terms found\"", ")", "return", "click", ".", "echo", "(", "\"hpo_id\\tdescription\\tnr_genes\"", ")", "for", "hpo_obj", "in", "hpo_terms", ":", "click", ".", "echo", "(", "\"{0}\\t{1}\\t{2}\"", ".", "format", "(", "hpo_obj", "[", "'hpo_id'", "]", ",", "hpo_obj", "[", "'description'", "]", ",", "len", "(", "hpo_obj", ".", "get", "(", "'genes'", ",", "[", "]", ")", ")", ")", ")"], "docstring": "Show all hpo terms in the database", "docstring_tokens": ["Show", "all", "hpo", "terms", "in", "the", "database"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/commands/view/hpo.py#L14-L45", "partition": "test"}
{"repo": "amyth/django-instapush", "path": "instapush/libs/gcm.py", "func_name": "gcm_send_message", "original_string": "def gcm_send_message(registration_id, data, encoding='utf-8', **kwargs):\n    \"\"\"\n    Standalone method to send a single gcm notification\n    \"\"\"\n\n    messenger = GCMMessenger(registration_id, data, encoding=encoding, **kwargs)\n    return messenger.send_plain()", "language": "python", "code": "def gcm_send_message(registration_id, data, encoding='utf-8', **kwargs):\n    \"\"\"\n    Standalone method to send a single gcm notification\n    \"\"\"\n\n    messenger = GCMMessenger(registration_id, data, encoding=encoding, **kwargs)\n    return messenger.send_plain()", "code_tokens": ["def", "gcm_send_message", "(", "registration_id", ",", "data", ",", "encoding", "=", "'utf-8'", ",", "*", "*", "kwargs", ")", ":", "messenger", "=", "GCMMessenger", "(", "registration_id", ",", "data", ",", "encoding", "=", "encoding", ",", "*", "*", "kwargs", ")", "return", "messenger", ".", "send_plain", "(", ")"], "docstring": "Standalone method to send a single gcm notification", "docstring_tokens": ["Standalone", "method", "to", "send", "a", "single", "gcm", "notification"], "sha": "f8643a2e342fc73a16c95dff79c3daac8ce4b034", "url": "https://github.com/amyth/django-instapush/blob/f8643a2e342fc73a16c95dff79c3daac8ce4b034/instapush/libs/gcm.py#L150-L156", "partition": "test"}
{"repo": "dossier/dossier.store", "path": "dossier/store/store.py", "func_name": "feature_index", "original_string": "def feature_index(*feature_names):\n    '''Returns a index creation function.\n\n    Returns a valid index ``create`` function for the feature names\n    given. This can be used with the :meth:`Store.define_index`\n    method to create indexes on any combination of features in a\n    feature collection.\n\n    :type feature_names: list(unicode)\n    :rtype: ``(val -> index val)\n              -> (content_id, FeatureCollection)\n              -> generator of [index val]``\n    '''\n    def _(trans, (cid, fc)):\n        for fname in feature_names:\n            feat = fc.get(fname)\n            if feat is None:\n                continue\n            elif isinstance(feat, unicode):\n                yield trans(feat)\n            else:  # string counter, sparse/dense vector\n                for val in feat.iterkeys():\n                    yield trans(val)\n    return _", "language": "python", "code": "def feature_index(*feature_names):\n    '''Returns a index creation function.\n\n    Returns a valid index ``create`` function for the feature names\n    given. This can be used with the :meth:`Store.define_index`\n    method to create indexes on any combination of features in a\n    feature collection.\n\n    :type feature_names: list(unicode)\n    :rtype: ``(val -> index val)\n              -> (content_id, FeatureCollection)\n              -> generator of [index val]``\n    '''\n    def _(trans, (cid, fc)):\n        for fname in feature_names:\n            feat = fc.get(fname)\n            if feat is None:\n                continue\n            elif isinstance(feat, unicode):\n                yield trans(feat)\n            else:  # string counter, sparse/dense vector\n                for val in feat.iterkeys():\n                    yield trans(val)\n    return _", "code_tokens": ["def", "feature_index", "(", "*", "feature_names", ")", ":", "def", "_", "(", "trans", ",", "(", "cid", ",", "fc", ")", ")", ":", "for", "fname", "in", "feature_names", ":", "feat", "=", "fc", ".", "get", "(", "fname", ")", "if", "feat", "is", "None", ":", "continue", "elif", "isinstance", "(", "feat", ",", "unicode", ")", ":", "yield", "trans", "(", "feat", ")", "else", ":", "# string counter, sparse/dense vector", "for", "val", "in", "feat", ".", "iterkeys", "(", ")", ":", "yield", "trans", "(", "val", ")", "return", "_"], "docstring": "Returns a index creation function.\n\n    Returns a valid index ``create`` function for the feature names\n    given. This can be used with the :meth:`Store.define_index`\n    method to create indexes on any combination of features in a\n    feature collection.\n\n    :type feature_names: list(unicode)\n    :rtype: ``(val -> index val)\n              -> (content_id, FeatureCollection)\n              -> generator of [index val]``", "docstring_tokens": ["Returns", "a", "index", "creation", "function", "."], "sha": "b22ffe2470bba9fcc98a30cb55b437bfa1521e7f", "url": "https://github.com/dossier/dossier.store/blob/b22ffe2470bba9fcc98a30cb55b437bfa1521e7f/dossier/store/store.py#L21-L44", "partition": "test"}
{"repo": "openergy/oplus", "path": "oplus/epm/table_descriptor.py", "func_name": "TableDescriptor.prepare_extensible", "original_string": "def prepare_extensible(self):\n        \"\"\"\n        This function finishes initialization, must be called once all field descriptors and tag have been filled.\n        \"\"\"\n        # see if extensible and store cycle len\n        for k in self._tags:\n            if \"extensible\" in k:\n                cycle_len = int(k.split(\":\")[1])\n                break\n        else:\n            # not extensible\n            return\n\n        # find cycle start and prepare patterns\n        cycle_start = None\n        cycle_patterns = []\n        for i, field_descriptor in enumerate(self._field_descriptors):\n            # quit if finished\n            if (cycle_start is not None) and (i >= (cycle_start + cycle_len)):\n                break\n\n            # set cycle start if not set yet\n            if (cycle_start is None) and (\"begin-extensible\" in field_descriptor.tags):\n                cycle_start = i\n\n            # leave if cycle start not reached yet\n            if cycle_start is None:\n                continue\n\n            # store pattern\n            cycle_patterns.append(field_descriptor.ref.replace(\"1\", r\"(\\d+)\"))\n        else:\n            raise RuntimeError(\"cycle start not found\")\n\n        # detach unnecessary field descriptors\n        self._field_descriptors = self._field_descriptors[:cycle_start + cycle_len]\n\n        # store cycle info\n        self.extensible_info = (cycle_start, cycle_len, tuple(cycle_patterns))\n\n        # set field descriptor cycle_start index (for error messages while serialization)\n        for i, fd in enumerate(self._field_descriptors[cycle_start:]):\n            fd.set_extensible_info(cycle_start, cycle_len, cycle_patterns[i])", "language": "python", "code": "def prepare_extensible(self):\n        \"\"\"\n        This function finishes initialization, must be called once all field descriptors and tag have been filled.\n        \"\"\"\n        # see if extensible and store cycle len\n        for k in self._tags:\n            if \"extensible\" in k:\n                cycle_len = int(k.split(\":\")[1])\n                break\n        else:\n            # not extensible\n            return\n\n        # find cycle start and prepare patterns\n        cycle_start = None\n        cycle_patterns = []\n        for i, field_descriptor in enumerate(self._field_descriptors):\n            # quit if finished\n            if (cycle_start is not None) and (i >= (cycle_start + cycle_len)):\n                break\n\n            # set cycle start if not set yet\n            if (cycle_start is None) and (\"begin-extensible\" in field_descriptor.tags):\n                cycle_start = i\n\n            # leave if cycle start not reached yet\n            if cycle_start is None:\n                continue\n\n            # store pattern\n            cycle_patterns.append(field_descriptor.ref.replace(\"1\", r\"(\\d+)\"))\n        else:\n            raise RuntimeError(\"cycle start not found\")\n\n        # detach unnecessary field descriptors\n        self._field_descriptors = self._field_descriptors[:cycle_start + cycle_len]\n\n        # store cycle info\n        self.extensible_info = (cycle_start, cycle_len, tuple(cycle_patterns))\n\n        # set field descriptor cycle_start index (for error messages while serialization)\n        for i, fd in enumerate(self._field_descriptors[cycle_start:]):\n            fd.set_extensible_info(cycle_start, cycle_len, cycle_patterns[i])", "code_tokens": ["def", "prepare_extensible", "(", "self", ")", ":", "# see if extensible and store cycle len", "for", "k", "in", "self", ".", "_tags", ":", "if", "\"extensible\"", "in", "k", ":", "cycle_len", "=", "int", "(", "k", ".", "split", "(", "\":\"", ")", "[", "1", "]", ")", "break", "else", ":", "# not extensible", "return", "# find cycle start and prepare patterns", "cycle_start", "=", "None", "cycle_patterns", "=", "[", "]", "for", "i", ",", "field_descriptor", "in", "enumerate", "(", "self", ".", "_field_descriptors", ")", ":", "# quit if finished", "if", "(", "cycle_start", "is", "not", "None", ")", "and", "(", "i", ">=", "(", "cycle_start", "+", "cycle_len", ")", ")", ":", "break", "# set cycle start if not set yet", "if", "(", "cycle_start", "is", "None", ")", "and", "(", "\"begin-extensible\"", "in", "field_descriptor", ".", "tags", ")", ":", "cycle_start", "=", "i", "# leave if cycle start not reached yet", "if", "cycle_start", "is", "None", ":", "continue", "# store pattern", "cycle_patterns", ".", "append", "(", "field_descriptor", ".", "ref", ".", "replace", "(", "\"1\"", ",", "r\"(\\d+)\"", ")", ")", "else", ":", "raise", "RuntimeError", "(", "\"cycle start not found\"", ")", "# detach unnecessary field descriptors", "self", ".", "_field_descriptors", "=", "self", ".", "_field_descriptors", "[", ":", "cycle_start", "+", "cycle_len", "]", "# store cycle info", "self", ".", "extensible_info", "=", "(", "cycle_start", ",", "cycle_len", ",", "tuple", "(", "cycle_patterns", ")", ")", "# set field descriptor cycle_start index (for error messages while serialization)", "for", "i", ",", "fd", "in", "enumerate", "(", "self", ".", "_field_descriptors", "[", "cycle_start", ":", "]", ")", ":", "fd", ".", "set_extensible_info", "(", "cycle_start", ",", "cycle_len", ",", "cycle_patterns", "[", "i", "]", ")"], "docstring": "This function finishes initialization, must be called once all field descriptors and tag have been filled.", "docstring_tokens": ["This", "function", "finishes", "initialization", "must", "be", "called", "once", "all", "field", "descriptors", "and", "tag", "have", "been", "filled", "."], "sha": "f095868d1990c1d126e906ada6acbab26348b3d3", "url": "https://github.com/openergy/oplus/blob/f095868d1990c1d126e906ada6acbab26348b3d3/oplus/epm/table_descriptor.py#L49-L91", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/runtime.py", "func_name": "Namespace.get_or_create", "original_string": "def get_or_create(\n        cls, name: sym.Symbol, module: types.ModuleType = None\n    ) -> \"Namespace\":\n        \"\"\"Get the namespace bound to the symbol `name` in the global namespace\n        cache, creating it if it does not exist.\n        Return the namespace.\"\"\"\n        return cls._NAMESPACES.swap(Namespace.__get_or_create, name, module=module)[\n            name\n        ]", "language": "python", "code": "def get_or_create(\n        cls, name: sym.Symbol, module: types.ModuleType = None\n    ) -> \"Namespace\":\n        \"\"\"Get the namespace bound to the symbol `name` in the global namespace\n        cache, creating it if it does not exist.\n        Return the namespace.\"\"\"\n        return cls._NAMESPACES.swap(Namespace.__get_or_create, name, module=module)[\n            name\n        ]", "code_tokens": ["def", "get_or_create", "(", "cls", ",", "name", ":", "sym", ".", "Symbol", ",", "module", ":", "types", ".", "ModuleType", "=", "None", ")", "->", "\"Namespace\"", ":", "return", "cls", ".", "_NAMESPACES", ".", "swap", "(", "Namespace", ".", "__get_or_create", ",", "name", ",", "module", "=", "module", ")", "[", "name", "]"], "docstring": "Get the namespace bound to the symbol `name` in the global namespace\n        cache, creating it if it does not exist.\n        Return the namespace.", "docstring_tokens": ["Get", "the", "namespace", "bound", "to", "the", "symbol", "name", "in", "the", "global", "namespace", "cache", "creating", "it", "if", "it", "does", "not", "exist", ".", "Return", "the", "namespace", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/runtime.py#L556-L564", "partition": "test"}
{"repo": "open-mmlab/mmcv", "path": "mmcv/utils/timer.py", "func_name": "check_time", "original_string": "def check_time(timer_id):\n    \"\"\"Add check points in a single line.\n\n    This method is suitable for running a task on a list of items. A timer will\n    be registered when the method is called for the first time.\n\n    :Example:\n\n    >>> import time\n    >>> import mmcv\n    >>> for i in range(1, 6):\n    >>>     # simulate a code block\n    >>>     time.sleep(i)\n    >>>     mmcv.check_time('task1')\n    2.000\n    3.000\n    4.000\n    5.000\n\n    Args:\n        timer_id (str): Timer identifier.\n    \"\"\"\n    if timer_id not in _g_timers:\n        _g_timers[timer_id] = Timer()\n        return 0\n    else:\n        return _g_timers[timer_id].since_last_check()", "language": "python", "code": "def check_time(timer_id):\n    \"\"\"Add check points in a single line.\n\n    This method is suitable for running a task on a list of items. A timer will\n    be registered when the method is called for the first time.\n\n    :Example:\n\n    >>> import time\n    >>> import mmcv\n    >>> for i in range(1, 6):\n    >>>     # simulate a code block\n    >>>     time.sleep(i)\n    >>>     mmcv.check_time('task1')\n    2.000\n    3.000\n    4.000\n    5.000\n\n    Args:\n        timer_id (str): Timer identifier.\n    \"\"\"\n    if timer_id not in _g_timers:\n        _g_timers[timer_id] = Timer()\n        return 0\n    else:\n        return _g_timers[timer_id].since_last_check()", "code_tokens": ["def", "check_time", "(", "timer_id", ")", ":", "if", "timer_id", "not", "in", "_g_timers", ":", "_g_timers", "[", "timer_id", "]", "=", "Timer", "(", ")", "return", "0", "else", ":", "return", "_g_timers", "[", "timer_id", "]", ".", "since_last_check", "(", ")"], "docstring": "Add check points in a single line.\n\n    This method is suitable for running a task on a list of items. A timer will\n    be registered when the method is called for the first time.\n\n    :Example:\n\n    >>> import time\n    >>> import mmcv\n    >>> for i in range(1, 6):\n    >>>     # simulate a code block\n    >>>     time.sleep(i)\n    >>>     mmcv.check_time('task1')\n    2.000\n    3.000\n    4.000\n    5.000\n\n    Args:\n        timer_id (str): Timer identifier.", "docstring_tokens": ["Add", "check", "points", "in", "a", "single", "line", "."], "sha": "0d77f61450aab4dde8b8585a577cc496acb95d7f", "url": "https://github.com/open-mmlab/mmcv/blob/0d77f61450aab4dde8b8585a577cc496acb95d7f/mmcv/utils/timer.py#L91-L117", "partition": "test"}
{"repo": "astrorafael/twisted-mqtt", "path": "mqtt/pdu.py", "func_name": "PUBREL.decode", "original_string": "def decode(self, packet):\n        '''\n        Decode a PUBREL control packet. \n        '''\n        self.encoded = packet\n        lenLen = 1\n        while packet[lenLen] & 0x80:\n            lenLen += 1\n        packet_remaining = packet[lenLen+1:]\n        self.msgId  = decode16Int(packet_remaining)\n        self.dup = (packet[0] & 0x08) == 0x08", "language": "python", "code": "def decode(self, packet):\n        '''\n        Decode a PUBREL control packet. \n        '''\n        self.encoded = packet\n        lenLen = 1\n        while packet[lenLen] & 0x80:\n            lenLen += 1\n        packet_remaining = packet[lenLen+1:]\n        self.msgId  = decode16Int(packet_remaining)\n        self.dup = (packet[0] & 0x08) == 0x08", "code_tokens": ["def", "decode", "(", "self", ",", "packet", ")", ":", "self", ".", "encoded", "=", "packet", "lenLen", "=", "1", "while", "packet", "[", "lenLen", "]", "&", "0x80", ":", "lenLen", "+=", "1", "packet_remaining", "=", "packet", "[", "lenLen", "+", "1", ":", "]", "self", ".", "msgId", "=", "decode16Int", "(", "packet_remaining", ")", "self", ".", "dup", "=", "(", "packet", "[", "0", "]", "&", "0x08", ")", "==", "0x08"], "docstring": "Decode a PUBREL control packet.", "docstring_tokens": ["Decode", "a", "PUBREL", "control", "packet", "."], "sha": "5b322f7c2b82a502b1e1b70703ae45f1f668d07d", "url": "https://github.com/astrorafael/twisted-mqtt/blob/5b322f7c2b82a502b1e1b70703ae45f1f668d07d/mqtt/pdu.py#L651-L661", "partition": "test"}
{"repo": "LLNL/scraper", "path": "scripts/get_traffic.py", "func_name": "GitHub_Traffic.write_to_file", "original_string": "def write_to_file(self, referrers_file_path='', views_file_path='',\n        clones_file_path='', date=(datetime.date.today()), organization='llnl',\n        views_row_count=0, clones_row_count=0):\n        \"\"\"\n        Writes all traffic data to file.\n        \"\"\"\n        self.write_referrers_to_file(file_path=referrers_file_path)\n        self.write_data_to_file(file_path=views_file_path,\n            dict_to_write=self.views, name='views',\n            row_count=views_row_count)\n        self.write_data_to_file(file_path=clones_file_path,\n            dict_to_write=self.clones, name='clones',\n            row_count=clones_row_count)", "language": "python", "code": "def write_to_file(self, referrers_file_path='', views_file_path='',\n        clones_file_path='', date=(datetime.date.today()), organization='llnl',\n        views_row_count=0, clones_row_count=0):\n        \"\"\"\n        Writes all traffic data to file.\n        \"\"\"\n        self.write_referrers_to_file(file_path=referrers_file_path)\n        self.write_data_to_file(file_path=views_file_path,\n            dict_to_write=self.views, name='views',\n            row_count=views_row_count)\n        self.write_data_to_file(file_path=clones_file_path,\n            dict_to_write=self.clones, name='clones',\n            row_count=clones_row_count)", "code_tokens": ["def", "write_to_file", "(", "self", ",", "referrers_file_path", "=", "''", ",", "views_file_path", "=", "''", ",", "clones_file_path", "=", "''", ",", "date", "=", "(", "datetime", ".", "date", ".", "today", "(", ")", ")", ",", "organization", "=", "'llnl'", ",", "views_row_count", "=", "0", ",", "clones_row_count", "=", "0", ")", ":", "self", ".", "write_referrers_to_file", "(", "file_path", "=", "referrers_file_path", ")", "self", ".", "write_data_to_file", "(", "file_path", "=", "views_file_path", ",", "dict_to_write", "=", "self", ".", "views", ",", "name", "=", "'views'", ",", "row_count", "=", "views_row_count", ")", "self", ".", "write_data_to_file", "(", "file_path", "=", "clones_file_path", ",", "dict_to_write", "=", "self", ".", "clones", ",", "name", "=", "'clones'", ",", "row_count", "=", "clones_row_count", ")"], "docstring": "Writes all traffic data to file.", "docstring_tokens": ["Writes", "all", "traffic", "data", "to", "file", "."], "sha": "881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea", "url": "https://github.com/LLNL/scraper/blob/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea/scripts/get_traffic.py#L219-L231", "partition": "test"}
{"repo": "assemblerflow/flowcraft", "path": "flowcraft/generator/engine.py", "func_name": "NextflowGenerator._set_configurations", "original_string": "def _set_configurations(self):\n        \"\"\"This method will iterate over all process in the pipeline and\n        populate the nextflow configuration files with the directives\n        of each process in the pipeline.\n        \"\"\"\n\n        logger.debug(\"======================\")\n        logger.debug(\"Setting configurations\")\n        logger.debug(\"======================\")\n\n        resources = \"\"\n        containers = \"\"\n        params = \"\"\n        manifest = \"\"\n\n        if self.merge_params:\n            params += self._get_merged_params_string()\n            help_list = self._get_merged_params_help()\n        else:\n            params += self._get_params_string()\n            help_list = self._get_params_help()\n\n        for p in self.processes:\n\n            # Skip processes with the directives attribute populated\n            if not p.directives:\n                continue\n\n            logger.debug(\"[{}] Adding directives: {}\".format(\n                p.template, p.directives))\n            resources += self._get_resources_string(p.directives, p.pid)\n            containers += self._get_container_string(p.directives, p.pid)\n\n        manifest = self._get_manifest_string()\n\n        self.resources = self._render_config(\"resources.config\", {\n            \"process_info\": resources\n        })\n        self.containers = self._render_config(\"containers.config\", {\n            \"container_info\": containers\n        })\n        self.params = self._render_config(\"params.config\", {\n            \"params_info\": params\n        })\n        self.manifest = self._render_config(\"manifest.config\", {\n            \"manifest_info\": manifest\n        })\n        self.help = self._render_config(\"Helper.groovy\", {\n            \"nf_file\": basename(self.nf_file),\n            \"help_list\": help_list,\n            \"version\": __version__,\n            \"pipeline_name\": \" \".join([x.upper() for x in self.pipeline_name])\n        })\n        self.user_config = self._render_config(\"user.config\", {})", "language": "python", "code": "def _set_configurations(self):\n        \"\"\"This method will iterate over all process in the pipeline and\n        populate the nextflow configuration files with the directives\n        of each process in the pipeline.\n        \"\"\"\n\n        logger.debug(\"======================\")\n        logger.debug(\"Setting configurations\")\n        logger.debug(\"======================\")\n\n        resources = \"\"\n        containers = \"\"\n        params = \"\"\n        manifest = \"\"\n\n        if self.merge_params:\n            params += self._get_merged_params_string()\n            help_list = self._get_merged_params_help()\n        else:\n            params += self._get_params_string()\n            help_list = self._get_params_help()\n\n        for p in self.processes:\n\n            # Skip processes with the directives attribute populated\n            if not p.directives:\n                continue\n\n            logger.debug(\"[{}] Adding directives: {}\".format(\n                p.template, p.directives))\n            resources += self._get_resources_string(p.directives, p.pid)\n            containers += self._get_container_string(p.directives, p.pid)\n\n        manifest = self._get_manifest_string()\n\n        self.resources = self._render_config(\"resources.config\", {\n            \"process_info\": resources\n        })\n        self.containers = self._render_config(\"containers.config\", {\n            \"container_info\": containers\n        })\n        self.params = self._render_config(\"params.config\", {\n            \"params_info\": params\n        })\n        self.manifest = self._render_config(\"manifest.config\", {\n            \"manifest_info\": manifest\n        })\n        self.help = self._render_config(\"Helper.groovy\", {\n            \"nf_file\": basename(self.nf_file),\n            \"help_list\": help_list,\n            \"version\": __version__,\n            \"pipeline_name\": \" \".join([x.upper() for x in self.pipeline_name])\n        })\n        self.user_config = self._render_config(\"user.config\", {})", "code_tokens": ["def", "_set_configurations", "(", "self", ")", ":", "logger", ".", "debug", "(", "\"======================\"", ")", "logger", ".", "debug", "(", "\"Setting configurations\"", ")", "logger", ".", "debug", "(", "\"======================\"", ")", "resources", "=", "\"\"", "containers", "=", "\"\"", "params", "=", "\"\"", "manifest", "=", "\"\"", "if", "self", ".", "merge_params", ":", "params", "+=", "self", ".", "_get_merged_params_string", "(", ")", "help_list", "=", "self", ".", "_get_merged_params_help", "(", ")", "else", ":", "params", "+=", "self", ".", "_get_params_string", "(", ")", "help_list", "=", "self", ".", "_get_params_help", "(", ")", "for", "p", "in", "self", ".", "processes", ":", "# Skip processes with the directives attribute populated", "if", "not", "p", ".", "directives", ":", "continue", "logger", ".", "debug", "(", "\"[{}] Adding directives: {}\"", ".", "format", "(", "p", ".", "template", ",", "p", ".", "directives", ")", ")", "resources", "+=", "self", ".", "_get_resources_string", "(", "p", ".", "directives", ",", "p", ".", "pid", ")", "containers", "+=", "self", ".", "_get_container_string", "(", "p", ".", "directives", ",", "p", ".", "pid", ")", "manifest", "=", "self", ".", "_get_manifest_string", "(", ")", "self", ".", "resources", "=", "self", ".", "_render_config", "(", "\"resources.config\"", ",", "{", "\"process_info\"", ":", "resources", "}", ")", "self", ".", "containers", "=", "self", ".", "_render_config", "(", "\"containers.config\"", ",", "{", "\"container_info\"", ":", "containers", "}", ")", "self", ".", "params", "=", "self", ".", "_render_config", "(", "\"params.config\"", ",", "{", "\"params_info\"", ":", "params", "}", ")", "self", ".", "manifest", "=", "self", ".", "_render_config", "(", "\"manifest.config\"", ",", "{", "\"manifest_info\"", ":", "manifest", "}", ")", "self", ".", "help", "=", "self", ".", "_render_config", "(", "\"Helper.groovy\"", ",", "{", "\"nf_file\"", ":", "basename", "(", "self", ".", "nf_file", ")", ",", "\"help_list\"", ":", "help_list", ",", "\"version\"", ":", "__version__", ",", "\"pipeline_name\"", ":", "\" \"", ".", "join", "(", "[", "x", ".", "upper", "(", ")", "for", "x", "in", "self", ".", "pipeline_name", "]", ")", "}", ")", "self", ".", "user_config", "=", "self", ".", "_render_config", "(", "\"user.config\"", ",", "{", "}", ")"], "docstring": "This method will iterate over all process in the pipeline and\n        populate the nextflow configuration files with the directives\n        of each process in the pipeline.", "docstring_tokens": ["This", "method", "will", "iterate", "over", "all", "process", "in", "the", "pipeline", "and", "populate", "the", "nextflow", "configuration", "files", "with", "the", "directives", "of", "each", "process", "in", "the", "pipeline", "."], "sha": "fc3f4bddded1efc76006600016dc71a06dd908c0", "url": "https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/generator/engine.py#L1267-L1320", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/hidden_markov_model.py", "func_name": "_vector_matrix", "original_string": "def _vector_matrix(vs, ms):\n  \"\"\"Multiply tensor of vectors by matrices.\"\"\"\n\n  return tf.reduce_sum(input_tensor=vs[..., tf.newaxis] * ms, axis=-2)", "language": "python", "code": "def _vector_matrix(vs, ms):\n  \"\"\"Multiply tensor of vectors by matrices.\"\"\"\n\n  return tf.reduce_sum(input_tensor=vs[..., tf.newaxis] * ms, axis=-2)", "code_tokens": ["def", "_vector_matrix", "(", "vs", ",", "ms", ")", ":", "return", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "vs", "[", "...", ",", "tf", ".", "newaxis", "]", "*", "ms", ",", "axis", "=", "-", "2", ")"], "docstring": "Multiply tensor of vectors by matrices.", "docstring_tokens": ["Multiply", "tensor", "of", "vectors", "by", "matrices", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/hidden_markov_model.py#L920-L923", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/dtype_util.py", "func_name": "as_numpy_dtype", "original_string": "def as_numpy_dtype(dtype):\n  \"\"\"Returns a `np.dtype` based on this `dtype`.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'as_numpy_dtype'):\n    return dtype.as_numpy_dtype\n  return dtype", "language": "python", "code": "def as_numpy_dtype(dtype):\n  \"\"\"Returns a `np.dtype` based on this `dtype`.\"\"\"\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'as_numpy_dtype'):\n    return dtype.as_numpy_dtype\n  return dtype", "code_tokens": ["def", "as_numpy_dtype", "(", "dtype", ")", ":", "dtype", "=", "tf", ".", "as_dtype", "(", "dtype", ")", "if", "hasattr", "(", "dtype", ",", "'as_numpy_dtype'", ")", ":", "return", "dtype", ".", "as_numpy_dtype", "return", "dtype"], "docstring": "Returns a `np.dtype` based on this `dtype`.", "docstring_tokens": ["Returns", "a", "np", ".", "dtype", "based", "on", "this", "dtype", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/dtype_util.py#L44-L49", "partition": "test"}
{"repo": "quantopian/pgcontents", "path": "pgcontents/checkpoints.py", "func_name": "PostgresCheckpoints.delete_checkpoint", "original_string": "def delete_checkpoint(self, checkpoint_id, path):\n        \"\"\"delete a checkpoint for a file\"\"\"\n        with self.engine.begin() as db:\n            return delete_single_remote_checkpoint(\n                db, self.user_id, path, checkpoint_id,\n            )", "language": "python", "code": "def delete_checkpoint(self, checkpoint_id, path):\n        \"\"\"delete a checkpoint for a file\"\"\"\n        with self.engine.begin() as db:\n            return delete_single_remote_checkpoint(\n                db, self.user_id, path, checkpoint_id,\n            )", "code_tokens": ["def", "delete_checkpoint", "(", "self", ",", "checkpoint_id", ",", "path", ")", ":", "with", "self", ".", "engine", ".", "begin", "(", ")", "as", "db", ":", "return", "delete_single_remote_checkpoint", "(", "db", ",", "self", ".", "user_id", ",", "path", ",", "checkpoint_id", ",", ")"], "docstring": "delete a checkpoint for a file", "docstring_tokens": ["delete", "a", "checkpoint", "for", "a", "file"], "sha": "ed36268b7917332d16868208e1e565742a8753e1", "url": "https://github.com/quantopian/pgcontents/blob/ed36268b7917332d16868208e1e565742a8753e1/pgcontents/checkpoints.py#L71-L76", "partition": "test"}
{"repo": "lincolnloop/goodconf", "path": "goodconf/__init__.py", "func_name": "GoodConf.generate_markdown", "original_string": "def generate_markdown(cls):\n        \"\"\"\n        Documents values in markdown\n        \"\"\"\n        lines = []\n        if cls.__doc__:\n            lines.extend(['# {}'.format(cls.__doc__), ''])\n        for k, v in cls._values.items():\n            lines.append('* **{}**  '.format(k))\n            if v.required:\n                lines[-1] = lines[-1] + '_REQUIRED_  '\n            if v.help:\n                lines.append('  {}  '.format(v.help))\n            lines.append('  type: `{}`  '.format(v.cast_as.__name__))\n            if v.default is not None:\n                lines.append('  default: `{}`  '.format(v.default))\n        return '\\n'.join(lines)", "language": "python", "code": "def generate_markdown(cls):\n        \"\"\"\n        Documents values in markdown\n        \"\"\"\n        lines = []\n        if cls.__doc__:\n            lines.extend(['# {}'.format(cls.__doc__), ''])\n        for k, v in cls._values.items():\n            lines.append('* **{}**  '.format(k))\n            if v.required:\n                lines[-1] = lines[-1] + '_REQUIRED_  '\n            if v.help:\n                lines.append('  {}  '.format(v.help))\n            lines.append('  type: `{}`  '.format(v.cast_as.__name__))\n            if v.default is not None:\n                lines.append('  default: `{}`  '.format(v.default))\n        return '\\n'.join(lines)", "code_tokens": ["def", "generate_markdown", "(", "cls", ")", ":", "lines", "=", "[", "]", "if", "cls", ".", "__doc__", ":", "lines", ".", "extend", "(", "[", "'# {}'", ".", "format", "(", "cls", ".", "__doc__", ")", ",", "''", "]", ")", "for", "k", ",", "v", "in", "cls", ".", "_values", ".", "items", "(", ")", ":", "lines", ".", "append", "(", "'* **{}**  '", ".", "format", "(", "k", ")", ")", "if", "v", ".", "required", ":", "lines", "[", "-", "1", "]", "=", "lines", "[", "-", "1", "]", "+", "'_REQUIRED_  '", "if", "v", ".", "help", ":", "lines", ".", "append", "(", "'  {}  '", ".", "format", "(", "v", ".", "help", ")", ")", "lines", ".", "append", "(", "'  type: `{}`  '", ".", "format", "(", "v", ".", "cast_as", ".", "__name__", ")", ")", "if", "v", ".", "default", "is", "not", "None", ":", "lines", ".", "append", "(", "'  default: `{}`  '", ".", "format", "(", "v", ".", "default", ")", ")", "return", "'\\n'", ".", "join", "(", "lines", ")"], "docstring": "Documents values in markdown", "docstring_tokens": ["Documents", "values", "in", "markdown"], "sha": "19515da5783f86b9516dbf81531107c2d9eae567", "url": "https://github.com/lincolnloop/goodconf/blob/19515da5783f86b9516dbf81531107c2d9eae567/goodconf/__init__.py#L129-L145", "partition": "test"}
{"repo": "google/gin-config", "path": "gin/config.py", "func_name": "query_parameter", "original_string": "def query_parameter(binding_key):\n  \"\"\"Returns the currently bound value to the specified `binding_key`.\n\n  The `binding_key` argument should look like\n  'maybe/some/scope/maybe.moduels.configurable_name.parameter_name'. Note that\n  this will not include default parameters.\n\n  Args:\n    binding_key: The parameter whose value should be set.\n\n  Returns:\n    The value bound to the configurable/parameter combination given in\n    `binding_key`.\n\n  Raises:\n    ValueError: If no function can be found matching the configurable name\n      specified by `biding_key`, or if the specified parameter name is\n      blacklisted or not in the function's whitelist (if present) or if there is\n      no value bound for the queried parameter or configurable.\n  \"\"\"\n  pbk = ParsedBindingKey(binding_key)\n  if pbk.config_key not in _CONFIG:\n    err_str = \"Configurable '{}' has no bound parameters.\"\n    raise ValueError(err_str.format(pbk.given_selector))\n  if pbk.arg_name not in _CONFIG[pbk.config_key]:\n    err_str = \"Configurable '{}' has no value bound for parameter '{}'.\"\n    raise ValueError(err_str.format(pbk.given_selector, pbk.arg_name))\n  return _CONFIG[pbk.config_key][pbk.arg_name]", "language": "python", "code": "def query_parameter(binding_key):\n  \"\"\"Returns the currently bound value to the specified `binding_key`.\n\n  The `binding_key` argument should look like\n  'maybe/some/scope/maybe.moduels.configurable_name.parameter_name'. Note that\n  this will not include default parameters.\n\n  Args:\n    binding_key: The parameter whose value should be set.\n\n  Returns:\n    The value bound to the configurable/parameter combination given in\n    `binding_key`.\n\n  Raises:\n    ValueError: If no function can be found matching the configurable name\n      specified by `biding_key`, or if the specified parameter name is\n      blacklisted or not in the function's whitelist (if present) or if there is\n      no value bound for the queried parameter or configurable.\n  \"\"\"\n  pbk = ParsedBindingKey(binding_key)\n  if pbk.config_key not in _CONFIG:\n    err_str = \"Configurable '{}' has no bound parameters.\"\n    raise ValueError(err_str.format(pbk.given_selector))\n  if pbk.arg_name not in _CONFIG[pbk.config_key]:\n    err_str = \"Configurable '{}' has no value bound for parameter '{}'.\"\n    raise ValueError(err_str.format(pbk.given_selector, pbk.arg_name))\n  return _CONFIG[pbk.config_key][pbk.arg_name]", "code_tokens": ["def", "query_parameter", "(", "binding_key", ")", ":", "pbk", "=", "ParsedBindingKey", "(", "binding_key", ")", "if", "pbk", ".", "config_key", "not", "in", "_CONFIG", ":", "err_str", "=", "\"Configurable '{}' has no bound parameters.\"", "raise", "ValueError", "(", "err_str", ".", "format", "(", "pbk", ".", "given_selector", ")", ")", "if", "pbk", ".", "arg_name", "not", "in", "_CONFIG", "[", "pbk", ".", "config_key", "]", ":", "err_str", "=", "\"Configurable '{}' has no value bound for parameter '{}'.\"", "raise", "ValueError", "(", "err_str", ".", "format", "(", "pbk", ".", "given_selector", ",", "pbk", ".", "arg_name", ")", ")", "return", "_CONFIG", "[", "pbk", ".", "config_key", "]", "[", "pbk", ".", "arg_name", "]"], "docstring": "Returns the currently bound value to the specified `binding_key`.\n\n  The `binding_key` argument should look like\n  'maybe/some/scope/maybe.moduels.configurable_name.parameter_name'. Note that\n  this will not include default parameters.\n\n  Args:\n    binding_key: The parameter whose value should be set.\n\n  Returns:\n    The value bound to the configurable/parameter combination given in\n    `binding_key`.\n\n  Raises:\n    ValueError: If no function can be found matching the configurable name\n      specified by `biding_key`, or if the specified parameter name is\n      blacklisted or not in the function's whitelist (if present) or if there is\n      no value bound for the queried parameter or configurable.", "docstring_tokens": ["Returns", "the", "currently", "bound", "value", "to", "the", "specified", "binding_key", "."], "sha": "17a170e0a6711005d1c78e67cf493dc44674d44f", "url": "https://github.com/google/gin-config/blob/17a170e0a6711005d1c78e67cf493dc44674d44f/gin/config.py#L605-L632", "partition": "test"}
{"repo": "aholkner/bacon", "path": "bacon/controller.py", "func_name": "ControllerMapping.get", "original_string": "def get(cls, controller):\n        '''Find a mapping that can apply to the given controller.  Returns None if unsuccessful.\n\n        :param controller: :class:`Controller` to look up\n        :return: :class:`ControllerMapping`\n        '''\n        try:\n            return cls._registry[(controller.vendor_id, controller.product_id)]\n        except KeyError:\n            return None", "language": "python", "code": "def get(cls, controller):\n        '''Find a mapping that can apply to the given controller.  Returns None if unsuccessful.\n\n        :param controller: :class:`Controller` to look up\n        :return: :class:`ControllerMapping`\n        '''\n        try:\n            return cls._registry[(controller.vendor_id, controller.product_id)]\n        except KeyError:\n            return None", "code_tokens": ["def", "get", "(", "cls", ",", "controller", ")", ":", "try", ":", "return", "cls", ".", "_registry", "[", "(", "controller", ".", "vendor_id", ",", "controller", ".", "product_id", ")", "]", "except", "KeyError", ":", "return", "None"], "docstring": "Find a mapping that can apply to the given controller.  Returns None if unsuccessful.\n\n        :param controller: :class:`Controller` to look up\n        :return: :class:`ControllerMapping`", "docstring_tokens": ["Find", "a", "mapping", "that", "can", "apply", "to", "the", "given", "controller", ".", "Returns", "None", "if", "unsuccessful", "."], "sha": "edf3810dcb211942d392a8637945871399b0650d", "url": "https://github.com/aholkner/bacon/blob/edf3810dcb211942d392a8637945871399b0650d/bacon/controller.py#L276-L285", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/cmdline.py", "func_name": "unshell_list", "original_string": "def unshell_list(s):\n    \"\"\"Turn a command-line argument into a list.\"\"\"\n    if not s:\n        return None\n    if sys.platform == 'win32':\n        # When running coverage as coverage.exe, some of the behavior\n        # of the shell is emulated: wildcards are expanded into a list of\n        # filenames.  So you have to single-quote patterns on the command\n        # line, but (not) helpfully, the single quotes are included in the\n        # argument, so we have to strip them off here.\n        s = s.strip(\"'\")\n    return s.split(',')", "language": "python", "code": "def unshell_list(s):\n    \"\"\"Turn a command-line argument into a list.\"\"\"\n    if not s:\n        return None\n    if sys.platform == 'win32':\n        # When running coverage as coverage.exe, some of the behavior\n        # of the shell is emulated: wildcards are expanded into a list of\n        # filenames.  So you have to single-quote patterns on the command\n        # line, but (not) helpfully, the single quotes are included in the\n        # argument, so we have to strip them off here.\n        s = s.strip(\"'\")\n    return s.split(',')", "code_tokens": ["def", "unshell_list", "(", "s", ")", ":", "if", "not", "s", ":", "return", "None", "if", "sys", ".", "platform", "==", "'win32'", ":", "# When running coverage as coverage.exe, some of the behavior", "# of the shell is emulated: wildcards are expanded into a list of", "# filenames.  So you have to single-quote patterns on the command", "# line, but (not) helpfully, the single quotes are included in the", "# argument, so we have to strip them off here.", "s", "=", "s", ".", "strip", "(", "\"'\"", ")", "return", "s", ".", "split", "(", "','", ")"], "docstring": "Turn a command-line argument into a list.", "docstring_tokens": ["Turn", "a", "command", "-", "line", "argument", "into", "a", "list", "."], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/cmdline.py#L617-L628", "partition": "test"}
{"repo": "mdgoldberg/sportsref", "path": "sportsref/nba/players.py", "func_name": "Player.gamelog_basic", "original_string": "def gamelog_basic(self, year, kind='R'):\n        \"\"\"Returns a table of a player's basic game-by-game stats for a season.\n\n        :param year: The year representing the desired season.\n        :param kind: specifies regular season, playoffs, or both. One of 'R',\n            'P', 'B'. Defaults to 'R'.\n        :returns: A DataFrame of the player's standard boxscore stats from each\n            game of the season.\n        :rtype: pd.DataFrame\n        \"\"\"\n        doc = self.get_sub_doc('gamelog/{}'.format(year))\n        table = (doc('table#pgl_basic_playoffs')\n                 if kind == 'P' else doc('table#pgl_basic'))\n        df = sportsref.utils.parse_table(table)\n        return df", "language": "python", "code": "def gamelog_basic(self, year, kind='R'):\n        \"\"\"Returns a table of a player's basic game-by-game stats for a season.\n\n        :param year: The year representing the desired season.\n        :param kind: specifies regular season, playoffs, or both. One of 'R',\n            'P', 'B'. Defaults to 'R'.\n        :returns: A DataFrame of the player's standard boxscore stats from each\n            game of the season.\n        :rtype: pd.DataFrame\n        \"\"\"\n        doc = self.get_sub_doc('gamelog/{}'.format(year))\n        table = (doc('table#pgl_basic_playoffs')\n                 if kind == 'P' else doc('table#pgl_basic'))\n        df = sportsref.utils.parse_table(table)\n        return df", "code_tokens": ["def", "gamelog_basic", "(", "self", ",", "year", ",", "kind", "=", "'R'", ")", ":", "doc", "=", "self", ".", "get_sub_doc", "(", "'gamelog/{}'", ".", "format", "(", "year", ")", ")", "table", "=", "(", "doc", "(", "'table#pgl_basic_playoffs'", ")", "if", "kind", "==", "'P'", "else", "doc", "(", "'table#pgl_basic'", ")", ")", "df", "=", "sportsref", ".", "utils", ".", "parse_table", "(", "table", ")", "return", "df"], "docstring": "Returns a table of a player's basic game-by-game stats for a season.\n\n        :param year: The year representing the desired season.\n        :param kind: specifies regular season, playoffs, or both. One of 'R',\n            'P', 'B'. Defaults to 'R'.\n        :returns: A DataFrame of the player's standard boxscore stats from each\n            game of the season.\n        :rtype: pd.DataFrame", "docstring_tokens": ["Returns", "a", "table", "of", "a", "player", "s", "basic", "game", "-", "by", "-", "game", "stats", "for", "a", "season", "."], "sha": "09f11ac856a23c96d666d1d510bb35d6f050b5c3", "url": "https://github.com/mdgoldberg/sportsref/blob/09f11ac856a23c96d666d1d510bb35d6f050b5c3/sportsref/nba/players.py#L197-L211", "partition": "test"}
{"repo": "neurosynth/neurosynth", "path": "neurosynth/analysis/classify.py", "func_name": "classify_regions", "original_string": "def classify_regions(dataset, masks, method='ERF', threshold=0.08,\n                     remove_overlap=True, regularization='scale',\n                     output='summary', studies=None, features=None,\n                     class_weight='auto', classifier=None,\n                     cross_val='4-Fold', param_grid=None, scoring='accuracy'):\n    \"\"\" Perform classification on specified regions\n\n        Given a set of masks, this function retrieves studies associated with\n        each mask at the specified threshold, optionally removes overlap and\n        filters by studies and features. Then it trains an algorithm to\n        classify studies based on features and tests performance.\n\n        Args:\n            dataset: a Neurosynth dataset\n            maks: a list of paths to Nifti masks\n            method: a string indicating which method to used.\n                'SVM': Support Vector Classifier with rbf kernel\n                'ERF': Extremely Randomized Forest classifier\n                'Dummy': A dummy classifier using stratified classes as\n                    predictor\n            threshold: percentage of voxels active within the mask for study\n                to be included\n            remove_overlap: A boolean indicating if studies studies that\n                appear in more than one mask should be excluded\n            regularization: A string indicating type of regularization to use.\n                If None, performs no regularization.\n                'scale': Unit scale without demeaning\n            output: A string indicating output type\n                'summary': Dictionary with summary statistics including score\n                    and n\n                'summary_clf': Same as above but also includes classifier\n                'clf': Only returns classifier\n                Warning: using cv without grid will return an untrained\n                classifier\n            studies: An optional list of study names used to constrain the set\n                used in classification. If None, will use all features in the\n                dataset.\n            features: An optional list of feature names used to constrain the\n                set used in classification. If None, will use all features in\n                the dataset.\n            class_weight: Parameter to pass to classifier determining how to\n                weight classes\n            classifier: An optional sci-kit learn classifier to use instead of\n                pre-set up classifiers set up using 'method'\n            cross_val: A string indicating type of cross validation to use.\n                Can also pass a scikit_classifier\n            param_grid: A dictionary indicating which parameters to optimize\n                using GridSearchCV. If None, no GridSearch will be used\n\n        Returns:\n            A tuple (X, y) of np arrays.\n            X is a feature by studies matrix and y is a vector of class labels\n    \"\"\"\n\n    (X, y) = get_studies_by_regions(dataset, masks, threshold, remove_overlap,\n                                    studies, features,\n                                    regularization=regularization)\n\n    return classify(X, y, method, classifier, output, cross_val,\n                    class_weight, scoring=scoring, param_grid=param_grid)", "language": "python", "code": "def classify_regions(dataset, masks, method='ERF', threshold=0.08,\n                     remove_overlap=True, regularization='scale',\n                     output='summary', studies=None, features=None,\n                     class_weight='auto', classifier=None,\n                     cross_val='4-Fold', param_grid=None, scoring='accuracy'):\n    \"\"\" Perform classification on specified regions\n\n        Given a set of masks, this function retrieves studies associated with\n        each mask at the specified threshold, optionally removes overlap and\n        filters by studies and features. Then it trains an algorithm to\n        classify studies based on features and tests performance.\n\n        Args:\n            dataset: a Neurosynth dataset\n            maks: a list of paths to Nifti masks\n            method: a string indicating which method to used.\n                'SVM': Support Vector Classifier with rbf kernel\n                'ERF': Extremely Randomized Forest classifier\n                'Dummy': A dummy classifier using stratified classes as\n                    predictor\n            threshold: percentage of voxels active within the mask for study\n                to be included\n            remove_overlap: A boolean indicating if studies studies that\n                appear in more than one mask should be excluded\n            regularization: A string indicating type of regularization to use.\n                If None, performs no regularization.\n                'scale': Unit scale without demeaning\n            output: A string indicating output type\n                'summary': Dictionary with summary statistics including score\n                    and n\n                'summary_clf': Same as above but also includes classifier\n                'clf': Only returns classifier\n                Warning: using cv without grid will return an untrained\n                classifier\n            studies: An optional list of study names used to constrain the set\n                used in classification. If None, will use all features in the\n                dataset.\n            features: An optional list of feature names used to constrain the\n                set used in classification. If None, will use all features in\n                the dataset.\n            class_weight: Parameter to pass to classifier determining how to\n                weight classes\n            classifier: An optional sci-kit learn classifier to use instead of\n                pre-set up classifiers set up using 'method'\n            cross_val: A string indicating type of cross validation to use.\n                Can also pass a scikit_classifier\n            param_grid: A dictionary indicating which parameters to optimize\n                using GridSearchCV. If None, no GridSearch will be used\n\n        Returns:\n            A tuple (X, y) of np arrays.\n            X is a feature by studies matrix and y is a vector of class labels\n    \"\"\"\n\n    (X, y) = get_studies_by_regions(dataset, masks, threshold, remove_overlap,\n                                    studies, features,\n                                    regularization=regularization)\n\n    return classify(X, y, method, classifier, output, cross_val,\n                    class_weight, scoring=scoring, param_grid=param_grid)", "code_tokens": ["def", "classify_regions", "(", "dataset", ",", "masks", ",", "method", "=", "'ERF'", ",", "threshold", "=", "0.08", ",", "remove_overlap", "=", "True", ",", "regularization", "=", "'scale'", ",", "output", "=", "'summary'", ",", "studies", "=", "None", ",", "features", "=", "None", ",", "class_weight", "=", "'auto'", ",", "classifier", "=", "None", ",", "cross_val", "=", "'4-Fold'", ",", "param_grid", "=", "None", ",", "scoring", "=", "'accuracy'", ")", ":", "(", "X", ",", "y", ")", "=", "get_studies_by_regions", "(", "dataset", ",", "masks", ",", "threshold", ",", "remove_overlap", ",", "studies", ",", "features", ",", "regularization", "=", "regularization", ")", "return", "classify", "(", "X", ",", "y", ",", "method", ",", "classifier", ",", "output", ",", "cross_val", ",", "class_weight", ",", "scoring", "=", "scoring", ",", "param_grid", "=", "param_grid", ")"], "docstring": "Perform classification on specified regions\n\n        Given a set of masks, this function retrieves studies associated with\n        each mask at the specified threshold, optionally removes overlap and\n        filters by studies and features. Then it trains an algorithm to\n        classify studies based on features and tests performance.\n\n        Args:\n            dataset: a Neurosynth dataset\n            maks: a list of paths to Nifti masks\n            method: a string indicating which method to used.\n                'SVM': Support Vector Classifier with rbf kernel\n                'ERF': Extremely Randomized Forest classifier\n                'Dummy': A dummy classifier using stratified classes as\n                    predictor\n            threshold: percentage of voxels active within the mask for study\n                to be included\n            remove_overlap: A boolean indicating if studies studies that\n                appear in more than one mask should be excluded\n            regularization: A string indicating type of regularization to use.\n                If None, performs no regularization.\n                'scale': Unit scale without demeaning\n            output: A string indicating output type\n                'summary': Dictionary with summary statistics including score\n                    and n\n                'summary_clf': Same as above but also includes classifier\n                'clf': Only returns classifier\n                Warning: using cv without grid will return an untrained\n                classifier\n            studies: An optional list of study names used to constrain the set\n                used in classification. If None, will use all features in the\n                dataset.\n            features: An optional list of feature names used to constrain the\n                set used in classification. If None, will use all features in\n                the dataset.\n            class_weight: Parameter to pass to classifier determining how to\n                weight classes\n            classifier: An optional sci-kit learn classifier to use instead of\n                pre-set up classifiers set up using 'method'\n            cross_val: A string indicating type of cross validation to use.\n                Can also pass a scikit_classifier\n            param_grid: A dictionary indicating which parameters to optimize\n                using GridSearchCV. If None, no GridSearch will be used\n\n        Returns:\n            A tuple (X, y) of np arrays.\n            X is a feature by studies matrix and y is a vector of class labels", "docstring_tokens": ["Perform", "classification", "on", "specified", "regions"], "sha": "948ce7edce15d7df693446e76834e0c23bfe8f11", "url": "https://github.com/neurosynth/neurosynth/blob/948ce7edce15d7df693446e76834e0c23bfe8f11/neurosynth/analysis/classify.py#L150-L209", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/runtime.py", "func_name": "Namespace.find", "original_string": "def find(self, sym: sym.Symbol) -> Optional[Var]:\n        \"\"\"Find Vars mapped by the given Symbol input or None if no Vars are\n        mapped by that Symbol.\"\"\"\n        v = self.interns.entry(sym, None)\n        if v is None:\n            return self.refers.entry(sym, None)\n        return v", "language": "python", "code": "def find(self, sym: sym.Symbol) -> Optional[Var]:\n        \"\"\"Find Vars mapped by the given Symbol input or None if no Vars are\n        mapped by that Symbol.\"\"\"\n        v = self.interns.entry(sym, None)\n        if v is None:\n            return self.refers.entry(sym, None)\n        return v", "code_tokens": ["def", "find", "(", "self", ",", "sym", ":", "sym", ".", "Symbol", ")", "->", "Optional", "[", "Var", "]", ":", "v", "=", "self", ".", "interns", ".", "entry", "(", "sym", ",", "None", ")", "if", "v", "is", "None", ":", "return", "self", ".", "refers", ".", "entry", "(", "sym", ",", "None", ")", "return", "v"], "docstring": "Find Vars mapped by the given Symbol input or None if no Vars are\n        mapped by that Symbol.", "docstring_tokens": ["Find", "Vars", "mapped", "by", "the", "given", "Symbol", "input", "or", "None", "if", "no", "Vars", "are", "mapped", "by", "that", "Symbol", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/runtime.py#L472-L478", "partition": "test"}
{"repo": "hamelsmu/ktext", "path": "ktext/preprocess.py", "func_name": "processor.token_count_pandas", "original_string": "def token_count_pandas(self):\n        \"\"\" See token counts as pandas dataframe\"\"\"\n        freq_df = pd.DataFrame.from_dict(self.indexer.word_counts, orient='index')\n        freq_df.columns = ['count']\n        return freq_df.sort_values('count', ascending=False)", "language": "python", "code": "def token_count_pandas(self):\n        \"\"\" See token counts as pandas dataframe\"\"\"\n        freq_df = pd.DataFrame.from_dict(self.indexer.word_counts, orient='index')\n        freq_df.columns = ['count']\n        return freq_df.sort_values('count', ascending=False)", "code_tokens": ["def", "token_count_pandas", "(", "self", ")", ":", "freq_df", "=", "pd", ".", "DataFrame", ".", "from_dict", "(", "self", ".", "indexer", ".", "word_counts", ",", "orient", "=", "'index'", ")", "freq_df", ".", "columns", "=", "[", "'count'", "]", "return", "freq_df", ".", "sort_values", "(", "'count'", ",", "ascending", "=", "False", ")"], "docstring": "See token counts as pandas dataframe", "docstring_tokens": ["See", "token", "counts", "as", "pandas", "dataframe"], "sha": "221f09f5b1762705075fd1bd914881c0724d5e02", "url": "https://github.com/hamelsmu/ktext/blob/221f09f5b1762705075fd1bd914881c0724d5e02/ktext/preprocess.py#L307-L311", "partition": "test"}
{"repo": "vaexio/vaex", "path": "packages/vaex-core/vaex/functions.py", "func_name": "dt_year", "original_string": "def dt_year(x):\n    \"\"\"Extracts the year out of a datetime sample.\n\n    :returns: an expression containing the year extracted from a datetime column.\n\n    Example:\n\n    >>> import vaex\n    >>> import numpy as np\n    >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)\n    >>> df = vaex.from_arrays(date=date)\n    >>> df\n      #  date\n      0  2009-10-12 03:31:00\n      1  2016-02-11 10:17:34\n      2  2015-11-12 11:34:22\n\n    >>> df.date.dt.year\n    Expression = dt_year(date)\n    Length: 3 dtype: int64 (expression)\n    -----------------------------------\n    0  2009\n    1  2016\n    2  2015\n    \"\"\"\n    import pandas as pd\n    return pd.Series(x).dt.year.values", "language": "python", "code": "def dt_year(x):\n    \"\"\"Extracts the year out of a datetime sample.\n\n    :returns: an expression containing the year extracted from a datetime column.\n\n    Example:\n\n    >>> import vaex\n    >>> import numpy as np\n    >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)\n    >>> df = vaex.from_arrays(date=date)\n    >>> df\n      #  date\n      0  2009-10-12 03:31:00\n      1  2016-02-11 10:17:34\n      2  2015-11-12 11:34:22\n\n    >>> df.date.dt.year\n    Expression = dt_year(date)\n    Length: 3 dtype: int64 (expression)\n    -----------------------------------\n    0  2009\n    1  2016\n    2  2015\n    \"\"\"\n    import pandas as pd\n    return pd.Series(x).dt.year.values", "code_tokens": ["def", "dt_year", "(", "x", ")", ":", "import", "pandas", "as", "pd", "return", "pd", ".", "Series", "(", "x", ")", ".", "dt", ".", "year", ".", "values"], "docstring": "Extracts the year out of a datetime sample.\n\n    :returns: an expression containing the year extracted from a datetime column.\n\n    Example:\n\n    >>> import vaex\n    >>> import numpy as np\n    >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)\n    >>> df = vaex.from_arrays(date=date)\n    >>> df\n      #  date\n      0  2009-10-12 03:31:00\n      1  2016-02-11 10:17:34\n      2  2015-11-12 11:34:22\n\n    >>> df.date.dt.year\n    Expression = dt_year(date)\n    Length: 3 dtype: int64 (expression)\n    -----------------------------------\n    0  2009\n    1  2016\n    2  2015", "docstring_tokens": ["Extracts", "the", "year", "out", "of", "a", "datetime", "sample", "."], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/functions.py#L247-L273", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/hdl/statements.py", "func_name": "HdlStatement._merge_statements", "original_string": "def _merge_statements(statements: List[\"HdlStatement\"])\\\n            -> Tuple[List[\"HdlStatement\"], int]:\n        \"\"\"\n        Merge statements in list to remove duplicated if-then-else trees\n\n        :return: tuple (list of merged statements, rank decrease due merging)\n        :note: rank decrease is sum of ranks of reduced statements\n        :attention: statement list has to me mergable\n        \"\"\"\n        order = {}\n        for i, stm in enumerate(statements):\n            order[stm] = i\n\n        new_statements = []\n        rank_decrease = 0\n\n        for rank, stms in groupedby(statements, lambda s: s.rank):\n            if rank == 0:\n                new_statements.extend(stms)\n            else:\n                if len(stms) == 1:\n                    new_statements.extend(stms)\n                    continue\n\n                # try to merge statements if they are same condition tree\n                for iA, stmA in enumerate(stms):\n                    if stmA is None:\n                        continue\n\n                    for iB, stmB in enumerate(islice(stms, iA + 1, None)):\n                        if stmB is None:\n                            continue\n\n                        if stmA._is_mergable(stmB):\n                            rank_decrease += stmB.rank\n                            stmA._merge_with_other_stm(stmB)\n                            stms[iA + 1 + iB] = None\n                            new_statements.append(stmA)\n                        else:\n                            new_statements.append(stmA)\n                            new_statements.append(stmB)\n\n        new_statements.sort(key=lambda stm: order[stm])\n        return new_statements, rank_decrease", "language": "python", "code": "def _merge_statements(statements: List[\"HdlStatement\"])\\\n            -> Tuple[List[\"HdlStatement\"], int]:\n        \"\"\"\n        Merge statements in list to remove duplicated if-then-else trees\n\n        :return: tuple (list of merged statements, rank decrease due merging)\n        :note: rank decrease is sum of ranks of reduced statements\n        :attention: statement list has to me mergable\n        \"\"\"\n        order = {}\n        for i, stm in enumerate(statements):\n            order[stm] = i\n\n        new_statements = []\n        rank_decrease = 0\n\n        for rank, stms in groupedby(statements, lambda s: s.rank):\n            if rank == 0:\n                new_statements.extend(stms)\n            else:\n                if len(stms) == 1:\n                    new_statements.extend(stms)\n                    continue\n\n                # try to merge statements if they are same condition tree\n                for iA, stmA in enumerate(stms):\n                    if stmA is None:\n                        continue\n\n                    for iB, stmB in enumerate(islice(stms, iA + 1, None)):\n                        if stmB is None:\n                            continue\n\n                        if stmA._is_mergable(stmB):\n                            rank_decrease += stmB.rank\n                            stmA._merge_with_other_stm(stmB)\n                            stms[iA + 1 + iB] = None\n                            new_statements.append(stmA)\n                        else:\n                            new_statements.append(stmA)\n                            new_statements.append(stmB)\n\n        new_statements.sort(key=lambda stm: order[stm])\n        return new_statements, rank_decrease", "code_tokens": ["def", "_merge_statements", "(", "statements", ":", "List", "[", "\"HdlStatement\"", "]", ")", "->", "Tuple", "[", "List", "[", "\"HdlStatement\"", "]", ",", "int", "]", ":", "order", "=", "{", "}", "for", "i", ",", "stm", "in", "enumerate", "(", "statements", ")", ":", "order", "[", "stm", "]", "=", "i", "new_statements", "=", "[", "]", "rank_decrease", "=", "0", "for", "rank", ",", "stms", "in", "groupedby", "(", "statements", ",", "lambda", "s", ":", "s", ".", "rank", ")", ":", "if", "rank", "==", "0", ":", "new_statements", ".", "extend", "(", "stms", ")", "else", ":", "if", "len", "(", "stms", ")", "==", "1", ":", "new_statements", ".", "extend", "(", "stms", ")", "continue", "# try to merge statements if they are same condition tree", "for", "iA", ",", "stmA", "in", "enumerate", "(", "stms", ")", ":", "if", "stmA", "is", "None", ":", "continue", "for", "iB", ",", "stmB", "in", "enumerate", "(", "islice", "(", "stms", ",", "iA", "+", "1", ",", "None", ")", ")", ":", "if", "stmB", "is", "None", ":", "continue", "if", "stmA", ".", "_is_mergable", "(", "stmB", ")", ":", "rank_decrease", "+=", "stmB", ".", "rank", "stmA", ".", "_merge_with_other_stm", "(", "stmB", ")", "stms", "[", "iA", "+", "1", "+", "iB", "]", "=", "None", "new_statements", ".", "append", "(", "stmA", ")", "else", ":", "new_statements", ".", "append", "(", "stmA", ")", "new_statements", ".", "append", "(", "stmB", ")", "new_statements", ".", "sort", "(", "key", "=", "lambda", "stm", ":", "order", "[", "stm", "]", ")", "return", "new_statements", ",", "rank_decrease"], "docstring": "Merge statements in list to remove duplicated if-then-else trees\n\n        :return: tuple (list of merged statements, rank decrease due merging)\n        :note: rank decrease is sum of ranks of reduced statements\n        :attention: statement list has to me mergable", "docstring_tokens": ["Merge", "statements", "in", "list", "to", "remove", "duplicated", "if", "-", "then", "-", "else", "trees"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/hdl/statements.py#L325-L368", "partition": "test"}
{"repo": "agile-geoscience/striplog", "path": "striplog/striplog.py", "func_name": "Striplog.from_las3", "original_string": "def from_las3(cls, string, lexicon=None,\n                  source=\"LAS\",\n                  dlm=',',\n                  abbreviations=False):\n        \"\"\"\n        Turn LAS3 'lithology' section into a Striplog.\n\n        Args:\n            string (str): A section from an LAS3 file.\n            lexicon (Lexicon): The language for conversion to components.\n            source (str): A source for the data.\n            dlm (str): The delimiter.\n            abbreviations (bool): Whether to expand abbreviations.\n\n        Returns:\n            Striplog: The ``striplog`` object.\n\n        Note:\n            Handles multiple 'Data' sections. It would be smarter for it\n            to handle one at a time, and to deal with parsing the multiple\n            sections in the Well object.\n\n            Does not read an actual LAS file. Use the Well object for that.\n        \"\"\"\n        f = re.DOTALL | re.IGNORECASE\n        regex = r'\\~\\w+?_Data.+?\\n(.+?)(?:\\n\\n+|\\n*\\~|\\n*$)'\n        pattern = re.compile(regex, flags=f)\n        text = pattern.search(string).group(1)\n\n        s = re.search(r'\\.(.+?)\\: ?.+?source', string)\n        if s:\n            source = s.group(1).strip()\n\n        return cls.from_descriptions(text, lexicon,\n                                     source=source,\n                                     dlm=dlm,\n                                     abbreviations=abbreviations)", "language": "python", "code": "def from_las3(cls, string, lexicon=None,\n                  source=\"LAS\",\n                  dlm=',',\n                  abbreviations=False):\n        \"\"\"\n        Turn LAS3 'lithology' section into a Striplog.\n\n        Args:\n            string (str): A section from an LAS3 file.\n            lexicon (Lexicon): The language for conversion to components.\n            source (str): A source for the data.\n            dlm (str): The delimiter.\n            abbreviations (bool): Whether to expand abbreviations.\n\n        Returns:\n            Striplog: The ``striplog`` object.\n\n        Note:\n            Handles multiple 'Data' sections. It would be smarter for it\n            to handle one at a time, and to deal with parsing the multiple\n            sections in the Well object.\n\n            Does not read an actual LAS file. Use the Well object for that.\n        \"\"\"\n        f = re.DOTALL | re.IGNORECASE\n        regex = r'\\~\\w+?_Data.+?\\n(.+?)(?:\\n\\n+|\\n*\\~|\\n*$)'\n        pattern = re.compile(regex, flags=f)\n        text = pattern.search(string).group(1)\n\n        s = re.search(r'\\.(.+?)\\: ?.+?source', string)\n        if s:\n            source = s.group(1).strip()\n\n        return cls.from_descriptions(text, lexicon,\n                                     source=source,\n                                     dlm=dlm,\n                                     abbreviations=abbreviations)", "code_tokens": ["def", "from_las3", "(", "cls", ",", "string", ",", "lexicon", "=", "None", ",", "source", "=", "\"LAS\"", ",", "dlm", "=", "','", ",", "abbreviations", "=", "False", ")", ":", "f", "=", "re", ".", "DOTALL", "|", "re", ".", "IGNORECASE", "regex", "=", "r'\\~\\w+?_Data.+?\\n(.+?)(?:\\n\\n+|\\n*\\~|\\n*$)'", "pattern", "=", "re", ".", "compile", "(", "regex", ",", "flags", "=", "f", ")", "text", "=", "pattern", ".", "search", "(", "string", ")", ".", "group", "(", "1", ")", "s", "=", "re", ".", "search", "(", "r'\\.(.+?)\\: ?.+?source'", ",", "string", ")", "if", "s", ":", "source", "=", "s", ".", "group", "(", "1", ")", ".", "strip", "(", ")", "return", "cls", ".", "from_descriptions", "(", "text", ",", "lexicon", ",", "source", "=", "source", ",", "dlm", "=", "dlm", ",", "abbreviations", "=", "abbreviations", ")"], "docstring": "Turn LAS3 'lithology' section into a Striplog.\n\n        Args:\n            string (str): A section from an LAS3 file.\n            lexicon (Lexicon): The language for conversion to components.\n            source (str): A source for the data.\n            dlm (str): The delimiter.\n            abbreviations (bool): Whether to expand abbreviations.\n\n        Returns:\n            Striplog: The ``striplog`` object.\n\n        Note:\n            Handles multiple 'Data' sections. It would be smarter for it\n            to handle one at a time, and to deal with parsing the multiple\n            sections in the Well object.\n\n            Does not read an actual LAS file. Use the Well object for that.", "docstring_tokens": ["Turn", "LAS3", "lithology", "section", "into", "a", "Striplog", "."], "sha": "8033b673a151f96c29802b43763e863519a3124c", "url": "https://github.com/agile-geoscience/striplog/blob/8033b673a151f96c29802b43763e863519a3124c/striplog/striplog.py#L942-L978", "partition": "test"}
{"repo": "katerina7479/pypdflite", "path": "pypdflite/pdfdocument.py", "func_name": "PDFDocument.add_page", "original_string": "def add_page(self, page=None):\r\n        \"\"\" May generate and add a PDFPage separately, or use this to generate\r\n            a default page.\"\"\"\r\n        if page is None:\r\n            self.page = PDFPage(self.orientation_default, self.layout_default, self.margins)\r\n        else:\r\n            self.page = page\r\n        self.page._set_index(len(self.pages))\r\n        self.pages.append(self.page)\r\n        currentfont = self.font\r\n        self.set_font(font=currentfont)\r\n        self.session._reset_colors()", "language": "python", "code": "def add_page(self, page=None):\r\n        \"\"\" May generate and add a PDFPage separately, or use this to generate\r\n            a default page.\"\"\"\r\n        if page is None:\r\n            self.page = PDFPage(self.orientation_default, self.layout_default, self.margins)\r\n        else:\r\n            self.page = page\r\n        self.page._set_index(len(self.pages))\r\n        self.pages.append(self.page)\r\n        currentfont = self.font\r\n        self.set_font(font=currentfont)\r\n        self.session._reset_colors()", "code_tokens": ["def", "add_page", "(", "self", ",", "page", "=", "None", ")", ":", "if", "page", "is", "None", ":", "self", ".", "page", "=", "PDFPage", "(", "self", ".", "orientation_default", ",", "self", ".", "layout_default", ",", "self", ".", "margins", ")", "else", ":", "self", ".", "page", "=", "page", "self", ".", "page", ".", "_set_index", "(", "len", "(", "self", ".", "pages", ")", ")", "self", ".", "pages", ".", "append", "(", "self", ".", "page", ")", "currentfont", "=", "self", ".", "font", "self", ".", "set_font", "(", "font", "=", "currentfont", ")", "self", ".", "session", ".", "_reset_colors", "(", ")"], "docstring": "May generate and add a PDFPage separately, or use this to generate\r\n            a default page.", "docstring_tokens": ["May", "generate", "and", "add", "a", "PDFPage", "separately", "or", "use", "this", "to", "generate", "a", "default", "page", "."], "sha": "ac2501f30d6619eae9dea5644717575ca9263d0a", "url": "https://github.com/katerina7479/pypdflite/blob/ac2501f30d6619eae9dea5644717575ca9263d0a/pypdflite/pdfdocument.py#L120-L131", "partition": "test"}
{"repo": "myint/autoflake", "path": "autoflake.py", "func_name": "get_messages_by_line", "original_string": "def get_messages_by_line(messages):\n    \"\"\"Return dictionary that maps line number to message.\"\"\"\n    line_messages = {}\n    for message in messages:\n        line_messages[message.lineno] = message\n    return line_messages", "language": "python", "code": "def get_messages_by_line(messages):\n    \"\"\"Return dictionary that maps line number to message.\"\"\"\n    line_messages = {}\n    for message in messages:\n        line_messages[message.lineno] = message\n    return line_messages", "code_tokens": ["def", "get_messages_by_line", "(", "messages", ")", ":", "line_messages", "=", "{", "}", "for", "message", "in", "messages", ":", "line_messages", "[", "message", ".", "lineno", "]", "=", "message", "return", "line_messages"], "docstring": "Return dictionary that maps line number to message.", "docstring_tokens": ["Return", "dictionary", "that", "maps", "line", "number", "to", "message", "."], "sha": "68fea68646922b920d55975f9f2adaeafd84df4f", "url": "https://github.com/myint/autoflake/blob/68fea68646922b920d55975f9f2adaeafd84df4f/autoflake.py#L414-L419", "partition": "test"}
{"repo": "not-na/peng3d", "path": "peng3d/model.py", "func_name": "Bone.setLength", "original_string": "def setLength(self,data,blength):\n        \"\"\"\n        Sets the length of this bone on the given entity.\n        \n        ``data`` is the entity to modify in dictionary form.\n        \n        ``blength`` is the new length of the bone.\n        \"\"\"\n        self.ensureBones(data)\n        data[\"_bones\"][self.name][\"length\"]=blength", "language": "python", "code": "def setLength(self,data,blength):\n        \"\"\"\n        Sets the length of this bone on the given entity.\n        \n        ``data`` is the entity to modify in dictionary form.\n        \n        ``blength`` is the new length of the bone.\n        \"\"\"\n        self.ensureBones(data)\n        data[\"_bones\"][self.name][\"length\"]=blength", "code_tokens": ["def", "setLength", "(", "self", ",", "data", ",", "blength", ")", ":", "self", ".", "ensureBones", "(", "data", ")", "data", "[", "\"_bones\"", "]", "[", "self", ".", "name", "]", "[", "\"length\"", "]", "=", "blength"], "docstring": "Sets the length of this bone on the given entity.\n        \n        ``data`` is the entity to modify in dictionary form.\n        \n        ``blength`` is the new length of the bone.", "docstring_tokens": ["Sets", "the", "length", "of", "this", "bone", "on", "the", "given", "entity", ".", "data", "is", "the", "entity", "to", "modify", "in", "dictionary", "form", ".", "blength", "is", "the", "new", "length", "of", "the", "bone", "."], "sha": "1151be665b26cc8a479f6307086ba919e4d32d85", "url": "https://github.com/not-na/peng3d/blob/1151be665b26cc8a479f6307086ba919e4d32d85/peng3d/model.py#L295-L304", "partition": "test"}
{"repo": "zqfang/GSEApy", "path": "gseapy/__main__.py", "func_name": "add_enrichr_parser", "original_string": "def add_enrichr_parser(subparsers):\n    \"\"\"Add function 'enrichr' argument parsers.\"\"\"\n\n    argparser_enrichr = subparsers.add_parser(\"enrichr\", help=\"Using Enrichr API to perform GO analysis.\")\n\n    # group for required options.\n    enrichr_opt = argparser_enrichr.add_argument_group(\"Input arguments\")\n    enrichr_opt.add_argument(\"-i\", \"--input-list\", action=\"store\", dest=\"gene_list\", type=str, required=True, metavar='IDs',\n                              help=\"Enrichr uses a list of gene names as input.\")\n    enrichr_opt.add_argument(\"-g\", \"--gene-sets\", action=\"store\", dest=\"library\", type=str, required=True, metavar='GMT',\n                              help=\"Enrichr library name(s) required. Separate each name by comma.\")\n    enrichr_opt.add_argument(\"--org\", \"--organism\", action=\"store\", dest=\"organism\", type=str, default='',\n                             help=\"Enrichr supported organism name. Default: human. See here: https://amp.pharm.mssm.edu/modEnrichr.\")\n    enrichr_opt.add_argument(\"--ds\", \"--description\", action=\"store\", dest=\"descrip\", type=str, default='enrichr', metavar='STRING',\n                              help=\"It is recommended to enter a short description for your list so that multiple lists \\\n                              can be differentiated from each other if you choose to save or share your list.\")\n    enrichr_opt.add_argument(\"--cut\", \"--cut-off\", action=\"store\", dest=\"thresh\", metavar='float', type=float, default=0.05,\n                              help=\"Adjust-Pval cutoff, used for generating plots. Default: 0.05.\")\n    enrichr_opt.add_argument(\"--bg\", \"--background\", action=\"store\", dest=\"bg\", default='hsapiens_gene_ensembl', metavar='BGNUM',\n                              help=\"BioMart Dataset name or Background total genes number. Default: None\")\n    enrichr_opt.add_argument(\"-t\", \"--top-term\", dest=\"term\", action=\"store\", type=int, default=10, metavar='int',\n                              help=\"Numbers of top terms shown in the plot. Default: 10\")\n    # enrichr_opt.add_argument(\"--scale\", dest = \"scale\", action=\"store\", type=float, default=0.5, metavar='float',\n    #                          help=\"scatter dot scale in the dotplot. Default: 0.5\")\n    # enrichr_opt.add_argument(\"--no-plot\", action='store_true', dest='no_plot', default=False,\n    #                           help=\"Suppress the plot output.This is useful only if data are interested. Default: False.\")\n\n    enrichr_output = argparser_enrichr.add_argument_group(\"Output figure arguments\")\n    add_output_option(enrichr_output)\n    return", "language": "python", "code": "def add_enrichr_parser(subparsers):\n    \"\"\"Add function 'enrichr' argument parsers.\"\"\"\n\n    argparser_enrichr = subparsers.add_parser(\"enrichr\", help=\"Using Enrichr API to perform GO analysis.\")\n\n    # group for required options.\n    enrichr_opt = argparser_enrichr.add_argument_group(\"Input arguments\")\n    enrichr_opt.add_argument(\"-i\", \"--input-list\", action=\"store\", dest=\"gene_list\", type=str, required=True, metavar='IDs',\n                              help=\"Enrichr uses a list of gene names as input.\")\n    enrichr_opt.add_argument(\"-g\", \"--gene-sets\", action=\"store\", dest=\"library\", type=str, required=True, metavar='GMT',\n                              help=\"Enrichr library name(s) required. Separate each name by comma.\")\n    enrichr_opt.add_argument(\"--org\", \"--organism\", action=\"store\", dest=\"organism\", type=str, default='',\n                             help=\"Enrichr supported organism name. Default: human. See here: https://amp.pharm.mssm.edu/modEnrichr.\")\n    enrichr_opt.add_argument(\"--ds\", \"--description\", action=\"store\", dest=\"descrip\", type=str, default='enrichr', metavar='STRING',\n                              help=\"It is recommended to enter a short description for your list so that multiple lists \\\n                              can be differentiated from each other if you choose to save or share your list.\")\n    enrichr_opt.add_argument(\"--cut\", \"--cut-off\", action=\"store\", dest=\"thresh\", metavar='float', type=float, default=0.05,\n                              help=\"Adjust-Pval cutoff, used for generating plots. Default: 0.05.\")\n    enrichr_opt.add_argument(\"--bg\", \"--background\", action=\"store\", dest=\"bg\", default='hsapiens_gene_ensembl', metavar='BGNUM',\n                              help=\"BioMart Dataset name or Background total genes number. Default: None\")\n    enrichr_opt.add_argument(\"-t\", \"--top-term\", dest=\"term\", action=\"store\", type=int, default=10, metavar='int',\n                              help=\"Numbers of top terms shown in the plot. Default: 10\")\n    # enrichr_opt.add_argument(\"--scale\", dest = \"scale\", action=\"store\", type=float, default=0.5, metavar='float',\n    #                          help=\"scatter dot scale in the dotplot. Default: 0.5\")\n    # enrichr_opt.add_argument(\"--no-plot\", action='store_true', dest='no_plot', default=False,\n    #                           help=\"Suppress the plot output.This is useful only if data are interested. Default: False.\")\n\n    enrichr_output = argparser_enrichr.add_argument_group(\"Output figure arguments\")\n    add_output_option(enrichr_output)\n    return", "code_tokens": ["def", "add_enrichr_parser", "(", "subparsers", ")", ":", "argparser_enrichr", "=", "subparsers", ".", "add_parser", "(", "\"enrichr\"", ",", "help", "=", "\"Using Enrichr API to perform GO analysis.\"", ")", "# group for required options.", "enrichr_opt", "=", "argparser_enrichr", ".", "add_argument_group", "(", "\"Input arguments\"", ")", "enrichr_opt", ".", "add_argument", "(", "\"-i\"", ",", "\"--input-list\"", ",", "action", "=", "\"store\"", ",", "dest", "=", "\"gene_list\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "metavar", "=", "'IDs'", ",", "help", "=", "\"Enrichr uses a list of gene names as input.\"", ")", "enrichr_opt", ".", "add_argument", "(", "\"-g\"", ",", "\"--gene-sets\"", ",", "action", "=", "\"store\"", ",", "dest", "=", "\"library\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "metavar", "=", "'GMT'", ",", "help", "=", "\"Enrichr library name(s) required. Separate each name by comma.\"", ")", "enrichr_opt", ".", "add_argument", "(", "\"--org\"", ",", "\"--organism\"", ",", "action", "=", "\"store\"", ",", "dest", "=", "\"organism\"", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Enrichr supported organism name. Default: human. See here: https://amp.pharm.mssm.edu/modEnrichr.\"", ")", "enrichr_opt", ".", "add_argument", "(", "\"--ds\"", ",", "\"--description\"", ",", "action", "=", "\"store\"", ",", "dest", "=", "\"descrip\"", ",", "type", "=", "str", ",", "default", "=", "'enrichr'", ",", "metavar", "=", "'STRING'", ",", "help", "=", "\"It is recommended to enter a short description for your list so that multiple lists \\\n                              can be differentiated from each other if you choose to save or share your list.\"", ")", "enrichr_opt", ".", "add_argument", "(", "\"--cut\"", ",", "\"--cut-off\"", ",", "action", "=", "\"store\"", ",", "dest", "=", "\"thresh\"", ",", "metavar", "=", "'float'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "\"Adjust-Pval cutoff, used for generating plots. Default: 0.05.\"", ")", "enrichr_opt", ".", "add_argument", "(", "\"--bg\"", ",", "\"--background\"", ",", "action", "=", "\"store\"", ",", "dest", "=", "\"bg\"", ",", "default", "=", "'hsapiens_gene_ensembl'", ",", "metavar", "=", "'BGNUM'", ",", "help", "=", "\"BioMart Dataset name or Background total genes number. Default: None\"", ")", "enrichr_opt", ".", "add_argument", "(", "\"-t\"", ",", "\"--top-term\"", ",", "dest", "=", "\"term\"", ",", "action", "=", "\"store\"", ",", "type", "=", "int", ",", "default", "=", "10", ",", "metavar", "=", "'int'", ",", "help", "=", "\"Numbers of top terms shown in the plot. Default: 10\"", ")", "# enrichr_opt.add_argument(\"--scale\", dest = \"scale\", action=\"store\", type=float, default=0.5, metavar='float',", "#                          help=\"scatter dot scale in the dotplot. Default: 0.5\")", "# enrichr_opt.add_argument(\"--no-plot\", action='store_true', dest='no_plot', default=False,", "#                           help=\"Suppress the plot output.This is useful only if data are interested. Default: False.\")", "enrichr_output", "=", "argparser_enrichr", ".", "add_argument_group", "(", "\"Output figure arguments\"", ")", "add_output_option", "(", "enrichr_output", ")", "return"], "docstring": "Add function 'enrichr' argument parsers.", "docstring_tokens": ["Add", "function", "enrichr", "argument", "parsers", "."], "sha": "673e9ec1391e3b14d3e8a4353117151fd2cb9345", "url": "https://github.com/zqfang/GSEApy/blob/673e9ec1391e3b14d3e8a4353117151fd2cb9345/gseapy/__main__.py#L288-L317", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/logistic_regression.py", "func_name": "visualize_decision", "original_string": "def visualize_decision(features, labels, true_w_b, candidate_w_bs, fname):\n  \"\"\"Utility method to visualize decision boundaries in R^2.\n\n  Args:\n    features: Input points, as a Numpy `array` of shape `[num_examples, 2]`.\n    labels: Numpy `float`-like array of shape `[num_examples, 1]` giving a\n      label for each point.\n    true_w_b: A `tuple` `(w, b)` where `w` is a Numpy array of\n       shape `[2]` and `b` is a scalar `float`, interpreted as a\n       decision rule of the form `dot(features, w) + b > 0`.\n    candidate_w_bs: Python `iterable` containing tuples of the same form as\n       true_w_b.\n    fname: The filename to save the plot as a PNG image (Python `str`).\n  \"\"\"\n  fig = figure.Figure(figsize=(6, 6))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n  ax = fig.add_subplot(1, 1, 1)\n  ax.scatter(features[:, 0], features[:, 1],\n             c=np.float32(labels[:, 0]),\n             cmap=cm.get_cmap(\"binary\"),\n             edgecolors=\"k\")\n\n  def plot_weights(w, b, **kwargs):\n    w1, w2 = w\n    x1s = np.linspace(-1, 1, 100)\n    x2s = -(w1  * x1s + b) / w2\n    ax.plot(x1s, x2s, **kwargs)\n\n  for w, b in candidate_w_bs:\n    plot_weights(w, b,\n                 alpha=1./np.sqrt(len(candidate_w_bs)),\n                 lw=1, color=\"blue\")\n\n  if true_w_b is not None:\n    plot_weights(*true_w_b, lw=4,\n                 color=\"green\", label=\"true separator\")\n\n  ax.set_xlim([-1.5, 1.5])\n  ax.set_ylim([-1.5, 1.5])\n  ax.legend()\n\n  canvas.print_figure(fname, format=\"png\")\n  print(\"saved {}\".format(fname))", "language": "python", "code": "def visualize_decision(features, labels, true_w_b, candidate_w_bs, fname):\n  \"\"\"Utility method to visualize decision boundaries in R^2.\n\n  Args:\n    features: Input points, as a Numpy `array` of shape `[num_examples, 2]`.\n    labels: Numpy `float`-like array of shape `[num_examples, 1]` giving a\n      label for each point.\n    true_w_b: A `tuple` `(w, b)` where `w` is a Numpy array of\n       shape `[2]` and `b` is a scalar `float`, interpreted as a\n       decision rule of the form `dot(features, w) + b > 0`.\n    candidate_w_bs: Python `iterable` containing tuples of the same form as\n       true_w_b.\n    fname: The filename to save the plot as a PNG image (Python `str`).\n  \"\"\"\n  fig = figure.Figure(figsize=(6, 6))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n  ax = fig.add_subplot(1, 1, 1)\n  ax.scatter(features[:, 0], features[:, 1],\n             c=np.float32(labels[:, 0]),\n             cmap=cm.get_cmap(\"binary\"),\n             edgecolors=\"k\")\n\n  def plot_weights(w, b, **kwargs):\n    w1, w2 = w\n    x1s = np.linspace(-1, 1, 100)\n    x2s = -(w1  * x1s + b) / w2\n    ax.plot(x1s, x2s, **kwargs)\n\n  for w, b in candidate_w_bs:\n    plot_weights(w, b,\n                 alpha=1./np.sqrt(len(candidate_w_bs)),\n                 lw=1, color=\"blue\")\n\n  if true_w_b is not None:\n    plot_weights(*true_w_b, lw=4,\n                 color=\"green\", label=\"true separator\")\n\n  ax.set_xlim([-1.5, 1.5])\n  ax.set_ylim([-1.5, 1.5])\n  ax.legend()\n\n  canvas.print_figure(fname, format=\"png\")\n  print(\"saved {}\".format(fname))", "code_tokens": ["def", "visualize_decision", "(", "features", ",", "labels", ",", "true_w_b", ",", "candidate_w_bs", ",", "fname", ")", ":", "fig", "=", "figure", ".", "Figure", "(", "figsize", "=", "(", "6", ",", "6", ")", ")", "canvas", "=", "backend_agg", ".", "FigureCanvasAgg", "(", "fig", ")", "ax", "=", "fig", ".", "add_subplot", "(", "1", ",", "1", ",", "1", ")", "ax", ".", "scatter", "(", "features", "[", ":", ",", "0", "]", ",", "features", "[", ":", ",", "1", "]", ",", "c", "=", "np", ".", "float32", "(", "labels", "[", ":", ",", "0", "]", ")", ",", "cmap", "=", "cm", ".", "get_cmap", "(", "\"binary\"", ")", ",", "edgecolors", "=", "\"k\"", ")", "def", "plot_weights", "(", "w", ",", "b", ",", "*", "*", "kwargs", ")", ":", "w1", ",", "w2", "=", "w", "x1s", "=", "np", ".", "linspace", "(", "-", "1", ",", "1", ",", "100", ")", "x2s", "=", "-", "(", "w1", "*", "x1s", "+", "b", ")", "/", "w2", "ax", ".", "plot", "(", "x1s", ",", "x2s", ",", "*", "*", "kwargs", ")", "for", "w", ",", "b", "in", "candidate_w_bs", ":", "plot_weights", "(", "w", ",", "b", ",", "alpha", "=", "1.", "/", "np", ".", "sqrt", "(", "len", "(", "candidate_w_bs", ")", ")", ",", "lw", "=", "1", ",", "color", "=", "\"blue\"", ")", "if", "true_w_b", "is", "not", "None", ":", "plot_weights", "(", "*", "true_w_b", ",", "lw", "=", "4", ",", "color", "=", "\"green\"", ",", "label", "=", "\"true separator\"", ")", "ax", ".", "set_xlim", "(", "[", "-", "1.5", ",", "1.5", "]", ")", "ax", ".", "set_ylim", "(", "[", "-", "1.5", ",", "1.5", "]", ")", "ax", ".", "legend", "(", ")", "canvas", ".", "print_figure", "(", "fname", ",", "format", "=", "\"png\"", ")", "print", "(", "\"saved {}\"", ".", "format", "(", "fname", ")", ")"], "docstring": "Utility method to visualize decision boundaries in R^2.\n\n  Args:\n    features: Input points, as a Numpy `array` of shape `[num_examples, 2]`.\n    labels: Numpy `float`-like array of shape `[num_examples, 1]` giving a\n      label for each point.\n    true_w_b: A `tuple` `(w, b)` where `w` is a Numpy array of\n       shape `[2]` and `b` is a scalar `float`, interpreted as a\n       decision rule of the form `dot(features, w) + b > 0`.\n    candidate_w_bs: Python `iterable` containing tuples of the same form as\n       true_w_b.\n    fname: The filename to save the plot as a PNG image (Python `str`).", "docstring_tokens": ["Utility", "method", "to", "visualize", "decision", "boundaries", "in", "R^2", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/logistic_regression.py#L89-L131", "partition": "test"}
{"repo": "labstreaminglayer/liblsl-Python", "path": "pylsl/pylsl.py", "func_name": "XMLElement.append_copy", "original_string": "def append_copy(self, elem):\n        \"\"\"Append a copy of the specified element as a child.\"\"\"\n        return XMLElement(lib.lsl_append_copy(self.e, elem.e))", "language": "python", "code": "def append_copy(self, elem):\n        \"\"\"Append a copy of the specified element as a child.\"\"\"\n        return XMLElement(lib.lsl_append_copy(self.e, elem.e))", "code_tokens": ["def", "append_copy", "(", "self", ",", "elem", ")", ":", "return", "XMLElement", "(", "lib", ".", "lsl_append_copy", "(", "self", ".", "e", ",", "elem", ".", "e", ")", ")"], "docstring": "Append a copy of the specified element as a child.", "docstring_tokens": ["Append", "a", "copy", "of", "the", "specified", "element", "as", "a", "child", "."], "sha": "1ff6fe2794f8dba286b7491d1f7a4c915b8a0605", "url": "https://github.com/labstreaminglayer/liblsl-Python/blob/1ff6fe2794f8dba286b7491d1f7a4c915b8a0605/pylsl/pylsl.py#L1026-L1028", "partition": "test"}
{"repo": "dagster-io/dagster", "path": "python_modules/dagster/dagster/core/types/field.py", "func_name": "Field", "original_string": "def Field(\n    dagster_type,\n    default_value=FIELD_NO_DEFAULT_PROVIDED,\n    is_optional=INFER_OPTIONAL_COMPOSITE_FIELD,\n    is_secret=False,\n    description=None,\n):\n    '''\n    The schema for configuration data that describes the type, optionality, defaults, and description.\n\n    Args:\n        dagster_type (DagsterType):\n            A ``DagsterType`` describing the schema of this field, ie `Dict({'example': Field(String)})`\n        default_value (Any):\n            A default value to use that respects the schema provided via dagster_type\n        is_optional (bool): Whether the presence of this field is optional\n        despcription (str):\n    '''\n    config_type = resolve_to_config_type(dagster_type)\n    if not config_type:\n        raise DagsterInvalidDefinitionError(\n            (\n                'Attempted to pass {value_repr} to a Field that expects a valid '\n                'dagster type usable in config (e.g. Dict, NamedDict, Int, String et al).'\n            ).format(value_repr=repr(dagster_type))\n        )\n    return FieldImpl(\n        config_type=resolve_to_config_type(dagster_type),\n        default_value=default_value,\n        is_optional=is_optional,\n        is_secret=is_secret,\n        description=description,\n    )", "language": "python", "code": "def Field(\n    dagster_type,\n    default_value=FIELD_NO_DEFAULT_PROVIDED,\n    is_optional=INFER_OPTIONAL_COMPOSITE_FIELD,\n    is_secret=False,\n    description=None,\n):\n    '''\n    The schema for configuration data that describes the type, optionality, defaults, and description.\n\n    Args:\n        dagster_type (DagsterType):\n            A ``DagsterType`` describing the schema of this field, ie `Dict({'example': Field(String)})`\n        default_value (Any):\n            A default value to use that respects the schema provided via dagster_type\n        is_optional (bool): Whether the presence of this field is optional\n        despcription (str):\n    '''\n    config_type = resolve_to_config_type(dagster_type)\n    if not config_type:\n        raise DagsterInvalidDefinitionError(\n            (\n                'Attempted to pass {value_repr} to a Field that expects a valid '\n                'dagster type usable in config (e.g. Dict, NamedDict, Int, String et al).'\n            ).format(value_repr=repr(dagster_type))\n        )\n    return FieldImpl(\n        config_type=resolve_to_config_type(dagster_type),\n        default_value=default_value,\n        is_optional=is_optional,\n        is_secret=is_secret,\n        description=description,\n    )", "code_tokens": ["def", "Field", "(", "dagster_type", ",", "default_value", "=", "FIELD_NO_DEFAULT_PROVIDED", ",", "is_optional", "=", "INFER_OPTIONAL_COMPOSITE_FIELD", ",", "is_secret", "=", "False", ",", "description", "=", "None", ",", ")", ":", "config_type", "=", "resolve_to_config_type", "(", "dagster_type", ")", "if", "not", "config_type", ":", "raise", "DagsterInvalidDefinitionError", "(", "(", "'Attempted to pass {value_repr} to a Field that expects a valid '", "'dagster type usable in config (e.g. Dict, NamedDict, Int, String et al).'", ")", ".", "format", "(", "value_repr", "=", "repr", "(", "dagster_type", ")", ")", ")", "return", "FieldImpl", "(", "config_type", "=", "resolve_to_config_type", "(", "dagster_type", ")", ",", "default_value", "=", "default_value", ",", "is_optional", "=", "is_optional", ",", "is_secret", "=", "is_secret", ",", "description", "=", "description", ",", ")"], "docstring": "The schema for configuration data that describes the type, optionality, defaults, and description.\n\n    Args:\n        dagster_type (DagsterType):\n            A ``DagsterType`` describing the schema of this field, ie `Dict({'example': Field(String)})`\n        default_value (Any):\n            A default value to use that respects the schema provided via dagster_type\n        is_optional (bool): Whether the presence of this field is optional\n        despcription (str):", "docstring_tokens": ["The", "schema", "for", "configuration", "data", "that", "describes", "the", "type", "optionality", "defaults", "and", "description", "."], "sha": "4119f8c773089de64831b1dfb9e168e353d401dc", "url": "https://github.com/dagster-io/dagster/blob/4119f8c773089de64831b1dfb9e168e353d401dc/python_modules/dagster/dagster/core/types/field.py#L34-L66", "partition": "test"}
{"repo": "greenbender/pynntp", "path": "nntp/nntp.py", "func_name": "NNTPClient.article", "original_string": "def article(self, msgid_article=None, decode=None):\n        \"\"\"ARTICLE command.\n        \"\"\"\n        args = None\n        if msgid_article is not None:\n            args = utils.unparse_msgid_article(msgid_article)\n\n        code, message = self.command(\"ARTICLE\", args)\n        if code != 220:\n            raise NNTPReplyError(code, message)\n\n        parts = message.split(None, 1)\n\n        try:\n            articleno = int(parts[0])\n        except ValueError:\n            raise NNTPProtocolError(message)\n\n        # headers\n        headers = utils.parse_headers(self.info_gen(code, message))\n\n        # decoding setup\n        decode = \"yEnc\" in headers.get(\"subject\", \"\")\n        escape = 0\n        crc32 = 0\n\n        # body\n        body = []\n        for line in self.info_gen(code, message):\n\n            # decode body if required\n            if decode:\n                if line.startswith(\"=y\"):\n                    continue\n                line, escape, crc32 = yenc.decode(line, escape, crc32)\n\n            body.append(line)\n\n        return articleno, headers, \"\".join(body)", "language": "python", "code": "def article(self, msgid_article=None, decode=None):\n        \"\"\"ARTICLE command.\n        \"\"\"\n        args = None\n        if msgid_article is not None:\n            args = utils.unparse_msgid_article(msgid_article)\n\n        code, message = self.command(\"ARTICLE\", args)\n        if code != 220:\n            raise NNTPReplyError(code, message)\n\n        parts = message.split(None, 1)\n\n        try:\n            articleno = int(parts[0])\n        except ValueError:\n            raise NNTPProtocolError(message)\n\n        # headers\n        headers = utils.parse_headers(self.info_gen(code, message))\n\n        # decoding setup\n        decode = \"yEnc\" in headers.get(\"subject\", \"\")\n        escape = 0\n        crc32 = 0\n\n        # body\n        body = []\n        for line in self.info_gen(code, message):\n\n            # decode body if required\n            if decode:\n                if line.startswith(\"=y\"):\n                    continue\n                line, escape, crc32 = yenc.decode(line, escape, crc32)\n\n            body.append(line)\n\n        return articleno, headers, \"\".join(body)", "code_tokens": ["def", "article", "(", "self", ",", "msgid_article", "=", "None", ",", "decode", "=", "None", ")", ":", "args", "=", "None", "if", "msgid_article", "is", "not", "None", ":", "args", "=", "utils", ".", "unparse_msgid_article", "(", "msgid_article", ")", "code", ",", "message", "=", "self", ".", "command", "(", "\"ARTICLE\"", ",", "args", ")", "if", "code", "!=", "220", ":", "raise", "NNTPReplyError", "(", "code", ",", "message", ")", "parts", "=", "message", ".", "split", "(", "None", ",", "1", ")", "try", ":", "articleno", "=", "int", "(", "parts", "[", "0", "]", ")", "except", "ValueError", ":", "raise", "NNTPProtocolError", "(", "message", ")", "# headers", "headers", "=", "utils", ".", "parse_headers", "(", "self", ".", "info_gen", "(", "code", ",", "message", ")", ")", "# decoding setup", "decode", "=", "\"yEnc\"", "in", "headers", ".", "get", "(", "\"subject\"", ",", "\"\"", ")", "escape", "=", "0", "crc32", "=", "0", "# body", "body", "=", "[", "]", "for", "line", "in", "self", ".", "info_gen", "(", "code", ",", "message", ")", ":", "# decode body if required", "if", "decode", ":", "if", "line", ".", "startswith", "(", "\"=y\"", ")", ":", "continue", "line", ",", "escape", ",", "crc32", "=", "yenc", ".", "decode", "(", "line", ",", "escape", ",", "crc32", ")", "body", ".", "append", "(", "line", ")", "return", "articleno", ",", "headers", ",", "\"\"", ".", "join", "(", "body", ")"], "docstring": "ARTICLE command.", "docstring_tokens": ["ARTICLE", "command", "."], "sha": "991a76331cdf5d8f9dbf5b18f6e29adc80749a2f", "url": "https://github.com/greenbender/pynntp/blob/991a76331cdf5d8f9dbf5b18f6e29adc80749a2f/nntp/nntp.py#L1004-L1042", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicebus/azure/servicebus/control_client/_serialization.py", "func_name": "_convert_etree_element_to_topic", "original_string": "def _convert_etree_element_to_topic(entry_element):\n    '''Converts entry element to topic\n\n    The xml format for topic:\n<entry xmlns='http://www.w3.org/2005/Atom'>\n    <content type='application/xml'>\n    <TopicDescription\n        xmlns:i=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xmlns=\"http://schemas.microsoft.com/netservices/2010/10/servicebus/connect\">\n        <DefaultMessageTimeToLive>P10675199DT2H48M5.4775807S</DefaultMessageTimeToLive>\n        <MaxSizeInMegabytes>1024</MaxSizeInMegabytes>\n        <RequiresDuplicateDetection>false</RequiresDuplicateDetection>\n        <DuplicateDetectionHistoryTimeWindow>P7D</DuplicateDetectionHistoryTimeWindow>\n        <DeadLetteringOnFilterEvaluationExceptions>true</DeadLetteringOnFilterEvaluationExceptions>\n    </TopicDescription>\n    </content>\n</entry>\n    '''\n    topic = Topic()\n\n    invalid_topic = True\n\n    topic_element = entry_element.find('./atom:content/sb:TopicDescription', _etree_sb_feed_namespaces)\n    if topic_element is not None:\n        mappings = [\n            ('DefaultMessageTimeToLive', 'default_message_time_to_live', None),\n            ('MaxSizeInMegabytes', 'max_size_in_megabytes', int),\n            ('RequiresDuplicateDetection', 'requires_duplicate_detection', _parse_bool),\n            ('DuplicateDetectionHistoryTimeWindow', 'duplicate_detection_history_time_window', None),\n            ('EnableBatchedOperations', 'enable_batched_operations', _parse_bool),\n            ('SizeInBytes', 'size_in_bytes', int),\n        ]\n\n        for mapping in mappings:\n            if _read_etree_element(topic_element, mapping[0], topic, mapping[1], mapping[2]):\n                invalid_topic = False\n\n    if invalid_topic:\n        raise AzureServiceBusResourceNotFound(_ERROR_TOPIC_NOT_FOUND)\n\n    # extract id, updated and name value from feed entry and set them of topic.\n    for name, value in _ETreeXmlToObject.get_entry_properties_from_element(\n            entry_element, True).items():\n        setattr(topic, name, value)\n\n    return topic", "language": "python", "code": "def _convert_etree_element_to_topic(entry_element):\n    '''Converts entry element to topic\n\n    The xml format for topic:\n<entry xmlns='http://www.w3.org/2005/Atom'>\n    <content type='application/xml'>\n    <TopicDescription\n        xmlns:i=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xmlns=\"http://schemas.microsoft.com/netservices/2010/10/servicebus/connect\">\n        <DefaultMessageTimeToLive>P10675199DT2H48M5.4775807S</DefaultMessageTimeToLive>\n        <MaxSizeInMegabytes>1024</MaxSizeInMegabytes>\n        <RequiresDuplicateDetection>false</RequiresDuplicateDetection>\n        <DuplicateDetectionHistoryTimeWindow>P7D</DuplicateDetectionHistoryTimeWindow>\n        <DeadLetteringOnFilterEvaluationExceptions>true</DeadLetteringOnFilterEvaluationExceptions>\n    </TopicDescription>\n    </content>\n</entry>\n    '''\n    topic = Topic()\n\n    invalid_topic = True\n\n    topic_element = entry_element.find('./atom:content/sb:TopicDescription', _etree_sb_feed_namespaces)\n    if topic_element is not None:\n        mappings = [\n            ('DefaultMessageTimeToLive', 'default_message_time_to_live', None),\n            ('MaxSizeInMegabytes', 'max_size_in_megabytes', int),\n            ('RequiresDuplicateDetection', 'requires_duplicate_detection', _parse_bool),\n            ('DuplicateDetectionHistoryTimeWindow', 'duplicate_detection_history_time_window', None),\n            ('EnableBatchedOperations', 'enable_batched_operations', _parse_bool),\n            ('SizeInBytes', 'size_in_bytes', int),\n        ]\n\n        for mapping in mappings:\n            if _read_etree_element(topic_element, mapping[0], topic, mapping[1], mapping[2]):\n                invalid_topic = False\n\n    if invalid_topic:\n        raise AzureServiceBusResourceNotFound(_ERROR_TOPIC_NOT_FOUND)\n\n    # extract id, updated and name value from feed entry and set them of topic.\n    for name, value in _ETreeXmlToObject.get_entry_properties_from_element(\n            entry_element, True).items():\n        setattr(topic, name, value)\n\n    return topic", "code_tokens": ["def", "_convert_etree_element_to_topic", "(", "entry_element", ")", ":", "topic", "=", "Topic", "(", ")", "invalid_topic", "=", "True", "topic_element", "=", "entry_element", ".", "find", "(", "'./atom:content/sb:TopicDescription'", ",", "_etree_sb_feed_namespaces", ")", "if", "topic_element", "is", "not", "None", ":", "mappings", "=", "[", "(", "'DefaultMessageTimeToLive'", ",", "'default_message_time_to_live'", ",", "None", ")", ",", "(", "'MaxSizeInMegabytes'", ",", "'max_size_in_megabytes'", ",", "int", ")", ",", "(", "'RequiresDuplicateDetection'", ",", "'requires_duplicate_detection'", ",", "_parse_bool", ")", ",", "(", "'DuplicateDetectionHistoryTimeWindow'", ",", "'duplicate_detection_history_time_window'", ",", "None", ")", ",", "(", "'EnableBatchedOperations'", ",", "'enable_batched_operations'", ",", "_parse_bool", ")", ",", "(", "'SizeInBytes'", ",", "'size_in_bytes'", ",", "int", ")", ",", "]", "for", "mapping", "in", "mappings", ":", "if", "_read_etree_element", "(", "topic_element", ",", "mapping", "[", "0", "]", ",", "topic", ",", "mapping", "[", "1", "]", ",", "mapping", "[", "2", "]", ")", ":", "invalid_topic", "=", "False", "if", "invalid_topic", ":", "raise", "AzureServiceBusResourceNotFound", "(", "_ERROR_TOPIC_NOT_FOUND", ")", "# extract id, updated and name value from feed entry and set them of topic.", "for", "name", ",", "value", "in", "_ETreeXmlToObject", ".", "get_entry_properties_from_element", "(", "entry_element", ",", "True", ")", ".", "items", "(", ")", ":", "setattr", "(", "topic", ",", "name", ",", "value", ")", "return", "topic"], "docstring": "Converts entry element to topic\n\n    The xml format for topic:\n<entry xmlns='http://www.w3.org/2005/Atom'>\n    <content type='application/xml'>\n    <TopicDescription\n        xmlns:i=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xmlns=\"http://schemas.microsoft.com/netservices/2010/10/servicebus/connect\">\n        <DefaultMessageTimeToLive>P10675199DT2H48M5.4775807S</DefaultMessageTimeToLive>\n        <MaxSizeInMegabytes>1024</MaxSizeInMegabytes>\n        <RequiresDuplicateDetection>false</RequiresDuplicateDetection>\n        <DuplicateDetectionHistoryTimeWindow>P7D</DuplicateDetectionHistoryTimeWindow>\n        <DeadLetteringOnFilterEvaluationExceptions>true</DeadLetteringOnFilterEvaluationExceptions>\n    </TopicDescription>\n    </content>\n</entry>", "docstring_tokens": ["Converts", "entry", "element", "to", "topic"], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicebus/azure/servicebus/control_client/_serialization.py#L253-L298", "partition": "test"}
{"repo": "UCBerkeleySETI/blimpy", "path": "blimpy/sigproc.py", "func_name": "fix_header", "original_string": "def fix_header(filename, keyword, new_value):\n    \"\"\" Apply a quick patch-up to a Filterbank header by overwriting a header value\n\n\n    Args:\n        filename (str): name of file to open and fix. WILL BE MODIFIED.\n        keyword (stt):  header keyword to update\n        new_value (long, double, angle or string): New value to write.\n\n    Notes:\n        This will overwrite the current value of the blimpy with a desired\n        'fixed' version. Note that this has limited support for patching\n        string-type values - if the length of the string changes, all hell will\n        break loose.\n\n    \"\"\"\n\n    # Read header data and return indexes of data offsets in file\n    hd = read_header(filename)\n    hi = read_header(filename, return_idxs=True)\n    idx = hi[keyword]\n\n    # Find out the datatype for the given keyword\n    dtype = header_keyword_types[keyword]\n    dtype_to_type = {b'<l'  : np.int32,\n                     b'str' : bytes,\n                     b'<d'  : np.float64,\n                     b'angle' : to_sigproc_angle}\n    value_dtype = dtype_to_type[dtype]\n\n    # Generate the new string\n    if isinstance(value_dtype, bytes):\n        if len(hd[keyword]) == len(new_value):\n            val_str = np.int32(len(new_value)).tostring() + new_value\n        else:\n            raise RuntimeError(\"String size mismatch. Cannot update without rewriting entire file.\")\n    else:\n        val_str = value_dtype(new_value).tostring()\n\n    # Write the new string to file\n    with open(filename, 'rb+') as fh:\n        fh.seek(idx)\n        fh.write(val_str)", "language": "python", "code": "def fix_header(filename, keyword, new_value):\n    \"\"\" Apply a quick patch-up to a Filterbank header by overwriting a header value\n\n\n    Args:\n        filename (str): name of file to open and fix. WILL BE MODIFIED.\n        keyword (stt):  header keyword to update\n        new_value (long, double, angle or string): New value to write.\n\n    Notes:\n        This will overwrite the current value of the blimpy with a desired\n        'fixed' version. Note that this has limited support for patching\n        string-type values - if the length of the string changes, all hell will\n        break loose.\n\n    \"\"\"\n\n    # Read header data and return indexes of data offsets in file\n    hd = read_header(filename)\n    hi = read_header(filename, return_idxs=True)\n    idx = hi[keyword]\n\n    # Find out the datatype for the given keyword\n    dtype = header_keyword_types[keyword]\n    dtype_to_type = {b'<l'  : np.int32,\n                     b'str' : bytes,\n                     b'<d'  : np.float64,\n                     b'angle' : to_sigproc_angle}\n    value_dtype = dtype_to_type[dtype]\n\n    # Generate the new string\n    if isinstance(value_dtype, bytes):\n        if len(hd[keyword]) == len(new_value):\n            val_str = np.int32(len(new_value)).tostring() + new_value\n        else:\n            raise RuntimeError(\"String size mismatch. Cannot update without rewriting entire file.\")\n    else:\n        val_str = value_dtype(new_value).tostring()\n\n    # Write the new string to file\n    with open(filename, 'rb+') as fh:\n        fh.seek(idx)\n        fh.write(val_str)", "code_tokens": ["def", "fix_header", "(", "filename", ",", "keyword", ",", "new_value", ")", ":", "# Read header data and return indexes of data offsets in file", "hd", "=", "read_header", "(", "filename", ")", "hi", "=", "read_header", "(", "filename", ",", "return_idxs", "=", "True", ")", "idx", "=", "hi", "[", "keyword", "]", "# Find out the datatype for the given keyword", "dtype", "=", "header_keyword_types", "[", "keyword", "]", "dtype_to_type", "=", "{", "b'<l'", ":", "np", ".", "int32", ",", "b'str'", ":", "bytes", ",", "b'<d'", ":", "np", ".", "float64", ",", "b'angle'", ":", "to_sigproc_angle", "}", "value_dtype", "=", "dtype_to_type", "[", "dtype", "]", "# Generate the new string", "if", "isinstance", "(", "value_dtype", ",", "bytes", ")", ":", "if", "len", "(", "hd", "[", "keyword", "]", ")", "==", "len", "(", "new_value", ")", ":", "val_str", "=", "np", ".", "int32", "(", "len", "(", "new_value", ")", ")", ".", "tostring", "(", ")", "+", "new_value", "else", ":", "raise", "RuntimeError", "(", "\"String size mismatch. Cannot update without rewriting entire file.\"", ")", "else", ":", "val_str", "=", "value_dtype", "(", "new_value", ")", ".", "tostring", "(", ")", "# Write the new string to file", "with", "open", "(", "filename", ",", "'rb+'", ")", "as", "fh", ":", "fh", ".", "seek", "(", "idx", ")", "fh", ".", "write", "(", "val_str", ")"], "docstring": "Apply a quick patch-up to a Filterbank header by overwriting a header value\n\n\n    Args:\n        filename (str): name of file to open and fix. WILL BE MODIFIED.\n        keyword (stt):  header keyword to update\n        new_value (long, double, angle or string): New value to write.\n\n    Notes:\n        This will overwrite the current value of the blimpy with a desired\n        'fixed' version. Note that this has limited support for patching\n        string-type values - if the length of the string changes, all hell will\n        break loose.", "docstring_tokens": ["Apply", "a", "quick", "patch", "-", "up", "to", "a", "Filterbank", "header", "by", "overwriting", "a", "header", "value"], "sha": "b8822d3e3e911944370d84371a91fa0c29e9772e", "url": "https://github.com/UCBerkeleySETI/blimpy/blob/b8822d3e3e911944370d84371a91fa0c29e9772e/blimpy/sigproc.py#L198-L240", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/parallel/apps/winhpcjob.py", "func_name": "WinHPCJob.write", "original_string": "def write(self, filename):\n        \"\"\"Write the XML job description to a file.\"\"\"\n        txt = self.tostring()\n        with open(filename, 'w') as f:\n            f.write(txt)", "language": "python", "code": "def write(self, filename):\n        \"\"\"Write the XML job description to a file.\"\"\"\n        txt = self.tostring()\n        with open(filename, 'w') as f:\n            f.write(txt)", "code_tokens": ["def", "write", "(", "self", ",", "filename", ")", ":", "txt", "=", "self", ".", "tostring", "(", ")", "with", "open", "(", "filename", ",", "'w'", ")", "as", "f", ":", "f", ".", "write", "(", "txt", ")"], "docstring": "Write the XML job description to a file.", "docstring_tokens": ["Write", "the", "XML", "job", "description", "to", "a", "file", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/parallel/apps/winhpcjob.py#L154-L158", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/backend/connection.py", "func_name": "H2OConnection.close", "original_string": "def close(self):\n        \"\"\"\n        Close an existing connection; once closed it cannot be used again.\n\n        Strictly speaking it is not necessary to close all connection that you opened -- we have several mechanisms\n        in place that will do so automatically (__del__(), __exit__() and atexit() handlers), however there is also\n        no good reason to make this method private.\n        \"\"\"\n        if self._session_id:\n            try:\n                # If the server gone bad, we don't want to wait forever...\n                if self._timeout is None: self._timeout = 1\n                self.request(\"DELETE /4/sessions/%s\" % self._session_id)\n                self._print(\"H2O session %s closed.\" % self._session_id)\n            except Exception:\n                pass\n            self._session_id = None\n        self._stage = -1", "language": "python", "code": "def close(self):\n        \"\"\"\n        Close an existing connection; once closed it cannot be used again.\n\n        Strictly speaking it is not necessary to close all connection that you opened -- we have several mechanisms\n        in place that will do so automatically (__del__(), __exit__() and atexit() handlers), however there is also\n        no good reason to make this method private.\n        \"\"\"\n        if self._session_id:\n            try:\n                # If the server gone bad, we don't want to wait forever...\n                if self._timeout is None: self._timeout = 1\n                self.request(\"DELETE /4/sessions/%s\" % self._session_id)\n                self._print(\"H2O session %s closed.\" % self._session_id)\n            except Exception:\n                pass\n            self._session_id = None\n        self._stage = -1", "code_tokens": ["def", "close", "(", "self", ")", ":", "if", "self", ".", "_session_id", ":", "try", ":", "# If the server gone bad, we don't want to wait forever...", "if", "self", ".", "_timeout", "is", "None", ":", "self", ".", "_timeout", "=", "1", "self", ".", "request", "(", "\"DELETE /4/sessions/%s\"", "%", "self", ".", "_session_id", ")", "self", ".", "_print", "(", "\"H2O session %s closed.\"", "%", "self", ".", "_session_id", ")", "except", "Exception", ":", "pass", "self", ".", "_session_id", "=", "None", "self", ".", "_stage", "=", "-", "1"], "docstring": "Close an existing connection; once closed it cannot be used again.\n\n        Strictly speaking it is not necessary to close all connection that you opened -- we have several mechanisms\n        in place that will do so automatically (__del__(), __exit__() and atexit() handlers), however there is also\n        no good reason to make this method private.", "docstring_tokens": ["Close", "an", "existing", "connection", ";", "once", "closed", "it", "cannot", "be", "used", "again", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/backend/connection.py#L427-L444", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/shareddata.py", "func_name": "make_ordinary_result", "original_string": "def make_ordinary_result(result, key, trajectory=None, reload=True):\n    \"\"\"Turns a given shared data item into a an ordinary one.\n\n    :param result: Result container with shared data\n    :param key: The name of the shared data\n    :param trajectory:\n\n        The trajectory, only needed if shared data has\n        no access to the trajectory, yet.\n\n    :param reload: If data should be reloaded after conversion\n\n    :return: The result\n\n\n    \"\"\"\n    shared_data = result.f_get(key)\n    if trajectory is not None:\n        shared_data.traj = trajectory\n    shared_data._request_data('make_ordinary')\n    result.f_remove(key)\n    if reload:\n        trajectory.f_load_item(result, load_data=pypetconstants.OVERWRITE_DATA)\n    return result", "language": "python", "code": "def make_ordinary_result(result, key, trajectory=None, reload=True):\n    \"\"\"Turns a given shared data item into a an ordinary one.\n\n    :param result: Result container with shared data\n    :param key: The name of the shared data\n    :param trajectory:\n\n        The trajectory, only needed if shared data has\n        no access to the trajectory, yet.\n\n    :param reload: If data should be reloaded after conversion\n\n    :return: The result\n\n\n    \"\"\"\n    shared_data = result.f_get(key)\n    if trajectory is not None:\n        shared_data.traj = trajectory\n    shared_data._request_data('make_ordinary')\n    result.f_remove(key)\n    if reload:\n        trajectory.f_load_item(result, load_data=pypetconstants.OVERWRITE_DATA)\n    return result", "code_tokens": ["def", "make_ordinary_result", "(", "result", ",", "key", ",", "trajectory", "=", "None", ",", "reload", "=", "True", ")", ":", "shared_data", "=", "result", ".", "f_get", "(", "key", ")", "if", "trajectory", "is", "not", "None", ":", "shared_data", ".", "traj", "=", "trajectory", "shared_data", ".", "_request_data", "(", "'make_ordinary'", ")", "result", ".", "f_remove", "(", "key", ")", "if", "reload", ":", "trajectory", ".", "f_load_item", "(", "result", ",", "load_data", "=", "pypetconstants", ".", "OVERWRITE_DATA", ")", "return", "result"], "docstring": "Turns a given shared data item into a an ordinary one.\n\n    :param result: Result container with shared data\n    :param key: The name of the shared data\n    :param trajectory:\n\n        The trajectory, only needed if shared data has\n        no access to the trajectory, yet.\n\n    :param reload: If data should be reloaded after conversion\n\n    :return: The result", "docstring_tokens": ["Turns", "a", "given", "shared", "data", "item", "into", "a", "an", "ordinary", "one", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/shareddata.py#L81-L104", "partition": "test"}
{"repo": "farzadghanei/statsd-metrics", "path": "statsdmetrics/client/__init__.py", "func_name": "AbstractClient.set", "original_string": "def set(self, name, value, rate=1):\n        # type: (str, str, float) -> None\n        \"\"\"Send a Set metric with the specified unique value\"\"\"\n\n        if self._should_send_metric(name, rate):\n            value = str(value)\n            self._request(\n                Set(\n                    self._create_metric_name_for_request(name),\n                    value,\n                    rate\n                ).to_request()\n            )", "language": "python", "code": "def set(self, name, value, rate=1):\n        # type: (str, str, float) -> None\n        \"\"\"Send a Set metric with the specified unique value\"\"\"\n\n        if self._should_send_metric(name, rate):\n            value = str(value)\n            self._request(\n                Set(\n                    self._create_metric_name_for_request(name),\n                    value,\n                    rate\n                ).to_request()\n            )", "code_tokens": ["def", "set", "(", "self", ",", "name", ",", "value", ",", "rate", "=", "1", ")", ":", "# type: (str, str, float) -> None", "if", "self", ".", "_should_send_metric", "(", "name", ",", "rate", ")", ":", "value", "=", "str", "(", "value", ")", "self", ".", "_request", "(", "Set", "(", "self", ".", "_create_metric_name_for_request", "(", "name", ")", ",", "value", ",", "rate", ")", ".", "to_request", "(", ")", ")"], "docstring": "Send a Set metric with the specified unique value", "docstring_tokens": ["Send", "a", "Set", "metric", "with", "the", "specified", "unique", "value"], "sha": "153ff37b79777f208e49bb9d3fb737ba52b99f98", "url": "https://github.com/farzadghanei/statsd-metrics/blob/153ff37b79777f208e49bb9d3fb737ba52b99f98/statsdmetrics/client/__init__.py#L216-L228", "partition": "test"}
{"repo": "BD2KGenomics/toil-lib", "path": "src/toil_lib/tools/quantifiers.py", "func_name": "run_rsem", "original_string": "def run_rsem(job, bam_id, rsem_ref_url, paired=True):\n    \"\"\"\n    RNA quantification with RSEM\n\n    :param JobFunctionWrappingJob job: Passed automatically by Toil\n    :param str bam_id: FileStoreID of transcriptome bam for quantification\n    :param str rsem_ref_url: URL of RSEM reference (tarball)\n    :param bool paired: If True, uses parameters for paired end data\n    :return: FileStoreIDs for RSEM's gene and isoform output\n    :rtype: str\n    \"\"\"\n    work_dir = job.fileStore.getLocalTempDir()\n    download_url(job, url=rsem_ref_url, name='rsem_ref.tar.gz', work_dir=work_dir)\n    subprocess.check_call(['tar', '-xvf', os.path.join(work_dir, 'rsem_ref.tar.gz'), '-C', work_dir])\n    os.remove(os.path.join(work_dir, 'rsem_ref.tar.gz'))\n    # Determine tarball structure - based on it, ascertain folder name and rsem reference prefix\n    rsem_files = []\n    for root, directories, files in os.walk(work_dir):\n        rsem_files.extend([os.path.join(root, x) for x in files])\n    # \"grp\" is a required RSEM extension that should exist in the RSEM reference\n    ref_prefix = [os.path.basename(os.path.splitext(x)[0]) for x in rsem_files if 'grp' in x][0]\n    ref_folder = os.path.join('/data', os.listdir(work_dir)[0]) if len(os.listdir(work_dir)) == 1 else '/data'\n    # I/O\n    job.fileStore.readGlobalFile(bam_id, os.path.join(work_dir, 'transcriptome.bam'))\n    output_prefix = 'rsem'\n    # Call: RSEM\n    parameters = ['--quiet',\n                  '--no-qualities',\n                  '-p', str(job.cores),\n                  '--forward-prob', '0.5',\n                  '--seed-length', '25',\n                  '--fragment-length-mean', '-1.0',\n                  '--bam', '/data/transcriptome.bam',\n                  os.path.join(ref_folder, ref_prefix),\n                  output_prefix]\n    if paired:\n        parameters = ['--paired-end'] + parameters\n    dockerCall(job=job, tool='quay.io/ucsc_cgl/rsem:1.2.25--d4275175cc8df36967db460b06337a14f40d2f21',\n               parameters=parameters, workDir=work_dir)\n    # Write to FileStore\n    gene_id = job.fileStore.writeGlobalFile(os.path.join(work_dir, output_prefix + '.genes.results'))\n    isoform_id = job.fileStore.writeGlobalFile(os.path.join(work_dir, output_prefix + '.isoforms.results'))\n    return gene_id, isoform_id", "language": "python", "code": "def run_rsem(job, bam_id, rsem_ref_url, paired=True):\n    \"\"\"\n    RNA quantification with RSEM\n\n    :param JobFunctionWrappingJob job: Passed automatically by Toil\n    :param str bam_id: FileStoreID of transcriptome bam for quantification\n    :param str rsem_ref_url: URL of RSEM reference (tarball)\n    :param bool paired: If True, uses parameters for paired end data\n    :return: FileStoreIDs for RSEM's gene and isoform output\n    :rtype: str\n    \"\"\"\n    work_dir = job.fileStore.getLocalTempDir()\n    download_url(job, url=rsem_ref_url, name='rsem_ref.tar.gz', work_dir=work_dir)\n    subprocess.check_call(['tar', '-xvf', os.path.join(work_dir, 'rsem_ref.tar.gz'), '-C', work_dir])\n    os.remove(os.path.join(work_dir, 'rsem_ref.tar.gz'))\n    # Determine tarball structure - based on it, ascertain folder name and rsem reference prefix\n    rsem_files = []\n    for root, directories, files in os.walk(work_dir):\n        rsem_files.extend([os.path.join(root, x) for x in files])\n    # \"grp\" is a required RSEM extension that should exist in the RSEM reference\n    ref_prefix = [os.path.basename(os.path.splitext(x)[0]) for x in rsem_files if 'grp' in x][0]\n    ref_folder = os.path.join('/data', os.listdir(work_dir)[0]) if len(os.listdir(work_dir)) == 1 else '/data'\n    # I/O\n    job.fileStore.readGlobalFile(bam_id, os.path.join(work_dir, 'transcriptome.bam'))\n    output_prefix = 'rsem'\n    # Call: RSEM\n    parameters = ['--quiet',\n                  '--no-qualities',\n                  '-p', str(job.cores),\n                  '--forward-prob', '0.5',\n                  '--seed-length', '25',\n                  '--fragment-length-mean', '-1.0',\n                  '--bam', '/data/transcriptome.bam',\n                  os.path.join(ref_folder, ref_prefix),\n                  output_prefix]\n    if paired:\n        parameters = ['--paired-end'] + parameters\n    dockerCall(job=job, tool='quay.io/ucsc_cgl/rsem:1.2.25--d4275175cc8df36967db460b06337a14f40d2f21',\n               parameters=parameters, workDir=work_dir)\n    # Write to FileStore\n    gene_id = job.fileStore.writeGlobalFile(os.path.join(work_dir, output_prefix + '.genes.results'))\n    isoform_id = job.fileStore.writeGlobalFile(os.path.join(work_dir, output_prefix + '.isoforms.results'))\n    return gene_id, isoform_id", "code_tokens": ["def", "run_rsem", "(", "job", ",", "bam_id", ",", "rsem_ref_url", ",", "paired", "=", "True", ")", ":", "work_dir", "=", "job", ".", "fileStore", ".", "getLocalTempDir", "(", ")", "download_url", "(", "job", ",", "url", "=", "rsem_ref_url", ",", "name", "=", "'rsem_ref.tar.gz'", ",", "work_dir", "=", "work_dir", ")", "subprocess", ".", "check_call", "(", "[", "'tar'", ",", "'-xvf'", ",", "os", ".", "path", ".", "join", "(", "work_dir", ",", "'rsem_ref.tar.gz'", ")", ",", "'-C'", ",", "work_dir", "]", ")", "os", ".", "remove", "(", "os", ".", "path", ".", "join", "(", "work_dir", ",", "'rsem_ref.tar.gz'", ")", ")", "# Determine tarball structure - based on it, ascertain folder name and rsem reference prefix", "rsem_files", "=", "[", "]", "for", "root", ",", "directories", ",", "files", "in", "os", ".", "walk", "(", "work_dir", ")", ":", "rsem_files", ".", "extend", "(", "[", "os", ".", "path", ".", "join", "(", "root", ",", "x", ")", "for", "x", "in", "files", "]", ")", "# \"grp\" is a required RSEM extension that should exist in the RSEM reference", "ref_prefix", "=", "[", "os", ".", "path", ".", "basename", "(", "os", ".", "path", ".", "splitext", "(", "x", ")", "[", "0", "]", ")", "for", "x", "in", "rsem_files", "if", "'grp'", "in", "x", "]", "[", "0", "]", "ref_folder", "=", "os", ".", "path", ".", "join", "(", "'/data'", ",", "os", ".", "listdir", "(", "work_dir", ")", "[", "0", "]", ")", "if", "len", "(", "os", ".", "listdir", "(", "work_dir", ")", ")", "==", "1", "else", "'/data'", "# I/O", "job", ".", "fileStore", ".", "readGlobalFile", "(", "bam_id", ",", "os", ".", "path", ".", "join", "(", "work_dir", ",", "'transcriptome.bam'", ")", ")", "output_prefix", "=", "'rsem'", "# Call: RSEM", "parameters", "=", "[", "'--quiet'", ",", "'--no-qualities'", ",", "'-p'", ",", "str", "(", "job", ".", "cores", ")", ",", "'--forward-prob'", ",", "'0.5'", ",", "'--seed-length'", ",", "'25'", ",", "'--fragment-length-mean'", ",", "'-1.0'", ",", "'--bam'", ",", "'/data/transcriptome.bam'", ",", "os", ".", "path", ".", "join", "(", "ref_folder", ",", "ref_prefix", ")", ",", "output_prefix", "]", "if", "paired", ":", "parameters", "=", "[", "'--paired-end'", "]", "+", "parameters", "dockerCall", "(", "job", "=", "job", ",", "tool", "=", "'quay.io/ucsc_cgl/rsem:1.2.25--d4275175cc8df36967db460b06337a14f40d2f21'", ",", "parameters", "=", "parameters", ",", "workDir", "=", "work_dir", ")", "# Write to FileStore", "gene_id", "=", "job", ".", "fileStore", ".", "writeGlobalFile", "(", "os", ".", "path", ".", "join", "(", "work_dir", ",", "output_prefix", "+", "'.genes.results'", ")", ")", "isoform_id", "=", "job", ".", "fileStore", ".", "writeGlobalFile", "(", "os", ".", "path", ".", "join", "(", "work_dir", ",", "output_prefix", "+", "'.isoforms.results'", ")", ")", "return", "gene_id", ",", "isoform_id"], "docstring": "RNA quantification with RSEM\n\n    :param JobFunctionWrappingJob job: Passed automatically by Toil\n    :param str bam_id: FileStoreID of transcriptome bam for quantification\n    :param str rsem_ref_url: URL of RSEM reference (tarball)\n    :param bool paired: If True, uses parameters for paired end data\n    :return: FileStoreIDs for RSEM's gene and isoform output\n    :rtype: str", "docstring_tokens": ["RNA", "quantification", "with", "RSEM"], "sha": "022a615fc3dc98fc1aaa7bfd232409962ca44fbd", "url": "https://github.com/BD2KGenomics/toil-lib/blob/022a615fc3dc98fc1aaa7bfd232409962ca44fbd/src/toil_lib/tools/quantifiers.py#L48-L90", "partition": "test"}
{"repo": "willkg/markus", "path": "markus/main.py", "func_name": "get_metrics", "original_string": "def get_metrics(thing, extra=''):\n    \"\"\"Return MetricsInterface instance with specified name.\n\n    The name is used as the prefix for all keys generated with this\n    :py:class:`markus.main.MetricsInterface`.\n\n    The :py:class:`markus.main.MetricsInterface` is not tied to metrics\n    backends. The list of active backends are globally configured. This allows\n    us to create :py:class:`markus.main.MetricsInterface` classes without\n    having to worry about bootstrapping order of the app.\n\n    :arg class/instance/str thing: The name to use as a key prefix.\n\n        If this is a class, it uses the dotted Python path. If this is an\n        instance, it uses the dotted Python path plus ``str(instance)``.\n\n    :arg str extra: Any extra bits to add to the end of the name.\n\n    :returns: a ``MetricsInterface`` instance\n\n    Examples:\n\n    >>> from markus import get_metrics\n\n    Create a MetricsInterface with the name \"myapp\" and generate a count with\n    stat \"myapp.thing1\" and value 1:\n\n    >>> metrics = get_metrics('myapp')\n    >>> metrics.incr('thing1', value=1)\n\n    Create a MetricsInterface with the prefix of the Python module it's being\n    called in:\n\n    >>> metrics = get_metrics(__name__)\n\n    Create a MetricsInterface with the prefix as the qualname of the class:\n\n    >>> class Foo:\n    ...     def __init__(self):\n    ...         self.metrics = get_metrics(self)\n\n    Create a prefix of the class path plus some identifying information:\n\n    >>> class Foo:\n    ...     def __init__(self, myname):\n    ...         self.metrics = get_metrics(self, extra=myname)\n    ...\n    >>> foo = Foo('jim')\n\n    Assume that ``Foo`` is defined in the ``myapp`` module. Then this will\n    generate the name ``myapp.Foo.jim``.\n\n    \"\"\"\n    thing = thing or ''\n\n    if not isinstance(thing, str):\n        # If it's not a str, it's either a class or an instance. Handle\n        # accordingly.\n        if type(thing) == type:\n            thing = '%s.%s' % (thing.__module__, thing.__name__)\n        else:\n            thing = '%s.%s' % (\n                thing.__class__.__module__, thing.__class__.__name__\n            )\n\n    if extra:\n        thing = '%s.%s' % (thing, extra)\n\n    return MetricsInterface(thing)", "language": "python", "code": "def get_metrics(thing, extra=''):\n    \"\"\"Return MetricsInterface instance with specified name.\n\n    The name is used as the prefix for all keys generated with this\n    :py:class:`markus.main.MetricsInterface`.\n\n    The :py:class:`markus.main.MetricsInterface` is not tied to metrics\n    backends. The list of active backends are globally configured. This allows\n    us to create :py:class:`markus.main.MetricsInterface` classes without\n    having to worry about bootstrapping order of the app.\n\n    :arg class/instance/str thing: The name to use as a key prefix.\n\n        If this is a class, it uses the dotted Python path. If this is an\n        instance, it uses the dotted Python path plus ``str(instance)``.\n\n    :arg str extra: Any extra bits to add to the end of the name.\n\n    :returns: a ``MetricsInterface`` instance\n\n    Examples:\n\n    >>> from markus import get_metrics\n\n    Create a MetricsInterface with the name \"myapp\" and generate a count with\n    stat \"myapp.thing1\" and value 1:\n\n    >>> metrics = get_metrics('myapp')\n    >>> metrics.incr('thing1', value=1)\n\n    Create a MetricsInterface with the prefix of the Python module it's being\n    called in:\n\n    >>> metrics = get_metrics(__name__)\n\n    Create a MetricsInterface with the prefix as the qualname of the class:\n\n    >>> class Foo:\n    ...     def __init__(self):\n    ...         self.metrics = get_metrics(self)\n\n    Create a prefix of the class path plus some identifying information:\n\n    >>> class Foo:\n    ...     def __init__(self, myname):\n    ...         self.metrics = get_metrics(self, extra=myname)\n    ...\n    >>> foo = Foo('jim')\n\n    Assume that ``Foo`` is defined in the ``myapp`` module. Then this will\n    generate the name ``myapp.Foo.jim``.\n\n    \"\"\"\n    thing = thing or ''\n\n    if not isinstance(thing, str):\n        # If it's not a str, it's either a class or an instance. Handle\n        # accordingly.\n        if type(thing) == type:\n            thing = '%s.%s' % (thing.__module__, thing.__name__)\n        else:\n            thing = '%s.%s' % (\n                thing.__class__.__module__, thing.__class__.__name__\n            )\n\n    if extra:\n        thing = '%s.%s' % (thing, extra)\n\n    return MetricsInterface(thing)", "code_tokens": ["def", "get_metrics", "(", "thing", ",", "extra", "=", "''", ")", ":", "thing", "=", "thing", "or", "''", "if", "not", "isinstance", "(", "thing", ",", "str", ")", ":", "# If it's not a str, it's either a class or an instance. Handle", "# accordingly.", "if", "type", "(", "thing", ")", "==", "type", ":", "thing", "=", "'%s.%s'", "%", "(", "thing", ".", "__module__", ",", "thing", ".", "__name__", ")", "else", ":", "thing", "=", "'%s.%s'", "%", "(", "thing", ".", "__class__", ".", "__module__", ",", "thing", ".", "__class__", ".", "__name__", ")", "if", "extra", ":", "thing", "=", "'%s.%s'", "%", "(", "thing", ",", "extra", ")", "return", "MetricsInterface", "(", "thing", ")"], "docstring": "Return MetricsInterface instance with specified name.\n\n    The name is used as the prefix for all keys generated with this\n    :py:class:`markus.main.MetricsInterface`.\n\n    The :py:class:`markus.main.MetricsInterface` is not tied to metrics\n    backends. The list of active backends are globally configured. This allows\n    us to create :py:class:`markus.main.MetricsInterface` classes without\n    having to worry about bootstrapping order of the app.\n\n    :arg class/instance/str thing: The name to use as a key prefix.\n\n        If this is a class, it uses the dotted Python path. If this is an\n        instance, it uses the dotted Python path plus ``str(instance)``.\n\n    :arg str extra: Any extra bits to add to the end of the name.\n\n    :returns: a ``MetricsInterface`` instance\n\n    Examples:\n\n    >>> from markus import get_metrics\n\n    Create a MetricsInterface with the name \"myapp\" and generate a count with\n    stat \"myapp.thing1\" and value 1:\n\n    >>> metrics = get_metrics('myapp')\n    >>> metrics.incr('thing1', value=1)\n\n    Create a MetricsInterface with the prefix of the Python module it's being\n    called in:\n\n    >>> metrics = get_metrics(__name__)\n\n    Create a MetricsInterface with the prefix as the qualname of the class:\n\n    >>> class Foo:\n    ...     def __init__(self):\n    ...         self.metrics = get_metrics(self)\n\n    Create a prefix of the class path plus some identifying information:\n\n    >>> class Foo:\n    ...     def __init__(self, myname):\n    ...         self.metrics = get_metrics(self, extra=myname)\n    ...\n    >>> foo = Foo('jim')\n\n    Assume that ``Foo`` is defined in the ``myapp`` module. Then this will\n    generate the name ``myapp.Foo.jim``.", "docstring_tokens": ["Return", "MetricsInterface", "instance", "with", "specified", "name", "."], "sha": "0cfbe67fb7ccfa7488b0120d21ddc0cdc1f8ed33", "url": "https://github.com/willkg/markus/blob/0cfbe67fb7ccfa7488b0120d21ddc0cdc1f8ed33/markus/main.py#L396-L464", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval-mozilla", "path": "perceval/backends/mozilla/kitsune.py", "func_name": "Kitsune.fetch_items", "original_string": "def fetch_items(self, category, **kwargs):\n        \"\"\"Fetch questions from the Kitsune url\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"\n        offset = kwargs['offset']\n\n        logger.info(\"Looking for questions at url '%s' using offset %s\",\n                    self.url, str(offset))\n\n        nquestions = 0  # number of questions processed\n        tquestions = 0  # number of questions from API data\n        equestions = 0  # number of questions dropped by errors\n\n        # Always get complete pages so the first item is always\n        # the first one in the page\n        page = int(offset / KitsuneClient.ITEMS_PER_PAGE)\n        page_offset = page * KitsuneClient.ITEMS_PER_PAGE\n        # drop questions from page before the offset\n        drop_questions = offset - page_offset\n        current_offset = offset\n\n        questions_page = self.client.get_questions(offset)\n\n        while True:\n            try:\n                raw_questions = next(questions_page)\n            except StopIteration:\n                break\n            except requests.exceptions.HTTPError as e:\n                # Continue with the next page if it is a 500 error\n                if e.response.status_code == 500:\n                    logger.exception(e)\n                    logger.error(\"Problem getting Kitsune questions. \"\n                                 \"Loosing %i questions. Going to the next page.\",\n                                 KitsuneClient.ITEMS_PER_PAGE)\n                    equestions += KitsuneClient.ITEMS_PER_PAGE\n                    current_offset += KitsuneClient.ITEMS_PER_PAGE\n                    questions_page = self.client.get_questions(current_offset)\n                    continue\n                else:\n                    # If it is another error just propagate the exception\n                    raise e\n\n            try:\n                questions_data = json.loads(raw_questions)\n                tquestions = questions_data['count']\n                questions = questions_data['results']\n            except (ValueError, KeyError) as ex:\n                logger.error(ex)\n                cause = (\"Bad JSON format for mozilla_questions: %s\" % (raw_questions))\n                raise ParseError(cause=cause)\n\n            for question in questions:\n                if drop_questions > 0:\n                    # Remove extra questions due to page base retrieval\n                    drop_questions -= 1\n                    continue\n                question['offset'] = current_offset\n                current_offset += 1\n                question['answers_data'] = []\n                for raw_answers in self.client.get_question_answers(question['id']):\n                    answers = json.loads(raw_answers)['results']\n                    question['answers_data'] += answers\n                yield question\n                nquestions += 1\n\n            logger.debug(\"Questions: %i/%i\", nquestions + offset, tquestions)\n\n        logger.info(\"Total number of questions: %i (%i total)\", nquestions, tquestions)\n        logger.info(\"Questions with errors dropped: %i\", equestions)", "language": "python", "code": "def fetch_items(self, category, **kwargs):\n        \"\"\"Fetch questions from the Kitsune url\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"\n        offset = kwargs['offset']\n\n        logger.info(\"Looking for questions at url '%s' using offset %s\",\n                    self.url, str(offset))\n\n        nquestions = 0  # number of questions processed\n        tquestions = 0  # number of questions from API data\n        equestions = 0  # number of questions dropped by errors\n\n        # Always get complete pages so the first item is always\n        # the first one in the page\n        page = int(offset / KitsuneClient.ITEMS_PER_PAGE)\n        page_offset = page * KitsuneClient.ITEMS_PER_PAGE\n        # drop questions from page before the offset\n        drop_questions = offset - page_offset\n        current_offset = offset\n\n        questions_page = self.client.get_questions(offset)\n\n        while True:\n            try:\n                raw_questions = next(questions_page)\n            except StopIteration:\n                break\n            except requests.exceptions.HTTPError as e:\n                # Continue with the next page if it is a 500 error\n                if e.response.status_code == 500:\n                    logger.exception(e)\n                    logger.error(\"Problem getting Kitsune questions. \"\n                                 \"Loosing %i questions. Going to the next page.\",\n                                 KitsuneClient.ITEMS_PER_PAGE)\n                    equestions += KitsuneClient.ITEMS_PER_PAGE\n                    current_offset += KitsuneClient.ITEMS_PER_PAGE\n                    questions_page = self.client.get_questions(current_offset)\n                    continue\n                else:\n                    # If it is another error just propagate the exception\n                    raise e\n\n            try:\n                questions_data = json.loads(raw_questions)\n                tquestions = questions_data['count']\n                questions = questions_data['results']\n            except (ValueError, KeyError) as ex:\n                logger.error(ex)\n                cause = (\"Bad JSON format for mozilla_questions: %s\" % (raw_questions))\n                raise ParseError(cause=cause)\n\n            for question in questions:\n                if drop_questions > 0:\n                    # Remove extra questions due to page base retrieval\n                    drop_questions -= 1\n                    continue\n                question['offset'] = current_offset\n                current_offset += 1\n                question['answers_data'] = []\n                for raw_answers in self.client.get_question_answers(question['id']):\n                    answers = json.loads(raw_answers)['results']\n                    question['answers_data'] += answers\n                yield question\n                nquestions += 1\n\n            logger.debug(\"Questions: %i/%i\", nquestions + offset, tquestions)\n\n        logger.info(\"Total number of questions: %i (%i total)\", nquestions, tquestions)\n        logger.info(\"Questions with errors dropped: %i\", equestions)", "code_tokens": ["def", "fetch_items", "(", "self", ",", "category", ",", "*", "*", "kwargs", ")", ":", "offset", "=", "kwargs", "[", "'offset'", "]", "logger", ".", "info", "(", "\"Looking for questions at url '%s' using offset %s\"", ",", "self", ".", "url", ",", "str", "(", "offset", ")", ")", "nquestions", "=", "0", "# number of questions processed", "tquestions", "=", "0", "# number of questions from API data", "equestions", "=", "0", "# number of questions dropped by errors", "# Always get complete pages so the first item is always", "# the first one in the page", "page", "=", "int", "(", "offset", "/", "KitsuneClient", ".", "ITEMS_PER_PAGE", ")", "page_offset", "=", "page", "*", "KitsuneClient", ".", "ITEMS_PER_PAGE", "# drop questions from page before the offset", "drop_questions", "=", "offset", "-", "page_offset", "current_offset", "=", "offset", "questions_page", "=", "self", ".", "client", ".", "get_questions", "(", "offset", ")", "while", "True", ":", "try", ":", "raw_questions", "=", "next", "(", "questions_page", ")", "except", "StopIteration", ":", "break", "except", "requests", ".", "exceptions", ".", "HTTPError", "as", "e", ":", "# Continue with the next page if it is a 500 error", "if", "e", ".", "response", ".", "status_code", "==", "500", ":", "logger", ".", "exception", "(", "e", ")", "logger", ".", "error", "(", "\"Problem getting Kitsune questions. \"", "\"Loosing %i questions. Going to the next page.\"", ",", "KitsuneClient", ".", "ITEMS_PER_PAGE", ")", "equestions", "+=", "KitsuneClient", ".", "ITEMS_PER_PAGE", "current_offset", "+=", "KitsuneClient", ".", "ITEMS_PER_PAGE", "questions_page", "=", "self", ".", "client", ".", "get_questions", "(", "current_offset", ")", "continue", "else", ":", "# If it is another error just propagate the exception", "raise", "e", "try", ":", "questions_data", "=", "json", ".", "loads", "(", "raw_questions", ")", "tquestions", "=", "questions_data", "[", "'count'", "]", "questions", "=", "questions_data", "[", "'results'", "]", "except", "(", "ValueError", ",", "KeyError", ")", "as", "ex", ":", "logger", ".", "error", "(", "ex", ")", "cause", "=", "(", "\"Bad JSON format for mozilla_questions: %s\"", "%", "(", "raw_questions", ")", ")", "raise", "ParseError", "(", "cause", "=", "cause", ")", "for", "question", "in", "questions", ":", "if", "drop_questions", ">", "0", ":", "# Remove extra questions due to page base retrieval", "drop_questions", "-=", "1", "continue", "question", "[", "'offset'", "]", "=", "current_offset", "current_offset", "+=", "1", "question", "[", "'answers_data'", "]", "=", "[", "]", "for", "raw_answers", "in", "self", ".", "client", ".", "get_question_answers", "(", "question", "[", "'id'", "]", ")", ":", "answers", "=", "json", ".", "loads", "(", "raw_answers", ")", "[", "'results'", "]", "question", "[", "'answers_data'", "]", "+=", "answers", "yield", "question", "nquestions", "+=", "1", "logger", ".", "debug", "(", "\"Questions: %i/%i\"", ",", "nquestions", "+", "offset", ",", "tquestions", ")", "logger", ".", "info", "(", "\"Total number of questions: %i (%i total)\"", ",", "nquestions", ",", "tquestions", ")", "logger", ".", "info", "(", "\"Questions with errors dropped: %i\"", ",", "equestions", ")"], "docstring": "Fetch questions from the Kitsune url\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items", "docstring_tokens": ["Fetch", "questions", "from", "the", "Kitsune", "url"], "sha": "4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4", "url": "https://github.com/chaoss/grimoirelab-perceval-mozilla/blob/4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4/perceval/backends/mozilla/kitsune.py#L89-L162", "partition": "test"}
{"repo": "HumanBrainProject/hbp-service-client", "path": "hbp_service_client/storage_service/api.py", "func_name": "ApiClient.update_metadata", "original_string": "def update_metadata(self, entity_type, entity_id, metadata):\n        '''Update the metadata of an entity.\n\n        Existing non-modified metadata will not be affected.\n\n        Args:\n            entity_type (str): Type of the entity. Admitted values: 'project',\n                'folder', 'file'.\n            entity_id (str): The UUID of the entity to be modified.\n            metadata (dict): A dictionary of key/value pairs to be written as\n                metadata.\n\n        Returns:\n            A dictionary of the updated object metadata::\n\n                {\n                    u'bar': u'200',\n                    u'foo': u'100'\n                }\n\n        Raises:\n            StorageArgumentException: Invalid arguments\n            StorageForbiddenException: Server response code 403\n            StorageNotFoundException: Server response code 404\n            StorageException: other 400-600 error codes\n        '''\n        if not is_valid_uuid(entity_id):\n            raise StorageArgumentException(\n                'Invalid UUID for entity_id: {0}'.format(entity_id))\n        if not isinstance(metadata, dict):\n            raise StorageArgumentException('The metadata was not provided as a '\n                                           'dictionary')\n\n        return self._authenticated_request \\\n            .to_endpoint('{}/{}/metadata/'.format(entity_type, entity_id)) \\\n            .with_json_body(metadata) \\\n            .return_body() \\\n            .put()", "language": "python", "code": "def update_metadata(self, entity_type, entity_id, metadata):\n        '''Update the metadata of an entity.\n\n        Existing non-modified metadata will not be affected.\n\n        Args:\n            entity_type (str): Type of the entity. Admitted values: 'project',\n                'folder', 'file'.\n            entity_id (str): The UUID of the entity to be modified.\n            metadata (dict): A dictionary of key/value pairs to be written as\n                metadata.\n\n        Returns:\n            A dictionary of the updated object metadata::\n\n                {\n                    u'bar': u'200',\n                    u'foo': u'100'\n                }\n\n        Raises:\n            StorageArgumentException: Invalid arguments\n            StorageForbiddenException: Server response code 403\n            StorageNotFoundException: Server response code 404\n            StorageException: other 400-600 error codes\n        '''\n        if not is_valid_uuid(entity_id):\n            raise StorageArgumentException(\n                'Invalid UUID for entity_id: {0}'.format(entity_id))\n        if not isinstance(metadata, dict):\n            raise StorageArgumentException('The metadata was not provided as a '\n                                           'dictionary')\n\n        return self._authenticated_request \\\n            .to_endpoint('{}/{}/metadata/'.format(entity_type, entity_id)) \\\n            .with_json_body(metadata) \\\n            .return_body() \\\n            .put()", "code_tokens": ["def", "update_metadata", "(", "self", ",", "entity_type", ",", "entity_id", ",", "metadata", ")", ":", "if", "not", "is_valid_uuid", "(", "entity_id", ")", ":", "raise", "StorageArgumentException", "(", "'Invalid UUID for entity_id: {0}'", ".", "format", "(", "entity_id", ")", ")", "if", "not", "isinstance", "(", "metadata", ",", "dict", ")", ":", "raise", "StorageArgumentException", "(", "'The metadata was not provided as a '", "'dictionary'", ")", "return", "self", ".", "_authenticated_request", ".", "to_endpoint", "(", "'{}/{}/metadata/'", ".", "format", "(", "entity_type", ",", "entity_id", ")", ")", ".", "with_json_body", "(", "metadata", ")", ".", "return_body", "(", ")", ".", "put", "(", ")"], "docstring": "Update the metadata of an entity.\n\n        Existing non-modified metadata will not be affected.\n\n        Args:\n            entity_type (str): Type of the entity. Admitted values: 'project',\n                'folder', 'file'.\n            entity_id (str): The UUID of the entity to be modified.\n            metadata (dict): A dictionary of key/value pairs to be written as\n                metadata.\n\n        Returns:\n            A dictionary of the updated object metadata::\n\n                {\n                    u'bar': u'200',\n                    u'foo': u'100'\n                }\n\n        Raises:\n            StorageArgumentException: Invalid arguments\n            StorageForbiddenException: Server response code 403\n            StorageNotFoundException: Server response code 404\n            StorageException: other 400-600 error codes", "docstring_tokens": ["Update", "the", "metadata", "of", "an", "entity", "."], "sha": "b338fb41a7f0e7b9d654ff28fcf13a56d03bff4d", "url": "https://github.com/HumanBrainProject/hbp-service-client/blob/b338fb41a7f0e7b9d654ff28fcf13a56d03bff4d/hbp_service_client/storage_service/api.py#L302-L339", "partition": "test"}
{"repo": "lvh/txampext", "path": "txampext/multiplexing.py", "func_name": "ProxyingProtocol.connectionLost", "original_string": "def connectionLost(self, reason):\n        \"\"\"If we already have an AMP connection registered on the factory,\n        get rid of it.\n\n        \"\"\"\n        if self.connection is not None:\n            del self.factory.protocols[self.connection]", "language": "python", "code": "def connectionLost(self, reason):\n        \"\"\"If we already have an AMP connection registered on the factory,\n        get rid of it.\n\n        \"\"\"\n        if self.connection is not None:\n            del self.factory.protocols[self.connection]", "code_tokens": ["def", "connectionLost", "(", "self", ",", "reason", ")", ":", "if", "self", ".", "connection", "is", "not", "None", ":", "del", "self", ".", "factory", ".", "protocols", "[", "self", ".", "connection", "]"], "docstring": "If we already have an AMP connection registered on the factory,\n        get rid of it.", "docstring_tokens": ["If", "we", "already", "have", "an", "AMP", "connection", "registered", "on", "the", "factory", "get", "rid", "of", "it", "."], "sha": "a7d6cb9f1e9200dba597378cd40eb6a2096d4fd9", "url": "https://github.com/lvh/txampext/blob/a7d6cb9f1e9200dba597378cd40eb6a2096d4fd9/txampext/multiplexing.py#L328-L334", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/itsdangerous.py", "func_name": "Signer.get_signature", "original_string": "def get_signature(self, value):\n        \"\"\"Returns the signature for the given value\"\"\"\n        value = want_bytes(value)\n        key = self.derive_key()\n        sig = self.algorithm.get_signature(key, value)\n        return base64_encode(sig)", "language": "python", "code": "def get_signature(self, value):\n        \"\"\"Returns the signature for the given value\"\"\"\n        value = want_bytes(value)\n        key = self.derive_key()\n        sig = self.algorithm.get_signature(key, value)\n        return base64_encode(sig)", "code_tokens": ["def", "get_signature", "(", "self", ",", "value", ")", ":", "value", "=", "want_bytes", "(", "value", ")", "key", "=", "self", ".", "derive_key", "(", ")", "sig", "=", "self", ".", "algorithm", ".", "get_signature", "(", "key", ",", "value", ")", "return", "base64_encode", "(", "sig", ")"], "docstring": "Returns the signature for the given value", "docstring_tokens": ["Returns", "the", "signature", "for", "the", "given", "value"], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/itsdangerous.py#L344-L349", "partition": "test"}
{"repo": "solvebio/solvebio-python", "path": "solvebio/resource/task.py", "func_name": "Task.child_object", "original_string": "def child_object(self):\n        \"\"\" Get Task child object class \"\"\"\n        from . import types\n        child_klass = types.get(self.task_type.split('.')[1])\n        return child_klass.retrieve(self.task_id, client=self._client)", "language": "python", "code": "def child_object(self):\n        \"\"\" Get Task child object class \"\"\"\n        from . import types\n        child_klass = types.get(self.task_type.split('.')[1])\n        return child_klass.retrieve(self.task_id, client=self._client)", "code_tokens": ["def", "child_object", "(", "self", ")", ":", "from", ".", "import", "types", "child_klass", "=", "types", ".", "get", "(", "self", ".", "task_type", ".", "split", "(", "'.'", ")", "[", "1", "]", ")", "return", "child_klass", ".", "retrieve", "(", "self", ".", "task_id", ",", "client", "=", "self", ".", "_client", ")"], "docstring": "Get Task child object class", "docstring_tokens": ["Get", "Task", "child", "object", "class"], "sha": "b29614643043afd19c1d8074e8f25c6700d51a73", "url": "https://github.com/solvebio/solvebio-python/blob/b29614643043afd19c1d8074e8f25c6700d51a73/solvebio/resource/task.py#L22-L26", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/frame.py", "func_name": "H2OFrame.summary", "original_string": "def summary(self, return_data=False):\n        \"\"\"\n        Display summary information about the frame.\n\n        Summary includes min/mean/max/sigma and other rollup data.\n\n        :param bool return_data: Return a dictionary of the summary output\n        \"\"\"\n        if not self._has_content():\n            print(\"This H2OFrame is empty and not initialized.\")\n            return self._ex._cache._data;\n        if not self._ex._cache.is_valid(): self._frame()._ex._cache.fill()\n        if not return_data:\n            if self.nrows == 0:\n                print(\"This H2OFrame is empty.\")\n            elif H2ODisplay._in_ipy():\n                import IPython.display\n                IPython.display.display_html(self._ex._cache._tabulate(\"html\", True), raw=True)\n            else:\n                print(self._ex._cache._tabulate(\"simple\", True))\n        else:\n            return self._ex._cache._data", "language": "python", "code": "def summary(self, return_data=False):\n        \"\"\"\n        Display summary information about the frame.\n\n        Summary includes min/mean/max/sigma and other rollup data.\n\n        :param bool return_data: Return a dictionary of the summary output\n        \"\"\"\n        if not self._has_content():\n            print(\"This H2OFrame is empty and not initialized.\")\n            return self._ex._cache._data;\n        if not self._ex._cache.is_valid(): self._frame()._ex._cache.fill()\n        if not return_data:\n            if self.nrows == 0:\n                print(\"This H2OFrame is empty.\")\n            elif H2ODisplay._in_ipy():\n                import IPython.display\n                IPython.display.display_html(self._ex._cache._tabulate(\"html\", True), raw=True)\n            else:\n                print(self._ex._cache._tabulate(\"simple\", True))\n        else:\n            return self._ex._cache._data", "code_tokens": ["def", "summary", "(", "self", ",", "return_data", "=", "False", ")", ":", "if", "not", "self", ".", "_has_content", "(", ")", ":", "print", "(", "\"This H2OFrame is empty and not initialized.\"", ")", "return", "self", ".", "_ex", ".", "_cache", ".", "_data", "if", "not", "self", ".", "_ex", ".", "_cache", ".", "is_valid", "(", ")", ":", "self", ".", "_frame", "(", ")", ".", "_ex", ".", "_cache", ".", "fill", "(", ")", "if", "not", "return_data", ":", "if", "self", ".", "nrows", "==", "0", ":", "print", "(", "\"This H2OFrame is empty.\"", ")", "elif", "H2ODisplay", ".", "_in_ipy", "(", ")", ":", "import", "IPython", ".", "display", "IPython", ".", "display", ".", "display_html", "(", "self", ".", "_ex", ".", "_cache", ".", "_tabulate", "(", "\"html\"", ",", "True", ")", ",", "raw", "=", "True", ")", "else", ":", "print", "(", "self", ".", "_ex", ".", "_cache", ".", "_tabulate", "(", "\"simple\"", ",", "True", ")", ")", "else", ":", "return", "self", ".", "_ex", ".", "_cache", ".", "_data"], "docstring": "Display summary information about the frame.\n\n        Summary includes min/mean/max/sigma and other rollup data.\n\n        :param bool return_data: Return a dictionary of the summary output", "docstring_tokens": ["Display", "summary", "information", "about", "the", "frame", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/frame.py#L459-L480", "partition": "test"}
{"repo": "opencast/pyCA", "path": "pyca/utils.py", "func_name": "get_service", "original_string": "def get_service(service_type):\n    '''Get available service endpoints for a given service type from the\n    Opencast ServiceRegistry.\n    '''\n    endpoint = '/services/available.json?serviceType=' + str(service_type)\n    url = '%s%s' % (config()['server']['url'], endpoint)\n    response = http_request(url).decode('utf-8')\n    services = (json.loads(response).get('services') or {}).get('service', [])\n    services = ensurelist(services)\n    endpoints = [service['host'] + service['path'] for service in services\n                 if service['online'] and service['active']]\n    for endpoint in endpoints:\n        logger.info(u'Endpoint for %s: %s', service_type, endpoint)\n    return endpoints", "language": "python", "code": "def get_service(service_type):\n    '''Get available service endpoints for a given service type from the\n    Opencast ServiceRegistry.\n    '''\n    endpoint = '/services/available.json?serviceType=' + str(service_type)\n    url = '%s%s' % (config()['server']['url'], endpoint)\n    response = http_request(url).decode('utf-8')\n    services = (json.loads(response).get('services') or {}).get('service', [])\n    services = ensurelist(services)\n    endpoints = [service['host'] + service['path'] for service in services\n                 if service['online'] and service['active']]\n    for endpoint in endpoints:\n        logger.info(u'Endpoint for %s: %s', service_type, endpoint)\n    return endpoints", "code_tokens": ["def", "get_service", "(", "service_type", ")", ":", "endpoint", "=", "'/services/available.json?serviceType='", "+", "str", "(", "service_type", ")", "url", "=", "'%s%s'", "%", "(", "config", "(", ")", "[", "'server'", "]", "[", "'url'", "]", ",", "endpoint", ")", "response", "=", "http_request", "(", "url", ")", ".", "decode", "(", "'utf-8'", ")", "services", "=", "(", "json", ".", "loads", "(", "response", ")", ".", "get", "(", "'services'", ")", "or", "{", "}", ")", ".", "get", "(", "'service'", ",", "[", "]", ")", "services", "=", "ensurelist", "(", "services", ")", "endpoints", "=", "[", "service", "[", "'host'", "]", "+", "service", "[", "'path'", "]", "for", "service", "in", "services", "if", "service", "[", "'online'", "]", "and", "service", "[", "'active'", "]", "]", "for", "endpoint", "in", "endpoints", ":", "logger", ".", "info", "(", "u'Endpoint for %s: %s'", ",", "service_type", ",", "endpoint", ")", "return", "endpoints"], "docstring": "Get available service endpoints for a given service type from the\n    Opencast ServiceRegistry.", "docstring_tokens": ["Get", "available", "service", "endpoints", "for", "a", "given", "service", "type", "from", "the", "Opencast", "ServiceRegistry", "."], "sha": "c89b168d4780d157e1b3f7676628c1b131956a88", "url": "https://github.com/opencast/pyCA/blob/c89b168d4780d157e1b3f7676628c1b131956a88/pyca/utils.py#L69-L82", "partition": "test"}
{"repo": "rackerlabs/timid", "path": "timid/steps.py", "func_name": "StepItem.init", "original_string": "def init(self, ctxt, step_addr):\n        \"\"\"\n        Initialize the item.  This calls the class constructor with the\n        appropriate arguments and returns the initialized object.\n\n        :param ctxt: The context object.\n        :param step_addr: The address of the step in the test\n                          configuration.\n        \"\"\"\n\n        return self.cls(ctxt, self.name, self.conf, step_addr)", "language": "python", "code": "def init(self, ctxt, step_addr):\n        \"\"\"\n        Initialize the item.  This calls the class constructor with the\n        appropriate arguments and returns the initialized object.\n\n        :param ctxt: The context object.\n        :param step_addr: The address of the step in the test\n                          configuration.\n        \"\"\"\n\n        return self.cls(ctxt, self.name, self.conf, step_addr)", "code_tokens": ["def", "init", "(", "self", ",", "ctxt", ",", "step_addr", ")", ":", "return", "self", ".", "cls", "(", "ctxt", ",", "self", ".", "name", ",", "self", ".", "conf", ",", "step_addr", ")"], "docstring": "Initialize the item.  This calls the class constructor with the\n        appropriate arguments and returns the initialized object.\n\n        :param ctxt: The context object.\n        :param step_addr: The address of the step in the test\n                          configuration.", "docstring_tokens": ["Initialize", "the", "item", ".", "This", "calls", "the", "class", "constructor", "with", "the", "appropriate", "arguments", "and", "returns", "the", "initialized", "object", "."], "sha": "b1c6aa159ab380a033740f4aa392cf0d125e0ac6", "url": "https://github.com/rackerlabs/timid/blob/b1c6aa159ab380a033740f4aa392cf0d125e0ac6/timid/steps.py#L329-L339", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/gurumate-2.8.6-py2.7.egg/gurumate/web.py", "func_name": "url_has_contents", "original_string": "def url_has_contents(url, contents, case_sensitive=False, timeout=10):\n    ''' \n    Check whether the HTML page contains the content or not and return boolean\n    '''\n    try:    \n        req = urllib2.urlopen(url, timeout=timeout)\n    except Exception, _:\n        False\n    else:\n        rep = req.read()\n        if (not case_sensitive and rep.lower().find(contents.lower()) >= 0) or (case_sensitive and rep.find(contents) >= 0):\n            return True\n        else:\n            return False", "language": "python", "code": "def url_has_contents(url, contents, case_sensitive=False, timeout=10):\n    ''' \n    Check whether the HTML page contains the content or not and return boolean\n    '''\n    try:    \n        req = urllib2.urlopen(url, timeout=timeout)\n    except Exception, _:\n        False\n    else:\n        rep = req.read()\n        if (not case_sensitive and rep.lower().find(contents.lower()) >= 0) or (case_sensitive and rep.find(contents) >= 0):\n            return True\n        else:\n            return False", "code_tokens": ["def", "url_has_contents", "(", "url", ",", "contents", ",", "case_sensitive", "=", "False", ",", "timeout", "=", "10", ")", ":", "try", ":", "req", "=", "urllib2", ".", "urlopen", "(", "url", ",", "timeout", "=", "timeout", ")", "except", "Exception", ",", "_", ":", "False", "else", ":", "rep", "=", "req", ".", "read", "(", ")", "if", "(", "not", "case_sensitive", "and", "rep", ".", "lower", "(", ")", ".", "find", "(", "contents", ".", "lower", "(", ")", ")", ">=", "0", ")", "or", "(", "case_sensitive", "and", "rep", ".", "find", "(", "contents", ")", ">=", "0", ")", ":", "return", "True", "else", ":", "return", "False"], "docstring": "Check whether the HTML page contains the content or not and return boolean", "docstring_tokens": ["Check", "whether", "the", "HTML", "page", "contains", "the", "content", "or", "not", "and", "return", "boolean"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/gurumate-2.8.6-py2.7.egg/gurumate/web.py#L20-L33", "partition": "test"}
{"repo": "RRZE-HPC/kerncraft", "path": "kerncraft/picklemerge.py", "func_name": "main", "original_string": "def main():\n    \"\"\"Comand line interface of picklemerge.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Recursively merges two or more pickle files. Only supports pickles consisting '\n        'of a single dictionary object.')\n    parser.add_argument('destination', type=argparse.FileType('r+b'),\n                        help='File to write to and include in resulting pickle. (WILL BE CHANGED)')\n    parser.add_argument('source', type=argparse.FileType('rb'), nargs='+',\n                        help='File to include in resulting pickle.')\n\n    args = parser.parse_args()\n\n    result = pickle.load(args.destination)\n    assert isinstance(result, collections.Mapping), \"only Mapping types can be handled.\"\n\n    for s in args.source:\n        data = pickle.load(s)\n        assert isinstance(data, collections.Mapping), \"only Mapping types can be handled.\"\n\n        update(result, data)\n\n    args.destination.seek(0)\n    args.destination.truncate()\n    pickle.dump(result, args.destination)", "language": "python", "code": "def main():\n    \"\"\"Comand line interface of picklemerge.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Recursively merges two or more pickle files. Only supports pickles consisting '\n        'of a single dictionary object.')\n    parser.add_argument('destination', type=argparse.FileType('r+b'),\n                        help='File to write to and include in resulting pickle. (WILL BE CHANGED)')\n    parser.add_argument('source', type=argparse.FileType('rb'), nargs='+',\n                        help='File to include in resulting pickle.')\n\n    args = parser.parse_args()\n\n    result = pickle.load(args.destination)\n    assert isinstance(result, collections.Mapping), \"only Mapping types can be handled.\"\n\n    for s in args.source:\n        data = pickle.load(s)\n        assert isinstance(data, collections.Mapping), \"only Mapping types can be handled.\"\n\n        update(result, data)\n\n    args.destination.seek(0)\n    args.destination.truncate()\n    pickle.dump(result, args.destination)", "code_tokens": ["def", "main", "(", ")", ":", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Recursively merges two or more pickle files. Only supports pickles consisting '", "'of a single dictionary object.'", ")", "parser", ".", "add_argument", "(", "'destination'", ",", "type", "=", "argparse", ".", "FileType", "(", "'r+b'", ")", ",", "help", "=", "'File to write to and include in resulting pickle. (WILL BE CHANGED)'", ")", "parser", ".", "add_argument", "(", "'source'", ",", "type", "=", "argparse", ".", "FileType", "(", "'rb'", ")", ",", "nargs", "=", "'+'", ",", "help", "=", "'File to include in resulting pickle.'", ")", "args", "=", "parser", ".", "parse_args", "(", ")", "result", "=", "pickle", ".", "load", "(", "args", ".", "destination", ")", "assert", "isinstance", "(", "result", ",", "collections", ".", "Mapping", ")", ",", "\"only Mapping types can be handled.\"", "for", "s", "in", "args", ".", "source", ":", "data", "=", "pickle", ".", "load", "(", "s", ")", "assert", "isinstance", "(", "data", ",", "collections", ".", "Mapping", ")", ",", "\"only Mapping types can be handled.\"", "update", "(", "result", ",", "data", ")", "args", ".", "destination", ".", "seek", "(", "0", ")", "args", ".", "destination", ".", "truncate", "(", ")", "pickle", ".", "dump", "(", "result", ",", "args", ".", "destination", ")"], "docstring": "Comand line interface of picklemerge.", "docstring_tokens": ["Comand", "line", "interface", "of", "picklemerge", "."], "sha": "c60baf8043e4da8d8d66da7575021c2f4c6c78af", "url": "https://github.com/RRZE-HPC/kerncraft/blob/c60baf8043e4da8d8d66da7575021c2f4c6c78af/kerncraft/picklemerge.py#L23-L46", "partition": "test"}
{"repo": "chrisjrn/registrasion", "path": "registrasion/controllers/invoice.py", "func_name": "InvoiceController._apply_credit_notes", "original_string": "def _apply_credit_notes(cls, invoice):\n        ''' Applies the user's credit notes to the given invoice on creation.\n        '''\n\n        # We only automatically apply credit notes if this is the *only*\n        # unpaid invoice for this user.\n        invoices = commerce.Invoice.objects.filter(\n            user=invoice.user,\n            status=commerce.Invoice.STATUS_UNPAID,\n        )\n        if invoices.count() > 1:\n            return\n\n        notes = commerce.CreditNote.unclaimed().filter(\n            invoice__user=invoice.user\n        )\n        for note in notes:\n            try:\n                CreditNoteController(note).apply_to_invoice(invoice)\n            except ValidationError:\n                # ValidationError will get raised once we're overpaying.\n                break\n\n        invoice.refresh_from_db()", "language": "python", "code": "def _apply_credit_notes(cls, invoice):\n        ''' Applies the user's credit notes to the given invoice on creation.\n        '''\n\n        # We only automatically apply credit notes if this is the *only*\n        # unpaid invoice for this user.\n        invoices = commerce.Invoice.objects.filter(\n            user=invoice.user,\n            status=commerce.Invoice.STATUS_UNPAID,\n        )\n        if invoices.count() > 1:\n            return\n\n        notes = commerce.CreditNote.unclaimed().filter(\n            invoice__user=invoice.user\n        )\n        for note in notes:\n            try:\n                CreditNoteController(note).apply_to_invoice(invoice)\n            except ValidationError:\n                # ValidationError will get raised once we're overpaying.\n                break\n\n        invoice.refresh_from_db()", "code_tokens": ["def", "_apply_credit_notes", "(", "cls", ",", "invoice", ")", ":", "# We only automatically apply credit notes if this is the *only*", "# unpaid invoice for this user.", "invoices", "=", "commerce", ".", "Invoice", ".", "objects", ".", "filter", "(", "user", "=", "invoice", ".", "user", ",", "status", "=", "commerce", ".", "Invoice", ".", "STATUS_UNPAID", ",", ")", "if", "invoices", ".", "count", "(", ")", ">", "1", ":", "return", "notes", "=", "commerce", ".", "CreditNote", ".", "unclaimed", "(", ")", ".", "filter", "(", "invoice__user", "=", "invoice", ".", "user", ")", "for", "note", "in", "notes", ":", "try", ":", "CreditNoteController", "(", "note", ")", ".", "apply_to_invoice", "(", "invoice", ")", "except", "ValidationError", ":", "# ValidationError will get raised once we're overpaying.", "break", "invoice", ".", "refresh_from_db", "(", ")"], "docstring": "Applies the user's credit notes to the given invoice on creation.", "docstring_tokens": ["Applies", "the", "user", "s", "credit", "notes", "to", "the", "given", "invoice", "on", "creation", "."], "sha": "461d5846c6f9f3b7099322a94f5d9911564448e4", "url": "https://github.com/chrisjrn/registrasion/blob/461d5846c6f9f3b7099322a94f5d9911564448e4/registrasion/controllers/invoice.py#L206-L229", "partition": "test"}
{"repo": "tmontaigu/pylas", "path": "pylas/point/dims.py", "func_name": "_build_unpacked_point_formats_dtypes", "original_string": "def _build_unpacked_point_formats_dtypes(\n        point_formats_dimensions, composed_fields_dict, dimensions_dict\n):\n    \"\"\" Builds the dict mapping point format id to numpy.dtype\n    In the dtypes, bit fields are unpacked and can be accessed directly\n    \"\"\"\n    unpacked_dtypes = {}\n    for fmt_id, dim_names in point_formats_dimensions.items():\n        composed_dims, dtype = composed_fields_dict[fmt_id], []\n        for dim_name in dim_names:\n            if dim_name in composed_dims:\n                dtype.extend((f.name, f.type) for f in composed_dims[dim_name])\n            else:\n                dtype.append(dimensions_dict[dim_name])\n        unpacked_dtypes[fmt_id] = np.dtype(dtype)\n    return unpacked_dtypes", "language": "python", "code": "def _build_unpacked_point_formats_dtypes(\n        point_formats_dimensions, composed_fields_dict, dimensions_dict\n):\n    \"\"\" Builds the dict mapping point format id to numpy.dtype\n    In the dtypes, bit fields are unpacked and can be accessed directly\n    \"\"\"\n    unpacked_dtypes = {}\n    for fmt_id, dim_names in point_formats_dimensions.items():\n        composed_dims, dtype = composed_fields_dict[fmt_id], []\n        for dim_name in dim_names:\n            if dim_name in composed_dims:\n                dtype.extend((f.name, f.type) for f in composed_dims[dim_name])\n            else:\n                dtype.append(dimensions_dict[dim_name])\n        unpacked_dtypes[fmt_id] = np.dtype(dtype)\n    return unpacked_dtypes", "code_tokens": ["def", "_build_unpacked_point_formats_dtypes", "(", "point_formats_dimensions", ",", "composed_fields_dict", ",", "dimensions_dict", ")", ":", "unpacked_dtypes", "=", "{", "}", "for", "fmt_id", ",", "dim_names", "in", "point_formats_dimensions", ".", "items", "(", ")", ":", "composed_dims", ",", "dtype", "=", "composed_fields_dict", "[", "fmt_id", "]", ",", "[", "]", "for", "dim_name", "in", "dim_names", ":", "if", "dim_name", "in", "composed_dims", ":", "dtype", ".", "extend", "(", "(", "f", ".", "name", ",", "f", ".", "type", ")", "for", "f", "in", "composed_dims", "[", "dim_name", "]", ")", "else", ":", "dtype", ".", "append", "(", "dimensions_dict", "[", "dim_name", "]", ")", "unpacked_dtypes", "[", "fmt_id", "]", "=", "np", ".", "dtype", "(", "dtype", ")", "return", "unpacked_dtypes"], "docstring": "Builds the dict mapping point format id to numpy.dtype\n    In the dtypes, bit fields are unpacked and can be accessed directly", "docstring_tokens": ["Builds", "the", "dict", "mapping", "point", "format", "id", "to", "numpy", ".", "dtype", "In", "the", "dtypes", "bit", "fields", "are", "unpacked", "and", "can", "be", "accessed", "directly"], "sha": "8335a1a7d7677f0e4bc391bb6fa3c75b42ed5b06", "url": "https://github.com/tmontaigu/pylas/blob/8335a1a7d7677f0e4bc391bb6fa3c75b42ed5b06/pylas/point/dims.py#L40-L55", "partition": "test"}
{"repo": "h2non/pook", "path": "pook/mock.py", "func_name": "Mock.match", "original_string": "def match(self, request):\n        \"\"\"\n        Matches an outgoing HTTP request against the current mock matchers.\n\n        This method acts like a delegator to `pook.MatcherEngine`.\n\n        Arguments:\n            request (pook.Request): request instance to match.\n\n        Raises:\n            Exception: if the mock has an exception defined.\n\n        Returns:\n            tuple(bool, list[Exception]): ``True`` if the mock matches\n                the outgoing HTTP request, otherwise ``False``. Also returns\n                an optional list of error exceptions.\n        \"\"\"\n        # If mock already expired, fail it\n        if self._times <= 0:\n            raise PookExpiredMock('Mock expired')\n\n        # Trigger mock filters\n        for test in self.filters:\n            if not test(request, self):\n                return False, []\n\n        # Trigger mock mappers\n        for mapper in self.mappers:\n            request = mapper(request, self)\n            if not request:\n                raise ValueError('map function must return a request object')\n\n        # Match incoming request against registered mock matchers\n        matches, errors = self.matchers.match(request)\n\n        # If not matched, return False\n        if not matches:\n            return False, errors\n\n        # Register matched request for further inspecion and reference\n        self._calls.append(request)\n\n        # Increase mock call counter\n        self._matches += 1\n        if not self._persist:\n            self._times -= 1\n\n        # Raise simulated error\n        if self._error:\n            raise self._error\n\n        # Trigger callback when matched\n        for callback in self.callbacks:\n            callback(request, self)\n\n        return True, []", "language": "python", "code": "def match(self, request):\n        \"\"\"\n        Matches an outgoing HTTP request against the current mock matchers.\n\n        This method acts like a delegator to `pook.MatcherEngine`.\n\n        Arguments:\n            request (pook.Request): request instance to match.\n\n        Raises:\n            Exception: if the mock has an exception defined.\n\n        Returns:\n            tuple(bool, list[Exception]): ``True`` if the mock matches\n                the outgoing HTTP request, otherwise ``False``. Also returns\n                an optional list of error exceptions.\n        \"\"\"\n        # If mock already expired, fail it\n        if self._times <= 0:\n            raise PookExpiredMock('Mock expired')\n\n        # Trigger mock filters\n        for test in self.filters:\n            if not test(request, self):\n                return False, []\n\n        # Trigger mock mappers\n        for mapper in self.mappers:\n            request = mapper(request, self)\n            if not request:\n                raise ValueError('map function must return a request object')\n\n        # Match incoming request against registered mock matchers\n        matches, errors = self.matchers.match(request)\n\n        # If not matched, return False\n        if not matches:\n            return False, errors\n\n        # Register matched request for further inspecion and reference\n        self._calls.append(request)\n\n        # Increase mock call counter\n        self._matches += 1\n        if not self._persist:\n            self._times -= 1\n\n        # Raise simulated error\n        if self._error:\n            raise self._error\n\n        # Trigger callback when matched\n        for callback in self.callbacks:\n            callback(request, self)\n\n        return True, []", "code_tokens": ["def", "match", "(", "self", ",", "request", ")", ":", "# If mock already expired, fail it", "if", "self", ".", "_times", "<=", "0", ":", "raise", "PookExpiredMock", "(", "'Mock expired'", ")", "# Trigger mock filters", "for", "test", "in", "self", ".", "filters", ":", "if", "not", "test", "(", "request", ",", "self", ")", ":", "return", "False", ",", "[", "]", "# Trigger mock mappers", "for", "mapper", "in", "self", ".", "mappers", ":", "request", "=", "mapper", "(", "request", ",", "self", ")", "if", "not", "request", ":", "raise", "ValueError", "(", "'map function must return a request object'", ")", "# Match incoming request against registered mock matchers", "matches", ",", "errors", "=", "self", ".", "matchers", ".", "match", "(", "request", ")", "# If not matched, return False", "if", "not", "matches", ":", "return", "False", ",", "errors", "# Register matched request for further inspecion and reference", "self", ".", "_calls", ".", "append", "(", "request", ")", "# Increase mock call counter", "self", ".", "_matches", "+=", "1", "if", "not", "self", ".", "_persist", ":", "self", ".", "_times", "-=", "1", "# Raise simulated error", "if", "self", ".", "_error", ":", "raise", "self", ".", "_error", "# Trigger callback when matched", "for", "callback", "in", "self", ".", "callbacks", ":", "callback", "(", "request", ",", "self", ")", "return", "True", ",", "[", "]"], "docstring": "Matches an outgoing HTTP request against the current mock matchers.\n\n        This method acts like a delegator to `pook.MatcherEngine`.\n\n        Arguments:\n            request (pook.Request): request instance to match.\n\n        Raises:\n            Exception: if the mock has an exception defined.\n\n        Returns:\n            tuple(bool, list[Exception]): ``True`` if the mock matches\n                the outgoing HTTP request, otherwise ``False``. Also returns\n                an optional list of error exceptions.", "docstring_tokens": ["Matches", "an", "outgoing", "HTTP", "request", "against", "the", "current", "mock", "matchers", "."], "sha": "e64094e41e4d89d98d2d29af7608ef27dc50cf19", "url": "https://github.com/h2non/pook/blob/e64094e41e4d89d98d2d29af7608ef27dc50cf19/pook/mock.py#L697-L752", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/server/blueprints/variants/controllers.py", "func_name": "expected_inheritance", "original_string": "def expected_inheritance(variant_obj):\n    \"\"\"Gather information from common gene information.\"\"\"\n    manual_models = set()\n    for gene in variant_obj.get('genes', []):\n        manual_models.update(gene.get('manual_inheritance', []))\n    return list(manual_models)", "language": "python", "code": "def expected_inheritance(variant_obj):\n    \"\"\"Gather information from common gene information.\"\"\"\n    manual_models = set()\n    for gene in variant_obj.get('genes', []):\n        manual_models.update(gene.get('manual_inheritance', []))\n    return list(manual_models)", "code_tokens": ["def", "expected_inheritance", "(", "variant_obj", ")", ":", "manual_models", "=", "set", "(", ")", "for", "gene", "in", "variant_obj", ".", "get", "(", "'genes'", ",", "[", "]", ")", ":", "manual_models", ".", "update", "(", "gene", ".", "get", "(", "'manual_inheritance'", ",", "[", "]", ")", ")", "return", "list", "(", "manual_models", ")"], "docstring": "Gather information from common gene information.", "docstring_tokens": ["Gather", "information", "from", "common", "gene", "information", "."], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/server/blueprints/variants/controllers.py#L856-L861", "partition": "test"}
{"repo": "mseclab/PyJFuzz", "path": "gramfuzz/gramfuzz/fields.py", "func_name": "Q.build", "original_string": "def build(self, pre=None, shortest=False):\n        \"\"\"Build the ``Quote`` instance\n\n        :param list pre: The prerequisites list\n        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated.\n        \"\"\"\n        res = super(Q, self).build(pre, shortest=shortest)\n\n        if self.escape:\n            return repr(res)\n        elif self.html_js_escape:\n            return (\"'\" + res.encode(\"string_escape\").replace(\"<\", \"\\\\x3c\").replace(\">\", \"\\\\x3e\") + \"'\")\n        else:\n            return \"{q}{r}{q}\".format(q=self.quote, r=res)", "language": "python", "code": "def build(self, pre=None, shortest=False):\n        \"\"\"Build the ``Quote`` instance\n\n        :param list pre: The prerequisites list\n        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated.\n        \"\"\"\n        res = super(Q, self).build(pre, shortest=shortest)\n\n        if self.escape:\n            return repr(res)\n        elif self.html_js_escape:\n            return (\"'\" + res.encode(\"string_escape\").replace(\"<\", \"\\\\x3c\").replace(\">\", \"\\\\x3e\") + \"'\")\n        else:\n            return \"{q}{r}{q}\".format(q=self.quote, r=res)", "code_tokens": ["def", "build", "(", "self", ",", "pre", "=", "None", ",", "shortest", "=", "False", ")", ":", "res", "=", "super", "(", "Q", ",", "self", ")", ".", "build", "(", "pre", ",", "shortest", "=", "shortest", ")", "if", "self", ".", "escape", ":", "return", "repr", "(", "res", ")", "elif", "self", ".", "html_js_escape", ":", "return", "(", "\"'\"", "+", "res", ".", "encode", "(", "\"string_escape\"", ")", ".", "replace", "(", "\"<\"", ",", "\"\\\\x3c\"", ")", ".", "replace", "(", "\">\"", ",", "\"\\\\x3e\"", ")", "+", "\"'\"", ")", "else", ":", "return", "\"{q}{r}{q}\"", ".", "format", "(", "q", "=", "self", ".", "quote", ",", "r", "=", "res", ")"], "docstring": "Build the ``Quote`` instance\n\n        :param list pre: The prerequisites list\n        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated.", "docstring_tokens": ["Build", "the", "Quote", "instance"], "sha": "f777067076f62c9ab74ffea6e90fd54402b7a1b4", "url": "https://github.com/mseclab/PyJFuzz/blob/f777067076f62c9ab74ffea6e90fd54402b7a1b4/gramfuzz/gramfuzz/fields.py#L491-L504", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "scripts/addjavamessage2ignore.py", "func_name": "update_message_dict", "original_string": "def update_message_dict(message_dict,action):\n    \"\"\"\n    Update the g_ok_java_messages dict structure by\n    1. add the new java ignored messages stored in message_dict if action == 1\n    2. remove the java ignored messages stired in message_dict if action == 2.\n\n    Parameters\n    ----------\n\n    message_dict :  Python dict\n      key: unit test name or \"general\"\n      value: list of java messages that are to be ignored if they are found when running the test stored as the key.  If\n        the key is \"general\", the list of java messages are to be ignored when running all tests.\n    action : int\n      if 1: add java ignored messages stored in message_dict to g_ok_java_messages dict;\n      if 2: remove java ignored messages stored in message_dict from g_ok_java_messages dict.\n\n    :return: none\n    \"\"\"\n    global g_ok_java_messages\n\n    allKeys = g_ok_java_messages.keys()\n\n    for key in message_dict.keys():\n        if key in allKeys:  # key already exists, just add to it\n            for message in message_dict[key]:\n\n                if action == 1:\n                    if message not in g_ok_java_messages[key]:\n                        g_ok_java_messages[key].append(message)\n\n                if action == 2:\n                    if message in g_ok_java_messages[key]:\n                        g_ok_java_messages[key].remove(message)\n        else:   # new key here.  Can only add and cannot remove\n            if action == 1:\n                g_ok_java_messages[key] = message_dict[key]", "language": "python", "code": "def update_message_dict(message_dict,action):\n    \"\"\"\n    Update the g_ok_java_messages dict structure by\n    1. add the new java ignored messages stored in message_dict if action == 1\n    2. remove the java ignored messages stired in message_dict if action == 2.\n\n    Parameters\n    ----------\n\n    message_dict :  Python dict\n      key: unit test name or \"general\"\n      value: list of java messages that are to be ignored if they are found when running the test stored as the key.  If\n        the key is \"general\", the list of java messages are to be ignored when running all tests.\n    action : int\n      if 1: add java ignored messages stored in message_dict to g_ok_java_messages dict;\n      if 2: remove java ignored messages stored in message_dict from g_ok_java_messages dict.\n\n    :return: none\n    \"\"\"\n    global g_ok_java_messages\n\n    allKeys = g_ok_java_messages.keys()\n\n    for key in message_dict.keys():\n        if key in allKeys:  # key already exists, just add to it\n            for message in message_dict[key]:\n\n                if action == 1:\n                    if message not in g_ok_java_messages[key]:\n                        g_ok_java_messages[key].append(message)\n\n                if action == 2:\n                    if message in g_ok_java_messages[key]:\n                        g_ok_java_messages[key].remove(message)\n        else:   # new key here.  Can only add and cannot remove\n            if action == 1:\n                g_ok_java_messages[key] = message_dict[key]", "code_tokens": ["def", "update_message_dict", "(", "message_dict", ",", "action", ")", ":", "global", "g_ok_java_messages", "allKeys", "=", "g_ok_java_messages", ".", "keys", "(", ")", "for", "key", "in", "message_dict", ".", "keys", "(", ")", ":", "if", "key", "in", "allKeys", ":", "# key already exists, just add to it", "for", "message", "in", "message_dict", "[", "key", "]", ":", "if", "action", "==", "1", ":", "if", "message", "not", "in", "g_ok_java_messages", "[", "key", "]", ":", "g_ok_java_messages", "[", "key", "]", ".", "append", "(", "message", ")", "if", "action", "==", "2", ":", "if", "message", "in", "g_ok_java_messages", "[", "key", "]", ":", "g_ok_java_messages", "[", "key", "]", ".", "remove", "(", "message", ")", "else", ":", "# new key here.  Can only add and cannot remove", "if", "action", "==", "1", ":", "g_ok_java_messages", "[", "key", "]", "=", "message_dict", "[", "key", "]"], "docstring": "Update the g_ok_java_messages dict structure by\n    1. add the new java ignored messages stored in message_dict if action == 1\n    2. remove the java ignored messages stired in message_dict if action == 2.\n\n    Parameters\n    ----------\n\n    message_dict :  Python dict\n      key: unit test name or \"general\"\n      value: list of java messages that are to be ignored if they are found when running the test stored as the key.  If\n        the key is \"general\", the list of java messages are to be ignored when running all tests.\n    action : int\n      if 1: add java ignored messages stored in message_dict to g_ok_java_messages dict;\n      if 2: remove java ignored messages stored in message_dict from g_ok_java_messages dict.\n\n    :return: none", "docstring_tokens": ["Update", "the", "g_ok_java_messages", "dict", "structure", "by", "1", ".", "add", "the", "new", "java", "ignored", "messages", "stored", "in", "message_dict", "if", "action", "==", "1", "2", ".", "remove", "the", "java", "ignored", "messages", "stired", "in", "message_dict", "if", "action", "==", "2", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/scripts/addjavamessage2ignore.py#L140-L176", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/utils.py", "func_name": "clobber_in_except", "original_string": "def clobber_in_except(\n    node: astroid.node_classes.NodeNG\n) -> Tuple[bool, Tuple[str, str]]:\n    \"\"\"Checks if an assignment node in an except handler clobbers an existing\n    variable.\n\n    Returns (True, args for W0623) if assignment clobbers an existing variable,\n    (False, None) otherwise.\n    \"\"\"\n    if isinstance(node, astroid.AssignAttr):\n        return True, (node.attrname, \"object %r\" % (node.expr.as_string(),))\n    if isinstance(node, astroid.AssignName):\n        name = node.name\n        if is_builtin(name):\n            return (True, (name, \"builtins\"))\n\n        stmts = node.lookup(name)[1]\n        if stmts and not isinstance(\n            stmts[0].assign_type(),\n            (astroid.Assign, astroid.AugAssign, astroid.ExceptHandler),\n        ):\n            return True, (name, \"outer scope (line %s)\" % stmts[0].fromlineno)\n    return False, None", "language": "python", "code": "def clobber_in_except(\n    node: astroid.node_classes.NodeNG\n) -> Tuple[bool, Tuple[str, str]]:\n    \"\"\"Checks if an assignment node in an except handler clobbers an existing\n    variable.\n\n    Returns (True, args for W0623) if assignment clobbers an existing variable,\n    (False, None) otherwise.\n    \"\"\"\n    if isinstance(node, astroid.AssignAttr):\n        return True, (node.attrname, \"object %r\" % (node.expr.as_string(),))\n    if isinstance(node, astroid.AssignName):\n        name = node.name\n        if is_builtin(name):\n            return (True, (name, \"builtins\"))\n\n        stmts = node.lookup(name)[1]\n        if stmts and not isinstance(\n            stmts[0].assign_type(),\n            (astroid.Assign, astroid.AugAssign, astroid.ExceptHandler),\n        ):\n            return True, (name, \"outer scope (line %s)\" % stmts[0].fromlineno)\n    return False, None", "code_tokens": ["def", "clobber_in_except", "(", "node", ":", "astroid", ".", "node_classes", ".", "NodeNG", ")", "->", "Tuple", "[", "bool", ",", "Tuple", "[", "str", ",", "str", "]", "]", ":", "if", "isinstance", "(", "node", ",", "astroid", ".", "AssignAttr", ")", ":", "return", "True", ",", "(", "node", ".", "attrname", ",", "\"object %r\"", "%", "(", "node", ".", "expr", ".", "as_string", "(", ")", ",", ")", ")", "if", "isinstance", "(", "node", ",", "astroid", ".", "AssignName", ")", ":", "name", "=", "node", ".", "name", "if", "is_builtin", "(", "name", ")", ":", "return", "(", "True", ",", "(", "name", ",", "\"builtins\"", ")", ")", "stmts", "=", "node", ".", "lookup", "(", "name", ")", "[", "1", "]", "if", "stmts", "and", "not", "isinstance", "(", "stmts", "[", "0", "]", ".", "assign_type", "(", ")", ",", "(", "astroid", ".", "Assign", ",", "astroid", ".", "AugAssign", ",", "astroid", ".", "ExceptHandler", ")", ",", ")", ":", "return", "True", ",", "(", "name", ",", "\"outer scope (line %s)\"", "%", "stmts", "[", "0", "]", ".", "fromlineno", ")", "return", "False", ",", "None"], "docstring": "Checks if an assignment node in an except handler clobbers an existing\n    variable.\n\n    Returns (True, args for W0623) if assignment clobbers an existing variable,\n    (False, None) otherwise.", "docstring_tokens": ["Checks", "if", "an", "assignment", "node", "in", "an", "except", "handler", "clobbers", "an", "existing", "variable", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/utils.py#L240-L262", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/nose/plugins/multiprocess.py", "func_name": "MultiProcess.options", "original_string": "def options(self, parser, env):\n        \"\"\"\n        Register command-line options.\n        \"\"\"\n        parser.add_option(\"--processes\", action=\"store\",\n                          default=env.get('NOSE_PROCESSES', 0),\n                          dest=\"multiprocess_workers\",\n                          metavar=\"NUM\",\n                          help=\"Spread test run among this many processes. \"\n                          \"Set a number equal to the number of processors \"\n                          \"or cores in your machine for best results. \"\n                          \"[NOSE_PROCESSES]\")\n        parser.add_option(\"--process-timeout\", action=\"store\",\n                          default=env.get('NOSE_PROCESS_TIMEOUT', 10),\n                          dest=\"multiprocess_timeout\",\n                          metavar=\"SECONDS\",\n                          help=\"Set timeout for return of results from each \"\n                          \"test runner process. [NOSE_PROCESS_TIMEOUT]\")\n        parser.add_option(\"--process-restartworker\", action=\"store_true\",\n                          default=env.get('NOSE_PROCESS_RESTARTWORKER', False),\n                          dest=\"multiprocess_restartworker\",\n                          help=\"If set, will restart each worker process once\"\n                          \" their tests are done, this helps control memory \"\n                          \"leaks from killing the system. \"\n                          \"[NOSE_PROCESS_RESTARTWORKER]\")", "language": "python", "code": "def options(self, parser, env):\n        \"\"\"\n        Register command-line options.\n        \"\"\"\n        parser.add_option(\"--processes\", action=\"store\",\n                          default=env.get('NOSE_PROCESSES', 0),\n                          dest=\"multiprocess_workers\",\n                          metavar=\"NUM\",\n                          help=\"Spread test run among this many processes. \"\n                          \"Set a number equal to the number of processors \"\n                          \"or cores in your machine for best results. \"\n                          \"[NOSE_PROCESSES]\")\n        parser.add_option(\"--process-timeout\", action=\"store\",\n                          default=env.get('NOSE_PROCESS_TIMEOUT', 10),\n                          dest=\"multiprocess_timeout\",\n                          metavar=\"SECONDS\",\n                          help=\"Set timeout for return of results from each \"\n                          \"test runner process. [NOSE_PROCESS_TIMEOUT]\")\n        parser.add_option(\"--process-restartworker\", action=\"store_true\",\n                          default=env.get('NOSE_PROCESS_RESTARTWORKER', False),\n                          dest=\"multiprocess_restartworker\",\n                          help=\"If set, will restart each worker process once\"\n                          \" their tests are done, this helps control memory \"\n                          \"leaks from killing the system. \"\n                          \"[NOSE_PROCESS_RESTARTWORKER]\")", "code_tokens": ["def", "options", "(", "self", ",", "parser", ",", "env", ")", ":", "parser", ".", "add_option", "(", "\"--processes\"", ",", "action", "=", "\"store\"", ",", "default", "=", "env", ".", "get", "(", "'NOSE_PROCESSES'", ",", "0", ")", ",", "dest", "=", "\"multiprocess_workers\"", ",", "metavar", "=", "\"NUM\"", ",", "help", "=", "\"Spread test run among this many processes. \"", "\"Set a number equal to the number of processors \"", "\"or cores in your machine for best results. \"", "\"[NOSE_PROCESSES]\"", ")", "parser", ".", "add_option", "(", "\"--process-timeout\"", ",", "action", "=", "\"store\"", ",", "default", "=", "env", ".", "get", "(", "'NOSE_PROCESS_TIMEOUT'", ",", "10", ")", ",", "dest", "=", "\"multiprocess_timeout\"", ",", "metavar", "=", "\"SECONDS\"", ",", "help", "=", "\"Set timeout for return of results from each \"", "\"test runner process. [NOSE_PROCESS_TIMEOUT]\"", ")", "parser", ".", "add_option", "(", "\"--process-restartworker\"", ",", "action", "=", "\"store_true\"", ",", "default", "=", "env", ".", "get", "(", "'NOSE_PROCESS_RESTARTWORKER'", ",", "False", ")", ",", "dest", "=", "\"multiprocess_restartworker\"", ",", "help", "=", "\"If set, will restart each worker process once\"", "\" their tests are done, this helps control memory \"", "\"leaks from killing the system. \"", "\"[NOSE_PROCESS_RESTARTWORKER]\"", ")"], "docstring": "Register command-line options.", "docstring_tokens": ["Register", "command", "-", "line", "options", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/nose/plugins/multiprocess.py#L187-L211", "partition": "test"}
{"repo": "iotile/typedargs", "path": "typedargs/typeinfo.py", "func_name": "TypeSystem._validate_type", "original_string": "def _validate_type(cls, typeobj):\n        \"\"\"\n        Validate that all required type methods are implemented.\n\n        At minimum a type must have:\n        - a convert() or convert_binary() function\n        - a default_formatter() function\n\n        Raises an ArgumentError if the type is not valid\n        \"\"\"\n\n        if not (hasattr(typeobj, \"convert\") or hasattr(typeobj, \"convert_binary\")):\n            raise ArgumentError(\"type is invalid, does not have convert or convert_binary function\", type=typeobj, methods=dir(typeobj))\n\n        if not hasattr(typeobj, \"default_formatter\"):\n            raise ArgumentError(\"type is invalid, does not have default_formatter function\", type=typeobj, methods=dir(typeobj))", "language": "python", "code": "def _validate_type(cls, typeobj):\n        \"\"\"\n        Validate that all required type methods are implemented.\n\n        At minimum a type must have:\n        - a convert() or convert_binary() function\n        - a default_formatter() function\n\n        Raises an ArgumentError if the type is not valid\n        \"\"\"\n\n        if not (hasattr(typeobj, \"convert\") or hasattr(typeobj, \"convert_binary\")):\n            raise ArgumentError(\"type is invalid, does not have convert or convert_binary function\", type=typeobj, methods=dir(typeobj))\n\n        if not hasattr(typeobj, \"default_formatter\"):\n            raise ArgumentError(\"type is invalid, does not have default_formatter function\", type=typeobj, methods=dir(typeobj))", "code_tokens": ["def", "_validate_type", "(", "cls", ",", "typeobj", ")", ":", "if", "not", "(", "hasattr", "(", "typeobj", ",", "\"convert\"", ")", "or", "hasattr", "(", "typeobj", ",", "\"convert_binary\"", ")", ")", ":", "raise", "ArgumentError", "(", "\"type is invalid, does not have convert or convert_binary function\"", ",", "type", "=", "typeobj", ",", "methods", "=", "dir", "(", "typeobj", ")", ")", "if", "not", "hasattr", "(", "typeobj", ",", "\"default_formatter\"", ")", ":", "raise", "ArgumentError", "(", "\"type is invalid, does not have default_formatter function\"", ",", "type", "=", "typeobj", ",", "methods", "=", "dir", "(", "typeobj", ")", ")"], "docstring": "Validate that all required type methods are implemented.\n\n        At minimum a type must have:\n        - a convert() or convert_binary() function\n        - a default_formatter() function\n\n        Raises an ArgumentError if the type is not valid", "docstring_tokens": ["Validate", "that", "all", "required", "type", "methods", "are", "implemented", "."], "sha": "0a5091a664b9b4d836e091e9ba583e944f438fd8", "url": "https://github.com/iotile/typedargs/blob/0a5091a664b9b4d836e091e9ba583e944f438fd8/typedargs/typeinfo.py#L157-L172", "partition": "test"}
{"repo": "Microsoft/botbuilder-python", "path": "libraries/botbuilder-azure/botbuilder/azure/cosmosdb_storage.py", "func_name": "CosmosDbStorage.__create_si", "original_string": "def __create_si(self, result) -> StoreItem:\n        \"\"\"Create a StoreItem from a result out of CosmosDB.\n\n        :param result:\n        :return StoreItem:\n        \"\"\"\n        # get the document item from the result and turn into a dict\n        doc = result.get('document')\n        # readd the e_tag from Cosmos\n        doc['e_tag'] = result.get('_etag')\n        # create and return the StoreItem\n        return StoreItem(**doc)", "language": "python", "code": "def __create_si(self, result) -> StoreItem:\n        \"\"\"Create a StoreItem from a result out of CosmosDB.\n\n        :param result:\n        :return StoreItem:\n        \"\"\"\n        # get the document item from the result and turn into a dict\n        doc = result.get('document')\n        # readd the e_tag from Cosmos\n        doc['e_tag'] = result.get('_etag')\n        # create and return the StoreItem\n        return StoreItem(**doc)", "code_tokens": ["def", "__create_si", "(", "self", ",", "result", ")", "->", "StoreItem", ":", "# get the document item from the result and turn into a dict", "doc", "=", "result", ".", "get", "(", "'document'", ")", "# readd the e_tag from Cosmos", "doc", "[", "'e_tag'", "]", "=", "result", ".", "get", "(", "'_etag'", ")", "# create and return the StoreItem", "return", "StoreItem", "(", "*", "*", "doc", ")"], "docstring": "Create a StoreItem from a result out of CosmosDB.\n\n        :param result:\n        :return StoreItem:", "docstring_tokens": ["Create", "a", "StoreItem", "from", "a", "result", "out", "of", "CosmosDB", "."], "sha": "274663dd91c811bae6ac4488915ba5880771b0a7", "url": "https://github.com/Microsoft/botbuilder-python/blob/274663dd91c811bae6ac4488915ba5880771b0a7/libraries/botbuilder-azure/botbuilder/azure/cosmosdb_storage.py#L163-L174", "partition": "test"}
{"repo": "ndrlslz/ternya", "path": "ternya/modules.py", "func_name": "ServiceModules.import_modules", "original_string": "def import_modules(self):\n        \"\"\"Import customer's service module.\"\"\"\n        modules = self.get_modules()\n        log.info(\"import service modules: \" + str(modules))\n        try:\n            for module in modules:\n                __import__(module)\n        except ImportError as error:\n            raise ImportModulesError(error.msg)", "language": "python", "code": "def import_modules(self):\n        \"\"\"Import customer's service module.\"\"\"\n        modules = self.get_modules()\n        log.info(\"import service modules: \" + str(modules))\n        try:\n            for module in modules:\n                __import__(module)\n        except ImportError as error:\n            raise ImportModulesError(error.msg)", "code_tokens": ["def", "import_modules", "(", "self", ")", ":", "modules", "=", "self", ".", "get_modules", "(", ")", "log", ".", "info", "(", "\"import service modules: \"", "+", "str", "(", "modules", ")", ")", "try", ":", "for", "module", "in", "modules", ":", "__import__", "(", "module", ")", "except", "ImportError", "as", "error", ":", "raise", "ImportModulesError", "(", "error", ".", "msg", ")"], "docstring": "Import customer's service module.", "docstring_tokens": ["Import", "customer", "s", "service", "module", "."], "sha": "c05aec10029e645d63ff04313dbcf2644743481f", "url": "https://github.com/ndrlslz/ternya/blob/c05aec10029e645d63ff04313dbcf2644743481f/ternya/modules.py#L50-L58", "partition": "test"}
{"repo": "neurodata/ndio", "path": "ndio/remote/grute.py", "func_name": "grute.compute_invariants", "original_string": "def compute_invariants(self, graph_file, input_format,\n                           invariants=Invariants.ALL, email=None,\n                           use_threads=False, callback=None):\n        \"\"\"\n        Compute invariants from an existing GraphML file using the remote\n        grute graph services.\n\n        Arguments:\n            graph_file (str): The filename of the graphml file\n            input_format (str): One of grute.GraphFormats\n            invariants (str[]: Invariants.ALL)*: An array of grute.Invariants\n                to compute on the graph\n            email (str: self.email)*: The email to notify upon completion\n            use_threads (bool: False)*: Whether to use Python threads to run\n                computation in the background when waiting for the server to\n                return the invariants\n            callback (function: None)*: The function to run upon completion of\n                the call, if using threads. (Will not be called if use_threads\n                is set to False.)\n\n        Returns:\n            HTTP Response if use_threads is False. Otherwise, None\n\n        Raises:\n            ValueError: If the graph file does not exist, or if there are\n                issues with the passed arguments\n            RemoteDataUploadError: If there is an issue packing the file\n            RemoteError: If the server experiences difficulty computing invs\n        \"\"\"\n        if email is None:\n            email = self.email\n\n        if input_format not in GraphFormats._any:\n            raise ValueError(\"Invalid input format, {}.\".format(input_format))\n\n        if not set(invariants) <= set(Invariants.ALL):\n            raise ValueError(\"Invariants must be a subset of Invariants.ALL.\")\n\n        if use_threads and callback is not None:\n            if not hasattr(callback, '__call__'):\n                raise ValueError(\"callback must be a function.\")\n            if len(inspect.getargspec(callback).args) != 1:\n                raise ValueError(\"callback must take exactly 1 argument.\")\n\n        url = \"graphupload/{}/{}/{}/\".format(\n            email,\n            input_format,\n            \"/\".join(invariants)\n        )\n\n        if \" \" in url:\n            raise ValueError(\"Arguments cannot have spaces in them.\")\n\n        if not (os.path.exists(graph_file)):\n            raise ValueError(\"File {} does not exist.\".format(graph_file))\n\n        if use_threads:\n            # Run in the background.\n            upload_thread = threading.Thread(\n                target=self._run_compute_invariants,\n                args=[url, graph_file, callback]\n            )\n            upload_thread.start()\n\n        else:\n            # Run in the foreground.\n            return self._run_compute_invariants(url, graph_file)\n        return", "language": "python", "code": "def compute_invariants(self, graph_file, input_format,\n                           invariants=Invariants.ALL, email=None,\n                           use_threads=False, callback=None):\n        \"\"\"\n        Compute invariants from an existing GraphML file using the remote\n        grute graph services.\n\n        Arguments:\n            graph_file (str): The filename of the graphml file\n            input_format (str): One of grute.GraphFormats\n            invariants (str[]: Invariants.ALL)*: An array of grute.Invariants\n                to compute on the graph\n            email (str: self.email)*: The email to notify upon completion\n            use_threads (bool: False)*: Whether to use Python threads to run\n                computation in the background when waiting for the server to\n                return the invariants\n            callback (function: None)*: The function to run upon completion of\n                the call, if using threads. (Will not be called if use_threads\n                is set to False.)\n\n        Returns:\n            HTTP Response if use_threads is False. Otherwise, None\n\n        Raises:\n            ValueError: If the graph file does not exist, or if there are\n                issues with the passed arguments\n            RemoteDataUploadError: If there is an issue packing the file\n            RemoteError: If the server experiences difficulty computing invs\n        \"\"\"\n        if email is None:\n            email = self.email\n\n        if input_format not in GraphFormats._any:\n            raise ValueError(\"Invalid input format, {}.\".format(input_format))\n\n        if not set(invariants) <= set(Invariants.ALL):\n            raise ValueError(\"Invariants must be a subset of Invariants.ALL.\")\n\n        if use_threads and callback is not None:\n            if not hasattr(callback, '__call__'):\n                raise ValueError(\"callback must be a function.\")\n            if len(inspect.getargspec(callback).args) != 1:\n                raise ValueError(\"callback must take exactly 1 argument.\")\n\n        url = \"graphupload/{}/{}/{}/\".format(\n            email,\n            input_format,\n            \"/\".join(invariants)\n        )\n\n        if \" \" in url:\n            raise ValueError(\"Arguments cannot have spaces in them.\")\n\n        if not (os.path.exists(graph_file)):\n            raise ValueError(\"File {} does not exist.\".format(graph_file))\n\n        if use_threads:\n            # Run in the background.\n            upload_thread = threading.Thread(\n                target=self._run_compute_invariants,\n                args=[url, graph_file, callback]\n            )\n            upload_thread.start()\n\n        else:\n            # Run in the foreground.\n            return self._run_compute_invariants(url, graph_file)\n        return", "code_tokens": ["def", "compute_invariants", "(", "self", ",", "graph_file", ",", "input_format", ",", "invariants", "=", "Invariants", ".", "ALL", ",", "email", "=", "None", ",", "use_threads", "=", "False", ",", "callback", "=", "None", ")", ":", "if", "email", "is", "None", ":", "email", "=", "self", ".", "email", "if", "input_format", "not", "in", "GraphFormats", ".", "_any", ":", "raise", "ValueError", "(", "\"Invalid input format, {}.\"", ".", "format", "(", "input_format", ")", ")", "if", "not", "set", "(", "invariants", ")", "<=", "set", "(", "Invariants", ".", "ALL", ")", ":", "raise", "ValueError", "(", "\"Invariants must be a subset of Invariants.ALL.\"", ")", "if", "use_threads", "and", "callback", "is", "not", "None", ":", "if", "not", "hasattr", "(", "callback", ",", "'__call__'", ")", ":", "raise", "ValueError", "(", "\"callback must be a function.\"", ")", "if", "len", "(", "inspect", ".", "getargspec", "(", "callback", ")", ".", "args", ")", "!=", "1", ":", "raise", "ValueError", "(", "\"callback must take exactly 1 argument.\"", ")", "url", "=", "\"graphupload/{}/{}/{}/\"", ".", "format", "(", "email", ",", "input_format", ",", "\"/\"", ".", "join", "(", "invariants", ")", ")", "if", "\" \"", "in", "url", ":", "raise", "ValueError", "(", "\"Arguments cannot have spaces in them.\"", ")", "if", "not", "(", "os", ".", "path", ".", "exists", "(", "graph_file", ")", ")", ":", "raise", "ValueError", "(", "\"File {} does not exist.\"", ".", "format", "(", "graph_file", ")", ")", "if", "use_threads", ":", "# Run in the background.", "upload_thread", "=", "threading", ".", "Thread", "(", "target", "=", "self", ".", "_run_compute_invariants", ",", "args", "=", "[", "url", ",", "graph_file", ",", "callback", "]", ")", "upload_thread", ".", "start", "(", ")", "else", ":", "# Run in the foreground.", "return", "self", ".", "_run_compute_invariants", "(", "url", ",", "graph_file", ")", "return"], "docstring": "Compute invariants from an existing GraphML file using the remote\n        grute graph services.\n\n        Arguments:\n            graph_file (str): The filename of the graphml file\n            input_format (str): One of grute.GraphFormats\n            invariants (str[]: Invariants.ALL)*: An array of grute.Invariants\n                to compute on the graph\n            email (str: self.email)*: The email to notify upon completion\n            use_threads (bool: False)*: Whether to use Python threads to run\n                computation in the background when waiting for the server to\n                return the invariants\n            callback (function: None)*: The function to run upon completion of\n                the call, if using threads. (Will not be called if use_threads\n                is set to False.)\n\n        Returns:\n            HTTP Response if use_threads is False. Otherwise, None\n\n        Raises:\n            ValueError: If the graph file does not exist, or if there are\n                issues with the passed arguments\n            RemoteDataUploadError: If there is an issue packing the file\n            RemoteError: If the server experiences difficulty computing invs", "docstring_tokens": ["Compute", "invariants", "from", "an", "existing", "GraphML", "file", "using", "the", "remote", "grute", "graph", "services", "."], "sha": "792dd5816bc770b05a3db2f4327da42ff6253531", "url": "https://github.com/neurodata/ndio/blob/792dd5816bc770b05a3db2f4327da42ff6253531/ndio/remote/grute.py#L262-L329", "partition": "test"}
{"repo": "instagrambot/instabot", "path": "instabot/bot/bot_like.py", "func_name": "like_user", "original_string": "def like_user(self, user_id, amount=None, filtration=True):\n    \"\"\" Likes last user_id's medias \"\"\"\n    if filtration:\n        if not self.check_user(user_id):\n            return False\n    self.logger.info(\"Liking user_%s's feed:\" % user_id)\n    user_id = self.convert_to_user_id(user_id)\n    medias = self.get_user_medias(user_id, filtration=filtration)\n    if not medias:\n        self.logger.info(\n            \"None medias received: account is closed or medias have been filtered.\")\n        return False\n    return self.like_medias(medias[:amount])", "language": "python", "code": "def like_user(self, user_id, amount=None, filtration=True):\n    \"\"\" Likes last user_id's medias \"\"\"\n    if filtration:\n        if not self.check_user(user_id):\n            return False\n    self.logger.info(\"Liking user_%s's feed:\" % user_id)\n    user_id = self.convert_to_user_id(user_id)\n    medias = self.get_user_medias(user_id, filtration=filtration)\n    if not medias:\n        self.logger.info(\n            \"None medias received: account is closed or medias have been filtered.\")\n        return False\n    return self.like_medias(medias[:amount])", "code_tokens": ["def", "like_user", "(", "self", ",", "user_id", ",", "amount", "=", "None", ",", "filtration", "=", "True", ")", ":", "if", "filtration", ":", "if", "not", "self", ".", "check_user", "(", "user_id", ")", ":", "return", "False", "self", ".", "logger", ".", "info", "(", "\"Liking user_%s's feed:\"", "%", "user_id", ")", "user_id", "=", "self", ".", "convert_to_user_id", "(", "user_id", ")", "medias", "=", "self", ".", "get_user_medias", "(", "user_id", ",", "filtration", "=", "filtration", ")", "if", "not", "medias", ":", "self", ".", "logger", ".", "info", "(", "\"None medias received: account is closed or medias have been filtered.\"", ")", "return", "False", "return", "self", ".", "like_medias", "(", "medias", "[", ":", "amount", "]", ")"], "docstring": "Likes last user_id's medias", "docstring_tokens": ["Likes", "last", "user_id", "s", "medias"], "sha": "d734f892ac4cc35d22746a4f2680425ffaff0927", "url": "https://github.com/instagrambot/instabot/blob/d734f892ac4cc35d22746a4f2680425ffaff0927/instabot/bot/bot_like.py#L94-L106", "partition": "test"}
{"repo": "IdentityPython/fedoidcmsg", "path": "src/fedoidcmsg/utils.py", "func_name": "self_sign_jwks", "original_string": "def self_sign_jwks(keyjar, iss, kid='', lifetime=3600):\n    \"\"\"\n    Create a signed JWT containing a JWKS. The JWT is signed by one of the\n    keys in the JWKS.\n\n    :param keyjar: A KeyJar instance with at least one private signing key\n    :param iss: issuer of the JWT, should be the owner of the keys\n    :param kid: A key ID if a special key should be used otherwise one\n        is picked at random.\n    :param lifetime: The lifetime of the signed JWT\n    :return: A signed JWT\n    \"\"\"\n\n    # _json = json.dumps(jwks)\n    _jwt = JWT(keyjar, iss=iss, lifetime=lifetime)\n\n    jwks = keyjar.export_jwks(issuer=iss)\n\n    return _jwt.pack(payload={'jwks': jwks}, owner=iss, kid=kid)", "language": "python", "code": "def self_sign_jwks(keyjar, iss, kid='', lifetime=3600):\n    \"\"\"\n    Create a signed JWT containing a JWKS. The JWT is signed by one of the\n    keys in the JWKS.\n\n    :param keyjar: A KeyJar instance with at least one private signing key\n    :param iss: issuer of the JWT, should be the owner of the keys\n    :param kid: A key ID if a special key should be used otherwise one\n        is picked at random.\n    :param lifetime: The lifetime of the signed JWT\n    :return: A signed JWT\n    \"\"\"\n\n    # _json = json.dumps(jwks)\n    _jwt = JWT(keyjar, iss=iss, lifetime=lifetime)\n\n    jwks = keyjar.export_jwks(issuer=iss)\n\n    return _jwt.pack(payload={'jwks': jwks}, owner=iss, kid=kid)", "code_tokens": ["def", "self_sign_jwks", "(", "keyjar", ",", "iss", ",", "kid", "=", "''", ",", "lifetime", "=", "3600", ")", ":", "# _json = json.dumps(jwks)", "_jwt", "=", "JWT", "(", "keyjar", ",", "iss", "=", "iss", ",", "lifetime", "=", "lifetime", ")", "jwks", "=", "keyjar", ".", "export_jwks", "(", "issuer", "=", "iss", ")", "return", "_jwt", ".", "pack", "(", "payload", "=", "{", "'jwks'", ":", "jwks", "}", ",", "owner", "=", "iss", ",", "kid", "=", "kid", ")"], "docstring": "Create a signed JWT containing a JWKS. The JWT is signed by one of the\n    keys in the JWKS.\n\n    :param keyjar: A KeyJar instance with at least one private signing key\n    :param iss: issuer of the JWT, should be the owner of the keys\n    :param kid: A key ID if a special key should be used otherwise one\n        is picked at random.\n    :param lifetime: The lifetime of the signed JWT\n    :return: A signed JWT", "docstring_tokens": ["Create", "a", "signed", "JWT", "containing", "a", "JWKS", ".", "The", "JWT", "is", "signed", "by", "one", "of", "the", "keys", "in", "the", "JWKS", "."], "sha": "d30107be02521fa6cdfe285da3b6b0cdd153c8cc", "url": "https://github.com/IdentityPython/fedoidcmsg/blob/d30107be02521fa6cdfe285da3b6b0cdd153c8cc/src/fedoidcmsg/utils.py#L15-L33", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/spark_sql_hook.py", "func_name": "SparkSqlHook._prepare_command", "original_string": "def _prepare_command(self, cmd):\n        \"\"\"\n        Construct the spark-sql command to execute. Verbose output is enabled\n        as default.\n\n        :param cmd: command to append to the spark-sql command\n        :type cmd: str\n        :return: full command to be executed\n        \"\"\"\n        connection_cmd = [\"spark-sql\"]\n        if self._conf:\n            for conf_el in self._conf.split(\",\"):\n                connection_cmd += [\"--conf\", conf_el]\n        if self._total_executor_cores:\n            connection_cmd += [\"--total-executor-cores\", str(self._total_executor_cores)]\n        if self._executor_cores:\n            connection_cmd += [\"--executor-cores\", str(self._executor_cores)]\n        if self._executor_memory:\n            connection_cmd += [\"--executor-memory\", self._executor_memory]\n        if self._keytab:\n            connection_cmd += [\"--keytab\", self._keytab]\n        if self._principal:\n            connection_cmd += [\"--principal\", self._principal]\n        if self._num_executors:\n            connection_cmd += [\"--num-executors\", str(self._num_executors)]\n        if self._sql:\n            sql = self._sql.strip()\n            if sql.endswith(\".sql\") or sql.endswith(\".hql\"):\n                connection_cmd += [\"-f\", sql]\n            else:\n                connection_cmd += [\"-e\", sql]\n        if self._master:\n            connection_cmd += [\"--master\", self._master]\n        if self._name:\n            connection_cmd += [\"--name\", self._name]\n        if self._verbose:\n            connection_cmd += [\"--verbose\"]\n        if self._yarn_queue:\n            connection_cmd += [\"--queue\", self._yarn_queue]\n\n        connection_cmd += cmd\n        self.log.debug(\"Spark-Sql cmd: %s\", connection_cmd)\n\n        return connection_cmd", "language": "python", "code": "def _prepare_command(self, cmd):\n        \"\"\"\n        Construct the spark-sql command to execute. Verbose output is enabled\n        as default.\n\n        :param cmd: command to append to the spark-sql command\n        :type cmd: str\n        :return: full command to be executed\n        \"\"\"\n        connection_cmd = [\"spark-sql\"]\n        if self._conf:\n            for conf_el in self._conf.split(\",\"):\n                connection_cmd += [\"--conf\", conf_el]\n        if self._total_executor_cores:\n            connection_cmd += [\"--total-executor-cores\", str(self._total_executor_cores)]\n        if self._executor_cores:\n            connection_cmd += [\"--executor-cores\", str(self._executor_cores)]\n        if self._executor_memory:\n            connection_cmd += [\"--executor-memory\", self._executor_memory]\n        if self._keytab:\n            connection_cmd += [\"--keytab\", self._keytab]\n        if self._principal:\n            connection_cmd += [\"--principal\", self._principal]\n        if self._num_executors:\n            connection_cmd += [\"--num-executors\", str(self._num_executors)]\n        if self._sql:\n            sql = self._sql.strip()\n            if sql.endswith(\".sql\") or sql.endswith(\".hql\"):\n                connection_cmd += [\"-f\", sql]\n            else:\n                connection_cmd += [\"-e\", sql]\n        if self._master:\n            connection_cmd += [\"--master\", self._master]\n        if self._name:\n            connection_cmd += [\"--name\", self._name]\n        if self._verbose:\n            connection_cmd += [\"--verbose\"]\n        if self._yarn_queue:\n            connection_cmd += [\"--queue\", self._yarn_queue]\n\n        connection_cmd += cmd\n        self.log.debug(\"Spark-Sql cmd: %s\", connection_cmd)\n\n        return connection_cmd", "code_tokens": ["def", "_prepare_command", "(", "self", ",", "cmd", ")", ":", "connection_cmd", "=", "[", "\"spark-sql\"", "]", "if", "self", ".", "_conf", ":", "for", "conf_el", "in", "self", ".", "_conf", ".", "split", "(", "\",\"", ")", ":", "connection_cmd", "+=", "[", "\"--conf\"", ",", "conf_el", "]", "if", "self", ".", "_total_executor_cores", ":", "connection_cmd", "+=", "[", "\"--total-executor-cores\"", ",", "str", "(", "self", ".", "_total_executor_cores", ")", "]", "if", "self", ".", "_executor_cores", ":", "connection_cmd", "+=", "[", "\"--executor-cores\"", ",", "str", "(", "self", ".", "_executor_cores", ")", "]", "if", "self", ".", "_executor_memory", ":", "connection_cmd", "+=", "[", "\"--executor-memory\"", ",", "self", ".", "_executor_memory", "]", "if", "self", ".", "_keytab", ":", "connection_cmd", "+=", "[", "\"--keytab\"", ",", "self", ".", "_keytab", "]", "if", "self", ".", "_principal", ":", "connection_cmd", "+=", "[", "\"--principal\"", ",", "self", ".", "_principal", "]", "if", "self", ".", "_num_executors", ":", "connection_cmd", "+=", "[", "\"--num-executors\"", ",", "str", "(", "self", ".", "_num_executors", ")", "]", "if", "self", ".", "_sql", ":", "sql", "=", "self", ".", "_sql", ".", "strip", "(", ")", "if", "sql", ".", "endswith", "(", "\".sql\"", ")", "or", "sql", ".", "endswith", "(", "\".hql\"", ")", ":", "connection_cmd", "+=", "[", "\"-f\"", ",", "sql", "]", "else", ":", "connection_cmd", "+=", "[", "\"-e\"", ",", "sql", "]", "if", "self", ".", "_master", ":", "connection_cmd", "+=", "[", "\"--master\"", ",", "self", ".", "_master", "]", "if", "self", ".", "_name", ":", "connection_cmd", "+=", "[", "\"--name\"", ",", "self", ".", "_name", "]", "if", "self", ".", "_verbose", ":", "connection_cmd", "+=", "[", "\"--verbose\"", "]", "if", "self", ".", "_yarn_queue", ":", "connection_cmd", "+=", "[", "\"--queue\"", ",", "self", ".", "_yarn_queue", "]", "connection_cmd", "+=", "cmd", "self", ".", "log", ".", "debug", "(", "\"Spark-Sql cmd: %s\"", ",", "connection_cmd", ")", "return", "connection_cmd"], "docstring": "Construct the spark-sql command to execute. Verbose output is enabled\n        as default.\n\n        :param cmd: command to append to the spark-sql command\n        :type cmd: str\n        :return: full command to be executed", "docstring_tokens": ["Construct", "the", "spark", "-", "sql", "command", "to", "execute", ".", "Verbose", "output", "is", "enabled", "as", "default", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/spark_sql_hook.py#L91-L134", "partition": "test"}
{"repo": "aholkner/bacon", "path": "native/Vendor/FreeType/src/tools/docmaker/tohtml.py", "func_name": "HtmlFormatter.make_html_word", "original_string": "def  make_html_word( self, word ):\n        \"\"\"analyze a simple word to detect cross-references and styling\"\"\"\n        # look for cross-references\n        m = re_crossref.match( word )\n        if m:\n            try:\n                name = m.group( 1 )\n                rest = m.group( 2 )\n                block = self.identifiers[name]\n                url   = self.make_block_url( block )\n                return '<a href=\"' + url + '\">' + name + '</a>' + rest\n            except:\n                # we detected a cross-reference to an unknown item\n                sys.stderr.write( \\\n                   \"WARNING: undefined cross reference '\" + name + \"'.\\n\" )\n                return '?' + name + '?' + rest\n\n        # look for italics and bolds\n        m = re_italic.match( word )\n        if m:\n            name = m.group( 1 )\n            rest = m.group( 3 )\n            return '<i>' + name + '</i>' + rest\n\n        m = re_bold.match( word )\n        if m:\n            name = m.group( 1 )\n            rest = m.group( 3 )\n            return '<b>' + name + '</b>' + rest\n\n        return html_quote( word )", "language": "python", "code": "def  make_html_word( self, word ):\n        \"\"\"analyze a simple word to detect cross-references and styling\"\"\"\n        # look for cross-references\n        m = re_crossref.match( word )\n        if m:\n            try:\n                name = m.group( 1 )\n                rest = m.group( 2 )\n                block = self.identifiers[name]\n                url   = self.make_block_url( block )\n                return '<a href=\"' + url + '\">' + name + '</a>' + rest\n            except:\n                # we detected a cross-reference to an unknown item\n                sys.stderr.write( \\\n                   \"WARNING: undefined cross reference '\" + name + \"'.\\n\" )\n                return '?' + name + '?' + rest\n\n        # look for italics and bolds\n        m = re_italic.match( word )\n        if m:\n            name = m.group( 1 )\n            rest = m.group( 3 )\n            return '<i>' + name + '</i>' + rest\n\n        m = re_bold.match( word )\n        if m:\n            name = m.group( 1 )\n            rest = m.group( 3 )\n            return '<b>' + name + '</b>' + rest\n\n        return html_quote( word )", "code_tokens": ["def", "make_html_word", "(", "self", ",", "word", ")", ":", "# look for cross-references", "m", "=", "re_crossref", ".", "match", "(", "word", ")", "if", "m", ":", "try", ":", "name", "=", "m", ".", "group", "(", "1", ")", "rest", "=", "m", ".", "group", "(", "2", ")", "block", "=", "self", ".", "identifiers", "[", "name", "]", "url", "=", "self", ".", "make_block_url", "(", "block", ")", "return", "'<a href=\"'", "+", "url", "+", "'\">'", "+", "name", "+", "'</a>'", "+", "rest", "except", ":", "# we detected a cross-reference to an unknown item", "sys", ".", "stderr", ".", "write", "(", "\"WARNING: undefined cross reference '\"", "+", "name", "+", "\"'.\\n\"", ")", "return", "'?'", "+", "name", "+", "'?'", "+", "rest", "# look for italics and bolds", "m", "=", "re_italic", ".", "match", "(", "word", ")", "if", "m", ":", "name", "=", "m", ".", "group", "(", "1", ")", "rest", "=", "m", ".", "group", "(", "3", ")", "return", "'<i>'", "+", "name", "+", "'</i>'", "+", "rest", "m", "=", "re_bold", ".", "match", "(", "word", ")", "if", "m", ":", "name", "=", "m", ".", "group", "(", "1", ")", "rest", "=", "m", ".", "group", "(", "3", ")", "return", "'<b>'", "+", "name", "+", "'</b>'", "+", "rest", "return", "html_quote", "(", "word", ")"], "docstring": "analyze a simple word to detect cross-references and styling", "docstring_tokens": ["analyze", "a", "simple", "word", "to", "detect", "cross", "-", "references", "and", "styling"], "sha": "edf3810dcb211942d392a8637945871399b0650d", "url": "https://github.com/aholkner/bacon/blob/edf3810dcb211942d392a8637945871399b0650d/native/Vendor/FreeType/src/tools/docmaker/tohtml.py#L255-L285", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/flask/templating.py", "func_name": "render_template", "original_string": "def render_template(template_name_or_list, **context):\n    \"\"\"Renders a template from the template folder with the given\n    context.\n\n    :param template_name_or_list: the name of the template to be\n                                  rendered, or an iterable with template names\n                                  the first one existing will be rendered\n    :param context: the variables that should be available in the\n                    context of the template.\n    \"\"\"\n    ctx = _app_ctx_stack.top\n    ctx.app.update_template_context(context)\n    return _render(ctx.app.jinja_env.get_or_select_template(template_name_or_list),\n                   context, ctx.app)", "language": "python", "code": "def render_template(template_name_or_list, **context):\n    \"\"\"Renders a template from the template folder with the given\n    context.\n\n    :param template_name_or_list: the name of the template to be\n                                  rendered, or an iterable with template names\n                                  the first one existing will be rendered\n    :param context: the variables that should be available in the\n                    context of the template.\n    \"\"\"\n    ctx = _app_ctx_stack.top\n    ctx.app.update_template_context(context)\n    return _render(ctx.app.jinja_env.get_or_select_template(template_name_or_list),\n                   context, ctx.app)", "code_tokens": ["def", "render_template", "(", "template_name_or_list", ",", "*", "*", "context", ")", ":", "ctx", "=", "_app_ctx_stack", ".", "top", "ctx", ".", "app", ".", "update_template_context", "(", "context", ")", "return", "_render", "(", "ctx", ".", "app", ".", "jinja_env", ".", "get_or_select_template", "(", "template_name_or_list", ")", ",", "context", ",", "ctx", ".", "app", ")"], "docstring": "Renders a template from the template folder with the given\n    context.\n\n    :param template_name_or_list: the name of the template to be\n                                  rendered, or an iterable with template names\n                                  the first one existing will be rendered\n    :param context: the variables that should be available in the\n                    context of the template.", "docstring_tokens": ["Renders", "a", "template", "from", "the", "template", "folder", "with", "the", "given", "context", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/flask/templating.py#L115-L128", "partition": "test"}
{"repo": "rollbar/pyrollbar", "path": "rollbar/__init__.py", "func_name": "_build_request_data", "original_string": "def _build_request_data(request):\n    \"\"\"\n    Returns a dictionary containing data from the request.\n    Can handle webob or werkzeug-based request objects.\n    \"\"\"\n\n    # webob (pyramid)\n    if WebobBaseRequest and isinstance(request, WebobBaseRequest):\n        return _build_webob_request_data(request)\n\n    # django\n    if DjangoHttpRequest and isinstance(request, DjangoHttpRequest):\n        return _build_django_request_data(request)\n\n    # django rest framework\n    if RestFrameworkRequest and isinstance(request, RestFrameworkRequest):\n        return _build_django_request_data(request)\n\n    # werkzeug (flask)\n    if WerkzeugRequest and isinstance(request, WerkzeugRequest):\n        return _build_werkzeug_request_data(request)\n\n    # tornado\n    if TornadoRequest and isinstance(request, TornadoRequest):\n        return _build_tornado_request_data(request)\n\n    # bottle\n    if BottleRequest and isinstance(request, BottleRequest):\n        return _build_bottle_request_data(request)\n\n    # Sanic\n    if SanicRequest and isinstance(request, SanicRequest):\n        return _build_sanic_request_data(request)\n\n    # falcon\n    if FalconRequest and isinstance(request, FalconRequest):\n        return _build_falcon_request_data(request)\n\n    # Plain wsgi (should be last)\n    if isinstance(request, dict) and 'wsgi.version' in request:\n        return _build_wsgi_request_data(request)\n\n    return None", "language": "python", "code": "def _build_request_data(request):\n    \"\"\"\n    Returns a dictionary containing data from the request.\n    Can handle webob or werkzeug-based request objects.\n    \"\"\"\n\n    # webob (pyramid)\n    if WebobBaseRequest and isinstance(request, WebobBaseRequest):\n        return _build_webob_request_data(request)\n\n    # django\n    if DjangoHttpRequest and isinstance(request, DjangoHttpRequest):\n        return _build_django_request_data(request)\n\n    # django rest framework\n    if RestFrameworkRequest and isinstance(request, RestFrameworkRequest):\n        return _build_django_request_data(request)\n\n    # werkzeug (flask)\n    if WerkzeugRequest and isinstance(request, WerkzeugRequest):\n        return _build_werkzeug_request_data(request)\n\n    # tornado\n    if TornadoRequest and isinstance(request, TornadoRequest):\n        return _build_tornado_request_data(request)\n\n    # bottle\n    if BottleRequest and isinstance(request, BottleRequest):\n        return _build_bottle_request_data(request)\n\n    # Sanic\n    if SanicRequest and isinstance(request, SanicRequest):\n        return _build_sanic_request_data(request)\n\n    # falcon\n    if FalconRequest and isinstance(request, FalconRequest):\n        return _build_falcon_request_data(request)\n\n    # Plain wsgi (should be last)\n    if isinstance(request, dict) and 'wsgi.version' in request:\n        return _build_wsgi_request_data(request)\n\n    return None", "code_tokens": ["def", "_build_request_data", "(", "request", ")", ":", "# webob (pyramid)", "if", "WebobBaseRequest", "and", "isinstance", "(", "request", ",", "WebobBaseRequest", ")", ":", "return", "_build_webob_request_data", "(", "request", ")", "# django", "if", "DjangoHttpRequest", "and", "isinstance", "(", "request", ",", "DjangoHttpRequest", ")", ":", "return", "_build_django_request_data", "(", "request", ")", "# django rest framework", "if", "RestFrameworkRequest", "and", "isinstance", "(", "request", ",", "RestFrameworkRequest", ")", ":", "return", "_build_django_request_data", "(", "request", ")", "# werkzeug (flask)", "if", "WerkzeugRequest", "and", "isinstance", "(", "request", ",", "WerkzeugRequest", ")", ":", "return", "_build_werkzeug_request_data", "(", "request", ")", "# tornado", "if", "TornadoRequest", "and", "isinstance", "(", "request", ",", "TornadoRequest", ")", ":", "return", "_build_tornado_request_data", "(", "request", ")", "# bottle", "if", "BottleRequest", "and", "isinstance", "(", "request", ",", "BottleRequest", ")", ":", "return", "_build_bottle_request_data", "(", "request", ")", "# Sanic", "if", "SanicRequest", "and", "isinstance", "(", "request", ",", "SanicRequest", ")", ":", "return", "_build_sanic_request_data", "(", "request", ")", "# falcon", "if", "FalconRequest", "and", "isinstance", "(", "request", ",", "FalconRequest", ")", ":", "return", "_build_falcon_request_data", "(", "request", ")", "# Plain wsgi (should be last)", "if", "isinstance", "(", "request", ",", "dict", ")", "and", "'wsgi.version'", "in", "request", ":", "return", "_build_wsgi_request_data", "(", "request", ")", "return", "None"], "docstring": "Returns a dictionary containing data from the request.\n    Can handle webob or werkzeug-based request objects.", "docstring_tokens": ["Returns", "a", "dictionary", "containing", "data", "from", "the", "request", ".", "Can", "handle", "webob", "or", "werkzeug", "-", "based", "request", "objects", "."], "sha": "33ef2e723a33d09dd6302f978f4a3908be95b9d2", "url": "https://github.com/rollbar/pyrollbar/blob/33ef2e723a33d09dd6302f978f4a3908be95b9d2/rollbar/__init__.py#L1049-L1091", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/lib/pretty.py", "func_name": "PrettyPrinter.text", "original_string": "def text(self, obj):\n        \"\"\"Add literal text to the output.\"\"\"\n        width = len(obj)\n        if self.buffer:\n            text = self.buffer[-1]\n            if not isinstance(text, Text):\n                text = Text()\n                self.buffer.append(text)\n            text.add(obj, width)\n            self.buffer_width += width\n            self._break_outer_groups()\n        else:\n            self.output.write(obj)\n            self.output_width += width", "language": "python", "code": "def text(self, obj):\n        \"\"\"Add literal text to the output.\"\"\"\n        width = len(obj)\n        if self.buffer:\n            text = self.buffer[-1]\n            if not isinstance(text, Text):\n                text = Text()\n                self.buffer.append(text)\n            text.add(obj, width)\n            self.buffer_width += width\n            self._break_outer_groups()\n        else:\n            self.output.write(obj)\n            self.output_width += width", "code_tokens": ["def", "text", "(", "self", ",", "obj", ")", ":", "width", "=", "len", "(", "obj", ")", "if", "self", ".", "buffer", ":", "text", "=", "self", ".", "buffer", "[", "-", "1", "]", "if", "not", "isinstance", "(", "text", ",", "Text", ")", ":", "text", "=", "Text", "(", ")", "self", ".", "buffer", ".", "append", "(", "text", ")", "text", ".", "add", "(", "obj", ",", "width", ")", "self", ".", "buffer_width", "+=", "width", "self", ".", "_break_outer_groups", "(", ")", "else", ":", "self", ".", "output", ".", "write", "(", "obj", ")", "self", ".", "output_width", "+=", "width"], "docstring": "Add literal text to the output.", "docstring_tokens": ["Add", "literal", "text", "to", "the", "output", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/lib/pretty.py#L197-L210", "partition": "test"}
{"repo": "urinieto/msaf", "path": "msaf/input_output.py", "func_name": "get_dataset_files", "original_string": "def get_dataset_files(in_path):\n    \"\"\"Gets the files of the given dataset.\"\"\"\n    # Get audio files\n    audio_files = []\n    for ext in ds_config.audio_exts:\n        audio_files += glob.glob(\n            os.path.join(in_path, ds_config.audio_dir, \"*\" + ext))\n\n    # Make sure directories exist\n    utils.ensure_dir(os.path.join(in_path, ds_config.features_dir))\n    utils.ensure_dir(os.path.join(in_path, ds_config.estimations_dir))\n    utils.ensure_dir(os.path.join(in_path, ds_config.references_dir))\n\n    # Get the file structs\n    file_structs = []\n    for audio_file in audio_files:\n        file_structs.append(FileStruct(audio_file))\n\n    # Sort by audio file name\n    file_structs = sorted(file_structs,\n                          key=lambda file_struct: file_struct.audio_file)\n\n    return file_structs", "language": "python", "code": "def get_dataset_files(in_path):\n    \"\"\"Gets the files of the given dataset.\"\"\"\n    # Get audio files\n    audio_files = []\n    for ext in ds_config.audio_exts:\n        audio_files += glob.glob(\n            os.path.join(in_path, ds_config.audio_dir, \"*\" + ext))\n\n    # Make sure directories exist\n    utils.ensure_dir(os.path.join(in_path, ds_config.features_dir))\n    utils.ensure_dir(os.path.join(in_path, ds_config.estimations_dir))\n    utils.ensure_dir(os.path.join(in_path, ds_config.references_dir))\n\n    # Get the file structs\n    file_structs = []\n    for audio_file in audio_files:\n        file_structs.append(FileStruct(audio_file))\n\n    # Sort by audio file name\n    file_structs = sorted(file_structs,\n                          key=lambda file_struct: file_struct.audio_file)\n\n    return file_structs", "code_tokens": ["def", "get_dataset_files", "(", "in_path", ")", ":", "# Get audio files", "audio_files", "=", "[", "]", "for", "ext", "in", "ds_config", ".", "audio_exts", ":", "audio_files", "+=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "in_path", ",", "ds_config", ".", "audio_dir", ",", "\"*\"", "+", "ext", ")", ")", "# Make sure directories exist", "utils", ".", "ensure_dir", "(", "os", ".", "path", ".", "join", "(", "in_path", ",", "ds_config", ".", "features_dir", ")", ")", "utils", ".", "ensure_dir", "(", "os", ".", "path", ".", "join", "(", "in_path", ",", "ds_config", ".", "estimations_dir", ")", ")", "utils", ".", "ensure_dir", "(", "os", ".", "path", ".", "join", "(", "in_path", ",", "ds_config", ".", "references_dir", ")", ")", "# Get the file structs", "file_structs", "=", "[", "]", "for", "audio_file", "in", "audio_files", ":", "file_structs", ".", "append", "(", "FileStruct", "(", "audio_file", ")", ")", "# Sort by audio file name", "file_structs", "=", "sorted", "(", "file_structs", ",", "key", "=", "lambda", "file_struct", ":", "file_struct", ".", "audio_file", ")", "return", "file_structs"], "docstring": "Gets the files of the given dataset.", "docstring_tokens": ["Gets", "the", "files", "of", "the", "given", "dataset", "."], "sha": "9dbb57d77a1310465a65cc40f1641d083ca74385", "url": "https://github.com/urinieto/msaf/blob/9dbb57d77a1310465a65cc40f1641d083ca74385/msaf/input_output.py#L366-L388", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", "func_name": "_line_search_after_bracketing", "original_string": "def _line_search_after_bracketing(\n    value_and_gradients_function,\n    search_interval,\n    val_0,\n    f_lim,\n    max_iterations,\n    sufficient_decrease_param,\n    curvature_param,\n    shrinkage_param):\n  \"\"\"The main loop of line search after the minimum has been bracketed.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    search_interval: Instance of `HagerZhangLineSearchResults` containing\n      the current line search interval.\n    val_0: A namedtuple as returned by value_and_gradients_function evaluated\n      at `0.`. The gradient must be negative (i.e. must be a descent direction).\n    f_lim: Scalar `Tensor` of float dtype.\n    max_iterations: Positive scalar `Tensor` of integral dtype. The maximum\n      number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in [Hager and Zhang (2006)][2].\n\n  Returns:\n    A namedtuple containing the following fields.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the updated bracketing interval.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the updated bracketing interval.\n  \"\"\"\n\n  def _loop_cond(curr_interval):\n    \"\"\"Loop condition.\"\"\"\n    active = ~(curr_interval.converged | curr_interval.failed)\n    return (curr_interval.iterations <\n            max_iterations) & tf.reduce_any(input_tensor=active)\n\n  def _loop_body(curr_interval):\n    \"\"\"The loop body.\"\"\"\n    secant2_raw_result = hzl.secant2(\n        value_and_gradients_function, val_0, curr_interval, f_lim,\n        sufficient_decrease_param, curvature_param)\n    secant2_result = HagerZhangLineSearchResult(\n        converged=secant2_raw_result.converged,\n        failed=secant2_raw_result.failed,\n        iterations=curr_interval.iterations + 1,\n        func_evals=secant2_raw_result.num_evals,\n        left=secant2_raw_result.left,\n        right=secant2_raw_result.right)\n\n    should_check_shrinkage = ~(secant2_result.converged | secant2_result.failed)\n\n    def _do_check_shrinkage():\n      \"\"\"Check if interval has shrinked enough.\"\"\"\n      old_width = curr_interval.right.x - curr_interval.left.x\n      new_width = secant2_result.right.x - secant2_result.left.x\n      sufficient_shrinkage = new_width < old_width * shrinkage_param\n      func_is_flat = (\n          _very_close(curr_interval.left.f, curr_interval.right.f) &\n          _very_close(secant2_result.left.f, secant2_result.right.f))\n\n      new_converged = (\n          should_check_shrinkage & sufficient_shrinkage & func_is_flat)\n      needs_inner_bisect = should_check_shrinkage & ~sufficient_shrinkage\n\n      inner_bisect_args = secant2_result._replace(\n          converged=secant2_result.converged | new_converged)\n\n      def _apply_inner_bisect():\n        return _line_search_inner_bisection(\n            value_and_gradients_function, inner_bisect_args,\n            needs_inner_bisect, f_lim)\n\n      return prefer_static.cond(\n          tf.reduce_any(input_tensor=needs_inner_bisect),\n          _apply_inner_bisect,\n          lambda: inner_bisect_args)\n\n    next_args = prefer_static.cond(\n        tf.reduce_any(input_tensor=should_check_shrinkage),\n        _do_check_shrinkage,\n        lambda: secant2_result)\n\n    interval_shrunk = (\n        ~next_args.failed & _very_close(next_args.left.x, next_args.right.x))\n    return [next_args._replace(converged=next_args.converged | interval_shrunk)]\n\n  return tf.while_loop(\n      cond=_loop_cond,\n      body=_loop_body,\n      loop_vars=[search_interval],\n      parallel_iterations=1)[0]", "language": "python", "code": "def _line_search_after_bracketing(\n    value_and_gradients_function,\n    search_interval,\n    val_0,\n    f_lim,\n    max_iterations,\n    sufficient_decrease_param,\n    curvature_param,\n    shrinkage_param):\n  \"\"\"The main loop of line search after the minimum has been bracketed.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    search_interval: Instance of `HagerZhangLineSearchResults` containing\n      the current line search interval.\n    val_0: A namedtuple as returned by value_and_gradients_function evaluated\n      at `0.`. The gradient must be negative (i.e. must be a descent direction).\n    f_lim: Scalar `Tensor` of float dtype.\n    max_iterations: Positive scalar `Tensor` of integral dtype. The maximum\n      number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in [Hager and Zhang (2006)][2].\n\n  Returns:\n    A namedtuple containing the following fields.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the updated bracketing interval.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the updated bracketing interval.\n  \"\"\"\n\n  def _loop_cond(curr_interval):\n    \"\"\"Loop condition.\"\"\"\n    active = ~(curr_interval.converged | curr_interval.failed)\n    return (curr_interval.iterations <\n            max_iterations) & tf.reduce_any(input_tensor=active)\n\n  def _loop_body(curr_interval):\n    \"\"\"The loop body.\"\"\"\n    secant2_raw_result = hzl.secant2(\n        value_and_gradients_function, val_0, curr_interval, f_lim,\n        sufficient_decrease_param, curvature_param)\n    secant2_result = HagerZhangLineSearchResult(\n        converged=secant2_raw_result.converged,\n        failed=secant2_raw_result.failed,\n        iterations=curr_interval.iterations + 1,\n        func_evals=secant2_raw_result.num_evals,\n        left=secant2_raw_result.left,\n        right=secant2_raw_result.right)\n\n    should_check_shrinkage = ~(secant2_result.converged | secant2_result.failed)\n\n    def _do_check_shrinkage():\n      \"\"\"Check if interval has shrinked enough.\"\"\"\n      old_width = curr_interval.right.x - curr_interval.left.x\n      new_width = secant2_result.right.x - secant2_result.left.x\n      sufficient_shrinkage = new_width < old_width * shrinkage_param\n      func_is_flat = (\n          _very_close(curr_interval.left.f, curr_interval.right.f) &\n          _very_close(secant2_result.left.f, secant2_result.right.f))\n\n      new_converged = (\n          should_check_shrinkage & sufficient_shrinkage & func_is_flat)\n      needs_inner_bisect = should_check_shrinkage & ~sufficient_shrinkage\n\n      inner_bisect_args = secant2_result._replace(\n          converged=secant2_result.converged | new_converged)\n\n      def _apply_inner_bisect():\n        return _line_search_inner_bisection(\n            value_and_gradients_function, inner_bisect_args,\n            needs_inner_bisect, f_lim)\n\n      return prefer_static.cond(\n          tf.reduce_any(input_tensor=needs_inner_bisect),\n          _apply_inner_bisect,\n          lambda: inner_bisect_args)\n\n    next_args = prefer_static.cond(\n        tf.reduce_any(input_tensor=should_check_shrinkage),\n        _do_check_shrinkage,\n        lambda: secant2_result)\n\n    interval_shrunk = (\n        ~next_args.failed & _very_close(next_args.left.x, next_args.right.x))\n    return [next_args._replace(converged=next_args.converged | interval_shrunk)]\n\n  return tf.while_loop(\n      cond=_loop_cond,\n      body=_loop_body,\n      loop_vars=[search_interval],\n      parallel_iterations=1)[0]", "code_tokens": ["def", "_line_search_after_bracketing", "(", "value_and_gradients_function", ",", "search_interval", ",", "val_0", ",", "f_lim", ",", "max_iterations", ",", "sufficient_decrease_param", ",", "curvature_param", ",", "shrinkage_param", ")", ":", "def", "_loop_cond", "(", "curr_interval", ")", ":", "\"\"\"Loop condition.\"\"\"", "active", "=", "~", "(", "curr_interval", ".", "converged", "|", "curr_interval", ".", "failed", ")", "return", "(", "curr_interval", ".", "iterations", "<", "max_iterations", ")", "&", "tf", ".", "reduce_any", "(", "input_tensor", "=", "active", ")", "def", "_loop_body", "(", "curr_interval", ")", ":", "\"\"\"The loop body.\"\"\"", "secant2_raw_result", "=", "hzl", ".", "secant2", "(", "value_and_gradients_function", ",", "val_0", ",", "curr_interval", ",", "f_lim", ",", "sufficient_decrease_param", ",", "curvature_param", ")", "secant2_result", "=", "HagerZhangLineSearchResult", "(", "converged", "=", "secant2_raw_result", ".", "converged", ",", "failed", "=", "secant2_raw_result", ".", "failed", ",", "iterations", "=", "curr_interval", ".", "iterations", "+", "1", ",", "func_evals", "=", "secant2_raw_result", ".", "num_evals", ",", "left", "=", "secant2_raw_result", ".", "left", ",", "right", "=", "secant2_raw_result", ".", "right", ")", "should_check_shrinkage", "=", "~", "(", "secant2_result", ".", "converged", "|", "secant2_result", ".", "failed", ")", "def", "_do_check_shrinkage", "(", ")", ":", "\"\"\"Check if interval has shrinked enough.\"\"\"", "old_width", "=", "curr_interval", ".", "right", ".", "x", "-", "curr_interval", ".", "left", ".", "x", "new_width", "=", "secant2_result", ".", "right", ".", "x", "-", "secant2_result", ".", "left", ".", "x", "sufficient_shrinkage", "=", "new_width", "<", "old_width", "*", "shrinkage_param", "func_is_flat", "=", "(", "_very_close", "(", "curr_interval", ".", "left", ".", "f", ",", "curr_interval", ".", "right", ".", "f", ")", "&", "_very_close", "(", "secant2_result", ".", "left", ".", "f", ",", "secant2_result", ".", "right", ".", "f", ")", ")", "new_converged", "=", "(", "should_check_shrinkage", "&", "sufficient_shrinkage", "&", "func_is_flat", ")", "needs_inner_bisect", "=", "should_check_shrinkage", "&", "~", "sufficient_shrinkage", "inner_bisect_args", "=", "secant2_result", ".", "_replace", "(", "converged", "=", "secant2_result", ".", "converged", "|", "new_converged", ")", "def", "_apply_inner_bisect", "(", ")", ":", "return", "_line_search_inner_bisection", "(", "value_and_gradients_function", ",", "inner_bisect_args", ",", "needs_inner_bisect", ",", "f_lim", ")", "return", "prefer_static", ".", "cond", "(", "tf", ".", "reduce_any", "(", "input_tensor", "=", "needs_inner_bisect", ")", ",", "_apply_inner_bisect", ",", "lambda", ":", "inner_bisect_args", ")", "next_args", "=", "prefer_static", ".", "cond", "(", "tf", ".", "reduce_any", "(", "input_tensor", "=", "should_check_shrinkage", ")", ",", "_do_check_shrinkage", ",", "lambda", ":", "secant2_result", ")", "interval_shrunk", "=", "(", "~", "next_args", ".", "failed", "&", "_very_close", "(", "next_args", ".", "left", ".", "x", ",", "next_args", ".", "right", ".", "x", ")", ")", "return", "[", "next_args", ".", "_replace", "(", "converged", "=", "next_args", ".", "converged", "|", "interval_shrunk", ")", "]", "return", "tf", ".", "while_loop", "(", "cond", "=", "_loop_cond", ",", "body", "=", "_loop_body", ",", "loop_vars", "=", "[", "search_interval", "]", ",", "parallel_iterations", "=", "1", ")", "[", "0", "]"], "docstring": "The main loop of line search after the minimum has been bracketed.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    search_interval: Instance of `HagerZhangLineSearchResults` containing\n      the current line search interval.\n    val_0: A namedtuple as returned by value_and_gradients_function evaluated\n      at `0.`. The gradient must be negative (i.e. must be a descent direction).\n    f_lim: Scalar `Tensor` of float dtype.\n    max_iterations: Positive scalar `Tensor` of integral dtype. The maximum\n      number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in [Hager and Zhang (2006)][2].\n\n  Returns:\n    A namedtuple containing the following fields.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the updated bracketing interval.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the updated bracketing interval.", "docstring_tokens": ["The", "main", "loop", "of", "line", "search", "after", "the", "minimum", "has", "been", "bracketed", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py#L420-L542", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/internal/util.py", "func_name": "smart_for_loop", "original_string": "def smart_for_loop(loop_num_iter, body_fn, initial_loop_vars,\n                   parallel_iterations=10, name=None):\n  \"\"\"Construct a for loop, preferring a python loop if `n` is staticaly known.\n\n  Given `loop_num_iter` and `body_fn`, return an op corresponding to executing\n  `body_fn` `loop_num_iter` times, feeding previous outputs of `body_fn` into\n  the next iteration.\n\n  If `loop_num_iter` is statically known, the op is constructed via python for\n  loop, and otherwise a `tf.while_loop` is used.\n\n  Args:\n    loop_num_iter: `Integer` `Tensor` representing the number of loop\n      iterations.\n    body_fn: Callable to be executed `loop_num_iter` times.\n    initial_loop_vars: Listlike object of `Tensors` to be passed in to\n      `body_fn`'s first execution.\n    parallel_iterations: The number of iterations allowed to run in parallel.\n      It must be a positive integer. See `tf.while_loop` for more details.\n      Default value: `10`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"smart_for_loop\").\n  Returns:\n    result: `Tensor` representing applying `body_fn` iteratively `n` times.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'smart_for_loop',\n                               [loop_num_iter, initial_loop_vars]):\n    loop_num_iter_ = tf.get_static_value(loop_num_iter)\n    if (loop_num_iter_ is None or tf.executing_eagerly() or\n        control_flow_util.GraphOrParentsInXlaContext(\n            tf.compat.v1.get_default_graph())):\n      # Cast to int32 to run the comparison against i in host memory,\n      # where while/LoopCond needs it.\n      loop_num_iter = tf.cast(loop_num_iter, dtype=tf.int32)\n      return tf.while_loop(\n          cond=lambda i, *args: i < loop_num_iter,\n          body=lambda i, *args: [i + 1] + list(body_fn(*args)),\n          loop_vars=[np.int32(0)] + initial_loop_vars,\n          parallel_iterations=parallel_iterations\n      )[1:]\n    result = initial_loop_vars\n    for _ in range(loop_num_iter_):\n      result = body_fn(*result)\n    return result", "language": "python", "code": "def smart_for_loop(loop_num_iter, body_fn, initial_loop_vars,\n                   parallel_iterations=10, name=None):\n  \"\"\"Construct a for loop, preferring a python loop if `n` is staticaly known.\n\n  Given `loop_num_iter` and `body_fn`, return an op corresponding to executing\n  `body_fn` `loop_num_iter` times, feeding previous outputs of `body_fn` into\n  the next iteration.\n\n  If `loop_num_iter` is statically known, the op is constructed via python for\n  loop, and otherwise a `tf.while_loop` is used.\n\n  Args:\n    loop_num_iter: `Integer` `Tensor` representing the number of loop\n      iterations.\n    body_fn: Callable to be executed `loop_num_iter` times.\n    initial_loop_vars: Listlike object of `Tensors` to be passed in to\n      `body_fn`'s first execution.\n    parallel_iterations: The number of iterations allowed to run in parallel.\n      It must be a positive integer. See `tf.while_loop` for more details.\n      Default value: `10`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"smart_for_loop\").\n  Returns:\n    result: `Tensor` representing applying `body_fn` iteratively `n` times.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'smart_for_loop',\n                               [loop_num_iter, initial_loop_vars]):\n    loop_num_iter_ = tf.get_static_value(loop_num_iter)\n    if (loop_num_iter_ is None or tf.executing_eagerly() or\n        control_flow_util.GraphOrParentsInXlaContext(\n            tf.compat.v1.get_default_graph())):\n      # Cast to int32 to run the comparison against i in host memory,\n      # where while/LoopCond needs it.\n      loop_num_iter = tf.cast(loop_num_iter, dtype=tf.int32)\n      return tf.while_loop(\n          cond=lambda i, *args: i < loop_num_iter,\n          body=lambda i, *args: [i + 1] + list(body_fn(*args)),\n          loop_vars=[np.int32(0)] + initial_loop_vars,\n          parallel_iterations=parallel_iterations\n      )[1:]\n    result = initial_loop_vars\n    for _ in range(loop_num_iter_):\n      result = body_fn(*result)\n    return result", "code_tokens": ["def", "smart_for_loop", "(", "loop_num_iter", ",", "body_fn", ",", "initial_loop_vars", ",", "parallel_iterations", "=", "10", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'smart_for_loop'", ",", "[", "loop_num_iter", ",", "initial_loop_vars", "]", ")", ":", "loop_num_iter_", "=", "tf", ".", "get_static_value", "(", "loop_num_iter", ")", "if", "(", "loop_num_iter_", "is", "None", "or", "tf", ".", "executing_eagerly", "(", ")", "or", "control_flow_util", ".", "GraphOrParentsInXlaContext", "(", "tf", ".", "compat", ".", "v1", ".", "get_default_graph", "(", ")", ")", ")", ":", "# Cast to int32 to run the comparison against i in host memory,", "# where while/LoopCond needs it.", "loop_num_iter", "=", "tf", ".", "cast", "(", "loop_num_iter", ",", "dtype", "=", "tf", ".", "int32", ")", "return", "tf", ".", "while_loop", "(", "cond", "=", "lambda", "i", ",", "*", "args", ":", "i", "<", "loop_num_iter", ",", "body", "=", "lambda", "i", ",", "*", "args", ":", "[", "i", "+", "1", "]", "+", "list", "(", "body_fn", "(", "*", "args", ")", ")", ",", "loop_vars", "=", "[", "np", ".", "int32", "(", "0", ")", "]", "+", "initial_loop_vars", ",", "parallel_iterations", "=", "parallel_iterations", ")", "[", "1", ":", "]", "result", "=", "initial_loop_vars", "for", "_", "in", "range", "(", "loop_num_iter_", ")", ":", "result", "=", "body_fn", "(", "*", "result", ")", "return", "result"], "docstring": "Construct a for loop, preferring a python loop if `n` is staticaly known.\n\n  Given `loop_num_iter` and `body_fn`, return an op corresponding to executing\n  `body_fn` `loop_num_iter` times, feeding previous outputs of `body_fn` into\n  the next iteration.\n\n  If `loop_num_iter` is statically known, the op is constructed via python for\n  loop, and otherwise a `tf.while_loop` is used.\n\n  Args:\n    loop_num_iter: `Integer` `Tensor` representing the number of loop\n      iterations.\n    body_fn: Callable to be executed `loop_num_iter` times.\n    initial_loop_vars: Listlike object of `Tensors` to be passed in to\n      `body_fn`'s first execution.\n    parallel_iterations: The number of iterations allowed to run in parallel.\n      It must be a positive integer. See `tf.while_loop` for more details.\n      Default value: `10`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"smart_for_loop\").\n  Returns:\n    result: `Tensor` representing applying `body_fn` iteratively `n` times.", "docstring_tokens": ["Construct", "a", "for", "loop", "preferring", "a", "python", "loop", "if", "n", "is", "staticaly", "known", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/internal/util.py#L247-L290", "partition": "test"}
{"repo": "lvh/txampext", "path": "txampext/multiplexing.py", "func_name": "ProxyingProtocol._multiplexedConnectionMade", "original_string": "def _multiplexedConnectionMade(self, response):\n        \"\"\"Stores a reference to the connection, registers this protocol on\n        the factory as one related to a multiplexed AMP connection,\n        and sends currently buffered data. Gets rid of the buffer\n        afterwards.\n\n        \"\"\"\n        self.connection = conn = response[\"connection\"]\n        self.factory.protocols[conn] = self\n\n        log.msg(\"Multiplexed AMP connection ({!r}) made!\".format(conn))\n\n        data, self._buffer = self._buffer.getvalue(), None\n        if data:\n            log.msg(\"Sending {} bytes of buffered data...\".format(len(data)))\n            self._sendData(data)\n        else:\n            log.msg(\"No buffered data to send!\")", "language": "python", "code": "def _multiplexedConnectionMade(self, response):\n        \"\"\"Stores a reference to the connection, registers this protocol on\n        the factory as one related to a multiplexed AMP connection,\n        and sends currently buffered data. Gets rid of the buffer\n        afterwards.\n\n        \"\"\"\n        self.connection = conn = response[\"connection\"]\n        self.factory.protocols[conn] = self\n\n        log.msg(\"Multiplexed AMP connection ({!r}) made!\".format(conn))\n\n        data, self._buffer = self._buffer.getvalue(), None\n        if data:\n            log.msg(\"Sending {} bytes of buffered data...\".format(len(data)))\n            self._sendData(data)\n        else:\n            log.msg(\"No buffered data to send!\")", "code_tokens": ["def", "_multiplexedConnectionMade", "(", "self", ",", "response", ")", ":", "self", ".", "connection", "=", "conn", "=", "response", "[", "\"connection\"", "]", "self", ".", "factory", ".", "protocols", "[", "conn", "]", "=", "self", "log", ".", "msg", "(", "\"Multiplexed AMP connection ({!r}) made!\"", ".", "format", "(", "conn", ")", ")", "data", ",", "self", ".", "_buffer", "=", "self", ".", "_buffer", ".", "getvalue", "(", ")", ",", "None", "if", "data", ":", "log", ".", "msg", "(", "\"Sending {} bytes of buffered data...\"", ".", "format", "(", "len", "(", "data", ")", ")", ")", "self", ".", "_sendData", "(", "data", ")", "else", ":", "log", ".", "msg", "(", "\"No buffered data to send!\"", ")"], "docstring": "Stores a reference to the connection, registers this protocol on\n        the factory as one related to a multiplexed AMP connection,\n        and sends currently buffered data. Gets rid of the buffer\n        afterwards.", "docstring_tokens": ["Stores", "a", "reference", "to", "the", "connection", "registers", "this", "protocol", "on", "the", "factory", "as", "one", "related", "to", "a", "multiplexed", "AMP", "connection", "and", "sends", "currently", "buffered", "data", ".", "Gets", "rid", "of", "the", "buffer", "afterwards", "."], "sha": "a7d6cb9f1e9200dba597378cd40eb6a2096d4fd9", "url": "https://github.com/lvh/txampext/blob/a7d6cb9f1e9200dba597378cd40eb6a2096d4fd9/txampext/multiplexing.py#L283-L300", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "scripts/addjavamessage2ignore.py", "func_name": "usage", "original_string": "def usage():\n    \"\"\"\n    Illustrate what the various input flags are and the options should be.\n\n    :return: none\n    \"\"\"\n    global g_script_name    # name of the script being run.\n\n    print(\"\")\n    print(\"Usage:  \" + g_script_name + \" [...options...]\")\n    print(\"\")\n    print(\"     --help print out this help menu and show all the valid flags and inputs.\")\n    print(\"\")\n    print(\"    --inputfileadd filename where the new java messages to ignore are stored in.\")\n    print(\"\")\n    print(\"    --inputfilerm filename where the java messages are removed from the ignored list.\")\n    print(\"\")\n    print(\"    --loadjavamessage filename pickle file that stores the dict structure containing java messages to include.\")\n    print(\"\")\n    print(\"    --savejavamessage filename pickle file that saves the final dict structure after update.\")\n    print(\"\")\n    print(\"    --printjavamessage filename print java ignored java messages stored in pickle file filenam onto console and save into a text file.\")\n    print(\"\")\n    sys.exit(1)", "language": "python", "code": "def usage():\n    \"\"\"\n    Illustrate what the various input flags are and the options should be.\n\n    :return: none\n    \"\"\"\n    global g_script_name    # name of the script being run.\n\n    print(\"\")\n    print(\"Usage:  \" + g_script_name + \" [...options...]\")\n    print(\"\")\n    print(\"     --help print out this help menu and show all the valid flags and inputs.\")\n    print(\"\")\n    print(\"    --inputfileadd filename where the new java messages to ignore are stored in.\")\n    print(\"\")\n    print(\"    --inputfilerm filename where the java messages are removed from the ignored list.\")\n    print(\"\")\n    print(\"    --loadjavamessage filename pickle file that stores the dict structure containing java messages to include.\")\n    print(\"\")\n    print(\"    --savejavamessage filename pickle file that saves the final dict structure after update.\")\n    print(\"\")\n    print(\"    --printjavamessage filename print java ignored java messages stored in pickle file filenam onto console and save into a text file.\")\n    print(\"\")\n    sys.exit(1)", "code_tokens": ["def", "usage", "(", ")", ":", "global", "g_script_name", "# name of the script being run.", "print", "(", "\"\"", ")", "print", "(", "\"Usage:  \"", "+", "g_script_name", "+", "\" [...options...]\"", ")", "print", "(", "\"\"", ")", "print", "(", "\"     --help print out this help menu and show all the valid flags and inputs.\"", ")", "print", "(", "\"\"", ")", "print", "(", "\"    --inputfileadd filename where the new java messages to ignore are stored in.\"", ")", "print", "(", "\"\"", ")", "print", "(", "\"    --inputfilerm filename where the java messages are removed from the ignored list.\"", ")", "print", "(", "\"\"", ")", "print", "(", "\"    --loadjavamessage filename pickle file that stores the dict structure containing java messages to include.\"", ")", "print", "(", "\"\"", ")", "print", "(", "\"    --savejavamessage filename pickle file that saves the final dict structure after update.\"", ")", "print", "(", "\"\"", ")", "print", "(", "\"    --printjavamessage filename print java ignored java messages stored in pickle file filenam onto console and save into a text file.\"", ")", "print", "(", "\"\"", ")", "sys", ".", "exit", "(", "1", ")"], "docstring": "Illustrate what the various input flags are and the options should be.\n\n    :return: none", "docstring_tokens": ["Illustrate", "what", "the", "various", "input", "flags", "are", "and", "the", "options", "should", "be", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/scripts/addjavamessage2ignore.py#L370-L393", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py", "func_name": "MarkerEvaluation.evaluate_marker", "original_string": "def evaluate_marker(cls, text, extra=None):\n        \"\"\"\n        Evaluate a PEP 426 environment marker on CPython 2.4+.\n        Return a boolean indicating the marker result in this environment.\n        Raise SyntaxError if marker is invalid.\n\n        This implementation uses the 'parser' module, which is not implemented\n        on\n        Jython and has been superseded by the 'ast' module in Python 2.6 and\n        later.\n        \"\"\"\n        return cls.interpret(parser.expr(text).totuple(1)[1])", "language": "python", "code": "def evaluate_marker(cls, text, extra=None):\n        \"\"\"\n        Evaluate a PEP 426 environment marker on CPython 2.4+.\n        Return a boolean indicating the marker result in this environment.\n        Raise SyntaxError if marker is invalid.\n\n        This implementation uses the 'parser' module, which is not implemented\n        on\n        Jython and has been superseded by the 'ast' module in Python 2.6 and\n        later.\n        \"\"\"\n        return cls.interpret(parser.expr(text).totuple(1)[1])", "code_tokens": ["def", "evaluate_marker", "(", "cls", ",", "text", ",", "extra", "=", "None", ")", ":", "return", "cls", ".", "interpret", "(", "parser", ".", "expr", "(", "text", ")", ".", "totuple", "(", "1", ")", "[", "1", "]", ")"], "docstring": "Evaluate a PEP 426 environment marker on CPython 2.4+.\n        Return a boolean indicating the marker result in this environment.\n        Raise SyntaxError if marker is invalid.\n\n        This implementation uses the 'parser' module, which is not implemented\n        on\n        Jython and has been superseded by the 'ast' module in Python 2.6 and\n        later.", "docstring_tokens": ["Evaluate", "a", "PEP", "426", "environment", "marker", "on", "CPython", "2", ".", "4", "+", ".", "Return", "a", "boolean", "indicating", "the", "marker", "result", "in", "this", "environment", ".", "Raise", "SyntaxError", "if", "marker", "is", "invalid", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py#L1486-L1497", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/base.py", "func_name": "BasicChecker.visit_lambda", "original_string": "def visit_lambda(self, node):\n        \"\"\"check whether or not the lambda is suspicious\n        \"\"\"\n        # if the body of the lambda is a call expression with the same\n        # argument list as the lambda itself, then the lambda is\n        # possibly unnecessary and at least suspicious.\n        if node.args.defaults:\n            # If the arguments of the lambda include defaults, then a\n            # judgment cannot be made because there is no way to check\n            # that the defaults defined by the lambda are the same as\n            # the defaults defined by the function called in the body\n            # of the lambda.\n            return\n        call = node.body\n        if not isinstance(call, astroid.Call):\n            # The body of the lambda must be a function call expression\n            # for the lambda to be unnecessary.\n            return\n        if isinstance(node.body.func, astroid.Attribute) and isinstance(\n            node.body.func.expr, astroid.Call\n        ):\n            # Chained call, the intermediate call might\n            # return something else (but we don't check that, yet).\n            return\n\n        call_site = CallSite.from_call(call)\n        ordinary_args = list(node.args.args)\n        new_call_args = list(self._filter_vararg(node, call.args))\n        if node.args.kwarg:\n            if self._has_variadic_argument(call.kwargs, node.args.kwarg):\n                return\n\n        if node.args.vararg:\n            if self._has_variadic_argument(call.starargs, node.args.vararg):\n                return\n        elif call.starargs:\n            return\n\n        if call.keywords:\n            # Look for additional keyword arguments that are not part\n            # of the lambda's signature\n            lambda_kwargs = {keyword.name for keyword in node.args.defaults}\n            if len(lambda_kwargs) != len(call_site.keyword_arguments):\n                # Different lengths, so probably not identical\n                return\n            if set(call_site.keyword_arguments).difference(lambda_kwargs):\n                return\n\n        # The \"ordinary\" arguments must be in a correspondence such that:\n        # ordinary_args[i].name == call.args[i].name.\n        if len(ordinary_args) != len(new_call_args):\n            return\n        for arg, passed_arg in zip(ordinary_args, new_call_args):\n            if not isinstance(passed_arg, astroid.Name):\n                return\n            if arg.name != passed_arg.name:\n                return\n\n        self.add_message(\"unnecessary-lambda\", line=node.fromlineno, node=node)", "language": "python", "code": "def visit_lambda(self, node):\n        \"\"\"check whether or not the lambda is suspicious\n        \"\"\"\n        # if the body of the lambda is a call expression with the same\n        # argument list as the lambda itself, then the lambda is\n        # possibly unnecessary and at least suspicious.\n        if node.args.defaults:\n            # If the arguments of the lambda include defaults, then a\n            # judgment cannot be made because there is no way to check\n            # that the defaults defined by the lambda are the same as\n            # the defaults defined by the function called in the body\n            # of the lambda.\n            return\n        call = node.body\n        if not isinstance(call, astroid.Call):\n            # The body of the lambda must be a function call expression\n            # for the lambda to be unnecessary.\n            return\n        if isinstance(node.body.func, astroid.Attribute) and isinstance(\n            node.body.func.expr, astroid.Call\n        ):\n            # Chained call, the intermediate call might\n            # return something else (but we don't check that, yet).\n            return\n\n        call_site = CallSite.from_call(call)\n        ordinary_args = list(node.args.args)\n        new_call_args = list(self._filter_vararg(node, call.args))\n        if node.args.kwarg:\n            if self._has_variadic_argument(call.kwargs, node.args.kwarg):\n                return\n\n        if node.args.vararg:\n            if self._has_variadic_argument(call.starargs, node.args.vararg):\n                return\n        elif call.starargs:\n            return\n\n        if call.keywords:\n            # Look for additional keyword arguments that are not part\n            # of the lambda's signature\n            lambda_kwargs = {keyword.name for keyword in node.args.defaults}\n            if len(lambda_kwargs) != len(call_site.keyword_arguments):\n                # Different lengths, so probably not identical\n                return\n            if set(call_site.keyword_arguments).difference(lambda_kwargs):\n                return\n\n        # The \"ordinary\" arguments must be in a correspondence such that:\n        # ordinary_args[i].name == call.args[i].name.\n        if len(ordinary_args) != len(new_call_args):\n            return\n        for arg, passed_arg in zip(ordinary_args, new_call_args):\n            if not isinstance(passed_arg, astroid.Name):\n                return\n            if arg.name != passed_arg.name:\n                return\n\n        self.add_message(\"unnecessary-lambda\", line=node.fromlineno, node=node)", "code_tokens": ["def", "visit_lambda", "(", "self", ",", "node", ")", ":", "# if the body of the lambda is a call expression with the same", "# argument list as the lambda itself, then the lambda is", "# possibly unnecessary and at least suspicious.", "if", "node", ".", "args", ".", "defaults", ":", "# If the arguments of the lambda include defaults, then a", "# judgment cannot be made because there is no way to check", "# that the defaults defined by the lambda are the same as", "# the defaults defined by the function called in the body", "# of the lambda.", "return", "call", "=", "node", ".", "body", "if", "not", "isinstance", "(", "call", ",", "astroid", ".", "Call", ")", ":", "# The body of the lambda must be a function call expression", "# for the lambda to be unnecessary.", "return", "if", "isinstance", "(", "node", ".", "body", ".", "func", ",", "astroid", ".", "Attribute", ")", "and", "isinstance", "(", "node", ".", "body", ".", "func", ".", "expr", ",", "astroid", ".", "Call", ")", ":", "# Chained call, the intermediate call might", "# return something else (but we don't check that, yet).", "return", "call_site", "=", "CallSite", ".", "from_call", "(", "call", ")", "ordinary_args", "=", "list", "(", "node", ".", "args", ".", "args", ")", "new_call_args", "=", "list", "(", "self", ".", "_filter_vararg", "(", "node", ",", "call", ".", "args", ")", ")", "if", "node", ".", "args", ".", "kwarg", ":", "if", "self", ".", "_has_variadic_argument", "(", "call", ".", "kwargs", ",", "node", ".", "args", ".", "kwarg", ")", ":", "return", "if", "node", ".", "args", ".", "vararg", ":", "if", "self", ".", "_has_variadic_argument", "(", "call", ".", "starargs", ",", "node", ".", "args", ".", "vararg", ")", ":", "return", "elif", "call", ".", "starargs", ":", "return", "if", "call", ".", "keywords", ":", "# Look for additional keyword arguments that are not part", "# of the lambda's signature", "lambda_kwargs", "=", "{", "keyword", ".", "name", "for", "keyword", "in", "node", ".", "args", ".", "defaults", "}", "if", "len", "(", "lambda_kwargs", ")", "!=", "len", "(", "call_site", ".", "keyword_arguments", ")", ":", "# Different lengths, so probably not identical", "return", "if", "set", "(", "call_site", ".", "keyword_arguments", ")", ".", "difference", "(", "lambda_kwargs", ")", ":", "return", "# The \"ordinary\" arguments must be in a correspondence such that:", "# ordinary_args[i].name == call.args[i].name.", "if", "len", "(", "ordinary_args", ")", "!=", "len", "(", "new_call_args", ")", ":", "return", "for", "arg", ",", "passed_arg", "in", "zip", "(", "ordinary_args", ",", "new_call_args", ")", ":", "if", "not", "isinstance", "(", "passed_arg", ",", "astroid", ".", "Name", ")", ":", "return", "if", "arg", ".", "name", "!=", "passed_arg", ".", "name", ":", "return", "self", ".", "add_message", "(", "\"unnecessary-lambda\"", ",", "line", "=", "node", ".", "fromlineno", ",", "node", "=", "node", ")"], "docstring": "check whether or not the lambda is suspicious", "docstring_tokens": ["check", "whether", "or", "not", "the", "lambda", "is", "suspicious"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/base.py#L1153-L1211", "partition": "test"}
{"repo": "mwarkentin/django-watchman", "path": "watchman/decorators.py", "func_name": "token_required", "original_string": "def token_required(view_func):\n    \"\"\"\n    Decorator which ensures that one of the WATCHMAN_TOKENS is provided if set.\n\n    WATCHMAN_TOKEN_NAME can also be set if the token GET parameter must be\n    customized.\n\n    \"\"\"\n\n    def _parse_auth_header(auth_header):\n        \"\"\"\n        Parse the `Authorization` header\n\n        Expected format: `WATCHMAN-TOKEN Token=\"ABC123\"`\n        \"\"\"\n\n        # TODO: Figure out full set of allowed characters\n        # http://stackoverflow.com/questions/19028068/illegal-characters-in-http-headers\n        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec2.html#sec2.2\n        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.2\n        reg = re.compile('(\\w+)[=] ?\"?([\\w-]+)\"?')\n        header_dict = dict(reg.findall(auth_header))\n        return header_dict['Token']\n\n    def _get_passed_token(request):\n        \"\"\"\n        Try to get the passed token, starting with the header and fall back to `GET` param\n        \"\"\"\n\n        try:\n            auth_header = request.META['HTTP_AUTHORIZATION']\n            token = _parse_auth_header(auth_header)\n        except KeyError:\n            token = request.GET.get(settings.WATCHMAN_TOKEN_NAME)\n        return token\n\n    def _validate_token(request):\n        if settings.WATCHMAN_TOKENS:\n            watchman_tokens = settings.WATCHMAN_TOKENS.split(',')\n        elif settings.WATCHMAN_TOKEN:\n            watchman_tokens = [settings.WATCHMAN_TOKEN, ]\n        else:\n            return True\n\n        return _get_passed_token(request) in watchman_tokens\n\n    @csrf_exempt\n    @wraps(view_func)\n    def _wrapped_view(request, *args, **kwargs):\n        if _validate_token(request):\n            return view_func(request, *args, **kwargs)\n\n        return HttpResponseForbidden()\n\n    return _wrapped_view", "language": "python", "code": "def token_required(view_func):\n    \"\"\"\n    Decorator which ensures that one of the WATCHMAN_TOKENS is provided if set.\n\n    WATCHMAN_TOKEN_NAME can also be set if the token GET parameter must be\n    customized.\n\n    \"\"\"\n\n    def _parse_auth_header(auth_header):\n        \"\"\"\n        Parse the `Authorization` header\n\n        Expected format: `WATCHMAN-TOKEN Token=\"ABC123\"`\n        \"\"\"\n\n        # TODO: Figure out full set of allowed characters\n        # http://stackoverflow.com/questions/19028068/illegal-characters-in-http-headers\n        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec2.html#sec2.2\n        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.2\n        reg = re.compile('(\\w+)[=] ?\"?([\\w-]+)\"?')\n        header_dict = dict(reg.findall(auth_header))\n        return header_dict['Token']\n\n    def _get_passed_token(request):\n        \"\"\"\n        Try to get the passed token, starting with the header and fall back to `GET` param\n        \"\"\"\n\n        try:\n            auth_header = request.META['HTTP_AUTHORIZATION']\n            token = _parse_auth_header(auth_header)\n        except KeyError:\n            token = request.GET.get(settings.WATCHMAN_TOKEN_NAME)\n        return token\n\n    def _validate_token(request):\n        if settings.WATCHMAN_TOKENS:\n            watchman_tokens = settings.WATCHMAN_TOKENS.split(',')\n        elif settings.WATCHMAN_TOKEN:\n            watchman_tokens = [settings.WATCHMAN_TOKEN, ]\n        else:\n            return True\n\n        return _get_passed_token(request) in watchman_tokens\n\n    @csrf_exempt\n    @wraps(view_func)\n    def _wrapped_view(request, *args, **kwargs):\n        if _validate_token(request):\n            return view_func(request, *args, **kwargs)\n\n        return HttpResponseForbidden()\n\n    return _wrapped_view", "code_tokens": ["def", "token_required", "(", "view_func", ")", ":", "def", "_parse_auth_header", "(", "auth_header", ")", ":", "\"\"\"\n        Parse the `Authorization` header\n\n        Expected format: `WATCHMAN-TOKEN Token=\"ABC123\"`\n        \"\"\"", "# TODO: Figure out full set of allowed characters", "# http://stackoverflow.com/questions/19028068/illegal-characters-in-http-headers", "# https://www.w3.org/Protocols/rfc2616/rfc2616-sec2.html#sec2.2", "# https://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.2", "reg", "=", "re", ".", "compile", "(", "'(\\w+)[=] ?\"?([\\w-]+)\"?'", ")", "header_dict", "=", "dict", "(", "reg", ".", "findall", "(", "auth_header", ")", ")", "return", "header_dict", "[", "'Token'", "]", "def", "_get_passed_token", "(", "request", ")", ":", "\"\"\"\n        Try to get the passed token, starting with the header and fall back to `GET` param\n        \"\"\"", "try", ":", "auth_header", "=", "request", ".", "META", "[", "'HTTP_AUTHORIZATION'", "]", "token", "=", "_parse_auth_header", "(", "auth_header", ")", "except", "KeyError", ":", "token", "=", "request", ".", "GET", ".", "get", "(", "settings", ".", "WATCHMAN_TOKEN_NAME", ")", "return", "token", "def", "_validate_token", "(", "request", ")", ":", "if", "settings", ".", "WATCHMAN_TOKENS", ":", "watchman_tokens", "=", "settings", ".", "WATCHMAN_TOKENS", ".", "split", "(", "','", ")", "elif", "settings", ".", "WATCHMAN_TOKEN", ":", "watchman_tokens", "=", "[", "settings", ".", "WATCHMAN_TOKEN", ",", "]", "else", ":", "return", "True", "return", "_get_passed_token", "(", "request", ")", "in", "watchman_tokens", "@", "csrf_exempt", "@", "wraps", "(", "view_func", ")", "def", "_wrapped_view", "(", "request", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "if", "_validate_token", "(", "request", ")", ":", "return", "view_func", "(", "request", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "HttpResponseForbidden", "(", ")", "return", "_wrapped_view"], "docstring": "Decorator which ensures that one of the WATCHMAN_TOKENS is provided if set.\n\n    WATCHMAN_TOKEN_NAME can also be set if the token GET parameter must be\n    customized.", "docstring_tokens": ["Decorator", "which", "ensures", "that", "one", "of", "the", "WATCHMAN_TOKENS", "is", "provided", "if", "set", "."], "sha": "6ef98ba54dc52f27e7b42d42028b59dc67550268", "url": "https://github.com/mwarkentin/django-watchman/blob/6ef98ba54dc52f27e7b42d42028b59dc67550268/watchman/decorators.py#L57-L111", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/vector.py", "func_name": "vector", "original_string": "def vector(members: Iterable[T], meta: Optional[IPersistentMap] = None) -> Vector[T]:\n    \"\"\"Creates a new vector.\"\"\"\n    return Vector(pvector(members), meta=meta)", "language": "python", "code": "def vector(members: Iterable[T], meta: Optional[IPersistentMap] = None) -> Vector[T]:\n    \"\"\"Creates a new vector.\"\"\"\n    return Vector(pvector(members), meta=meta)", "code_tokens": ["def", "vector", "(", "members", ":", "Iterable", "[", "T", "]", ",", "meta", ":", "Optional", "[", "IPersistentMap", "]", "=", "None", ")", "->", "Vector", "[", "T", "]", ":", "return", "Vector", "(", "pvector", "(", "members", ")", ",", "meta", "=", "meta", ")"], "docstring": "Creates a new vector.", "docstring_tokens": ["Creates", "a", "new", "vector", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/vector.py#L98-L100", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/prefilter.py", "func_name": "AliasChecker.check", "original_string": "def check(self, line_info):\n        \"Check if the initital identifier on the line is an alias.\"\n        # Note: aliases can not contain '.'\n        head = line_info.ifun.split('.',1)[0]\n        if line_info.ifun not in self.shell.alias_manager \\\n               or head not in self.shell.alias_manager \\\n               or is_shadowed(head, self.shell):\n            return None\n\n        return self.prefilter_manager.get_handler_by_name('alias')", "language": "python", "code": "def check(self, line_info):\n        \"Check if the initital identifier on the line is an alias.\"\n        # Note: aliases can not contain '.'\n        head = line_info.ifun.split('.',1)[0]\n        if line_info.ifun not in self.shell.alias_manager \\\n               or head not in self.shell.alias_manager \\\n               or is_shadowed(head, self.shell):\n            return None\n\n        return self.prefilter_manager.get_handler_by_name('alias')", "code_tokens": ["def", "check", "(", "self", ",", "line_info", ")", ":", "# Note: aliases can not contain '.'", "head", "=", "line_info", ".", "ifun", ".", "split", "(", "'.'", ",", "1", ")", "[", "0", "]", "if", "line_info", ".", "ifun", "not", "in", "self", ".", "shell", ".", "alias_manager", "or", "head", "not", "in", "self", ".", "shell", ".", "alias_manager", "or", "is_shadowed", "(", "head", ",", "self", ".", "shell", ")", ":", "return", "None", "return", "self", ".", "prefilter_manager", ".", "get_handler_by_name", "(", "'alias'", ")"], "docstring": "Check if the initital identifier on the line is an alias.", "docstring_tokens": ["Check", "if", "the", "initital", "identifier", "on", "the", "line", "is", "an", "alias", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/prefilter.py#L632-L641", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/bugzilla.py", "func_name": "Bugzilla.parse_buglist", "original_string": "def parse_buglist(raw_csv):\n        \"\"\"Parse a Bugzilla CSV bug list.\n\n        The method parses the CSV file and returns an iterator of\n        dictionaries. Each one of this, contains the summary of a bug.\n\n        :param raw_csv: CSV string to parse\n\n        :returns: a generator of parsed bugs\n        \"\"\"\n        reader = csv.DictReader(raw_csv.split('\\n'),\n                                delimiter=',', quotechar='\"')\n        for row in reader:\n            yield row", "language": "python", "code": "def parse_buglist(raw_csv):\n        \"\"\"Parse a Bugzilla CSV bug list.\n\n        The method parses the CSV file and returns an iterator of\n        dictionaries. Each one of this, contains the summary of a bug.\n\n        :param raw_csv: CSV string to parse\n\n        :returns: a generator of parsed bugs\n        \"\"\"\n        reader = csv.DictReader(raw_csv.split('\\n'),\n                                delimiter=',', quotechar='\"')\n        for row in reader:\n            yield row", "code_tokens": ["def", "parse_buglist", "(", "raw_csv", ")", ":", "reader", "=", "csv", ".", "DictReader", "(", "raw_csv", ".", "split", "(", "'\\n'", ")", ",", "delimiter", "=", "','", ",", "quotechar", "=", "'\"'", ")", "for", "row", "in", "reader", ":", "yield", "row"], "docstring": "Parse a Bugzilla CSV bug list.\n\n        The method parses the CSV file and returns an iterator of\n        dictionaries. Each one of this, contains the summary of a bug.\n\n        :param raw_csv: CSV string to parse\n\n        :returns: a generator of parsed bugs", "docstring_tokens": ["Parse", "a", "Bugzilla", "CSV", "bug", "list", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/bugzilla.py#L185-L198", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/setuptools/extension.py", "func_name": "Extension._convert_pyx_sources_to_lang", "original_string": "def _convert_pyx_sources_to_lang(self):\n        \"\"\"\n        Replace sources with .pyx extensions to sources with the target\n        language extension. This mechanism allows language authors to supply\n        pre-converted sources but to prefer the .pyx sources.\n        \"\"\"\n        if have_pyrex():\n            # the build has Cython, so allow it to compile the .pyx files\n            return\n        lang = self.language or ''\n        target_ext = '.cpp' if lang.lower() == 'c++' else '.c'\n        sub = functools.partial(re.sub, '.pyx$', target_ext)\n        self.sources = list(map(sub, self.sources))", "language": "python", "code": "def _convert_pyx_sources_to_lang(self):\n        \"\"\"\n        Replace sources with .pyx extensions to sources with the target\n        language extension. This mechanism allows language authors to supply\n        pre-converted sources but to prefer the .pyx sources.\n        \"\"\"\n        if have_pyrex():\n            # the build has Cython, so allow it to compile the .pyx files\n            return\n        lang = self.language or ''\n        target_ext = '.cpp' if lang.lower() == 'c++' else '.c'\n        sub = functools.partial(re.sub, '.pyx$', target_ext)\n        self.sources = list(map(sub, self.sources))", "code_tokens": ["def", "_convert_pyx_sources_to_lang", "(", "self", ")", ":", "if", "have_pyrex", "(", ")", ":", "# the build has Cython, so allow it to compile the .pyx files", "return", "lang", "=", "self", ".", "language", "or", "''", "target_ext", "=", "'.cpp'", "if", "lang", ".", "lower", "(", ")", "==", "'c++'", "else", "'.c'", "sub", "=", "functools", ".", "partial", "(", "re", ".", "sub", ",", "'.pyx$'", ",", "target_ext", ")", "self", ".", "sources", "=", "list", "(", "map", "(", "sub", ",", "self", ".", "sources", ")", ")"], "docstring": "Replace sources with .pyx extensions to sources with the target\n        language extension. This mechanism allows language authors to supply\n        pre-converted sources but to prefer the .pyx sources.", "docstring_tokens": ["Replace", "sources", "with", ".", "pyx", "extensions", "to", "sources", "with", "the", "target", "language", "extension", ".", "This", "mechanism", "allows", "language", "authors", "to", "supply", "pre", "-", "converted", "sources", "but", "to", "prefer", "the", ".", "pyx", "sources", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/setuptools/extension.py#L37-L49", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/logging.py", "func_name": "LoggingChecker.visit_call", "original_string": "def visit_call(self, node):\n        \"\"\"Checks calls to logging methods.\"\"\"\n\n        def is_logging_name():\n            return (\n                isinstance(node.func, astroid.Attribute)\n                and isinstance(node.func.expr, astroid.Name)\n                and node.func.expr.name in self._logging_names\n            )\n\n        def is_logger_class():\n            try:\n                for inferred in node.func.infer():\n                    if isinstance(inferred, astroid.BoundMethod):\n                        parent = inferred._proxied.parent\n                        if isinstance(parent, astroid.ClassDef) and (\n                            parent.qname() == \"logging.Logger\"\n                            or any(\n                                ancestor.qname() == \"logging.Logger\"\n                                for ancestor in parent.ancestors()\n                            )\n                        ):\n                            return True, inferred._proxied.name\n            except astroid.exceptions.InferenceError:\n                pass\n            return False, None\n\n        if is_logging_name():\n            name = node.func.attrname\n        else:\n            result, name = is_logger_class()\n            if not result:\n                return\n        self._check_log_method(node, name)", "language": "python", "code": "def visit_call(self, node):\n        \"\"\"Checks calls to logging methods.\"\"\"\n\n        def is_logging_name():\n            return (\n                isinstance(node.func, astroid.Attribute)\n                and isinstance(node.func.expr, astroid.Name)\n                and node.func.expr.name in self._logging_names\n            )\n\n        def is_logger_class():\n            try:\n                for inferred in node.func.infer():\n                    if isinstance(inferred, astroid.BoundMethod):\n                        parent = inferred._proxied.parent\n                        if isinstance(parent, astroid.ClassDef) and (\n                            parent.qname() == \"logging.Logger\"\n                            or any(\n                                ancestor.qname() == \"logging.Logger\"\n                                for ancestor in parent.ancestors()\n                            )\n                        ):\n                            return True, inferred._proxied.name\n            except astroid.exceptions.InferenceError:\n                pass\n            return False, None\n\n        if is_logging_name():\n            name = node.func.attrname\n        else:\n            result, name = is_logger_class()\n            if not result:\n                return\n        self._check_log_method(node, name)", "code_tokens": ["def", "visit_call", "(", "self", ",", "node", ")", ":", "def", "is_logging_name", "(", ")", ":", "return", "(", "isinstance", "(", "node", ".", "func", ",", "astroid", ".", "Attribute", ")", "and", "isinstance", "(", "node", ".", "func", ".", "expr", ",", "astroid", ".", "Name", ")", "and", "node", ".", "func", ".", "expr", ".", "name", "in", "self", ".", "_logging_names", ")", "def", "is_logger_class", "(", ")", ":", "try", ":", "for", "inferred", "in", "node", ".", "func", ".", "infer", "(", ")", ":", "if", "isinstance", "(", "inferred", ",", "astroid", ".", "BoundMethod", ")", ":", "parent", "=", "inferred", ".", "_proxied", ".", "parent", "if", "isinstance", "(", "parent", ",", "astroid", ".", "ClassDef", ")", "and", "(", "parent", ".", "qname", "(", ")", "==", "\"logging.Logger\"", "or", "any", "(", "ancestor", ".", "qname", "(", ")", "==", "\"logging.Logger\"", "for", "ancestor", "in", "parent", ".", "ancestors", "(", ")", ")", ")", ":", "return", "True", ",", "inferred", ".", "_proxied", ".", "name", "except", "astroid", ".", "exceptions", ".", "InferenceError", ":", "pass", "return", "False", ",", "None", "if", "is_logging_name", "(", ")", ":", "name", "=", "node", ".", "func", ".", "attrname", "else", ":", "result", ",", "name", "=", "is_logger_class", "(", ")", "if", "not", "result", ":", "return", "self", ".", "_check_log_method", "(", "node", ",", "name", ")"], "docstring": "Checks calls to logging methods.", "docstring_tokens": ["Checks", "calls", "to", "logging", "methods", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/logging.py#L183-L216", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py", "func_name": "ServiceManagementService.delete_hosted_service", "original_string": "def delete_hosted_service(self, service_name, complete=False):\n        '''\n        Deletes the specified hosted service from Windows Azure.\n\n        service_name:\n            Name of the hosted service.\n        complete:\n            True if all OS/data disks and the source blobs for the disks should\n            also be deleted from storage.\n        '''\n\n        _validate_not_none('service_name', service_name)\n\n        path = self._get_hosted_service_path(service_name)\n\n        if complete == True:\n            path = path +'?comp=media'\n\n        return self._perform_delete(path, as_async=True)", "language": "python", "code": "def delete_hosted_service(self, service_name, complete=False):\n        '''\n        Deletes the specified hosted service from Windows Azure.\n\n        service_name:\n            Name of the hosted service.\n        complete:\n            True if all OS/data disks and the source blobs for the disks should\n            also be deleted from storage.\n        '''\n\n        _validate_not_none('service_name', service_name)\n\n        path = self._get_hosted_service_path(service_name)\n\n        if complete == True:\n            path = path +'?comp=media'\n\n        return self._perform_delete(path, as_async=True)", "code_tokens": ["def", "delete_hosted_service", "(", "self", ",", "service_name", ",", "complete", "=", "False", ")", ":", "_validate_not_none", "(", "'service_name'", ",", "service_name", ")", "path", "=", "self", ".", "_get_hosted_service_path", "(", "service_name", ")", "if", "complete", "==", "True", ":", "path", "=", "path", "+", "'?comp=media'", "return", "self", ".", "_perform_delete", "(", "path", ",", "as_async", "=", "True", ")"], "docstring": "Deletes the specified hosted service from Windows Azure.\n\n        service_name:\n            Name of the hosted service.\n        complete:\n            True if all OS/data disks and the source blobs for the disks should\n            also be deleted from storage.", "docstring_tokens": ["Deletes", "the", "specified", "hosted", "service", "from", "Windows", "Azure", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py#L428-L446", "partition": "test"}
{"repo": "pmacosta/peng", "path": "peng/wave_functions.py", "func_name": "acosh", "original_string": "def acosh(wave):\n    r\"\"\"\n    Return the hyperbolic arc cosine of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.acosh\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"\n    pexdoc.exh.addex(ValueError, \"Math domain error\", bool(min(wave._dep_vector) < 1))\n    return _operation(wave, \"acosh\", \"\", np.arccosh)", "language": "python", "code": "def acosh(wave):\n    r\"\"\"\n    Return the hyperbolic arc cosine of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.acosh\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"\n    pexdoc.exh.addex(ValueError, \"Math domain error\", bool(min(wave._dep_vector) < 1))\n    return _operation(wave, \"acosh\", \"\", np.arccosh)", "code_tokens": ["def", "acosh", "(", "wave", ")", ":", "pexdoc", ".", "exh", ".", "addex", "(", "ValueError", ",", "\"Math domain error\"", ",", "bool", "(", "min", "(", "wave", ".", "_dep_vector", ")", "<", "1", ")", ")", "return", "_operation", "(", "wave", ",", "\"acosh\"", ",", "\"\"", ",", "np", ".", "arccosh", ")"], "docstring": "r\"\"\"\n    Return the hyperbolic arc cosine of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.acosh\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]", "docstring_tokens": ["r", "Return", "the", "hyperbolic", "arc", "cosine", "of", "a", "waveform", "s", "dependent", "variable", "vector", "."], "sha": "976935377adaa3de26fc5677aceb2cdfbd6f93a7", "url": "https://github.com/pmacosta/peng/blob/976935377adaa3de26fc5677aceb2cdfbd6f93a7/peng/wave_functions.py#L163-L184", "partition": "test"}
{"repo": "eyeseast/python-frontmatter", "path": "frontmatter/default_handlers.py", "func_name": "JSONHandler.export", "original_string": "def export(self, metadata, **kwargs):\n        \"Turn metadata into JSON\"\n        kwargs.setdefault('indent', 4)\n        metadata = json.dumps(metadata, **kwargs)\n        return u(metadata)", "language": "python", "code": "def export(self, metadata, **kwargs):\n        \"Turn metadata into JSON\"\n        kwargs.setdefault('indent', 4)\n        metadata = json.dumps(metadata, **kwargs)\n        return u(metadata)", "code_tokens": ["def", "export", "(", "self", ",", "metadata", ",", "*", "*", "kwargs", ")", ":", "kwargs", ".", "setdefault", "(", "'indent'", ",", "4", ")", "metadata", "=", "json", ".", "dumps", "(", "metadata", ",", "*", "*", "kwargs", ")", "return", "u", "(", "metadata", ")"], "docstring": "Turn metadata into JSON", "docstring_tokens": ["Turn", "metadata", "into", "JSON"], "sha": "c318e583c48599eb597e0ad59c5d972258c3febc", "url": "https://github.com/eyeseast/python-frontmatter/blob/c318e583c48599eb597e0ad59c5d972258c3febc/frontmatter/default_handlers.py#L238-L242", "partition": "test"}
{"repo": "oscarbranson/latools", "path": "latools/latools.py", "func_name": "analyse.gradient_plots", "original_string": "def gradient_plots(self, analytes=None, win=15, samples=None, ranges=False,\n                       focus=None, outdir=None,\n                       figsize=[10, 4], subset='All_Analyses'):\n        \"\"\"\n        Plot analyte gradients as a function of time.\n\n        Parameters\n        ----------\n        analytes : optional, array_like or str\n            The analyte(s) to plot. Defaults to all analytes.\n        samples: optional, array_like or str\n            The sample(s) to plot. Defaults to all samples.\n        ranges : bool\n            Whether or not to show the signal/backgroudn regions\n            identified by 'autorange'.\n        focus : str\n            The focus 'stage' of the analysis to plot. Can be\n            'rawdata', 'despiked':, 'signal', 'background',\n            'bkgsub', 'ratios' or 'calibrated'.\n        outdir : str\n            Path to a directory where you'd like the plots to be\n            saved. Defaults to 'reports/[focus]' in your data directory.\n        filt : str, dict or bool\n            Either logical filter expression contained in a str,\n            a dict of expressions specifying the filter string to\n            use for each analyte or a boolean. Passed to `grab_filt`.\n        scale : str\n            If 'log', plots the data on a log scale.\n        figsize : array_like\n            Array of length 2 specifying figure [width, height] in\n            inches.\n        stats : bool\n            Whether or not to overlay the mean and standard deviations\n            for each trace.\n        stat, err: str\n            The names of the statistic and error components to plot.\n            Deafaults to 'nanmean' and 'nanstd'.\n\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if focus is None:\n            focus = self.focus_stage\n        if outdir is None:\n            outdir = self.report_dir + '/' + focus + '_gradient'\n        if not os.path.isdir(outdir):\n            os.mkdir(outdir)\n\n        # if samples is not None:\n        #     subset = self.make_subset(samples)\n\n        if subset is not None:\n            samples = self._get_samples(subset)\n        elif samples is None:\n            samples = self.subsets['All_Analyses']\n        elif isinstance(samples, str):\n            samples = [samples]\n\n        with self.pbar.set(total=len(samples), desc='Drawing Plots') as prog:\n            for s in samples:\n                f, a = self.data[s].gplot(analytes=analytes, win=win, figsize=figsize,\n                                        ranges=ranges, focus_stage=focus)\n                # ax = fig.axes[0]\n                # for l, u in s.sigrng:\n                #     ax.axvspan(l, u, color='r', alpha=0.1)\n                # for l, u in s.bkgrng:\n                #     ax.axvspan(l, u, color='k', alpha=0.1)\n                f.savefig(outdir + '/' + s + '_gradients.pdf')\n                # TODO: on older(?) computers raises\n                # 'OSError: [Errno 24] Too many open files'\n                plt.close(f)\n                prog.update()\n        return", "language": "python", "code": "def gradient_plots(self, analytes=None, win=15, samples=None, ranges=False,\n                       focus=None, outdir=None,\n                       figsize=[10, 4], subset='All_Analyses'):\n        \"\"\"\n        Plot analyte gradients as a function of time.\n\n        Parameters\n        ----------\n        analytes : optional, array_like or str\n            The analyte(s) to plot. Defaults to all analytes.\n        samples: optional, array_like or str\n            The sample(s) to plot. Defaults to all samples.\n        ranges : bool\n            Whether or not to show the signal/backgroudn regions\n            identified by 'autorange'.\n        focus : str\n            The focus 'stage' of the analysis to plot. Can be\n            'rawdata', 'despiked':, 'signal', 'background',\n            'bkgsub', 'ratios' or 'calibrated'.\n        outdir : str\n            Path to a directory where you'd like the plots to be\n            saved. Defaults to 'reports/[focus]' in your data directory.\n        filt : str, dict or bool\n            Either logical filter expression contained in a str,\n            a dict of expressions specifying the filter string to\n            use for each analyte or a boolean. Passed to `grab_filt`.\n        scale : str\n            If 'log', plots the data on a log scale.\n        figsize : array_like\n            Array of length 2 specifying figure [width, height] in\n            inches.\n        stats : bool\n            Whether or not to overlay the mean and standard deviations\n            for each trace.\n        stat, err: str\n            The names of the statistic and error components to plot.\n            Deafaults to 'nanmean' and 'nanstd'.\n\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if focus is None:\n            focus = self.focus_stage\n        if outdir is None:\n            outdir = self.report_dir + '/' + focus + '_gradient'\n        if not os.path.isdir(outdir):\n            os.mkdir(outdir)\n\n        # if samples is not None:\n        #     subset = self.make_subset(samples)\n\n        if subset is not None:\n            samples = self._get_samples(subset)\n        elif samples is None:\n            samples = self.subsets['All_Analyses']\n        elif isinstance(samples, str):\n            samples = [samples]\n\n        with self.pbar.set(total=len(samples), desc='Drawing Plots') as prog:\n            for s in samples:\n                f, a = self.data[s].gplot(analytes=analytes, win=win, figsize=figsize,\n                                        ranges=ranges, focus_stage=focus)\n                # ax = fig.axes[0]\n                # for l, u in s.sigrng:\n                #     ax.axvspan(l, u, color='r', alpha=0.1)\n                # for l, u in s.bkgrng:\n                #     ax.axvspan(l, u, color='k', alpha=0.1)\n                f.savefig(outdir + '/' + s + '_gradients.pdf')\n                # TODO: on older(?) computers raises\n                # 'OSError: [Errno 24] Too many open files'\n                plt.close(f)\n                prog.update()\n        return", "code_tokens": ["def", "gradient_plots", "(", "self", ",", "analytes", "=", "None", ",", "win", "=", "15", ",", "samples", "=", "None", ",", "ranges", "=", "False", ",", "focus", "=", "None", ",", "outdir", "=", "None", ",", "figsize", "=", "[", "10", ",", "4", "]", ",", "subset", "=", "'All_Analyses'", ")", ":", "if", "focus", "is", "None", ":", "focus", "=", "self", ".", "focus_stage", "if", "outdir", "is", "None", ":", "outdir", "=", "self", ".", "report_dir", "+", "'/'", "+", "focus", "+", "'_gradient'", "if", "not", "os", ".", "path", ".", "isdir", "(", "outdir", ")", ":", "os", ".", "mkdir", "(", "outdir", ")", "# if samples is not None:", "#     subset = self.make_subset(samples)", "if", "subset", "is", "not", "None", ":", "samples", "=", "self", ".", "_get_samples", "(", "subset", ")", "elif", "samples", "is", "None", ":", "samples", "=", "self", ".", "subsets", "[", "'All_Analyses'", "]", "elif", "isinstance", "(", "samples", ",", "str", ")", ":", "samples", "=", "[", "samples", "]", "with", "self", ".", "pbar", ".", "set", "(", "total", "=", "len", "(", "samples", ")", ",", "desc", "=", "'Drawing Plots'", ")", "as", "prog", ":", "for", "s", "in", "samples", ":", "f", ",", "a", "=", "self", ".", "data", "[", "s", "]", ".", "gplot", "(", "analytes", "=", "analytes", ",", "win", "=", "win", ",", "figsize", "=", "figsize", ",", "ranges", "=", "ranges", ",", "focus_stage", "=", "focus", ")", "# ax = fig.axes[0]", "# for l, u in s.sigrng:", "#     ax.axvspan(l, u, color='r', alpha=0.1)", "# for l, u in s.bkgrng:", "#     ax.axvspan(l, u, color='k', alpha=0.1)", "f", ".", "savefig", "(", "outdir", "+", "'/'", "+", "s", "+", "'_gradients.pdf'", ")", "# TODO: on older(?) computers raises", "# 'OSError: [Errno 24] Too many open files'", "plt", ".", "close", "(", "f", ")", "prog", ".", "update", "(", ")", "return"], "docstring": "Plot analyte gradients as a function of time.\n\n        Parameters\n        ----------\n        analytes : optional, array_like or str\n            The analyte(s) to plot. Defaults to all analytes.\n        samples: optional, array_like or str\n            The sample(s) to plot. Defaults to all samples.\n        ranges : bool\n            Whether or not to show the signal/backgroudn regions\n            identified by 'autorange'.\n        focus : str\n            The focus 'stage' of the analysis to plot. Can be\n            'rawdata', 'despiked':, 'signal', 'background',\n            'bkgsub', 'ratios' or 'calibrated'.\n        outdir : str\n            Path to a directory where you'd like the plots to be\n            saved. Defaults to 'reports/[focus]' in your data directory.\n        filt : str, dict or bool\n            Either logical filter expression contained in a str,\n            a dict of expressions specifying the filter string to\n            use for each analyte or a boolean. Passed to `grab_filt`.\n        scale : str\n            If 'log', plots the data on a log scale.\n        figsize : array_like\n            Array of length 2 specifying figure [width, height] in\n            inches.\n        stats : bool\n            Whether or not to overlay the mean and standard deviations\n            for each trace.\n        stat, err: str\n            The names of the statistic and error components to plot.\n            Deafaults to 'nanmean' and 'nanstd'.\n\n\n        Returns\n        -------\n        None", "docstring_tokens": ["Plot", "analyte", "gradients", "as", "a", "function", "of", "time", "."], "sha": "cd25a650cfee318152f234d992708511f7047fbe", "url": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/latools.py#L3371-L3445", "partition": "test"}
{"repo": "3DLIRIOUS/MeshLabXML", "path": "meshlabxml/normals.py", "func_name": "flip", "original_string": "def flip(script, force_flip=False, selected=False):\n    \"\"\" Invert faces orientation, flipping the normals of the mesh.\n\n    If requested, it tries to guess the right orientation; mainly it decides to\n    flip all the faces if the minimum/maximum vertexes have not outward point\n    normals for a few directions. Works well for single component watertight\n    objects.\n\n    Args:\n        script: the FilterScript object or script filename to write\n            the filter to.\n        force_flip (bool): If selected, the normals will always be flipped;\n            otherwise, the filter tries to set them outside.\n        selected (bool): If selected, only selected faces will be affected.\n\n    Layer stack:\n        No impacts\n\n    MeshLab versions:\n        2016.12\n        1.3.4BETA\n    \"\"\"\n    filter_xml = ''.join([\n        '  <filter name=\"Invert Faces Orientation\">\\n',\n        '    <Param name=\"forceFlip\" ',\n        'value=\"{}\" '.format(str(force_flip).lower()),\n        'description=\"Force Flip\" ',\n        'type=\"RichBool\" ',\n        '/>\\n',\n        '    <Param name=\"onlySelected\" ',\n        'value=\"{}\" '.format(str(selected).lower()),\n        'description=\"Flip only selected faces\" ',\n        'type=\"RichBool\" ',\n        '/>\\n',\n        '  </filter>\\n'])\n    util.write_filter(script, filter_xml)\n    return None", "language": "python", "code": "def flip(script, force_flip=False, selected=False):\n    \"\"\" Invert faces orientation, flipping the normals of the mesh.\n\n    If requested, it tries to guess the right orientation; mainly it decides to\n    flip all the faces if the minimum/maximum vertexes have not outward point\n    normals for a few directions. Works well for single component watertight\n    objects.\n\n    Args:\n        script: the FilterScript object or script filename to write\n            the filter to.\n        force_flip (bool): If selected, the normals will always be flipped;\n            otherwise, the filter tries to set them outside.\n        selected (bool): If selected, only selected faces will be affected.\n\n    Layer stack:\n        No impacts\n\n    MeshLab versions:\n        2016.12\n        1.3.4BETA\n    \"\"\"\n    filter_xml = ''.join([\n        '  <filter name=\"Invert Faces Orientation\">\\n',\n        '    <Param name=\"forceFlip\" ',\n        'value=\"{}\" '.format(str(force_flip).lower()),\n        'description=\"Force Flip\" ',\n        'type=\"RichBool\" ',\n        '/>\\n',\n        '    <Param name=\"onlySelected\" ',\n        'value=\"{}\" '.format(str(selected).lower()),\n        'description=\"Flip only selected faces\" ',\n        'type=\"RichBool\" ',\n        '/>\\n',\n        '  </filter>\\n'])\n    util.write_filter(script, filter_xml)\n    return None", "code_tokens": ["def", "flip", "(", "script", ",", "force_flip", "=", "False", ",", "selected", "=", "False", ")", ":", "filter_xml", "=", "''", ".", "join", "(", "[", "'  <filter name=\"Invert Faces Orientation\">\\n'", ",", "'    <Param name=\"forceFlip\" '", ",", "'value=\"{}\" '", ".", "format", "(", "str", "(", "force_flip", ")", ".", "lower", "(", ")", ")", ",", "'description=\"Force Flip\" '", ",", "'type=\"RichBool\" '", ",", "'/>\\n'", ",", "'    <Param name=\"onlySelected\" '", ",", "'value=\"{}\" '", ".", "format", "(", "str", "(", "selected", ")", ".", "lower", "(", ")", ")", ",", "'description=\"Flip only selected faces\" '", ",", "'type=\"RichBool\" '", ",", "'/>\\n'", ",", "'  </filter>\\n'", "]", ")", "util", ".", "write_filter", "(", "script", ",", "filter_xml", ")", "return", "None"], "docstring": "Invert faces orientation, flipping the normals of the mesh.\n\n    If requested, it tries to guess the right orientation; mainly it decides to\n    flip all the faces if the minimum/maximum vertexes have not outward point\n    normals for a few directions. Works well for single component watertight\n    objects.\n\n    Args:\n        script: the FilterScript object or script filename to write\n            the filter to.\n        force_flip (bool): If selected, the normals will always be flipped;\n            otherwise, the filter tries to set them outside.\n        selected (bool): If selected, only selected faces will be affected.\n\n    Layer stack:\n        No impacts\n\n    MeshLab versions:\n        2016.12\n        1.3.4BETA", "docstring_tokens": ["Invert", "faces", "orientation", "flipping", "the", "normals", "of", "the", "mesh", "."], "sha": "177cce21e92baca500f56a932d66bd9a33257af8", "url": "https://github.com/3DLIRIOUS/MeshLabXML/blob/177cce21e92baca500f56a932d66bd9a33257af8/meshlabxml/normals.py#L33-L69", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py", "func_name": "ServiceManagementService.get_storage_account_keys", "original_string": "def get_storage_account_keys(self, service_name):\n        '''\n        Returns the primary and secondary access keys for the specified\n        storage account.\n\n        service_name:\n            Name of the storage service account.\n        '''\n        _validate_not_none('service_name', service_name)\n        return self._perform_get(\n            self._get_storage_service_path(service_name) + '/keys',\n            StorageService)", "language": "python", "code": "def get_storage_account_keys(self, service_name):\n        '''\n        Returns the primary and secondary access keys for the specified\n        storage account.\n\n        service_name:\n            Name of the storage service account.\n        '''\n        _validate_not_none('service_name', service_name)\n        return self._perform_get(\n            self._get_storage_service_path(service_name) + '/keys',\n            StorageService)", "code_tokens": ["def", "get_storage_account_keys", "(", "self", ",", "service_name", ")", ":", "_validate_not_none", "(", "'service_name'", ",", "service_name", ")", "return", "self", ".", "_perform_get", "(", "self", ".", "_get_storage_service_path", "(", "service_name", ")", "+", "'/keys'", ",", "StorageService", ")"], "docstring": "Returns the primary and secondary access keys for the specified\n        storage account.\n\n        service_name:\n            Name of the storage service account.", "docstring_tokens": ["Returns", "the", "primary", "and", "secondary", "access", "keys", "for", "the", "specified", "storage", "account", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py#L139-L150", "partition": "test"}
{"repo": "bakwc/PySyncObj", "path": "pysyncobj/transport.py", "func_name": "TCPTransport.dropNode", "original_string": "def dropNode(self, node):\n        \"\"\"\n        Drop a node from the network\n\n        :param node: node to drop\n        :type node: Node\n        \"\"\"\n\n        conn = self._connections.pop(node, None)\n        if conn is not None:\n            # Calling conn.disconnect() immediately triggers the onDisconnected callback if the connection isn't already disconnected, so this is necessary to prevent the automatic reconnect.\n            self._preventConnectNodes.add(node)\n            conn.disconnect()\n            self._preventConnectNodes.remove(node)\n        if isinstance(node, TCPNode):\n            self._nodes.discard(node)\n            self._nodeAddrToNode.pop(node.address, None)\n        else:\n            self._readonlyNodes.discard(node)\n        self._lastConnectAttempt.pop(node, None)", "language": "python", "code": "def dropNode(self, node):\n        \"\"\"\n        Drop a node from the network\n\n        :param node: node to drop\n        :type node: Node\n        \"\"\"\n\n        conn = self._connections.pop(node, None)\n        if conn is not None:\n            # Calling conn.disconnect() immediately triggers the onDisconnected callback if the connection isn't already disconnected, so this is necessary to prevent the automatic reconnect.\n            self._preventConnectNodes.add(node)\n            conn.disconnect()\n            self._preventConnectNodes.remove(node)\n        if isinstance(node, TCPNode):\n            self._nodes.discard(node)\n            self._nodeAddrToNode.pop(node.address, None)\n        else:\n            self._readonlyNodes.discard(node)\n        self._lastConnectAttempt.pop(node, None)", "code_tokens": ["def", "dropNode", "(", "self", ",", "node", ")", ":", "conn", "=", "self", ".", "_connections", ".", "pop", "(", "node", ",", "None", ")", "if", "conn", "is", "not", "None", ":", "# Calling conn.disconnect() immediately triggers the onDisconnected callback if the connection isn't already disconnected, so this is necessary to prevent the automatic reconnect.", "self", ".", "_preventConnectNodes", ".", "add", "(", "node", ")", "conn", ".", "disconnect", "(", ")", "self", ".", "_preventConnectNodes", ".", "remove", "(", "node", ")", "if", "isinstance", "(", "node", ",", "TCPNode", ")", ":", "self", ".", "_nodes", ".", "discard", "(", "node", ")", "self", ".", "_nodeAddrToNode", ".", "pop", "(", "node", ".", "address", ",", "None", ")", "else", ":", "self", ".", "_readonlyNodes", ".", "discard", "(", "node", ")", "self", ".", "_lastConnectAttempt", ".", "pop", "(", "node", ",", "None", ")"], "docstring": "Drop a node from the network\n\n        :param node: node to drop\n        :type node: Node", "docstring_tokens": ["Drop", "a", "node", "from", "the", "network"], "sha": "be3b0aaa932d5156f5df140c23c962430f51b7b8", "url": "https://github.com/bakwc/PySyncObj/blob/be3b0aaa932d5156f5df140c23c962430f51b7b8/pysyncobj/transport.py#L511-L530", "partition": "test"}
{"repo": "philipsoutham/py-mysql2pgsql", "path": "mysql2pgsql/lib/postgres_db_writer.py", "func_name": "PostgresDbWriter.write_constraints", "original_string": "def write_constraints(self, table):\n        \"\"\"Send DDL to create the specified `table` constraints\n\n        :Parameters:\n          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.\n\n        Returns None\n        \"\"\"\n        constraint_sql = super(PostgresDbWriter, self).write_constraints(table)\n        for sql in constraint_sql:\n            self.execute(sql)", "language": "python", "code": "def write_constraints(self, table):\n        \"\"\"Send DDL to create the specified `table` constraints\n\n        :Parameters:\n          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.\n\n        Returns None\n        \"\"\"\n        constraint_sql = super(PostgresDbWriter, self).write_constraints(table)\n        for sql in constraint_sql:\n            self.execute(sql)", "code_tokens": ["def", "write_constraints", "(", "self", ",", "table", ")", ":", "constraint_sql", "=", "super", "(", "PostgresDbWriter", ",", "self", ")", ".", "write_constraints", "(", "table", ")", "for", "sql", "in", "constraint_sql", ":", "self", ".", "execute", "(", "sql", ")"], "docstring": "Send DDL to create the specified `table` constraints\n\n        :Parameters:\n          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.\n\n        Returns None", "docstring_tokens": ["Send", "DDL", "to", "create", "the", "specified", "table", "constraints"], "sha": "66dc2a3a3119263b3fe77300fb636346509787ef", "url": "https://github.com/philipsoutham/py-mysql2pgsql/blob/66dc2a3a3119263b3fe77300fb636346509787ef/mysql2pgsql/lib/postgres_db_writer.py#L183-L193", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/parallel/controller/hub.py", "func_name": "Hub.get_history", "original_string": "def get_history(self, client_id, msg):\n        \"\"\"Get a list of all msg_ids in our DB records\"\"\"\n        try:\n            msg_ids = self.db.get_history()\n        except Exception as e:\n            content = error.wrap_exception()\n        else:\n            content = dict(status='ok', history=msg_ids)\n\n        self.session.send(self.query, \"history_reply\", content=content,\n                                            parent=msg, ident=client_id)", "language": "python", "code": "def get_history(self, client_id, msg):\n        \"\"\"Get a list of all msg_ids in our DB records\"\"\"\n        try:\n            msg_ids = self.db.get_history()\n        except Exception as e:\n            content = error.wrap_exception()\n        else:\n            content = dict(status='ok', history=msg_ids)\n\n        self.session.send(self.query, \"history_reply\", content=content,\n                                            parent=msg, ident=client_id)", "code_tokens": ["def", "get_history", "(", "self", ",", "client_id", ",", "msg", ")", ":", "try", ":", "msg_ids", "=", "self", ".", "db", ".", "get_history", "(", ")", "except", "Exception", "as", "e", ":", "content", "=", "error", ".", "wrap_exception", "(", ")", "else", ":", "content", "=", "dict", "(", "status", "=", "'ok'", ",", "history", "=", "msg_ids", ")", "self", ".", "session", ".", "send", "(", "self", ".", "query", ",", "\"history_reply\"", ",", "content", "=", "content", ",", "parent", "=", "msg", ",", "ident", "=", "client_id", ")"], "docstring": "Get a list of all msg_ids in our DB records", "docstring_tokens": ["Get", "a", "list", "of", "all", "msg_ids", "in", "our", "DB", "records"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/parallel/controller/hub.py#L1289-L1299", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs_utils.py", "func_name": "_broadcast", "original_string": "def _broadcast(value, target):\n  \"\"\"Broadcast a value to match the batching dimensions of a target.\n\n  If necessary the value is converted into a tensor. Both value and target\n  should be of the same dtype.\n\n  Args:\n    value: A value to broadcast.\n    target: A `Tensor` of shape [b1, ..., bn, d].\n\n  Returns:\n    A `Tensor` of shape [b1, ..., bn] and same dtype as the target.\n  \"\"\"\n  return tf.broadcast_to(\n      tf.convert_to_tensor(value=value, dtype=target.dtype),\n      distribution_util.prefer_static_shape(target)[:-1])", "language": "python", "code": "def _broadcast(value, target):\n  \"\"\"Broadcast a value to match the batching dimensions of a target.\n\n  If necessary the value is converted into a tensor. Both value and target\n  should be of the same dtype.\n\n  Args:\n    value: A value to broadcast.\n    target: A `Tensor` of shape [b1, ..., bn, d].\n\n  Returns:\n    A `Tensor` of shape [b1, ..., bn] and same dtype as the target.\n  \"\"\"\n  return tf.broadcast_to(\n      tf.convert_to_tensor(value=value, dtype=target.dtype),\n      distribution_util.prefer_static_shape(target)[:-1])", "code_tokens": ["def", "_broadcast", "(", "value", ",", "target", ")", ":", "return", "tf", ".", "broadcast_to", "(", "tf", ".", "convert_to_tensor", "(", "value", "=", "value", ",", "dtype", "=", "target", ".", "dtype", ")", ",", "distribution_util", ".", "prefer_static_shape", "(", "target", ")", "[", ":", "-", "1", "]", ")"], "docstring": "Broadcast a value to match the batching dimensions of a target.\n\n  If necessary the value is converted into a tensor. Both value and target\n  should be of the same dtype.\n\n  Args:\n    value: A value to broadcast.\n    target: A `Tensor` of shape [b1, ..., bn, d].\n\n  Returns:\n    A `Tensor` of shape [b1, ..., bn] and same dtype as the target.", "docstring_tokens": ["Broadcast", "a", "value", "to", "match", "the", "batching", "dimensions", "of", "a", "target", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs_utils.py#L325-L340", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/quantum_info/analyzation/average.py", "func_name": "average_data", "original_string": "def average_data(counts, observable):\n    \"\"\"Compute the mean value of an diagonal observable.\n\n    Takes in a diagonal observable in dictionary, list or matrix format and then\n    calculates the sum_i value(i) P(i) where value(i) is the value of the\n    observable for state i.\n\n    Args:\n        counts (dict): a dict of outcomes from an experiment\n        observable (dict or matrix or list): The observable to be averaged over.\n        As an example, ZZ on qubits can be given as:\n        * dict: {\"00\": 1, \"11\": 1, \"01\": -1, \"10\": -1}\n        * matrix: [[1, 0, 0, 0], [0, -1, 0, 0, ], [0, 0, -1, 0], [0, 0, 0, 1]]\n        * matrix diagonal (list): [1, -1, -1, 1]\n\n    Returns:\n        Double: Average of the observable\n    \"\"\"\n    if not isinstance(observable, dict):\n        observable = make_dict_observable(observable)\n    temp = 0\n    tot = sum(counts.values())\n    for key in counts:\n        if key in observable:\n            temp += counts[key] * observable[key] / tot\n    return temp", "language": "python", "code": "def average_data(counts, observable):\n    \"\"\"Compute the mean value of an diagonal observable.\n\n    Takes in a diagonal observable in dictionary, list or matrix format and then\n    calculates the sum_i value(i) P(i) where value(i) is the value of the\n    observable for state i.\n\n    Args:\n        counts (dict): a dict of outcomes from an experiment\n        observable (dict or matrix or list): The observable to be averaged over.\n        As an example, ZZ on qubits can be given as:\n        * dict: {\"00\": 1, \"11\": 1, \"01\": -1, \"10\": -1}\n        * matrix: [[1, 0, 0, 0], [0, -1, 0, 0, ], [0, 0, -1, 0], [0, 0, 0, 1]]\n        * matrix diagonal (list): [1, -1, -1, 1]\n\n    Returns:\n        Double: Average of the observable\n    \"\"\"\n    if not isinstance(observable, dict):\n        observable = make_dict_observable(observable)\n    temp = 0\n    tot = sum(counts.values())\n    for key in counts:\n        if key in observable:\n            temp += counts[key] * observable[key] / tot\n    return temp", "code_tokens": ["def", "average_data", "(", "counts", ",", "observable", ")", ":", "if", "not", "isinstance", "(", "observable", ",", "dict", ")", ":", "observable", "=", "make_dict_observable", "(", "observable", ")", "temp", "=", "0", "tot", "=", "sum", "(", "counts", ".", "values", "(", ")", ")", "for", "key", "in", "counts", ":", "if", "key", "in", "observable", ":", "temp", "+=", "counts", "[", "key", "]", "*", "observable", "[", "key", "]", "/", "tot", "return", "temp"], "docstring": "Compute the mean value of an diagonal observable.\n\n    Takes in a diagonal observable in dictionary, list or matrix format and then\n    calculates the sum_i value(i) P(i) where value(i) is the value of the\n    observable for state i.\n\n    Args:\n        counts (dict): a dict of outcomes from an experiment\n        observable (dict or matrix or list): The observable to be averaged over.\n        As an example, ZZ on qubits can be given as:\n        * dict: {\"00\": 1, \"11\": 1, \"01\": -1, \"10\": -1}\n        * matrix: [[1, 0, 0, 0], [0, -1, 0, 0, ], [0, 0, -1, 0], [0, 0, 0, 1]]\n        * matrix diagonal (list): [1, -1, -1, 1]\n\n    Returns:\n        Double: Average of the observable", "docstring_tokens": ["Compute", "the", "mean", "value", "of", "an", "diagonal", "observable", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/quantum_info/analyzation/average.py#L13-L38", "partition": "test"}
{"repo": "ambitioninc/python-logentries-api", "path": "logentries_api/resources.py", "func_name": "Tags.create", "original_string": "def create(self, label_id):\n        \"\"\"\n        Create a new tag\n\n        :param label_id: The Label ID (the 'sn' key of the create label response)\n        :type label_id: str\n\n        :returns: The response of your post\n        :rtype: dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"\n        data = {\n            'type': 'tagit',\n            'rate_count': 0,\n            'rate_range': 'day',\n            'limit_count': 0,\n            'limit_range': 'day',\n            'schedule': [],\n            'enabled': True,\n            'args': {\n                'sn': label_id,\n                'tag_sn': label_id\n            }\n        }\n        # Yes, it's confusing. the `/actions/` endpoint is used for tags, while\n        # the /tags/ endpoint is used for labels.\n        return self._post(\n            request=ApiActions.CREATE.value,\n            uri=ApiUri.ACTIONS.value,\n            params=data\n        )", "language": "python", "code": "def create(self, label_id):\n        \"\"\"\n        Create a new tag\n\n        :param label_id: The Label ID (the 'sn' key of the create label response)\n        :type label_id: str\n\n        :returns: The response of your post\n        :rtype: dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"\n        data = {\n            'type': 'tagit',\n            'rate_count': 0,\n            'rate_range': 'day',\n            'limit_count': 0,\n            'limit_range': 'day',\n            'schedule': [],\n            'enabled': True,\n            'args': {\n                'sn': label_id,\n                'tag_sn': label_id\n            }\n        }\n        # Yes, it's confusing. the `/actions/` endpoint is used for tags, while\n        # the /tags/ endpoint is used for labels.\n        return self._post(\n            request=ApiActions.CREATE.value,\n            uri=ApiUri.ACTIONS.value,\n            params=data\n        )", "code_tokens": ["def", "create", "(", "self", ",", "label_id", ")", ":", "data", "=", "{", "'type'", ":", "'tagit'", ",", "'rate_count'", ":", "0", ",", "'rate_range'", ":", "'day'", ",", "'limit_count'", ":", "0", ",", "'limit_range'", ":", "'day'", ",", "'schedule'", ":", "[", "]", ",", "'enabled'", ":", "True", ",", "'args'", ":", "{", "'sn'", ":", "label_id", ",", "'tag_sn'", ":", "label_id", "}", "}", "# Yes, it's confusing. the `/actions/` endpoint is used for tags, while", "# the /tags/ endpoint is used for labels.", "return", "self", ".", "_post", "(", "request", "=", "ApiActions", ".", "CREATE", ".", "value", ",", "uri", "=", "ApiUri", ".", "ACTIONS", ".", "value", ",", "params", "=", "data", ")"], "docstring": "Create a new tag\n\n        :param label_id: The Label ID (the 'sn' key of the create label response)\n        :type label_id: str\n\n        :returns: The response of your post\n        :rtype: dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries", "docstring_tokens": ["Create", "a", "new", "tag"], "sha": "77ff1a7a2995d7ea2725b74e34c0f880f4ee23bc", "url": "https://github.com/ambitioninc/python-logentries-api/blob/77ff1a7a2995d7ea2725b74e34c0f880f4ee23bc/logentries_api/resources.py#L219-L252", "partition": "test"}
{"repo": "assemblerflow/flowcraft", "path": "flowcraft/templates/trimmomatic.py", "func_name": "parse_log", "original_string": "def parse_log(log_file):\n    \"\"\"Retrieves some statistics from a single Trimmomatic log file.\n\n    This function parses Trimmomatic's log file and stores some trimming\n    statistics in an :py:class:`OrderedDict` object. This object contains\n    the following keys:\n\n        - ``clean_len``: Total length after trimming.\n        - ``total_trim``: Total trimmed base pairs.\n        - ``total_trim_perc``: Total trimmed base pairs in percentage.\n        - ``5trim``: Total base pairs trimmed at 5' end.\n        - ``3trim``: Total base pairs trimmed at 3' end.\n\n    Parameters\n    ----------\n    log_file : str\n        Path to trimmomatic log file.\n\n    Returns\n    -------\n    x : :py:class:`OrderedDict`\n        Object storing the trimming statistics.\n\n    \"\"\"\n\n    template = OrderedDict([\n        # Total length after trimming\n        (\"clean_len\", 0),\n        # Total trimmed base pairs\n        (\"total_trim\", 0),\n        # Total trimmed base pairs in percentage\n        (\"total_trim_perc\", 0),\n        # Total trimmed at 5' end\n        (\"5trim\", 0),\n        # Total trimmed at 3' end\n        (\"3trim\", 0),\n        # Bad reads (completely trimmed)\n        (\"bad_reads\", 0)\n    ])\n\n    with open(log_file) as fh:\n\n        for line in fh:\n            # This will split the log fields into:\n            # 0. read length after trimming\n            # 1. amount trimmed from the start\n            # 2. last surviving base\n            # 3. amount trimmed from the end\n            fields = [int(x) for x in line.strip().split()[-4:]]\n\n            if not fields[0]:\n                template[\"bad_reads\"] += 1\n\n            template[\"5trim\"] += fields[1]\n            template[\"3trim\"] += fields[3]\n            template[\"total_trim\"] += fields[1] + fields[3]\n            template[\"clean_len\"] += fields[0]\n\n        total_len = template[\"clean_len\"] + template[\"total_trim\"]\n\n        if total_len:\n            template[\"total_trim_perc\"] = round(\n                (template[\"total_trim\"] / total_len) * 100, 2)\n        else:\n            template[\"total_trim_perc\"] = 0\n\n    return template", "language": "python", "code": "def parse_log(log_file):\n    \"\"\"Retrieves some statistics from a single Trimmomatic log file.\n\n    This function parses Trimmomatic's log file and stores some trimming\n    statistics in an :py:class:`OrderedDict` object. This object contains\n    the following keys:\n\n        - ``clean_len``: Total length after trimming.\n        - ``total_trim``: Total trimmed base pairs.\n        - ``total_trim_perc``: Total trimmed base pairs in percentage.\n        - ``5trim``: Total base pairs trimmed at 5' end.\n        - ``3trim``: Total base pairs trimmed at 3' end.\n\n    Parameters\n    ----------\n    log_file : str\n        Path to trimmomatic log file.\n\n    Returns\n    -------\n    x : :py:class:`OrderedDict`\n        Object storing the trimming statistics.\n\n    \"\"\"\n\n    template = OrderedDict([\n        # Total length after trimming\n        (\"clean_len\", 0),\n        # Total trimmed base pairs\n        (\"total_trim\", 0),\n        # Total trimmed base pairs in percentage\n        (\"total_trim_perc\", 0),\n        # Total trimmed at 5' end\n        (\"5trim\", 0),\n        # Total trimmed at 3' end\n        (\"3trim\", 0),\n        # Bad reads (completely trimmed)\n        (\"bad_reads\", 0)\n    ])\n\n    with open(log_file) as fh:\n\n        for line in fh:\n            # This will split the log fields into:\n            # 0. read length after trimming\n            # 1. amount trimmed from the start\n            # 2. last surviving base\n            # 3. amount trimmed from the end\n            fields = [int(x) for x in line.strip().split()[-4:]]\n\n            if not fields[0]:\n                template[\"bad_reads\"] += 1\n\n            template[\"5trim\"] += fields[1]\n            template[\"3trim\"] += fields[3]\n            template[\"total_trim\"] += fields[1] + fields[3]\n            template[\"clean_len\"] += fields[0]\n\n        total_len = template[\"clean_len\"] + template[\"total_trim\"]\n\n        if total_len:\n            template[\"total_trim_perc\"] = round(\n                (template[\"total_trim\"] / total_len) * 100, 2)\n        else:\n            template[\"total_trim_perc\"] = 0\n\n    return template", "code_tokens": ["def", "parse_log", "(", "log_file", ")", ":", "template", "=", "OrderedDict", "(", "[", "# Total length after trimming", "(", "\"clean_len\"", ",", "0", ")", ",", "# Total trimmed base pairs", "(", "\"total_trim\"", ",", "0", ")", ",", "# Total trimmed base pairs in percentage", "(", "\"total_trim_perc\"", ",", "0", ")", ",", "# Total trimmed at 5' end", "(", "\"5trim\"", ",", "0", ")", ",", "# Total trimmed at 3' end", "(", "\"3trim\"", ",", "0", ")", ",", "# Bad reads (completely trimmed)", "(", "\"bad_reads\"", ",", "0", ")", "]", ")", "with", "open", "(", "log_file", ")", "as", "fh", ":", "for", "line", "in", "fh", ":", "# This will split the log fields into:", "# 0. read length after trimming", "# 1. amount trimmed from the start", "# 2. last surviving base", "# 3. amount trimmed from the end", "fields", "=", "[", "int", "(", "x", ")", "for", "x", "in", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "[", "-", "4", ":", "]", "]", "if", "not", "fields", "[", "0", "]", ":", "template", "[", "\"bad_reads\"", "]", "+=", "1", "template", "[", "\"5trim\"", "]", "+=", "fields", "[", "1", "]", "template", "[", "\"3trim\"", "]", "+=", "fields", "[", "3", "]", "template", "[", "\"total_trim\"", "]", "+=", "fields", "[", "1", "]", "+", "fields", "[", "3", "]", "template", "[", "\"clean_len\"", "]", "+=", "fields", "[", "0", "]", "total_len", "=", "template", "[", "\"clean_len\"", "]", "+", "template", "[", "\"total_trim\"", "]", "if", "total_len", ":", "template", "[", "\"total_trim_perc\"", "]", "=", "round", "(", "(", "template", "[", "\"total_trim\"", "]", "/", "total_len", ")", "*", "100", ",", "2", ")", "else", ":", "template", "[", "\"total_trim_perc\"", "]", "=", "0", "return", "template"], "docstring": "Retrieves some statistics from a single Trimmomatic log file.\n\n    This function parses Trimmomatic's log file and stores some trimming\n    statistics in an :py:class:`OrderedDict` object. This object contains\n    the following keys:\n\n        - ``clean_len``: Total length after trimming.\n        - ``total_trim``: Total trimmed base pairs.\n        - ``total_trim_perc``: Total trimmed base pairs in percentage.\n        - ``5trim``: Total base pairs trimmed at 5' end.\n        - ``3trim``: Total base pairs trimmed at 3' end.\n\n    Parameters\n    ----------\n    log_file : str\n        Path to trimmomatic log file.\n\n    Returns\n    -------\n    x : :py:class:`OrderedDict`\n        Object storing the trimming statistics.", "docstring_tokens": ["Retrieves", "some", "statistics", "from", "a", "single", "Trimmomatic", "log", "file", "."], "sha": "fc3f4bddded1efc76006600016dc71a06dd908c0", "url": "https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/templates/trimmomatic.py#L113-L179", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/kubernetes/worker_configuration.py", "func_name": "WorkerConfiguration._get_security_context", "original_string": "def _get_security_context(self):\n        \"\"\"Defines the security context\"\"\"\n        security_context = {}\n\n        if self.kube_config.worker_run_as_user:\n            security_context['runAsUser'] = self.kube_config.worker_run_as_user\n\n        if self.kube_config.worker_fs_group:\n            security_context['fsGroup'] = self.kube_config.worker_fs_group\n\n        # set fs_group to 65533 if not explicitly specified and using git ssh keypair auth\n        if self.kube_config.git_ssh_key_secret_name and security_context.get('fsGroup') is None:\n            security_context['fsGroup'] = 65533\n\n        return security_context", "language": "python", "code": "def _get_security_context(self):\n        \"\"\"Defines the security context\"\"\"\n        security_context = {}\n\n        if self.kube_config.worker_run_as_user:\n            security_context['runAsUser'] = self.kube_config.worker_run_as_user\n\n        if self.kube_config.worker_fs_group:\n            security_context['fsGroup'] = self.kube_config.worker_fs_group\n\n        # set fs_group to 65533 if not explicitly specified and using git ssh keypair auth\n        if self.kube_config.git_ssh_key_secret_name and security_context.get('fsGroup') is None:\n            security_context['fsGroup'] = 65533\n\n        return security_context", "code_tokens": ["def", "_get_security_context", "(", "self", ")", ":", "security_context", "=", "{", "}", "if", "self", ".", "kube_config", ".", "worker_run_as_user", ":", "security_context", "[", "'runAsUser'", "]", "=", "self", ".", "kube_config", ".", "worker_run_as_user", "if", "self", ".", "kube_config", ".", "worker_fs_group", ":", "security_context", "[", "'fsGroup'", "]", "=", "self", ".", "kube_config", ".", "worker_fs_group", "# set fs_group to 65533 if not explicitly specified and using git ssh keypair auth", "if", "self", ".", "kube_config", ".", "git_ssh_key_secret_name", "and", "security_context", ".", "get", "(", "'fsGroup'", ")", "is", "None", ":", "security_context", "[", "'fsGroup'", "]", "=", "65533", "return", "security_context"], "docstring": "Defines the security context", "docstring_tokens": ["Defines", "the", "security", "context"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/kubernetes/worker_configuration.py#L188-L202", "partition": "test"}
{"repo": "gunyarakun/python-shogi", "path": "shogi/__init__.py", "func_name": "Board.is_game_over", "original_string": "def is_game_over(self):\n        '''\n        Checks if the game is over due to checkmate, stalemate or\n        fourfold repetition.\n        '''\n\n        # Stalemate or checkmate.\n        try:\n            next(self.generate_legal_moves().__iter__())\n        except StopIteration:\n            return True\n\n        # Fourfold repetition.\n        if self.is_fourfold_repetition():\n            return True\n\n        return False", "language": "python", "code": "def is_game_over(self):\n        '''\n        Checks if the game is over due to checkmate, stalemate or\n        fourfold repetition.\n        '''\n\n        # Stalemate or checkmate.\n        try:\n            next(self.generate_legal_moves().__iter__())\n        except StopIteration:\n            return True\n\n        # Fourfold repetition.\n        if self.is_fourfold_repetition():\n            return True\n\n        return False", "code_tokens": ["def", "is_game_over", "(", "self", ")", ":", "# Stalemate or checkmate.", "try", ":", "next", "(", "self", ".", "generate_legal_moves", "(", ")", ".", "__iter__", "(", ")", ")", "except", "StopIteration", ":", "return", "True", "# Fourfold repetition.", "if", "self", ".", "is_fourfold_repetition", "(", ")", ":", "return", "True", "return", "False"], "docstring": "Checks if the game is over due to checkmate, stalemate or\n        fourfold repetition.", "docstring_tokens": ["Checks", "if", "the", "game", "is", "over", "due", "to", "checkmate", "stalemate", "or", "fourfold", "repetition", "."], "sha": "137fe5f5e72251e8a97a1dba4a9b44b7c3c79914", "url": "https://github.com/gunyarakun/python-shogi/blob/137fe5f5e72251e8a97a1dba4a9b44b7c3c79914/shogi/__init__.py#L915-L931", "partition": "test"}
{"repo": "pmacosta/peng", "path": "peng/wave_functions.py", "func_name": "wvalue", "original_string": "def wvalue(wave, indep_var):\n    r\"\"\"\n    Return the dependent variable value at a given independent variable point.\n\n    If the independent variable point is not in the independent variable vector\n    the dependent variable value is obtained by linear interpolation\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param indep_var: Independent variable point for which the dependent\n                      variable is to be obtained\n    :type  indep_var: integer or float\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.wvalue\n\n    :raises:\n     * RuntimeError (Argument \\`indep_var\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Argument \\`indep_var\\` is not in the independent\n       variable vector range)\n\n    .. [[[end]]]\n    \"\"\"\n    close_min = np.isclose(indep_var, wave._indep_vector[0], FP_RTOL, FP_ATOL)\n    close_max = np.isclose(indep_var, wave._indep_vector[-1], FP_RTOL, FP_ATOL)\n    pexdoc.exh.addex(\n        ValueError,\n        \"Argument `indep_var` is not in the independent variable vector range\",\n        bool(\n            ((indep_var < wave._indep_vector[0]) and (not close_min))\n            or ((indep_var > wave._indep_vector[-1]) and (not close_max))\n        ),\n    )\n    if close_min:\n        return wave._dep_vector[0]\n    if close_max:\n        return wave._dep_vector[-1]\n    idx = np.searchsorted(wave._indep_vector, indep_var)\n    xdelta = wave._indep_vector[idx] - wave._indep_vector[idx - 1]\n    ydelta = wave._dep_vector[idx] - wave._dep_vector[idx - 1]\n    slope = ydelta / float(xdelta)\n    return wave._dep_vector[idx - 1] + slope * (indep_var - wave._indep_vector[idx - 1])", "language": "python", "code": "def wvalue(wave, indep_var):\n    r\"\"\"\n    Return the dependent variable value at a given independent variable point.\n\n    If the independent variable point is not in the independent variable vector\n    the dependent variable value is obtained by linear interpolation\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param indep_var: Independent variable point for which the dependent\n                      variable is to be obtained\n    :type  indep_var: integer or float\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.wvalue\n\n    :raises:\n     * RuntimeError (Argument \\`indep_var\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Argument \\`indep_var\\` is not in the independent\n       variable vector range)\n\n    .. [[[end]]]\n    \"\"\"\n    close_min = np.isclose(indep_var, wave._indep_vector[0], FP_RTOL, FP_ATOL)\n    close_max = np.isclose(indep_var, wave._indep_vector[-1], FP_RTOL, FP_ATOL)\n    pexdoc.exh.addex(\n        ValueError,\n        \"Argument `indep_var` is not in the independent variable vector range\",\n        bool(\n            ((indep_var < wave._indep_vector[0]) and (not close_min))\n            or ((indep_var > wave._indep_vector[-1]) and (not close_max))\n        ),\n    )\n    if close_min:\n        return wave._dep_vector[0]\n    if close_max:\n        return wave._dep_vector[-1]\n    idx = np.searchsorted(wave._indep_vector, indep_var)\n    xdelta = wave._indep_vector[idx] - wave._indep_vector[idx - 1]\n    ydelta = wave._dep_vector[idx] - wave._dep_vector[idx - 1]\n    slope = ydelta / float(xdelta)\n    return wave._dep_vector[idx - 1] + slope * (indep_var - wave._indep_vector[idx - 1])", "code_tokens": ["def", "wvalue", "(", "wave", ",", "indep_var", ")", ":", "close_min", "=", "np", ".", "isclose", "(", "indep_var", ",", "wave", ".", "_indep_vector", "[", "0", "]", ",", "FP_RTOL", ",", "FP_ATOL", ")", "close_max", "=", "np", ".", "isclose", "(", "indep_var", ",", "wave", ".", "_indep_vector", "[", "-", "1", "]", ",", "FP_RTOL", ",", "FP_ATOL", ")", "pexdoc", ".", "exh", ".", "addex", "(", "ValueError", ",", "\"Argument `indep_var` is not in the independent variable vector range\"", ",", "bool", "(", "(", "(", "indep_var", "<", "wave", ".", "_indep_vector", "[", "0", "]", ")", "and", "(", "not", "close_min", ")", ")", "or", "(", "(", "indep_var", ">", "wave", ".", "_indep_vector", "[", "-", "1", "]", ")", "and", "(", "not", "close_max", ")", ")", ")", ",", ")", "if", "close_min", ":", "return", "wave", ".", "_dep_vector", "[", "0", "]", "if", "close_max", ":", "return", "wave", ".", "_dep_vector", "[", "-", "1", "]", "idx", "=", "np", ".", "searchsorted", "(", "wave", ".", "_indep_vector", ",", "indep_var", ")", "xdelta", "=", "wave", ".", "_indep_vector", "[", "idx", "]", "-", "wave", ".", "_indep_vector", "[", "idx", "-", "1", "]", "ydelta", "=", "wave", ".", "_dep_vector", "[", "idx", "]", "-", "wave", ".", "_dep_vector", "[", "idx", "-", "1", "]", "slope", "=", "ydelta", "/", "float", "(", "xdelta", ")", "return", "wave", ".", "_dep_vector", "[", "idx", "-", "1", "]", "+", "slope", "*", "(", "indep_var", "-", "wave", ".", "_indep_vector", "[", "idx", "-", "1", "]", ")"], "docstring": "r\"\"\"\n    Return the dependent variable value at a given independent variable point.\n\n    If the independent variable point is not in the independent variable vector\n    the dependent variable value is obtained by linear interpolation\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param indep_var: Independent variable point for which the dependent\n                      variable is to be obtained\n    :type  indep_var: integer or float\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.wvalue\n\n    :raises:\n     * RuntimeError (Argument \\`indep_var\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Argument \\`indep_var\\` is not in the independent\n       variable vector range)\n\n    .. [[[end]]]", "docstring_tokens": ["r", "Return", "the", "dependent", "variable", "value", "at", "a", "given", "independent", "variable", "point", "."], "sha": "976935377adaa3de26fc5677aceb2cdfbd6f93a7", "url": "https://github.com/pmacosta/peng/blob/976935377adaa3de26fc5677aceb2cdfbd6f93a7/peng/wave_functions.py#L1990-L2038", "partition": "test"}
{"repo": "pmacosta/peng", "path": "peng/functions.py", "func_name": "_parse_expr", "original_string": "def _parse_expr(text, ldelim=\"(\", rdelim=\")\"):\n    \"\"\"Parse mathematical expression using PyParsing.\"\"\"\n    var = pyparsing.Word(pyparsing.alphas + \"_\", pyparsing.alphanums + \"_\")\n    point = pyparsing.Literal(\".\")\n    exp = pyparsing.CaselessLiteral(\"E\")\n    number = pyparsing.Combine(\n        pyparsing.Word(\"+-\" + pyparsing.nums, pyparsing.nums)\n        + pyparsing.Optional(point + pyparsing.Optional(pyparsing.Word(pyparsing.nums)))\n        + pyparsing.Optional(\n            exp + pyparsing.Word(\"+-\" + pyparsing.nums, pyparsing.nums)\n        )\n    )\n    atom = var | number\n    oplist = [\n        (pyparsing.Literal(\"**\"), 2, pyparsing.opAssoc.RIGHT),\n        (pyparsing.oneOf(\"+ - ~\"), 1, pyparsing.opAssoc.RIGHT),\n        (pyparsing.oneOf(\"* / // %\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.oneOf(\"+ -\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.oneOf(\"<< >>\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.Literal(\"&\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.Literal(\"^\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.Literal(\"|\"), 2, pyparsing.opAssoc.LEFT),\n    ]\n    # Get functions\n    expr = pyparsing.infixNotation(\n        atom, oplist, lpar=pyparsing.Suppress(ldelim), rpar=pyparsing.Suppress(rdelim)\n    )\n    return expr.parseString(text)[0]", "language": "python", "code": "def _parse_expr(text, ldelim=\"(\", rdelim=\")\"):\n    \"\"\"Parse mathematical expression using PyParsing.\"\"\"\n    var = pyparsing.Word(pyparsing.alphas + \"_\", pyparsing.alphanums + \"_\")\n    point = pyparsing.Literal(\".\")\n    exp = pyparsing.CaselessLiteral(\"E\")\n    number = pyparsing.Combine(\n        pyparsing.Word(\"+-\" + pyparsing.nums, pyparsing.nums)\n        + pyparsing.Optional(point + pyparsing.Optional(pyparsing.Word(pyparsing.nums)))\n        + pyparsing.Optional(\n            exp + pyparsing.Word(\"+-\" + pyparsing.nums, pyparsing.nums)\n        )\n    )\n    atom = var | number\n    oplist = [\n        (pyparsing.Literal(\"**\"), 2, pyparsing.opAssoc.RIGHT),\n        (pyparsing.oneOf(\"+ - ~\"), 1, pyparsing.opAssoc.RIGHT),\n        (pyparsing.oneOf(\"* / // %\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.oneOf(\"+ -\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.oneOf(\"<< >>\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.Literal(\"&\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.Literal(\"^\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.Literal(\"|\"), 2, pyparsing.opAssoc.LEFT),\n    ]\n    # Get functions\n    expr = pyparsing.infixNotation(\n        atom, oplist, lpar=pyparsing.Suppress(ldelim), rpar=pyparsing.Suppress(rdelim)\n    )\n    return expr.parseString(text)[0]", "code_tokens": ["def", "_parse_expr", "(", "text", ",", "ldelim", "=", "\"(\"", ",", "rdelim", "=", "\")\"", ")", ":", "var", "=", "pyparsing", ".", "Word", "(", "pyparsing", ".", "alphas", "+", "\"_\"", ",", "pyparsing", ".", "alphanums", "+", "\"_\"", ")", "point", "=", "pyparsing", ".", "Literal", "(", "\".\"", ")", "exp", "=", "pyparsing", ".", "CaselessLiteral", "(", "\"E\"", ")", "number", "=", "pyparsing", ".", "Combine", "(", "pyparsing", ".", "Word", "(", "\"+-\"", "+", "pyparsing", ".", "nums", ",", "pyparsing", ".", "nums", ")", "+", "pyparsing", ".", "Optional", "(", "point", "+", "pyparsing", ".", "Optional", "(", "pyparsing", ".", "Word", "(", "pyparsing", ".", "nums", ")", ")", ")", "+", "pyparsing", ".", "Optional", "(", "exp", "+", "pyparsing", ".", "Word", "(", "\"+-\"", "+", "pyparsing", ".", "nums", ",", "pyparsing", ".", "nums", ")", ")", ")", "atom", "=", "var", "|", "number", "oplist", "=", "[", "(", "pyparsing", ".", "Literal", "(", "\"**\"", ")", ",", "2", ",", "pyparsing", ".", "opAssoc", ".", "RIGHT", ")", ",", "(", "pyparsing", ".", "oneOf", "(", "\"+ - ~\"", ")", ",", "1", ",", "pyparsing", ".", "opAssoc", ".", "RIGHT", ")", ",", "(", "pyparsing", ".", "oneOf", "(", "\"* / // %\"", ")", ",", "2", ",", "pyparsing", ".", "opAssoc", ".", "LEFT", ")", ",", "(", "pyparsing", ".", "oneOf", "(", "\"+ -\"", ")", ",", "2", ",", "pyparsing", ".", "opAssoc", ".", "LEFT", ")", ",", "(", "pyparsing", ".", "oneOf", "(", "\"<< >>\"", ")", ",", "2", ",", "pyparsing", ".", "opAssoc", ".", "LEFT", ")", ",", "(", "pyparsing", ".", "Literal", "(", "\"&\"", ")", ",", "2", ",", "pyparsing", ".", "opAssoc", ".", "LEFT", ")", ",", "(", "pyparsing", ".", "Literal", "(", "\"^\"", ")", ",", "2", ",", "pyparsing", ".", "opAssoc", ".", "LEFT", ")", ",", "(", "pyparsing", ".", "Literal", "(", "\"|\"", ")", ",", "2", ",", "pyparsing", ".", "opAssoc", ".", "LEFT", ")", ",", "]", "# Get functions", "expr", "=", "pyparsing", ".", "infixNotation", "(", "atom", ",", "oplist", ",", "lpar", "=", "pyparsing", ".", "Suppress", "(", "ldelim", ")", ",", "rpar", "=", "pyparsing", ".", "Suppress", "(", "rdelim", ")", ")", "return", "expr", ".", "parseString", "(", "text", ")", "[", "0", "]"], "docstring": "Parse mathematical expression using PyParsing.", "docstring_tokens": ["Parse", "mathematical", "expression", "using", "PyParsing", "."], "sha": "976935377adaa3de26fc5677aceb2cdfbd6f93a7", "url": "https://github.com/pmacosta/peng/blob/976935377adaa3de26fc5677aceb2cdfbd6f93a7/peng/functions.py#L176-L203", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/azure_fileshare_hook.py", "func_name": "AzureFileShareHook.check_for_directory", "original_string": "def check_for_directory(self, share_name, directory_name, **kwargs):\n        \"\"\"\n        Check if a directory exists on Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.exists()` takes.\n        :type kwargs: object\n        :return: True if the file exists, False otherwise.\n        :rtype: bool\n        \"\"\"\n        return self.connection.exists(share_name, directory_name,\n                                      **kwargs)", "language": "python", "code": "def check_for_directory(self, share_name, directory_name, **kwargs):\n        \"\"\"\n        Check if a directory exists on Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.exists()` takes.\n        :type kwargs: object\n        :return: True if the file exists, False otherwise.\n        :rtype: bool\n        \"\"\"\n        return self.connection.exists(share_name, directory_name,\n                                      **kwargs)", "code_tokens": ["def", "check_for_directory", "(", "self", ",", "share_name", ",", "directory_name", ",", "*", "*", "kwargs", ")", ":", "return", "self", ".", "connection", ".", "exists", "(", "share_name", ",", "directory_name", ",", "*", "*", "kwargs", ")"], "docstring": "Check if a directory exists on Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.exists()` takes.\n        :type kwargs: object\n        :return: True if the file exists, False otherwise.\n        :rtype: bool", "docstring_tokens": ["Check", "if", "a", "directory", "exists", "on", "Azure", "File", "Share", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L47-L62", "partition": "test"}
{"repo": "streamlink/streamlink", "path": "src/streamlink/stream/streamprocess.py", "func_name": "StreamProcess.spawn", "original_string": "def spawn(self, parameters=None, arguments=None, stderr=None, timeout=None, short_option_prefix=\"-\", long_option_prefix=\"--\"):\n        \"\"\"\n        Spawn the process defined in `cmd`\n\n        parameters is converted to options the short and long option prefixes\n        if a list is given as the value, the parameter is repeated with each\n        value\n\n        If timeout is set the spawn will block until the process returns or\n        the timeout expires.\n\n        :param parameters: optional parameters\n        :param arguments: positional arguments\n        :param stderr: where to redirect stderr to\n        :param timeout: timeout for short lived process\n        :param long_option_prefix: option prefix, default -\n        :param short_option_prefix: long option prefix, default --\n        :return: spawned process\n        \"\"\"\n        stderr = stderr or self.stderr\n        cmd = self.bake(self._check_cmd(), parameters, arguments, short_option_prefix, long_option_prefix)\n        log.debug(\"Spawning command: {0}\", subprocess.list2cmdline(cmd))\n\n        try:\n            process = subprocess.Popen(cmd, stderr=stderr, stdout=subprocess.PIPE)\n        except (OSError, IOError) as err:\n            raise StreamError(\"Failed to start process: {0} ({1})\".format(self._check_cmd(), str(err)))\n\n        if timeout:\n            elapsed = 0\n            while elapsed < timeout and not process.poll():\n                time.sleep(0.25)\n                elapsed += 0.25\n\n            # kill after the timeout has expired and the process still hasn't ended\n            if not process.poll():\n                try:\n                    log.debug(\"Process timeout expired ({0}s), killing process\".format(timeout))\n                    process.kill()\n                except Exception:\n                    pass\n\n            process.wait()\n\n        return process", "language": "python", "code": "def spawn(self, parameters=None, arguments=None, stderr=None, timeout=None, short_option_prefix=\"-\", long_option_prefix=\"--\"):\n        \"\"\"\n        Spawn the process defined in `cmd`\n\n        parameters is converted to options the short and long option prefixes\n        if a list is given as the value, the parameter is repeated with each\n        value\n\n        If timeout is set the spawn will block until the process returns or\n        the timeout expires.\n\n        :param parameters: optional parameters\n        :param arguments: positional arguments\n        :param stderr: where to redirect stderr to\n        :param timeout: timeout for short lived process\n        :param long_option_prefix: option prefix, default -\n        :param short_option_prefix: long option prefix, default --\n        :return: spawned process\n        \"\"\"\n        stderr = stderr or self.stderr\n        cmd = self.bake(self._check_cmd(), parameters, arguments, short_option_prefix, long_option_prefix)\n        log.debug(\"Spawning command: {0}\", subprocess.list2cmdline(cmd))\n\n        try:\n            process = subprocess.Popen(cmd, stderr=stderr, stdout=subprocess.PIPE)\n        except (OSError, IOError) as err:\n            raise StreamError(\"Failed to start process: {0} ({1})\".format(self._check_cmd(), str(err)))\n\n        if timeout:\n            elapsed = 0\n            while elapsed < timeout and not process.poll():\n                time.sleep(0.25)\n                elapsed += 0.25\n\n            # kill after the timeout has expired and the process still hasn't ended\n            if not process.poll():\n                try:\n                    log.debug(\"Process timeout expired ({0}s), killing process\".format(timeout))\n                    process.kill()\n                except Exception:\n                    pass\n\n            process.wait()\n\n        return process", "code_tokens": ["def", "spawn", "(", "self", ",", "parameters", "=", "None", ",", "arguments", "=", "None", ",", "stderr", "=", "None", ",", "timeout", "=", "None", ",", "short_option_prefix", "=", "\"-\"", ",", "long_option_prefix", "=", "\"--\"", ")", ":", "stderr", "=", "stderr", "or", "self", ".", "stderr", "cmd", "=", "self", ".", "bake", "(", "self", ".", "_check_cmd", "(", ")", ",", "parameters", ",", "arguments", ",", "short_option_prefix", ",", "long_option_prefix", ")", "log", ".", "debug", "(", "\"Spawning command: {0}\"", ",", "subprocess", ".", "list2cmdline", "(", "cmd", ")", ")", "try", ":", "process", "=", "subprocess", ".", "Popen", "(", "cmd", ",", "stderr", "=", "stderr", ",", "stdout", "=", "subprocess", ".", "PIPE", ")", "except", "(", "OSError", ",", "IOError", ")", "as", "err", ":", "raise", "StreamError", "(", "\"Failed to start process: {0} ({1})\"", ".", "format", "(", "self", ".", "_check_cmd", "(", ")", ",", "str", "(", "err", ")", ")", ")", "if", "timeout", ":", "elapsed", "=", "0", "while", "elapsed", "<", "timeout", "and", "not", "process", ".", "poll", "(", ")", ":", "time", ".", "sleep", "(", "0.25", ")", "elapsed", "+=", "0.25", "# kill after the timeout has expired and the process still hasn't ended", "if", "not", "process", ".", "poll", "(", ")", ":", "try", ":", "log", ".", "debug", "(", "\"Process timeout expired ({0}s), killing process\"", ".", "format", "(", "timeout", ")", ")", "process", ".", "kill", "(", ")", "except", "Exception", ":", "pass", "process", ".", "wait", "(", ")", "return", "process"], "docstring": "Spawn the process defined in `cmd`\n\n        parameters is converted to options the short and long option prefixes\n        if a list is given as the value, the parameter is repeated with each\n        value\n\n        If timeout is set the spawn will block until the process returns or\n        the timeout expires.\n\n        :param parameters: optional parameters\n        :param arguments: positional arguments\n        :param stderr: where to redirect stderr to\n        :param timeout: timeout for short lived process\n        :param long_option_prefix: option prefix, default -\n        :param short_option_prefix: long option prefix, default --\n        :return: spawned process", "docstring_tokens": ["Spawn", "the", "process", "defined", "in", "cmd"], "sha": "c8ed1daff14ac03195870238b9b900c1109dd5c1", "url": "https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink/stream/streamprocess.py#L107-L151", "partition": "test"}
{"repo": "treycucco/pyebnf", "path": "pyebnf/compiler.py", "func_name": "Compiler._ast_option_group_to_code", "original_string": "def _ast_option_group_to_code(self, option_group, **kwargs):\n    \"\"\"Convert an AST option group to python source code.\"\"\"\n    lines = [\"option(\"]\n    lines.extend(self._indent(self._ast_to_code(option_group.expression)))\n    lines.append(\")\")\n    return lines", "language": "python", "code": "def _ast_option_group_to_code(self, option_group, **kwargs):\n    \"\"\"Convert an AST option group to python source code.\"\"\"\n    lines = [\"option(\"]\n    lines.extend(self._indent(self._ast_to_code(option_group.expression)))\n    lines.append(\")\")\n    return lines", "code_tokens": ["def", "_ast_option_group_to_code", "(", "self", ",", "option_group", ",", "*", "*", "kwargs", ")", ":", "lines", "=", "[", "\"option(\"", "]", "lines", ".", "extend", "(", "self", ".", "_indent", "(", "self", ".", "_ast_to_code", "(", "option_group", ".", "expression", ")", ")", ")", "lines", ".", "append", "(", "\")\"", ")", "return", "lines"], "docstring": "Convert an AST option group to python source code.", "docstring_tokens": ["Convert", "an", "AST", "option", "group", "to", "python", "source", "code", "."], "sha": "3634ddabbe5d73508bcc20f4a591f86a46634e1d", "url": "https://github.com/treycucco/pyebnf/blob/3634ddabbe5d73508bcc20f4a591f86a46634e1d/pyebnf/compiler.py#L360-L365", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/runtime.py", "func_name": "Var.find_safe", "original_string": "def find_safe(ns_qualified_sym: sym.Symbol) -> \"Var\":\n        \"\"\"Return the Var currently bound to the name in the namespace specified\n        by `ns_qualified_sym`. If no Var is bound to that name, raise an exception.\n\n        This is a utility method to return useful debugging information when code\n        refers to an invalid symbol at runtime.\"\"\"\n        v = Var.find(ns_qualified_sym)\n        if v is None:\n            raise RuntimeException(\n                f\"Unable to resolve symbol {ns_qualified_sym} in this context\"\n            )\n        return v", "language": "python", "code": "def find_safe(ns_qualified_sym: sym.Symbol) -> \"Var\":\n        \"\"\"Return the Var currently bound to the name in the namespace specified\n        by `ns_qualified_sym`. If no Var is bound to that name, raise an exception.\n\n        This is a utility method to return useful debugging information when code\n        refers to an invalid symbol at runtime.\"\"\"\n        v = Var.find(ns_qualified_sym)\n        if v is None:\n            raise RuntimeException(\n                f\"Unable to resolve symbol {ns_qualified_sym} in this context\"\n            )\n        return v", "code_tokens": ["def", "find_safe", "(", "ns_qualified_sym", ":", "sym", ".", "Symbol", ")", "->", "\"Var\"", ":", "v", "=", "Var", ".", "find", "(", "ns_qualified_sym", ")", "if", "v", "is", "None", ":", "raise", "RuntimeException", "(", "f\"Unable to resolve symbol {ns_qualified_sym} in this context\"", ")", "return", "v"], "docstring": "Return the Var currently bound to the name in the namespace specified\n        by `ns_qualified_sym`. If no Var is bound to that name, raise an exception.\n\n        This is a utility method to return useful debugging information when code\n        refers to an invalid symbol at runtime.", "docstring_tokens": ["Return", "the", "Var", "currently", "bound", "to", "the", "name", "in", "the", "namespace", "specified", "by", "ns_qualified_sym", ".", "If", "no", "Var", "is", "bound", "to", "that", "name", "raise", "an", "exception", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/runtime.py#L273-L284", "partition": "test"}
{"repo": "urda/nistbeacon", "path": "nistbeacon/nistbeaconcrypto.py", "func_name": "NistBeaconCrypto.get_hash", "original_string": "def get_hash(\n            cls,\n            version: str,\n            frequency: int,\n            timestamp: int,\n            seed_value: str,\n            prev_output: str,\n            status_code: str,\n    ) -> SHA512Hash:\n        \"\"\"\n        Given required properties from a NistBeaconValue,\n        compute the SHA512Hash object.\n\n        :param version: NistBeaconValue.version\n        :param frequency: NistBeaconValue.frequency\n        :param timestamp: NistBeaconValue.timestamp\n        :param seed_value: NistBeaconValue.seed_value\n        :param prev_output: NistBeaconValue.previous_output_value\n        :param status_code: NistBeaconValue.status_code\n\n        :return: SHA512 Hash for NistBeaconValue signature verification\n        \"\"\"\n        return SHA512.new(\n            version.encode() +\n            struct.pack(\n                '>1I1Q64s64s1I',\n                frequency,\n                timestamp,\n                binascii.a2b_hex(seed_value),\n                binascii.a2b_hex(prev_output),\n                int(status_code),\n            )\n        )", "language": "python", "code": "def get_hash(\n            cls,\n            version: str,\n            frequency: int,\n            timestamp: int,\n            seed_value: str,\n            prev_output: str,\n            status_code: str,\n    ) -> SHA512Hash:\n        \"\"\"\n        Given required properties from a NistBeaconValue,\n        compute the SHA512Hash object.\n\n        :param version: NistBeaconValue.version\n        :param frequency: NistBeaconValue.frequency\n        :param timestamp: NistBeaconValue.timestamp\n        :param seed_value: NistBeaconValue.seed_value\n        :param prev_output: NistBeaconValue.previous_output_value\n        :param status_code: NistBeaconValue.status_code\n\n        :return: SHA512 Hash for NistBeaconValue signature verification\n        \"\"\"\n        return SHA512.new(\n            version.encode() +\n            struct.pack(\n                '>1I1Q64s64s1I',\n                frequency,\n                timestamp,\n                binascii.a2b_hex(seed_value),\n                binascii.a2b_hex(prev_output),\n                int(status_code),\n            )\n        )", "code_tokens": ["def", "get_hash", "(", "cls", ",", "version", ":", "str", ",", "frequency", ":", "int", ",", "timestamp", ":", "int", ",", "seed_value", ":", "str", ",", "prev_output", ":", "str", ",", "status_code", ":", "str", ",", ")", "->", "SHA512Hash", ":", "return", "SHA512", ".", "new", "(", "version", ".", "encode", "(", ")", "+", "struct", ".", "pack", "(", "'>1I1Q64s64s1I'", ",", "frequency", ",", "timestamp", ",", "binascii", ".", "a2b_hex", "(", "seed_value", ")", ",", "binascii", ".", "a2b_hex", "(", "prev_output", ")", ",", "int", "(", "status_code", ")", ",", ")", ")"], "docstring": "Given required properties from a NistBeaconValue,\n        compute the SHA512Hash object.\n\n        :param version: NistBeaconValue.version\n        :param frequency: NistBeaconValue.frequency\n        :param timestamp: NistBeaconValue.timestamp\n        :param seed_value: NistBeaconValue.seed_value\n        :param prev_output: NistBeaconValue.previous_output_value\n        :param status_code: NistBeaconValue.status_code\n\n        :return: SHA512 Hash for NistBeaconValue signature verification", "docstring_tokens": ["Given", "required", "properties", "from", "a", "NistBeaconValue", "compute", "the", "SHA512Hash", "object", "."], "sha": "43e0c3d1e186e71387f072daf98911abb14469dd", "url": "https://github.com/urda/nistbeacon/blob/43e0c3d1e186e71387f072daf98911abb14469dd/nistbeacon/nistbeaconcrypto.py#L114-L146", "partition": "test"}
{"repo": "boto/s3transfer", "path": "s3transfer/utils.py", "func_name": "TaskSemaphore.release", "original_string": "def release(self, tag, acquire_token):\n        \"\"\"Release the semaphore\n\n        :param tag: A tag identifying what is releasing the semaphore\n        :param acquire_token:  The token returned from when the semaphore was\n            acquired. Note that this is not really needed to directly use this\n            class but is needed for API compatibility with the\n            SlidingWindowSemaphore implementation.\n        \"\"\"\n        logger.debug(\"Releasing acquire %s/%s\" % (tag, acquire_token))\n        self._semaphore.release()", "language": "python", "code": "def release(self, tag, acquire_token):\n        \"\"\"Release the semaphore\n\n        :param tag: A tag identifying what is releasing the semaphore\n        :param acquire_token:  The token returned from when the semaphore was\n            acquired. Note that this is not really needed to directly use this\n            class but is needed for API compatibility with the\n            SlidingWindowSemaphore implementation.\n        \"\"\"\n        logger.debug(\"Releasing acquire %s/%s\" % (tag, acquire_token))\n        self._semaphore.release()", "code_tokens": ["def", "release", "(", "self", ",", "tag", ",", "acquire_token", ")", ":", "logger", ".", "debug", "(", "\"Releasing acquire %s/%s\"", "%", "(", "tag", ",", "acquire_token", ")", ")", "self", ".", "_semaphore", ".", "release", "(", ")"], "docstring": "Release the semaphore\n\n        :param tag: A tag identifying what is releasing the semaphore\n        :param acquire_token:  The token returned from when the semaphore was\n            acquired. Note that this is not really needed to directly use this\n            class but is needed for API compatibility with the\n            SlidingWindowSemaphore implementation.", "docstring_tokens": ["Release", "the", "semaphore"], "sha": "2aead638c8385d8ae0b1756b2de17e8fad45fffa", "url": "https://github.com/boto/s3transfer/blob/2aead638c8385d8ae0b1756b2de17e8fad45fffa/s3transfer/utils.py#L578-L588", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/services/server/auth/utils.py", "func_name": "get_client_identity_from_certificate", "original_string": "def get_client_identity_from_certificate(certificate):\n    \"\"\"\n    Given an X.509 certificate, extract and return the client identity.\n    \"\"\"\n    client_ids = get_common_names_from_certificate(certificate)\n\n    if len(client_ids) > 0:\n        if len(client_ids) > 1:\n            raise exceptions.PermissionDenied(\n                \"Multiple client identities found.\"\n            )\n        return client_ids[0]\n    else:\n        raise exceptions.PermissionDenied(\n            \"The certificate does not define any subject common names. \"\n            \"Client identity unavailable.\"\n        )", "language": "python", "code": "def get_client_identity_from_certificate(certificate):\n    \"\"\"\n    Given an X.509 certificate, extract and return the client identity.\n    \"\"\"\n    client_ids = get_common_names_from_certificate(certificate)\n\n    if len(client_ids) > 0:\n        if len(client_ids) > 1:\n            raise exceptions.PermissionDenied(\n                \"Multiple client identities found.\"\n            )\n        return client_ids[0]\n    else:\n        raise exceptions.PermissionDenied(\n            \"The certificate does not define any subject common names. \"\n            \"Client identity unavailable.\"\n        )", "code_tokens": ["def", "get_client_identity_from_certificate", "(", "certificate", ")", ":", "client_ids", "=", "get_common_names_from_certificate", "(", "certificate", ")", "if", "len", "(", "client_ids", ")", ">", "0", ":", "if", "len", "(", "client_ids", ")", ">", "1", ":", "raise", "exceptions", ".", "PermissionDenied", "(", "\"Multiple client identities found.\"", ")", "return", "client_ids", "[", "0", "]", "else", ":", "raise", "exceptions", ".", "PermissionDenied", "(", "\"The certificate does not define any subject common names. \"", "\"Client identity unavailable.\"", ")"], "docstring": "Given an X.509 certificate, extract and return the client identity.", "docstring_tokens": ["Given", "an", "X", ".", "509", "certificate", "extract", "and", "return", "the", "client", "identity", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/services/server/auth/utils.py#L59-L75", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/aws_athena_hook.py", "func_name": "AWSAthenaHook.get_conn", "original_string": "def get_conn(self):\n        \"\"\"\n        check if aws conn exists already or create one and return it\n\n        :return: boto3 session\n        \"\"\"\n        if not self.conn:\n            self.conn = self.get_client_type('athena')\n        return self.conn", "language": "python", "code": "def get_conn(self):\n        \"\"\"\n        check if aws conn exists already or create one and return it\n\n        :return: boto3 session\n        \"\"\"\n        if not self.conn:\n            self.conn = self.get_client_type('athena')\n        return self.conn", "code_tokens": ["def", "get_conn", "(", "self", ")", ":", "if", "not", "self", ".", "conn", ":", "self", ".", "conn", "=", "self", ".", "get_client_type", "(", "'athena'", ")", "return", "self", ".", "conn"], "docstring": "check if aws conn exists already or create one and return it\n\n        :return: boto3 session", "docstring_tokens": ["check", "if", "aws", "conn", "exists", "already", "or", "create", "one", "and", "return", "it"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_athena_hook.py#L43-L51", "partition": "test"}
{"repo": "gawel/panoramisk", "path": "panoramisk/fast_agi.py", "func_name": "Application.handler", "original_string": "def handler(self, reader, writer):\n        \"\"\"AsyncIO coroutine handler to launch socket listening.\n\n        :Example:\n\n        ::\n\n            @asyncio.coroutine\n            def start(request):\n                print('Receive a FastAGI request')\n                print(['AGI variables:', request.headers])\n\n            fa_app = Application()\n            fa_app.add_route('calls/start', start)\n            coro = asyncio.start_server(fa_app.handler, '0.0.0.0', 4574)\n            server = loop.run_until_complete(coro)\n\n        See https://docs.python.org/3/library/asyncio-stream.html\n        \"\"\"\n        buffer = b''\n        while b'\\n\\n' not in buffer:\n            buffer += yield from reader.read(self.buf_size)\n        lines = buffer[:-2].decode(self.default_encoding).split('\\n')\n        headers = OrderedDict([\n            line.split(': ', 1) for line in lines if ': ' in line\n        ])\n\n        agi_network_script = headers.get('agi_network_script')\n        log.info('Received FastAGI request from %r for \"%s\" route',\n                 writer.get_extra_info('peername'), agi_network_script)\n        log.debug(\"Asterisk Headers: %r\", headers)\n\n        if agi_network_script is not None:\n            route = self._route.get(agi_network_script)\n            if route is not None:\n                request = Request(app=self,\n                                  headers=headers,\n                                  reader=reader, writer=writer,\n                                  encoding=self.default_encoding)\n                try:\n                    yield from route(request)\n                except BaseException:\n                    log.exception(\n                        'An exception has been raised for the request \"%s\"',\n                        agi_network_script\n                    )\n            else:\n                log.error('No route for the request \"%s\"', agi_network_script)\n        else:\n            log.error('No agi_network_script header for the request')\n        log.debug(\"Closing client socket\")\n        writer.close()", "language": "python", "code": "def handler(self, reader, writer):\n        \"\"\"AsyncIO coroutine handler to launch socket listening.\n\n        :Example:\n\n        ::\n\n            @asyncio.coroutine\n            def start(request):\n                print('Receive a FastAGI request')\n                print(['AGI variables:', request.headers])\n\n            fa_app = Application()\n            fa_app.add_route('calls/start', start)\n            coro = asyncio.start_server(fa_app.handler, '0.0.0.0', 4574)\n            server = loop.run_until_complete(coro)\n\n        See https://docs.python.org/3/library/asyncio-stream.html\n        \"\"\"\n        buffer = b''\n        while b'\\n\\n' not in buffer:\n            buffer += yield from reader.read(self.buf_size)\n        lines = buffer[:-2].decode(self.default_encoding).split('\\n')\n        headers = OrderedDict([\n            line.split(': ', 1) for line in lines if ': ' in line\n        ])\n\n        agi_network_script = headers.get('agi_network_script')\n        log.info('Received FastAGI request from %r for \"%s\" route',\n                 writer.get_extra_info('peername'), agi_network_script)\n        log.debug(\"Asterisk Headers: %r\", headers)\n\n        if agi_network_script is not None:\n            route = self._route.get(agi_network_script)\n            if route is not None:\n                request = Request(app=self,\n                                  headers=headers,\n                                  reader=reader, writer=writer,\n                                  encoding=self.default_encoding)\n                try:\n                    yield from route(request)\n                except BaseException:\n                    log.exception(\n                        'An exception has been raised for the request \"%s\"',\n                        agi_network_script\n                    )\n            else:\n                log.error('No route for the request \"%s\"', agi_network_script)\n        else:\n            log.error('No agi_network_script header for the request')\n        log.debug(\"Closing client socket\")\n        writer.close()", "code_tokens": ["def", "handler", "(", "self", ",", "reader", ",", "writer", ")", ":", "buffer", "=", "b''", "while", "b'\\n\\n'", "not", "in", "buffer", ":", "buffer", "+=", "yield", "from", "reader", ".", "read", "(", "self", ".", "buf_size", ")", "lines", "=", "buffer", "[", ":", "-", "2", "]", ".", "decode", "(", "self", ".", "default_encoding", ")", ".", "split", "(", "'\\n'", ")", "headers", "=", "OrderedDict", "(", "[", "line", ".", "split", "(", "': '", ",", "1", ")", "for", "line", "in", "lines", "if", "': '", "in", "line", "]", ")", "agi_network_script", "=", "headers", ".", "get", "(", "'agi_network_script'", ")", "log", ".", "info", "(", "'Received FastAGI request from %r for \"%s\" route'", ",", "writer", ".", "get_extra_info", "(", "'peername'", ")", ",", "agi_network_script", ")", "log", ".", "debug", "(", "\"Asterisk Headers: %r\"", ",", "headers", ")", "if", "agi_network_script", "is", "not", "None", ":", "route", "=", "self", ".", "_route", ".", "get", "(", "agi_network_script", ")", "if", "route", "is", "not", "None", ":", "request", "=", "Request", "(", "app", "=", "self", ",", "headers", "=", "headers", ",", "reader", "=", "reader", ",", "writer", "=", "writer", ",", "encoding", "=", "self", ".", "default_encoding", ")", "try", ":", "yield", "from", "route", "(", "request", ")", "except", "BaseException", ":", "log", ".", "exception", "(", "'An exception has been raised for the request \"%s\"'", ",", "agi_network_script", ")", "else", ":", "log", ".", "error", "(", "'No route for the request \"%s\"'", ",", "agi_network_script", ")", "else", ":", "log", ".", "error", "(", "'No agi_network_script header for the request'", ")", "log", ".", "debug", "(", "\"Closing client socket\"", ")", "writer", ".", "close", "(", ")"], "docstring": "AsyncIO coroutine handler to launch socket listening.\n\n        :Example:\n\n        ::\n\n            @asyncio.coroutine\n            def start(request):\n                print('Receive a FastAGI request')\n                print(['AGI variables:', request.headers])\n\n            fa_app = Application()\n            fa_app.add_route('calls/start', start)\n            coro = asyncio.start_server(fa_app.handler, '0.0.0.0', 4574)\n            server = loop.run_until_complete(coro)\n\n        See https://docs.python.org/3/library/asyncio-stream.html", "docstring_tokens": ["AsyncIO", "coroutine", "handler", "to", "launch", "socket", "listening", "."], "sha": "2ccb5d18be28a8e8f444dc0cd3a3bfb59aa19a8e", "url": "https://github.com/gawel/panoramisk/blob/2ccb5d18be28a8e8f444dc0cd3a3bfb59aa19a8e/panoramisk/fast_agi.py#L133-L184", "partition": "test"}
{"repo": "willkg/markus", "path": "markus/backends/logging.py", "func_name": "LoggingMetrics.histogram", "original_string": "def histogram(self, stat, value, tags=None):\n        \"\"\"Report a histogram.\"\"\"\n        self._log('histogram', stat, value, tags)", "language": "python", "code": "def histogram(self, stat, value, tags=None):\n        \"\"\"Report a histogram.\"\"\"\n        self._log('histogram', stat, value, tags)", "code_tokens": ["def", "histogram", "(", "self", ",", "stat", ",", "value", ",", "tags", "=", "None", ")", ":", "self", ".", "_log", "(", "'histogram'", ",", "stat", ",", "value", ",", "tags", ")"], "docstring": "Report a histogram.", "docstring_tokens": ["Report", "a", "histogram", "."], "sha": "0cfbe67fb7ccfa7488b0120d21ddc0cdc1f8ed33", "url": "https://github.com/willkg/markus/blob/0cfbe67fb7ccfa7488b0120d21ddc0cdc1f8ed33/markus/backends/logging.py#L86-L88", "partition": "test"}
{"repo": "its-rigs/Trolly", "path": "trolly/checklist.py", "func_name": "Checklist.add_item", "original_string": "def add_item(self, query_params=None):\n        '''\n        Add an item to this checklist. Returns a dictionary of values of new\n        item.\n        '''\n        return self.fetch_json(\n            uri_path=self.base_uri + '/checkItems',\n            http_method='POST',\n            query_params=query_params or {}\n        )", "language": "python", "code": "def add_item(self, query_params=None):\n        '''\n        Add an item to this checklist. Returns a dictionary of values of new\n        item.\n        '''\n        return self.fetch_json(\n            uri_path=self.base_uri + '/checkItems',\n            http_method='POST',\n            query_params=query_params or {}\n        )", "code_tokens": ["def", "add_item", "(", "self", ",", "query_params", "=", "None", ")", ":", "return", "self", ".", "fetch_json", "(", "uri_path", "=", "self", ".", "base_uri", "+", "'/checkItems'", ",", "http_method", "=", "'POST'", ",", "query_params", "=", "query_params", "or", "{", "}", ")"], "docstring": "Add an item to this checklist. Returns a dictionary of values of new\n        item.", "docstring_tokens": ["Add", "an", "item", "to", "this", "checklist", ".", "Returns", "a", "dictionary", "of", "values", "of", "new", "item", "."], "sha": "483dc94c352df40dc05ead31820b059b2545cf82", "url": "https://github.com/its-rigs/Trolly/blob/483dc94c352df40dc05ead31820b059b2545cf82/trolly/checklist.py#L71-L80", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/pyreverse/diadefslib.py", "func_name": "DefaultDiadefGenerator.leave_project", "original_string": "def leave_project(self, node):  # pylint: disable=unused-argument\n        \"\"\"leave the pyreverse.utils.Project node\n\n        return the generated diagram definition\n        \"\"\"\n        if self.pkgdiagram:\n            return self.pkgdiagram, self.classdiagram\n        return (self.classdiagram,)", "language": "python", "code": "def leave_project(self, node):  # pylint: disable=unused-argument\n        \"\"\"leave the pyreverse.utils.Project node\n\n        return the generated diagram definition\n        \"\"\"\n        if self.pkgdiagram:\n            return self.pkgdiagram, self.classdiagram\n        return (self.classdiagram,)", "code_tokens": ["def", "leave_project", "(", "self", ",", "node", ")", ":", "# pylint: disable=unused-argument", "if", "self", ".", "pkgdiagram", ":", "return", "self", ".", "pkgdiagram", ",", "self", ".", "classdiagram", "return", "(", "self", ".", "classdiagram", ",", ")"], "docstring": "leave the pyreverse.utils.Project node\n\n        return the generated diagram definition", "docstring_tokens": ["leave", "the", "pyreverse", ".", "utils", ".", "Project", "node"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/pyreverse/diadefslib.py#L145-L152", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/utils/dag_processing.py", "func_name": "DagFileProcessorAgent.terminate", "original_string": "def terminate(self):\n        \"\"\"\n        Send termination signal to DAG parsing processor manager\n        and expect it to terminate all DAG file processors.\n        \"\"\"\n        self.log.info(\"Sending termination message to manager.\")\n        self._child_signal_conn.send(DagParsingSignal.TERMINATE_MANAGER)", "language": "python", "code": "def terminate(self):\n        \"\"\"\n        Send termination signal to DAG parsing processor manager\n        and expect it to terminate all DAG file processors.\n        \"\"\"\n        self.log.info(\"Sending termination message to manager.\")\n        self._child_signal_conn.send(DagParsingSignal.TERMINATE_MANAGER)", "code_tokens": ["def", "terminate", "(", "self", ")", ":", "self", ".", "log", ".", "info", "(", "\"Sending termination message to manager.\"", ")", "self", ".", "_child_signal_conn", ".", "send", "(", "DagParsingSignal", ".", "TERMINATE_MANAGER", ")"], "docstring": "Send termination signal to DAG parsing processor manager\n        and expect it to terminate all DAG file processors.", "docstring_tokens": ["Send", "termination", "signal", "to", "DAG", "parsing", "processor", "manager", "and", "expect", "it", "to", "terminate", "all", "DAG", "file", "processors", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L637-L643", "partition": "test"}
{"repo": "bloomreach/s4cmd", "path": "s4cmd.py", "func_name": "S3Handler.connect", "original_string": "def connect(self):\n    '''Connect to S3 storage'''\n    try:\n      if S3Handler.S3_KEYS:\n        self.s3 = BotoClient(self.opt, S3Handler.S3_KEYS[0], S3Handler.S3_KEYS[1])\n      else:\n        self.s3 = BotoClient(self.opt)\n    except Exception as e:\n      raise RetryFailure('Unable to connect to s3: %s' % e)", "language": "python", "code": "def connect(self):\n    '''Connect to S3 storage'''\n    try:\n      if S3Handler.S3_KEYS:\n        self.s3 = BotoClient(self.opt, S3Handler.S3_KEYS[0], S3Handler.S3_KEYS[1])\n      else:\n        self.s3 = BotoClient(self.opt)\n    except Exception as e:\n      raise RetryFailure('Unable to connect to s3: %s' % e)", "code_tokens": ["def", "connect", "(", "self", ")", ":", "try", ":", "if", "S3Handler", ".", "S3_KEYS", ":", "self", ".", "s3", "=", "BotoClient", "(", "self", ".", "opt", ",", "S3Handler", ".", "S3_KEYS", "[", "0", "]", ",", "S3Handler", ".", "S3_KEYS", "[", "1", "]", ")", "else", ":", "self", ".", "s3", "=", "BotoClient", "(", "self", ".", "opt", ")", "except", "Exception", "as", "e", ":", "raise", "RetryFailure", "(", "'Unable to connect to s3: %s'", "%", "e", ")"], "docstring": "Connect to S3 storage", "docstring_tokens": ["Connect", "to", "S3", "storage"], "sha": "bb51075bf43703e7cd95aa39288cf7732ec13a6d", "url": "https://github.com/bloomreach/s4cmd/blob/bb51075bf43703e7cd95aa39288cf7732ec13a6d/s4cmd.py#L680-L688", "partition": "test"}
{"repo": "jazzband/django-ddp", "path": "dddp/accounts/ddp.py", "func_name": "Users.serialize", "original_string": "def serialize(self, obj, *args, **kwargs):\n        \"\"\"Serialize user as per Meteor accounts serialization.\"\"\"\n        # use default serialization, then modify to suit our needs.\n        data = super(Users, self).serialize(obj, *args, **kwargs)\n\n        # everything that isn't handled explicitly ends up in `profile`\n        profile = data.pop('fields')\n        profile.setdefault('name', obj.get_full_name())\n        fields = data['fields'] = {\n            'username': obj.get_username(),\n            'emails': [],\n            'profile': profile,\n            'permissions': sorted(self.model.get_all_permissions(obj)),\n        }\n\n        # clear out sensitive data\n        for sensitive in [\n                'password',\n                'user_permissions_ids',\n                'is_active',\n                'is_staff',\n                'is_superuser',\n                'groups_ids',\n        ]:\n            profile.pop(sensitive, None)\n\n        # createdAt (default is django.contrib.auth.models.User.date_joined)\n        try:\n            fields['createdAt'] = profile.pop('date_joined')\n        except KeyError:\n            date_joined = getattr(\n                obj, 'get_date_joined',\n                lambda: getattr(obj, 'date_joined', None)\n            )()\n            if date_joined:\n                fields['createdAt'] = date_joined\n\n        # email (default is django.contrib.auth.models.User.email)\n        try:\n            email = profile.pop('email')\n        except KeyError:\n            email = getattr(\n                obj, 'get_email',\n                lambda: getattr(obj, 'email', None)\n            )()\n        if email:\n            fields['emails'].append({'address': email, 'verified': True})\n\n        return data", "language": "python", "code": "def serialize(self, obj, *args, **kwargs):\n        \"\"\"Serialize user as per Meteor accounts serialization.\"\"\"\n        # use default serialization, then modify to suit our needs.\n        data = super(Users, self).serialize(obj, *args, **kwargs)\n\n        # everything that isn't handled explicitly ends up in `profile`\n        profile = data.pop('fields')\n        profile.setdefault('name', obj.get_full_name())\n        fields = data['fields'] = {\n            'username': obj.get_username(),\n            'emails': [],\n            'profile': profile,\n            'permissions': sorted(self.model.get_all_permissions(obj)),\n        }\n\n        # clear out sensitive data\n        for sensitive in [\n                'password',\n                'user_permissions_ids',\n                'is_active',\n                'is_staff',\n                'is_superuser',\n                'groups_ids',\n        ]:\n            profile.pop(sensitive, None)\n\n        # createdAt (default is django.contrib.auth.models.User.date_joined)\n        try:\n            fields['createdAt'] = profile.pop('date_joined')\n        except KeyError:\n            date_joined = getattr(\n                obj, 'get_date_joined',\n                lambda: getattr(obj, 'date_joined', None)\n            )()\n            if date_joined:\n                fields['createdAt'] = date_joined\n\n        # email (default is django.contrib.auth.models.User.email)\n        try:\n            email = profile.pop('email')\n        except KeyError:\n            email = getattr(\n                obj, 'get_email',\n                lambda: getattr(obj, 'email', None)\n            )()\n        if email:\n            fields['emails'].append({'address': email, 'verified': True})\n\n        return data", "code_tokens": ["def", "serialize", "(", "self", ",", "obj", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "# use default serialization, then modify to suit our needs.", "data", "=", "super", "(", "Users", ",", "self", ")", ".", "serialize", "(", "obj", ",", "*", "args", ",", "*", "*", "kwargs", ")", "# everything that isn't handled explicitly ends up in `profile`", "profile", "=", "data", ".", "pop", "(", "'fields'", ")", "profile", ".", "setdefault", "(", "'name'", ",", "obj", ".", "get_full_name", "(", ")", ")", "fields", "=", "data", "[", "'fields'", "]", "=", "{", "'username'", ":", "obj", ".", "get_username", "(", ")", ",", "'emails'", ":", "[", "]", ",", "'profile'", ":", "profile", ",", "'permissions'", ":", "sorted", "(", "self", ".", "model", ".", "get_all_permissions", "(", "obj", ")", ")", ",", "}", "# clear out sensitive data", "for", "sensitive", "in", "[", "'password'", ",", "'user_permissions_ids'", ",", "'is_active'", ",", "'is_staff'", ",", "'is_superuser'", ",", "'groups_ids'", ",", "]", ":", "profile", ".", "pop", "(", "sensitive", ",", "None", ")", "# createdAt (default is django.contrib.auth.models.User.date_joined)", "try", ":", "fields", "[", "'createdAt'", "]", "=", "profile", ".", "pop", "(", "'date_joined'", ")", "except", "KeyError", ":", "date_joined", "=", "getattr", "(", "obj", ",", "'get_date_joined'", ",", "lambda", ":", "getattr", "(", "obj", ",", "'date_joined'", ",", "None", ")", ")", "(", ")", "if", "date_joined", ":", "fields", "[", "'createdAt'", "]", "=", "date_joined", "# email (default is django.contrib.auth.models.User.email)", "try", ":", "email", "=", "profile", ".", "pop", "(", "'email'", ")", "except", "KeyError", ":", "email", "=", "getattr", "(", "obj", ",", "'get_email'", ",", "lambda", ":", "getattr", "(", "obj", ",", "'email'", ",", "None", ")", ")", "(", ")", "if", "email", ":", "fields", "[", "'emails'", "]", ".", "append", "(", "{", "'address'", ":", "email", ",", "'verified'", ":", "True", "}", ")", "return", "data"], "docstring": "Serialize user as per Meteor accounts serialization.", "docstring_tokens": ["Serialize", "user", "as", "per", "Meteor", "accounts", "serialization", "."], "sha": "1e1954b06fe140346acea43582515991685e4e01", "url": "https://github.com/jazzband/django-ddp/blob/1e1954b06fe140346acea43582515991685e4e01/dddp/accounts/ddp.py#L125-L173", "partition": "test"}
{"repo": "dossier/dossier.store", "path": "dossier/store/elastic.py", "func_name": "ElasticStore._get_field_types", "original_string": "def _get_field_types(self):\n        'Retrieve the field types. Useful for debugging.'\n        mapping = self.conn.indices.get_mapping(\n            index=self.index, doc_type=self.type)\n        return mapping[self.index]['mappings'][self.type]['properties']", "language": "python", "code": "def _get_field_types(self):\n        'Retrieve the field types. Useful for debugging.'\n        mapping = self.conn.indices.get_mapping(\n            index=self.index, doc_type=self.type)\n        return mapping[self.index]['mappings'][self.type]['properties']", "code_tokens": ["def", "_get_field_types", "(", "self", ")", ":", "mapping", "=", "self", ".", "conn", ".", "indices", ".", "get_mapping", "(", "index", "=", "self", ".", "index", ",", "doc_type", "=", "self", ".", "type", ")", "return", "mapping", "[", "self", ".", "index", "]", "[", "'mappings'", "]", "[", "self", ".", "type", "]", "[", "'properties'", "]"], "docstring": "Retrieve the field types. Useful for debugging.", "docstring_tokens": ["Retrieve", "the", "field", "types", ".", "Useful", "for", "debugging", "."], "sha": "b22ffe2470bba9fcc98a30cb55b437bfa1521e7f", "url": "https://github.com/dossier/dossier.store/blob/b22ffe2470bba9fcc98a30cb55b437bfa1521e7f/dossier/store/elastic.py#L741-L745", "partition": "test"}
{"repo": "jazzband/django-ddp", "path": "dddp/websocket.py", "func_name": "validate_kwargs", "original_string": "def validate_kwargs(func, kwargs):\n    \"\"\"Validate arguments to be supplied to func.\"\"\"\n    func_name = func.__name__\n    argspec = inspect.getargspec(func)\n    all_args = argspec.args[:]\n    defaults = list(argspec.defaults or [])\n\n    # ignore implicit 'self' argument\n    if inspect.ismethod(func) and all_args[:1] == ['self']:\n        all_args[:1] = []\n\n    # don't require arguments that have defaults\n    if defaults:\n        required = all_args[:-len(defaults)]\n    else:\n        required = all_args[:]\n\n    # translate 'foo_' to avoid reserved names like 'id'\n    trans = {\n        arg: arg.endswith('_') and arg[:-1] or arg\n        for arg\n        in all_args\n    }\n    for key in list(kwargs):\n        key_adj = '%s_' % key\n        if key_adj in all_args:\n            kwargs[key_adj] = kwargs.pop(key)\n\n    # figure out what we're missing\n    supplied = sorted(kwargs)\n    missing = [\n        trans.get(arg, arg) for arg in required\n        if arg not in supplied\n    ]\n    if missing:\n        raise MeteorError(\n            400,\n            func.err,\n            'Missing required arguments to %s: %s' % (\n                func_name,\n                ' '.join(missing),\n            ),\n        )\n\n    # figure out what is extra\n    extra = [\n        arg for arg in supplied\n        if arg not in all_args\n    ]\n    if extra:\n        raise MeteorError(\n            400,\n            func.err,\n            'Unknown arguments to %s: %s' % (func_name, ' '.join(extra)),\n        )", "language": "python", "code": "def validate_kwargs(func, kwargs):\n    \"\"\"Validate arguments to be supplied to func.\"\"\"\n    func_name = func.__name__\n    argspec = inspect.getargspec(func)\n    all_args = argspec.args[:]\n    defaults = list(argspec.defaults or [])\n\n    # ignore implicit 'self' argument\n    if inspect.ismethod(func) and all_args[:1] == ['self']:\n        all_args[:1] = []\n\n    # don't require arguments that have defaults\n    if defaults:\n        required = all_args[:-len(defaults)]\n    else:\n        required = all_args[:]\n\n    # translate 'foo_' to avoid reserved names like 'id'\n    trans = {\n        arg: arg.endswith('_') and arg[:-1] or arg\n        for arg\n        in all_args\n    }\n    for key in list(kwargs):\n        key_adj = '%s_' % key\n        if key_adj in all_args:\n            kwargs[key_adj] = kwargs.pop(key)\n\n    # figure out what we're missing\n    supplied = sorted(kwargs)\n    missing = [\n        trans.get(arg, arg) for arg in required\n        if arg not in supplied\n    ]\n    if missing:\n        raise MeteorError(\n            400,\n            func.err,\n            'Missing required arguments to %s: %s' % (\n                func_name,\n                ' '.join(missing),\n            ),\n        )\n\n    # figure out what is extra\n    extra = [\n        arg for arg in supplied\n        if arg not in all_args\n    ]\n    if extra:\n        raise MeteorError(\n            400,\n            func.err,\n            'Unknown arguments to %s: %s' % (func_name, ' '.join(extra)),\n        )", "code_tokens": ["def", "validate_kwargs", "(", "func", ",", "kwargs", ")", ":", "func_name", "=", "func", ".", "__name__", "argspec", "=", "inspect", ".", "getargspec", "(", "func", ")", "all_args", "=", "argspec", ".", "args", "[", ":", "]", "defaults", "=", "list", "(", "argspec", ".", "defaults", "or", "[", "]", ")", "# ignore implicit 'self' argument", "if", "inspect", ".", "ismethod", "(", "func", ")", "and", "all_args", "[", ":", "1", "]", "==", "[", "'self'", "]", ":", "all_args", "[", ":", "1", "]", "=", "[", "]", "# don't require arguments that have defaults", "if", "defaults", ":", "required", "=", "all_args", "[", ":", "-", "len", "(", "defaults", ")", "]", "else", ":", "required", "=", "all_args", "[", ":", "]", "# translate 'foo_' to avoid reserved names like 'id'", "trans", "=", "{", "arg", ":", "arg", ".", "endswith", "(", "'_'", ")", "and", "arg", "[", ":", "-", "1", "]", "or", "arg", "for", "arg", "in", "all_args", "}", "for", "key", "in", "list", "(", "kwargs", ")", ":", "key_adj", "=", "'%s_'", "%", "key", "if", "key_adj", "in", "all_args", ":", "kwargs", "[", "key_adj", "]", "=", "kwargs", ".", "pop", "(", "key", ")", "# figure out what we're missing", "supplied", "=", "sorted", "(", "kwargs", ")", "missing", "=", "[", "trans", ".", "get", "(", "arg", ",", "arg", ")", "for", "arg", "in", "required", "if", "arg", "not", "in", "supplied", "]", "if", "missing", ":", "raise", "MeteorError", "(", "400", ",", "func", ".", "err", ",", "'Missing required arguments to %s: %s'", "%", "(", "func_name", ",", "' '", ".", "join", "(", "missing", ")", ",", ")", ",", ")", "# figure out what is extra", "extra", "=", "[", "arg", "for", "arg", "in", "supplied", "if", "arg", "not", "in", "all_args", "]", "if", "extra", ":", "raise", "MeteorError", "(", "400", ",", "func", ".", "err", ",", "'Unknown arguments to %s: %s'", "%", "(", "func_name", ",", "' '", ".", "join", "(", "extra", ")", ")", ",", ")"], "docstring": "Validate arguments to be supplied to func.", "docstring_tokens": ["Validate", "arguments", "to", "be", "supplied", "to", "func", "."], "sha": "1e1954b06fe140346acea43582515991685e4e01", "url": "https://github.com/jazzband/django-ddp/blob/1e1954b06fe140346acea43582515991685e4e01/dddp/websocket.py#L57-L111", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_container_hook.py", "func_name": "GKEClusterHook.get_cluster", "original_string": "def get_cluster(self, name, project_id=None, retry=DEFAULT, timeout=DEFAULT):\n        \"\"\"\n        Gets details of specified cluster\n\n        :param name: The name of the cluster to retrieve\n        :type name: str\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :param retry: A retry object used to retry requests. If None is specified,\n            requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to\n            complete. Note that if retry is specified, the timeout applies to each\n            individual attempt.\n        :type timeout: float\n        :return: google.cloud.container_v1.types.Cluster\n        \"\"\"\n        self.log.info(\n            \"Fetching cluster (project_id=%s, zone=%s, cluster_name=%s)\",\n            project_id or self.project_id, self.location, name\n        )\n\n        return self.get_client().get_cluster(project_id=project_id or self.project_id,\n                                             zone=self.location,\n                                             cluster_id=name,\n                                             retry=retry,\n                                             timeout=timeout).self_link", "language": "python", "code": "def get_cluster(self, name, project_id=None, retry=DEFAULT, timeout=DEFAULT):\n        \"\"\"\n        Gets details of specified cluster\n\n        :param name: The name of the cluster to retrieve\n        :type name: str\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :param retry: A retry object used to retry requests. If None is specified,\n            requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to\n            complete. Note that if retry is specified, the timeout applies to each\n            individual attempt.\n        :type timeout: float\n        :return: google.cloud.container_v1.types.Cluster\n        \"\"\"\n        self.log.info(\n            \"Fetching cluster (project_id=%s, zone=%s, cluster_name=%s)\",\n            project_id or self.project_id, self.location, name\n        )\n\n        return self.get_client().get_cluster(project_id=project_id or self.project_id,\n                                             zone=self.location,\n                                             cluster_id=name,\n                                             retry=retry,\n                                             timeout=timeout).self_link", "code_tokens": ["def", "get_cluster", "(", "self", ",", "name", ",", "project_id", "=", "None", ",", "retry", "=", "DEFAULT", ",", "timeout", "=", "DEFAULT", ")", ":", "self", ".", "log", ".", "info", "(", "\"Fetching cluster (project_id=%s, zone=%s, cluster_name=%s)\"", ",", "project_id", "or", "self", ".", "project_id", ",", "self", ".", "location", ",", "name", ")", "return", "self", ".", "get_client", "(", ")", ".", "get_cluster", "(", "project_id", "=", "project_id", "or", "self", ".", "project_id", ",", "zone", "=", "self", ".", "location", ",", "cluster_id", "=", "name", ",", "retry", "=", "retry", ",", "timeout", "=", "timeout", ")", ".", "self_link"], "docstring": "Gets details of specified cluster\n\n        :param name: The name of the cluster to retrieve\n        :type name: str\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :param retry: A retry object used to retry requests. If None is specified,\n            requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to\n            complete. Note that if retry is specified, the timeout applies to each\n            individual attempt.\n        :type timeout: float\n        :return: google.cloud.container_v1.types.Cluster", "docstring_tokens": ["Gets", "details", "of", "specified", "cluster"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_container_hook.py#L221-L247", "partition": "test"}
{"repo": "urinieto/msaf", "path": "msaf/algorithms/foote/segmenter.py", "func_name": "compute_ssm", "original_string": "def compute_ssm(X, metric=\"seuclidean\"):\n    \"\"\"Computes the self-similarity matrix of X.\"\"\"\n    D = distance.pdist(X, metric=metric)\n    D = distance.squareform(D)\n    D /= D.max()\n    return 1 - D", "language": "python", "code": "def compute_ssm(X, metric=\"seuclidean\"):\n    \"\"\"Computes the self-similarity matrix of X.\"\"\"\n    D = distance.pdist(X, metric=metric)\n    D = distance.squareform(D)\n    D /= D.max()\n    return 1 - D", "code_tokens": ["def", "compute_ssm", "(", "X", ",", "metric", "=", "\"seuclidean\"", ")", ":", "D", "=", "distance", ".", "pdist", "(", "X", ",", "metric", "=", "metric", ")", "D", "=", "distance", ".", "squareform", "(", "D", ")", "D", "/=", "D", ".", "max", "(", ")", "return", "1", "-", "D"], "docstring": "Computes the self-similarity matrix of X.", "docstring_tokens": ["Computes", "the", "self", "-", "similarity", "matrix", "of", "X", "."], "sha": "9dbb57d77a1310465a65cc40f1641d083ca74385", "url": "https://github.com/urinieto/msaf/blob/9dbb57d77a1310465a65cc40f1641d083ca74385/msaf/algorithms/foote/segmenter.py#L31-L36", "partition": "test"}
{"repo": "rocky/python3-trepan", "path": "trepan/lib/sighandler.py", "func_name": "SignalManager.info_signal", "original_string": "def info_signal(self, args):\n        \"\"\"Print information about a signal\"\"\"\n        if len(args) == 0: return None\n        signame = args[0]\n        if signame in ['handle', 'signal']:\n            # This has come from dbgr's info command\n            if len(args) == 1:\n                # Show all signal handlers\n                self.dbgr.core.processor.section(self.header)\n                for signame in self.siglist:\n                    self.print_info_signal_entry(signame)\n                return True\n            else:\n                signame = args[1]\n                pass\n            pass\n\n        signame = self.is_name_or_number(signame)\n        self.dbgr.core.processor.section(self.header)\n        self.print_info_signal_entry(signame)\n        return True", "language": "python", "code": "def info_signal(self, args):\n        \"\"\"Print information about a signal\"\"\"\n        if len(args) == 0: return None\n        signame = args[0]\n        if signame in ['handle', 'signal']:\n            # This has come from dbgr's info command\n            if len(args) == 1:\n                # Show all signal handlers\n                self.dbgr.core.processor.section(self.header)\n                for signame in self.siglist:\n                    self.print_info_signal_entry(signame)\n                return True\n            else:\n                signame = args[1]\n                pass\n            pass\n\n        signame = self.is_name_or_number(signame)\n        self.dbgr.core.processor.section(self.header)\n        self.print_info_signal_entry(signame)\n        return True", "code_tokens": ["def", "info_signal", "(", "self", ",", "args", ")", ":", "if", "len", "(", "args", ")", "==", "0", ":", "return", "None", "signame", "=", "args", "[", "0", "]", "if", "signame", "in", "[", "'handle'", ",", "'signal'", "]", ":", "# This has come from dbgr's info command", "if", "len", "(", "args", ")", "==", "1", ":", "# Show all signal handlers", "self", ".", "dbgr", ".", "core", ".", "processor", ".", "section", "(", "self", ".", "header", ")", "for", "signame", "in", "self", ".", "siglist", ":", "self", ".", "print_info_signal_entry", "(", "signame", ")", "return", "True", "else", ":", "signame", "=", "args", "[", "1", "]", "pass", "pass", "signame", "=", "self", ".", "is_name_or_number", "(", "signame", ")", "self", ".", "dbgr", ".", "core", ".", "processor", ".", "section", "(", "self", ".", "header", ")", "self", ".", "print_info_signal_entry", "(", "signame", ")", "return", "True"], "docstring": "Print information about a signal", "docstring_tokens": ["Print", "information", "about", "a", "signal"], "sha": "14e91bc0acce090d67be145b1ac040cab92ac5f3", "url": "https://github.com/rocky/python3-trepan/blob/14e91bc0acce090d67be145b1ac040cab92ac5f3/trepan/lib/sighandler.py#L320-L340", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/transforms/preprocessing.py", "func_name": "H2OScaler.transform", "original_string": "def transform(self, X, y=None, **params):\n        \"\"\"\n        Scale an H2OFrame with the fitted means and standard deviations.\n\n        :param X: An H2OFrame; may contain NAs and/or categoricals.\n        :param y: None (Ignored)\n        :param params: (Ignored)\n        :returns: A scaled H2OFrame.\n        \"\"\"\n        return X.scale(self.means, self.stds)", "language": "python", "code": "def transform(self, X, y=None, **params):\n        \"\"\"\n        Scale an H2OFrame with the fitted means and standard deviations.\n\n        :param X: An H2OFrame; may contain NAs and/or categoricals.\n        :param y: None (Ignored)\n        :param params: (Ignored)\n        :returns: A scaled H2OFrame.\n        \"\"\"\n        return X.scale(self.means, self.stds)", "code_tokens": ["def", "transform", "(", "self", ",", "X", ",", "y", "=", "None", ",", "*", "*", "params", ")", ":", "return", "X", ".", "scale", "(", "self", ".", "means", ",", "self", ".", "stds", ")"], "docstring": "Scale an H2OFrame with the fitted means and standard deviations.\n\n        :param X: An H2OFrame; may contain NAs and/or categoricals.\n        :param y: None (Ignored)\n        :param params: (Ignored)\n        :returns: A scaled H2OFrame.", "docstring_tokens": ["Scale", "an", "H2OFrame", "with", "the", "fitted", "means", "and", "standard", "deviations", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/transforms/preprocessing.py#L66-L75", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/server/blueprints/cases/controllers.py", "func_name": "case_report_content", "original_string": "def case_report_content(store, institute_obj, case_obj):\n    \"\"\"Gather contents to be visualized in a case report\n\n    Args:\n        store(adapter.MongoAdapter)\n        institute_obj(models.Institute)\n        case_obj(models.Case)\n\n    Returns:\n        data(dict)\n\n    \"\"\"\n    variant_types = {\n        'causatives_detailed': 'causatives',\n        'suspects_detailed': 'suspects',\n        'classified_detailed': 'acmg_classification',\n        'tagged_detailed': 'manual_rank',\n        'dismissed_detailed': 'dismiss_variant',\n        'commented_detailed': 'is_commented',\n    }\n    data = case_obj\n\n    for individual in data['individuals']:\n        try:\n            sex = int(individual.get('sex', 0))\n        except ValueError as err:\n            sex = 0\n        individual['sex_human'] = SEX_MAP[sex]\n        individual['phenotype_human'] = PHENOTYPE_MAP.get(individual['phenotype'])\n\n    # Add the case comments\n    data['comments'] = store.events(institute_obj, case=case_obj, comments=True)\n\n    data['manual_rank_options'] = MANUAL_RANK_OPTIONS\n    data['dismissed_options'] = DISMISS_VARIANT_OPTIONS\n    data['genetic_models'] = dict(GENETIC_MODELS)\n    data['report_created_at'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n\n    evaluated_variants = {}\n    for vt in variant_types:\n        evaluated_variants[vt] = []\n    # We collect all causatives and suspected variants\n    # These are handeled in separate since they are on case level\n    for var_type in ['causatives', 'suspects']:\n        #These include references to variants\n        vt = '_'.join([var_type, 'detailed'])\n        for var_id in case_obj.get(var_type,[]):\n            variant_obj = store.variant(var_id)\n            if not variant_obj:\n                continue\n            # If the variant exists we add it to the evaluated variants\n            evaluated_variants[vt].append(variant_obj)\n\n    ## get variants for this case that are either classified, commented, tagged or dismissed.\n    for var_obj in store.evaluated_variants(case_id=case_obj['_id']):\n        # Check which category it belongs to\n        for vt in variant_types:\n            keyword = variant_types[vt]\n            # When found we add it to the categpry\n            # Eac variant can belong to multiple categories\n            if keyword in var_obj:\n                evaluated_variants[vt].append(var_obj)\n\n    for var_type in evaluated_variants:\n        decorated_variants = []\n        for var_obj in evaluated_variants[var_type]:\n        # We decorate the variant with some extra information\n            if var_obj['category'] == 'snv':\n                decorated_info = variant_decorator(\n                        store=store,\n                        institute_obj=institute_obj,\n                        case_obj=case_obj,\n                        variant_id=None,\n                        variant_obj=var_obj,\n                        add_case=False,\n                        add_other=False,\n                        get_overlapping=False\n                    )\n            else:\n                decorated_info = sv_variant(\n                    store=store,\n                    institute_id=institute_obj['_id'],\n                    case_name=case_obj['display_name'],\n                    variant_obj=var_obj,\n                    add_case=False,\n                    get_overlapping=False\n                    )\n            decorated_variants.append(decorated_info['variant'])\n        # Add the decorated variants to the case\n        data[var_type] = decorated_variants\n\n    return data", "language": "python", "code": "def case_report_content(store, institute_obj, case_obj):\n    \"\"\"Gather contents to be visualized in a case report\n\n    Args:\n        store(adapter.MongoAdapter)\n        institute_obj(models.Institute)\n        case_obj(models.Case)\n\n    Returns:\n        data(dict)\n\n    \"\"\"\n    variant_types = {\n        'causatives_detailed': 'causatives',\n        'suspects_detailed': 'suspects',\n        'classified_detailed': 'acmg_classification',\n        'tagged_detailed': 'manual_rank',\n        'dismissed_detailed': 'dismiss_variant',\n        'commented_detailed': 'is_commented',\n    }\n    data = case_obj\n\n    for individual in data['individuals']:\n        try:\n            sex = int(individual.get('sex', 0))\n        except ValueError as err:\n            sex = 0\n        individual['sex_human'] = SEX_MAP[sex]\n        individual['phenotype_human'] = PHENOTYPE_MAP.get(individual['phenotype'])\n\n    # Add the case comments\n    data['comments'] = store.events(institute_obj, case=case_obj, comments=True)\n\n    data['manual_rank_options'] = MANUAL_RANK_OPTIONS\n    data['dismissed_options'] = DISMISS_VARIANT_OPTIONS\n    data['genetic_models'] = dict(GENETIC_MODELS)\n    data['report_created_at'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n\n    evaluated_variants = {}\n    for vt in variant_types:\n        evaluated_variants[vt] = []\n    # We collect all causatives and suspected variants\n    # These are handeled in separate since they are on case level\n    for var_type in ['causatives', 'suspects']:\n        #These include references to variants\n        vt = '_'.join([var_type, 'detailed'])\n        for var_id in case_obj.get(var_type,[]):\n            variant_obj = store.variant(var_id)\n            if not variant_obj:\n                continue\n            # If the variant exists we add it to the evaluated variants\n            evaluated_variants[vt].append(variant_obj)\n\n    ## get variants for this case that are either classified, commented, tagged or dismissed.\n    for var_obj in store.evaluated_variants(case_id=case_obj['_id']):\n        # Check which category it belongs to\n        for vt in variant_types:\n            keyword = variant_types[vt]\n            # When found we add it to the categpry\n            # Eac variant can belong to multiple categories\n            if keyword in var_obj:\n                evaluated_variants[vt].append(var_obj)\n\n    for var_type in evaluated_variants:\n        decorated_variants = []\n        for var_obj in evaluated_variants[var_type]:\n        # We decorate the variant with some extra information\n            if var_obj['category'] == 'snv':\n                decorated_info = variant_decorator(\n                        store=store,\n                        institute_obj=institute_obj,\n                        case_obj=case_obj,\n                        variant_id=None,\n                        variant_obj=var_obj,\n                        add_case=False,\n                        add_other=False,\n                        get_overlapping=False\n                    )\n            else:\n                decorated_info = sv_variant(\n                    store=store,\n                    institute_id=institute_obj['_id'],\n                    case_name=case_obj['display_name'],\n                    variant_obj=var_obj,\n                    add_case=False,\n                    get_overlapping=False\n                    )\n            decorated_variants.append(decorated_info['variant'])\n        # Add the decorated variants to the case\n        data[var_type] = decorated_variants\n\n    return data", "code_tokens": ["def", "case_report_content", "(", "store", ",", "institute_obj", ",", "case_obj", ")", ":", "variant_types", "=", "{", "'causatives_detailed'", ":", "'causatives'", ",", "'suspects_detailed'", ":", "'suspects'", ",", "'classified_detailed'", ":", "'acmg_classification'", ",", "'tagged_detailed'", ":", "'manual_rank'", ",", "'dismissed_detailed'", ":", "'dismiss_variant'", ",", "'commented_detailed'", ":", "'is_commented'", ",", "}", "data", "=", "case_obj", "for", "individual", "in", "data", "[", "'individuals'", "]", ":", "try", ":", "sex", "=", "int", "(", "individual", ".", "get", "(", "'sex'", ",", "0", ")", ")", "except", "ValueError", "as", "err", ":", "sex", "=", "0", "individual", "[", "'sex_human'", "]", "=", "SEX_MAP", "[", "sex", "]", "individual", "[", "'phenotype_human'", "]", "=", "PHENOTYPE_MAP", ".", "get", "(", "individual", "[", "'phenotype'", "]", ")", "# Add the case comments", "data", "[", "'comments'", "]", "=", "store", ".", "events", "(", "institute_obj", ",", "case", "=", "case_obj", ",", "comments", "=", "True", ")", "data", "[", "'manual_rank_options'", "]", "=", "MANUAL_RANK_OPTIONS", "data", "[", "'dismissed_options'", "]", "=", "DISMISS_VARIANT_OPTIONS", "data", "[", "'genetic_models'", "]", "=", "dict", "(", "GENETIC_MODELS", ")", "data", "[", "'report_created_at'", "]", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d %H:%M\"", ")", "evaluated_variants", "=", "{", "}", "for", "vt", "in", "variant_types", ":", "evaluated_variants", "[", "vt", "]", "=", "[", "]", "# We collect all causatives and suspected variants", "# These are handeled in separate since they are on case level", "for", "var_type", "in", "[", "'causatives'", ",", "'suspects'", "]", ":", "#These include references to variants", "vt", "=", "'_'", ".", "join", "(", "[", "var_type", ",", "'detailed'", "]", ")", "for", "var_id", "in", "case_obj", ".", "get", "(", "var_type", ",", "[", "]", ")", ":", "variant_obj", "=", "store", ".", "variant", "(", "var_id", ")", "if", "not", "variant_obj", ":", "continue", "# If the variant exists we add it to the evaluated variants", "evaluated_variants", "[", "vt", "]", ".", "append", "(", "variant_obj", ")", "## get variants for this case that are either classified, commented, tagged or dismissed.", "for", "var_obj", "in", "store", ".", "evaluated_variants", "(", "case_id", "=", "case_obj", "[", "'_id'", "]", ")", ":", "# Check which category it belongs to", "for", "vt", "in", "variant_types", ":", "keyword", "=", "variant_types", "[", "vt", "]", "# When found we add it to the categpry", "# Eac variant can belong to multiple categories", "if", "keyword", "in", "var_obj", ":", "evaluated_variants", "[", "vt", "]", ".", "append", "(", "var_obj", ")", "for", "var_type", "in", "evaluated_variants", ":", "decorated_variants", "=", "[", "]", "for", "var_obj", "in", "evaluated_variants", "[", "var_type", "]", ":", "# We decorate the variant with some extra information", "if", "var_obj", "[", "'category'", "]", "==", "'snv'", ":", "decorated_info", "=", "variant_decorator", "(", "store", "=", "store", ",", "institute_obj", "=", "institute_obj", ",", "case_obj", "=", "case_obj", ",", "variant_id", "=", "None", ",", "variant_obj", "=", "var_obj", ",", "add_case", "=", "False", ",", "add_other", "=", "False", ",", "get_overlapping", "=", "False", ")", "else", ":", "decorated_info", "=", "sv_variant", "(", "store", "=", "store", ",", "institute_id", "=", "institute_obj", "[", "'_id'", "]", ",", "case_name", "=", "case_obj", "[", "'display_name'", "]", ",", "variant_obj", "=", "var_obj", ",", "add_case", "=", "False", ",", "get_overlapping", "=", "False", ")", "decorated_variants", ".", "append", "(", "decorated_info", "[", "'variant'", "]", ")", "# Add the decorated variants to the case", "data", "[", "var_type", "]", "=", "decorated_variants", "return", "data"], "docstring": "Gather contents to be visualized in a case report\n\n    Args:\n        store(adapter.MongoAdapter)\n        institute_obj(models.Institute)\n        case_obj(models.Case)\n\n    Returns:\n        data(dict)", "docstring_tokens": ["Gather", "contents", "to", "be", "visualized", "in", "a", "case", "report"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/server/blueprints/cases/controllers.py#L167-L258", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/aws_dynamodb_hook.py", "func_name": "AwsDynamoDBHook.write_batch_data", "original_string": "def write_batch_data(self, items):\n        \"\"\"\n        Write batch items to dynamodb table with provisioned throughout capacity.\n        \"\"\"\n\n        dynamodb_conn = self.get_conn()\n\n        try:\n            table = dynamodb_conn.Table(self.table_name)\n\n            with table.batch_writer(overwrite_by_pkeys=self.table_keys) as batch:\n                for item in items:\n                    batch.put_item(Item=item)\n            return True\n        except Exception as general_error:\n            raise AirflowException(\n                'Failed to insert items in dynamodb, error: {error}'.format(\n                    error=str(general_error)\n                )\n            )", "language": "python", "code": "def write_batch_data(self, items):\n        \"\"\"\n        Write batch items to dynamodb table with provisioned throughout capacity.\n        \"\"\"\n\n        dynamodb_conn = self.get_conn()\n\n        try:\n            table = dynamodb_conn.Table(self.table_name)\n\n            with table.batch_writer(overwrite_by_pkeys=self.table_keys) as batch:\n                for item in items:\n                    batch.put_item(Item=item)\n            return True\n        except Exception as general_error:\n            raise AirflowException(\n                'Failed to insert items in dynamodb, error: {error}'.format(\n                    error=str(general_error)\n                )\n            )", "code_tokens": ["def", "write_batch_data", "(", "self", ",", "items", ")", ":", "dynamodb_conn", "=", "self", ".", "get_conn", "(", ")", "try", ":", "table", "=", "dynamodb_conn", ".", "Table", "(", "self", ".", "table_name", ")", "with", "table", ".", "batch_writer", "(", "overwrite_by_pkeys", "=", "self", ".", "table_keys", ")", "as", "batch", ":", "for", "item", "in", "items", ":", "batch", ".", "put_item", "(", "Item", "=", "item", ")", "return", "True", "except", "Exception", "as", "general_error", ":", "raise", "AirflowException", "(", "'Failed to insert items in dynamodb, error: {error}'", ".", "format", "(", "error", "=", "str", "(", "general_error", ")", ")", ")"], "docstring": "Write batch items to dynamodb table with provisioned throughout capacity.", "docstring_tokens": ["Write", "batch", "items", "to", "dynamodb", "table", "with", "provisioned", "throughout", "capacity", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_dynamodb_hook.py#L50-L69", "partition": "test"}
{"repo": "mpenning/polymer", "path": "polymer/Polymer.py", "func_name": "TaskMgrStats.log_time", "original_string": "def log_time(self):\n        \"\"\"Return True if it's time to log\"\"\"\n        if self.hot_loop and self.time_delta >= self.log_interval:\n            return True\n        return False", "language": "python", "code": "def log_time(self):\n        \"\"\"Return True if it's time to log\"\"\"\n        if self.hot_loop and self.time_delta >= self.log_interval:\n            return True\n        return False", "code_tokens": ["def", "log_time", "(", "self", ")", ":", "if", "self", ".", "hot_loop", "and", "self", ".", "time_delta", ">=", "self", ".", "log_interval", ":", "return", "True", "return", "False"], "docstring": "Return True if it's time to log", "docstring_tokens": ["Return", "True", "if", "it", "s", "time", "to", "log"], "sha": "1cdf4ed2573c894bde9d398fa173816b6b47e9f3", "url": "https://github.com/mpenning/polymer/blob/1cdf4ed2573c894bde9d398fa173816b6b47e9f3/polymer/Polymer.py#L268-L272", "partition": "test"}
{"repo": "PolicyStat/docx2html", "path": "docx2html/core.py", "func_name": "get_grid_span", "original_string": "def get_grid_span(tc):\n    \"\"\"\n    gridSpan is what docx uses to denote that a table cell has a colspan. This\n    is much more simple than rowspans in that there is a one-to-one mapping\n    from gridSpan to colspan.\n    \"\"\"\n    w_namespace = get_namespace(tc, 'w')\n    grid_spans = tc.xpath('.//w:gridSpan', namespaces=tc.nsmap)\n    if len(grid_spans) != 1:\n        return 1\n    grid_span = grid_spans[0]\n    return int(grid_span.get('%sval' % w_namespace))", "language": "python", "code": "def get_grid_span(tc):\n    \"\"\"\n    gridSpan is what docx uses to denote that a table cell has a colspan. This\n    is much more simple than rowspans in that there is a one-to-one mapping\n    from gridSpan to colspan.\n    \"\"\"\n    w_namespace = get_namespace(tc, 'w')\n    grid_spans = tc.xpath('.//w:gridSpan', namespaces=tc.nsmap)\n    if len(grid_spans) != 1:\n        return 1\n    grid_span = grid_spans[0]\n    return int(grid_span.get('%sval' % w_namespace))", "code_tokens": ["def", "get_grid_span", "(", "tc", ")", ":", "w_namespace", "=", "get_namespace", "(", "tc", ",", "'w'", ")", "grid_spans", "=", "tc", ".", "xpath", "(", "'.//w:gridSpan'", ",", "namespaces", "=", "tc", ".", "nsmap", ")", "if", "len", "(", "grid_spans", ")", "!=", "1", ":", "return", "1", "grid_span", "=", "grid_spans", "[", "0", "]", "return", "int", "(", "grid_span", ".", "get", "(", "'%sval'", "%", "w_namespace", ")", ")"], "docstring": "gridSpan is what docx uses to denote that a table cell has a colspan. This\n    is much more simple than rowspans in that there is a one-to-one mapping\n    from gridSpan to colspan.", "docstring_tokens": ["gridSpan", "is", "what", "docx", "uses", "to", "denote", "that", "a", "table", "cell", "has", "a", "colspan", ".", "This", "is", "much", "more", "simple", "than", "rowspans", "in", "that", "there", "is", "a", "one", "-", "to", "-", "one", "mapping", "from", "gridSpan", "to", "colspan", "."], "sha": "2dc4afd1e3a3f2f0b357d0bff903eb58bcc94429", "url": "https://github.com/PolicyStat/docx2html/blob/2dc4afd1e3a3f2f0b357d0bff903eb58bcc94429/docx2html/core.py#L410-L421", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/config/configurable.py", "func_name": "Configurable.class_config_section", "original_string": "def class_config_section(cls):\n        \"\"\"Get the config class config section\"\"\"\n        def c(s):\n            \"\"\"return a commented, wrapped block.\"\"\"\n            s = '\\n\\n'.join(wrap_paragraphs(s, 78))\n\n            return '# ' + s.replace('\\n', '\\n# ')\n\n        # section header\n        breaker = '#' + '-'*78\n        s = \"# %s configuration\"%cls.__name__\n        lines = [breaker, s, breaker, '']\n        # get the description trait\n        desc = cls.class_traits().get('description')\n        if desc:\n            desc = desc.default_value\n        else:\n            # no description trait, use __doc__\n            desc = getattr(cls, '__doc__', '')\n        if desc:\n            lines.append(c(desc))\n            lines.append('')\n\n        parents = []\n        for parent in cls.mro():\n            # only include parents that are not base classes\n            # and are not the class itself\n            # and have some configurable traits to inherit\n            if parent is not cls and issubclass(parent, Configurable) and \\\n                    parent.class_traits(config=True):\n                parents.append(parent)\n\n        if parents:\n            pstr = ', '.join([ p.__name__ for p in parents ])\n            lines.append(c('%s will inherit config from: %s'%(cls.__name__, pstr)))\n            lines.append('')\n\n        for name,trait in cls.class_traits(config=True).iteritems():\n            help = trait.get_metadata('help') or ''\n            lines.append(c(help))\n            lines.append('# c.%s.%s = %r'%(cls.__name__, name, trait.get_default_value()))\n            lines.append('')\n        return '\\n'.join(lines)", "language": "python", "code": "def class_config_section(cls):\n        \"\"\"Get the config class config section\"\"\"\n        def c(s):\n            \"\"\"return a commented, wrapped block.\"\"\"\n            s = '\\n\\n'.join(wrap_paragraphs(s, 78))\n\n            return '# ' + s.replace('\\n', '\\n# ')\n\n        # section header\n        breaker = '#' + '-'*78\n        s = \"# %s configuration\"%cls.__name__\n        lines = [breaker, s, breaker, '']\n        # get the description trait\n        desc = cls.class_traits().get('description')\n        if desc:\n            desc = desc.default_value\n        else:\n            # no description trait, use __doc__\n            desc = getattr(cls, '__doc__', '')\n        if desc:\n            lines.append(c(desc))\n            lines.append('')\n\n        parents = []\n        for parent in cls.mro():\n            # only include parents that are not base classes\n            # and are not the class itself\n            # and have some configurable traits to inherit\n            if parent is not cls and issubclass(parent, Configurable) and \\\n                    parent.class_traits(config=True):\n                parents.append(parent)\n\n        if parents:\n            pstr = ', '.join([ p.__name__ for p in parents ])\n            lines.append(c('%s will inherit config from: %s'%(cls.__name__, pstr)))\n            lines.append('')\n\n        for name,trait in cls.class_traits(config=True).iteritems():\n            help = trait.get_metadata('help') or ''\n            lines.append(c(help))\n            lines.append('# c.%s.%s = %r'%(cls.__name__, name, trait.get_default_value()))\n            lines.append('')\n        return '\\n'.join(lines)", "code_tokens": ["def", "class_config_section", "(", "cls", ")", ":", "def", "c", "(", "s", ")", ":", "\"\"\"return a commented, wrapped block.\"\"\"", "s", "=", "'\\n\\n'", ".", "join", "(", "wrap_paragraphs", "(", "s", ",", "78", ")", ")", "return", "'# '", "+", "s", ".", "replace", "(", "'\\n'", ",", "'\\n# '", ")", "# section header", "breaker", "=", "'#'", "+", "'-'", "*", "78", "s", "=", "\"# %s configuration\"", "%", "cls", ".", "__name__", "lines", "=", "[", "breaker", ",", "s", ",", "breaker", ",", "''", "]", "# get the description trait", "desc", "=", "cls", ".", "class_traits", "(", ")", ".", "get", "(", "'description'", ")", "if", "desc", ":", "desc", "=", "desc", ".", "default_value", "else", ":", "# no description trait, use __doc__", "desc", "=", "getattr", "(", "cls", ",", "'__doc__'", ",", "''", ")", "if", "desc", ":", "lines", ".", "append", "(", "c", "(", "desc", ")", ")", "lines", ".", "append", "(", "''", ")", "parents", "=", "[", "]", "for", "parent", "in", "cls", ".", "mro", "(", ")", ":", "# only include parents that are not base classes", "# and are not the class itself", "# and have some configurable traits to inherit", "if", "parent", "is", "not", "cls", "and", "issubclass", "(", "parent", ",", "Configurable", ")", "and", "parent", ".", "class_traits", "(", "config", "=", "True", ")", ":", "parents", ".", "append", "(", "parent", ")", "if", "parents", ":", "pstr", "=", "', '", ".", "join", "(", "[", "p", ".", "__name__", "for", "p", "in", "parents", "]", ")", "lines", ".", "append", "(", "c", "(", "'%s will inherit config from: %s'", "%", "(", "cls", ".", "__name__", ",", "pstr", ")", ")", ")", "lines", ".", "append", "(", "''", ")", "for", "name", ",", "trait", "in", "cls", ".", "class_traits", "(", "config", "=", "True", ")", ".", "iteritems", "(", ")", ":", "help", "=", "trait", ".", "get_metadata", "(", "'help'", ")", "or", "''", "lines", ".", "append", "(", "c", "(", "help", ")", ")", "lines", ".", "append", "(", "'# c.%s.%s = %r'", "%", "(", "cls", ".", "__name__", ",", "name", ",", "trait", ".", "get_default_value", "(", ")", ")", ")", "lines", ".", "append", "(", "''", ")", "return", "'\\n'", ".", "join", "(", "lines", ")"], "docstring": "Get the config class config section", "docstring_tokens": ["Get", "the", "config", "class", "config", "section"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/config/configurable.py#L206-L248", "partition": "test"}
{"repo": "joeblackwaslike/pricing", "path": "pricing/fields.py", "func_name": "price_converter", "original_string": "def price_converter(obj):\n    \"\"\"Ensures that string prices are converted into Price objects.\"\"\"\n    if isinstance(obj, str):\n        obj = PriceClass.parse(obj)\n    return obj", "language": "python", "code": "def price_converter(obj):\n    \"\"\"Ensures that string prices are converted into Price objects.\"\"\"\n    if isinstance(obj, str):\n        obj = PriceClass.parse(obj)\n    return obj", "code_tokens": ["def", "price_converter", "(", "obj", ")", ":", "if", "isinstance", "(", "obj", ",", "str", ")", ":", "obj", "=", "PriceClass", ".", "parse", "(", "obj", ")", "return", "obj"], "docstring": "Ensures that string prices are converted into Price objects.", "docstring_tokens": ["Ensures", "that", "string", "prices", "are", "converted", "into", "Price", "objects", "."], "sha": "be988b0851b4313af81f1db475bc33248700e39c", "url": "https://github.com/joeblackwaslike/pricing/blob/be988b0851b4313af81f1db475bc33248700e39c/pricing/fields.py#L24-L28", "partition": "test"}
{"repo": "grampajoe/happy", "path": "happy/cli.py", "func_name": "_infer_tarball_url", "original_string": "def _infer_tarball_url():\n    \"\"\"Returns the tarball URL inferred from an app.json, if present.\"\"\"\n    try:\n        with click.open_file('app.json', 'r') as f:\n            contents = f.read()\n\n        app_json = json.loads(contents)\n    except IOError:\n        return None\n\n    repository = app_json.get('repository')\n\n    if not repository:\n        return None\n    else:\n        return app_json.get('repository') + '/tarball/master/'", "language": "python", "code": "def _infer_tarball_url():\n    \"\"\"Returns the tarball URL inferred from an app.json, if present.\"\"\"\n    try:\n        with click.open_file('app.json', 'r') as f:\n            contents = f.read()\n\n        app_json = json.loads(contents)\n    except IOError:\n        return None\n\n    repository = app_json.get('repository')\n\n    if not repository:\n        return None\n    else:\n        return app_json.get('repository') + '/tarball/master/'", "code_tokens": ["def", "_infer_tarball_url", "(", ")", ":", "try", ":", "with", "click", ".", "open_file", "(", "'app.json'", ",", "'r'", ")", "as", "f", ":", "contents", "=", "f", ".", "read", "(", ")", "app_json", "=", "json", ".", "loads", "(", "contents", ")", "except", "IOError", ":", "return", "None", "repository", "=", "app_json", ".", "get", "(", "'repository'", ")", "if", "not", "repository", ":", "return", "None", "else", ":", "return", "app_json", ".", "get", "(", "'repository'", ")", "+", "'/tarball/master/'"], "docstring": "Returns the tarball URL inferred from an app.json, if present.", "docstring_tokens": ["Returns", "the", "tarball", "URL", "inferred", "from", "an", "app", ".", "json", "if", "present", "."], "sha": "707e056eb033ea010d181f5fab8d44e129ea9906", "url": "https://github.com/grampajoe/happy/blob/707e056eb033ea010d181f5fab8d44e129ea9906/happy/cli.py#L14-L29", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/tools/qcvv/tomography.py", "func_name": "process_tomography_set", "original_string": "def process_tomography_set(meas_qubits, meas_basis='Pauli',\n                           prep_qubits=None, prep_basis='SIC'):\n    \"\"\"\n    Generate a dictionary of process tomography experiment configurations.\n\n    This returns a data structure that is used by other tomography functions\n    to generate state and process tomography circuits, and extract tomography\n    data from results after execution on a backend.\n\n   A quantum process tomography set is created by specifying a preparation\n    basis along with a measurement basis. The preparation basis may be a\n    user defined `tomography_basis`, or one of the two built in basis 'SIC'\n    or 'Pauli'.\n    - SIC: Is a minimal symmetric informationally complete preparation\n           basis for 4 states for each qubit (4 ^ number of qubits total\n           preparation states). These correspond to the |0> state and the 3\n           other vertices of a tetrahedron on the Bloch-sphere.\n    - Pauli: Is a tomographically overcomplete preparation basis of the six\n             eigenstates of the 3 Pauli operators (6 ^ number of qubits\n             total preparation states).\n\n    Args:\n        meas_qubits (list): The qubits being measured.\n        meas_basis (tomography_basis or str): The qubit measurement basis.\n            The default value is 'Pauli'.\n        prep_qubits (list or None): The qubits being prepared. If None then\n            meas_qubits will be used for process tomography experiments.\n        prep_basis (tomography_basis or str): The qubit preparation basis.\n            The default value is 'SIC'.\n\n    Returns:\n        dict: A dict of tomography configurations that can be parsed by\n        `create_tomography_circuits` and `tomography_data` functions\n        for implementing quantum tomography experiments. This output contains\n        fields \"qubits\", \"meas_basis\", \"prep_basus\", circuits\".\n        ```\n        {\n            'qubits': qubits (list[ints]),\n            'meas_basis': meas_basis (tomography_basis),\n            'prep_basis': prep_basis (tomography_basis),\n            'circuit_labels': (list[string]),\n            'circuits': (list[dict])  # prep and meas configurations\n        }\n        ```\n    \"\"\"\n    return tomography_set(meas_qubits, meas_basis=meas_basis,\n                          prep_qubits=prep_qubits, prep_basis=prep_basis)", "language": "python", "code": "def process_tomography_set(meas_qubits, meas_basis='Pauli',\n                           prep_qubits=None, prep_basis='SIC'):\n    \"\"\"\n    Generate a dictionary of process tomography experiment configurations.\n\n    This returns a data structure that is used by other tomography functions\n    to generate state and process tomography circuits, and extract tomography\n    data from results after execution on a backend.\n\n   A quantum process tomography set is created by specifying a preparation\n    basis along with a measurement basis. The preparation basis may be a\n    user defined `tomography_basis`, or one of the two built in basis 'SIC'\n    or 'Pauli'.\n    - SIC: Is a minimal symmetric informationally complete preparation\n           basis for 4 states for each qubit (4 ^ number of qubits total\n           preparation states). These correspond to the |0> state and the 3\n           other vertices of a tetrahedron on the Bloch-sphere.\n    - Pauli: Is a tomographically overcomplete preparation basis of the six\n             eigenstates of the 3 Pauli operators (6 ^ number of qubits\n             total preparation states).\n\n    Args:\n        meas_qubits (list): The qubits being measured.\n        meas_basis (tomography_basis or str): The qubit measurement basis.\n            The default value is 'Pauli'.\n        prep_qubits (list or None): The qubits being prepared. If None then\n            meas_qubits will be used for process tomography experiments.\n        prep_basis (tomography_basis or str): The qubit preparation basis.\n            The default value is 'SIC'.\n\n    Returns:\n        dict: A dict of tomography configurations that can be parsed by\n        `create_tomography_circuits` and `tomography_data` functions\n        for implementing quantum tomography experiments. This output contains\n        fields \"qubits\", \"meas_basis\", \"prep_basus\", circuits\".\n        ```\n        {\n            'qubits': qubits (list[ints]),\n            'meas_basis': meas_basis (tomography_basis),\n            'prep_basis': prep_basis (tomography_basis),\n            'circuit_labels': (list[string]),\n            'circuits': (list[dict])  # prep and meas configurations\n        }\n        ```\n    \"\"\"\n    return tomography_set(meas_qubits, meas_basis=meas_basis,\n                          prep_qubits=prep_qubits, prep_basis=prep_basis)", "code_tokens": ["def", "process_tomography_set", "(", "meas_qubits", ",", "meas_basis", "=", "'Pauli'", ",", "prep_qubits", "=", "None", ",", "prep_basis", "=", "'SIC'", ")", ":", "return", "tomography_set", "(", "meas_qubits", ",", "meas_basis", "=", "meas_basis", ",", "prep_qubits", "=", "prep_qubits", ",", "prep_basis", "=", "prep_basis", ")"], "docstring": "Generate a dictionary of process tomography experiment configurations.\n\n    This returns a data structure that is used by other tomography functions\n    to generate state and process tomography circuits, and extract tomography\n    data from results after execution on a backend.\n\n   A quantum process tomography set is created by specifying a preparation\n    basis along with a measurement basis. The preparation basis may be a\n    user defined `tomography_basis`, or one of the two built in basis 'SIC'\n    or 'Pauli'.\n    - SIC: Is a minimal symmetric informationally complete preparation\n           basis for 4 states for each qubit (4 ^ number of qubits total\n           preparation states). These correspond to the |0> state and the 3\n           other vertices of a tetrahedron on the Bloch-sphere.\n    - Pauli: Is a tomographically overcomplete preparation basis of the six\n             eigenstates of the 3 Pauli operators (6 ^ number of qubits\n             total preparation states).\n\n    Args:\n        meas_qubits (list): The qubits being measured.\n        meas_basis (tomography_basis or str): The qubit measurement basis.\n            The default value is 'Pauli'.\n        prep_qubits (list or None): The qubits being prepared. If None then\n            meas_qubits will be used for process tomography experiments.\n        prep_basis (tomography_basis or str): The qubit preparation basis.\n            The default value is 'SIC'.\n\n    Returns:\n        dict: A dict of tomography configurations that can be parsed by\n        `create_tomography_circuits` and `tomography_data` functions\n        for implementing quantum tomography experiments. This output contains\n        fields \"qubits\", \"meas_basis\", \"prep_basus\", circuits\".\n        ```\n        {\n            'qubits': qubits (list[ints]),\n            'meas_basis': meas_basis (tomography_basis),\n            'prep_basis': prep_basis (tomography_basis),\n            'circuit_labels': (list[string]),\n            'circuits': (list[dict])  # prep and meas configurations\n        }\n        ```", "docstring_tokens": ["Generate", "a", "dictionary", "of", "process", "tomography", "experiment", "configurations", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/tools/qcvv/tomography.py#L441-L487", "partition": "test"}
{"repo": "trec-kba/streamcorpus-pipeline", "path": "streamcorpus_pipeline/_tokenizer.py", "func_name": "nltk_tokenizer._sentences", "original_string": "def _sentences(self, clean_visible):\n        'generate strings identified as sentences'\n        previous_end = 0\n        clean_visible = clean_visible.decode('utf8')\n        for start, end in self.sentence_tokenizer.span_tokenize(clean_visible):\n            # no need to check start, because the first byte of text\n            # is always first byte of first sentence, and we will\n            # have already made the previous sentence longer on the\n            # end if there was an overlap.\n            if start < previous_end:\n                start = previous_end\n                if start > end:\n                    # skip this sentence... because it was eaten by\n                    # an earlier sentence with a label\n                    continue\n            try:\n                label = self.label_index.find_le(end)\n            except ValueError:\n                label = None\n            if label:\n                ## avoid splitting a label\n                off = label.offsets[OffsetType.CHARS]\n                end = max(off.first + off.length, end)\n            previous_end = end\n            sent_str = clean_visible[start:end]\n            yield start, end, sent_str", "language": "python", "code": "def _sentences(self, clean_visible):\n        'generate strings identified as sentences'\n        previous_end = 0\n        clean_visible = clean_visible.decode('utf8')\n        for start, end in self.sentence_tokenizer.span_tokenize(clean_visible):\n            # no need to check start, because the first byte of text\n            # is always first byte of first sentence, and we will\n            # have already made the previous sentence longer on the\n            # end if there was an overlap.\n            if start < previous_end:\n                start = previous_end\n                if start > end:\n                    # skip this sentence... because it was eaten by\n                    # an earlier sentence with a label\n                    continue\n            try:\n                label = self.label_index.find_le(end)\n            except ValueError:\n                label = None\n            if label:\n                ## avoid splitting a label\n                off = label.offsets[OffsetType.CHARS]\n                end = max(off.first + off.length, end)\n            previous_end = end\n            sent_str = clean_visible[start:end]\n            yield start, end, sent_str", "code_tokens": ["def", "_sentences", "(", "self", ",", "clean_visible", ")", ":", "previous_end", "=", "0", "clean_visible", "=", "clean_visible", ".", "decode", "(", "'utf8'", ")", "for", "start", ",", "end", "in", "self", ".", "sentence_tokenizer", ".", "span_tokenize", "(", "clean_visible", ")", ":", "# no need to check start, because the first byte of text", "# is always first byte of first sentence, and we will", "# have already made the previous sentence longer on the", "# end if there was an overlap.", "if", "start", "<", "previous_end", ":", "start", "=", "previous_end", "if", "start", ">", "end", ":", "# skip this sentence... because it was eaten by", "# an earlier sentence with a label", "continue", "try", ":", "label", "=", "self", ".", "label_index", ".", "find_le", "(", "end", ")", "except", "ValueError", ":", "label", "=", "None", "if", "label", ":", "## avoid splitting a label", "off", "=", "label", ".", "offsets", "[", "OffsetType", ".", "CHARS", "]", "end", "=", "max", "(", "off", ".", "first", "+", "off", ".", "length", ",", "end", ")", "previous_end", "=", "end", "sent_str", "=", "clean_visible", "[", "start", ":", "end", "]", "yield", "start", ",", "end", ",", "sent_str"], "docstring": "generate strings identified as sentences", "docstring_tokens": ["generate", "strings", "identified", "as", "sentences"], "sha": "8bb82ea1beb83c6b40ed03fa1659df2897c2292a", "url": "https://github.com/trec-kba/streamcorpus-pipeline/blob/8bb82ea1beb83c6b40ed03fa1659df2897c2292a/streamcorpus_pipeline/_tokenizer.py#L51-L76", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/message/message_store.py", "func_name": "MessagesStore.help_message", "original_string": "def help_message(self, msgids):\n        \"\"\"Display help messages for the given message identifiers\"\"\"\n        for msgid in msgids:\n            try:\n                for message_definition in self.get_message_definitions(msgid):\n                    print(message_definition.format_help(checkerref=True))\n                    print(\"\")\n            except UnknownMessageError as ex:\n                print(ex)\n                print(\"\")\n                continue", "language": "python", "code": "def help_message(self, msgids):\n        \"\"\"Display help messages for the given message identifiers\"\"\"\n        for msgid in msgids:\n            try:\n                for message_definition in self.get_message_definitions(msgid):\n                    print(message_definition.format_help(checkerref=True))\n                    print(\"\")\n            except UnknownMessageError as ex:\n                print(ex)\n                print(\"\")\n                continue", "code_tokens": ["def", "help_message", "(", "self", ",", "msgids", ")", ":", "for", "msgid", "in", "msgids", ":", "try", ":", "for", "message_definition", "in", "self", ".", "get_message_definitions", "(", "msgid", ")", ":", "print", "(", "message_definition", ".", "format_help", "(", "checkerref", "=", "True", ")", ")", "print", "(", "\"\"", ")", "except", "UnknownMessageError", "as", "ex", ":", "print", "(", "ex", ")", "print", "(", "\"\"", ")", "continue"], "docstring": "Display help messages for the given message identifiers", "docstring_tokens": ["Display", "help", "messages", "for", "the", "given", "message", "identifiers"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/message/message_store.py#L192-L202", "partition": "test"}
{"repo": "tensorflow/probability", "path": "experimental/no_u_turn_sampler/nuts.py", "func_name": "_embed_no_none_gradient_check", "original_string": "def _embed_no_none_gradient_check(value_and_gradients_fn):\n  \"\"\"Wraps value and gradients function to assist with None gradients.\"\"\"\n  @functools.wraps(value_and_gradients_fn)\n  def func_wrapped(*args, **kwargs):\n    \"\"\"Wrapped function which checks for None gradients.\"\"\"\n    value, grads = value_and_gradients_fn(*args, **kwargs)\n    if any(grad is None for grad in grads):\n      raise ValueError(\"Gradient is None for a state.\")\n    return value, grads\n  return func_wrapped", "language": "python", "code": "def _embed_no_none_gradient_check(value_and_gradients_fn):\n  \"\"\"Wraps value and gradients function to assist with None gradients.\"\"\"\n  @functools.wraps(value_and_gradients_fn)\n  def func_wrapped(*args, **kwargs):\n    \"\"\"Wrapped function which checks for None gradients.\"\"\"\n    value, grads = value_and_gradients_fn(*args, **kwargs)\n    if any(grad is None for grad in grads):\n      raise ValueError(\"Gradient is None for a state.\")\n    return value, grads\n  return func_wrapped", "code_tokens": ["def", "_embed_no_none_gradient_check", "(", "value_and_gradients_fn", ")", ":", "@", "functools", ".", "wraps", "(", "value_and_gradients_fn", ")", "def", "func_wrapped", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "\"\"\"Wrapped function which checks for None gradients.\"\"\"", "value", ",", "grads", "=", "value_and_gradients_fn", "(", "*", "args", ",", "*", "*", "kwargs", ")", "if", "any", "(", "grad", "is", "None", "for", "grad", "in", "grads", ")", ":", "raise", "ValueError", "(", "\"Gradient is None for a state.\"", ")", "return", "value", ",", "grads", "return", "func_wrapped"], "docstring": "Wraps value and gradients function to assist with None gradients.", "docstring_tokens": ["Wraps", "value", "and", "gradients", "function", "to", "assist", "with", "None", "gradients", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/experimental/no_u_turn_sampler/nuts.py#L457-L466", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/jobs.py", "func_name": "BackfillJob._set_unfinished_dag_runs_to_failed", "original_string": "def _set_unfinished_dag_runs_to_failed(self, dag_runs, session=None):\n        \"\"\"\n        Go through the dag_runs and update the state based on the task_instance state.\n        Then set DAG runs that are not finished to failed.\n\n        :param dag_runs: DAG runs\n        :param session: session\n        :return: None\n        \"\"\"\n        for dag_run in dag_runs:\n            dag_run.update_state()\n            if dag_run.state not in State.finished():\n                dag_run.set_state(State.FAILED)\n            session.merge(dag_run)", "language": "python", "code": "def _set_unfinished_dag_runs_to_failed(self, dag_runs, session=None):\n        \"\"\"\n        Go through the dag_runs and update the state based on the task_instance state.\n        Then set DAG runs that are not finished to failed.\n\n        :param dag_runs: DAG runs\n        :param session: session\n        :return: None\n        \"\"\"\n        for dag_run in dag_runs:\n            dag_run.update_state()\n            if dag_run.state not in State.finished():\n                dag_run.set_state(State.FAILED)\n            session.merge(dag_run)", "code_tokens": ["def", "_set_unfinished_dag_runs_to_failed", "(", "self", ",", "dag_runs", ",", "session", "=", "None", ")", ":", "for", "dag_run", "in", "dag_runs", ":", "dag_run", ".", "update_state", "(", ")", "if", "dag_run", ".", "state", "not", "in", "State", ".", "finished", "(", ")", ":", "dag_run", ".", "set_state", "(", "State", ".", "FAILED", ")", "session", ".", "merge", "(", "dag_run", ")"], "docstring": "Go through the dag_runs and update the state based on the task_instance state.\n        Then set DAG runs that are not finished to failed.\n\n        :param dag_runs: DAG runs\n        :param session: session\n        :return: None", "docstring_tokens": ["Go", "through", "the", "dag_runs", "and", "update", "the", "state", "based", "on", "the", "task_instance", "state", ".", "Then", "set", "DAG", "runs", "that", "are", "not", "finished", "to", "failed", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L2445-L2458", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/dagcircuit/dagcircuit.py", "func_name": "DAGCircuit.add_creg", "original_string": "def add_creg(self, creg):\n        \"\"\"Add all wires in a classical register.\"\"\"\n        if not isinstance(creg, ClassicalRegister):\n            raise DAGCircuitError(\"not a ClassicalRegister instance.\")\n        if creg.name in self.cregs:\n            raise DAGCircuitError(\"duplicate register %s\" % creg.name)\n        self.cregs[creg.name] = creg\n        for j in range(creg.size):\n            self._add_wire((creg, j))", "language": "python", "code": "def add_creg(self, creg):\n        \"\"\"Add all wires in a classical register.\"\"\"\n        if not isinstance(creg, ClassicalRegister):\n            raise DAGCircuitError(\"not a ClassicalRegister instance.\")\n        if creg.name in self.cregs:\n            raise DAGCircuitError(\"duplicate register %s\" % creg.name)\n        self.cregs[creg.name] = creg\n        for j in range(creg.size):\n            self._add_wire((creg, j))", "code_tokens": ["def", "add_creg", "(", "self", ",", "creg", ")", ":", "if", "not", "isinstance", "(", "creg", ",", "ClassicalRegister", ")", ":", "raise", "DAGCircuitError", "(", "\"not a ClassicalRegister instance.\"", ")", "if", "creg", ".", "name", "in", "self", ".", "cregs", ":", "raise", "DAGCircuitError", "(", "\"duplicate register %s\"", "%", "creg", ".", "name", ")", "self", ".", "cregs", "[", "creg", ".", "name", "]", "=", "creg", "for", "j", "in", "range", "(", "creg", ".", "size", ")", ":", "self", ".", "_add_wire", "(", "(", "creg", ",", "j", ")", ")"], "docstring": "Add all wires in a classical register.", "docstring_tokens": ["Add", "all", "wires", "in", "a", "classical", "register", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/dagcircuit/dagcircuit.py#L190-L198", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicebus/azure/servicebus/common/mixins.py", "func_name": "BaseClient.from_connection_string", "original_string": "def from_connection_string(cls, conn_str, name=None, **kwargs):\n        \"\"\"Create a Client from a Service Bus connection string.\n\n        :param conn_str: The connection string.\n        :type conn_str: str\n        :param name: The name of the entity, if the 'EntityName' property is\n         not included in the connection string.\n        \"\"\"\n        address, policy, key, entity = parse_conn_str(conn_str)\n        entity = name or entity\n        address = build_uri(address, entity)\n        name = address.split('/')[-1]\n        return cls(address, name, shared_access_key_name=policy, shared_access_key_value=key, **kwargs)", "language": "python", "code": "def from_connection_string(cls, conn_str, name=None, **kwargs):\n        \"\"\"Create a Client from a Service Bus connection string.\n\n        :param conn_str: The connection string.\n        :type conn_str: str\n        :param name: The name of the entity, if the 'EntityName' property is\n         not included in the connection string.\n        \"\"\"\n        address, policy, key, entity = parse_conn_str(conn_str)\n        entity = name or entity\n        address = build_uri(address, entity)\n        name = address.split('/')[-1]\n        return cls(address, name, shared_access_key_name=policy, shared_access_key_value=key, **kwargs)", "code_tokens": ["def", "from_connection_string", "(", "cls", ",", "conn_str", ",", "name", "=", "None", ",", "*", "*", "kwargs", ")", ":", "address", ",", "policy", ",", "key", ",", "entity", "=", "parse_conn_str", "(", "conn_str", ")", "entity", "=", "name", "or", "entity", "address", "=", "build_uri", "(", "address", ",", "entity", ")", "name", "=", "address", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "return", "cls", "(", "address", ",", "name", ",", "shared_access_key_name", "=", "policy", ",", "shared_access_key_value", "=", "key", ",", "*", "*", "kwargs", ")"], "docstring": "Create a Client from a Service Bus connection string.\n\n        :param conn_str: The connection string.\n        :type conn_str: str\n        :param name: The name of the entity, if the 'EntityName' property is\n         not included in the connection string.", "docstring_tokens": ["Create", "a", "Client", "from", "a", "Service", "Bus", "connection", "string", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicebus/azure/servicebus/common/mixins.py#L307-L319", "partition": "test"}
{"repo": "assemblerflow/flowcraft", "path": "flowcraft/generator/recipe.py", "func_name": "InnuendoRecipe.build_downstream", "original_string": "def build_downstream(self, process_descriptions, task, all_tasks,\n                         task_pipeline,\n                         count_forks, total_tasks, forks):\n        \"\"\"Builds the downstream pipeline of the current process\n\n        Checks for the downstream processes to the current process and\n        adds them to the current pipeline fragment.\n\n        Parameters\n        ----------\n        process_descriptions : dict\n            Information of processes input, output and if is forkable\n        task : str\n            Current process\n        all_tasks : list\n            A list of all provided processes\n        task_pipeline : list\n            Current pipeline fragment\n        count_forks : int\n            Current number of forks\n        total_tasks : str\n            All space separated processes\n        forks : list\n            Current forks\n        Returns\n        -------\n        list : resulting pipeline fragment\n        \"\"\"\n\n        if task in process_descriptions:\n            if process_descriptions[task][2] is not None:\n                if len(process_descriptions[task][2].split(\"|\")) > 1:\n                    local_forks = process_descriptions[task][2].split(\"|\")\n\n                    # Adds the process to the pipeline fragment downstream\n                    # and defines a new pipeline fragment for each fork.\n                    # Those will only look for downstream processes\n                    for local_fork in local_forks:\n                        if local_fork in total_tasks:\n                            count_forks += 1\n                            task_pipeline.append(process_descriptions[task][2])\n                            self.define_pipeline_string(\n                                process_descriptions,\n                                local_fork,\n                                False,\n                                True,\n                                count_forks,\n                                total_tasks,\n                                forks\n                            )\n\n                    return task_pipeline\n                else:\n                    if process_descriptions[task][2] in total_tasks:\n                        task_pipeline.append(process_descriptions[task][2].split(\"|\")[0])\n\n                        # Proceeds building downstream until the output for a\n                        # process is None\n                        self.build_downstream(\n                            process_descriptions,\n                            process_descriptions[task][2].split(\"|\")[0],\n                            all_tasks,\n                            task_pipeline,\n                            count_forks,\n                            total_tasks,\n                            forks\n                        )\n\n                    return task_pipeline\n            else:\n                return task_pipeline", "language": "python", "code": "def build_downstream(self, process_descriptions, task, all_tasks,\n                         task_pipeline,\n                         count_forks, total_tasks, forks):\n        \"\"\"Builds the downstream pipeline of the current process\n\n        Checks for the downstream processes to the current process and\n        adds them to the current pipeline fragment.\n\n        Parameters\n        ----------\n        process_descriptions : dict\n            Information of processes input, output and if is forkable\n        task : str\n            Current process\n        all_tasks : list\n            A list of all provided processes\n        task_pipeline : list\n            Current pipeline fragment\n        count_forks : int\n            Current number of forks\n        total_tasks : str\n            All space separated processes\n        forks : list\n            Current forks\n        Returns\n        -------\n        list : resulting pipeline fragment\n        \"\"\"\n\n        if task in process_descriptions:\n            if process_descriptions[task][2] is not None:\n                if len(process_descriptions[task][2].split(\"|\")) > 1:\n                    local_forks = process_descriptions[task][2].split(\"|\")\n\n                    # Adds the process to the pipeline fragment downstream\n                    # and defines a new pipeline fragment for each fork.\n                    # Those will only look for downstream processes\n                    for local_fork in local_forks:\n                        if local_fork in total_tasks:\n                            count_forks += 1\n                            task_pipeline.append(process_descriptions[task][2])\n                            self.define_pipeline_string(\n                                process_descriptions,\n                                local_fork,\n                                False,\n                                True,\n                                count_forks,\n                                total_tasks,\n                                forks\n                            )\n\n                    return task_pipeline\n                else:\n                    if process_descriptions[task][2] in total_tasks:\n                        task_pipeline.append(process_descriptions[task][2].split(\"|\")[0])\n\n                        # Proceeds building downstream until the output for a\n                        # process is None\n                        self.build_downstream(\n                            process_descriptions,\n                            process_descriptions[task][2].split(\"|\")[0],\n                            all_tasks,\n                            task_pipeline,\n                            count_forks,\n                            total_tasks,\n                            forks\n                        )\n\n                    return task_pipeline\n            else:\n                return task_pipeline", "code_tokens": ["def", "build_downstream", "(", "self", ",", "process_descriptions", ",", "task", ",", "all_tasks", ",", "task_pipeline", ",", "count_forks", ",", "total_tasks", ",", "forks", ")", ":", "if", "task", "in", "process_descriptions", ":", "if", "process_descriptions", "[", "task", "]", "[", "2", "]", "is", "not", "None", ":", "if", "len", "(", "process_descriptions", "[", "task", "]", "[", "2", "]", ".", "split", "(", "\"|\"", ")", ")", ">", "1", ":", "local_forks", "=", "process_descriptions", "[", "task", "]", "[", "2", "]", ".", "split", "(", "\"|\"", ")", "# Adds the process to the pipeline fragment downstream", "# and defines a new pipeline fragment for each fork.", "# Those will only look for downstream processes", "for", "local_fork", "in", "local_forks", ":", "if", "local_fork", "in", "total_tasks", ":", "count_forks", "+=", "1", "task_pipeline", ".", "append", "(", "process_descriptions", "[", "task", "]", "[", "2", "]", ")", "self", ".", "define_pipeline_string", "(", "process_descriptions", ",", "local_fork", ",", "False", ",", "True", ",", "count_forks", ",", "total_tasks", ",", "forks", ")", "return", "task_pipeline", "else", ":", "if", "process_descriptions", "[", "task", "]", "[", "2", "]", "in", "total_tasks", ":", "task_pipeline", ".", "append", "(", "process_descriptions", "[", "task", "]", "[", "2", "]", ".", "split", "(", "\"|\"", ")", "[", "0", "]", ")", "# Proceeds building downstream until the output for a", "# process is None", "self", ".", "build_downstream", "(", "process_descriptions", ",", "process_descriptions", "[", "task", "]", "[", "2", "]", ".", "split", "(", "\"|\"", ")", "[", "0", "]", ",", "all_tasks", ",", "task_pipeline", ",", "count_forks", ",", "total_tasks", ",", "forks", ")", "return", "task_pipeline", "else", ":", "return", "task_pipeline"], "docstring": "Builds the downstream pipeline of the current process\n\n        Checks for the downstream processes to the current process and\n        adds them to the current pipeline fragment.\n\n        Parameters\n        ----------\n        process_descriptions : dict\n            Information of processes input, output and if is forkable\n        task : str\n            Current process\n        all_tasks : list\n            A list of all provided processes\n        task_pipeline : list\n            Current pipeline fragment\n        count_forks : int\n            Current number of forks\n        total_tasks : str\n            All space separated processes\n        forks : list\n            Current forks\n        Returns\n        -------\n        list : resulting pipeline fragment", "docstring_tokens": ["Builds", "the", "downstream", "pipeline", "of", "the", "current", "process"], "sha": "fc3f4bddded1efc76006600016dc71a06dd908c0", "url": "https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/generator/recipe.py#L165-L235", "partition": "test"}
{"repo": "LuminosoInsight/luminoso-api-client-python", "path": "luminoso_api/v4_json_stream.py", "func_name": "open_json_or_csv_somehow", "original_string": "def open_json_or_csv_somehow(filename, date_format=None):\n    \"\"\"\n    Deduce the format of a file, within reason.\n\n    - If the filename ends with .csv or .txt, it's csv.\n    - If the filename ends with .jsons, it's a JSON stream (conveniently the\n      format we want to output).\n    - If the filename ends with .json, it could be a legitimate JSON file, or\n      it could be a JSON stream, following a nonstandard convention that many\n      people including us are guilty of. In that case:\n      - If the first line is a complete JSON document, and there is more in the\n        file besides the first line, then it is a JSON stream.\n      - Otherwise, it is probably really JSON.\n    - If the filename does not end with .json, .jsons, or .csv, we have to guess\n      whether it's still CSV or tab-separated values or something like that.\n      If it's JSON, the first character would almost certainly have to be a\n      bracket or a brace. If it isn't, assume it's CSV or similar.\n    \"\"\"\n    fileformat = None\n    if filename.endswith('.csv'):\n        fileformat = 'csv'\n    elif filename.endswith('.jsons'):\n        fileformat = 'jsons'\n    else:\n        with open(filename) as opened:\n            line = opened.readline()\n            if line[0] not in '{[' and not filename.endswith('.json'):\n                fileformat = 'csv'\n            else:\n                if (line.count('{') == line.count('}') and\n                    line.count('[') == line.count(']')):\n                    # This line contains a complete JSON document. This probably\n                    # means it's in linewise JSON ('.jsons') format, unless the\n                    # whole file is on one line.\n                    char = ' '\n                    while char.isspace():\n                        char = opened.read()\n                        if char == '':\n                            fileformat = 'json'\n                            break\n                    if fileformat is None:\n                        fileformat = 'jsons'\n                else:\n                    fileformat = 'json'\n\n    if fileformat == 'json':\n        stream = json.load(open(filename), encoding='utf-8')\n    elif fileformat == 'csv':\n        stream = open_csv_somehow(filename)\n    else:\n        stream = stream_json_lines(filename)\n\n    return _normalize_data(stream, date_format=date_format)", "language": "python", "code": "def open_json_or_csv_somehow(filename, date_format=None):\n    \"\"\"\n    Deduce the format of a file, within reason.\n\n    - If the filename ends with .csv or .txt, it's csv.\n    - If the filename ends with .jsons, it's a JSON stream (conveniently the\n      format we want to output).\n    - If the filename ends with .json, it could be a legitimate JSON file, or\n      it could be a JSON stream, following a nonstandard convention that many\n      people including us are guilty of. In that case:\n      - If the first line is a complete JSON document, and there is more in the\n        file besides the first line, then it is a JSON stream.\n      - Otherwise, it is probably really JSON.\n    - If the filename does not end with .json, .jsons, or .csv, we have to guess\n      whether it's still CSV or tab-separated values or something like that.\n      If it's JSON, the first character would almost certainly have to be a\n      bracket or a brace. If it isn't, assume it's CSV or similar.\n    \"\"\"\n    fileformat = None\n    if filename.endswith('.csv'):\n        fileformat = 'csv'\n    elif filename.endswith('.jsons'):\n        fileformat = 'jsons'\n    else:\n        with open(filename) as opened:\n            line = opened.readline()\n            if line[0] not in '{[' and not filename.endswith('.json'):\n                fileformat = 'csv'\n            else:\n                if (line.count('{') == line.count('}') and\n                    line.count('[') == line.count(']')):\n                    # This line contains a complete JSON document. This probably\n                    # means it's in linewise JSON ('.jsons') format, unless the\n                    # whole file is on one line.\n                    char = ' '\n                    while char.isspace():\n                        char = opened.read()\n                        if char == '':\n                            fileformat = 'json'\n                            break\n                    if fileformat is None:\n                        fileformat = 'jsons'\n                else:\n                    fileformat = 'json'\n\n    if fileformat == 'json':\n        stream = json.load(open(filename), encoding='utf-8')\n    elif fileformat == 'csv':\n        stream = open_csv_somehow(filename)\n    else:\n        stream = stream_json_lines(filename)\n\n    return _normalize_data(stream, date_format=date_format)", "code_tokens": ["def", "open_json_or_csv_somehow", "(", "filename", ",", "date_format", "=", "None", ")", ":", "fileformat", "=", "None", "if", "filename", ".", "endswith", "(", "'.csv'", ")", ":", "fileformat", "=", "'csv'", "elif", "filename", ".", "endswith", "(", "'.jsons'", ")", ":", "fileformat", "=", "'jsons'", "else", ":", "with", "open", "(", "filename", ")", "as", "opened", ":", "line", "=", "opened", ".", "readline", "(", ")", "if", "line", "[", "0", "]", "not", "in", "'{['", "and", "not", "filename", ".", "endswith", "(", "'.json'", ")", ":", "fileformat", "=", "'csv'", "else", ":", "if", "(", "line", ".", "count", "(", "'{'", ")", "==", "line", ".", "count", "(", "'}'", ")", "and", "line", ".", "count", "(", "'['", ")", "==", "line", ".", "count", "(", "']'", ")", ")", ":", "# This line contains a complete JSON document. This probably", "# means it's in linewise JSON ('.jsons') format, unless the", "# whole file is on one line.", "char", "=", "' '", "while", "char", ".", "isspace", "(", ")", ":", "char", "=", "opened", ".", "read", "(", ")", "if", "char", "==", "''", ":", "fileformat", "=", "'json'", "break", "if", "fileformat", "is", "None", ":", "fileformat", "=", "'jsons'", "else", ":", "fileformat", "=", "'json'", "if", "fileformat", "==", "'json'", ":", "stream", "=", "json", ".", "load", "(", "open", "(", "filename", ")", ",", "encoding", "=", "'utf-8'", ")", "elif", "fileformat", "==", "'csv'", ":", "stream", "=", "open_csv_somehow", "(", "filename", ")", "else", ":", "stream", "=", "stream_json_lines", "(", "filename", ")", "return", "_normalize_data", "(", "stream", ",", "date_format", "=", "date_format", ")"], "docstring": "Deduce the format of a file, within reason.\n\n    - If the filename ends with .csv or .txt, it's csv.\n    - If the filename ends with .jsons, it's a JSON stream (conveniently the\n      format we want to output).\n    - If the filename ends with .json, it could be a legitimate JSON file, or\n      it could be a JSON stream, following a nonstandard convention that many\n      people including us are guilty of. In that case:\n      - If the first line is a complete JSON document, and there is more in the\n        file besides the first line, then it is a JSON stream.\n      - Otherwise, it is probably really JSON.\n    - If the filename does not end with .json, .jsons, or .csv, we have to guess\n      whether it's still CSV or tab-separated values or something like that.\n      If it's JSON, the first character would almost certainly have to be a\n      bracket or a brace. If it isn't, assume it's CSV or similar.", "docstring_tokens": ["Deduce", "the", "format", "of", "a", "file", "within", "reason", "."], "sha": "3bedf2a454aee39214c11fbf556ead3eecc27881", "url": "https://github.com/LuminosoInsight/luminoso-api-client-python/blob/3bedf2a454aee39214c11fbf556ead3eecc27881/luminoso_api/v4_json_stream.py#L73-L125", "partition": "test"}
{"repo": "urinieto/msaf", "path": "msaf/utils.py", "func_name": "align_end_hierarchies", "original_string": "def align_end_hierarchies(hier1, hier2, thres=0.5):\n    \"\"\"Align the end of the hierarchies such that they end at the same exact\n    second as long they have the same duration within a certain threshold.\n\n    Parameters\n    ----------\n    hier1: list\n        List containing hierarchical segment boundaries.\n    hier2: list\n        List containing hierarchical segment boundaries.\n    thres: float > 0\n        Threshold to decide whether two values are the same.\n    \"\"\"\n    # Make sure we have correctly formatted hierarchies\n    dur_h1 = hier1[0][-1]\n    for hier in hier1:\n        assert hier[-1] == dur_h1, \"hier1 is not correctly \" \\\n            \"formatted {} {}\".format(hier[-1], dur_h1)\n    dur_h2 = hier2[0][-1]\n    for hier in hier2:\n        assert hier[-1] == dur_h2, \"hier2 is not correctly formatted\"\n\n    # If durations are different, do nothing\n    if abs(dur_h1 - dur_h2) > thres:\n        return\n\n    # Align h1 with h2\n    for hier in hier1:\n        hier[-1] = dur_h2", "language": "python", "code": "def align_end_hierarchies(hier1, hier2, thres=0.5):\n    \"\"\"Align the end of the hierarchies such that they end at the same exact\n    second as long they have the same duration within a certain threshold.\n\n    Parameters\n    ----------\n    hier1: list\n        List containing hierarchical segment boundaries.\n    hier2: list\n        List containing hierarchical segment boundaries.\n    thres: float > 0\n        Threshold to decide whether two values are the same.\n    \"\"\"\n    # Make sure we have correctly formatted hierarchies\n    dur_h1 = hier1[0][-1]\n    for hier in hier1:\n        assert hier[-1] == dur_h1, \"hier1 is not correctly \" \\\n            \"formatted {} {}\".format(hier[-1], dur_h1)\n    dur_h2 = hier2[0][-1]\n    for hier in hier2:\n        assert hier[-1] == dur_h2, \"hier2 is not correctly formatted\"\n\n    # If durations are different, do nothing\n    if abs(dur_h1 - dur_h2) > thres:\n        return\n\n    # Align h1 with h2\n    for hier in hier1:\n        hier[-1] = dur_h2", "code_tokens": ["def", "align_end_hierarchies", "(", "hier1", ",", "hier2", ",", "thres", "=", "0.5", ")", ":", "# Make sure we have correctly formatted hierarchies", "dur_h1", "=", "hier1", "[", "0", "]", "[", "-", "1", "]", "for", "hier", "in", "hier1", ":", "assert", "hier", "[", "-", "1", "]", "==", "dur_h1", ",", "\"hier1 is not correctly \"", "\"formatted {} {}\"", ".", "format", "(", "hier", "[", "-", "1", "]", ",", "dur_h1", ")", "dur_h2", "=", "hier2", "[", "0", "]", "[", "-", "1", "]", "for", "hier", "in", "hier2", ":", "assert", "hier", "[", "-", "1", "]", "==", "dur_h2", ",", "\"hier2 is not correctly formatted\"", "# If durations are different, do nothing", "if", "abs", "(", "dur_h1", "-", "dur_h2", ")", ">", "thres", ":", "return", "# Align h1 with h2", "for", "hier", "in", "hier1", ":", "hier", "[", "-", "1", "]", "=", "dur_h2"], "docstring": "Align the end of the hierarchies such that they end at the same exact\n    second as long they have the same duration within a certain threshold.\n\n    Parameters\n    ----------\n    hier1: list\n        List containing hierarchical segment boundaries.\n    hier2: list\n        List containing hierarchical segment boundaries.\n    thres: float > 0\n        Threshold to decide whether two values are the same.", "docstring_tokens": ["Align", "the", "end", "of", "the", "hierarchies", "such", "that", "they", "end", "at", "the", "same", "exact", "second", "as", "long", "they", "have", "the", "same", "duration", "within", "a", "certain", "threshold", "."], "sha": "9dbb57d77a1310465a65cc40f1641d083ca74385", "url": "https://github.com/urinieto/msaf/blob/9dbb57d77a1310465a65cc40f1641d083ca74385/msaf/utils.py#L234-L262", "partition": "test"}
{"repo": "neurodata/ndio", "path": "ndio/remote/remote_utils.py", "func_name": "remote_utils.delete_url", "original_string": "def delete_url(self, url, token=''):\n        \"\"\"\n        Returns a delete resquest object taking in a url and user token.\n\n        Arguments:\n            url (str): The url to make post to\n            token (str): The authentication token\n\n        Returns:\n            obj: Delete request object\n        \"\"\"\n        if (token == ''):\n            token = self._user_token\n\n        return requests.delete(url,\n                               headers={\n                                   'Authorization': 'Token {}'.format(token)},\n                               verify=False,)", "language": "python", "code": "def delete_url(self, url, token=''):\n        \"\"\"\n        Returns a delete resquest object taking in a url and user token.\n\n        Arguments:\n            url (str): The url to make post to\n            token (str): The authentication token\n\n        Returns:\n            obj: Delete request object\n        \"\"\"\n        if (token == ''):\n            token = self._user_token\n\n        return requests.delete(url,\n                               headers={\n                                   'Authorization': 'Token {}'.format(token)},\n                               verify=False,)", "code_tokens": ["def", "delete_url", "(", "self", ",", "url", ",", "token", "=", "''", ")", ":", "if", "(", "token", "==", "''", ")", ":", "token", "=", "self", ".", "_user_token", "return", "requests", ".", "delete", "(", "url", ",", "headers", "=", "{", "'Authorization'", ":", "'Token {}'", ".", "format", "(", "token", ")", "}", ",", "verify", "=", "False", ",", ")"], "docstring": "Returns a delete resquest object taking in a url and user token.\n\n        Arguments:\n            url (str): The url to make post to\n            token (str): The authentication token\n\n        Returns:\n            obj: Delete request object", "docstring_tokens": ["Returns", "a", "delete", "resquest", "object", "taking", "in", "a", "url", "and", "user", "token", "."], "sha": "792dd5816bc770b05a3db2f4327da42ff6253531", "url": "https://github.com/neurodata/ndio/blob/792dd5816bc770b05a3db2f4327da42ff6253531/ndio/remote/remote_utils.py#L80-L97", "partition": "test"}
{"repo": "mdgoldberg/sportsref", "path": "sportsref/nba/players.py", "func_name": "Player.stats_advanced", "original_string": "def stats_advanced(self, kind='R', summary=False):\n        \"\"\"Returns a DataFrame of advanced stats.\"\"\"\n        return self._get_stats_table('advanced', kind=kind, summary=summary)", "language": "python", "code": "def stats_advanced(self, kind='R', summary=False):\n        \"\"\"Returns a DataFrame of advanced stats.\"\"\"\n        return self._get_stats_table('advanced', kind=kind, summary=summary)", "code_tokens": ["def", "stats_advanced", "(", "self", ",", "kind", "=", "'R'", ",", "summary", "=", "False", ")", ":", "return", "self", ".", "_get_stats_table", "(", "'advanced'", ",", "kind", "=", "kind", ",", "summary", "=", "summary", ")"], "docstring": "Returns a DataFrame of advanced stats.", "docstring_tokens": ["Returns", "a", "DataFrame", "of", "advanced", "stats", "."], "sha": "09f11ac856a23c96d666d1d510bb35d6f050b5c3", "url": "https://github.com/mdgoldberg/sportsref/blob/09f11ac856a23c96d666d1d510bb35d6f050b5c3/sportsref/nba/players.py#L180-L182", "partition": "test"}
{"repo": "rwl/godot", "path": "godot/plugin/action.py", "func_name": "NewDotGraphAction.perform", "original_string": "def perform(self, event):\n        \"\"\" Perform the action.\n        \"\"\"\n        wizard = NewDotGraphWizard(parent=self.window.control,\n            window=self.window, title=\"New Graph\")\n\n        # Open the wizard\n        if wizard.open() == OK:\n            wizard.finished = True", "language": "python", "code": "def perform(self, event):\n        \"\"\" Perform the action.\n        \"\"\"\n        wizard = NewDotGraphWizard(parent=self.window.control,\n            window=self.window, title=\"New Graph\")\n\n        # Open the wizard\n        if wizard.open() == OK:\n            wizard.finished = True", "code_tokens": ["def", "perform", "(", "self", ",", "event", ")", ":", "wizard", "=", "NewDotGraphWizard", "(", "parent", "=", "self", ".", "window", ".", "control", ",", "window", "=", "self", ".", "window", ",", "title", "=", "\"New Graph\"", ")", "# Open the wizard", "if", "wizard", ".", "open", "(", ")", "==", "OK", ":", "wizard", ".", "finished", "=", "True"], "docstring": "Perform the action.", "docstring_tokens": ["Perform", "the", "action", "."], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/plugin/action.py#L70-L78", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/structural_time_series.py", "func_name": "StructuralTimeSeries.make_state_space_model", "original_string": "def make_state_space_model(self,\n                             num_timesteps,\n                             param_vals=None,\n                             initial_state_prior=None,\n                             initial_step=0):\n    \"\"\"Instantiate this model as a Distribution over specified `num_timesteps`.\n\n    Args:\n      num_timesteps: Python `int` number of timesteps to model.\n      param_vals: a list of `Tensor` parameter values in order corresponding to\n        `self.parameters`, or a dict mapping from parameter names to values.\n      initial_state_prior: an optional `Distribution` instance overriding the\n        default prior on the model's initial state. This is used in forecasting\n        (\"today's prior is yesterday's posterior\").\n      initial_step: optional `int` specifying the initial timestep to model.\n        This is relevant when the model contains time-varying components,\n        e.g., holidays or seasonality.\n\n    Returns:\n      dist: a `LinearGaussianStateSpaceModel` Distribution object.\n    \"\"\"\n    return self._make_state_space_model(\n        num_timesteps=num_timesteps,\n        param_map=self._canonicalize_param_vals_as_map(param_vals),\n        initial_state_prior=initial_state_prior,\n        initial_step=initial_step)", "language": "python", "code": "def make_state_space_model(self,\n                             num_timesteps,\n                             param_vals=None,\n                             initial_state_prior=None,\n                             initial_step=0):\n    \"\"\"Instantiate this model as a Distribution over specified `num_timesteps`.\n\n    Args:\n      num_timesteps: Python `int` number of timesteps to model.\n      param_vals: a list of `Tensor` parameter values in order corresponding to\n        `self.parameters`, or a dict mapping from parameter names to values.\n      initial_state_prior: an optional `Distribution` instance overriding the\n        default prior on the model's initial state. This is used in forecasting\n        (\"today's prior is yesterday's posterior\").\n      initial_step: optional `int` specifying the initial timestep to model.\n        This is relevant when the model contains time-varying components,\n        e.g., holidays or seasonality.\n\n    Returns:\n      dist: a `LinearGaussianStateSpaceModel` Distribution object.\n    \"\"\"\n    return self._make_state_space_model(\n        num_timesteps=num_timesteps,\n        param_map=self._canonicalize_param_vals_as_map(param_vals),\n        initial_state_prior=initial_state_prior,\n        initial_step=initial_step)", "code_tokens": ["def", "make_state_space_model", "(", "self", ",", "num_timesteps", ",", "param_vals", "=", "None", ",", "initial_state_prior", "=", "None", ",", "initial_step", "=", "0", ")", ":", "return", "self", ".", "_make_state_space_model", "(", "num_timesteps", "=", "num_timesteps", ",", "param_map", "=", "self", ".", "_canonicalize_param_vals_as_map", "(", "param_vals", ")", ",", "initial_state_prior", "=", "initial_state_prior", ",", "initial_step", "=", "initial_step", ")"], "docstring": "Instantiate this model as a Distribution over specified `num_timesteps`.\n\n    Args:\n      num_timesteps: Python `int` number of timesteps to model.\n      param_vals: a list of `Tensor` parameter values in order corresponding to\n        `self.parameters`, or a dict mapping from parameter names to values.\n      initial_state_prior: an optional `Distribution` instance overriding the\n        default prior on the model's initial state. This is used in forecasting\n        (\"today's prior is yesterday's posterior\").\n      initial_step: optional `int` specifying the initial timestep to model.\n        This is relevant when the model contains time-varying components,\n        e.g., holidays or seasonality.\n\n    Returns:\n      dist: a `LinearGaussianStateSpaceModel` Distribution object.", "docstring_tokens": ["Instantiate", "this", "model", "as", "a", "Distribution", "over", "specified", "num_timesteps", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/structural_time_series.py#L134-L159", "partition": "test"}
{"repo": "lmjohns3/theanets", "path": "theanets/graph.py", "func_name": "Network.loss", "original_string": "def loss(self, **kwargs):\n        '''Return a variable representing the regularized loss for this network.\n\n        The regularized loss includes both the :ref:`loss computation <losses>`\n        for the network as well as any :ref:`regularizers <regularizers>` that\n        are in place.\n\n        Keyword arguments are passed directly to\n        :func:`theanets.regularizers.from_kwargs`.\n\n        Returns\n        -------\n        loss : Theano expression\n            A Theano expression representing the loss of this network.\n        '''\n        regs = regularizers.from_kwargs(self, **kwargs)\n        outputs, _ = self.build_graph(regs)\n        return sum(l.weight * l(outputs) for l in self.losses) + \\\n            sum(r.weight * r.loss(self.layers, outputs) for r in regs)", "language": "python", "code": "def loss(self, **kwargs):\n        '''Return a variable representing the regularized loss for this network.\n\n        The regularized loss includes both the :ref:`loss computation <losses>`\n        for the network as well as any :ref:`regularizers <regularizers>` that\n        are in place.\n\n        Keyword arguments are passed directly to\n        :func:`theanets.regularizers.from_kwargs`.\n\n        Returns\n        -------\n        loss : Theano expression\n            A Theano expression representing the loss of this network.\n        '''\n        regs = regularizers.from_kwargs(self, **kwargs)\n        outputs, _ = self.build_graph(regs)\n        return sum(l.weight * l(outputs) for l in self.losses) + \\\n            sum(r.weight * r.loss(self.layers, outputs) for r in regs)", "code_tokens": ["def", "loss", "(", "self", ",", "*", "*", "kwargs", ")", ":", "regs", "=", "regularizers", ".", "from_kwargs", "(", "self", ",", "*", "*", "kwargs", ")", "outputs", ",", "_", "=", "self", ".", "build_graph", "(", "regs", ")", "return", "sum", "(", "l", ".", "weight", "*", "l", "(", "outputs", ")", "for", "l", "in", "self", ".", "losses", ")", "+", "sum", "(", "r", ".", "weight", "*", "r", ".", "loss", "(", "self", ".", "layers", ",", "outputs", ")", "for", "r", "in", "regs", ")"], "docstring": "Return a variable representing the regularized loss for this network.\n\n        The regularized loss includes both the :ref:`loss computation <losses>`\n        for the network as well as any :ref:`regularizers <regularizers>` that\n        are in place.\n\n        Keyword arguments are passed directly to\n        :func:`theanets.regularizers.from_kwargs`.\n\n        Returns\n        -------\n        loss : Theano expression\n            A Theano expression representing the loss of this network.", "docstring_tokens": ["Return", "a", "variable", "representing", "the", "regularized", "loss", "for", "this", "network", "."], "sha": "79db9f878ef2071f2f576a1cf5d43a752a55894a", "url": "https://github.com/lmjohns3/theanets/blob/79db9f878ef2071f2f576a1cf5d43a752a55894a/theanets/graph.py#L636-L654", "partition": "test"}
{"repo": "marrabld/planarradpy", "path": "gui/gui_mainLayout.py", "func_name": "FormEvents.display_the_graphic_connection", "original_string": "def display_the_graphic_connection(self):\n        \"\"\"\n        The following permits to attribute the function \"display_the_graphic\" to the slider.\n        Because, to make a connection, we can not have parameters for the function, but \"display_the_graphic\" has some.\n        \"\"\"\n        self.display_the_graphic(self.num_line, self.wavelength, self.data_wanted, self.information)", "language": "python", "code": "def display_the_graphic_connection(self):\n        \"\"\"\n        The following permits to attribute the function \"display_the_graphic\" to the slider.\n        Because, to make a connection, we can not have parameters for the function, but \"display_the_graphic\" has some.\n        \"\"\"\n        self.display_the_graphic(self.num_line, self.wavelength, self.data_wanted, self.information)", "code_tokens": ["def", "display_the_graphic_connection", "(", "self", ")", ":", "self", ".", "display_the_graphic", "(", "self", ".", "num_line", ",", "self", ".", "wavelength", ",", "self", ".", "data_wanted", ",", "self", ".", "information", ")"], "docstring": "The following permits to attribute the function \"display_the_graphic\" to the slider.\n        Because, to make a connection, we can not have parameters for the function, but \"display_the_graphic\" has some.", "docstring_tokens": ["The", "following", "permits", "to", "attribute", "the", "function", "display_the_graphic", "to", "the", "slider", ".", "Because", "to", "make", "a", "connection", "we", "can", "not", "have", "parameters", "for", "the", "function", "but", "display_the_graphic", "has", "some", "."], "sha": "5095d1cb98d4f67a7c3108c9282f2d59253e89a8", "url": "https://github.com/marrabld/planarradpy/blob/5095d1cb98d4f67a7c3108c9282f2d59253e89a8/gui/gui_mainLayout.py#L564-L569", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/qasm/node/id.py", "func_name": "Id.latex", "original_string": "def latex(self, prec=15, nested_scope=None):\n        \"\"\"Return the correspond math mode latex string.\"\"\"\n        if not nested_scope:\n            return \"\\textrm{\" + self.name + \"}\"\n        else:\n            if self.name not in nested_scope[-1]:\n                raise NodeException(\"Expected local parameter name: \",\n                                    \"name=%s, \" % self.name,\n                                    \"line=%s, \" % self.line,\n                                    \"file=%s\" % self.file)\n            else:\n                return nested_scope[-1][self.name].latex(prec,\n                                                         nested_scope[0:-1])", "language": "python", "code": "def latex(self, prec=15, nested_scope=None):\n        \"\"\"Return the correspond math mode latex string.\"\"\"\n        if not nested_scope:\n            return \"\\textrm{\" + self.name + \"}\"\n        else:\n            if self.name not in nested_scope[-1]:\n                raise NodeException(\"Expected local parameter name: \",\n                                    \"name=%s, \" % self.name,\n                                    \"line=%s, \" % self.line,\n                                    \"file=%s\" % self.file)\n            else:\n                return nested_scope[-1][self.name].latex(prec,\n                                                         nested_scope[0:-1])", "code_tokens": ["def", "latex", "(", "self", ",", "prec", "=", "15", ",", "nested_scope", "=", "None", ")", ":", "if", "not", "nested_scope", ":", "return", "\"\\textrm{\"", "+", "self", ".", "name", "+", "\"}\"", "else", ":", "if", "self", ".", "name", "not", "in", "nested_scope", "[", "-", "1", "]", ":", "raise", "NodeException", "(", "\"Expected local parameter name: \"", ",", "\"name=%s, \"", "%", "self", ".", "name", ",", "\"line=%s, \"", "%", "self", ".", "line", ",", "\"file=%s\"", "%", "self", ".", "file", ")", "else", ":", "return", "nested_scope", "[", "-", "1", "]", "[", "self", ".", "name", "]", ".", "latex", "(", "prec", ",", "nested_scope", "[", "0", ":", "-", "1", "]", ")"], "docstring": "Return the correspond math mode latex string.", "docstring_tokens": ["Return", "the", "correspond", "math", "mode", "latex", "string", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/qasm/node/id.py#L42-L54", "partition": "test"}
{"repo": "edx/web-fragments", "path": "web_fragments/fragment.py", "func_name": "Fragment.initialize_js", "original_string": "def initialize_js(self, js_func, json_args=None):\n        \"\"\"\n        Register a Javascript function to initialize the Javascript resources.\n\n        `js_func` is the name of a Javascript function defined by one of the\n        Javascript resources.  As part of setting up the browser's runtime\n        environment, the function will be invoked, passing a runtime object\n        and a DOM element.\n        \"\"\"\n        self.js_init_fn = js_func\n        self.js_init_version = JS_API_VERSION\n        if json_args:\n            self.json_init_args = json_args", "language": "python", "code": "def initialize_js(self, js_func, json_args=None):\n        \"\"\"\n        Register a Javascript function to initialize the Javascript resources.\n\n        `js_func` is the name of a Javascript function defined by one of the\n        Javascript resources.  As part of setting up the browser's runtime\n        environment, the function will be invoked, passing a runtime object\n        and a DOM element.\n        \"\"\"\n        self.js_init_fn = js_func\n        self.js_init_version = JS_API_VERSION\n        if json_args:\n            self.json_init_args = json_args", "code_tokens": ["def", "initialize_js", "(", "self", ",", "js_func", ",", "json_args", "=", "None", ")", ":", "self", ".", "js_init_fn", "=", "js_func", "self", ".", "js_init_version", "=", "JS_API_VERSION", "if", "json_args", ":", "self", ".", "json_init_args", "=", "json_args"], "docstring": "Register a Javascript function to initialize the Javascript resources.\n\n        `js_func` is the name of a Javascript function defined by one of the\n        Javascript resources.  As part of setting up the browser's runtime\n        environment, the function will be invoked, passing a runtime object\n        and a DOM element.", "docstring_tokens": ["Register", "a", "Javascript", "function", "to", "initialize", "the", "Javascript", "resources", "."], "sha": "42d760d700d70465e4e573b7b41442d8802ccd3c", "url": "https://github.com/edx/web-fragments/blob/42d760d700d70465e4e573b7b41442d8802ccd3c/web_fragments/fragment.py#L189-L201", "partition": "test"}
{"repo": "vaexio/vaex", "path": "packages/vaex-core/vaex/delayed.py", "func_name": "delayed", "original_string": "def delayed(f):\n    '''Decorator to transparantly accept delayed computation.\n\n    Example:\n\n    >>> delayed_sum = ds.sum(ds.E, binby=ds.x, limits=limits,\n    >>>                   shape=4, delay=True)\n    >>> @vaex.delayed\n    >>> def total_sum(sums):\n    >>>     return sums.sum()\n    >>> sum_of_sums = total_sum(delayed_sum)\n    >>> ds.execute()\n    >>> sum_of_sums.get()\n    See the tutorial for a more complete example https://docs.vaex.io/en/latest/tutorial.html#Parallel-computations\n    '''\n\n    def wrapped(*args, **kwargs):\n        # print \"calling\", f, \"with\", kwargs\n        # key_values = kwargs.items()\n        key_promise = list([(key, promisify(value)) for key, value in kwargs.items()])\n        # key_promise = [(key, promisify(value)) for key, value in key_values]\n        arg_promises = list([promisify(value) for value in args])\n        kwarg_promises = list([promise for key, promise in key_promise])\n        promises = arg_promises + kwarg_promises\n        for promise in promises:\n            def echo_error(exc, promise=promise):\n                print(\"error with \", promise, \"exception is\", exc)\n                # raise exc\n\n            def echo(value, promise=promise):\n                print(\"done with \", repr(promise), \"value is\", value)\n            # promise.then(echo, echo_error)\n\n        # print promises\n        allarguments = aplus.listPromise(*promises)\n\n        def call(_):\n            kwargs_real = {key: promise.get() for key, promise in key_promise}\n            args_real = list([promise.get() for promise in arg_promises])\n            return f(*args_real, **kwargs_real)\n\n        def error(exc):\n            print(\"error\", exc)\n            raise exc\n        return allarguments.then(call, error)\n    return wrapped", "language": "python", "code": "def delayed(f):\n    '''Decorator to transparantly accept delayed computation.\n\n    Example:\n\n    >>> delayed_sum = ds.sum(ds.E, binby=ds.x, limits=limits,\n    >>>                   shape=4, delay=True)\n    >>> @vaex.delayed\n    >>> def total_sum(sums):\n    >>>     return sums.sum()\n    >>> sum_of_sums = total_sum(delayed_sum)\n    >>> ds.execute()\n    >>> sum_of_sums.get()\n    See the tutorial for a more complete example https://docs.vaex.io/en/latest/tutorial.html#Parallel-computations\n    '''\n\n    def wrapped(*args, **kwargs):\n        # print \"calling\", f, \"with\", kwargs\n        # key_values = kwargs.items()\n        key_promise = list([(key, promisify(value)) for key, value in kwargs.items()])\n        # key_promise = [(key, promisify(value)) for key, value in key_values]\n        arg_promises = list([promisify(value) for value in args])\n        kwarg_promises = list([promise for key, promise in key_promise])\n        promises = arg_promises + kwarg_promises\n        for promise in promises:\n            def echo_error(exc, promise=promise):\n                print(\"error with \", promise, \"exception is\", exc)\n                # raise exc\n\n            def echo(value, promise=promise):\n                print(\"done with \", repr(promise), \"value is\", value)\n            # promise.then(echo, echo_error)\n\n        # print promises\n        allarguments = aplus.listPromise(*promises)\n\n        def call(_):\n            kwargs_real = {key: promise.get() for key, promise in key_promise}\n            args_real = list([promise.get() for promise in arg_promises])\n            return f(*args_real, **kwargs_real)\n\n        def error(exc):\n            print(\"error\", exc)\n            raise exc\n        return allarguments.then(call, error)\n    return wrapped", "code_tokens": ["def", "delayed", "(", "f", ")", ":", "def", "wrapped", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "# print \"calling\", f, \"with\", kwargs", "# key_values = kwargs.items()", "key_promise", "=", "list", "(", "[", "(", "key", ",", "promisify", "(", "value", ")", ")", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", "]", ")", "# key_promise = [(key, promisify(value)) for key, value in key_values]", "arg_promises", "=", "list", "(", "[", "promisify", "(", "value", ")", "for", "value", "in", "args", "]", ")", "kwarg_promises", "=", "list", "(", "[", "promise", "for", "key", ",", "promise", "in", "key_promise", "]", ")", "promises", "=", "arg_promises", "+", "kwarg_promises", "for", "promise", "in", "promises", ":", "def", "echo_error", "(", "exc", ",", "promise", "=", "promise", ")", ":", "print", "(", "\"error with \"", ",", "promise", ",", "\"exception is\"", ",", "exc", ")", "# raise exc", "def", "echo", "(", "value", ",", "promise", "=", "promise", ")", ":", "print", "(", "\"done with \"", ",", "repr", "(", "promise", ")", ",", "\"value is\"", ",", "value", ")", "# promise.then(echo, echo_error)", "# print promises", "allarguments", "=", "aplus", ".", "listPromise", "(", "*", "promises", ")", "def", "call", "(", "_", ")", ":", "kwargs_real", "=", "{", "key", ":", "promise", ".", "get", "(", ")", "for", "key", ",", "promise", "in", "key_promise", "}", "args_real", "=", "list", "(", "[", "promise", ".", "get", "(", ")", "for", "promise", "in", "arg_promises", "]", ")", "return", "f", "(", "*", "args_real", ",", "*", "*", "kwargs_real", ")", "def", "error", "(", "exc", ")", ":", "print", "(", "\"error\"", ",", "exc", ")", "raise", "exc", "return", "allarguments", ".", "then", "(", "call", ",", "error", ")", "return", "wrapped"], "docstring": "Decorator to transparantly accept delayed computation.\n\n    Example:\n\n    >>> delayed_sum = ds.sum(ds.E, binby=ds.x, limits=limits,\n    >>>                   shape=4, delay=True)\n    >>> @vaex.delayed\n    >>> def total_sum(sums):\n    >>>     return sums.sum()\n    >>> sum_of_sums = total_sum(delayed_sum)\n    >>> ds.execute()\n    >>> sum_of_sums.get()\n    See the tutorial for a more complete example https://docs.vaex.io/en/latest/tutorial.html#Parallel-computations", "docstring_tokens": ["Decorator", "to", "transparantly", "accept", "delayed", "computation", "."], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/delayed.py#L28-L73", "partition": "test"}
{"repo": "lensacom/sparkit-learn", "path": "splearn/decomposition/truncated_svd.py", "func_name": "SparkTruncatedSVD.fit_transform", "original_string": "def fit_transform(self, Z):\n        \"\"\"Fit LSI model to X and perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        Returns\n        -------\n        X_new : array, shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        \"\"\"\n        X = Z[:, 'X'] if isinstance(Z, DictRDD) else Z\n        check_rdd(X, (sp.spmatrix, np.ndarray))\n        if self.algorithm == \"em\":\n            X = X.persist()  # boosting iterative svm\n            Sigma, V = svd_em(X, k=self.n_components, maxiter=self.n_iter,\n                              tol=self.tol, compute_u=False,\n                              seed=self.random_state)\n            self.components_ = V\n            X.unpersist()\n            return self.transform(Z)\n        else:\n            # TODO: raise warning non distributed\n            return super(SparkTruncatedSVD, self).fit_transform(X.tosparse())", "language": "python", "code": "def fit_transform(self, Z):\n        \"\"\"Fit LSI model to X and perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        Returns\n        -------\n        X_new : array, shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        \"\"\"\n        X = Z[:, 'X'] if isinstance(Z, DictRDD) else Z\n        check_rdd(X, (sp.spmatrix, np.ndarray))\n        if self.algorithm == \"em\":\n            X = X.persist()  # boosting iterative svm\n            Sigma, V = svd_em(X, k=self.n_components, maxiter=self.n_iter,\n                              tol=self.tol, compute_u=False,\n                              seed=self.random_state)\n            self.components_ = V\n            X.unpersist()\n            return self.transform(Z)\n        else:\n            # TODO: raise warning non distributed\n            return super(SparkTruncatedSVD, self).fit_transform(X.tosparse())", "code_tokens": ["def", "fit_transform", "(", "self", ",", "Z", ")", ":", "X", "=", "Z", "[", ":", ",", "'X'", "]", "if", "isinstance", "(", "Z", ",", "DictRDD", ")", "else", "Z", "check_rdd", "(", "X", ",", "(", "sp", ".", "spmatrix", ",", "np", ".", "ndarray", ")", ")", "if", "self", ".", "algorithm", "==", "\"em\"", ":", "X", "=", "X", ".", "persist", "(", ")", "# boosting iterative svm", "Sigma", ",", "V", "=", "svd_em", "(", "X", ",", "k", "=", "self", ".", "n_components", ",", "maxiter", "=", "self", ".", "n_iter", ",", "tol", "=", "self", ".", "tol", ",", "compute_u", "=", "False", ",", "seed", "=", "self", ".", "random_state", ")", "self", ".", "components_", "=", "V", "X", ".", "unpersist", "(", ")", "return", "self", ".", "transform", "(", "Z", ")", "else", ":", "# TODO: raise warning non distributed", "return", "super", "(", "SparkTruncatedSVD", ",", "self", ")", ".", "fit_transform", "(", "X", ".", "tosparse", "(", ")", ")"], "docstring": "Fit LSI model to X and perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        Returns\n        -------\n        X_new : array, shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.", "docstring_tokens": ["Fit", "LSI", "model", "to", "X", "and", "perform", "dimensionality", "reduction", "on", "X", "."], "sha": "0498502107c1f7dcf33cda0cdb6f5ba4b42524b7", "url": "https://github.com/lensacom/sparkit-learn/blob/0498502107c1f7dcf33cda0cdb6f5ba4b42524b7/splearn/decomposition/truncated_svd.py#L283-L308", "partition": "test"}
{"repo": "susam/ice", "path": "ice.py", "func_name": "Ice.exit", "original_string": "def exit(self):\n        \"\"\"Stop the simple WSGI server running the appliation.\"\"\"\n        if self._server is not None:\n            self._server.shutdown()\n            self._server.server_close()\n            self._server = None", "language": "python", "code": "def exit(self):\n        \"\"\"Stop the simple WSGI server running the appliation.\"\"\"\n        if self._server is not None:\n            self._server.shutdown()\n            self._server.server_close()\n            self._server = None", "code_tokens": ["def", "exit", "(", "self", ")", ":", "if", "self", ".", "_server", "is", "not", "None", ":", "self", ".", "_server", ".", "shutdown", "(", ")", "self", ".", "_server", ".", "server_close", "(", ")", "self", ".", "_server", "=", "None"], "docstring": "Stop the simple WSGI server running the appliation.", "docstring_tokens": ["Stop", "the", "simple", "WSGI", "server", "running", "the", "appliation", "."], "sha": "532e685c504ea96f9e42833594585159ac1d2068", "url": "https://github.com/susam/ice/blob/532e685c504ea96f9e42833594585159ac1d2068/ice.py#L119-L124", "partition": "test"}
{"repo": "LionelAuroux/pyrser", "path": "pyrser/parsing/stream.py", "func_name": "Stream.last_readed_line", "original_string": "def last_readed_line(self) -> str:\n        \"\"\"Usefull string to compute error message.\"\"\"\n        mpos = self._cursor.max_readed_position\n        mindex = mpos.index\n        # search last \\n\n        prevline = mindex - 1 if mindex == self.eos_index else mindex\n        while prevline >= 0 and self._content[prevline] != '\\n':\n            prevline -= 1\n        # search next \\n\n        nextline = mindex\n        while nextline < self.eos_index and self._content[nextline] != '\\n':\n            nextline += 1\n        last_line = self._content[prevline + 1:nextline]\n        return last_line", "language": "python", "code": "def last_readed_line(self) -> str:\n        \"\"\"Usefull string to compute error message.\"\"\"\n        mpos = self._cursor.max_readed_position\n        mindex = mpos.index\n        # search last \\n\n        prevline = mindex - 1 if mindex == self.eos_index else mindex\n        while prevline >= 0 and self._content[prevline] != '\\n':\n            prevline -= 1\n        # search next \\n\n        nextline = mindex\n        while nextline < self.eos_index and self._content[nextline] != '\\n':\n            nextline += 1\n        last_line = self._content[prevline + 1:nextline]\n        return last_line", "code_tokens": ["def", "last_readed_line", "(", "self", ")", "->", "str", ":", "mpos", "=", "self", ".", "_cursor", ".", "max_readed_position", "mindex", "=", "mpos", ".", "index", "# search last \\n", "prevline", "=", "mindex", "-", "1", "if", "mindex", "==", "self", ".", "eos_index", "else", "mindex", "while", "prevline", ">=", "0", "and", "self", ".", "_content", "[", "prevline", "]", "!=", "'\\n'", ":", "prevline", "-=", "1", "# search next \\n", "nextline", "=", "mindex", "while", "nextline", "<", "self", ".", "eos_index", "and", "self", ".", "_content", "[", "nextline", "]", "!=", "'\\n'", ":", "nextline", "+=", "1", "last_line", "=", "self", ".", "_content", "[", "prevline", "+", "1", ":", "nextline", "]", "return", "last_line"], "docstring": "Usefull string to compute error message.", "docstring_tokens": ["Usefull", "string", "to", "compute", "error", "message", "."], "sha": "f153a97ef2b6bf915a1ed468c0252a9a59b754d5", "url": "https://github.com/LionelAuroux/pyrser/blob/f153a97ef2b6bf915a1ed468c0252a9a59b754d5/pyrser/parsing/stream.py#L156-L169", "partition": "test"}
{"repo": "ninjaaron/libaaron", "path": "libaaron/libaaron.py", "func_name": "flatten", "original_string": "def flatten(iterable, map2iter=None):\n    \"\"\"recursively flatten nested objects\"\"\"\n    if map2iter and isinstance(iterable):\n        iterable = map2iter(iterable)\n\n    for item in iterable:\n        if isinstance(item, str) or not isinstance(item, abc.Iterable):\n            yield item\n        else:\n            yield from flatten(item, map2iter)", "language": "python", "code": "def flatten(iterable, map2iter=None):\n    \"\"\"recursively flatten nested objects\"\"\"\n    if map2iter and isinstance(iterable):\n        iterable = map2iter(iterable)\n\n    for item in iterable:\n        if isinstance(item, str) or not isinstance(item, abc.Iterable):\n            yield item\n        else:\n            yield from flatten(item, map2iter)", "code_tokens": ["def", "flatten", "(", "iterable", ",", "map2iter", "=", "None", ")", ":", "if", "map2iter", "and", "isinstance", "(", "iterable", ")", ":", "iterable", "=", "map2iter", "(", "iterable", ")", "for", "item", "in", "iterable", ":", "if", "isinstance", "(", "item", ",", "str", ")", "or", "not", "isinstance", "(", "item", ",", "abc", ".", "Iterable", ")", ":", "yield", "item", "else", ":", "yield", "from", "flatten", "(", "item", ",", "map2iter", ")"], "docstring": "recursively flatten nested objects", "docstring_tokens": ["recursively", "flatten", "nested", "objects"], "sha": "a2ee417b784ca72c89c05bddb2e3e815a6b95154", "url": "https://github.com/ninjaaron/libaaron/blob/a2ee417b784ca72c89c05bddb2e3e815a6b95154/libaaron/libaaron.py#L112-L121", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/console_widget.py", "func_name": "ConsoleWidget._append_html", "original_string": "def _append_html(self, html, before_prompt=False):\n        \"\"\" Appends HTML at the end of the console buffer.\n        \"\"\"\n        self._append_custom(self._insert_html, html, before_prompt)", "language": "python", "code": "def _append_html(self, html, before_prompt=False):\n        \"\"\" Appends HTML at the end of the console buffer.\n        \"\"\"\n        self._append_custom(self._insert_html, html, before_prompt)", "code_tokens": ["def", "_append_html", "(", "self", ",", "html", ",", "before_prompt", "=", "False", ")", ":", "self", ".", "_append_custom", "(", "self", ".", "_insert_html", ",", "html", ",", "before_prompt", ")"], "docstring": "Appends HTML at the end of the console buffer.", "docstring_tokens": ["Appends", "HTML", "at", "the", "end", "of", "the", "console", "buffer", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/console_widget.py#L861-L864", "partition": "test"}
{"repo": "takluyver/entrypoints", "path": "entrypoints.py", "func_name": "EntryPoint.from_string", "original_string": "def from_string(cls, epstr, name, distro=None):\n        \"\"\"Parse an entry point from the syntax in entry_points.txt\n\n        :param str epstr: The entry point string (not including 'name =')\n        :param str name: The name of this entry point\n        :param Distribution distro: The distribution in which the entry point was found\n        :rtype: EntryPoint\n        :raises BadEntryPoint: if *epstr* can't be parsed as an entry point.\n        \"\"\"\n        m = entry_point_pattern.match(epstr)\n        if m:\n            mod, obj, extras = m.group('modulename', 'objectname', 'extras')\n            if extras is not None:\n                extras = re.split(r',\\s*', extras)\n            return cls(name, mod, obj, extras, distro)\n        else:\n            raise BadEntryPoint(epstr)", "language": "python", "code": "def from_string(cls, epstr, name, distro=None):\n        \"\"\"Parse an entry point from the syntax in entry_points.txt\n\n        :param str epstr: The entry point string (not including 'name =')\n        :param str name: The name of this entry point\n        :param Distribution distro: The distribution in which the entry point was found\n        :rtype: EntryPoint\n        :raises BadEntryPoint: if *epstr* can't be parsed as an entry point.\n        \"\"\"\n        m = entry_point_pattern.match(epstr)\n        if m:\n            mod, obj, extras = m.group('modulename', 'objectname', 'extras')\n            if extras is not None:\n                extras = re.split(r',\\s*', extras)\n            return cls(name, mod, obj, extras, distro)\n        else:\n            raise BadEntryPoint(epstr)", "code_tokens": ["def", "from_string", "(", "cls", ",", "epstr", ",", "name", ",", "distro", "=", "None", ")", ":", "m", "=", "entry_point_pattern", ".", "match", "(", "epstr", ")", "if", "m", ":", "mod", ",", "obj", ",", "extras", "=", "m", ".", "group", "(", "'modulename'", ",", "'objectname'", ",", "'extras'", ")", "if", "extras", "is", "not", "None", ":", "extras", "=", "re", ".", "split", "(", "r',\\s*'", ",", "extras", ")", "return", "cls", "(", "name", ",", "mod", ",", "obj", ",", "extras", ",", "distro", ")", "else", ":", "raise", "BadEntryPoint", "(", "epstr", ")"], "docstring": "Parse an entry point from the syntax in entry_points.txt\n\n        :param str epstr: The entry point string (not including 'name =')\n        :param str name: The name of this entry point\n        :param Distribution distro: The distribution in which the entry point was found\n        :rtype: EntryPoint\n        :raises BadEntryPoint: if *epstr* can't be parsed as an entry point.", "docstring_tokens": ["Parse", "an", "entry", "point", "from", "the", "syntax", "in", "entry_points", ".", "txt"], "sha": "d7db29fd6136f86498d8c374f531d4c198d66bf6", "url": "https://github.com/takluyver/entrypoints/blob/d7db29fd6136f86498d8c374f531d4c198d66bf6/entrypoints.py#L90-L106", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/oinspect.py", "func_name": "Inspector._getdef", "original_string": "def _getdef(self,obj,oname=''):\n        \"\"\"Return the definition header for any callable object.\n\n        If any exception is generated, None is returned instead and the\n        exception is suppressed.\"\"\"\n\n        try:\n            # We need a plain string here, NOT unicode!\n            hdef = oname + inspect.formatargspec(*getargspec(obj))\n            return py3compat.unicode_to_str(hdef, 'ascii')\n        except:\n            return None", "language": "python", "code": "def _getdef(self,obj,oname=''):\n        \"\"\"Return the definition header for any callable object.\n\n        If any exception is generated, None is returned instead and the\n        exception is suppressed.\"\"\"\n\n        try:\n            # We need a plain string here, NOT unicode!\n            hdef = oname + inspect.formatargspec(*getargspec(obj))\n            return py3compat.unicode_to_str(hdef, 'ascii')\n        except:\n            return None", "code_tokens": ["def", "_getdef", "(", "self", ",", "obj", ",", "oname", "=", "''", ")", ":", "try", ":", "# We need a plain string here, NOT unicode!", "hdef", "=", "oname", "+", "inspect", ".", "formatargspec", "(", "*", "getargspec", "(", "obj", ")", ")", "return", "py3compat", ".", "unicode_to_str", "(", "hdef", ",", "'ascii'", ")", "except", ":", "return", "None"], "docstring": "Return the definition header for any callable object.\n\n        If any exception is generated, None is returned instead and the\n        exception is suppressed.", "docstring_tokens": ["Return", "the", "definition", "header", "for", "any", "callable", "object", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/oinspect.py#L314-L325", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/compiler/generator.py", "func_name": "_ns_var", "original_string": "def _ns_var(\n    py_ns_var: str = _NS_VAR, lisp_ns_var: str = LISP_NS_VAR, lisp_ns_ns: str = CORE_NS\n) -> ast.Assign:\n    \"\"\"Assign a Python variable named `ns_var` to the value of the current\n    namespace.\"\"\"\n    return ast.Assign(\n        targets=[ast.Name(id=py_ns_var, ctx=ast.Store())],\n        value=ast.Call(\n            func=_FIND_VAR_FN_NAME,\n            args=[\n                ast.Call(\n                    func=_NEW_SYM_FN_NAME,\n                    args=[ast.Str(lisp_ns_var)],\n                    keywords=[ast.keyword(arg=\"ns\", value=ast.Str(lisp_ns_ns))],\n                )\n            ],\n            keywords=[],\n        ),\n    )", "language": "python", "code": "def _ns_var(\n    py_ns_var: str = _NS_VAR, lisp_ns_var: str = LISP_NS_VAR, lisp_ns_ns: str = CORE_NS\n) -> ast.Assign:\n    \"\"\"Assign a Python variable named `ns_var` to the value of the current\n    namespace.\"\"\"\n    return ast.Assign(\n        targets=[ast.Name(id=py_ns_var, ctx=ast.Store())],\n        value=ast.Call(\n            func=_FIND_VAR_FN_NAME,\n            args=[\n                ast.Call(\n                    func=_NEW_SYM_FN_NAME,\n                    args=[ast.Str(lisp_ns_var)],\n                    keywords=[ast.keyword(arg=\"ns\", value=ast.Str(lisp_ns_ns))],\n                )\n            ],\n            keywords=[],\n        ),\n    )", "code_tokens": ["def", "_ns_var", "(", "py_ns_var", ":", "str", "=", "_NS_VAR", ",", "lisp_ns_var", ":", "str", "=", "LISP_NS_VAR", ",", "lisp_ns_ns", ":", "str", "=", "CORE_NS", ")", "->", "ast", ".", "Assign", ":", "return", "ast", ".", "Assign", "(", "targets", "=", "[", "ast", ".", "Name", "(", "id", "=", "py_ns_var", ",", "ctx", "=", "ast", ".", "Store", "(", ")", ")", "]", ",", "value", "=", "ast", ".", "Call", "(", "func", "=", "_FIND_VAR_FN_NAME", ",", "args", "=", "[", "ast", ".", "Call", "(", "func", "=", "_NEW_SYM_FN_NAME", ",", "args", "=", "[", "ast", ".", "Str", "(", "lisp_ns_var", ")", "]", ",", "keywords", "=", "[", "ast", ".", "keyword", "(", "arg", "=", "\"ns\"", ",", "value", "=", "ast", ".", "Str", "(", "lisp_ns_ns", ")", ")", "]", ",", ")", "]", ",", "keywords", "=", "[", "]", ",", ")", ",", ")"], "docstring": "Assign a Python variable named `ns_var` to the value of the current\n    namespace.", "docstring_tokens": ["Assign", "a", "Python", "variable", "named", "ns_var", "to", "the", "value", "of", "the", "current", "namespace", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/compiler/generator.py#L2487-L2505", "partition": "test"}
{"repo": "kgiusti/pyngus", "path": "pyngus/connection.py", "func_name": "Connection.create_receiver", "original_string": "def create_receiver(self, target_address, source_address=None,\n                        event_handler=None, name=None, properties=None):\n        \"\"\"Factory method for creating Receive links.\"\"\"\n        ident = name or str(target_address)\n        if ident in self._receiver_links:\n            raise KeyError(\"Receiver %s already exists!\" % ident)\n\n        session = _SessionProxy(\"session-%s\" % ident, self)\n        session.open()\n        rl = session.new_receiver(ident)\n        rl.configure(target_address, source_address, event_handler, properties)\n        self._receiver_links[ident] = rl\n        return rl", "language": "python", "code": "def create_receiver(self, target_address, source_address=None,\n                        event_handler=None, name=None, properties=None):\n        \"\"\"Factory method for creating Receive links.\"\"\"\n        ident = name or str(target_address)\n        if ident in self._receiver_links:\n            raise KeyError(\"Receiver %s already exists!\" % ident)\n\n        session = _SessionProxy(\"session-%s\" % ident, self)\n        session.open()\n        rl = session.new_receiver(ident)\n        rl.configure(target_address, source_address, event_handler, properties)\n        self._receiver_links[ident] = rl\n        return rl", "code_tokens": ["def", "create_receiver", "(", "self", ",", "target_address", ",", "source_address", "=", "None", ",", "event_handler", "=", "None", ",", "name", "=", "None", ",", "properties", "=", "None", ")", ":", "ident", "=", "name", "or", "str", "(", "target_address", ")", "if", "ident", "in", "self", ".", "_receiver_links", ":", "raise", "KeyError", "(", "\"Receiver %s already exists!\"", "%", "ident", ")", "session", "=", "_SessionProxy", "(", "\"session-%s\"", "%", "ident", ",", "self", ")", "session", ".", "open", "(", ")", "rl", "=", "session", ".", "new_receiver", "(", "ident", ")", "rl", ".", "configure", "(", "target_address", ",", "source_address", ",", "event_handler", ",", "properties", ")", "self", ".", "_receiver_links", "[", "ident", "]", "=", "rl", "return", "rl"], "docstring": "Factory method for creating Receive links.", "docstring_tokens": ["Factory", "method", "for", "creating", "Receive", "links", "."], "sha": "5392392046989f1bb84ba938c30e4d48311075f1", "url": "https://github.com/kgiusti/pyngus/blob/5392392046989f1bb84ba938c30e4d48311075f1/pyngus/connection.py#L681-L693", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/sample_halton_sequence.py", "func_name": "_base_expansion_size", "original_string": "def _base_expansion_size(num, bases):\n  \"\"\"Computes the number of terms in the place value expansion.\n\n  Let num = a0 + a1 b + a2 b^2 + ... ak b^k be the place value expansion of\n  `num` in base b (ak <> 0). This function computes and returns `k+1` for each\n  base `b` specified in `bases`.\n\n  This can be inferred from the base `b` logarithm of `num` as follows:\n    $$k = Floor(log_b (num)) + 1  = Floor( log(num) / log(b)) + 1$$\n\n  Args:\n    num: Scalar `Tensor` of dtype either `float32` or `float64`. The number to\n      compute the base expansion size of.\n    bases: `Tensor` of the same dtype as num. The bases to compute the size\n      against.\n\n  Returns:\n    Tensor of same dtype and shape as `bases` containing the size of num when\n    written in that base.\n  \"\"\"\n  return tf.floor(tf.math.log(num) / tf.math.log(bases)) + 1", "language": "python", "code": "def _base_expansion_size(num, bases):\n  \"\"\"Computes the number of terms in the place value expansion.\n\n  Let num = a0 + a1 b + a2 b^2 + ... ak b^k be the place value expansion of\n  `num` in base b (ak <> 0). This function computes and returns `k+1` for each\n  base `b` specified in `bases`.\n\n  This can be inferred from the base `b` logarithm of `num` as follows:\n    $$k = Floor(log_b (num)) + 1  = Floor( log(num) / log(b)) + 1$$\n\n  Args:\n    num: Scalar `Tensor` of dtype either `float32` or `float64`. The number to\n      compute the base expansion size of.\n    bases: `Tensor` of the same dtype as num. The bases to compute the size\n      against.\n\n  Returns:\n    Tensor of same dtype and shape as `bases` containing the size of num when\n    written in that base.\n  \"\"\"\n  return tf.floor(tf.math.log(num) / tf.math.log(bases)) + 1", "code_tokens": ["def", "_base_expansion_size", "(", "num", ",", "bases", ")", ":", "return", "tf", ".", "floor", "(", "tf", ".", "math", ".", "log", "(", "num", ")", "/", "tf", ".", "math", ".", "log", "(", "bases", ")", ")", "+", "1"], "docstring": "Computes the number of terms in the place value expansion.\n\n  Let num = a0 + a1 b + a2 b^2 + ... ak b^k be the place value expansion of\n  `num` in base b (ak <> 0). This function computes and returns `k+1` for each\n  base `b` specified in `bases`.\n\n  This can be inferred from the base `b` logarithm of `num` as follows:\n    $$k = Floor(log_b (num)) + 1  = Floor( log(num) / log(b)) + 1$$\n\n  Args:\n    num: Scalar `Tensor` of dtype either `float32` or `float64`. The number to\n      compute the base expansion size of.\n    bases: `Tensor` of the same dtype as num. The bases to compute the size\n      against.\n\n  Returns:\n    Tensor of same dtype and shape as `bases` containing the size of num when\n    written in that base.", "docstring_tokens": ["Computes", "the", "number", "of", "terms", "in", "the", "place", "value", "expansion", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/sample_halton_sequence.py#L345-L365", "partition": "test"}
{"repo": "python-useful-helpers/advanced-descriptors", "path": "advanced_descriptors/log_on_access.py", "func_name": "LogOnAccess.__traceback", "original_string": "def __traceback(self) -> str:\n        \"\"\"Get outer traceback text for logging.\"\"\"\n        if not self.log_traceback:\n            return \"\"\n        exc_info = sys.exc_info()\n        stack = traceback.extract_stack()\n        exc_tb = traceback.extract_tb(exc_info[2])\n        full_tb = stack[:1] + exc_tb  # cut decorator and build full traceback\n        exc_line: typing.List[str] = traceback.format_exception_only(*exc_info[:2])\n        # Make standard traceback string\n        tb_text = \"\\nTraceback (most recent call last):\\n\" + \"\".join(traceback.format_list(full_tb)) + \"\".join(exc_line)\n        return tb_text", "language": "python", "code": "def __traceback(self) -> str:\n        \"\"\"Get outer traceback text for logging.\"\"\"\n        if not self.log_traceback:\n            return \"\"\n        exc_info = sys.exc_info()\n        stack = traceback.extract_stack()\n        exc_tb = traceback.extract_tb(exc_info[2])\n        full_tb = stack[:1] + exc_tb  # cut decorator and build full traceback\n        exc_line: typing.List[str] = traceback.format_exception_only(*exc_info[:2])\n        # Make standard traceback string\n        tb_text = \"\\nTraceback (most recent call last):\\n\" + \"\".join(traceback.format_list(full_tb)) + \"\".join(exc_line)\n        return tb_text", "code_tokens": ["def", "__traceback", "(", "self", ")", "->", "str", ":", "if", "not", "self", ".", "log_traceback", ":", "return", "\"\"", "exc_info", "=", "sys", ".", "exc_info", "(", ")", "stack", "=", "traceback", ".", "extract_stack", "(", ")", "exc_tb", "=", "traceback", ".", "extract_tb", "(", "exc_info", "[", "2", "]", ")", "full_tb", "=", "stack", "[", ":", "1", "]", "+", "exc_tb", "# cut decorator and build full traceback", "exc_line", ":", "typing", ".", "List", "[", "str", "]", "=", "traceback", ".", "format_exception_only", "(", "*", "exc_info", "[", ":", "2", "]", ")", "# Make standard traceback string", "tb_text", "=", "\"\\nTraceback (most recent call last):\\n\"", "+", "\"\"", ".", "join", "(", "traceback", ".", "format_list", "(", "full_tb", ")", ")", "+", "\"\"", ".", "join", "(", "exc_line", ")", "return", "tb_text"], "docstring": "Get outer traceback text for logging.", "docstring_tokens": ["Get", "outer", "traceback", "text", "for", "logging", "."], "sha": "17ee4a35b3bfcb4adf4ed2f41e75c4c6b71cb003", "url": "https://github.com/python-useful-helpers/advanced-descriptors/blob/17ee4a35b3bfcb4adf4ed2f41e75c4c6b71cb003/advanced_descriptors/log_on_access.py#L183-L194", "partition": "test"}
{"repo": "aholkner/bacon", "path": "native/Vendor/FreeType/src/tools/docmaker/utils.py", "func_name": "make_file_list", "original_string": "def  make_file_list( args = None ):\n    \"\"\"builds a list of input files from command-line arguments\"\"\"\n    file_list = []\n    # sys.stderr.write( repr( sys.argv[1 :] ) + '\\n' )\n\n    if not args:\n        args = sys.argv[1 :]\n\n    for pathname in args:\n        if string.find( pathname, '*' ) >= 0:\n            newpath = glob.glob( pathname )\n            newpath.sort()  # sort files -- this is important because\n                            # of the order of files\n        else:\n            newpath = [pathname]\n\n        file_list.extend( newpath )\n\n    if len( file_list ) == 0:\n        file_list = None\n    else:\n        # now filter the file list to remove non-existing ones\n        file_list = filter( file_exists, file_list )\n\n    return file_list", "language": "python", "code": "def  make_file_list( args = None ):\n    \"\"\"builds a list of input files from command-line arguments\"\"\"\n    file_list = []\n    # sys.stderr.write( repr( sys.argv[1 :] ) + '\\n' )\n\n    if not args:\n        args = sys.argv[1 :]\n\n    for pathname in args:\n        if string.find( pathname, '*' ) >= 0:\n            newpath = glob.glob( pathname )\n            newpath.sort()  # sort files -- this is important because\n                            # of the order of files\n        else:\n            newpath = [pathname]\n\n        file_list.extend( newpath )\n\n    if len( file_list ) == 0:\n        file_list = None\n    else:\n        # now filter the file list to remove non-existing ones\n        file_list = filter( file_exists, file_list )\n\n    return file_list", "code_tokens": ["def", "make_file_list", "(", "args", "=", "None", ")", ":", "file_list", "=", "[", "]", "# sys.stderr.write( repr( sys.argv[1 :] ) + '\\n' )", "if", "not", "args", ":", "args", "=", "sys", ".", "argv", "[", "1", ":", "]", "for", "pathname", "in", "args", ":", "if", "string", ".", "find", "(", "pathname", ",", "'*'", ")", ">=", "0", ":", "newpath", "=", "glob", ".", "glob", "(", "pathname", ")", "newpath", ".", "sort", "(", ")", "# sort files -- this is important because", "# of the order of files", "else", ":", "newpath", "=", "[", "pathname", "]", "file_list", ".", "extend", "(", "newpath", ")", "if", "len", "(", "file_list", ")", "==", "0", ":", "file_list", "=", "None", "else", ":", "# now filter the file list to remove non-existing ones", "file_list", "=", "filter", "(", "file_exists", ",", "file_list", ")", "return", "file_list"], "docstring": "builds a list of input files from command-line arguments", "docstring_tokens": ["builds", "a", "list", "of", "input", "files", "from", "command", "-", "line", "arguments"], "sha": "edf3810dcb211942d392a8637945871399b0650d", "url": "https://github.com/aholkner/bacon/blob/edf3810dcb211942d392a8637945871399b0650d/native/Vendor/FreeType/src/tools/docmaker/utils.py#L106-L130", "partition": "test"}
{"repo": "chrisjrn/registrasion", "path": "registrasion/views.py", "func_name": "render_badge", "original_string": "def render_badge(user):\n    ''' Renders a single user's badge. '''\n\n    data = {\n        \"user\": user,\n    }\n\n    t = loader.get_template('registrasion/badge.svg')\n    return t.render(data)", "language": "python", "code": "def render_badge(user):\n    ''' Renders a single user's badge. '''\n\n    data = {\n        \"user\": user,\n    }\n\n    t = loader.get_template('registrasion/badge.svg')\n    return t.render(data)", "code_tokens": ["def", "render_badge", "(", "user", ")", ":", "data", "=", "{", "\"user\"", ":", "user", ",", "}", "t", "=", "loader", ".", "get_template", "(", "'registrasion/badge.svg'", ")", "return", "t", ".", "render", "(", "data", ")"], "docstring": "Renders a single user's badge.", "docstring_tokens": ["Renders", "a", "single", "user", "s", "badge", "."], "sha": "461d5846c6f9f3b7099322a94f5d9911564448e4", "url": "https://github.com/chrisjrn/registrasion/blob/461d5846c6f9f3b7099322a94f5d9911564448e4/registrasion/views.py#L1131-L1139", "partition": "test"}
{"repo": "celiao/tmdbsimple", "path": "tmdbsimple/movies.py", "func_name": "Movies.rating", "original_string": "def rating(self, **kwargs):\n        \"\"\"\n        This method lets users rate a movie. A valid session id or guest\n        session id is required.\n\n        Args:\n            session_id: see Authentication.\n            guest_session_id: see Authentication.\n            value: Rating value.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('rating')\n\n        payload = {\n            'value': kwargs.pop('value', None),\n        }\n\n        response = self._POST(path, kwargs, payload)\n        self._set_attrs_to_values(response)\n        return response", "language": "python", "code": "def rating(self, **kwargs):\n        \"\"\"\n        This method lets users rate a movie. A valid session id or guest\n        session id is required.\n\n        Args:\n            session_id: see Authentication.\n            guest_session_id: see Authentication.\n            value: Rating value.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('rating')\n\n        payload = {\n            'value': kwargs.pop('value', None),\n        }\n\n        response = self._POST(path, kwargs, payload)\n        self._set_attrs_to_values(response)\n        return response", "code_tokens": ["def", "rating", "(", "self", ",", "*", "*", "kwargs", ")", ":", "path", "=", "self", ".", "_get_id_path", "(", "'rating'", ")", "payload", "=", "{", "'value'", ":", "kwargs", ".", "pop", "(", "'value'", ",", "None", ")", ",", "}", "response", "=", "self", ".", "_POST", "(", "path", ",", "kwargs", ",", "payload", ")", "self", ".", "_set_attrs_to_values", "(", "response", ")", "return", "response"], "docstring": "This method lets users rate a movie. A valid session id or guest\n        session id is required.\n\n        Args:\n            session_id: see Authentication.\n            guest_session_id: see Authentication.\n            value: Rating value.\n\n        Returns:\n            A dict representation of the JSON returned from the API.", "docstring_tokens": ["This", "method", "lets", "users", "rate", "a", "movie", ".", "A", "valid", "session", "id", "or", "guest", "session", "id", "is", "required", "."], "sha": "ff17893110c99771d6398a62c35d36dd9735f4b9", "url": "https://github.com/celiao/tmdbsimple/blob/ff17893110c99771d6398a62c35d36dd9735f4b9/tmdbsimple/movies.py#L415-L436", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/adapter/mongo/case_events.py", "func_name": "CaseEventHandler.mark_checked", "original_string": "def mark_checked(self, institute, case, user, link,\n                     unmark=False):\n        \"\"\"Mark a case as checked from an analysis point of view.\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            unmark (bool): If case should ve unmarked\n\n        Return:\n            updated_case\n        \"\"\"\n\n        LOG.info(\"Updating checked status of {}\"\n                    .format(case['display_name']))\n\n        status = 'not checked' if unmark else 'checked'\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='case',\n            verb='check_case',\n            subject=status\n        )\n\n        LOG.info(\"Updating {0}'s checked status {1}\"\n                    .format(case['display_name'], status))\n        analysis_checked = False if unmark else True\n        updated_case = self.case_collection.find_one_and_update(\n            {'_id': case['_id']},\n            {\n                '$set': {'analysis_checked': analysis_checked}\n            },\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n        LOG.debug(\"Case updated\")\n        return updated_case", "language": "python", "code": "def mark_checked(self, institute, case, user, link,\n                     unmark=False):\n        \"\"\"Mark a case as checked from an analysis point of view.\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            unmark (bool): If case should ve unmarked\n\n        Return:\n            updated_case\n        \"\"\"\n\n        LOG.info(\"Updating checked status of {}\"\n                    .format(case['display_name']))\n\n        status = 'not checked' if unmark else 'checked'\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='case',\n            verb='check_case',\n            subject=status\n        )\n\n        LOG.info(\"Updating {0}'s checked status {1}\"\n                    .format(case['display_name'], status))\n        analysis_checked = False if unmark else True\n        updated_case = self.case_collection.find_one_and_update(\n            {'_id': case['_id']},\n            {\n                '$set': {'analysis_checked': analysis_checked}\n            },\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n        LOG.debug(\"Case updated\")\n        return updated_case", "code_tokens": ["def", "mark_checked", "(", "self", ",", "institute", ",", "case", ",", "user", ",", "link", ",", "unmark", "=", "False", ")", ":", "LOG", ".", "info", "(", "\"Updating checked status of {}\"", ".", "format", "(", "case", "[", "'display_name'", "]", ")", ")", "status", "=", "'not checked'", "if", "unmark", "else", "'checked'", "self", ".", "create_event", "(", "institute", "=", "institute", ",", "case", "=", "case", ",", "user", "=", "user", ",", "link", "=", "link", ",", "category", "=", "'case'", ",", "verb", "=", "'check_case'", ",", "subject", "=", "status", ")", "LOG", ".", "info", "(", "\"Updating {0}'s checked status {1}\"", ".", "format", "(", "case", "[", "'display_name'", "]", ",", "status", ")", ")", "analysis_checked", "=", "False", "if", "unmark", "else", "True", "updated_case", "=", "self", ".", "case_collection", ".", "find_one_and_update", "(", "{", "'_id'", ":", "case", "[", "'_id'", "]", "}", ",", "{", "'$set'", ":", "{", "'analysis_checked'", ":", "analysis_checked", "}", "}", ",", "return_document", "=", "pymongo", ".", "ReturnDocument", ".", "AFTER", ")", "LOG", ".", "debug", "(", "\"Case updated\"", ")", "return", "updated_case"], "docstring": "Mark a case as checked from an analysis point of view.\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            unmark (bool): If case should ve unmarked\n\n        Return:\n            updated_case", "docstring_tokens": ["Mark", "a", "case", "as", "checked", "from", "an", "analysis", "point", "of", "view", "."], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/adapter/mongo/case_events.py#L482-L522", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/task_runner/cgroup_task_runner.py", "func_name": "CgroupTaskRunner._delete_cgroup", "original_string": "def _delete_cgroup(self, path):\n        \"\"\"\n        Delete the specified cgroup.\n\n        :param path: The path of the cgroup to delete.\n        E.g. cpu/mygroup/mysubgroup\n        \"\"\"\n        node = trees.Tree().root\n        path_split = path.split(\"/\")\n        for path_element in path_split:\n            name_to_node = {x.name: x for x in node.children}\n            if path_element not in name_to_node:\n                self.log.warning(\"Cgroup does not exist: %s\", path)\n                return\n            else:\n                node = name_to_node[path_element]\n        # node is now the leaf node\n        parent = node.parent\n        self.log.debug(\"Deleting cgroup %s/%s\", parent, node.name)\n        parent.delete_cgroup(node.name)", "language": "python", "code": "def _delete_cgroup(self, path):\n        \"\"\"\n        Delete the specified cgroup.\n\n        :param path: The path of the cgroup to delete.\n        E.g. cpu/mygroup/mysubgroup\n        \"\"\"\n        node = trees.Tree().root\n        path_split = path.split(\"/\")\n        for path_element in path_split:\n            name_to_node = {x.name: x for x in node.children}\n            if path_element not in name_to_node:\n                self.log.warning(\"Cgroup does not exist: %s\", path)\n                return\n            else:\n                node = name_to_node[path_element]\n        # node is now the leaf node\n        parent = node.parent\n        self.log.debug(\"Deleting cgroup %s/%s\", parent, node.name)\n        parent.delete_cgroup(node.name)", "code_tokens": ["def", "_delete_cgroup", "(", "self", ",", "path", ")", ":", "node", "=", "trees", ".", "Tree", "(", ")", ".", "root", "path_split", "=", "path", ".", "split", "(", "\"/\"", ")", "for", "path_element", "in", "path_split", ":", "name_to_node", "=", "{", "x", ".", "name", ":", "x", "for", "x", "in", "node", ".", "children", "}", "if", "path_element", "not", "in", "name_to_node", ":", "self", ".", "log", ".", "warning", "(", "\"Cgroup does not exist: %s\"", ",", "path", ")", "return", "else", ":", "node", "=", "name_to_node", "[", "path_element", "]", "# node is now the leaf node", "parent", "=", "node", ".", "parent", "self", ".", "log", ".", "debug", "(", "\"Deleting cgroup %s/%s\"", ",", "parent", ",", "node", ".", "name", ")", "parent", ".", "delete_cgroup", "(", "node", ".", "name", ")"], "docstring": "Delete the specified cgroup.\n\n        :param path: The path of the cgroup to delete.\n        E.g. cpu/mygroup/mysubgroup", "docstring_tokens": ["Delete", "the", "specified", "cgroup", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/task_runner/cgroup_task_runner.py#L90-L109", "partition": "test"}
{"repo": "pydron/anycall", "path": "anycall/packetprotocol.py", "func_name": "PacketProtocol.dataReceived", "original_string": "def dataReceived(self, data):\n        \"\"\"\n        Do not overwrite this method. Instead implement `on_...` methods for the\n        registered typenames to handle incomming packets.\n        \"\"\"\n        \n        self._unprocessed_data.enqueue(data)\n        \n        \n        while True:\n            if len(self._unprocessed_data) < self._header.size:\n                return # not yet enough data\n            \n            hdr_data = self._unprocessed_data.peek(self._header.size)\n            packet_length, typekey = self._header.unpack(hdr_data)\n            total_length = self._header.size + packet_length\n            \n            if len(self._unprocessed_data) < total_length:\n                return # not yet enough data\n            \n            self._unprocessed_data.drop(self._header.size)\n            packet = self._unprocessed_data.dequeue(packet_length)\n            \n            self._start_receive = None\n            \n            typename = self._type_register.get(typekey, None)\n            if typename is None:\n                self.on_unregistered_type(typekey, packet)\n            else:\n                self.packet_received(typename, packet)", "language": "python", "code": "def dataReceived(self, data):\n        \"\"\"\n        Do not overwrite this method. Instead implement `on_...` methods for the\n        registered typenames to handle incomming packets.\n        \"\"\"\n        \n        self._unprocessed_data.enqueue(data)\n        \n        \n        while True:\n            if len(self._unprocessed_data) < self._header.size:\n                return # not yet enough data\n            \n            hdr_data = self._unprocessed_data.peek(self._header.size)\n            packet_length, typekey = self._header.unpack(hdr_data)\n            total_length = self._header.size + packet_length\n            \n            if len(self._unprocessed_data) < total_length:\n                return # not yet enough data\n            \n            self._unprocessed_data.drop(self._header.size)\n            packet = self._unprocessed_data.dequeue(packet_length)\n            \n            self._start_receive = None\n            \n            typename = self._type_register.get(typekey, None)\n            if typename is None:\n                self.on_unregistered_type(typekey, packet)\n            else:\n                self.packet_received(typename, packet)", "code_tokens": ["def", "dataReceived", "(", "self", ",", "data", ")", ":", "self", ".", "_unprocessed_data", ".", "enqueue", "(", "data", ")", "while", "True", ":", "if", "len", "(", "self", ".", "_unprocessed_data", ")", "<", "self", ".", "_header", ".", "size", ":", "return", "# not yet enough data", "hdr_data", "=", "self", ".", "_unprocessed_data", ".", "peek", "(", "self", ".", "_header", ".", "size", ")", "packet_length", ",", "typekey", "=", "self", ".", "_header", ".", "unpack", "(", "hdr_data", ")", "total_length", "=", "self", ".", "_header", ".", "size", "+", "packet_length", "if", "len", "(", "self", ".", "_unprocessed_data", ")", "<", "total_length", ":", "return", "# not yet enough data", "self", ".", "_unprocessed_data", ".", "drop", "(", "self", ".", "_header", ".", "size", ")", "packet", "=", "self", ".", "_unprocessed_data", ".", "dequeue", "(", "packet_length", ")", "self", ".", "_start_receive", "=", "None", "typename", "=", "self", ".", "_type_register", ".", "get", "(", "typekey", ",", "None", ")", "if", "typename", "is", "None", ":", "self", ".", "on_unregistered_type", "(", "typekey", ",", "packet", ")", "else", ":", "self", ".", "packet_received", "(", "typename", ",", "packet", ")"], "docstring": "Do not overwrite this method. Instead implement `on_...` methods for the\n        registered typenames to handle incomming packets.", "docstring_tokens": ["Do", "not", "overwrite", "this", "method", ".", "Instead", "implement", "on_", "...", "methods", "for", "the", "registered", "typenames", "to", "handle", "incomming", "packets", "."], "sha": "43add96660258a14b24aa8e8413dffb1741b72d7", "url": "https://github.com/pydron/anycall/blob/43add96660258a14b24aa8e8413dffb1741b72d7/anycall/packetprotocol.py#L73-L102", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/simulator/simModel.py", "func_name": "simEvalCond", "original_string": "def simEvalCond(simulator, *conds):\n    \"\"\"\n    Evaluate list of values as condition\n    \"\"\"\n    _cond = True\n    _vld = True\n    for v in conds:\n        val = bool(v.val)\n        fullVld = v.vldMask == 1\n        if fullVld:\n            if not val:\n                return False, True\n        else:\n            return False, False\n\n        _cond = _cond and val\n        _vld = _vld and fullVld\n\n    return _cond, _vld", "language": "python", "code": "def simEvalCond(simulator, *conds):\n    \"\"\"\n    Evaluate list of values as condition\n    \"\"\"\n    _cond = True\n    _vld = True\n    for v in conds:\n        val = bool(v.val)\n        fullVld = v.vldMask == 1\n        if fullVld:\n            if not val:\n                return False, True\n        else:\n            return False, False\n\n        _cond = _cond and val\n        _vld = _vld and fullVld\n\n    return _cond, _vld", "code_tokens": ["def", "simEvalCond", "(", "simulator", ",", "*", "conds", ")", ":", "_cond", "=", "True", "_vld", "=", "True", "for", "v", "in", "conds", ":", "val", "=", "bool", "(", "v", ".", "val", ")", "fullVld", "=", "v", ".", "vldMask", "==", "1", "if", "fullVld", ":", "if", "not", "val", ":", "return", "False", ",", "True", "else", ":", "return", "False", ",", "False", "_cond", "=", "_cond", "and", "val", "_vld", "=", "_vld", "and", "fullVld", "return", "_cond", ",", "_vld"], "docstring": "Evaluate list of values as condition", "docstring_tokens": ["Evaluate", "list", "of", "values", "as", "condition"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/simulator/simModel.py#L31-L49", "partition": "test"}
{"repo": "LionelAuroux/pyrser", "path": "pyrser/ast/state.py", "func_name": "StateRegister.add_state", "original_string": "def add_state(self, s: State):\n        \"\"\"\n        all state in the register have a uid\n        \"\"\"\n        ids = id(s)\n        uid = len(self.states)\n        if ids not in self.states:\n            self.states[ids] = (uid, s)", "language": "python", "code": "def add_state(self, s: State):\n        \"\"\"\n        all state in the register have a uid\n        \"\"\"\n        ids = id(s)\n        uid = len(self.states)\n        if ids not in self.states:\n            self.states[ids] = (uid, s)", "code_tokens": ["def", "add_state", "(", "self", ",", "s", ":", "State", ")", ":", "ids", "=", "id", "(", "s", ")", "uid", "=", "len", "(", "self", ".", "states", ")", "if", "ids", "not", "in", "self", ".", "states", ":", "self", ".", "states", "[", "ids", "]", "=", "(", "uid", ",", "s", ")"], "docstring": "all state in the register have a uid", "docstring_tokens": ["all", "state", "in", "the", "register", "have", "a", "uid"], "sha": "f153a97ef2b6bf915a1ed468c0252a9a59b754d5", "url": "https://github.com/LionelAuroux/pyrser/blob/f153a97ef2b6bf915a1ed468c0252a9a59b754d5/pyrser/ast/state.py#L47-L54", "partition": "test"}
{"repo": "MisterY/price-database", "path": "pricedb/config.py", "func_name": "Config.get_config_path", "original_string": "def get_config_path(self) -> str:\n        \"\"\"\n        Returns the path where the active config file is expected.\n        This is the user's profile folder.\n        \"\"\"\n        dst_dir = self.__get_user_path()\n        dst = dst_dir + \"/\" + config_filename\n        return dst", "language": "python", "code": "def get_config_path(self) -> str:\n        \"\"\"\n        Returns the path where the active config file is expected.\n        This is the user's profile folder.\n        \"\"\"\n        dst_dir = self.__get_user_path()\n        dst = dst_dir + \"/\" + config_filename\n        return dst", "code_tokens": ["def", "get_config_path", "(", "self", ")", "->", "str", ":", "dst_dir", "=", "self", ".", "__get_user_path", "(", ")", "dst", "=", "dst_dir", "+", "\"/\"", "+", "config_filename", "return", "dst"], "docstring": "Returns the path where the active config file is expected.\n        This is the user's profile folder.", "docstring_tokens": ["Returns", "the", "path", "where", "the", "active", "config", "file", "is", "expected", ".", "This", "is", "the", "user", "s", "profile", "folder", "."], "sha": "b4fd366b7763891c690fe3000b8840e656da023e", "url": "https://github.com/MisterY/price-database/blob/b4fd366b7763891c690fe3000b8840e656da023e/pricedb/config.py#L91-L98", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/serializer/systemC/statements.py", "func_name": "SystemCSerializer_statements.HWProcess", "original_string": "def HWProcess(cls, proc, ctx):\n        \"\"\"\n        Serialize HWProcess instance\n\n        :param scope: name scope to prevent name collisions\n        \"\"\"\n        body = proc.statements\n        childCtx = ctx.withIndent()\n        statemets = [cls.asHdl(s, childCtx) for s in body]\n        proc.name = ctx.scope.checkedName(proc.name, proc)\n\n        return cls.methodTmpl.render(\n            indent=getIndent(ctx.indent),\n            name=proc.name,\n            statements=statemets\n        )", "language": "python", "code": "def HWProcess(cls, proc, ctx):\n        \"\"\"\n        Serialize HWProcess instance\n\n        :param scope: name scope to prevent name collisions\n        \"\"\"\n        body = proc.statements\n        childCtx = ctx.withIndent()\n        statemets = [cls.asHdl(s, childCtx) for s in body]\n        proc.name = ctx.scope.checkedName(proc.name, proc)\n\n        return cls.methodTmpl.render(\n            indent=getIndent(ctx.indent),\n            name=proc.name,\n            statements=statemets\n        )", "code_tokens": ["def", "HWProcess", "(", "cls", ",", "proc", ",", "ctx", ")", ":", "body", "=", "proc", ".", "statements", "childCtx", "=", "ctx", ".", "withIndent", "(", ")", "statemets", "=", "[", "cls", ".", "asHdl", "(", "s", ",", "childCtx", ")", "for", "s", "in", "body", "]", "proc", ".", "name", "=", "ctx", ".", "scope", ".", "checkedName", "(", "proc", ".", "name", ",", "proc", ")", "return", "cls", ".", "methodTmpl", ".", "render", "(", "indent", "=", "getIndent", "(", "ctx", ".", "indent", ")", ",", "name", "=", "proc", ".", "name", ",", "statements", "=", "statemets", ")"], "docstring": "Serialize HWProcess instance\n\n        :param scope: name scope to prevent name collisions", "docstring_tokens": ["Serialize", "HWProcess", "instance"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/serializer/systemC/statements.py#L43-L58", "partition": "test"}
{"repo": "incuna/django-pgcrypto-fields", "path": "pgcrypto/mixins.py", "func_name": "get_setting", "original_string": "def get_setting(connection, key):\n    \"\"\"Get key from connection or default to settings.\"\"\"\n    if key in connection.settings_dict:\n        return connection.settings_dict[key]\n    else:\n        return getattr(settings, key)", "language": "python", "code": "def get_setting(connection, key):\n    \"\"\"Get key from connection or default to settings.\"\"\"\n    if key in connection.settings_dict:\n        return connection.settings_dict[key]\n    else:\n        return getattr(settings, key)", "code_tokens": ["def", "get_setting", "(", "connection", ",", "key", ")", ":", "if", "key", "in", "connection", ".", "settings_dict", ":", "return", "connection", ".", "settings_dict", "[", "key", "]", "else", ":", "return", "getattr", "(", "settings", ",", "key", ")"], "docstring": "Get key from connection or default to settings.", "docstring_tokens": ["Get", "key", "from", "connection", "or", "default", "to", "settings", "."], "sha": "406fddf0cbe9091ba71b97206d0f4719c0450ac1", "url": "https://github.com/incuna/django-pgcrypto-fields/blob/406fddf0cbe9091ba71b97206d0f4719c0450ac1/pgcrypto/mixins.py#L13-L18", "partition": "test"}
{"repo": "funilrys/PyFunceble", "path": "PyFunceble/database.py", "func_name": "Whois._retrieve", "original_string": "def _retrieve(self):\n        \"\"\"\n        Retrieve the data from the database.\n        \"\"\"\n\n        if self._authorization() and \"whois_db\" not in PyFunceble.INTERN:\n            # The usage of the whois database is activated.\n\n            if PyFunceble.path.isfile(self.whois_db_path):\n                # The database file exist.\n\n                # We merge our current database into already initiated one.\n                PyFunceble.INTERN[\"whois_db\"] = Dict().from_json(\n                    File(self.whois_db_path).read()\n                )\n            else:\n                # The database file does not exist.\n\n                # We initiate an empty database.\n                PyFunceble.INTERN[\"whois_db\"] = {}", "language": "python", "code": "def _retrieve(self):\n        \"\"\"\n        Retrieve the data from the database.\n        \"\"\"\n\n        if self._authorization() and \"whois_db\" not in PyFunceble.INTERN:\n            # The usage of the whois database is activated.\n\n            if PyFunceble.path.isfile(self.whois_db_path):\n                # The database file exist.\n\n                # We merge our current database into already initiated one.\n                PyFunceble.INTERN[\"whois_db\"] = Dict().from_json(\n                    File(self.whois_db_path).read()\n                )\n            else:\n                # The database file does not exist.\n\n                # We initiate an empty database.\n                PyFunceble.INTERN[\"whois_db\"] = {}", "code_tokens": ["def", "_retrieve", "(", "self", ")", ":", "if", "self", ".", "_authorization", "(", ")", "and", "\"whois_db\"", "not", "in", "PyFunceble", ".", "INTERN", ":", "# The usage of the whois database is activated.", "if", "PyFunceble", ".", "path", ".", "isfile", "(", "self", ".", "whois_db_path", ")", ":", "# The database file exist.", "# We merge our current database into already initiated one.", "PyFunceble", ".", "INTERN", "[", "\"whois_db\"", "]", "=", "Dict", "(", ")", ".", "from_json", "(", "File", "(", "self", ".", "whois_db_path", ")", ".", "read", "(", ")", ")", "else", ":", "# The database file does not exist.", "# We initiate an empty database.", "PyFunceble", ".", "INTERN", "[", "\"whois_db\"", "]", "=", "{", "}"], "docstring": "Retrieve the data from the database.", "docstring_tokens": ["Retrieve", "the", "data", "from", "the", "database", "."], "sha": "cdf69cbde120199171f7158e1c33635753e6e2f5", "url": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/database.py#L726-L745", "partition": "test"}
{"repo": "addok/addok", "path": "addok/shell.py", "func_name": "Cmd.do_DBKEY", "original_string": "def do_DBKEY(self, key):\n        \"\"\"Print raw content of a DB key.\n        DBKEY g|u09tyzfe\"\"\"\n        type_ = DB.type(key).decode()\n        if type_ == 'set':\n            out = DB.smembers(key)\n        elif type_ == 'string':\n            out = DB.get(key)\n        else:\n            out = 'Unsupported type {}'.format(type_)\n        print('type:', magenta(type_))\n        print('value:', white(out))", "language": "python", "code": "def do_DBKEY(self, key):\n        \"\"\"Print raw content of a DB key.\n        DBKEY g|u09tyzfe\"\"\"\n        type_ = DB.type(key).decode()\n        if type_ == 'set':\n            out = DB.smembers(key)\n        elif type_ == 'string':\n            out = DB.get(key)\n        else:\n            out = 'Unsupported type {}'.format(type_)\n        print('type:', magenta(type_))\n        print('value:', white(out))", "code_tokens": ["def", "do_DBKEY", "(", "self", ",", "key", ")", ":", "type_", "=", "DB", ".", "type", "(", "key", ")", ".", "decode", "(", ")", "if", "type_", "==", "'set'", ":", "out", "=", "DB", ".", "smembers", "(", "key", ")", "elif", "type_", "==", "'string'", ":", "out", "=", "DB", ".", "get", "(", "key", ")", "else", ":", "out", "=", "'Unsupported type {}'", ".", "format", "(", "type_", ")", "print", "(", "'type:'", ",", "magenta", "(", "type_", ")", ")", "print", "(", "'value:'", ",", "white", "(", "out", ")", ")"], "docstring": "Print raw content of a DB key.\n        DBKEY g|u09tyzfe", "docstring_tokens": ["Print", "raw", "content", "of", "a", "DB", "key", ".", "DBKEY", "g|u09tyzfe"], "sha": "46a270d76ec778d2b445c2be753e5c6ba070a9b2", "url": "https://github.com/addok/addok/blob/46a270d76ec778d2b445c2be753e5c6ba070a9b2/addok/shell.py#L252-L263", "partition": "test"}
{"repo": "deepmipt/DeepPavlov", "path": "deeppavlov/models/morpho_tagger/network.py", "func_name": "CharacterTagger.predict_on_batch", "original_string": "def predict_on_batch(self, data: Union[list, tuple],\n                         return_indexes: bool = False) -> List[List[str]]:\n        \"\"\"\n        Makes predictions on a single batch\n\n        Args:\n            data: a batch of word sequences together with additional inputs\n            return_indexes: whether to return tag indexes in vocabulary or tags themselves\n\n        Returns:\n            a batch of label sequences\n        \"\"\"\n        X = self._transform_batch(data)\n        objects_number, lengths = len(X[0]), [len(elem) for elem in data[0]]\n        Y = self.model_.predict_on_batch(X)\n        labels = np.argmax(Y, axis=-1)\n        answer: List[List[str]] = [None] * objects_number\n        for i, (elem, length) in enumerate(zip(labels, lengths)):\n            elem = elem[:length]\n            answer[i] = elem if return_indexes else self.tags.idxs2toks(elem)\n        return answer", "language": "python", "code": "def predict_on_batch(self, data: Union[list, tuple],\n                         return_indexes: bool = False) -> List[List[str]]:\n        \"\"\"\n        Makes predictions on a single batch\n\n        Args:\n            data: a batch of word sequences together with additional inputs\n            return_indexes: whether to return tag indexes in vocabulary or tags themselves\n\n        Returns:\n            a batch of label sequences\n        \"\"\"\n        X = self._transform_batch(data)\n        objects_number, lengths = len(X[0]), [len(elem) for elem in data[0]]\n        Y = self.model_.predict_on_batch(X)\n        labels = np.argmax(Y, axis=-1)\n        answer: List[List[str]] = [None] * objects_number\n        for i, (elem, length) in enumerate(zip(labels, lengths)):\n            elem = elem[:length]\n            answer[i] = elem if return_indexes else self.tags.idxs2toks(elem)\n        return answer", "code_tokens": ["def", "predict_on_batch", "(", "self", ",", "data", ":", "Union", "[", "list", ",", "tuple", "]", ",", "return_indexes", ":", "bool", "=", "False", ")", "->", "List", "[", "List", "[", "str", "]", "]", ":", "X", "=", "self", ".", "_transform_batch", "(", "data", ")", "objects_number", ",", "lengths", "=", "len", "(", "X", "[", "0", "]", ")", ",", "[", "len", "(", "elem", ")", "for", "elem", "in", "data", "[", "0", "]", "]", "Y", "=", "self", ".", "model_", ".", "predict_on_batch", "(", "X", ")", "labels", "=", "np", ".", "argmax", "(", "Y", ",", "axis", "=", "-", "1", ")", "answer", ":", "List", "[", "List", "[", "str", "]", "]", "=", "[", "None", "]", "*", "objects_number", "for", "i", ",", "(", "elem", ",", "length", ")", "in", "enumerate", "(", "zip", "(", "labels", ",", "lengths", ")", ")", ":", "elem", "=", "elem", "[", ":", "length", "]", "answer", "[", "i", "]", "=", "elem", "if", "return_indexes", "else", "self", ".", "tags", ".", "idxs2toks", "(", "elem", ")", "return", "answer"], "docstring": "Makes predictions on a single batch\n\n        Args:\n            data: a batch of word sequences together with additional inputs\n            return_indexes: whether to return tag indexes in vocabulary or tags themselves\n\n        Returns:\n            a batch of label sequences", "docstring_tokens": ["Makes", "predictions", "on", "a", "single", "batch"], "sha": "f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c", "url": "https://github.com/deepmipt/DeepPavlov/blob/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c/deeppavlov/models/morpho_tagger/network.py#L246-L266", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/setuptools/depends.py", "func_name": "extract_constant", "original_string": "def extract_constant(code, symbol, default=-1):\n    \"\"\"Extract the constant value of 'symbol' from 'code'\n\n    If the name 'symbol' is bound to a constant value by the Python code\n    object 'code', return that value.  If 'symbol' is bound to an expression,\n    return 'default'.  Otherwise, return 'None'.\n\n    Return value is based on the first assignment to 'symbol'.  'symbol' must\n    be a global, or at least a non-\"fast\" local in the code block.  That is,\n    only 'STORE_NAME' and 'STORE_GLOBAL' opcodes are checked, and 'symbol'\n    must be present in 'code.co_names'.\n    \"\"\"\n\n    if symbol not in code.co_names:\n        # name's not there, can't possibly be an assigment\n        return None\n\n    name_idx = list(code.co_names).index(symbol)\n\n    STORE_NAME = 90\n    STORE_GLOBAL = 97\n    LOAD_CONST = 100\n\n    const = default\n\n    for op, arg in _iter_code(code):\n\n        if op==LOAD_CONST:\n            const = code.co_consts[arg]\n        elif arg==name_idx and (op==STORE_NAME or op==STORE_GLOBAL):\n            return const\n        else:\n            const = default", "language": "python", "code": "def extract_constant(code, symbol, default=-1):\n    \"\"\"Extract the constant value of 'symbol' from 'code'\n\n    If the name 'symbol' is bound to a constant value by the Python code\n    object 'code', return that value.  If 'symbol' is bound to an expression,\n    return 'default'.  Otherwise, return 'None'.\n\n    Return value is based on the first assignment to 'symbol'.  'symbol' must\n    be a global, or at least a non-\"fast\" local in the code block.  That is,\n    only 'STORE_NAME' and 'STORE_GLOBAL' opcodes are checked, and 'symbol'\n    must be present in 'code.co_names'.\n    \"\"\"\n\n    if symbol not in code.co_names:\n        # name's not there, can't possibly be an assigment\n        return None\n\n    name_idx = list(code.co_names).index(symbol)\n\n    STORE_NAME = 90\n    STORE_GLOBAL = 97\n    LOAD_CONST = 100\n\n    const = default\n\n    for op, arg in _iter_code(code):\n\n        if op==LOAD_CONST:\n            const = code.co_consts[arg]\n        elif arg==name_idx and (op==STORE_NAME or op==STORE_GLOBAL):\n            return const\n        else:\n            const = default", "code_tokens": ["def", "extract_constant", "(", "code", ",", "symbol", ",", "default", "=", "-", "1", ")", ":", "if", "symbol", "not", "in", "code", ".", "co_names", ":", "# name's not there, can't possibly be an assigment", "return", "None", "name_idx", "=", "list", "(", "code", ".", "co_names", ")", ".", "index", "(", "symbol", ")", "STORE_NAME", "=", "90", "STORE_GLOBAL", "=", "97", "LOAD_CONST", "=", "100", "const", "=", "default", "for", "op", ",", "arg", "in", "_iter_code", "(", "code", ")", ":", "if", "op", "==", "LOAD_CONST", ":", "const", "=", "code", ".", "co_consts", "[", "arg", "]", "elif", "arg", "==", "name_idx", "and", "(", "op", "==", "STORE_NAME", "or", "op", "==", "STORE_GLOBAL", ")", ":", "return", "const", "else", ":", "const", "=", "default"], "docstring": "Extract the constant value of 'symbol' from 'code'\n\n    If the name 'symbol' is bound to a constant value by the Python code\n    object 'code', return that value.  If 'symbol' is bound to an expression,\n    return 'default'.  Otherwise, return 'None'.\n\n    Return value is based on the first assignment to 'symbol'.  'symbol' must\n    be a global, or at least a non-\"fast\" local in the code block.  That is,\n    only 'STORE_NAME' and 'STORE_GLOBAL' opcodes are checked, and 'symbol'\n    must be present in 'code.co_names'.", "docstring_tokens": ["Extract", "the", "constant", "value", "of", "symbol", "from", "code"], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/setuptools/depends.py#L166-L198", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/redshift_hook.py", "func_name": "RedshiftHook.describe_cluster_snapshots", "original_string": "def describe_cluster_snapshots(self, cluster_identifier):\n        \"\"\"\n        Gets a list of snapshots for a cluster\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        \"\"\"\n        response = self.get_conn().describe_cluster_snapshots(\n            ClusterIdentifier=cluster_identifier\n        )\n        if 'Snapshots' not in response:\n            return None\n        snapshots = response['Snapshots']\n        snapshots = filter(lambda x: x['Status'], snapshots)\n        snapshots.sort(key=lambda x: x['SnapshotCreateTime'], reverse=True)\n        return snapshots", "language": "python", "code": "def describe_cluster_snapshots(self, cluster_identifier):\n        \"\"\"\n        Gets a list of snapshots for a cluster\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        \"\"\"\n        response = self.get_conn().describe_cluster_snapshots(\n            ClusterIdentifier=cluster_identifier\n        )\n        if 'Snapshots' not in response:\n            return None\n        snapshots = response['Snapshots']\n        snapshots = filter(lambda x: x['Status'], snapshots)\n        snapshots.sort(key=lambda x: x['SnapshotCreateTime'], reverse=True)\n        return snapshots", "code_tokens": ["def", "describe_cluster_snapshots", "(", "self", ",", "cluster_identifier", ")", ":", "response", "=", "self", ".", "get_conn", "(", ")", ".", "describe_cluster_snapshots", "(", "ClusterIdentifier", "=", "cluster_identifier", ")", "if", "'Snapshots'", "not", "in", "response", ":", "return", "None", "snapshots", "=", "response", "[", "'Snapshots'", "]", "snapshots", "=", "filter", "(", "lambda", "x", ":", "x", "[", "'Status'", "]", ",", "snapshots", ")", "snapshots", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "'SnapshotCreateTime'", "]", ",", "reverse", "=", "True", ")", "return", "snapshots"], "docstring": "Gets a list of snapshots for a cluster\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str", "docstring_tokens": ["Gets", "a", "list", "of", "snapshots", "for", "a", "cluster"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/redshift_hook.py#L68-L83", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/vi/csiszar_divergence.py", "func_name": "jensen_shannon", "original_string": "def jensen_shannon(logu, self_normalized=False, name=None):\n  \"\"\"The Jensen-Shannon Csiszar-function in log-space.\n\n  A Csiszar-function is a member of,\n\n  ```none\n  F = { f:R_+ to R : f convex }.\n  ```\n\n  When `self_normalized = True`, the Jensen-Shannon Csiszar-function is:\n\n  ```none\n  f(u) = u log(u) - (1 + u) log(1 + u) + (u + 1) log(2)\n  ```\n\n  When `self_normalized = False` the `(u + 1) log(2)` term is omitted.\n\n  Observe that as an f-Divergence, this Csiszar-function implies:\n\n  ```none\n  D_f[p, q] = KL[p, m] + KL[q, m]\n  m(x) = 0.5 p(x) + 0.5 q(x)\n  ```\n\n  In a sense, this divergence is the \"reverse\" of the Arithmetic-Geometric\n  f-Divergence.\n\n  This Csiszar-function induces a symmetric f-Divergence, i.e.,\n  `D_f[p, q] = D_f[q, p]`.\n\n  Warning: this function makes non-log-space calculations and may therefore be\n  numerically unstable for `|logu| >> 0`.\n\n  For more information, see:\n    Lin, J. \"Divergence measures based on the Shannon entropy.\" IEEE Trans.\n    Inf. Th., 37, 145-151, 1991.\n\n  Args:\n    logu: `float`-like `Tensor` representing `log(u)` from above.\n    self_normalized: Python `bool` indicating whether `f'(u=1)=0`. When\n      `f'(u=1)=0` the implied Csiszar f-Divergence remains non-negative even\n      when `p, q` are unnormalized measures.\n    name: Python `str` name prefixed to Ops created by this function.\n\n  Returns:\n    jensen_shannon_of_u: `float`-like `Tensor` of the Csiszar-function\n      evaluated at `u = exp(logu)`.\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, \"jensen_shannon\", [logu]):\n    logu = tf.convert_to_tensor(value=logu, name=\"logu\")\n    npdt = logu.dtype.as_numpy_dtype\n    y = tf.nn.softplus(logu)\n    if self_normalized:\n      y -= np.log(2).astype(npdt)\n    return tf.exp(logu) * logu - (1. + tf.exp(logu)) * y", "language": "python", "code": "def jensen_shannon(logu, self_normalized=False, name=None):\n  \"\"\"The Jensen-Shannon Csiszar-function in log-space.\n\n  A Csiszar-function is a member of,\n\n  ```none\n  F = { f:R_+ to R : f convex }.\n  ```\n\n  When `self_normalized = True`, the Jensen-Shannon Csiszar-function is:\n\n  ```none\n  f(u) = u log(u) - (1 + u) log(1 + u) + (u + 1) log(2)\n  ```\n\n  When `self_normalized = False` the `(u + 1) log(2)` term is omitted.\n\n  Observe that as an f-Divergence, this Csiszar-function implies:\n\n  ```none\n  D_f[p, q] = KL[p, m] + KL[q, m]\n  m(x) = 0.5 p(x) + 0.5 q(x)\n  ```\n\n  In a sense, this divergence is the \"reverse\" of the Arithmetic-Geometric\n  f-Divergence.\n\n  This Csiszar-function induces a symmetric f-Divergence, i.e.,\n  `D_f[p, q] = D_f[q, p]`.\n\n  Warning: this function makes non-log-space calculations and may therefore be\n  numerically unstable for `|logu| >> 0`.\n\n  For more information, see:\n    Lin, J. \"Divergence measures based on the Shannon entropy.\" IEEE Trans.\n    Inf. Th., 37, 145-151, 1991.\n\n  Args:\n    logu: `float`-like `Tensor` representing `log(u)` from above.\n    self_normalized: Python `bool` indicating whether `f'(u=1)=0`. When\n      `f'(u=1)=0` the implied Csiszar f-Divergence remains non-negative even\n      when `p, q` are unnormalized measures.\n    name: Python `str` name prefixed to Ops created by this function.\n\n  Returns:\n    jensen_shannon_of_u: `float`-like `Tensor` of the Csiszar-function\n      evaluated at `u = exp(logu)`.\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, \"jensen_shannon\", [logu]):\n    logu = tf.convert_to_tensor(value=logu, name=\"logu\")\n    npdt = logu.dtype.as_numpy_dtype\n    y = tf.nn.softplus(logu)\n    if self_normalized:\n      y -= np.log(2).astype(npdt)\n    return tf.exp(logu) * logu - (1. + tf.exp(logu)) * y", "code_tokens": ["def", "jensen_shannon", "(", "logu", ",", "self_normalized", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "\"jensen_shannon\"", ",", "[", "logu", "]", ")", ":", "logu", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "logu", ",", "name", "=", "\"logu\"", ")", "npdt", "=", "logu", ".", "dtype", ".", "as_numpy_dtype", "y", "=", "tf", ".", "nn", ".", "softplus", "(", "logu", ")", "if", "self_normalized", ":", "y", "-=", "np", ".", "log", "(", "2", ")", ".", "astype", "(", "npdt", ")", "return", "tf", ".", "exp", "(", "logu", ")", "*", "logu", "-", "(", "1.", "+", "tf", ".", "exp", "(", "logu", ")", ")", "*", "y"], "docstring": "The Jensen-Shannon Csiszar-function in log-space.\n\n  A Csiszar-function is a member of,\n\n  ```none\n  F = { f:R_+ to R : f convex }.\n  ```\n\n  When `self_normalized = True`, the Jensen-Shannon Csiszar-function is:\n\n  ```none\n  f(u) = u log(u) - (1 + u) log(1 + u) + (u + 1) log(2)\n  ```\n\n  When `self_normalized = False` the `(u + 1) log(2)` term is omitted.\n\n  Observe that as an f-Divergence, this Csiszar-function implies:\n\n  ```none\n  D_f[p, q] = KL[p, m] + KL[q, m]\n  m(x) = 0.5 p(x) + 0.5 q(x)\n  ```\n\n  In a sense, this divergence is the \"reverse\" of the Arithmetic-Geometric\n  f-Divergence.\n\n  This Csiszar-function induces a symmetric f-Divergence, i.e.,\n  `D_f[p, q] = D_f[q, p]`.\n\n  Warning: this function makes non-log-space calculations and may therefore be\n  numerically unstable for `|logu| >> 0`.\n\n  For more information, see:\n    Lin, J. \"Divergence measures based on the Shannon entropy.\" IEEE Trans.\n    Inf. Th., 37, 145-151, 1991.\n\n  Args:\n    logu: `float`-like `Tensor` representing `log(u)` from above.\n    self_normalized: Python `bool` indicating whether `f'(u=1)=0`. When\n      `f'(u=1)=0` the implied Csiszar f-Divergence remains non-negative even\n      when `p, q` are unnormalized measures.\n    name: Python `str` name prefixed to Ops created by this function.\n\n  Returns:\n    jensen_shannon_of_u: `float`-like `Tensor` of the Csiszar-function\n      evaluated at `u = exp(logu)`.", "docstring_tokens": ["The", "Jensen", "-", "Shannon", "Csiszar", "-", "function", "in", "log", "-", "space", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/vi/csiszar_divergence.py#L217-L272", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/reporters/json_reporter.py", "func_name": "JSONReporter.handle_message", "original_string": "def handle_message(self, msg):\n        \"\"\"Manage message of different type and in the context of path.\"\"\"\n        self.messages.append(\n            {\n                \"type\": msg.category,\n                \"module\": msg.module,\n                \"obj\": msg.obj,\n                \"line\": msg.line,\n                \"column\": msg.column,\n                \"path\": msg.path,\n                \"symbol\": msg.symbol,\n                \"message\": html.escape(msg.msg or \"\", quote=False),\n                \"message-id\": msg.msg_id,\n            }\n        )", "language": "python", "code": "def handle_message(self, msg):\n        \"\"\"Manage message of different type and in the context of path.\"\"\"\n        self.messages.append(\n            {\n                \"type\": msg.category,\n                \"module\": msg.module,\n                \"obj\": msg.obj,\n                \"line\": msg.line,\n                \"column\": msg.column,\n                \"path\": msg.path,\n                \"symbol\": msg.symbol,\n                \"message\": html.escape(msg.msg or \"\", quote=False),\n                \"message-id\": msg.msg_id,\n            }\n        )", "code_tokens": ["def", "handle_message", "(", "self", ",", "msg", ")", ":", "self", ".", "messages", ".", "append", "(", "{", "\"type\"", ":", "msg", ".", "category", ",", "\"module\"", ":", "msg", ".", "module", ",", "\"obj\"", ":", "msg", ".", "obj", ",", "\"line\"", ":", "msg", ".", "line", ",", "\"column\"", ":", "msg", ".", "column", ",", "\"path\"", ":", "msg", ".", "path", ",", "\"symbol\"", ":", "msg", ".", "symbol", ",", "\"message\"", ":", "html", ".", "escape", "(", "msg", ".", "msg", "or", "\"\"", ",", "quote", "=", "False", ")", ",", "\"message-id\"", ":", "msg", ".", "msg_id", ",", "}", ")"], "docstring": "Manage message of different type and in the context of path.", "docstring_tokens": ["Manage", "message", "of", "different", "type", "and", "in", "the", "context", "of", "path", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/reporters/json_reporter.py#L31-L45", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/ultratb.py", "func_name": "TBTools.text", "original_string": "def text(self, etype, value, tb, tb_offset=None, context=5):\n        \"\"\"Return formatted traceback.\n\n        Subclasses may override this if they add extra arguments.\n        \"\"\"\n        tb_list = self.structured_traceback(etype, value, tb,\n                                            tb_offset, context)\n        return self.stb2text(tb_list)", "language": "python", "code": "def text(self, etype, value, tb, tb_offset=None, context=5):\n        \"\"\"Return formatted traceback.\n\n        Subclasses may override this if they add extra arguments.\n        \"\"\"\n        tb_list = self.structured_traceback(etype, value, tb,\n                                            tb_offset, context)\n        return self.stb2text(tb_list)", "code_tokens": ["def", "text", "(", "self", ",", "etype", ",", "value", ",", "tb", ",", "tb_offset", "=", "None", ",", "context", "=", "5", ")", ":", "tb_list", "=", "self", ".", "structured_traceback", "(", "etype", ",", "value", ",", "tb", ",", "tb_offset", ",", "context", ")", "return", "self", ".", "stb2text", "(", "tb_list", ")"], "docstring": "Return formatted traceback.\n\n        Subclasses may override this if they add extra arguments.", "docstring_tokens": ["Return", "formatted", "traceback", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/ultratb.py#L406-L413", "partition": "test"}
{"repo": "its-rigs/Trolly", "path": "trolly/checklist.py", "func_name": "ChecklistItem.update_state", "original_string": "def update_state(self, state):\n        \"\"\"\n        Set the state of the current checklist item. Returns a new ChecklistItem object.\n        \"\"\"\n        checklistitem_json = self.fetch_json(\n            uri_path = self.base_uri + '/state',\n            http_method = 'PUT',\n            query_params = {'value': 'complete' if state else 'incomplete'}\n        )\n\n        return self.create_checklist_item(self.idCard, self.idChecklist, checklistitem_json)", "language": "python", "code": "def update_state(self, state):\n        \"\"\"\n        Set the state of the current checklist item. Returns a new ChecklistItem object.\n        \"\"\"\n        checklistitem_json = self.fetch_json(\n            uri_path = self.base_uri + '/state',\n            http_method = 'PUT',\n            query_params = {'value': 'complete' if state else 'incomplete'}\n        )\n\n        return self.create_checklist_item(self.idCard, self.idChecklist, checklistitem_json)", "code_tokens": ["def", "update_state", "(", "self", ",", "state", ")", ":", "checklistitem_json", "=", "self", ".", "fetch_json", "(", "uri_path", "=", "self", ".", "base_uri", "+", "'/state'", ",", "http_method", "=", "'PUT'", ",", "query_params", "=", "{", "'value'", ":", "'complete'", "if", "state", "else", "'incomplete'", "}", ")", "return", "self", ".", "create_checklist_item", "(", "self", ".", "idCard", ",", "self", ".", "idChecklist", ",", "checklistitem_json", ")"], "docstring": "Set the state of the current checklist item. Returns a new ChecklistItem object.", "docstring_tokens": ["Set", "the", "state", "of", "the", "current", "checklist", "item", ".", "Returns", "a", "new", "ChecklistItem", "object", "."], "sha": "483dc94c352df40dc05ead31820b059b2545cf82", "url": "https://github.com/its-rigs/Trolly/blob/483dc94c352df40dc05ead31820b059b2545cf82/trolly/checklist.py#L121-L131", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/zmq/kernelmanager.py", "func_name": "KernelManager.shutdown_kernel", "original_string": "def shutdown_kernel(self, restart=False):\n        \"\"\" Attempts to the stop the kernel process cleanly. If the kernel\n        cannot be stopped, it is killed, if possible.\n        \"\"\"\n        # FIXME: Shutdown does not work on Windows due to ZMQ errors!\n        if sys.platform == 'win32':\n            self.kill_kernel()\n            return\n\n        # Pause the heart beat channel if it exists.\n        if self._hb_channel is not None:\n            self._hb_channel.pause()\n\n        # Don't send any additional kernel kill messages immediately, to give\n        # the kernel a chance to properly execute shutdown actions. Wait for at\n        # most 1s, checking every 0.1s.\n        self.shell_channel.shutdown(restart=restart)\n        for i in range(10):\n            if self.is_alive:\n                time.sleep(0.1)\n            else:\n                break\n        else:\n            # OK, we've waited long enough.\n            if self.has_kernel:\n                self.kill_kernel()\n\n        if not restart and self._connection_file_written:\n            # cleanup connection files on full shutdown of kernel we started\n            self._connection_file_written = False\n            try:\n                os.remove(self.connection_file)\n            except IOError:\n                pass", "language": "python", "code": "def shutdown_kernel(self, restart=False):\n        \"\"\" Attempts to the stop the kernel process cleanly. If the kernel\n        cannot be stopped, it is killed, if possible.\n        \"\"\"\n        # FIXME: Shutdown does not work on Windows due to ZMQ errors!\n        if sys.platform == 'win32':\n            self.kill_kernel()\n            return\n\n        # Pause the heart beat channel if it exists.\n        if self._hb_channel is not None:\n            self._hb_channel.pause()\n\n        # Don't send any additional kernel kill messages immediately, to give\n        # the kernel a chance to properly execute shutdown actions. Wait for at\n        # most 1s, checking every 0.1s.\n        self.shell_channel.shutdown(restart=restart)\n        for i in range(10):\n            if self.is_alive:\n                time.sleep(0.1)\n            else:\n                break\n        else:\n            # OK, we've waited long enough.\n            if self.has_kernel:\n                self.kill_kernel()\n\n        if not restart and self._connection_file_written:\n            # cleanup connection files on full shutdown of kernel we started\n            self._connection_file_written = False\n            try:\n                os.remove(self.connection_file)\n            except IOError:\n                pass", "code_tokens": ["def", "shutdown_kernel", "(", "self", ",", "restart", "=", "False", ")", ":", "# FIXME: Shutdown does not work on Windows due to ZMQ errors!", "if", "sys", ".", "platform", "==", "'win32'", ":", "self", ".", "kill_kernel", "(", ")", "return", "# Pause the heart beat channel if it exists.", "if", "self", ".", "_hb_channel", "is", "not", "None", ":", "self", ".", "_hb_channel", ".", "pause", "(", ")", "# Don't send any additional kernel kill messages immediately, to give", "# the kernel a chance to properly execute shutdown actions. Wait for at", "# most 1s, checking every 0.1s.", "self", ".", "shell_channel", ".", "shutdown", "(", "restart", "=", "restart", ")", "for", "i", "in", "range", "(", "10", ")", ":", "if", "self", ".", "is_alive", ":", "time", ".", "sleep", "(", "0.1", ")", "else", ":", "break", "else", ":", "# OK, we've waited long enough.", "if", "self", ".", "has_kernel", ":", "self", ".", "kill_kernel", "(", ")", "if", "not", "restart", "and", "self", ".", "_connection_file_written", ":", "# cleanup connection files on full shutdown of kernel we started", "self", ".", "_connection_file_written", "=", "False", "try", ":", "os", ".", "remove", "(", "self", ".", "connection_file", ")", "except", "IOError", ":", "pass"], "docstring": "Attempts to the stop the kernel process cleanly. If the kernel\n        cannot be stopped, it is killed, if possible.", "docstring_tokens": ["Attempts", "to", "the", "stop", "the", "kernel", "process", "cleanly", ".", "If", "the", "kernel", "cannot", "be", "stopped", "it", "is", "killed", "if", "possible", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/zmq/kernelmanager.py#L808-L841", "partition": "test"}
{"repo": "UDST/pandana", "path": "pandana/network.py", "func_name": "Network.aggregate", "original_string": "def aggregate(self, distance, type=\"sum\", decay=\"linear\", imp_name=None,\n                  name=\"tmp\"):\n        \"\"\"\n        Aggregate information for every source node in the network - this is\n        really the main purpose of this library.  This allows you to touch\n        the data specified by calling set and perform some aggregation on it\n        within the specified distance.  For instance, summing the population\n        within 1000 meters.\n\n        Parameters\n        ----------\n        distance : float\n            The maximum distance to aggregate data within. 'distance' can\n            represent any impedance unit that you have set as your edge\n            weight. This will usually be a distance unit in meters however\n            if you have customized the impedance this could be in other\n            units such as utility or time etc.\n        type : string\n            The type of aggregation, can be one of \"ave\", \"sum\", \"std\",\n            \"count\", and now \"min\", \"25pct\", \"median\", \"75pct\", and \"max\" will\n            compute the associated quantiles.  (Quantiles are computed by\n            sorting so might be slower than the others.)\n        decay : string\n            The type of decay to apply, which makes things that are further\n            away count less in the aggregation - must be one of \"linear\",\n            \"exponential\" or \"flat\" (which means no decay).  Linear is the\n            fastest computation to perform.  When performing an \"ave\",\n            the decay is typically \"flat\"\n        imp_name : string, optional\n            The impedance name to use for the aggregation on this network.\n            Must be one of the impedance names passed in the constructor of\n            this object.  If not specified, there must be only one impedance\n            passed in the constructor, which will be used.\n        name : string, optional\n            The variable to aggregate.  This variable will have been created\n            and named by a call to set.  If not specified, the default\n            variable name will be used so that the most recent call to set\n            without giving a name will be the variable used.\n\n        Returns\n        -------\n        agg : Pandas Series\n            Returns a Pandas Series for every origin node in the network,\n            with the index which is the same as the node_ids passed to the\n            init method and the values are the aggregations for each source\n            node in the network.\n        \"\"\"\n\n        imp_num = self._imp_name_to_num(imp_name)\n        type = type.lower()\n        if type == \"ave\":\n            type = \"mean\"  # changed generic ave to mean\n\n        assert name in self.variable_names, \"A variable with that name \" \\\n                                            \"has not yet been initialized\"\n\n        res = self.net.get_all_aggregate_accessibility_variables(distance,\n                                                                 name.encode('utf-8'),\n                                                                 type.encode('utf-8'),\n                                                                 decay.encode('utf-8'),\n                                                                 imp_num)\n\n        return pd.Series(res, index=self.node_ids)", "language": "python", "code": "def aggregate(self, distance, type=\"sum\", decay=\"linear\", imp_name=None,\n                  name=\"tmp\"):\n        \"\"\"\n        Aggregate information for every source node in the network - this is\n        really the main purpose of this library.  This allows you to touch\n        the data specified by calling set and perform some aggregation on it\n        within the specified distance.  For instance, summing the population\n        within 1000 meters.\n\n        Parameters\n        ----------\n        distance : float\n            The maximum distance to aggregate data within. 'distance' can\n            represent any impedance unit that you have set as your edge\n            weight. This will usually be a distance unit in meters however\n            if you have customized the impedance this could be in other\n            units such as utility or time etc.\n        type : string\n            The type of aggregation, can be one of \"ave\", \"sum\", \"std\",\n            \"count\", and now \"min\", \"25pct\", \"median\", \"75pct\", and \"max\" will\n            compute the associated quantiles.  (Quantiles are computed by\n            sorting so might be slower than the others.)\n        decay : string\n            The type of decay to apply, which makes things that are further\n            away count less in the aggregation - must be one of \"linear\",\n            \"exponential\" or \"flat\" (which means no decay).  Linear is the\n            fastest computation to perform.  When performing an \"ave\",\n            the decay is typically \"flat\"\n        imp_name : string, optional\n            The impedance name to use for the aggregation on this network.\n            Must be one of the impedance names passed in the constructor of\n            this object.  If not specified, there must be only one impedance\n            passed in the constructor, which will be used.\n        name : string, optional\n            The variable to aggregate.  This variable will have been created\n            and named by a call to set.  If not specified, the default\n            variable name will be used so that the most recent call to set\n            without giving a name will be the variable used.\n\n        Returns\n        -------\n        agg : Pandas Series\n            Returns a Pandas Series for every origin node in the network,\n            with the index which is the same as the node_ids passed to the\n            init method and the values are the aggregations for each source\n            node in the network.\n        \"\"\"\n\n        imp_num = self._imp_name_to_num(imp_name)\n        type = type.lower()\n        if type == \"ave\":\n            type = \"mean\"  # changed generic ave to mean\n\n        assert name in self.variable_names, \"A variable with that name \" \\\n                                            \"has not yet been initialized\"\n\n        res = self.net.get_all_aggregate_accessibility_variables(distance,\n                                                                 name.encode('utf-8'),\n                                                                 type.encode('utf-8'),\n                                                                 decay.encode('utf-8'),\n                                                                 imp_num)\n\n        return pd.Series(res, index=self.node_ids)", "code_tokens": ["def", "aggregate", "(", "self", ",", "distance", ",", "type", "=", "\"sum\"", ",", "decay", "=", "\"linear\"", ",", "imp_name", "=", "None", ",", "name", "=", "\"tmp\"", ")", ":", "imp_num", "=", "self", ".", "_imp_name_to_num", "(", "imp_name", ")", "type", "=", "type", ".", "lower", "(", ")", "if", "type", "==", "\"ave\"", ":", "type", "=", "\"mean\"", "# changed generic ave to mean", "assert", "name", "in", "self", ".", "variable_names", ",", "\"A variable with that name \"", "\"has not yet been initialized\"", "res", "=", "self", ".", "net", ".", "get_all_aggregate_accessibility_variables", "(", "distance", ",", "name", ".", "encode", "(", "'utf-8'", ")", ",", "type", ".", "encode", "(", "'utf-8'", ")", ",", "decay", ".", "encode", "(", "'utf-8'", ")", ",", "imp_num", ")", "return", "pd", ".", "Series", "(", "res", ",", "index", "=", "self", ".", "node_ids", ")"], "docstring": "Aggregate information for every source node in the network - this is\n        really the main purpose of this library.  This allows you to touch\n        the data specified by calling set and perform some aggregation on it\n        within the specified distance.  For instance, summing the population\n        within 1000 meters.\n\n        Parameters\n        ----------\n        distance : float\n            The maximum distance to aggregate data within. 'distance' can\n            represent any impedance unit that you have set as your edge\n            weight. This will usually be a distance unit in meters however\n            if you have customized the impedance this could be in other\n            units such as utility or time etc.\n        type : string\n            The type of aggregation, can be one of \"ave\", \"sum\", \"std\",\n            \"count\", and now \"min\", \"25pct\", \"median\", \"75pct\", and \"max\" will\n            compute the associated quantiles.  (Quantiles are computed by\n            sorting so might be slower than the others.)\n        decay : string\n            The type of decay to apply, which makes things that are further\n            away count less in the aggregation - must be one of \"linear\",\n            \"exponential\" or \"flat\" (which means no decay).  Linear is the\n            fastest computation to perform.  When performing an \"ave\",\n            the decay is typically \"flat\"\n        imp_name : string, optional\n            The impedance name to use for the aggregation on this network.\n            Must be one of the impedance names passed in the constructor of\n            this object.  If not specified, there must be only one impedance\n            passed in the constructor, which will be used.\n        name : string, optional\n            The variable to aggregate.  This variable will have been created\n            and named by a call to set.  If not specified, the default\n            variable name will be used so that the most recent call to set\n            without giving a name will be the variable used.\n\n        Returns\n        -------\n        agg : Pandas Series\n            Returns a Pandas Series for every origin node in the network,\n            with the index which is the same as the node_ids passed to the\n            init method and the values are the aggregations for each source\n            node in the network.", "docstring_tokens": ["Aggregate", "information", "for", "every", "source", "node", "in", "the", "network", "-", "this", "is", "really", "the", "main", "purpose", "of", "this", "library", ".", "This", "allows", "you", "to", "touch", "the", "data", "specified", "by", "calling", "set", "and", "perform", "some", "aggregation", "on", "it", "within", "the", "specified", "distance", ".", "For", "instance", "summing", "the", "population", "within", "1000", "meters", "."], "sha": "961a7ef8d3b0144b190cb60bbd61845fca6fb314", "url": "https://github.com/UDST/pandana/blob/961a7ef8d3b0144b190cb60bbd61845fca6fb314/pandana/network.py#L274-L336", "partition": "test"}
{"repo": "yandex/yandex-tank", "path": "yandextank/plugins/ShellExec/plugin.py", "func_name": "Plugin.execute", "original_string": "def execute(self, cmd):\n        \"\"\"\n        Execute and check exit code\n        \"\"\"\n        self.log.info(\"Executing: %s\", cmd)\n        retcode = execute(\n            cmd, shell=True, poll_period=0.1, catch_out=self.catch_out)[0]\n        if retcode:\n            raise RuntimeError(\"Subprocess returned %s\" % retcode)\n        return retcode", "language": "python", "code": "def execute(self, cmd):\n        \"\"\"\n        Execute and check exit code\n        \"\"\"\n        self.log.info(\"Executing: %s\", cmd)\n        retcode = execute(\n            cmd, shell=True, poll_period=0.1, catch_out=self.catch_out)[0]\n        if retcode:\n            raise RuntimeError(\"Subprocess returned %s\" % retcode)\n        return retcode", "code_tokens": ["def", "execute", "(", "self", ",", "cmd", ")", ":", "self", ".", "log", ".", "info", "(", "\"Executing: %s\"", ",", "cmd", ")", "retcode", "=", "execute", "(", "cmd", ",", "shell", "=", "True", ",", "poll_period", "=", "0.1", ",", "catch_out", "=", "self", ".", "catch_out", ")", "[", "0", "]", "if", "retcode", ":", "raise", "RuntimeError", "(", "\"Subprocess returned %s\"", "%", "retcode", ")", "return", "retcode"], "docstring": "Execute and check exit code", "docstring_tokens": ["Execute", "and", "check", "exit", "code"], "sha": "d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b", "url": "https://github.com/yandex/yandex-tank/blob/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b/yandextank/plugins/ShellExec/plugin.py#L71-L80", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/glm/fisher_scoring.py", "func_name": "fit", "original_string": "def fit(\n    model_matrix,\n    response,\n    model,\n    model_coefficients_start=None,\n    predicted_linear_response_start=None,\n    l2_regularizer=None,\n    dispersion=None,\n    offset=None,\n    convergence_criteria_fn=None,\n    learning_rate=None,\n    fast_unsafe_numerics=True,\n    maximum_iterations=None,\n    name=None):\n  \"\"\"Runs multiple Fisher scoring steps.\n\n  Args:\n    model_matrix: (Batch of) `float`-like, matrix-shaped `Tensor` where each row\n      represents a sample's features.\n    response: (Batch of) vector-shaped `Tensor` where each element represents a\n      sample's observed response (to the corresponding row of features). Must\n      have same `dtype` as `model_matrix`.\n    model: `tfp.glm.ExponentialFamily`-like instance which implicitly\n      characterizes a negative log-likelihood loss by specifying the\n      distribuion's `mean`, `gradient_mean`, and `variance`.\n    model_coefficients_start: Optional (batch of) vector-shaped `Tensor`\n      representing the initial model coefficients, one for each column in\n      `model_matrix`. Must have same `dtype` as `model_matrix`.\n      Default value: Zeros.\n    predicted_linear_response_start: Optional `Tensor` with `shape`, `dtype`\n      matching `response`; represents `offset` shifted initial linear\n      predictions based on `model_coefficients_start`.\n      Default value: `offset` if `model_coefficients is None`, and\n      `tf.linalg.matvec(model_matrix, model_coefficients_start) + offset`\n      otherwise.\n    l2_regularizer: Optional scalar `Tensor` representing L2 regularization\n      penalty, i.e.,\n      `loss(w) = sum{-log p(y[i]|x[i],w) : i=1..n} + l2_regularizer ||w||_2^2`.\n      Default value: `None` (i.e., no L2 regularization).\n    dispersion: Optional (batch of) `Tensor` representing `response` dispersion,\n      i.e., as in, `p(y|theta) := exp((y theta - A(theta)) / dispersion)`.\n      Must broadcast with rows of `model_matrix`.\n      Default value: `None` (i.e., \"no dispersion\").\n    offset: Optional `Tensor` representing constant shift applied to\n      `predicted_linear_response`.  Must broadcast to `response`.\n      Default value: `None` (i.e., `tf.zeros_like(response)`).\n    convergence_criteria_fn: Python `callable` taking:\n      `is_converged_previous`, `iter_`, `model_coefficients_previous`,\n      `predicted_linear_response_previous`, `model_coefficients_next`,\n      `predicted_linear_response_next`, `response`, `model`, `dispersion` and\n      returning a `bool` `Tensor` indicating that Fisher scoring has converged.\n      See `convergence_criteria_small_relative_norm_weights_change` as an\n      example function.\n      Default value: `None` (i.e.,\n      `convergence_criteria_small_relative_norm_weights_change`).\n    learning_rate: Optional (batch of) scalar `Tensor` used to dampen iterative\n      progress. Typically only needed if optimization diverges, should be no\n      larger than `1` and typically very close to `1`.\n      Default value: `None` (i.e., `1`).\n    fast_unsafe_numerics: Optional Python `bool` indicating if faster, less\n      numerically accurate methods can be employed for computing the weighted\n      least-squares solution.\n      Default value: `True` (i.e., \"fast but possibly diminished accuracy\").\n    maximum_iterations: Optional maximum number of iterations of Fisher scoring\n      to run; \"and-ed\" with result of `convergence_criteria_fn`.\n      Default value: `None` (i.e., `infinity`).\n    name: Python `str` used as name prefix to ops created by this function.\n      Default value: `\"fit\"`.\n\n  Returns:\n    model_coefficients: (Batch of) vector-shaped `Tensor`; represents the\n      fitted model coefficients, one for each column in `model_matrix`.\n    predicted_linear_response: `response`-shaped `Tensor` representing linear\n      predictions based on new `model_coefficients`, i.e.,\n      `tf.linalg.matvec(model_matrix, model_coefficients) + offset`.\n    is_converged: `bool` `Tensor` indicating that the returned\n      `model_coefficients` met the `convergence_criteria_fn` criteria within the\n      `maximum_iterations` limit.\n    iter_: `int32` `Tensor` indicating the number of iterations taken.\n\n  #### Example\n\n  ```python\n  from __future__ import print_function\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  def make_dataset(n, d, link, scale=1., dtype=np.float32):\n    model_coefficients = tfd.Uniform(\n        low=np.array(-1, dtype),\n        high=np.array(1, dtype)).sample(d, seed=42)\n    radius = np.sqrt(2.)\n    model_coefficients *= radius / tf.linalg.norm(model_coefficients)\n    model_matrix = tfd.Normal(\n        loc=np.array(0, dtype),\n        scale=np.array(1, dtype)).sample([n, d], seed=43)\n    scale = tf.convert_to_tensor(scale, dtype)\n    linear_response = tf.tensordot(\n        model_matrix, model_coefficients, axes=[[1], [0]])\n    if link == 'linear':\n      response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)\n    elif link == 'probit':\n      response = tf.cast(\n          tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,\n          dtype)\n    elif link == 'logit':\n      response = tfd.Bernoulli(logits=linear_response).sample(seed=44)\n    else:\n      raise ValueError('unrecognized true link: {}'.format(link))\n    return model_matrix, response, model_coefficients\n\n  X, Y, w_true = make_dataset(n=int(1e6), d=100, link='probit')\n\n  w, linear_response, is_converged, num_iter = tfp.glm.fit(\n      model_matrix=X,\n      response=Y,\n      model=tfp.glm.BernoulliNormalCDF())\n  log_likelihood = tfp.glm.BernoulliNormalCDF().log_prob(Y, linear_response)\n\n  with tf.Session() as sess:\n    [w_, linear_response_, is_converged_, num_iter_, Y_, w_true_,\n     log_likelihood_] = sess.run([\n        w, linear_response, is_converged, num_iter, Y, w_true,\n        log_likelihood])\n\n  print('is_converged: ', is_converged_)\n  print('    num_iter: ', num_iter_)\n  print('    accuracy: ', np.mean((linear_response_ > 0.) == Y_))\n  print('    deviance: ', 2. * np.mean(log_likelihood_))\n  print('||w0-w1||_2 / (1+||w0||_2): ', (np.linalg.norm(w_true_ - w_, ord=2) /\n                                         (1. + np.linalg.norm(w_true_, ord=2))))\n\n  # ==>\n  # is_converged:  True\n  #     num_iter:  6\n  #     accuracy:  0.804382\n  #     deviance:  -0.820746600628\n  # ||w0-w1||_2 / (1+||w0||_2):  0.00619245105309\n  ```\n\n  \"\"\"\n  graph_deps = [model_matrix, response, model_coefficients_start,\n                predicted_linear_response_start, dispersion, offset,\n                learning_rate, maximum_iterations]\n  with tf.compat.v1.name_scope(name, 'fit', graph_deps):\n    [\n        model_matrix,\n        response,\n        model_coefficients_start,\n        predicted_linear_response_start,\n        offset,\n    ] = prepare_args(\n        model_matrix,\n        response,\n        model_coefficients_start,\n        predicted_linear_response_start,\n        offset)\n    if convergence_criteria_fn is None:\n      convergence_criteria_fn = (\n          convergence_criteria_small_relative_norm_weights_change())\n\n    def _body(\n        is_converged_previous,\n        iter_,\n        model_coefficients_previous,\n        predicted_linear_response_previous):\n      \"\"\"`tf.while_loop` body.\"\"\"\n      model_coefficients_next, predicted_linear_response_next = fit_one_step(\n          model_matrix,\n          response,\n          model,\n          model_coefficients_previous,\n          predicted_linear_response_previous,\n          l2_regularizer,\n          dispersion,\n          offset,\n          learning_rate,\n          fast_unsafe_numerics)\n      is_converged_next = convergence_criteria_fn(\n          is_converged_previous=is_converged_previous,\n          iter_=iter_,\n          model_coefficients_previous=model_coefficients_previous,\n          predicted_linear_response_previous=predicted_linear_response_previous,\n          model_coefficients_next=model_coefficients_next,\n          predicted_linear_response_next=predicted_linear_response_next,\n          response=response,\n          model=model,\n          dispersion=dispersion)\n      return [\n          is_converged_next,\n          iter_ + 1,\n          model_coefficients_next,\n          predicted_linear_response_next,\n      ]\n\n    # while not converged:\n    #   fit_one_step\n    [\n        is_converged,\n        iter_,\n        model_coefficients,\n        predicted_linear_response,\n    ] = tf.while_loop(\n        cond=lambda is_converged, *args: tf.logical_not(is_converged),\n        body=_body,\n        loop_vars=[\n            tf.zeros([], np.bool),   # is_converged\n            tf.zeros([], np.int32),  # iter_\n            model_coefficients_start,\n            predicted_linear_response_start,\n        ],\n        maximum_iterations=maximum_iterations)\n\n    return [\n        model_coefficients,\n        predicted_linear_response,\n        is_converged,\n        iter_\n    ]", "language": "python", "code": "def fit(\n    model_matrix,\n    response,\n    model,\n    model_coefficients_start=None,\n    predicted_linear_response_start=None,\n    l2_regularizer=None,\n    dispersion=None,\n    offset=None,\n    convergence_criteria_fn=None,\n    learning_rate=None,\n    fast_unsafe_numerics=True,\n    maximum_iterations=None,\n    name=None):\n  \"\"\"Runs multiple Fisher scoring steps.\n\n  Args:\n    model_matrix: (Batch of) `float`-like, matrix-shaped `Tensor` where each row\n      represents a sample's features.\n    response: (Batch of) vector-shaped `Tensor` where each element represents a\n      sample's observed response (to the corresponding row of features). Must\n      have same `dtype` as `model_matrix`.\n    model: `tfp.glm.ExponentialFamily`-like instance which implicitly\n      characterizes a negative log-likelihood loss by specifying the\n      distribuion's `mean`, `gradient_mean`, and `variance`.\n    model_coefficients_start: Optional (batch of) vector-shaped `Tensor`\n      representing the initial model coefficients, one for each column in\n      `model_matrix`. Must have same `dtype` as `model_matrix`.\n      Default value: Zeros.\n    predicted_linear_response_start: Optional `Tensor` with `shape`, `dtype`\n      matching `response`; represents `offset` shifted initial linear\n      predictions based on `model_coefficients_start`.\n      Default value: `offset` if `model_coefficients is None`, and\n      `tf.linalg.matvec(model_matrix, model_coefficients_start) + offset`\n      otherwise.\n    l2_regularizer: Optional scalar `Tensor` representing L2 regularization\n      penalty, i.e.,\n      `loss(w) = sum{-log p(y[i]|x[i],w) : i=1..n} + l2_regularizer ||w||_2^2`.\n      Default value: `None` (i.e., no L2 regularization).\n    dispersion: Optional (batch of) `Tensor` representing `response` dispersion,\n      i.e., as in, `p(y|theta) := exp((y theta - A(theta)) / dispersion)`.\n      Must broadcast with rows of `model_matrix`.\n      Default value: `None` (i.e., \"no dispersion\").\n    offset: Optional `Tensor` representing constant shift applied to\n      `predicted_linear_response`.  Must broadcast to `response`.\n      Default value: `None` (i.e., `tf.zeros_like(response)`).\n    convergence_criteria_fn: Python `callable` taking:\n      `is_converged_previous`, `iter_`, `model_coefficients_previous`,\n      `predicted_linear_response_previous`, `model_coefficients_next`,\n      `predicted_linear_response_next`, `response`, `model`, `dispersion` and\n      returning a `bool` `Tensor` indicating that Fisher scoring has converged.\n      See `convergence_criteria_small_relative_norm_weights_change` as an\n      example function.\n      Default value: `None` (i.e.,\n      `convergence_criteria_small_relative_norm_weights_change`).\n    learning_rate: Optional (batch of) scalar `Tensor` used to dampen iterative\n      progress. Typically only needed if optimization diverges, should be no\n      larger than `1` and typically very close to `1`.\n      Default value: `None` (i.e., `1`).\n    fast_unsafe_numerics: Optional Python `bool` indicating if faster, less\n      numerically accurate methods can be employed for computing the weighted\n      least-squares solution.\n      Default value: `True` (i.e., \"fast but possibly diminished accuracy\").\n    maximum_iterations: Optional maximum number of iterations of Fisher scoring\n      to run; \"and-ed\" with result of `convergence_criteria_fn`.\n      Default value: `None` (i.e., `infinity`).\n    name: Python `str` used as name prefix to ops created by this function.\n      Default value: `\"fit\"`.\n\n  Returns:\n    model_coefficients: (Batch of) vector-shaped `Tensor`; represents the\n      fitted model coefficients, one for each column in `model_matrix`.\n    predicted_linear_response: `response`-shaped `Tensor` representing linear\n      predictions based on new `model_coefficients`, i.e.,\n      `tf.linalg.matvec(model_matrix, model_coefficients) + offset`.\n    is_converged: `bool` `Tensor` indicating that the returned\n      `model_coefficients` met the `convergence_criteria_fn` criteria within the\n      `maximum_iterations` limit.\n    iter_: `int32` `Tensor` indicating the number of iterations taken.\n\n  #### Example\n\n  ```python\n  from __future__ import print_function\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  def make_dataset(n, d, link, scale=1., dtype=np.float32):\n    model_coefficients = tfd.Uniform(\n        low=np.array(-1, dtype),\n        high=np.array(1, dtype)).sample(d, seed=42)\n    radius = np.sqrt(2.)\n    model_coefficients *= radius / tf.linalg.norm(model_coefficients)\n    model_matrix = tfd.Normal(\n        loc=np.array(0, dtype),\n        scale=np.array(1, dtype)).sample([n, d], seed=43)\n    scale = tf.convert_to_tensor(scale, dtype)\n    linear_response = tf.tensordot(\n        model_matrix, model_coefficients, axes=[[1], [0]])\n    if link == 'linear':\n      response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)\n    elif link == 'probit':\n      response = tf.cast(\n          tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,\n          dtype)\n    elif link == 'logit':\n      response = tfd.Bernoulli(logits=linear_response).sample(seed=44)\n    else:\n      raise ValueError('unrecognized true link: {}'.format(link))\n    return model_matrix, response, model_coefficients\n\n  X, Y, w_true = make_dataset(n=int(1e6), d=100, link='probit')\n\n  w, linear_response, is_converged, num_iter = tfp.glm.fit(\n      model_matrix=X,\n      response=Y,\n      model=tfp.glm.BernoulliNormalCDF())\n  log_likelihood = tfp.glm.BernoulliNormalCDF().log_prob(Y, linear_response)\n\n  with tf.Session() as sess:\n    [w_, linear_response_, is_converged_, num_iter_, Y_, w_true_,\n     log_likelihood_] = sess.run([\n        w, linear_response, is_converged, num_iter, Y, w_true,\n        log_likelihood])\n\n  print('is_converged: ', is_converged_)\n  print('    num_iter: ', num_iter_)\n  print('    accuracy: ', np.mean((linear_response_ > 0.) == Y_))\n  print('    deviance: ', 2. * np.mean(log_likelihood_))\n  print('||w0-w1||_2 / (1+||w0||_2): ', (np.linalg.norm(w_true_ - w_, ord=2) /\n                                         (1. + np.linalg.norm(w_true_, ord=2))))\n\n  # ==>\n  # is_converged:  True\n  #     num_iter:  6\n  #     accuracy:  0.804382\n  #     deviance:  -0.820746600628\n  # ||w0-w1||_2 / (1+||w0||_2):  0.00619245105309\n  ```\n\n  \"\"\"\n  graph_deps = [model_matrix, response, model_coefficients_start,\n                predicted_linear_response_start, dispersion, offset,\n                learning_rate, maximum_iterations]\n  with tf.compat.v1.name_scope(name, 'fit', graph_deps):\n    [\n        model_matrix,\n        response,\n        model_coefficients_start,\n        predicted_linear_response_start,\n        offset,\n    ] = prepare_args(\n        model_matrix,\n        response,\n        model_coefficients_start,\n        predicted_linear_response_start,\n        offset)\n    if convergence_criteria_fn is None:\n      convergence_criteria_fn = (\n          convergence_criteria_small_relative_norm_weights_change())\n\n    def _body(\n        is_converged_previous,\n        iter_,\n        model_coefficients_previous,\n        predicted_linear_response_previous):\n      \"\"\"`tf.while_loop` body.\"\"\"\n      model_coefficients_next, predicted_linear_response_next = fit_one_step(\n          model_matrix,\n          response,\n          model,\n          model_coefficients_previous,\n          predicted_linear_response_previous,\n          l2_regularizer,\n          dispersion,\n          offset,\n          learning_rate,\n          fast_unsafe_numerics)\n      is_converged_next = convergence_criteria_fn(\n          is_converged_previous=is_converged_previous,\n          iter_=iter_,\n          model_coefficients_previous=model_coefficients_previous,\n          predicted_linear_response_previous=predicted_linear_response_previous,\n          model_coefficients_next=model_coefficients_next,\n          predicted_linear_response_next=predicted_linear_response_next,\n          response=response,\n          model=model,\n          dispersion=dispersion)\n      return [\n          is_converged_next,\n          iter_ + 1,\n          model_coefficients_next,\n          predicted_linear_response_next,\n      ]\n\n    # while not converged:\n    #   fit_one_step\n    [\n        is_converged,\n        iter_,\n        model_coefficients,\n        predicted_linear_response,\n    ] = tf.while_loop(\n        cond=lambda is_converged, *args: tf.logical_not(is_converged),\n        body=_body,\n        loop_vars=[\n            tf.zeros([], np.bool),   # is_converged\n            tf.zeros([], np.int32),  # iter_\n            model_coefficients_start,\n            predicted_linear_response_start,\n        ],\n        maximum_iterations=maximum_iterations)\n\n    return [\n        model_coefficients,\n        predicted_linear_response,\n        is_converged,\n        iter_\n    ]", "code_tokens": ["def", "fit", "(", "model_matrix", ",", "response", ",", "model", ",", "model_coefficients_start", "=", "None", ",", "predicted_linear_response_start", "=", "None", ",", "l2_regularizer", "=", "None", ",", "dispersion", "=", "None", ",", "offset", "=", "None", ",", "convergence_criteria_fn", "=", "None", ",", "learning_rate", "=", "None", ",", "fast_unsafe_numerics", "=", "True", ",", "maximum_iterations", "=", "None", ",", "name", "=", "None", ")", ":", "graph_deps", "=", "[", "model_matrix", ",", "response", ",", "model_coefficients_start", ",", "predicted_linear_response_start", ",", "dispersion", ",", "offset", ",", "learning_rate", ",", "maximum_iterations", "]", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'fit'", ",", "graph_deps", ")", ":", "[", "model_matrix", ",", "response", ",", "model_coefficients_start", ",", "predicted_linear_response_start", ",", "offset", ",", "]", "=", "prepare_args", "(", "model_matrix", ",", "response", ",", "model_coefficients_start", ",", "predicted_linear_response_start", ",", "offset", ")", "if", "convergence_criteria_fn", "is", "None", ":", "convergence_criteria_fn", "=", "(", "convergence_criteria_small_relative_norm_weights_change", "(", ")", ")", "def", "_body", "(", "is_converged_previous", ",", "iter_", ",", "model_coefficients_previous", ",", "predicted_linear_response_previous", ")", ":", "\"\"\"`tf.while_loop` body.\"\"\"", "model_coefficients_next", ",", "predicted_linear_response_next", "=", "fit_one_step", "(", "model_matrix", ",", "response", ",", "model", ",", "model_coefficients_previous", ",", "predicted_linear_response_previous", ",", "l2_regularizer", ",", "dispersion", ",", "offset", ",", "learning_rate", ",", "fast_unsafe_numerics", ")", "is_converged_next", "=", "convergence_criteria_fn", "(", "is_converged_previous", "=", "is_converged_previous", ",", "iter_", "=", "iter_", ",", "model_coefficients_previous", "=", "model_coefficients_previous", ",", "predicted_linear_response_previous", "=", "predicted_linear_response_previous", ",", "model_coefficients_next", "=", "model_coefficients_next", ",", "predicted_linear_response_next", "=", "predicted_linear_response_next", ",", "response", "=", "response", ",", "model", "=", "model", ",", "dispersion", "=", "dispersion", ")", "return", "[", "is_converged_next", ",", "iter_", "+", "1", ",", "model_coefficients_next", ",", "predicted_linear_response_next", ",", "]", "# while not converged:", "#   fit_one_step", "[", "is_converged", ",", "iter_", ",", "model_coefficients", ",", "predicted_linear_response", ",", "]", "=", "tf", ".", "while_loop", "(", "cond", "=", "lambda", "is_converged", ",", "*", "args", ":", "tf", ".", "logical_not", "(", "is_converged", ")", ",", "body", "=", "_body", ",", "loop_vars", "=", "[", "tf", ".", "zeros", "(", "[", "]", ",", "np", ".", "bool", ")", ",", "# is_converged", "tf", ".", "zeros", "(", "[", "]", ",", "np", ".", "int32", ")", ",", "# iter_", "model_coefficients_start", ",", "predicted_linear_response_start", ",", "]", ",", "maximum_iterations", "=", "maximum_iterations", ")", "return", "[", "model_coefficients", ",", "predicted_linear_response", ",", "is_converged", ",", "iter_", "]"], "docstring": "Runs multiple Fisher scoring steps.\n\n  Args:\n    model_matrix: (Batch of) `float`-like, matrix-shaped `Tensor` where each row\n      represents a sample's features.\n    response: (Batch of) vector-shaped `Tensor` where each element represents a\n      sample's observed response (to the corresponding row of features). Must\n      have same `dtype` as `model_matrix`.\n    model: `tfp.glm.ExponentialFamily`-like instance which implicitly\n      characterizes a negative log-likelihood loss by specifying the\n      distribuion's `mean`, `gradient_mean`, and `variance`.\n    model_coefficients_start: Optional (batch of) vector-shaped `Tensor`\n      representing the initial model coefficients, one for each column in\n      `model_matrix`. Must have same `dtype` as `model_matrix`.\n      Default value: Zeros.\n    predicted_linear_response_start: Optional `Tensor` with `shape`, `dtype`\n      matching `response`; represents `offset` shifted initial linear\n      predictions based on `model_coefficients_start`.\n      Default value: `offset` if `model_coefficients is None`, and\n      `tf.linalg.matvec(model_matrix, model_coefficients_start) + offset`\n      otherwise.\n    l2_regularizer: Optional scalar `Tensor` representing L2 regularization\n      penalty, i.e.,\n      `loss(w) = sum{-log p(y[i]|x[i],w) : i=1..n} + l2_regularizer ||w||_2^2`.\n      Default value: `None` (i.e., no L2 regularization).\n    dispersion: Optional (batch of) `Tensor` representing `response` dispersion,\n      i.e., as in, `p(y|theta) := exp((y theta - A(theta)) / dispersion)`.\n      Must broadcast with rows of `model_matrix`.\n      Default value: `None` (i.e., \"no dispersion\").\n    offset: Optional `Tensor` representing constant shift applied to\n      `predicted_linear_response`.  Must broadcast to `response`.\n      Default value: `None` (i.e., `tf.zeros_like(response)`).\n    convergence_criteria_fn: Python `callable` taking:\n      `is_converged_previous`, `iter_`, `model_coefficients_previous`,\n      `predicted_linear_response_previous`, `model_coefficients_next`,\n      `predicted_linear_response_next`, `response`, `model`, `dispersion` and\n      returning a `bool` `Tensor` indicating that Fisher scoring has converged.\n      See `convergence_criteria_small_relative_norm_weights_change` as an\n      example function.\n      Default value: `None` (i.e.,\n      `convergence_criteria_small_relative_norm_weights_change`).\n    learning_rate: Optional (batch of) scalar `Tensor` used to dampen iterative\n      progress. Typically only needed if optimization diverges, should be no\n      larger than `1` and typically very close to `1`.\n      Default value: `None` (i.e., `1`).\n    fast_unsafe_numerics: Optional Python `bool` indicating if faster, less\n      numerically accurate methods can be employed for computing the weighted\n      least-squares solution.\n      Default value: `True` (i.e., \"fast but possibly diminished accuracy\").\n    maximum_iterations: Optional maximum number of iterations of Fisher scoring\n      to run; \"and-ed\" with result of `convergence_criteria_fn`.\n      Default value: `None` (i.e., `infinity`).\n    name: Python `str` used as name prefix to ops created by this function.\n      Default value: `\"fit\"`.\n\n  Returns:\n    model_coefficients: (Batch of) vector-shaped `Tensor`; represents the\n      fitted model coefficients, one for each column in `model_matrix`.\n    predicted_linear_response: `response`-shaped `Tensor` representing linear\n      predictions based on new `model_coefficients`, i.e.,\n      `tf.linalg.matvec(model_matrix, model_coefficients) + offset`.\n    is_converged: `bool` `Tensor` indicating that the returned\n      `model_coefficients` met the `convergence_criteria_fn` criteria within the\n      `maximum_iterations` limit.\n    iter_: `int32` `Tensor` indicating the number of iterations taken.\n\n  #### Example\n\n  ```python\n  from __future__ import print_function\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  def make_dataset(n, d, link, scale=1., dtype=np.float32):\n    model_coefficients = tfd.Uniform(\n        low=np.array(-1, dtype),\n        high=np.array(1, dtype)).sample(d, seed=42)\n    radius = np.sqrt(2.)\n    model_coefficients *= radius / tf.linalg.norm(model_coefficients)\n    model_matrix = tfd.Normal(\n        loc=np.array(0, dtype),\n        scale=np.array(1, dtype)).sample([n, d], seed=43)\n    scale = tf.convert_to_tensor(scale, dtype)\n    linear_response = tf.tensordot(\n        model_matrix, model_coefficients, axes=[[1], [0]])\n    if link == 'linear':\n      response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)\n    elif link == 'probit':\n      response = tf.cast(\n          tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,\n          dtype)\n    elif link == 'logit':\n      response = tfd.Bernoulli(logits=linear_response).sample(seed=44)\n    else:\n      raise ValueError('unrecognized true link: {}'.format(link))\n    return model_matrix, response, model_coefficients\n\n  X, Y, w_true = make_dataset(n=int(1e6), d=100, link='probit')\n\n  w, linear_response, is_converged, num_iter = tfp.glm.fit(\n      model_matrix=X,\n      response=Y,\n      model=tfp.glm.BernoulliNormalCDF())\n  log_likelihood = tfp.glm.BernoulliNormalCDF().log_prob(Y, linear_response)\n\n  with tf.Session() as sess:\n    [w_, linear_response_, is_converged_, num_iter_, Y_, w_true_,\n     log_likelihood_] = sess.run([\n        w, linear_response, is_converged, num_iter, Y, w_true,\n        log_likelihood])\n\n  print('is_converged: ', is_converged_)\n  print('    num_iter: ', num_iter_)\n  print('    accuracy: ', np.mean((linear_response_ > 0.) == Y_))\n  print('    deviance: ', 2. * np.mean(log_likelihood_))\n  print('||w0-w1||_2 / (1+||w0||_2): ', (np.linalg.norm(w_true_ - w_, ord=2) /\n                                         (1. + np.linalg.norm(w_true_, ord=2))))\n\n  # ==>\n  # is_converged:  True\n  #     num_iter:  6\n  #     accuracy:  0.804382\n  #     deviance:  -0.820746600628\n  # ||w0-w1||_2 / (1+||w0||_2):  0.00619245105309\n  ```", "docstring_tokens": ["Runs", "multiple", "Fisher", "scoring", "steps", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/glm/fisher_scoring.py#L36-L256", "partition": "test"}
{"repo": "genialis/genesis-pyapi", "path": "genesis/utils.py", "func_name": "iterate_schema", "original_string": "def iterate_schema(fields, schema, path=None):\n    \"\"\"Recursively iterate over all schema sub-fields.\n\n    :param fields: Field instance (e.g. input)\n    :type fields: dict\n    :param schema: Schema instance (e.g. input_schema)\n    :type schema: dict\n    :path schema: Field path\n    :path schema: string\n\n    \"\"\"\n    for field_schema in schema:\n        name = field_schema['name']\n        if 'group' in field_schema:\n            for rvals in iterate_schema(fields[name] if name in fields else {},\n                                        field_schema['group'],\n                                        None if path is None else '{}.{}'.format(path, name)):\n                yield rvals\n        else:\n            if path is None:\n                yield (field_schema, fields)\n            else:\n                yield (field_schema, fields, '{}.{}'.format(path, name))", "language": "python", "code": "def iterate_schema(fields, schema, path=None):\n    \"\"\"Recursively iterate over all schema sub-fields.\n\n    :param fields: Field instance (e.g. input)\n    :type fields: dict\n    :param schema: Schema instance (e.g. input_schema)\n    :type schema: dict\n    :path schema: Field path\n    :path schema: string\n\n    \"\"\"\n    for field_schema in schema:\n        name = field_schema['name']\n        if 'group' in field_schema:\n            for rvals in iterate_schema(fields[name] if name in fields else {},\n                                        field_schema['group'],\n                                        None if path is None else '{}.{}'.format(path, name)):\n                yield rvals\n        else:\n            if path is None:\n                yield (field_schema, fields)\n            else:\n                yield (field_schema, fields, '{}.{}'.format(path, name))", "code_tokens": ["def", "iterate_schema", "(", "fields", ",", "schema", ",", "path", "=", "None", ")", ":", "for", "field_schema", "in", "schema", ":", "name", "=", "field_schema", "[", "'name'", "]", "if", "'group'", "in", "field_schema", ":", "for", "rvals", "in", "iterate_schema", "(", "fields", "[", "name", "]", "if", "name", "in", "fields", "else", "{", "}", ",", "field_schema", "[", "'group'", "]", ",", "None", "if", "path", "is", "None", "else", "'{}.{}'", ".", "format", "(", "path", ",", "name", ")", ")", ":", "yield", "rvals", "else", ":", "if", "path", "is", "None", ":", "yield", "(", "field_schema", ",", "fields", ")", "else", ":", "yield", "(", "field_schema", ",", "fields", ",", "'{}.{}'", ".", "format", "(", "path", ",", "name", ")", ")"], "docstring": "Recursively iterate over all schema sub-fields.\n\n    :param fields: Field instance (e.g. input)\n    :type fields: dict\n    :param schema: Schema instance (e.g. input_schema)\n    :type schema: dict\n    :path schema: Field path\n    :path schema: string", "docstring_tokens": ["Recursively", "iterate", "over", "all", "schema", "sub", "-", "fields", "."], "sha": "dfe9bcc8b332a8b9873db4ab9994b0cc10eb209a", "url": "https://github.com/genialis/genesis-pyapi/blob/dfe9bcc8b332a8b9873db4ab9994b0cc10eb209a/genesis/utils.py#L36-L58", "partition": "test"}
{"repo": "nprapps/copydoc", "path": "copydoc.py", "func_name": "CopyDoc.check_next", "original_string": "def check_next(self, tag):\n        \"\"\"\n        If next tag is link with same href, combine them.\n        \"\"\"\n        if (type(tag.next_sibling) == element.Tag and\n                tag.next_sibling.name == 'a'):\n\n            next_tag = tag.next_sibling\n            if tag.get('href') and next_tag.get('href'):\n                href = self._parse_href(tag.get('href'))\n                next_href = self._parse_href(next_tag.get('href'))\n\n                if href == next_href:\n                    next_text = next_tag.get_text()\n                    tag.append(next_text)\n                    self.tags_blacklist.append(next_tag)", "language": "python", "code": "def check_next(self, tag):\n        \"\"\"\n        If next tag is link with same href, combine them.\n        \"\"\"\n        if (type(tag.next_sibling) == element.Tag and\n                tag.next_sibling.name == 'a'):\n\n            next_tag = tag.next_sibling\n            if tag.get('href') and next_tag.get('href'):\n                href = self._parse_href(tag.get('href'))\n                next_href = self._parse_href(next_tag.get('href'))\n\n                if href == next_href:\n                    next_text = next_tag.get_text()\n                    tag.append(next_text)\n                    self.tags_blacklist.append(next_tag)", "code_tokens": ["def", "check_next", "(", "self", ",", "tag", ")", ":", "if", "(", "type", "(", "tag", ".", "next_sibling", ")", "==", "element", ".", "Tag", "and", "tag", ".", "next_sibling", ".", "name", "==", "'a'", ")", ":", "next_tag", "=", "tag", ".", "next_sibling", "if", "tag", ".", "get", "(", "'href'", ")", "and", "next_tag", ".", "get", "(", "'href'", ")", ":", "href", "=", "self", ".", "_parse_href", "(", "tag", ".", "get", "(", "'href'", ")", ")", "next_href", "=", "self", ".", "_parse_href", "(", "next_tag", ".", "get", "(", "'href'", ")", ")", "if", "href", "==", "next_href", ":", "next_text", "=", "next_tag", ".", "get_text", "(", ")", "tag", ".", "append", "(", "next_text", ")", "self", ".", "tags_blacklist", ".", "append", "(", "next_tag", ")"], "docstring": "If next tag is link with same href, combine them.", "docstring_tokens": ["If", "next", "tag", "is", "link", "with", "same", "href", "combine", "them", "."], "sha": "e1ab09b287beb0439748c319cf165cbc06c66624", "url": "https://github.com/nprapps/copydoc/blob/e1ab09b287beb0439748c319cf165cbc06c66624/copydoc.py#L89-L104", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/quantum_info/operators/channel/choi.py", "func_name": "Choi._bipartite_shape", "original_string": "def _bipartite_shape(self):\n        \"\"\"Return the shape for bipartite matrix\"\"\"\n        return (self._input_dim, self._output_dim, self._input_dim,\n                self._output_dim)", "language": "python", "code": "def _bipartite_shape(self):\n        \"\"\"Return the shape for bipartite matrix\"\"\"\n        return (self._input_dim, self._output_dim, self._input_dim,\n                self._output_dim)", "code_tokens": ["def", "_bipartite_shape", "(", "self", ")", ":", "return", "(", "self", ".", "_input_dim", ",", "self", ".", "_output_dim", ",", "self", ".", "_input_dim", ",", "self", ".", "_output_dim", ")"], "docstring": "Return the shape for bipartite matrix", "docstring_tokens": ["Return", "the", "shape", "for", "bipartite", "matrix"], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/quantum_info/operators/channel/choi.py#L113-L116", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/services/kmip_client.py", "func_name": "KMIPProxy.get_attribute_list", "original_string": "def get_attribute_list(self, uid=None):\n        \"\"\"\n        Send a GetAttributeList request to the server.\n\n        Args:\n            uid (string): The ID of the managed object with which the retrieved\n                attribute names should be associated.\n\n        Returns:\n            result (GetAttributeListResult): A structure containing the results\n                of the operation.\n        \"\"\"\n        batch_item = self._build_get_attribute_list_batch_item(uid)\n\n        request = self._build_request_message(None, [batch_item])\n        response = self._send_and_receive_message(request)\n        results = self._process_batch_items(response)\n        return results[0]", "language": "python", "code": "def get_attribute_list(self, uid=None):\n        \"\"\"\n        Send a GetAttributeList request to the server.\n\n        Args:\n            uid (string): The ID of the managed object with which the retrieved\n                attribute names should be associated.\n\n        Returns:\n            result (GetAttributeListResult): A structure containing the results\n                of the operation.\n        \"\"\"\n        batch_item = self._build_get_attribute_list_batch_item(uid)\n\n        request = self._build_request_message(None, [batch_item])\n        response = self._send_and_receive_message(request)\n        results = self._process_batch_items(response)\n        return results[0]", "code_tokens": ["def", "get_attribute_list", "(", "self", ",", "uid", "=", "None", ")", ":", "batch_item", "=", "self", ".", "_build_get_attribute_list_batch_item", "(", "uid", ")", "request", "=", "self", ".", "_build_request_message", "(", "None", ",", "[", "batch_item", "]", ")", "response", "=", "self", ".", "_send_and_receive_message", "(", "request", ")", "results", "=", "self", ".", "_process_batch_items", "(", "response", ")", "return", "results", "[", "0", "]"], "docstring": "Send a GetAttributeList request to the server.\n\n        Args:\n            uid (string): The ID of the managed object with which the retrieved\n                attribute names should be associated.\n\n        Returns:\n            result (GetAttributeListResult): A structure containing the results\n                of the operation.", "docstring_tokens": ["Send", "a", "GetAttributeList", "request", "to", "the", "server", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/services/kmip_client.py#L641-L658", "partition": "test"}
{"repo": "farzadghanei/statsd-metrics", "path": "statsdmetrics/client/tcp.py", "func_name": "TCPBatchClient.flush", "original_string": "def flush(self):\n        \"\"\"Send buffered metrics in batch requests over TCP\"\"\"\n        # type: () -> TCPBatchClient\n        while len(self._batches) > 0:\n            self._socket.sendall(self._batches[0])\n            self._batches.popleft()\n        return self", "language": "python", "code": "def flush(self):\n        \"\"\"Send buffered metrics in batch requests over TCP\"\"\"\n        # type: () -> TCPBatchClient\n        while len(self._batches) > 0:\n            self._socket.sendall(self._batches[0])\n            self._batches.popleft()\n        return self", "code_tokens": ["def", "flush", "(", "self", ")", ":", "# type: () -> TCPBatchClient", "while", "len", "(", "self", ".", "_batches", ")", ">", "0", ":", "self", ".", "_socket", ".", "sendall", "(", "self", ".", "_batches", "[", "0", "]", ")", "self", ".", "_batches", ".", "popleft", "(", ")", "return", "self"], "docstring": "Send buffered metrics in batch requests over TCP", "docstring_tokens": ["Send", "buffered", "metrics", "in", "batch", "requests", "over", "TCP"], "sha": "153ff37b79777f208e49bb9d3fb737ba52b99f98", "url": "https://github.com/farzadghanei/statsd-metrics/blob/153ff37b79777f208e49bb9d3fb737ba52b99f98/statsdmetrics/client/tcp.py#L65-L71", "partition": "test"}
{"repo": "chrisjrn/registrasion", "path": "registrasion/controllers/cart.py", "func_name": "CartController._autoextend_reservation", "original_string": "def _autoextend_reservation(self):\n        ''' Updates the cart's time last updated value, which is used to\n        determine whether the cart has reserved the items and discounts it\n        holds. '''\n\n        time = timezone.now()\n\n        # Calculate the residual of the _old_ reservation duration\n        # if it's greater than what's in the cart now, keep it.\n        time_elapsed_since_updated = (time - self.cart.time_last_updated)\n        residual = self.cart.reservation_duration - time_elapsed_since_updated\n\n        reservations = [datetime.timedelta(0), residual]\n\n        # If we have vouchers, we're entitled to an hour at minimum.\n        if len(self.cart.vouchers.all()) >= 1:\n            reservations.append(inventory.Voucher.RESERVATION_DURATION)\n\n        # Else, it's the maximum of the included products\n        items = commerce.ProductItem.objects.filter(cart=self.cart)\n        agg = items.aggregate(Max(\"product__reservation_duration\"))\n        product_max = agg[\"product__reservation_duration__max\"]\n\n        if product_max is not None:\n            reservations.append(product_max)\n\n        self.cart.time_last_updated = time\n        self.cart.reservation_duration = max(reservations)", "language": "python", "code": "def _autoextend_reservation(self):\n        ''' Updates the cart's time last updated value, which is used to\n        determine whether the cart has reserved the items and discounts it\n        holds. '''\n\n        time = timezone.now()\n\n        # Calculate the residual of the _old_ reservation duration\n        # if it's greater than what's in the cart now, keep it.\n        time_elapsed_since_updated = (time - self.cart.time_last_updated)\n        residual = self.cart.reservation_duration - time_elapsed_since_updated\n\n        reservations = [datetime.timedelta(0), residual]\n\n        # If we have vouchers, we're entitled to an hour at minimum.\n        if len(self.cart.vouchers.all()) >= 1:\n            reservations.append(inventory.Voucher.RESERVATION_DURATION)\n\n        # Else, it's the maximum of the included products\n        items = commerce.ProductItem.objects.filter(cart=self.cart)\n        agg = items.aggregate(Max(\"product__reservation_duration\"))\n        product_max = agg[\"product__reservation_duration__max\"]\n\n        if product_max is not None:\n            reservations.append(product_max)\n\n        self.cart.time_last_updated = time\n        self.cart.reservation_duration = max(reservations)", "code_tokens": ["def", "_autoextend_reservation", "(", "self", ")", ":", "time", "=", "timezone", ".", "now", "(", ")", "# Calculate the residual of the _old_ reservation duration", "# if it's greater than what's in the cart now, keep it.", "time_elapsed_since_updated", "=", "(", "time", "-", "self", ".", "cart", ".", "time_last_updated", ")", "residual", "=", "self", ".", "cart", ".", "reservation_duration", "-", "time_elapsed_since_updated", "reservations", "=", "[", "datetime", ".", "timedelta", "(", "0", ")", ",", "residual", "]", "# If we have vouchers, we're entitled to an hour at minimum.", "if", "len", "(", "self", ".", "cart", ".", "vouchers", ".", "all", "(", ")", ")", ">=", "1", ":", "reservations", ".", "append", "(", "inventory", ".", "Voucher", ".", "RESERVATION_DURATION", ")", "# Else, it's the maximum of the included products", "items", "=", "commerce", ".", "ProductItem", ".", "objects", ".", "filter", "(", "cart", "=", "self", ".", "cart", ")", "agg", "=", "items", ".", "aggregate", "(", "Max", "(", "\"product__reservation_duration\"", ")", ")", "product_max", "=", "agg", "[", "\"product__reservation_duration__max\"", "]", "if", "product_max", "is", "not", "None", ":", "reservations", ".", "append", "(", "product_max", ")", "self", ".", "cart", ".", "time_last_updated", "=", "time", "self", ".", "cart", ".", "reservation_duration", "=", "max", "(", "reservations", ")"], "docstring": "Updates the cart's time last updated value, which is used to\n        determine whether the cart has reserved the items and discounts it\n        holds.", "docstring_tokens": ["Updates", "the", "cart", "s", "time", "last", "updated", "value", "which", "is", "used", "to", "determine", "whether", "the", "cart", "has", "reserved", "the", "items", "and", "discounts", "it", "holds", "."], "sha": "461d5846c6f9f3b7099322a94f5d9911564448e4", "url": "https://github.com/chrisjrn/registrasion/blob/461d5846c6f9f3b7099322a94f5d9911564448e4/registrasion/controllers/cart.py#L74-L101", "partition": "test"}
{"repo": "neurodata/ndio", "path": "ndio/convert/volume.py", "func_name": "to_voxels", "original_string": "def to_voxels(array):\n    \"\"\"\n    Converts an array to its voxel list.\n\n    Arguments:\n        array (numpy.ndarray): A numpy nd array. This must be boolean!\n\n    Returns:\n        A list of n-tuples\n    \"\"\"\n    if type(array) is not numpy.ndarray:\n        raise ValueError(\"array argument must be of type numpy.ndarray\")\n    return numpy.argwhere(array)", "language": "python", "code": "def to_voxels(array):\n    \"\"\"\n    Converts an array to its voxel list.\n\n    Arguments:\n        array (numpy.ndarray): A numpy nd array. This must be boolean!\n\n    Returns:\n        A list of n-tuples\n    \"\"\"\n    if type(array) is not numpy.ndarray:\n        raise ValueError(\"array argument must be of type numpy.ndarray\")\n    return numpy.argwhere(array)", "code_tokens": ["def", "to_voxels", "(", "array", ")", ":", "if", "type", "(", "array", ")", "is", "not", "numpy", ".", "ndarray", ":", "raise", "ValueError", "(", "\"array argument must be of type numpy.ndarray\"", ")", "return", "numpy", ".", "argwhere", "(", "array", ")"], "docstring": "Converts an array to its voxel list.\n\n    Arguments:\n        array (numpy.ndarray): A numpy nd array. This must be boolean!\n\n    Returns:\n        A list of n-tuples", "docstring_tokens": ["Converts", "an", "array", "to", "its", "voxel", "list", "."], "sha": "792dd5816bc770b05a3db2f4327da42ff6253531", "url": "https://github.com/neurodata/ndio/blob/792dd5816bc770b05a3db2f4327da42ff6253531/ndio/convert/volume.py#L5-L17", "partition": "test"}
{"repo": "dagster-io/dagster", "path": "examples/airline-demo/airline_demo/utils.py", "func_name": "mkdir_p", "original_string": "def mkdir_p(newdir, mode=0o777):\n    \"\"\"The missing mkdir -p functionality in os.\"\"\"\n    try:\n        os.makedirs(newdir, mode)\n    except OSError as err:\n        # Reraise the error unless it's about an already existing directory\n        if err.errno != errno.EEXIST or not os.path.isdir(newdir):\n            raise", "language": "python", "code": "def mkdir_p(newdir, mode=0o777):\n    \"\"\"The missing mkdir -p functionality in os.\"\"\"\n    try:\n        os.makedirs(newdir, mode)\n    except OSError as err:\n        # Reraise the error unless it's about an already existing directory\n        if err.errno != errno.EEXIST or not os.path.isdir(newdir):\n            raise", "code_tokens": ["def", "mkdir_p", "(", "newdir", ",", "mode", "=", "0o777", ")", ":", "try", ":", "os", ".", "makedirs", "(", "newdir", ",", "mode", ")", "except", "OSError", "as", "err", ":", "# Reraise the error unless it's about an already existing directory", "if", "err", ".", "errno", "!=", "errno", ".", "EEXIST", "or", "not", "os", ".", "path", ".", "isdir", "(", "newdir", ")", ":", "raise"], "docstring": "The missing mkdir -p functionality in os.", "docstring_tokens": ["The", "missing", "mkdir", "-", "p", "functionality", "in", "os", "."], "sha": "4119f8c773089de64831b1dfb9e168e353d401dc", "url": "https://github.com/dagster-io/dagster/blob/4119f8c773089de64831b1dfb9e168e353d401dc/examples/airline-demo/airline_demo/utils.py#L10-L17", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/model/regression.py", "func_name": "h2o_mean_squared_error", "original_string": "def h2o_mean_squared_error(y_actual, y_predicted, weights=None):\n    \"\"\"\n    Mean squared error regression loss\n\n    :param y_actual: H2OFrame of actual response.\n    :param y_predicted: H2OFrame of predicted response.\n    :param weights: (Optional) sample weights\n    :returns: mean squared error loss (best is 0.0).\n    \"\"\"\n    ModelBase._check_targets(y_actual, y_predicted)\n    return _colmean((y_predicted - y_actual) ** 2)", "language": "python", "code": "def h2o_mean_squared_error(y_actual, y_predicted, weights=None):\n    \"\"\"\n    Mean squared error regression loss\n\n    :param y_actual: H2OFrame of actual response.\n    :param y_predicted: H2OFrame of predicted response.\n    :param weights: (Optional) sample weights\n    :returns: mean squared error loss (best is 0.0).\n    \"\"\"\n    ModelBase._check_targets(y_actual, y_predicted)\n    return _colmean((y_predicted - y_actual) ** 2)", "code_tokens": ["def", "h2o_mean_squared_error", "(", "y_actual", ",", "y_predicted", ",", "weights", "=", "None", ")", ":", "ModelBase", ".", "_check_targets", "(", "y_actual", ",", "y_predicted", ")", "return", "_colmean", "(", "(", "y_predicted", "-", "y_actual", ")", "**", "2", ")"], "docstring": "Mean squared error regression loss\n\n    :param y_actual: H2OFrame of actual response.\n    :param y_predicted: H2OFrame of predicted response.\n    :param weights: (Optional) sample weights\n    :returns: mean squared error loss (best is 0.0).", "docstring_tokens": ["Mean", "squared", "error", "regression", "loss"], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/model/regression.py#L56-L66", "partition": "test"}
{"repo": "urinieto/msaf", "path": "msaf/algorithms/olda/make_train.py", "func_name": "align_segmentation", "original_string": "def align_segmentation(beat_times, song):\n    '''Load a ground-truth segmentation, and align times to the nearest\n    detected beats.\n\n    Arguments:\n        beat_times -- array\n        song -- path to the audio file\n\n    Returns:\n        segment_beats -- array\n            beat-aligned segment boundaries\n\n        segment_times -- array\n            true segment times\n\n        segment_labels -- array\n            list of segment labels\n    '''\n    try:\n        segment_times, segment_labels = msaf.io.read_references(song)\n    except:\n        return None, None, None\n    segment_times = np.asarray(segment_times)\n\n    # Map to intervals\n    segment_intervals = msaf.utils.times_to_intervals(segment_times)\n\n    # Map beats to intervals\n    beat_intervals = np.asarray(zip(beat_times[:-1], beat_times[1:]))\n\n    # Map beats to segments\n    beat_segment_ids = librosa.util.match_intervals(beat_intervals,\n                                                    segment_intervals)\n\n    segment_beats = []\n    segment_times_out = []\n    segment_labels_out = []\n\n    # print segment_times, beat_segment_ids, len(beat_times),\n    # len(beat_segment_ids)\n    for i in range(segment_times.shape[0]):\n        hits = np.argwhere(beat_segment_ids == i)\n        if len(hits) > 0 and i < len(segment_intervals) and \\\n                i < len(segment_labels):\n            segment_beats.extend(hits[0])\n            segment_times_out.append(segment_intervals[i, :])\n            segment_labels_out.append(segment_labels[i])\n\n    # Pull out the segment start times\n    segment_beats = list(segment_beats)\n    # segment_times_out = np.asarray(\n    #  segment_times_out)[:, 0].squeeze().reshape((-1, 1))\n\n    # if segment_times_out.ndim == 0:\n    #    segment_times_out = segment_times_out[np.newaxis]\n    segment_times_out = segment_times\n\n    return segment_beats, segment_times_out, segment_labels_out", "language": "python", "code": "def align_segmentation(beat_times, song):\n    '''Load a ground-truth segmentation, and align times to the nearest\n    detected beats.\n\n    Arguments:\n        beat_times -- array\n        song -- path to the audio file\n\n    Returns:\n        segment_beats -- array\n            beat-aligned segment boundaries\n\n        segment_times -- array\n            true segment times\n\n        segment_labels -- array\n            list of segment labels\n    '''\n    try:\n        segment_times, segment_labels = msaf.io.read_references(song)\n    except:\n        return None, None, None\n    segment_times = np.asarray(segment_times)\n\n    # Map to intervals\n    segment_intervals = msaf.utils.times_to_intervals(segment_times)\n\n    # Map beats to intervals\n    beat_intervals = np.asarray(zip(beat_times[:-1], beat_times[1:]))\n\n    # Map beats to segments\n    beat_segment_ids = librosa.util.match_intervals(beat_intervals,\n                                                    segment_intervals)\n\n    segment_beats = []\n    segment_times_out = []\n    segment_labels_out = []\n\n    # print segment_times, beat_segment_ids, len(beat_times),\n    # len(beat_segment_ids)\n    for i in range(segment_times.shape[0]):\n        hits = np.argwhere(beat_segment_ids == i)\n        if len(hits) > 0 and i < len(segment_intervals) and \\\n                i < len(segment_labels):\n            segment_beats.extend(hits[0])\n            segment_times_out.append(segment_intervals[i, :])\n            segment_labels_out.append(segment_labels[i])\n\n    # Pull out the segment start times\n    segment_beats = list(segment_beats)\n    # segment_times_out = np.asarray(\n    #  segment_times_out)[:, 0].squeeze().reshape((-1, 1))\n\n    # if segment_times_out.ndim == 0:\n    #    segment_times_out = segment_times_out[np.newaxis]\n    segment_times_out = segment_times\n\n    return segment_beats, segment_times_out, segment_labels_out", "code_tokens": ["def", "align_segmentation", "(", "beat_times", ",", "song", ")", ":", "try", ":", "segment_times", ",", "segment_labels", "=", "msaf", ".", "io", ".", "read_references", "(", "song", ")", "except", ":", "return", "None", ",", "None", ",", "None", "segment_times", "=", "np", ".", "asarray", "(", "segment_times", ")", "# Map to intervals", "segment_intervals", "=", "msaf", ".", "utils", ".", "times_to_intervals", "(", "segment_times", ")", "# Map beats to intervals", "beat_intervals", "=", "np", ".", "asarray", "(", "zip", "(", "beat_times", "[", ":", "-", "1", "]", ",", "beat_times", "[", "1", ":", "]", ")", ")", "# Map beats to segments", "beat_segment_ids", "=", "librosa", ".", "util", ".", "match_intervals", "(", "beat_intervals", ",", "segment_intervals", ")", "segment_beats", "=", "[", "]", "segment_times_out", "=", "[", "]", "segment_labels_out", "=", "[", "]", "# print segment_times, beat_segment_ids, len(beat_times),", "# len(beat_segment_ids)", "for", "i", "in", "range", "(", "segment_times", ".", "shape", "[", "0", "]", ")", ":", "hits", "=", "np", ".", "argwhere", "(", "beat_segment_ids", "==", "i", ")", "if", "len", "(", "hits", ")", ">", "0", "and", "i", "<", "len", "(", "segment_intervals", ")", "and", "i", "<", "len", "(", "segment_labels", ")", ":", "segment_beats", ".", "extend", "(", "hits", "[", "0", "]", ")", "segment_times_out", ".", "append", "(", "segment_intervals", "[", "i", ",", ":", "]", ")", "segment_labels_out", ".", "append", "(", "segment_labels", "[", "i", "]", ")", "# Pull out the segment start times", "segment_beats", "=", "list", "(", "segment_beats", ")", "# segment_times_out = np.asarray(", "#  segment_times_out)[:, 0].squeeze().reshape((-1, 1))", "# if segment_times_out.ndim == 0:", "#    segment_times_out = segment_times_out[np.newaxis]", "segment_times_out", "=", "segment_times", "return", "segment_beats", ",", "segment_times_out", ",", "segment_labels_out"], "docstring": "Load a ground-truth segmentation, and align times to the nearest\n    detected beats.\n\n    Arguments:\n        beat_times -- array\n        song -- path to the audio file\n\n    Returns:\n        segment_beats -- array\n            beat-aligned segment boundaries\n\n        segment_times -- array\n            true segment times\n\n        segment_labels -- array\n            list of segment labels", "docstring_tokens": ["Load", "a", "ground", "-", "truth", "segmentation", "and", "align", "times", "to", "the", "nearest", "detected", "beats", "."], "sha": "9dbb57d77a1310465a65cc40f1641d083ca74385", "url": "https://github.com/urinieto/msaf/blob/9dbb57d77a1310465a65cc40f1641d083ca74385/msaf/algorithms/olda/make_train.py#L18-L75", "partition": "test"}
{"repo": "pmacosta/peng", "path": "peng/wave_functions.py", "func_name": "atanh", "original_string": "def atanh(wave):\n    r\"\"\"\n    Return the hyperbolic arc tangent of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.atanh\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"\n    pexdoc.exh.addex(\n        ValueError,\n        \"Math domain error\",\n        bool((min(wave._dep_vector) < -1) or (max(wave._dep_vector) > 1)),\n    )\n    return _operation(wave, \"atanh\", \"\", np.arctanh)", "language": "python", "code": "def atanh(wave):\n    r\"\"\"\n    Return the hyperbolic arc tangent of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.atanh\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"\n    pexdoc.exh.addex(\n        ValueError,\n        \"Math domain error\",\n        bool((min(wave._dep_vector) < -1) or (max(wave._dep_vector) > 1)),\n    )\n    return _operation(wave, \"atanh\", \"\", np.arctanh)", "code_tokens": ["def", "atanh", "(", "wave", ")", ":", "pexdoc", ".", "exh", ".", "addex", "(", "ValueError", ",", "\"Math domain error\"", ",", "bool", "(", "(", "min", "(", "wave", ".", "_dep_vector", ")", "<", "-", "1", ")", "or", "(", "max", "(", "wave", ".", "_dep_vector", ")", ">", "1", ")", ")", ",", ")", "return", "_operation", "(", "wave", ",", "\"atanh\"", ",", "\"\"", ",", "np", ".", "arctanh", ")"], "docstring": "r\"\"\"\n    Return the hyperbolic arc tangent of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.atanh\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]", "docstring_tokens": ["r", "Return", "the", "hyperbolic", "arc", "tangent", "of", "a", "waveform", "s", "dependent", "variable", "vector", "."], "sha": "976935377adaa3de26fc5677aceb2cdfbd6f93a7", "url": "https://github.com/pmacosta/peng/blob/976935377adaa3de26fc5677aceb2cdfbd6f93a7/peng/wave_functions.py#L259-L284", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/hdl/statements.py", "func_name": "HdlStatement._merge_statement_lists", "original_string": "def _merge_statement_lists(stmsA: List[\"HdlStatement\"], stmsB: List[\"HdlStatement\"])\\\n            -> List[\"HdlStatement\"]:\n        \"\"\"\n        Merge two lists of statements into one\n\n        :return: list of merged statements\n        \"\"\"\n        if stmsA is None and stmsB is None:\n            return None\n\n        tmp = []\n\n        a_it = iter(stmsA)\n        b_it = iter(stmsB)\n\n        a = None\n        b = None\n        a_empty = False\n        b_empty = False\n\n        while not a_empty and not b_empty:\n            while not a_empty:\n                a = next(a_it, None)\n                if a is None:\n                    a_empty = True\n                    break\n                elif a.rank == 0:\n                    # simple statement does not require merging\n                    tmp.append(a)\n                    a = None\n                else:\n                    break\n\n            while not b_empty:\n                b = next(b_it, None)\n                if b is None:\n                    b_empty = True\n                    break\n                elif b.rank == 0:\n                    # simple statement does not require merging\n                    tmp.append(b)\n                    b = None\n                else:\n                    break\n\n            if a is not None or b is not None:\n                a._merge_with_other_stm(b)\n                tmp.append(a)\n                a = None\n                b = None\n\n        return tmp", "language": "python", "code": "def _merge_statement_lists(stmsA: List[\"HdlStatement\"], stmsB: List[\"HdlStatement\"])\\\n            -> List[\"HdlStatement\"]:\n        \"\"\"\n        Merge two lists of statements into one\n\n        :return: list of merged statements\n        \"\"\"\n        if stmsA is None and stmsB is None:\n            return None\n\n        tmp = []\n\n        a_it = iter(stmsA)\n        b_it = iter(stmsB)\n\n        a = None\n        b = None\n        a_empty = False\n        b_empty = False\n\n        while not a_empty and not b_empty:\n            while not a_empty:\n                a = next(a_it, None)\n                if a is None:\n                    a_empty = True\n                    break\n                elif a.rank == 0:\n                    # simple statement does not require merging\n                    tmp.append(a)\n                    a = None\n                else:\n                    break\n\n            while not b_empty:\n                b = next(b_it, None)\n                if b is None:\n                    b_empty = True\n                    break\n                elif b.rank == 0:\n                    # simple statement does not require merging\n                    tmp.append(b)\n                    b = None\n                else:\n                    break\n\n            if a is not None or b is not None:\n                a._merge_with_other_stm(b)\n                tmp.append(a)\n                a = None\n                b = None\n\n        return tmp", "code_tokens": ["def", "_merge_statement_lists", "(", "stmsA", ":", "List", "[", "\"HdlStatement\"", "]", ",", "stmsB", ":", "List", "[", "\"HdlStatement\"", "]", ")", "->", "List", "[", "\"HdlStatement\"", "]", ":", "if", "stmsA", "is", "None", "and", "stmsB", "is", "None", ":", "return", "None", "tmp", "=", "[", "]", "a_it", "=", "iter", "(", "stmsA", ")", "b_it", "=", "iter", "(", "stmsB", ")", "a", "=", "None", "b", "=", "None", "a_empty", "=", "False", "b_empty", "=", "False", "while", "not", "a_empty", "and", "not", "b_empty", ":", "while", "not", "a_empty", ":", "a", "=", "next", "(", "a_it", ",", "None", ")", "if", "a", "is", "None", ":", "a_empty", "=", "True", "break", "elif", "a", ".", "rank", "==", "0", ":", "# simple statement does not require merging", "tmp", ".", "append", "(", "a", ")", "a", "=", "None", "else", ":", "break", "while", "not", "b_empty", ":", "b", "=", "next", "(", "b_it", ",", "None", ")", "if", "b", "is", "None", ":", "b_empty", "=", "True", "break", "elif", "b", ".", "rank", "==", "0", ":", "# simple statement does not require merging", "tmp", ".", "append", "(", "b", ")", "b", "=", "None", "else", ":", "break", "if", "a", "is", "not", "None", "or", "b", "is", "not", "None", ":", "a", ".", "_merge_with_other_stm", "(", "b", ")", "tmp", ".", "append", "(", "a", ")", "a", "=", "None", "b", "=", "None", "return", "tmp"], "docstring": "Merge two lists of statements into one\n\n        :return: list of merged statements", "docstring_tokens": ["Merge", "two", "lists", "of", "statements", "into", "one"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/hdl/statements.py#L372-L423", "partition": "test"}
{"repo": "cyberdelia/metrology", "path": "metrology/reporter/statsd.py", "func_name": "StatsDReporter.format_metric_string", "original_string": "def format_metric_string(self, name, value, m_type):\n        \"\"\"Compose a statsd compatible string for a metric's measurement.\"\"\"\n\n        # NOTE(romcheg): This serialized metric template is based on\n        #                statsd's documentation.\n        template = '{name}:{value}|{m_type}\\n'\n\n        if self.prefix:\n            name = \"{prefix}.{m_name}\".format(prefix=self.prefix, m_name=name)\n\n        return template.format(name=name, value=value, m_type=m_type)", "language": "python", "code": "def format_metric_string(self, name, value, m_type):\n        \"\"\"Compose a statsd compatible string for a metric's measurement.\"\"\"\n\n        # NOTE(romcheg): This serialized metric template is based on\n        #                statsd's documentation.\n        template = '{name}:{value}|{m_type}\\n'\n\n        if self.prefix:\n            name = \"{prefix}.{m_name}\".format(prefix=self.prefix, m_name=name)\n\n        return template.format(name=name, value=value, m_type=m_type)", "code_tokens": ["def", "format_metric_string", "(", "self", ",", "name", ",", "value", ",", "m_type", ")", ":", "# NOTE(romcheg): This serialized metric template is based on", "#                statsd's documentation.", "template", "=", "'{name}:{value}|{m_type}\\n'", "if", "self", ".", "prefix", ":", "name", "=", "\"{prefix}.{m_name}\"", ".", "format", "(", "prefix", "=", "self", ".", "prefix", ",", "m_name", "=", "name", ")", "return", "template", ".", "format", "(", "name", "=", "name", ",", "value", "=", "value", ",", "m_type", "=", "m_type", ")"], "docstring": "Compose a statsd compatible string for a metric's measurement.", "docstring_tokens": ["Compose", "a", "statsd", "compatible", "string", "for", "a", "metric", "s", "measurement", "."], "sha": "7599bea7de1fd59374c06e2f8041a217e3cf9c01", "url": "https://github.com/cyberdelia/metrology/blob/7599bea7de1fd59374c06e2f8041a217e3cf9c01/metrology/reporter/statsd.py#L178-L188", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/math/linalg.py", "func_name": "pinv", "original_string": "def pinv(a, rcond=None, validate_args=False, name=None):\n  \"\"\"Compute the Moore-Penrose pseudo-inverse of a matrix.\n\n  Calculate the [generalized inverse of a matrix](\n  https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) using its\n  singular-value decomposition (SVD) and including all large singular values.\n\n  The pseudo-inverse of a matrix `A`, is defined as: \"the matrix that 'solves'\n  [the least-squares problem] `A @ x = b`,\" i.e., if `x_hat` is a solution, then\n  `A_pinv` is the matrix such that `x_hat = A_pinv @ b`. It can be shown that if\n  `U @ Sigma @ V.T = A` is the singular value decomposition of `A`, then\n  `A_pinv = V @ inv(Sigma) U^T`. [(Strang, 1980)][1]\n\n  This function is analogous to [`numpy.linalg.pinv`](\n  https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html).\n  It differs only in default value of `rcond`. In `numpy.linalg.pinv`, the\n  default `rcond` is `1e-15`. Here the default is\n  `10. * max(num_rows, num_cols) * np.finfo(dtype).eps`.\n\n  Args:\n    a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be\n      pseudo-inverted.\n    rcond: `Tensor` of small singular value cutoffs.  Singular values smaller\n      (in modulus) than `rcond` * largest_singular_value (again, in modulus) are\n      set to zero. Must broadcast against `tf.shape(a)[:-2]`.\n      Default value: `10. * max(num_rows, num_cols) * np.finfo(a.dtype).eps`.\n    validate_args: When `True`, additional assertions might be embedded in the\n      graph.\n      Default value: `False` (i.e., no graph assertions are added).\n    name: Python `str` prefixed to ops created by this function.\n      Default value: \"pinv\".\n\n  Returns:\n    a_pinv: The pseudo-inverse of input `a`. Has same shape as `a` except\n      rightmost two dimensions are transposed.\n\n  Raises:\n    TypeError: if input `a` does not have `float`-like `dtype`.\n    ValueError: if input `a` has fewer than 2 dimensions.\n\n  #### Examples\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  a = tf.constant([[1.,  0.4,  0.5],\n                   [0.4, 0.2,  0.25],\n                   [0.5, 0.25, 0.35]])\n  tf.matmul(tfp.math.pinv(a), a)\n  # ==> array([[1., 0., 0.],\n               [0., 1., 0.],\n               [0., 0., 1.]], dtype=float32)\n\n  a = tf.constant([[1.,  0.4,  0.5,  1.],\n                   [0.4, 0.2,  0.25, 2.],\n                   [0.5, 0.25, 0.35, 3.]])\n  tf.matmul(tfp.math.pinv(a), a)\n  # ==> array([[ 0.76,  0.37,  0.21, -0.02],\n               [ 0.37,  0.43, -0.33,  0.02],\n               [ 0.21, -0.33,  0.81,  0.01],\n               [-0.02,  0.02,  0.01,  1.  ]], dtype=float32)\n  ```\n\n  #### References\n\n  [1]: G. Strang. \"Linear Algebra and Its Applications, 2nd Ed.\" Academic Press,\n       Inc., 1980, pp. 139-142.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'pinv', [a, rcond]):\n    a = tf.convert_to_tensor(value=a, name='a')\n\n    assertions = _maybe_validate_matrix(a, validate_args)\n    if assertions:\n      with tf.control_dependencies(assertions):\n        a = tf.identity(a)\n\n    dtype = a.dtype.as_numpy_dtype\n\n    if rcond is None:\n      def get_dim_size(dim):\n        if tf.compat.dimension_value(a.shape[dim]) is not None:\n          return tf.compat.dimension_value(a.shape[dim])\n        return tf.shape(input=a)[dim]\n\n      num_rows = get_dim_size(-2)\n      num_cols = get_dim_size(-1)\n      if isinstance(num_rows, int) and isinstance(num_cols, int):\n        max_rows_cols = float(max(num_rows, num_cols))\n      else:\n        max_rows_cols = tf.cast(tf.maximum(num_rows, num_cols), dtype)\n      rcond = 10. * max_rows_cols * np.finfo(dtype).eps\n\n    rcond = tf.convert_to_tensor(value=rcond, dtype=dtype, name='rcond')\n\n    # Calculate pseudo inverse via SVD.\n    # Note: if a is symmetric then u == v. (We might observe additional\n    # performance by explicitly setting `v = u` in such cases.)\n    [\n        singular_values,         # Sigma\n        left_singular_vectors,   # U\n        right_singular_vectors,  # V\n    ] = tf.linalg.svd(a, full_matrices=False, compute_uv=True)\n\n    # Saturate small singular values to inf. This has the effect of make\n    # `1. / s = 0.` while not resulting in `NaN` gradients.\n    cutoff = rcond * tf.reduce_max(input_tensor=singular_values, axis=-1)\n    singular_values = tf.where(\n        singular_values > cutoff[..., tf.newaxis], singular_values,\n        tf.fill(tf.shape(input=singular_values), np.array(np.inf, dtype)))\n\n    # Although `a == tf.matmul(u, s * v, transpose_b=True)` we swap\n    # `u` and `v` here so that `tf.matmul(pinv(A), A) = tf.eye()`, i.e.,\n    # a matrix inverse has \"transposed\" semantics.\n    a_pinv = tf.matmul(\n        right_singular_vectors / singular_values[..., tf.newaxis, :],\n        left_singular_vectors,\n        adjoint_b=True)\n\n    if a.shape.ndims is not None:\n      a_pinv.set_shape(a.shape[:-2].concatenate([a.shape[-1], a.shape[-2]]))\n\n    return a_pinv", "language": "python", "code": "def pinv(a, rcond=None, validate_args=False, name=None):\n  \"\"\"Compute the Moore-Penrose pseudo-inverse of a matrix.\n\n  Calculate the [generalized inverse of a matrix](\n  https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) using its\n  singular-value decomposition (SVD) and including all large singular values.\n\n  The pseudo-inverse of a matrix `A`, is defined as: \"the matrix that 'solves'\n  [the least-squares problem] `A @ x = b`,\" i.e., if `x_hat` is a solution, then\n  `A_pinv` is the matrix such that `x_hat = A_pinv @ b`. It can be shown that if\n  `U @ Sigma @ V.T = A` is the singular value decomposition of `A`, then\n  `A_pinv = V @ inv(Sigma) U^T`. [(Strang, 1980)][1]\n\n  This function is analogous to [`numpy.linalg.pinv`](\n  https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html).\n  It differs only in default value of `rcond`. In `numpy.linalg.pinv`, the\n  default `rcond` is `1e-15`. Here the default is\n  `10. * max(num_rows, num_cols) * np.finfo(dtype).eps`.\n\n  Args:\n    a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be\n      pseudo-inverted.\n    rcond: `Tensor` of small singular value cutoffs.  Singular values smaller\n      (in modulus) than `rcond` * largest_singular_value (again, in modulus) are\n      set to zero. Must broadcast against `tf.shape(a)[:-2]`.\n      Default value: `10. * max(num_rows, num_cols) * np.finfo(a.dtype).eps`.\n    validate_args: When `True`, additional assertions might be embedded in the\n      graph.\n      Default value: `False` (i.e., no graph assertions are added).\n    name: Python `str` prefixed to ops created by this function.\n      Default value: \"pinv\".\n\n  Returns:\n    a_pinv: The pseudo-inverse of input `a`. Has same shape as `a` except\n      rightmost two dimensions are transposed.\n\n  Raises:\n    TypeError: if input `a` does not have `float`-like `dtype`.\n    ValueError: if input `a` has fewer than 2 dimensions.\n\n  #### Examples\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  a = tf.constant([[1.,  0.4,  0.5],\n                   [0.4, 0.2,  0.25],\n                   [0.5, 0.25, 0.35]])\n  tf.matmul(tfp.math.pinv(a), a)\n  # ==> array([[1., 0., 0.],\n               [0., 1., 0.],\n               [0., 0., 1.]], dtype=float32)\n\n  a = tf.constant([[1.,  0.4,  0.5,  1.],\n                   [0.4, 0.2,  0.25, 2.],\n                   [0.5, 0.25, 0.35, 3.]])\n  tf.matmul(tfp.math.pinv(a), a)\n  # ==> array([[ 0.76,  0.37,  0.21, -0.02],\n               [ 0.37,  0.43, -0.33,  0.02],\n               [ 0.21, -0.33,  0.81,  0.01],\n               [-0.02,  0.02,  0.01,  1.  ]], dtype=float32)\n  ```\n\n  #### References\n\n  [1]: G. Strang. \"Linear Algebra and Its Applications, 2nd Ed.\" Academic Press,\n       Inc., 1980, pp. 139-142.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'pinv', [a, rcond]):\n    a = tf.convert_to_tensor(value=a, name='a')\n\n    assertions = _maybe_validate_matrix(a, validate_args)\n    if assertions:\n      with tf.control_dependencies(assertions):\n        a = tf.identity(a)\n\n    dtype = a.dtype.as_numpy_dtype\n\n    if rcond is None:\n      def get_dim_size(dim):\n        if tf.compat.dimension_value(a.shape[dim]) is not None:\n          return tf.compat.dimension_value(a.shape[dim])\n        return tf.shape(input=a)[dim]\n\n      num_rows = get_dim_size(-2)\n      num_cols = get_dim_size(-1)\n      if isinstance(num_rows, int) and isinstance(num_cols, int):\n        max_rows_cols = float(max(num_rows, num_cols))\n      else:\n        max_rows_cols = tf.cast(tf.maximum(num_rows, num_cols), dtype)\n      rcond = 10. * max_rows_cols * np.finfo(dtype).eps\n\n    rcond = tf.convert_to_tensor(value=rcond, dtype=dtype, name='rcond')\n\n    # Calculate pseudo inverse via SVD.\n    # Note: if a is symmetric then u == v. (We might observe additional\n    # performance by explicitly setting `v = u` in such cases.)\n    [\n        singular_values,         # Sigma\n        left_singular_vectors,   # U\n        right_singular_vectors,  # V\n    ] = tf.linalg.svd(a, full_matrices=False, compute_uv=True)\n\n    # Saturate small singular values to inf. This has the effect of make\n    # `1. / s = 0.` while not resulting in `NaN` gradients.\n    cutoff = rcond * tf.reduce_max(input_tensor=singular_values, axis=-1)\n    singular_values = tf.where(\n        singular_values > cutoff[..., tf.newaxis], singular_values,\n        tf.fill(tf.shape(input=singular_values), np.array(np.inf, dtype)))\n\n    # Although `a == tf.matmul(u, s * v, transpose_b=True)` we swap\n    # `u` and `v` here so that `tf.matmul(pinv(A), A) = tf.eye()`, i.e.,\n    # a matrix inverse has \"transposed\" semantics.\n    a_pinv = tf.matmul(\n        right_singular_vectors / singular_values[..., tf.newaxis, :],\n        left_singular_vectors,\n        adjoint_b=True)\n\n    if a.shape.ndims is not None:\n      a_pinv.set_shape(a.shape[:-2].concatenate([a.shape[-1], a.shape[-2]]))\n\n    return a_pinv", "code_tokens": ["def", "pinv", "(", "a", ",", "rcond", "=", "None", ",", "validate_args", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'pinv'", ",", "[", "a", ",", "rcond", "]", ")", ":", "a", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "a", ",", "name", "=", "'a'", ")", "assertions", "=", "_maybe_validate_matrix", "(", "a", ",", "validate_args", ")", "if", "assertions", ":", "with", "tf", ".", "control_dependencies", "(", "assertions", ")", ":", "a", "=", "tf", ".", "identity", "(", "a", ")", "dtype", "=", "a", ".", "dtype", ".", "as_numpy_dtype", "if", "rcond", "is", "None", ":", "def", "get_dim_size", "(", "dim", ")", ":", "if", "tf", ".", "compat", ".", "dimension_value", "(", "a", ".", "shape", "[", "dim", "]", ")", "is", "not", "None", ":", "return", "tf", ".", "compat", ".", "dimension_value", "(", "a", ".", "shape", "[", "dim", "]", ")", "return", "tf", ".", "shape", "(", "input", "=", "a", ")", "[", "dim", "]", "num_rows", "=", "get_dim_size", "(", "-", "2", ")", "num_cols", "=", "get_dim_size", "(", "-", "1", ")", "if", "isinstance", "(", "num_rows", ",", "int", ")", "and", "isinstance", "(", "num_cols", ",", "int", ")", ":", "max_rows_cols", "=", "float", "(", "max", "(", "num_rows", ",", "num_cols", ")", ")", "else", ":", "max_rows_cols", "=", "tf", ".", "cast", "(", "tf", ".", "maximum", "(", "num_rows", ",", "num_cols", ")", ",", "dtype", ")", "rcond", "=", "10.", "*", "max_rows_cols", "*", "np", ".", "finfo", "(", "dtype", ")", ".", "eps", "rcond", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "rcond", ",", "dtype", "=", "dtype", ",", "name", "=", "'rcond'", ")", "# Calculate pseudo inverse via SVD.", "# Note: if a is symmetric then u == v. (We might observe additional", "# performance by explicitly setting `v = u` in such cases.)", "[", "singular_values", ",", "# Sigma", "left_singular_vectors", ",", "# U", "right_singular_vectors", ",", "# V", "]", "=", "tf", ".", "linalg", ".", "svd", "(", "a", ",", "full_matrices", "=", "False", ",", "compute_uv", "=", "True", ")", "# Saturate small singular values to inf. This has the effect of make", "# `1. / s = 0.` while not resulting in `NaN` gradients.", "cutoff", "=", "rcond", "*", "tf", ".", "reduce_max", "(", "input_tensor", "=", "singular_values", ",", "axis", "=", "-", "1", ")", "singular_values", "=", "tf", ".", "where", "(", "singular_values", ">", "cutoff", "[", "...", ",", "tf", ".", "newaxis", "]", ",", "singular_values", ",", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "input", "=", "singular_values", ")", ",", "np", ".", "array", "(", "np", ".", "inf", ",", "dtype", ")", ")", ")", "# Although `a == tf.matmul(u, s * v, transpose_b=True)` we swap", "# `u` and `v` here so that `tf.matmul(pinv(A), A) = tf.eye()`, i.e.,", "# a matrix inverse has \"transposed\" semantics.", "a_pinv", "=", "tf", ".", "matmul", "(", "right_singular_vectors", "/", "singular_values", "[", "...", ",", "tf", ".", "newaxis", ",", ":", "]", ",", "left_singular_vectors", ",", "adjoint_b", "=", "True", ")", "if", "a", ".", "shape", ".", "ndims", "is", "not", "None", ":", "a_pinv", ".", "set_shape", "(", "a", ".", "shape", "[", ":", "-", "2", "]", ".", "concatenate", "(", "[", "a", ".", "shape", "[", "-", "1", "]", ",", "a", ".", "shape", "[", "-", "2", "]", "]", ")", ")", "return", "a_pinv"], "docstring": "Compute the Moore-Penrose pseudo-inverse of a matrix.\n\n  Calculate the [generalized inverse of a matrix](\n  https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) using its\n  singular-value decomposition (SVD) and including all large singular values.\n\n  The pseudo-inverse of a matrix `A`, is defined as: \"the matrix that 'solves'\n  [the least-squares problem] `A @ x = b`,\" i.e., if `x_hat` is a solution, then\n  `A_pinv` is the matrix such that `x_hat = A_pinv @ b`. It can be shown that if\n  `U @ Sigma @ V.T = A` is the singular value decomposition of `A`, then\n  `A_pinv = V @ inv(Sigma) U^T`. [(Strang, 1980)][1]\n\n  This function is analogous to [`numpy.linalg.pinv`](\n  https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html).\n  It differs only in default value of `rcond`. In `numpy.linalg.pinv`, the\n  default `rcond` is `1e-15`. Here the default is\n  `10. * max(num_rows, num_cols) * np.finfo(dtype).eps`.\n\n  Args:\n    a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be\n      pseudo-inverted.\n    rcond: `Tensor` of small singular value cutoffs.  Singular values smaller\n      (in modulus) than `rcond` * largest_singular_value (again, in modulus) are\n      set to zero. Must broadcast against `tf.shape(a)[:-2]`.\n      Default value: `10. * max(num_rows, num_cols) * np.finfo(a.dtype).eps`.\n    validate_args: When `True`, additional assertions might be embedded in the\n      graph.\n      Default value: `False` (i.e., no graph assertions are added).\n    name: Python `str` prefixed to ops created by this function.\n      Default value: \"pinv\".\n\n  Returns:\n    a_pinv: The pseudo-inverse of input `a`. Has same shape as `a` except\n      rightmost two dimensions are transposed.\n\n  Raises:\n    TypeError: if input `a` does not have `float`-like `dtype`.\n    ValueError: if input `a` has fewer than 2 dimensions.\n\n  #### Examples\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  a = tf.constant([[1.,  0.4,  0.5],\n                   [0.4, 0.2,  0.25],\n                   [0.5, 0.25, 0.35]])\n  tf.matmul(tfp.math.pinv(a), a)\n  # ==> array([[1., 0., 0.],\n               [0., 1., 0.],\n               [0., 0., 1.]], dtype=float32)\n\n  a = tf.constant([[1.,  0.4,  0.5,  1.],\n                   [0.4, 0.2,  0.25, 2.],\n                   [0.5, 0.25, 0.35, 3.]])\n  tf.matmul(tfp.math.pinv(a), a)\n  # ==> array([[ 0.76,  0.37,  0.21, -0.02],\n               [ 0.37,  0.43, -0.33,  0.02],\n               [ 0.21, -0.33,  0.81,  0.01],\n               [-0.02,  0.02,  0.01,  1.  ]], dtype=float32)\n  ```\n\n  #### References\n\n  [1]: G. Strang. \"Linear Algebra and Its Applications, 2nd Ed.\" Academic Press,\n       Inc., 1980, pp. 139-142.", "docstring_tokens": ["Compute", "the", "Moore", "-", "Penrose", "pseudo", "-", "inverse", "of", "a", "matrix", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/math/linalg.py#L323-L445", "partition": "test"}
{"repo": "snowplow/snowplow-python-analytics-sdk", "path": "snowplow_analytics_sdk/json_shredder.py", "func_name": "parse_unstruct", "original_string": "def parse_unstruct(unstruct):\n    \"\"\"\n    Convert an unstructured event JSON to a list containing one Elasticsearch-compatible key-value pair\n    For example, the JSON\n\n    {\n      \"data\": {\n        \"data\": {\n          \"key\": \"value\"\n        },\n        \"schema\": \"iglu:com.snowplowanalytics.snowplow/link_click/jsonschema/1-0-1\"\n      },\n      \"schema\": \"iglu:com.snowplowanalytics.snowplow/unstruct_event/jsonschema/1-0-0\"\n    }\n\n    would become\n\n    [\n      (\n        \"unstruct_com_snowplowanalytics_snowplow_link_click_1\", {\n          \"key\": \"value\"\n        }\n      )\n    ]\n    \"\"\"\n    my_json = json.loads(unstruct)\n    data = my_json['data']\n    schema = data['schema']\n    if 'data' in data:\n        inner_data = data['data']\n    else:\n        raise SnowplowEventTransformationException([\"Could not extract inner data field from unstructured event\"])\n    fixed_schema = fix_schema(\"unstruct_event\", schema)\n    return [(fixed_schema, inner_data)]", "language": "python", "code": "def parse_unstruct(unstruct):\n    \"\"\"\n    Convert an unstructured event JSON to a list containing one Elasticsearch-compatible key-value pair\n    For example, the JSON\n\n    {\n      \"data\": {\n        \"data\": {\n          \"key\": \"value\"\n        },\n        \"schema\": \"iglu:com.snowplowanalytics.snowplow/link_click/jsonschema/1-0-1\"\n      },\n      \"schema\": \"iglu:com.snowplowanalytics.snowplow/unstruct_event/jsonschema/1-0-0\"\n    }\n\n    would become\n\n    [\n      (\n        \"unstruct_com_snowplowanalytics_snowplow_link_click_1\", {\n          \"key\": \"value\"\n        }\n      )\n    ]\n    \"\"\"\n    my_json = json.loads(unstruct)\n    data = my_json['data']\n    schema = data['schema']\n    if 'data' in data:\n        inner_data = data['data']\n    else:\n        raise SnowplowEventTransformationException([\"Could not extract inner data field from unstructured event\"])\n    fixed_schema = fix_schema(\"unstruct_event\", schema)\n    return [(fixed_schema, inner_data)]", "code_tokens": ["def", "parse_unstruct", "(", "unstruct", ")", ":", "my_json", "=", "json", ".", "loads", "(", "unstruct", ")", "data", "=", "my_json", "[", "'data'", "]", "schema", "=", "data", "[", "'schema'", "]", "if", "'data'", "in", "data", ":", "inner_data", "=", "data", "[", "'data'", "]", "else", ":", "raise", "SnowplowEventTransformationException", "(", "[", "\"Could not extract inner data field from unstructured event\"", "]", ")", "fixed_schema", "=", "fix_schema", "(", "\"unstruct_event\"", ",", "schema", ")", "return", "[", "(", "fixed_schema", ",", "inner_data", ")", "]"], "docstring": "Convert an unstructured event JSON to a list containing one Elasticsearch-compatible key-value pair\n    For example, the JSON\n\n    {\n      \"data\": {\n        \"data\": {\n          \"key\": \"value\"\n        },\n        \"schema\": \"iglu:com.snowplowanalytics.snowplow/link_click/jsonschema/1-0-1\"\n      },\n      \"schema\": \"iglu:com.snowplowanalytics.snowplow/unstruct_event/jsonschema/1-0-0\"\n    }\n\n    would become\n\n    [\n      (\n        \"unstruct_com_snowplowanalytics_snowplow_link_click_1\", {\n          \"key\": \"value\"\n        }\n      )\n    ]", "docstring_tokens": ["Convert", "an", "unstructured", "event", "JSON", "to", "a", "list", "containing", "one", "Elasticsearch", "-", "compatible", "key", "-", "value", "pair", "For", "example", "the", "JSON"], "sha": "0ddca91e3f6d8bed88627fa557790aa4868bdace", "url": "https://github.com/snowplow/snowplow-python-analytics-sdk/blob/0ddca91e3f6d8bed88627fa557790aa4868bdace/snowplow_analytics_sdk/json_shredder.py#L122-L155", "partition": "test"}
{"repo": "google/brotli", "path": "research/brotlidump.py", "func_name": "Layout.processStream", "original_string": "def processStream(self):\n        \"\"\"Process a brotli stream.\n        \"\"\"\n        print('addr  hex{:{}s}binary context explanation'.format(\n            '', self.width-10))\n        print('Stream header'.center(60, '-'))\n        self.windowSize = self.verboseRead(WindowSizeAlphabet())\n        print('Metablock header'.center(60, '='))\n        self.ISLAST = False\n        self.output = bytearray()\n        while not self.ISLAST:\n            self.ISLAST = self.verboseRead(\n                BoolCode('LAST', description=\"Last block\"))\n            if self.ISLAST:\n                if self.verboseRead(\n                    BoolCode('EMPTY', description=\"Empty block\")): break\n            if self.metablockLength(): continue\n            if not self.ISLAST and self.uncompressed(): continue\n            print('Block type descriptors'.center(60, '-'))\n            self.numberOfBlockTypes = {}\n            self.currentBlockCounts = {}\n            self.blockTypeCodes = {}\n            self.blockCountCodes = {}\n            for blockType in (L,I,D): self.blockType(blockType)\n            print('Distance code parameters'.center(60, '-'))\n            self.NPOSTFIX, self.NDIRECT = self.verboseRead(DistanceParamAlphabet())\n            self.readLiteralContextModes()\n            print('Context maps'.center(60, '-'))\n            self.cmaps = {}\n            #keep the number of each kind of prefix tree for the last loop\n            numberOfTrees = {I: self.numberOfBlockTypes[I]}\n            for blockType in (L,D):\n                numberOfTrees[blockType] = self.contextMap(blockType)\n            print('Prefix code lists'.center(60, '-'))\n            self.prefixCodes = {}\n            for blockType in (L,I,D):\n                self.readPrefixArray(blockType, numberOfTrees[blockType])\n            self.metablock()", "language": "python", "code": "def processStream(self):\n        \"\"\"Process a brotli stream.\n        \"\"\"\n        print('addr  hex{:{}s}binary context explanation'.format(\n            '', self.width-10))\n        print('Stream header'.center(60, '-'))\n        self.windowSize = self.verboseRead(WindowSizeAlphabet())\n        print('Metablock header'.center(60, '='))\n        self.ISLAST = False\n        self.output = bytearray()\n        while not self.ISLAST:\n            self.ISLAST = self.verboseRead(\n                BoolCode('LAST', description=\"Last block\"))\n            if self.ISLAST:\n                if self.verboseRead(\n                    BoolCode('EMPTY', description=\"Empty block\")): break\n            if self.metablockLength(): continue\n            if not self.ISLAST and self.uncompressed(): continue\n            print('Block type descriptors'.center(60, '-'))\n            self.numberOfBlockTypes = {}\n            self.currentBlockCounts = {}\n            self.blockTypeCodes = {}\n            self.blockCountCodes = {}\n            for blockType in (L,I,D): self.blockType(blockType)\n            print('Distance code parameters'.center(60, '-'))\n            self.NPOSTFIX, self.NDIRECT = self.verboseRead(DistanceParamAlphabet())\n            self.readLiteralContextModes()\n            print('Context maps'.center(60, '-'))\n            self.cmaps = {}\n            #keep the number of each kind of prefix tree for the last loop\n            numberOfTrees = {I: self.numberOfBlockTypes[I]}\n            for blockType in (L,D):\n                numberOfTrees[blockType] = self.contextMap(blockType)\n            print('Prefix code lists'.center(60, '-'))\n            self.prefixCodes = {}\n            for blockType in (L,I,D):\n                self.readPrefixArray(blockType, numberOfTrees[blockType])\n            self.metablock()", "code_tokens": ["def", "processStream", "(", "self", ")", ":", "print", "(", "'addr  hex{:{}s}binary context explanation'", ".", "format", "(", "''", ",", "self", ".", "width", "-", "10", ")", ")", "print", "(", "'Stream header'", ".", "center", "(", "60", ",", "'-'", ")", ")", "self", ".", "windowSize", "=", "self", ".", "verboseRead", "(", "WindowSizeAlphabet", "(", ")", ")", "print", "(", "'Metablock header'", ".", "center", "(", "60", ",", "'='", ")", ")", "self", ".", "ISLAST", "=", "False", "self", ".", "output", "=", "bytearray", "(", ")", "while", "not", "self", ".", "ISLAST", ":", "self", ".", "ISLAST", "=", "self", ".", "verboseRead", "(", "BoolCode", "(", "'LAST'", ",", "description", "=", "\"Last block\"", ")", ")", "if", "self", ".", "ISLAST", ":", "if", "self", ".", "verboseRead", "(", "BoolCode", "(", "'EMPTY'", ",", "description", "=", "\"Empty block\"", ")", ")", ":", "break", "if", "self", ".", "metablockLength", "(", ")", ":", "continue", "if", "not", "self", ".", "ISLAST", "and", "self", ".", "uncompressed", "(", ")", ":", "continue", "print", "(", "'Block type descriptors'", ".", "center", "(", "60", ",", "'-'", ")", ")", "self", ".", "numberOfBlockTypes", "=", "{", "}", "self", ".", "currentBlockCounts", "=", "{", "}", "self", ".", "blockTypeCodes", "=", "{", "}", "self", ".", "blockCountCodes", "=", "{", "}", "for", "blockType", "in", "(", "L", ",", "I", ",", "D", ")", ":", "self", ".", "blockType", "(", "blockType", ")", "print", "(", "'Distance code parameters'", ".", "center", "(", "60", ",", "'-'", ")", ")", "self", ".", "NPOSTFIX", ",", "self", ".", "NDIRECT", "=", "self", ".", "verboseRead", "(", "DistanceParamAlphabet", "(", ")", ")", "self", ".", "readLiteralContextModes", "(", ")", "print", "(", "'Context maps'", ".", "center", "(", "60", ",", "'-'", ")", ")", "self", ".", "cmaps", "=", "{", "}", "#keep the number of each kind of prefix tree for the last loop", "numberOfTrees", "=", "{", "I", ":", "self", ".", "numberOfBlockTypes", "[", "I", "]", "}", "for", "blockType", "in", "(", "L", ",", "D", ")", ":", "numberOfTrees", "[", "blockType", "]", "=", "self", ".", "contextMap", "(", "blockType", ")", "print", "(", "'Prefix code lists'", ".", "center", "(", "60", ",", "'-'", ")", ")", "self", ".", "prefixCodes", "=", "{", "}", "for", "blockType", "in", "(", "L", ",", "I", ",", "D", ")", ":", "self", ".", "readPrefixArray", "(", "blockType", ",", "numberOfTrees", "[", "blockType", "]", ")", "self", ".", "metablock", "(", ")"], "docstring": "Process a brotli stream.", "docstring_tokens": ["Process", "a", "brotli", "stream", "."], "sha": "4b2b2d4f83ffeaac7708e44409fe34896a01a278", "url": "https://github.com/google/brotli/blob/4b2b2d4f83ffeaac7708e44409fe34896a01a278/research/brotlidump.py#L1578-L1615", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/reporters/text.py", "func_name": "register", "original_string": "def register(linter):\n    \"\"\"Register the reporter classes with the linter.\"\"\"\n    linter.register_reporter(TextReporter)\n    linter.register_reporter(ParseableTextReporter)\n    linter.register_reporter(VSTextReporter)\n    linter.register_reporter(ColorizedTextReporter)", "language": "python", "code": "def register(linter):\n    \"\"\"Register the reporter classes with the linter.\"\"\"\n    linter.register_reporter(TextReporter)\n    linter.register_reporter(ParseableTextReporter)\n    linter.register_reporter(VSTextReporter)\n    linter.register_reporter(ColorizedTextReporter)", "code_tokens": ["def", "register", "(", "linter", ")", ":", "linter", ".", "register_reporter", "(", "TextReporter", ")", "linter", ".", "register_reporter", "(", "ParseableTextReporter", ")", "linter", ".", "register_reporter", "(", "VSTextReporter", ")", "linter", ".", "register_reporter", "(", "ColorizedTextReporter", ")"], "docstring": "Register the reporter classes with the linter.", "docstring_tokens": ["Register", "the", "reporter", "classes", "with", "the", "linter", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/reporters/text.py#L244-L249", "partition": "test"}
{"repo": "cthoyt/ols-client", "path": "src/ols_client/constants.py", "func_name": "get_config", "original_string": "def get_config():\n    \"\"\"Gets the configuration for this project from the default JSON file, or writes one if it doesn't exist\n\n    :rtype: dict\n    \"\"\"\n    if not os.path.exists(CONFIG_PATH):\n        write_config({})\n\n    with open(CONFIG_PATH) as f:\n        return json.load(f)", "language": "python", "code": "def get_config():\n    \"\"\"Gets the configuration for this project from the default JSON file, or writes one if it doesn't exist\n\n    :rtype: dict\n    \"\"\"\n    if not os.path.exists(CONFIG_PATH):\n        write_config({})\n\n    with open(CONFIG_PATH) as f:\n        return json.load(f)", "code_tokens": ["def", "get_config", "(", ")", ":", "if", "not", "os", ".", "path", ".", "exists", "(", "CONFIG_PATH", ")", ":", "write_config", "(", "{", "}", ")", "with", "open", "(", "CONFIG_PATH", ")", "as", "f", ":", "return", "json", ".", "load", "(", "f", ")"], "docstring": "Gets the configuration for this project from the default JSON file, or writes one if it doesn't exist\n\n    :rtype: dict", "docstring_tokens": ["Gets", "the", "configuration", "for", "this", "project", "from", "the", "default", "JSON", "file", "or", "writes", "one", "if", "it", "doesn", "t", "exist"], "sha": "8c6bb54888675652d25324184967392d00d128fc", "url": "https://github.com/cthoyt/ols-client/blob/8c6bb54888675652d25324184967392d00d128fc/src/ols_client/constants.py#L27-L36", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/nest_util.py", "func_name": "_nested_convert_to_tensor", "original_string": "def _nested_convert_to_tensor(struct, dtype=None, name=None):\n  \"\"\"Eagerly converts struct to Tensor, recursing upon failure.\"\"\"\n  if dtype is not None or not tf.nest.is_nested(struct):\n    return tf.convert_to_tensor(struct, dtype=dtype)\n\n  if _maybe_convertible_to_tensor(struct):\n    try:\n      # Try converting the structure wholesale.\n      return tf.convert_to_tensor(value=struct, name=name)\n    except (ValueError, TypeError):\n      # Unfortunately Eager/Graph mode don't agree on the error type.\n      pass\n  # Try converting all of its children.\n  shallow_struct = _get_shallow_structure(struct)\n  return nest.map_structure_up_to(\n      shallow_struct, lambda s: _nested_convert_to_tensor(s, name=name), struct)", "language": "python", "code": "def _nested_convert_to_tensor(struct, dtype=None, name=None):\n  \"\"\"Eagerly converts struct to Tensor, recursing upon failure.\"\"\"\n  if dtype is not None or not tf.nest.is_nested(struct):\n    return tf.convert_to_tensor(struct, dtype=dtype)\n\n  if _maybe_convertible_to_tensor(struct):\n    try:\n      # Try converting the structure wholesale.\n      return tf.convert_to_tensor(value=struct, name=name)\n    except (ValueError, TypeError):\n      # Unfortunately Eager/Graph mode don't agree on the error type.\n      pass\n  # Try converting all of its children.\n  shallow_struct = _get_shallow_structure(struct)\n  return nest.map_structure_up_to(\n      shallow_struct, lambda s: _nested_convert_to_tensor(s, name=name), struct)", "code_tokens": ["def", "_nested_convert_to_tensor", "(", "struct", ",", "dtype", "=", "None", ",", "name", "=", "None", ")", ":", "if", "dtype", "is", "not", "None", "or", "not", "tf", ".", "nest", ".", "is_nested", "(", "struct", ")", ":", "return", "tf", ".", "convert_to_tensor", "(", "struct", ",", "dtype", "=", "dtype", ")", "if", "_maybe_convertible_to_tensor", "(", "struct", ")", ":", "try", ":", "# Try converting the structure wholesale.", "return", "tf", ".", "convert_to_tensor", "(", "value", "=", "struct", ",", "name", "=", "name", ")", "except", "(", "ValueError", ",", "TypeError", ")", ":", "# Unfortunately Eager/Graph mode don't agree on the error type.", "pass", "# Try converting all of its children.", "shallow_struct", "=", "_get_shallow_structure", "(", "struct", ")", "return", "nest", ".", "map_structure_up_to", "(", "shallow_struct", ",", "lambda", "s", ":", "_nested_convert_to_tensor", "(", "s", ",", "name", "=", "name", ")", ",", "struct", ")"], "docstring": "Eagerly converts struct to Tensor, recursing upon failure.", "docstring_tokens": ["Eagerly", "converts", "struct", "to", "Tensor", "recursing", "upon", "failure", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/nest_util.py#L98-L113", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/message/message_handler_mix_in.py", "func_name": "MessagesHandlerMixIn._message_symbol", "original_string": "def _message_symbol(self, msgid):\n        \"\"\"Get the message symbol of the given message id\n\n        Return the original message id if the message does not\n        exist.\n        \"\"\"\n        try:\n            return [md.symbol for md in self.msgs_store.get_message_definitions(msgid)]\n        except UnknownMessageError:\n            return msgid", "language": "python", "code": "def _message_symbol(self, msgid):\n        \"\"\"Get the message symbol of the given message id\n\n        Return the original message id if the message does not\n        exist.\n        \"\"\"\n        try:\n            return [md.symbol for md in self.msgs_store.get_message_definitions(msgid)]\n        except UnknownMessageError:\n            return msgid", "code_tokens": ["def", "_message_symbol", "(", "self", ",", "msgid", ")", ":", "try", ":", "return", "[", "md", ".", "symbol", "for", "md", "in", "self", ".", "msgs_store", ".", "get_message_definitions", "(", "msgid", ")", "]", "except", "UnknownMessageError", ":", "return", "msgid"], "docstring": "Get the message symbol of the given message id\n\n        Return the original message id if the message does not\n        exist.", "docstring_tokens": ["Get", "the", "message", "symbol", "of", "the", "given", "message", "id"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/message/message_handler_mix_in.py#L176-L185", "partition": "test"}
{"repo": "assemblerflow/flowcraft", "path": "flowcraft/templates/trimmomatic.py", "func_name": "clean_up", "original_string": "def clean_up(fastq_pairs, clear):\n    \"\"\"Cleans the working directory of unwanted temporary files\"\"\"\n\n    # Find unpaired fastq files\n    unpaired_fastq = [f for f in os.listdir(\".\")\n                      if f.endswith(\"_U.fastq.gz\")]\n\n    # Remove unpaired fastq files, if any\n    for fpath in unpaired_fastq:\n        os.remove(fpath)\n\n    # Expected output to assess whether it is safe to remove temporary input\n    expected_out = [f for f in os.listdir(\".\") if f.endswith(\"_trim.fastq.gz\")]\n\n    if clear == \"true\" and len(expected_out) == 2:\n        for fq in fastq_pairs:\n            # Get real path of fastq files, following symlinks\n            rp = os.path.realpath(fq)\n            logger.debug(\"Removing temporary fastq file path: {}\".format(rp))\n            if re.match(\".*/work/.{2}/.{30}/.*\", rp):\n                os.remove(rp)", "language": "python", "code": "def clean_up(fastq_pairs, clear):\n    \"\"\"Cleans the working directory of unwanted temporary files\"\"\"\n\n    # Find unpaired fastq files\n    unpaired_fastq = [f for f in os.listdir(\".\")\n                      if f.endswith(\"_U.fastq.gz\")]\n\n    # Remove unpaired fastq files, if any\n    for fpath in unpaired_fastq:\n        os.remove(fpath)\n\n    # Expected output to assess whether it is safe to remove temporary input\n    expected_out = [f for f in os.listdir(\".\") if f.endswith(\"_trim.fastq.gz\")]\n\n    if clear == \"true\" and len(expected_out) == 2:\n        for fq in fastq_pairs:\n            # Get real path of fastq files, following symlinks\n            rp = os.path.realpath(fq)\n            logger.debug(\"Removing temporary fastq file path: {}\".format(rp))\n            if re.match(\".*/work/.{2}/.{30}/.*\", rp):\n                os.remove(rp)", "code_tokens": ["def", "clean_up", "(", "fastq_pairs", ",", "clear", ")", ":", "# Find unpaired fastq files", "unpaired_fastq", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "\".\"", ")", "if", "f", ".", "endswith", "(", "\"_U.fastq.gz\"", ")", "]", "# Remove unpaired fastq files, if any", "for", "fpath", "in", "unpaired_fastq", ":", "os", ".", "remove", "(", "fpath", ")", "# Expected output to assess whether it is safe to remove temporary input", "expected_out", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "\".\"", ")", "if", "f", ".", "endswith", "(", "\"_trim.fastq.gz\"", ")", "]", "if", "clear", "==", "\"true\"", "and", "len", "(", "expected_out", ")", "==", "2", ":", "for", "fq", "in", "fastq_pairs", ":", "# Get real path of fastq files, following symlinks", "rp", "=", "os", ".", "path", ".", "realpath", "(", "fq", ")", "logger", ".", "debug", "(", "\"Removing temporary fastq file path: {}\"", ".", "format", "(", "rp", ")", ")", "if", "re", ".", "match", "(", "\".*/work/.{2}/.{30}/.*\"", ",", "rp", ")", ":", "os", ".", "remove", "(", "rp", ")"], "docstring": "Cleans the working directory of unwanted temporary files", "docstring_tokens": ["Cleans", "the", "working", "directory", "of", "unwanted", "temporary", "files"], "sha": "fc3f4bddded1efc76006600016dc71a06dd908c0", "url": "https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/templates/trimmomatic.py#L242-L262", "partition": "test"}
{"repo": "nsqio/pynsq", "path": "nsq/event.py", "func_name": "EventedMixin.on", "original_string": "def on(self, name, callback):\n        \"\"\"\n        Listen for the named event with the specified callback.\n\n        :param name: the name of the event\n        :type name: string\n\n        :param callback: the callback to execute when the event is triggered\n        :type callback: callable\n        \"\"\"\n        assert callable(callback), 'callback is not callable'\n        if callback in self.__listeners[name]:\n            raise DuplicateListenerError\n        self.__listeners[name].append(callback)", "language": "python", "code": "def on(self, name, callback):\n        \"\"\"\n        Listen for the named event with the specified callback.\n\n        :param name: the name of the event\n        :type name: string\n\n        :param callback: the callback to execute when the event is triggered\n        :type callback: callable\n        \"\"\"\n        assert callable(callback), 'callback is not callable'\n        if callback in self.__listeners[name]:\n            raise DuplicateListenerError\n        self.__listeners[name].append(callback)", "code_tokens": ["def", "on", "(", "self", ",", "name", ",", "callback", ")", ":", "assert", "callable", "(", "callback", ")", ",", "'callback is not callable'", "if", "callback", "in", "self", ".", "__listeners", "[", "name", "]", ":", "raise", "DuplicateListenerError", "self", ".", "__listeners", "[", "name", "]", ".", "append", "(", "callback", ")"], "docstring": "Listen for the named event with the specified callback.\n\n        :param name: the name of the event\n        :type name: string\n\n        :param callback: the callback to execute when the event is triggered\n        :type callback: callable", "docstring_tokens": ["Listen", "for", "the", "named", "event", "with", "the", "specified", "callback", "."], "sha": "48bf62d65ea63cddaa401efb23187b95511dbc84", "url": "https://github.com/nsqio/pynsq/blob/48bf62d65ea63cddaa401efb23187b95511dbc84/nsq/event.py#L44-L57", "partition": "test"}
{"repo": "rocky/python3-trepan", "path": "trepan/lib/term_background.py", "func_name": "is_dark_rgb", "original_string": "def is_dark_rgb(r, g, b):\n    \"\"\"Pass as parameters R G B values in hex\n    On return, variable is_dark_bg is set\n    \"\"\"\n\n    try:\n        midpoint = int(environ.get('TERMINAL_COLOR_MIDPOINT', None))\n    except:\n        pass\n    if not midpoint:\n        term = environ.get('TERM', None)\n        # 117963 = (* .6 (+ 65535 65535 65535))\n        # 382.5 = (* .6 (+ 65535 65535 65535))\n        print(\"midpoint\", midpoint, 'vs',  (16*5 + 16*g + 16*b))\n        midpoint = 383 if term and term == 'xterm-256color' else 117963\n\n    if ( (16*5 + 16*g + 16*b) < midpoint ):\n        return True\n    else:\n        return False", "language": "python", "code": "def is_dark_rgb(r, g, b):\n    \"\"\"Pass as parameters R G B values in hex\n    On return, variable is_dark_bg is set\n    \"\"\"\n\n    try:\n        midpoint = int(environ.get('TERMINAL_COLOR_MIDPOINT', None))\n    except:\n        pass\n    if not midpoint:\n        term = environ.get('TERM', None)\n        # 117963 = (* .6 (+ 65535 65535 65535))\n        # 382.5 = (* .6 (+ 65535 65535 65535))\n        print(\"midpoint\", midpoint, 'vs',  (16*5 + 16*g + 16*b))\n        midpoint = 383 if term and term == 'xterm-256color' else 117963\n\n    if ( (16*5 + 16*g + 16*b) < midpoint ):\n        return True\n    else:\n        return False", "code_tokens": ["def", "is_dark_rgb", "(", "r", ",", "g", ",", "b", ")", ":", "try", ":", "midpoint", "=", "int", "(", "environ", ".", "get", "(", "'TERMINAL_COLOR_MIDPOINT'", ",", "None", ")", ")", "except", ":", "pass", "if", "not", "midpoint", ":", "term", "=", "environ", ".", "get", "(", "'TERM'", ",", "None", ")", "# 117963 = (* .6 (+ 65535 65535 65535))", "# 382.5 = (* .6 (+ 65535 65535 65535))", "print", "(", "\"midpoint\"", ",", "midpoint", ",", "'vs'", ",", "(", "16", "*", "5", "+", "16", "*", "g", "+", "16", "*", "b", ")", ")", "midpoint", "=", "383", "if", "term", "and", "term", "==", "'xterm-256color'", "else", "117963", "if", "(", "(", "16", "*", "5", "+", "16", "*", "g", "+", "16", "*", "b", ")", "<", "midpoint", ")", ":", "return", "True", "else", ":", "return", "False"], "docstring": "Pass as parameters R G B values in hex\n    On return, variable is_dark_bg is set", "docstring_tokens": ["Pass", "as", "parameters", "R", "G", "B", "values", "in", "hex", "On", "return", "variable", "is_dark_bg", "is", "set"], "sha": "14e91bc0acce090d67be145b1ac040cab92ac5f3", "url": "https://github.com/rocky/python3-trepan/blob/14e91bc0acce090d67be145b1ac040cab92ac5f3/trepan/lib/term_background.py#L40-L59", "partition": "test"}
{"repo": "tomMoral/loky", "path": "loky/backend/semlock.py", "func_name": "_sem_open", "original_string": "def _sem_open(name, value=None):\n    \"\"\" Construct or retrieve a semaphore with the given name\n\n    If value is None, try to retrieve an existing named semaphore.\n    Else create a new semaphore with the given value\n    \"\"\"\n    if value is None:\n        handle = pthread.sem_open(ctypes.c_char_p(name), 0)\n    else:\n        handle = pthread.sem_open(ctypes.c_char_p(name), SEM_OFLAG, SEM_PERM,\n                                  ctypes.c_int(value))\n\n    if handle == SEM_FAILURE:\n        e = ctypes.get_errno()\n        if e == errno.EEXIST:\n            raise FileExistsError(\"a semaphore named %s already exists\" % name)\n        elif e == errno.ENOENT:\n            raise FileNotFoundError('cannot find semaphore named %s' % name)\n        elif e == errno.ENOSYS:\n            raise NotImplementedError('No semaphore implementation on this '\n                                      'system')\n        else:\n            raiseFromErrno()\n\n    return handle", "language": "python", "code": "def _sem_open(name, value=None):\n    \"\"\" Construct or retrieve a semaphore with the given name\n\n    If value is None, try to retrieve an existing named semaphore.\n    Else create a new semaphore with the given value\n    \"\"\"\n    if value is None:\n        handle = pthread.sem_open(ctypes.c_char_p(name), 0)\n    else:\n        handle = pthread.sem_open(ctypes.c_char_p(name), SEM_OFLAG, SEM_PERM,\n                                  ctypes.c_int(value))\n\n    if handle == SEM_FAILURE:\n        e = ctypes.get_errno()\n        if e == errno.EEXIST:\n            raise FileExistsError(\"a semaphore named %s already exists\" % name)\n        elif e == errno.ENOENT:\n            raise FileNotFoundError('cannot find semaphore named %s' % name)\n        elif e == errno.ENOSYS:\n            raise NotImplementedError('No semaphore implementation on this '\n                                      'system')\n        else:\n            raiseFromErrno()\n\n    return handle", "code_tokens": ["def", "_sem_open", "(", "name", ",", "value", "=", "None", ")", ":", "if", "value", "is", "None", ":", "handle", "=", "pthread", ".", "sem_open", "(", "ctypes", ".", "c_char_p", "(", "name", ")", ",", "0", ")", "else", ":", "handle", "=", "pthread", ".", "sem_open", "(", "ctypes", ".", "c_char_p", "(", "name", ")", ",", "SEM_OFLAG", ",", "SEM_PERM", ",", "ctypes", ".", "c_int", "(", "value", ")", ")", "if", "handle", "==", "SEM_FAILURE", ":", "e", "=", "ctypes", ".", "get_errno", "(", ")", "if", "e", "==", "errno", ".", "EEXIST", ":", "raise", "FileExistsError", "(", "\"a semaphore named %s already exists\"", "%", "name", ")", "elif", "e", "==", "errno", ".", "ENOENT", ":", "raise", "FileNotFoundError", "(", "'cannot find semaphore named %s'", "%", "name", ")", "elif", "e", "==", "errno", ".", "ENOSYS", ":", "raise", "NotImplementedError", "(", "'No semaphore implementation on this '", "'system'", ")", "else", ":", "raiseFromErrno", "(", ")", "return", "handle"], "docstring": "Construct or retrieve a semaphore with the given name\n\n    If value is None, try to retrieve an existing named semaphore.\n    Else create a new semaphore with the given value", "docstring_tokens": ["Construct", "or", "retrieve", "a", "semaphore", "with", "the", "given", "name"], "sha": "dc2d941d8285a96f3a5b666a4bd04875b0b25984", "url": "https://github.com/tomMoral/loky/blob/dc2d941d8285a96f3a5b666a4bd04875b0b25984/loky/backend/semlock.py#L75-L99", "partition": "test"}
{"repo": "funilrys/PyFunceble", "path": "PyFunceble/lookup.py", "func_name": "Lookup.whois", "original_string": "def whois(cls, whois_server, domain=None, timeout=None):  # pragma: no cover\n        \"\"\"\n        Implementation of UNIX whois.\n\n        :param whois_server: The WHOIS server to use to get the record.\n        :type whois_server: str\n\n        :param domain: The domain to get the whois record from.\n        :type domain: str\n\n        :param timeout: The timeout to apply to the request.\n        :type timeout: int\n\n        :return: The whois record from the given whois server, if exist.\n        :rtype: str|None\n        \"\"\"\n\n        if domain is None:\n            # The domain is not given (localy).\n\n            # We consider the domain as the domain or IP we are currently testing.\n            domain = PyFunceble.INTERN[\"to_test\"]\n\n        if timeout is None:\n            # The time is not given (localy).\n\n            # We consider the timeout from the configuration as the timeout to use.\n            timeout = PyFunceble.CONFIGURATION[\"seconds_before_http_timeout\"]\n\n        if whois_server:\n            # A whois server is given.\n\n            # We initiate a PyFunceble.socket.\n            req = PyFunceble.socket.socket(\n                PyFunceble.socket.AF_INET, PyFunceble.socket.SOCK_STREAM\n            )\n\n            if timeout % 3 == 0:\n                # The timeout is modulo 3.\n\n                # We report the timeout to our initiated PyFunceble.socket.\n                req.settimeout(timeout)\n            else:\n                # The timeout is not modulo 3.\n\n                # We report 3 seconds as the timeout to our initiated PyFunceble.socket.\n                req.settimeout(3)\n\n            try:\n                # We try to connect to the whois server at the port 43.\n                req.connect((whois_server, 43))\n            except PyFunceble.socket.error:\n                # We got an error.\n\n                # We return None.\n                return None\n\n            # We send end encode the domain we want the data from.\n            req.send((domain + \"\\r\\n\").encode())\n\n            # We initiate a bytes variable which will save the response\n            # from the server.\n            response = b\"\"\n\n            while True:\n                # We loop infinitly.\n                try:\n                    # We try to receive the data in a buffer of 4096 bytes.\n                    data = req.recv(4096)\n                except (PyFunceble.socket.timeout, ConnectionResetError):\n                    # We got an error.\n\n                    # We close the connection.\n                    req.close()\n\n                    # And we return None.\n                    return None\n\n                # Everything goes right.\n\n                # We append data to the response we got.\n                response += data\n\n                if not data:\n                    # The data is empty.\n\n                    # We break the loop.\n                    break\n\n            # We close the connection.\n            req.close()\n\n            try:\n\n                # We finally decode and return the response we got from the\n                # server.\n                return response.decode()\n\n            except UnicodeDecodeError:\n                # We got an encoding error.\n\n                # We decode the response.\n                # Note: Because we don't want to deal with other issue, we\n                # decided to use `replace` in order to automatically replace\n                # all non utf-8 encoded characters.\n                return response.decode(\"utf-8\", \"replace\")\n\n        # The whois server is not given.\n\n        # We return None.\n        return None", "language": "python", "code": "def whois(cls, whois_server, domain=None, timeout=None):  # pragma: no cover\n        \"\"\"\n        Implementation of UNIX whois.\n\n        :param whois_server: The WHOIS server to use to get the record.\n        :type whois_server: str\n\n        :param domain: The domain to get the whois record from.\n        :type domain: str\n\n        :param timeout: The timeout to apply to the request.\n        :type timeout: int\n\n        :return: The whois record from the given whois server, if exist.\n        :rtype: str|None\n        \"\"\"\n\n        if domain is None:\n            # The domain is not given (localy).\n\n            # We consider the domain as the domain or IP we are currently testing.\n            domain = PyFunceble.INTERN[\"to_test\"]\n\n        if timeout is None:\n            # The time is not given (localy).\n\n            # We consider the timeout from the configuration as the timeout to use.\n            timeout = PyFunceble.CONFIGURATION[\"seconds_before_http_timeout\"]\n\n        if whois_server:\n            # A whois server is given.\n\n            # We initiate a PyFunceble.socket.\n            req = PyFunceble.socket.socket(\n                PyFunceble.socket.AF_INET, PyFunceble.socket.SOCK_STREAM\n            )\n\n            if timeout % 3 == 0:\n                # The timeout is modulo 3.\n\n                # We report the timeout to our initiated PyFunceble.socket.\n                req.settimeout(timeout)\n            else:\n                # The timeout is not modulo 3.\n\n                # We report 3 seconds as the timeout to our initiated PyFunceble.socket.\n                req.settimeout(3)\n\n            try:\n                # We try to connect to the whois server at the port 43.\n                req.connect((whois_server, 43))\n            except PyFunceble.socket.error:\n                # We got an error.\n\n                # We return None.\n                return None\n\n            # We send end encode the domain we want the data from.\n            req.send((domain + \"\\r\\n\").encode())\n\n            # We initiate a bytes variable which will save the response\n            # from the server.\n            response = b\"\"\n\n            while True:\n                # We loop infinitly.\n                try:\n                    # We try to receive the data in a buffer of 4096 bytes.\n                    data = req.recv(4096)\n                except (PyFunceble.socket.timeout, ConnectionResetError):\n                    # We got an error.\n\n                    # We close the connection.\n                    req.close()\n\n                    # And we return None.\n                    return None\n\n                # Everything goes right.\n\n                # We append data to the response we got.\n                response += data\n\n                if not data:\n                    # The data is empty.\n\n                    # We break the loop.\n                    break\n\n            # We close the connection.\n            req.close()\n\n            try:\n\n                # We finally decode and return the response we got from the\n                # server.\n                return response.decode()\n\n            except UnicodeDecodeError:\n                # We got an encoding error.\n\n                # We decode the response.\n                # Note: Because we don't want to deal with other issue, we\n                # decided to use `replace` in order to automatically replace\n                # all non utf-8 encoded characters.\n                return response.decode(\"utf-8\", \"replace\")\n\n        # The whois server is not given.\n\n        # We return None.\n        return None", "code_tokens": ["def", "whois", "(", "cls", ",", "whois_server", ",", "domain", "=", "None", ",", "timeout", "=", "None", ")", ":", "# pragma: no cover", "if", "domain", "is", "None", ":", "# The domain is not given (localy).", "# We consider the domain as the domain or IP we are currently testing.", "domain", "=", "PyFunceble", ".", "INTERN", "[", "\"to_test\"", "]", "if", "timeout", "is", "None", ":", "# The time is not given (localy).", "# We consider the timeout from the configuration as the timeout to use.", "timeout", "=", "PyFunceble", ".", "CONFIGURATION", "[", "\"seconds_before_http_timeout\"", "]", "if", "whois_server", ":", "# A whois server is given.", "# We initiate a PyFunceble.socket.", "req", "=", "PyFunceble", ".", "socket", ".", "socket", "(", "PyFunceble", ".", "socket", ".", "AF_INET", ",", "PyFunceble", ".", "socket", ".", "SOCK_STREAM", ")", "if", "timeout", "%", "3", "==", "0", ":", "# The timeout is modulo 3.", "# We report the timeout to our initiated PyFunceble.socket.", "req", ".", "settimeout", "(", "timeout", ")", "else", ":", "# The timeout is not modulo 3.", "# We report 3 seconds as the timeout to our initiated PyFunceble.socket.", "req", ".", "settimeout", "(", "3", ")", "try", ":", "# We try to connect to the whois server at the port 43.", "req", ".", "connect", "(", "(", "whois_server", ",", "43", ")", ")", "except", "PyFunceble", ".", "socket", ".", "error", ":", "# We got an error.", "# We return None.", "return", "None", "# We send end encode the domain we want the data from.", "req", ".", "send", "(", "(", "domain", "+", "\"\\r\\n\"", ")", ".", "encode", "(", ")", ")", "# We initiate a bytes variable which will save the response", "# from the server.", "response", "=", "b\"\"", "while", "True", ":", "# We loop infinitly.", "try", ":", "# We try to receive the data in a buffer of 4096 bytes.", "data", "=", "req", ".", "recv", "(", "4096", ")", "except", "(", "PyFunceble", ".", "socket", ".", "timeout", ",", "ConnectionResetError", ")", ":", "# We got an error.", "# We close the connection.", "req", ".", "close", "(", ")", "# And we return None.", "return", "None", "# Everything goes right.", "# We append data to the response we got.", "response", "+=", "data", "if", "not", "data", ":", "# The data is empty.", "# We break the loop.", "break", "# We close the connection.", "req", ".", "close", "(", ")", "try", ":", "# We finally decode and return the response we got from the", "# server.", "return", "response", ".", "decode", "(", ")", "except", "UnicodeDecodeError", ":", "# We got an encoding error.", "# We decode the response.", "# Note: Because we don't want to deal with other issue, we", "# decided to use `replace` in order to automatically replace", "# all non utf-8 encoded characters.", "return", "response", ".", "decode", "(", "\"utf-8\"", ",", "\"replace\"", ")", "# The whois server is not given.", "# We return None.", "return", "None"], "docstring": "Implementation of UNIX whois.\n\n        :param whois_server: The WHOIS server to use to get the record.\n        :type whois_server: str\n\n        :param domain: The domain to get the whois record from.\n        :type domain: str\n\n        :param timeout: The timeout to apply to the request.\n        :type timeout: int\n\n        :return: The whois record from the given whois server, if exist.\n        :rtype: str|None", "docstring_tokens": ["Implementation", "of", "UNIX", "whois", "."], "sha": "cdf69cbde120199171f7158e1c33635753e6e2f5", "url": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/lookup.py#L148-L258", "partition": "test"}
{"repo": "calston/tensor", "path": "tensor/objects.py", "func_name": "Source.tick", "original_string": "def tick(self):\n        \"\"\"Called for every timer tick. Calls self.get which can be a deferred\n        and passes that result back to the queueBack method\n        \n        Returns a deferred\"\"\"\n\n        if self.sync:\n            if self.running:\n                defer.returnValue(None)\n\n        self.running = True\n\n        try:\n            event = yield self._get()\n            if event:\n                self.queueBack(event)\n\n        except Exception as e:\n            log.msg(\"[%s] Unhandled error: %s\" % (self.service, e))\n\n        self.running = False", "language": "python", "code": "def tick(self):\n        \"\"\"Called for every timer tick. Calls self.get which can be a deferred\n        and passes that result back to the queueBack method\n        \n        Returns a deferred\"\"\"\n\n        if self.sync:\n            if self.running:\n                defer.returnValue(None)\n\n        self.running = True\n\n        try:\n            event = yield self._get()\n            if event:\n                self.queueBack(event)\n\n        except Exception as e:\n            log.msg(\"[%s] Unhandled error: %s\" % (self.service, e))\n\n        self.running = False", "code_tokens": ["def", "tick", "(", "self", ")", ":", "if", "self", ".", "sync", ":", "if", "self", ".", "running", ":", "defer", ".", "returnValue", "(", "None", ")", "self", ".", "running", "=", "True", "try", ":", "event", "=", "yield", "self", ".", "_get", "(", ")", "if", "event", ":", "self", ".", "queueBack", "(", "event", ")", "except", "Exception", "as", "e", ":", "log", ".", "msg", "(", "\"[%s] Unhandled error: %s\"", "%", "(", "self", ".", "service", ",", "e", ")", ")", "self", ".", "running", "=", "False"], "docstring": "Called for every timer tick. Calls self.get which can be a deferred\n        and passes that result back to the queueBack method\n        \n        Returns a deferred", "docstring_tokens": ["Called", "for", "every", "timer", "tick", ".", "Calls", "self", ".", "get", "which", "can", "be", "a", "deferred", "and", "passes", "that", "result", "back", "to", "the", "queueBack", "method", "Returns", "a", "deferred"], "sha": "7c0c99708b5dbff97f3895f705e11996b608549d", "url": "https://github.com/calston/tensor/blob/7c0c99708b5dbff97f3895f705e11996b608549d/tensor/objects.py#L273-L293", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval-mozilla", "path": "perceval/backends/mozilla/crates.py", "func_name": "Crates.__fetch_crate_owner_team", "original_string": "def __fetch_crate_owner_team(self, crate_id):\n        \"\"\"Get crate team owner\"\"\"\n\n        raw_owner_team = self.client.crate_attribute(crate_id, 'owner_team')\n\n        owner_team = json.loads(raw_owner_team)\n\n        return owner_team", "language": "python", "code": "def __fetch_crate_owner_team(self, crate_id):\n        \"\"\"Get crate team owner\"\"\"\n\n        raw_owner_team = self.client.crate_attribute(crate_id, 'owner_team')\n\n        owner_team = json.loads(raw_owner_team)\n\n        return owner_team", "code_tokens": ["def", "__fetch_crate_owner_team", "(", "self", ",", "crate_id", ")", ":", "raw_owner_team", "=", "self", ".", "client", ".", "crate_attribute", "(", "crate_id", ",", "'owner_team'", ")", "owner_team", "=", "json", ".", "loads", "(", "raw_owner_team", ")", "return", "owner_team"], "docstring": "Get crate team owner", "docstring_tokens": ["Get", "crate", "team", "owner"], "sha": "4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4", "url": "https://github.com/chaoss/grimoirelab-perceval-mozilla/blob/4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4/perceval/backends/mozilla/crates.py#L204-L211", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/reader.py", "func_name": "_read_deref", "original_string": "def _read_deref(ctx: ReaderContext) -> LispForm:\n    \"\"\"Read a derefed form from the input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"@\"\n    next_form = _read_next_consuming_comment(ctx)\n    return llist.l(_DEREF, next_form)", "language": "python", "code": "def _read_deref(ctx: ReaderContext) -> LispForm:\n    \"\"\"Read a derefed form from the input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"@\"\n    next_form = _read_next_consuming_comment(ctx)\n    return llist.l(_DEREF, next_form)", "code_tokens": ["def", "_read_deref", "(", "ctx", ":", "ReaderContext", ")", "->", "LispForm", ":", "start", "=", "ctx", ".", "reader", ".", "advance", "(", ")", "assert", "start", "==", "\"@\"", "next_form", "=", "_read_next_consuming_comment", "(", "ctx", ")", "return", "llist", ".", "l", "(", "_DEREF", ",", "next_form", ")"], "docstring": "Read a derefed form from the input stream.", "docstring_tokens": ["Read", "a", "derefed", "form", "from", "the", "input", "stream", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/reader.py#L855-L860", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/console_widget.py", "func_name": "ConsoleWidget._get_end_cursor", "original_string": "def _get_end_cursor(self):\n        \"\"\" Convenience method that returns a cursor for the last character.\n        \"\"\"\n        cursor = self._control.textCursor()\n        cursor.movePosition(QtGui.QTextCursor.End)\n        return cursor", "language": "python", "code": "def _get_end_cursor(self):\n        \"\"\" Convenience method that returns a cursor for the last character.\n        \"\"\"\n        cursor = self._control.textCursor()\n        cursor.movePosition(QtGui.QTextCursor.End)\n        return cursor", "code_tokens": ["def", "_get_end_cursor", "(", "self", ")", ":", "cursor", "=", "self", ".", "_control", ".", "textCursor", "(", ")", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "End", ")", "return", "cursor"], "docstring": "Convenience method that returns a cursor for the last character.", "docstring_tokens": ["Convenience", "method", "that", "returns", "a", "cursor", "for", "the", "last", "character", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/console_widget.py#L1444-L1449", "partition": "test"}
{"repo": "bjoernricks/python-quilt", "path": "quilt/delete.py", "func_name": "Delete.delete_next", "original_string": "def delete_next(self, remove=False, backup=False):\n        \"\"\" Delete next unapplied patch\n        If remove is True the patch file will also be removed. If remove and\n        backup are True a copy of the deleted patch file will be made.\n        \"\"\"\n        patch = self.db.top_patch()\n        if patch:\n            after = self.series.patch_after(patch)\n        else:\n            after = self.series.first_patch()\n        if not after:\n            raise QuiltError(\"No next patch\")\n\n        self._delete_patch(after, remove=remove, backup=backup)", "language": "python", "code": "def delete_next(self, remove=False, backup=False):\n        \"\"\" Delete next unapplied patch\n        If remove is True the patch file will also be removed. If remove and\n        backup are True a copy of the deleted patch file will be made.\n        \"\"\"\n        patch = self.db.top_patch()\n        if patch:\n            after = self.series.patch_after(patch)\n        else:\n            after = self.series.first_patch()\n        if not after:\n            raise QuiltError(\"No next patch\")\n\n        self._delete_patch(after, remove=remove, backup=backup)", "code_tokens": ["def", "delete_next", "(", "self", ",", "remove", "=", "False", ",", "backup", "=", "False", ")", ":", "patch", "=", "self", ".", "db", ".", "top_patch", "(", ")", "if", "patch", ":", "after", "=", "self", ".", "series", ".", "patch_after", "(", "patch", ")", "else", ":", "after", "=", "self", ".", "series", ".", "first_patch", "(", ")", "if", "not", "after", ":", "raise", "QuiltError", "(", "\"No next patch\"", ")", "self", ".", "_delete_patch", "(", "after", ",", "remove", "=", "remove", ",", "backup", "=", "backup", ")"], "docstring": "Delete next unapplied patch\n        If remove is True the patch file will also be removed. If remove and\n        backup are True a copy of the deleted patch file will be made.", "docstring_tokens": ["Delete", "next", "unapplied", "patch", "If", "remove", "is", "True", "the", "patch", "file", "will", "also", "be", "removed", ".", "If", "remove", "and", "backup", "are", "True", "a", "copy", "of", "the", "deleted", "patch", "file", "will", "be", "made", "."], "sha": "fae88237f601848cc34d073584d9dcb409f01777", "url": "https://github.com/bjoernricks/python-quilt/blob/fae88237f601848cc34d073584d9dcb409f01777/quilt/delete.py#L62-L75", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/frontend_widget.py", "func_name": "FrontendWidget.reset", "original_string": "def reset(self, clear=False):\n        \"\"\" Resets the widget to its initial state if ``clear`` parameter or\n        ``clear_on_kernel_restart`` configuration setting is True, otherwise\n        prints a visual indication of the fact that the kernel restarted, but\n        does not clear the traces from previous usage of the kernel before it\n        was restarted.  With ``clear=True``, it is similar to ``%clear``, but\n        also re-writes the banner and aborts execution if necessary.\n        \"\"\"\n        if self._executing:\n            self._executing = False\n            self._request_info['execute'] = {}\n        self._reading = False\n        self._highlighter.highlighting_on = False\n\n        if self.clear_on_kernel_restart or clear:\n            self._control.clear()\n            self._append_plain_text(self.banner)\n        else:\n            self._append_plain_text(\"# restarting kernel...\")\n            self._append_html(\"<hr><br>\")\n            # XXX: Reprinting the full banner may be too much, but once #1680 is\n            # addressed, that will mitigate it.\n            #self._append_plain_text(self.banner)\n        # update output marker for stdout/stderr, so that startup\n        # messages appear after banner:\n        self._append_before_prompt_pos = self._get_cursor().position()\n        self._show_interpreter_prompt()", "language": "python", "code": "def reset(self, clear=False):\n        \"\"\" Resets the widget to its initial state if ``clear`` parameter or\n        ``clear_on_kernel_restart`` configuration setting is True, otherwise\n        prints a visual indication of the fact that the kernel restarted, but\n        does not clear the traces from previous usage of the kernel before it\n        was restarted.  With ``clear=True``, it is similar to ``%clear``, but\n        also re-writes the banner and aborts execution if necessary.\n        \"\"\"\n        if self._executing:\n            self._executing = False\n            self._request_info['execute'] = {}\n        self._reading = False\n        self._highlighter.highlighting_on = False\n\n        if self.clear_on_kernel_restart or clear:\n            self._control.clear()\n            self._append_plain_text(self.banner)\n        else:\n            self._append_plain_text(\"# restarting kernel...\")\n            self._append_html(\"<hr><br>\")\n            # XXX: Reprinting the full banner may be too much, but once #1680 is\n            # addressed, that will mitigate it.\n            #self._append_plain_text(self.banner)\n        # update output marker for stdout/stderr, so that startup\n        # messages appear after banner:\n        self._append_before_prompt_pos = self._get_cursor().position()\n        self._show_interpreter_prompt()", "code_tokens": ["def", "reset", "(", "self", ",", "clear", "=", "False", ")", ":", "if", "self", ".", "_executing", ":", "self", ".", "_executing", "=", "False", "self", ".", "_request_info", "[", "'execute'", "]", "=", "{", "}", "self", ".", "_reading", "=", "False", "self", ".", "_highlighter", ".", "highlighting_on", "=", "False", "if", "self", ".", "clear_on_kernel_restart", "or", "clear", ":", "self", ".", "_control", ".", "clear", "(", ")", "self", ".", "_append_plain_text", "(", "self", ".", "banner", ")", "else", ":", "self", ".", "_append_plain_text", "(", "\"# restarting kernel...\"", ")", "self", ".", "_append_html", "(", "\"<hr><br>\"", ")", "# XXX: Reprinting the full banner may be too much, but once #1680 is", "# addressed, that will mitigate it.", "#self._append_plain_text(self.banner)", "# update output marker for stdout/stderr, so that startup", "# messages appear after banner:", "self", ".", "_append_before_prompt_pos", "=", "self", ".", "_get_cursor", "(", ")", ".", "position", "(", ")", "self", ".", "_show_interpreter_prompt", "(", ")"], "docstring": "Resets the widget to its initial state if ``clear`` parameter or\n        ``clear_on_kernel_restart`` configuration setting is True, otherwise\n        prints a visual indication of the fact that the kernel restarted, but\n        does not clear the traces from previous usage of the kernel before it\n        was restarted.  With ``clear=True``, it is similar to ``%clear``, but\n        also re-writes the banner and aborts execution if necessary.", "docstring_tokens": ["Resets", "the", "widget", "to", "its", "initial", "state", "if", "clear", "parameter", "or", "clear_on_kernel_restart", "configuration", "setting", "is", "True", "otherwise", "prints", "a", "visual", "indication", "of", "the", "fact", "that", "the", "kernel", "restarted", "but", "does", "not", "clear", "the", "traces", "from", "previous", "usage", "of", "the", "kernel", "before", "it", "was", "restarted", ".", "With", "clear", "=", "True", "it", "is", "similar", "to", "%clear", "but", "also", "re", "-", "writes", "the", "banner", "and", "aborts", "execution", "if", "necessary", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/frontend_widget.py#L574-L600", "partition": "test"}
{"repo": "sublee/zeronimo", "path": "zeronimo/core.py", "func_name": "Collector.dispatch_reply", "original_string": "def dispatch_reply(self, reply, value):\n        \"\"\"Dispatches the reply to the proper queue.\"\"\"\n        method = reply.method\n        call_id = reply.call_id\n        task_id = reply.task_id\n        if method & ACK:\n            try:\n                result_queue = self.result_queues[call_id]\n            except KeyError:\n                raise KeyError('already established or unprepared call')\n            if method == ACCEPT:\n                worker_info = value\n                result = RemoteResult(self, call_id, task_id, worker_info)\n                self.results[call_id][task_id] = result\n                result_queue.put_nowait(result)\n            elif method == REJECT:\n                result_queue.put_nowait(None)\n        else:\n            result = self.results[call_id][task_id]\n            result.set_reply(reply.method, value)", "language": "python", "code": "def dispatch_reply(self, reply, value):\n        \"\"\"Dispatches the reply to the proper queue.\"\"\"\n        method = reply.method\n        call_id = reply.call_id\n        task_id = reply.task_id\n        if method & ACK:\n            try:\n                result_queue = self.result_queues[call_id]\n            except KeyError:\n                raise KeyError('already established or unprepared call')\n            if method == ACCEPT:\n                worker_info = value\n                result = RemoteResult(self, call_id, task_id, worker_info)\n                self.results[call_id][task_id] = result\n                result_queue.put_nowait(result)\n            elif method == REJECT:\n                result_queue.put_nowait(None)\n        else:\n            result = self.results[call_id][task_id]\n            result.set_reply(reply.method, value)", "code_tokens": ["def", "dispatch_reply", "(", "self", ",", "reply", ",", "value", ")", ":", "method", "=", "reply", ".", "method", "call_id", "=", "reply", ".", "call_id", "task_id", "=", "reply", ".", "task_id", "if", "method", "&", "ACK", ":", "try", ":", "result_queue", "=", "self", ".", "result_queues", "[", "call_id", "]", "except", "KeyError", ":", "raise", "KeyError", "(", "'already established or unprepared call'", ")", "if", "method", "==", "ACCEPT", ":", "worker_info", "=", "value", "result", "=", "RemoteResult", "(", "self", ",", "call_id", ",", "task_id", ",", "worker_info", ")", "self", ".", "results", "[", "call_id", "]", "[", "task_id", "]", "=", "result", "result_queue", ".", "put_nowait", "(", "result", ")", "elif", "method", "==", "REJECT", ":", "result_queue", ".", "put_nowait", "(", "None", ")", "else", ":", "result", "=", "self", ".", "results", "[", "call_id", "]", "[", "task_id", "]", "result", ".", "set_reply", "(", "reply", ".", "method", ",", "value", ")"], "docstring": "Dispatches the reply to the proper queue.", "docstring_tokens": ["Dispatches", "the", "reply", "to", "the", "proper", "queue", "."], "sha": "b216638232932718d2cbc5eabd870c8f5b5e83fb", "url": "https://github.com/sublee/zeronimo/blob/b216638232932718d2cbc5eabd870c8f5b5e83fb/zeronimo/core.py#L675-L694", "partition": "test"}
{"repo": "katerina7479/pypdflite", "path": "pypdflite/pdflite.py", "func_name": "PDFLite.close", "original_string": "def close(self):\r\n        \"\"\" Prompt the objects to output pdf code, and save to file. \"\"\"\r\n        self.document._set_page_numbers()\r\n        # Places header, pages, page content first.\r\n        self._put_header()\r\n        self._put_pages()\r\n        self._put_resources()\r\n        # Information object\r\n        self._put_information()\r\n        # Catalog object\r\n        self._put_catalog()\r\n        # Cross-reference object\r\n        #self._put_cross_reference()\r\n        # Trailer object\r\n        self._put_trailer()\r\n\r\n        if hasattr(self.destination, \"write\"):\r\n            output = self._output_to_io()\r\n        elif self.destination == 'string':\r\n            output = self._output_to_string()\r\n        else:\r\n            self._output_to_file()\r\n            output = None\r\n        return output", "language": "python", "code": "def close(self):\r\n        \"\"\" Prompt the objects to output pdf code, and save to file. \"\"\"\r\n        self.document._set_page_numbers()\r\n        # Places header, pages, page content first.\r\n        self._put_header()\r\n        self._put_pages()\r\n        self._put_resources()\r\n        # Information object\r\n        self._put_information()\r\n        # Catalog object\r\n        self._put_catalog()\r\n        # Cross-reference object\r\n        #self._put_cross_reference()\r\n        # Trailer object\r\n        self._put_trailer()\r\n\r\n        if hasattr(self.destination, \"write\"):\r\n            output = self._output_to_io()\r\n        elif self.destination == 'string':\r\n            output = self._output_to_string()\r\n        else:\r\n            self._output_to_file()\r\n            output = None\r\n        return output", "code_tokens": ["def", "close", "(", "self", ")", ":", "self", ".", "document", ".", "_set_page_numbers", "(", ")", "# Places header, pages, page content first.\r", "self", ".", "_put_header", "(", ")", "self", ".", "_put_pages", "(", ")", "self", ".", "_put_resources", "(", ")", "# Information object\r", "self", ".", "_put_information", "(", ")", "# Catalog object\r", "self", ".", "_put_catalog", "(", ")", "# Cross-reference object\r", "#self._put_cross_reference()\r", "# Trailer object\r", "self", ".", "_put_trailer", "(", ")", "if", "hasattr", "(", "self", ".", "destination", ",", "\"write\"", ")", ":", "output", "=", "self", ".", "_output_to_io", "(", ")", "elif", "self", ".", "destination", "==", "'string'", ":", "output", "=", "self", ".", "_output_to_string", "(", ")", "else", ":", "self", ".", "_output_to_file", "(", ")", "output", "=", "None", "return", "output"], "docstring": "Prompt the objects to output pdf code, and save to file.", "docstring_tokens": ["Prompt", "the", "objects", "to", "output", "pdf", "code", "and", "save", "to", "file", "."], "sha": "ac2501f30d6619eae9dea5644717575ca9263d0a", "url": "https://github.com/katerina7479/pypdflite/blob/ac2501f30d6619eae9dea5644717575ca9263d0a/pypdflite/pdflite.py#L90-L113", "partition": "test"}
{"repo": "umich-brcf-bioinf/Jacquard", "path": "jacquard/variant_caller_transforms/varscan.py", "func_name": "Varscan.claim", "original_string": "def claim(self, file_readers):\n        \"\"\"Recognizes and claims VarScan VCFs form the set of all input VCFs.\n\n        Each defined caller has a chance to evaluate and claim all the incoming\n        files as something that it can process. Since VarScan can claim\n        high-confidence files as well, this process is significantly more\n        complex than for other callers.\n\n        Args:\n            file_readers: the collection of currently unclaimed files\n\n        Returns:\n            A tuple of unclaimed readers and VarScanVcfReaders.\n        \"\"\"\n\n        (prefix_to_readers,\n         filter_files,\n         unclaimed_set) = self._find_varscan_files(file_readers)\n\n        prefix_by_patients = self._split_prefix_by_patient(prefix_to_readers)\n        self._validate_vcf_readers(prefix_by_patients)\n        vcf_hc_pairs = self._pair_files(prefix_to_readers, filter_files)\n        self._validate_vcf_hc_pairs(vcf_hc_pairs)\n        vcf_readers = self._create_vcf_readers(vcf_hc_pairs)\n\n        return list(unclaimed_set), vcf_readers", "language": "python", "code": "def claim(self, file_readers):\n        \"\"\"Recognizes and claims VarScan VCFs form the set of all input VCFs.\n\n        Each defined caller has a chance to evaluate and claim all the incoming\n        files as something that it can process. Since VarScan can claim\n        high-confidence files as well, this process is significantly more\n        complex than for other callers.\n\n        Args:\n            file_readers: the collection of currently unclaimed files\n\n        Returns:\n            A tuple of unclaimed readers and VarScanVcfReaders.\n        \"\"\"\n\n        (prefix_to_readers,\n         filter_files,\n         unclaimed_set) = self._find_varscan_files(file_readers)\n\n        prefix_by_patients = self._split_prefix_by_patient(prefix_to_readers)\n        self._validate_vcf_readers(prefix_by_patients)\n        vcf_hc_pairs = self._pair_files(prefix_to_readers, filter_files)\n        self._validate_vcf_hc_pairs(vcf_hc_pairs)\n        vcf_readers = self._create_vcf_readers(vcf_hc_pairs)\n\n        return list(unclaimed_set), vcf_readers", "code_tokens": ["def", "claim", "(", "self", ",", "file_readers", ")", ":", "(", "prefix_to_readers", ",", "filter_files", ",", "unclaimed_set", ")", "=", "self", ".", "_find_varscan_files", "(", "file_readers", ")", "prefix_by_patients", "=", "self", ".", "_split_prefix_by_patient", "(", "prefix_to_readers", ")", "self", ".", "_validate_vcf_readers", "(", "prefix_by_patients", ")", "vcf_hc_pairs", "=", "self", ".", "_pair_files", "(", "prefix_to_readers", ",", "filter_files", ")", "self", ".", "_validate_vcf_hc_pairs", "(", "vcf_hc_pairs", ")", "vcf_readers", "=", "self", ".", "_create_vcf_readers", "(", "vcf_hc_pairs", ")", "return", "list", "(", "unclaimed_set", ")", ",", "vcf_readers"], "docstring": "Recognizes and claims VarScan VCFs form the set of all input VCFs.\n\n        Each defined caller has a chance to evaluate and claim all the incoming\n        files as something that it can process. Since VarScan can claim\n        high-confidence files as well, this process is significantly more\n        complex than for other callers.\n\n        Args:\n            file_readers: the collection of currently unclaimed files\n\n        Returns:\n            A tuple of unclaimed readers and VarScanVcfReaders.", "docstring_tokens": ["Recognizes", "and", "claims", "VarScan", "VCFs", "form", "the", "set", "of", "all", "input", "VCFs", "."], "sha": "83dd61dd2b5e4110468493beec7bc121e6cb3cd1", "url": "https://github.com/umich-brcf-bioinf/Jacquard/blob/83dd61dd2b5e4110468493beec7bc121e6cb3cd1/jacquard/variant_caller_transforms/varscan.py#L393-L418", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicebus/azure/servicebus/control_client/_serialization.py", "func_name": "_create_message", "original_string": "def _create_message(response, service_instance):\n    ''' Create message from response.\n\n    response:\n        response from Service Bus cloud server.\n    service_instance:\n        the Service Bus client.\n    '''\n    respbody = response.body\n    custom_properties = {}\n    broker_properties = None\n    message_type = None\n    message_location = None\n\n    # gets all information from respheaders.\n    for name, value in response.headers:\n        if name.lower() == 'brokerproperties':\n            broker_properties = json.loads(value)\n        elif name.lower() == 'content-type':\n            message_type = value\n        elif name.lower() == 'location':\n            message_location = value\n        # Exclude common HTTP headers to avoid noise. List\n        # is not exhaustive. At worst, custom properties will contains\n        # an unexpected content generated by the webserver and not the customer.\n        elif name.lower() not in ['transfer-encoding',\n                                  'server',\n                                  'date',\n                                  'strict-transport-security']:\n            # Follow the spec:\n            # https://docs.microsoft.com/rest/api/servicebus/message-headers-and-properties\n            if '\"' in value:\n                value = value[1:-1].replace('\\\\\"', '\"')\n                try:\n                    custom_properties[name] = datetime.strptime(\n                        value, '%a, %d %b %Y %H:%M:%S GMT')\n                except ValueError:\n                    custom_properties[name] = value\n            elif value.lower() == 'true':\n                custom_properties[name] = True\n            elif value.lower() == 'false':\n                custom_properties[name] = False\n            else:  # in theory, only int or float\n                try:\n                    # int('3.1') doesn't work so need to get float('3.14') first\n                    float_value = float(value)\n                    if str(int(float_value)) == value:\n                        custom_properties[name] = int(value)\n                    else:\n                        custom_properties[name] = float_value\n                except ValueError:\n                    # If we are here, this header does not respect the spec.\n                    # Could be an unexpected HTTP header or an invalid\n                    # header value. In both case we ignore without failing.\n                    pass\n\n    if message_type is None:\n        message = Message(\n            respbody, service_instance, message_location, custom_properties,\n            'application/atom+xml;type=entry;charset=utf-8', broker_properties)\n    else:\n        message = Message(respbody, service_instance, message_location,\n                          custom_properties, message_type, broker_properties)\n    return message", "language": "python", "code": "def _create_message(response, service_instance):\n    ''' Create message from response.\n\n    response:\n        response from Service Bus cloud server.\n    service_instance:\n        the Service Bus client.\n    '''\n    respbody = response.body\n    custom_properties = {}\n    broker_properties = None\n    message_type = None\n    message_location = None\n\n    # gets all information from respheaders.\n    for name, value in response.headers:\n        if name.lower() == 'brokerproperties':\n            broker_properties = json.loads(value)\n        elif name.lower() == 'content-type':\n            message_type = value\n        elif name.lower() == 'location':\n            message_location = value\n        # Exclude common HTTP headers to avoid noise. List\n        # is not exhaustive. At worst, custom properties will contains\n        # an unexpected content generated by the webserver and not the customer.\n        elif name.lower() not in ['transfer-encoding',\n                                  'server',\n                                  'date',\n                                  'strict-transport-security']:\n            # Follow the spec:\n            # https://docs.microsoft.com/rest/api/servicebus/message-headers-and-properties\n            if '\"' in value:\n                value = value[1:-1].replace('\\\\\"', '\"')\n                try:\n                    custom_properties[name] = datetime.strptime(\n                        value, '%a, %d %b %Y %H:%M:%S GMT')\n                except ValueError:\n                    custom_properties[name] = value\n            elif value.lower() == 'true':\n                custom_properties[name] = True\n            elif value.lower() == 'false':\n                custom_properties[name] = False\n            else:  # in theory, only int or float\n                try:\n                    # int('3.1') doesn't work so need to get float('3.14') first\n                    float_value = float(value)\n                    if str(int(float_value)) == value:\n                        custom_properties[name] = int(value)\n                    else:\n                        custom_properties[name] = float_value\n                except ValueError:\n                    # If we are here, this header does not respect the spec.\n                    # Could be an unexpected HTTP header or an invalid\n                    # header value. In both case we ignore without failing.\n                    pass\n\n    if message_type is None:\n        message = Message(\n            respbody, service_instance, message_location, custom_properties,\n            'application/atom+xml;type=entry;charset=utf-8', broker_properties)\n    else:\n        message = Message(respbody, service_instance, message_location,\n                          custom_properties, message_type, broker_properties)\n    return message", "code_tokens": ["def", "_create_message", "(", "response", ",", "service_instance", ")", ":", "respbody", "=", "response", ".", "body", "custom_properties", "=", "{", "}", "broker_properties", "=", "None", "message_type", "=", "None", "message_location", "=", "None", "# gets all information from respheaders.", "for", "name", ",", "value", "in", "response", ".", "headers", ":", "if", "name", ".", "lower", "(", ")", "==", "'brokerproperties'", ":", "broker_properties", "=", "json", ".", "loads", "(", "value", ")", "elif", "name", ".", "lower", "(", ")", "==", "'content-type'", ":", "message_type", "=", "value", "elif", "name", ".", "lower", "(", ")", "==", "'location'", ":", "message_location", "=", "value", "# Exclude common HTTP headers to avoid noise. List", "# is not exhaustive. At worst, custom properties will contains", "# an unexpected content generated by the webserver and not the customer.", "elif", "name", ".", "lower", "(", ")", "not", "in", "[", "'transfer-encoding'", ",", "'server'", ",", "'date'", ",", "'strict-transport-security'", "]", ":", "# Follow the spec:", "# https://docs.microsoft.com/rest/api/servicebus/message-headers-and-properties", "if", "'\"'", "in", "value", ":", "value", "=", "value", "[", "1", ":", "-", "1", "]", ".", "replace", "(", "'\\\\\"'", ",", "'\"'", ")", "try", ":", "custom_properties", "[", "name", "]", "=", "datetime", ".", "strptime", "(", "value", ",", "'%a, %d %b %Y %H:%M:%S GMT'", ")", "except", "ValueError", ":", "custom_properties", "[", "name", "]", "=", "value", "elif", "value", ".", "lower", "(", ")", "==", "'true'", ":", "custom_properties", "[", "name", "]", "=", "True", "elif", "value", ".", "lower", "(", ")", "==", "'false'", ":", "custom_properties", "[", "name", "]", "=", "False", "else", ":", "# in theory, only int or float", "try", ":", "# int('3.1') doesn't work so need to get float('3.14') first", "float_value", "=", "float", "(", "value", ")", "if", "str", "(", "int", "(", "float_value", ")", ")", "==", "value", ":", "custom_properties", "[", "name", "]", "=", "int", "(", "value", ")", "else", ":", "custom_properties", "[", "name", "]", "=", "float_value", "except", "ValueError", ":", "# If we are here, this header does not respect the spec.", "# Could be an unexpected HTTP header or an invalid", "# header value. In both case we ignore without failing.", "pass", "if", "message_type", "is", "None", ":", "message", "=", "Message", "(", "respbody", ",", "service_instance", ",", "message_location", ",", "custom_properties", ",", "'application/atom+xml;type=entry;charset=utf-8'", ",", "broker_properties", ")", "else", ":", "message", "=", "Message", "(", "respbody", ",", "service_instance", ",", "message_location", ",", "custom_properties", ",", "message_type", ",", "broker_properties", ")", "return", "message"], "docstring": "Create message from response.\n\n    response:\n        response from Service Bus cloud server.\n    service_instance:\n        the Service Bus client.", "docstring_tokens": ["Create", "message", "from", "response", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicebus/azure/servicebus/control_client/_serialization.py#L41-L104", "partition": "test"}
{"repo": "elliterate/capybara.py", "path": "capybara/node/base.py", "func_name": "Base.synchronize", "original_string": "def synchronize(self, func=None, wait=None, errors=()):\n        \"\"\"\n        This method is Capybara's primary defense against asynchronicity problems. It works by\n        attempting to run a given decorated function until it succeeds. The exact behavior of this\n        method depends on a number of factors. Basically there are certain exceptions which, when\n        raised from the decorated function, instead of bubbling up, are caught, and the function is\n        re-run.\n\n        Certain drivers have no support for asynchronous processes. These drivers run the function,\n        and any error raised bubbles up immediately. This allows faster turn around in the case\n        where an expectation fails.\n\n        Only exceptions that are :exc:`ElementNotFound` or any subclass thereof cause the block to\n        be rerun. Drivers may specify additional exceptions which also cause reruns. This usually\n        occurs when a node is manipulated which no longer exists on the page. For example, the\n        Selenium driver specifies ``selenium.common.exceptions.StateElementReferenceException``.\n\n        As long as any of these exceptions are thrown, the function is re-run, until a certain\n        amount of time passes. The amount of time defaults to :data:`capybara.default_max_wait_time`\n        and can be overridden through the ``wait`` argument. This time is compared with the system\n        time to see how much time has passed. If the return value of ``time.time()`` is stubbed\n        out, Capybara will raise :exc:`FrozenInTime`.\n\n        Args:\n            func (Callable, optional): The function to decorate.\n            wait (int, optional): Number of seconds to retry this function.\n            errors (Tuple[Type[Exception]], optional): Exception types that cause the function to be\n                rerun. Defaults to ``driver.invalid_element_errors`` + :exc:`ElementNotFound`.\n\n        Returns:\n            Callable: The decorated function, or a decorator function.\n\n        Raises:\n            FrozenInTime: If the return value of ``time.time()`` appears stuck.\n        \"\"\"\n\n        def decorator(func):\n            @wraps(func)\n            def outer(*args, **kwargs):\n                seconds = wait if wait is not None else capybara.default_max_wait_time\n\n                def inner():\n                    return func(*args, **kwargs)\n\n                if self.session.synchronized:\n                    return inner()\n                else:\n                    timer = Timer(seconds)\n                    self.session.synchronized = True\n                    try:\n                        while True:\n                            try:\n                                return inner()\n                            except Exception as e:\n                                self.session.raise_server_error()\n\n                                if not self._should_catch_error(e, errors):\n                                    raise\n                                if timer.expired:\n                                    raise\n\n                                sleep(0.05)\n\n                                if timer.stalled:\n                                    raise FrozenInTime(\n                                        \"time appears to be frozen, Capybara does not work with \"\n                                        \"libraries which freeze time, consider using time \"\n                                        \"traveling instead\")\n                                if capybara.automatic_reload:\n                                    self.reload()\n                    finally:\n                        self.session.synchronized = False\n\n            return outer\n\n        if func:\n            return decorator(func)\n        else:\n            return decorator", "language": "python", "code": "def synchronize(self, func=None, wait=None, errors=()):\n        \"\"\"\n        This method is Capybara's primary defense against asynchronicity problems. It works by\n        attempting to run a given decorated function until it succeeds. The exact behavior of this\n        method depends on a number of factors. Basically there are certain exceptions which, when\n        raised from the decorated function, instead of bubbling up, are caught, and the function is\n        re-run.\n\n        Certain drivers have no support for asynchronous processes. These drivers run the function,\n        and any error raised bubbles up immediately. This allows faster turn around in the case\n        where an expectation fails.\n\n        Only exceptions that are :exc:`ElementNotFound` or any subclass thereof cause the block to\n        be rerun. Drivers may specify additional exceptions which also cause reruns. This usually\n        occurs when a node is manipulated which no longer exists on the page. For example, the\n        Selenium driver specifies ``selenium.common.exceptions.StateElementReferenceException``.\n\n        As long as any of these exceptions are thrown, the function is re-run, until a certain\n        amount of time passes. The amount of time defaults to :data:`capybara.default_max_wait_time`\n        and can be overridden through the ``wait`` argument. This time is compared with the system\n        time to see how much time has passed. If the return value of ``time.time()`` is stubbed\n        out, Capybara will raise :exc:`FrozenInTime`.\n\n        Args:\n            func (Callable, optional): The function to decorate.\n            wait (int, optional): Number of seconds to retry this function.\n            errors (Tuple[Type[Exception]], optional): Exception types that cause the function to be\n                rerun. Defaults to ``driver.invalid_element_errors`` + :exc:`ElementNotFound`.\n\n        Returns:\n            Callable: The decorated function, or a decorator function.\n\n        Raises:\n            FrozenInTime: If the return value of ``time.time()`` appears stuck.\n        \"\"\"\n\n        def decorator(func):\n            @wraps(func)\n            def outer(*args, **kwargs):\n                seconds = wait if wait is not None else capybara.default_max_wait_time\n\n                def inner():\n                    return func(*args, **kwargs)\n\n                if self.session.synchronized:\n                    return inner()\n                else:\n                    timer = Timer(seconds)\n                    self.session.synchronized = True\n                    try:\n                        while True:\n                            try:\n                                return inner()\n                            except Exception as e:\n                                self.session.raise_server_error()\n\n                                if not self._should_catch_error(e, errors):\n                                    raise\n                                if timer.expired:\n                                    raise\n\n                                sleep(0.05)\n\n                                if timer.stalled:\n                                    raise FrozenInTime(\n                                        \"time appears to be frozen, Capybara does not work with \"\n                                        \"libraries which freeze time, consider using time \"\n                                        \"traveling instead\")\n                                if capybara.automatic_reload:\n                                    self.reload()\n                    finally:\n                        self.session.synchronized = False\n\n            return outer\n\n        if func:\n            return decorator(func)\n        else:\n            return decorator", "code_tokens": ["def", "synchronize", "(", "self", ",", "func", "=", "None", ",", "wait", "=", "None", ",", "errors", "=", "(", ")", ")", ":", "def", "decorator", "(", "func", ")", ":", "@", "wraps", "(", "func", ")", "def", "outer", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "seconds", "=", "wait", "if", "wait", "is", "not", "None", "else", "capybara", ".", "default_max_wait_time", "def", "inner", "(", ")", ":", "return", "func", "(", "*", "args", ",", "*", "*", "kwargs", ")", "if", "self", ".", "session", ".", "synchronized", ":", "return", "inner", "(", ")", "else", ":", "timer", "=", "Timer", "(", "seconds", ")", "self", ".", "session", ".", "synchronized", "=", "True", "try", ":", "while", "True", ":", "try", ":", "return", "inner", "(", ")", "except", "Exception", "as", "e", ":", "self", ".", "session", ".", "raise_server_error", "(", ")", "if", "not", "self", ".", "_should_catch_error", "(", "e", ",", "errors", ")", ":", "raise", "if", "timer", ".", "expired", ":", "raise", "sleep", "(", "0.05", ")", "if", "timer", ".", "stalled", ":", "raise", "FrozenInTime", "(", "\"time appears to be frozen, Capybara does not work with \"", "\"libraries which freeze time, consider using time \"", "\"traveling instead\"", ")", "if", "capybara", ".", "automatic_reload", ":", "self", ".", "reload", "(", ")", "finally", ":", "self", ".", "session", ".", "synchronized", "=", "False", "return", "outer", "if", "func", ":", "return", "decorator", "(", "func", ")", "else", ":", "return", "decorator"], "docstring": "This method is Capybara's primary defense against asynchronicity problems. It works by\n        attempting to run a given decorated function until it succeeds. The exact behavior of this\n        method depends on a number of factors. Basically there are certain exceptions which, when\n        raised from the decorated function, instead of bubbling up, are caught, and the function is\n        re-run.\n\n        Certain drivers have no support for asynchronous processes. These drivers run the function,\n        and any error raised bubbles up immediately. This allows faster turn around in the case\n        where an expectation fails.\n\n        Only exceptions that are :exc:`ElementNotFound` or any subclass thereof cause the block to\n        be rerun. Drivers may specify additional exceptions which also cause reruns. This usually\n        occurs when a node is manipulated which no longer exists on the page. For example, the\n        Selenium driver specifies ``selenium.common.exceptions.StateElementReferenceException``.\n\n        As long as any of these exceptions are thrown, the function is re-run, until a certain\n        amount of time passes. The amount of time defaults to :data:`capybara.default_max_wait_time`\n        and can be overridden through the ``wait`` argument. This time is compared with the system\n        time to see how much time has passed. If the return value of ``time.time()`` is stubbed\n        out, Capybara will raise :exc:`FrozenInTime`.\n\n        Args:\n            func (Callable, optional): The function to decorate.\n            wait (int, optional): Number of seconds to retry this function.\n            errors (Tuple[Type[Exception]], optional): Exception types that cause the function to be\n                rerun. Defaults to ``driver.invalid_element_errors`` + :exc:`ElementNotFound`.\n\n        Returns:\n            Callable: The decorated function, or a decorator function.\n\n        Raises:\n            FrozenInTime: If the return value of ``time.time()`` appears stuck.", "docstring_tokens": ["This", "method", "is", "Capybara", "s", "primary", "defense", "against", "asynchronicity", "problems", ".", "It", "works", "by", "attempting", "to", "run", "a", "given", "decorated", "function", "until", "it", "succeeds", ".", "The", "exact", "behavior", "of", "this", "method", "depends", "on", "a", "number", "of", "factors", ".", "Basically", "there", "are", "certain", "exceptions", "which", "when", "raised", "from", "the", "decorated", "function", "instead", "of", "bubbling", "up", "are", "caught", "and", "the", "function", "is", "re", "-", "run", "."], "sha": "0c6ae449cc37e4445ec3cd6af95674533beedc6c", "url": "https://github.com/elliterate/capybara.py/blob/0c6ae449cc37e4445ec3cd6af95674533beedc6c/capybara/node/base.py#L97-L175", "partition": "test"}
{"repo": "inveniosoftware/invenio-migrator", "path": "invenio_migrator/records.py", "func_name": "RecordDumpLoader.delete_buckets", "original_string": "def delete_buckets(cls, record):\n        \"\"\"Delete the bucket.\"\"\"\n        files = record.get('_files', [])\n        buckets = set()\n        for f in files:\n            buckets.add(f.get('bucket'))\n        for b_id in buckets:\n            b = Bucket.get(b_id)\n            b.deleted = True", "language": "python", "code": "def delete_buckets(cls, record):\n        \"\"\"Delete the bucket.\"\"\"\n        files = record.get('_files', [])\n        buckets = set()\n        for f in files:\n            buckets.add(f.get('bucket'))\n        for b_id in buckets:\n            b = Bucket.get(b_id)\n            b.deleted = True", "code_tokens": ["def", "delete_buckets", "(", "cls", ",", "record", ")", ":", "files", "=", "record", ".", "get", "(", "'_files'", ",", "[", "]", ")", "buckets", "=", "set", "(", ")", "for", "f", "in", "files", ":", "buckets", ".", "add", "(", "f", ".", "get", "(", "'bucket'", ")", ")", "for", "b_id", "in", "buckets", ":", "b", "=", "Bucket", ".", "get", "(", "b_id", ")", "b", ".", "deleted", "=", "True"], "docstring": "Delete the bucket.", "docstring_tokens": ["Delete", "the", "bucket", "."], "sha": "6902c6968a39b747d15e32363f43b7dffe2622c2", "url": "https://github.com/inveniosoftware/invenio-migrator/blob/6902c6968a39b747d15e32363f43b7dffe2622c2/invenio_migrator/records.py#L216-L224", "partition": "test"}
{"repo": "hivetech/dna", "path": "python/dna/logging.py", "func_name": "logger", "original_string": "def logger(name=__name__, output=None, uuid=False, timestamp=False):\n    ''' Configure and return a new logger for hivy modules '''\n    processors = []\n    if output == 'json':\n        processors.append(structlog.processors.JSONRenderer())\n\n    if uuid:\n        processors.append(add_unique_id)\n    if uuid:\n        processors.append(add_timestamp)\n    return structlog.wrap_logger(\n        logbook.Logger(name),\n        processors=processors\n    )", "language": "python", "code": "def logger(name=__name__, output=None, uuid=False, timestamp=False):\n    ''' Configure and return a new logger for hivy modules '''\n    processors = []\n    if output == 'json':\n        processors.append(structlog.processors.JSONRenderer())\n\n    if uuid:\n        processors.append(add_unique_id)\n    if uuid:\n        processors.append(add_timestamp)\n    return structlog.wrap_logger(\n        logbook.Logger(name),\n        processors=processors\n    )", "code_tokens": ["def", "logger", "(", "name", "=", "__name__", ",", "output", "=", "None", ",", "uuid", "=", "False", ",", "timestamp", "=", "False", ")", ":", "processors", "=", "[", "]", "if", "output", "==", "'json'", ":", "processors", ".", "append", "(", "structlog", ".", "processors", ".", "JSONRenderer", "(", ")", ")", "if", "uuid", ":", "processors", ".", "append", "(", "add_unique_id", ")", "if", "uuid", ":", "processors", ".", "append", "(", "add_timestamp", ")", "return", "structlog", ".", "wrap_logger", "(", "logbook", ".", "Logger", "(", "name", ")", ",", "processors", "=", "processors", ")"], "docstring": "Configure and return a new logger for hivy modules", "docstring_tokens": ["Configure", "and", "return", "a", "new", "logger", "for", "hivy", "modules"], "sha": "50ad00031be29765b2576fa407d35a36e0608de9", "url": "https://github.com/hivetech/dna/blob/50ad00031be29765b2576fa407d35a36e0608de9/python/dna/logging.py#L57-L70", "partition": "test"}
{"repo": "iotaledger/iota.lib.py", "path": "iota/multisig/api.py", "func_name": "MultisigIota.get_private_keys", "original_string": "def get_private_keys(\n            self,\n            index=0,\n            count=1,\n            security_level=AddressGenerator.DEFAULT_SECURITY_LEVEL,\n    ):\n        # type: (int, int, int) -> dict\n        \"\"\"\n        Generates one or more private keys from the seed.\n\n        As the name implies, private keys should not be shared.\n        However, in a few cases it may be necessary (e.g., for M-of-N\n        transactions).\n\n        :param index:\n            The starting key index.\n\n        :param count:\n            Number of keys to generate.\n\n        :param security_level:\n            Number of iterations to use when generating new keys.\n\n            Larger values take longer, but the resulting signatures are\n            more secure.\n\n            This value must be between 1 and 3, inclusive.\n\n        :return:\n            Dict with the following items::\n\n                {\n                    'keys': List[PrivateKey],\n                        Always contains a list, even if only one key was\n                        generated.\n                }\n\n        References:\n\n        - :py:class:`iota.crypto.signing.KeyGenerator`\n        - https://github.com/iotaledger/wiki/blob/master/multisigs.md#how-m-of-n-works\n        \"\"\"\n        return commands.GetPrivateKeysCommand(self.adapter)(\n            seed=self.seed,\n            index=index,\n            count=count,\n            securityLevel=security_level,\n        )", "language": "python", "code": "def get_private_keys(\n            self,\n            index=0,\n            count=1,\n            security_level=AddressGenerator.DEFAULT_SECURITY_LEVEL,\n    ):\n        # type: (int, int, int) -> dict\n        \"\"\"\n        Generates one or more private keys from the seed.\n\n        As the name implies, private keys should not be shared.\n        However, in a few cases it may be necessary (e.g., for M-of-N\n        transactions).\n\n        :param index:\n            The starting key index.\n\n        :param count:\n            Number of keys to generate.\n\n        :param security_level:\n            Number of iterations to use when generating new keys.\n\n            Larger values take longer, but the resulting signatures are\n            more secure.\n\n            This value must be between 1 and 3, inclusive.\n\n        :return:\n            Dict with the following items::\n\n                {\n                    'keys': List[PrivateKey],\n                        Always contains a list, even if only one key was\n                        generated.\n                }\n\n        References:\n\n        - :py:class:`iota.crypto.signing.KeyGenerator`\n        - https://github.com/iotaledger/wiki/blob/master/multisigs.md#how-m-of-n-works\n        \"\"\"\n        return commands.GetPrivateKeysCommand(self.adapter)(\n            seed=self.seed,\n            index=index,\n            count=count,\n            securityLevel=security_level,\n        )", "code_tokens": ["def", "get_private_keys", "(", "self", ",", "index", "=", "0", ",", "count", "=", "1", ",", "security_level", "=", "AddressGenerator", ".", "DEFAULT_SECURITY_LEVEL", ",", ")", ":", "# type: (int, int, int) -> dict", "return", "commands", ".", "GetPrivateKeysCommand", "(", "self", ".", "adapter", ")", "(", "seed", "=", "self", ".", "seed", ",", "index", "=", "index", ",", "count", "=", "count", ",", "securityLevel", "=", "security_level", ",", ")"], "docstring": "Generates one or more private keys from the seed.\n\n        As the name implies, private keys should not be shared.\n        However, in a few cases it may be necessary (e.g., for M-of-N\n        transactions).\n\n        :param index:\n            The starting key index.\n\n        :param count:\n            Number of keys to generate.\n\n        :param security_level:\n            Number of iterations to use when generating new keys.\n\n            Larger values take longer, but the resulting signatures are\n            more secure.\n\n            This value must be between 1 and 3, inclusive.\n\n        :return:\n            Dict with the following items::\n\n                {\n                    'keys': List[PrivateKey],\n                        Always contains a list, even if only one key was\n                        generated.\n                }\n\n        References:\n\n        - :py:class:`iota.crypto.signing.KeyGenerator`\n        - https://github.com/iotaledger/wiki/blob/master/multisigs.md#how-m-of-n-works", "docstring_tokens": ["Generates", "one", "or", "more", "private", "keys", "from", "the", "seed", "."], "sha": "97cdd1e241498446b46157b79b2a1ea2ec6d387a", "url": "https://github.com/iotaledger/iota.lib.py/blob/97cdd1e241498446b46157b79b2a1ea2ec6d387a/iota/multisig/api.py#L104-L151", "partition": "test"}
{"repo": "LionelAuroux/pyrser", "path": "pyrser/meta.py", "func_name": "set_one", "original_string": "def set_one(chainmap, thing_name, callobject):\n    \"\"\" Add a mapping with key thing_name for callobject in chainmap with\n        namespace handling.\n    \"\"\"\n    namespaces = reversed(thing_name.split(\".\"))\n    lstname = []\n    for name in namespaces:\n        lstname.insert(0, name)\n        strname = '.'.join(lstname)\n        chainmap[strname] = callobject", "language": "python", "code": "def set_one(chainmap, thing_name, callobject):\n    \"\"\" Add a mapping with key thing_name for callobject in chainmap with\n        namespace handling.\n    \"\"\"\n    namespaces = reversed(thing_name.split(\".\"))\n    lstname = []\n    for name in namespaces:\n        lstname.insert(0, name)\n        strname = '.'.join(lstname)\n        chainmap[strname] = callobject", "code_tokens": ["def", "set_one", "(", "chainmap", ",", "thing_name", ",", "callobject", ")", ":", "namespaces", "=", "reversed", "(", "thing_name", ".", "split", "(", "\".\"", ")", ")", "lstname", "=", "[", "]", "for", "name", "in", "namespaces", ":", "lstname", ".", "insert", "(", "0", ",", "name", ")", "strname", "=", "'.'", ".", "join", "(", "lstname", ")", "chainmap", "[", "strname", "]", "=", "callobject"], "docstring": "Add a mapping with key thing_name for callobject in chainmap with\n        namespace handling.", "docstring_tokens": ["Add", "a", "mapping", "with", "key", "thing_name", "for", "callobject", "in", "chainmap", "with", "namespace", "handling", "."], "sha": "f153a97ef2b6bf915a1ed468c0252a9a59b754d5", "url": "https://github.com/LionelAuroux/pyrser/blob/f153a97ef2b6bf915a1ed468c0252a9a59b754d5/pyrser/meta.py#L105-L114", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/core/objects.py", "func_name": "Credential.read", "original_string": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the Credential struct and decode it into its\n        constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if either the credential type or value are\n                missing from the encoding.\n        \"\"\"\n        super(Credential, self).read(input_stream, kmip_version=kmip_version)\n        local_stream = BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(enums.Tags.CREDENTIAL_TYPE, local_stream):\n            self._credential_type = primitives.Enumeration(\n                enum=enums.CredentialType,\n                tag=enums.Tags.CREDENTIAL_TYPE\n            )\n            self._credential_type.read(local_stream, kmip_version=kmip_version)\n        else:\n            raise ValueError(\n                \"Credential encoding missing the credential type.\"\n            )\n\n        if self.is_tag_next(enums.Tags.CREDENTIAL_VALUE, local_stream):\n            if self.credential_type == \\\n                    enums.CredentialType.USERNAME_AND_PASSWORD:\n                self._credential_value = UsernamePasswordCredential()\n            elif self.credential_type == enums.CredentialType.DEVICE:\n                self._credential_value = DeviceCredential()\n            elif self.credential_type == enums.CredentialType.ATTESTATION:\n                self._credential_value = AttestationCredential()\n            else:\n                raise ValueError(\n                    \"Credential encoding includes unrecognized credential \"\n                    \"type.\"\n                )\n            self._credential_value.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        else:\n            raise ValueError(\n                \"Credential encoding missing the credential value.\"\n            )\n\n        self.is_oversized(local_stream)", "language": "python", "code": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the Credential struct and decode it into its\n        constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if either the credential type or value are\n                missing from the encoding.\n        \"\"\"\n        super(Credential, self).read(input_stream, kmip_version=kmip_version)\n        local_stream = BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(enums.Tags.CREDENTIAL_TYPE, local_stream):\n            self._credential_type = primitives.Enumeration(\n                enum=enums.CredentialType,\n                tag=enums.Tags.CREDENTIAL_TYPE\n            )\n            self._credential_type.read(local_stream, kmip_version=kmip_version)\n        else:\n            raise ValueError(\n                \"Credential encoding missing the credential type.\"\n            )\n\n        if self.is_tag_next(enums.Tags.CREDENTIAL_VALUE, local_stream):\n            if self.credential_type == \\\n                    enums.CredentialType.USERNAME_AND_PASSWORD:\n                self._credential_value = UsernamePasswordCredential()\n            elif self.credential_type == enums.CredentialType.DEVICE:\n                self._credential_value = DeviceCredential()\n            elif self.credential_type == enums.CredentialType.ATTESTATION:\n                self._credential_value = AttestationCredential()\n            else:\n                raise ValueError(\n                    \"Credential encoding includes unrecognized credential \"\n                    \"type.\"\n                )\n            self._credential_value.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        else:\n            raise ValueError(\n                \"Credential encoding missing the credential value.\"\n            )\n\n        self.is_oversized(local_stream)", "code_tokens": ["def", "read", "(", "self", ",", "input_stream", ",", "kmip_version", "=", "enums", ".", "KMIPVersion", ".", "KMIP_1_0", ")", ":", "super", "(", "Credential", ",", "self", ")", ".", "read", "(", "input_stream", ",", "kmip_version", "=", "kmip_version", ")", "local_stream", "=", "BytearrayStream", "(", "input_stream", ".", "read", "(", "self", ".", "length", ")", ")", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "CREDENTIAL_TYPE", ",", "local_stream", ")", ":", "self", ".", "_credential_type", "=", "primitives", ".", "Enumeration", "(", "enum", "=", "enums", ".", "CredentialType", ",", "tag", "=", "enums", ".", "Tags", ".", "CREDENTIAL_TYPE", ")", "self", ".", "_credential_type", ".", "read", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "else", ":", "raise", "ValueError", "(", "\"Credential encoding missing the credential type.\"", ")", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "CREDENTIAL_VALUE", ",", "local_stream", ")", ":", "if", "self", ".", "credential_type", "==", "enums", ".", "CredentialType", ".", "USERNAME_AND_PASSWORD", ":", "self", ".", "_credential_value", "=", "UsernamePasswordCredential", "(", ")", "elif", "self", ".", "credential_type", "==", "enums", ".", "CredentialType", ".", "DEVICE", ":", "self", ".", "_credential_value", "=", "DeviceCredential", "(", ")", "elif", "self", ".", "credential_type", "==", "enums", ".", "CredentialType", ".", "ATTESTATION", ":", "self", ".", "_credential_value", "=", "AttestationCredential", "(", ")", "else", ":", "raise", "ValueError", "(", "\"Credential encoding includes unrecognized credential \"", "\"type.\"", ")", "self", ".", "_credential_value", ".", "read", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "else", ":", "raise", "ValueError", "(", "\"Credential encoding missing the credential value.\"", ")", "self", ".", "is_oversized", "(", "local_stream", ")"], "docstring": "Read the data encoding the Credential struct and decode it into its\n        constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if either the credential type or value are\n                missing from the encoding.", "docstring_tokens": ["Read", "the", "data", "encoding", "the", "Credential", "struct", "and", "decode", "it", "into", "its", "constituent", "parts", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/core/objects.py#L1658-L1711", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/googlehits.py", "func_name": "GoogleHits.__parse_hits", "original_string": "def __parse_hits(self, hit_raw):\n        \"\"\"Parse the hits returned by the Google Search API\"\"\"\n\n        # Create the soup and get the desired div\n        bs_result = bs4.BeautifulSoup(hit_raw, 'html.parser')\n        hit_string = bs_result.find(\"div\", id=\"resultStats\").text\n\n        # Remove commas or dots\n        hit_string = hit_string.replace(',', u'')\n        hit_string = hit_string.replace('.', u'')\n\n        fetched_on = datetime_utcnow().timestamp()\n        id_args = self.keywords[:]\n        id_args.append(str(fetched_on))\n\n        hits_json = {\n            'fetched_on': fetched_on,\n            'id': uuid(*id_args),\n            'keywords': self.keywords,\n            'type': 'googleSearchHits'\n        }\n\n        if not hit_string:\n            logger.warning(\"No hits for %s\", self.keywords)\n            hits_json['hits'] = 0\n\n            return hits_json\n\n        str_hits = re.search(r'\\d+', hit_string).group(0)\n        hits = int(str_hits)\n        hits_json['hits'] = hits\n\n        return hits_json", "language": "python", "code": "def __parse_hits(self, hit_raw):\n        \"\"\"Parse the hits returned by the Google Search API\"\"\"\n\n        # Create the soup and get the desired div\n        bs_result = bs4.BeautifulSoup(hit_raw, 'html.parser')\n        hit_string = bs_result.find(\"div\", id=\"resultStats\").text\n\n        # Remove commas or dots\n        hit_string = hit_string.replace(',', u'')\n        hit_string = hit_string.replace('.', u'')\n\n        fetched_on = datetime_utcnow().timestamp()\n        id_args = self.keywords[:]\n        id_args.append(str(fetched_on))\n\n        hits_json = {\n            'fetched_on': fetched_on,\n            'id': uuid(*id_args),\n            'keywords': self.keywords,\n            'type': 'googleSearchHits'\n        }\n\n        if not hit_string:\n            logger.warning(\"No hits for %s\", self.keywords)\n            hits_json['hits'] = 0\n\n            return hits_json\n\n        str_hits = re.search(r'\\d+', hit_string).group(0)\n        hits = int(str_hits)\n        hits_json['hits'] = hits\n\n        return hits_json", "code_tokens": ["def", "__parse_hits", "(", "self", ",", "hit_raw", ")", ":", "# Create the soup and get the desired div", "bs_result", "=", "bs4", ".", "BeautifulSoup", "(", "hit_raw", ",", "'html.parser'", ")", "hit_string", "=", "bs_result", ".", "find", "(", "\"div\"", ",", "id", "=", "\"resultStats\"", ")", ".", "text", "# Remove commas or dots", "hit_string", "=", "hit_string", ".", "replace", "(", "','", ",", "u''", ")", "hit_string", "=", "hit_string", ".", "replace", "(", "'.'", ",", "u''", ")", "fetched_on", "=", "datetime_utcnow", "(", ")", ".", "timestamp", "(", ")", "id_args", "=", "self", ".", "keywords", "[", ":", "]", "id_args", ".", "append", "(", "str", "(", "fetched_on", ")", ")", "hits_json", "=", "{", "'fetched_on'", ":", "fetched_on", ",", "'id'", ":", "uuid", "(", "*", "id_args", ")", ",", "'keywords'", ":", "self", ".", "keywords", ",", "'type'", ":", "'googleSearchHits'", "}", "if", "not", "hit_string", ":", "logger", ".", "warning", "(", "\"No hits for %s\"", ",", "self", ".", "keywords", ")", "hits_json", "[", "'hits'", "]", "=", "0", "return", "hits_json", "str_hits", "=", "re", ".", "search", "(", "r'\\d+'", ",", "hit_string", ")", ".", "group", "(", "0", ")", "hits", "=", "int", "(", "str_hits", ")", "hits_json", "[", "'hits'", "]", "=", "hits", "return", "hits_json"], "docstring": "Parse the hits returned by the Google Search API", "docstring_tokens": ["Parse", "the", "hits", "returned", "by", "the", "Google", "Search", "API"], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/googlehits.py#L164-L196", "partition": "test"}
{"repo": "disqus/python-phabricator", "path": "phabricator/__init__.py", "func_name": "parse_interfaces", "original_string": "def parse_interfaces(interfaces):\n    \"\"\"\n    Parse the conduit.query json dict response\n    This performs the logic of parsing the non-standard params dict\n        and then returning a dict Resource can understand\n    \"\"\"\n    parsed_interfaces = collections.defaultdict(dict)\n\n    for m, d in iteritems(interfaces):\n        app, func = m.split('.', 1)\n\n        method = parsed_interfaces[app][func] = {}\n\n        # Make default assumptions since these aren't provided by Phab\n        method['formats'] = ['json', 'human']\n        method['method'] = 'POST'\n\n        method['optional'] = {}\n        method['required'] = {}\n\n        for name, type_info in iteritems(dict(d['params'])):\n            # Set the defaults\n            optionality = 'required'\n            param_type = 'string'\n\n            # Usually in the format: <optionality> <param_type>\n            type_info = TYPE_INFO_COMMENT_RE.sub('', type_info)\n            info_pieces = TYPE_INFO_SPLITTER_RE.findall(type_info)\n            for info_piece in info_pieces:\n                if info_piece in ('optional', 'required'):\n                    optionality = info_piece\n                elif info_piece == 'ignored':\n                    optionality = 'optional'\n                    param_type = 'string'\n                elif info_piece == 'nonempty':\n                    optionality = 'required'\n                elif info_piece == 'deprecated':\n                    optionality = 'optional'\n                else:\n                    param_type = info_piece\n\n            method[optionality][name] = map_param_type(param_type)\n\n    return dict(parsed_interfaces)", "language": "python", "code": "def parse_interfaces(interfaces):\n    \"\"\"\n    Parse the conduit.query json dict response\n    This performs the logic of parsing the non-standard params dict\n        and then returning a dict Resource can understand\n    \"\"\"\n    parsed_interfaces = collections.defaultdict(dict)\n\n    for m, d in iteritems(interfaces):\n        app, func = m.split('.', 1)\n\n        method = parsed_interfaces[app][func] = {}\n\n        # Make default assumptions since these aren't provided by Phab\n        method['formats'] = ['json', 'human']\n        method['method'] = 'POST'\n\n        method['optional'] = {}\n        method['required'] = {}\n\n        for name, type_info in iteritems(dict(d['params'])):\n            # Set the defaults\n            optionality = 'required'\n            param_type = 'string'\n\n            # Usually in the format: <optionality> <param_type>\n            type_info = TYPE_INFO_COMMENT_RE.sub('', type_info)\n            info_pieces = TYPE_INFO_SPLITTER_RE.findall(type_info)\n            for info_piece in info_pieces:\n                if info_piece in ('optional', 'required'):\n                    optionality = info_piece\n                elif info_piece == 'ignored':\n                    optionality = 'optional'\n                    param_type = 'string'\n                elif info_piece == 'nonempty':\n                    optionality = 'required'\n                elif info_piece == 'deprecated':\n                    optionality = 'optional'\n                else:\n                    param_type = info_piece\n\n            method[optionality][name] = map_param_type(param_type)\n\n    return dict(parsed_interfaces)", "code_tokens": ["def", "parse_interfaces", "(", "interfaces", ")", ":", "parsed_interfaces", "=", "collections", ".", "defaultdict", "(", "dict", ")", "for", "m", ",", "d", "in", "iteritems", "(", "interfaces", ")", ":", "app", ",", "func", "=", "m", ".", "split", "(", "'.'", ",", "1", ")", "method", "=", "parsed_interfaces", "[", "app", "]", "[", "func", "]", "=", "{", "}", "# Make default assumptions since these aren't provided by Phab", "method", "[", "'formats'", "]", "=", "[", "'json'", ",", "'human'", "]", "method", "[", "'method'", "]", "=", "'POST'", "method", "[", "'optional'", "]", "=", "{", "}", "method", "[", "'required'", "]", "=", "{", "}", "for", "name", ",", "type_info", "in", "iteritems", "(", "dict", "(", "d", "[", "'params'", "]", ")", ")", ":", "# Set the defaults", "optionality", "=", "'required'", "param_type", "=", "'string'", "# Usually in the format: <optionality> <param_type>", "type_info", "=", "TYPE_INFO_COMMENT_RE", ".", "sub", "(", "''", ",", "type_info", ")", "info_pieces", "=", "TYPE_INFO_SPLITTER_RE", ".", "findall", "(", "type_info", ")", "for", "info_piece", "in", "info_pieces", ":", "if", "info_piece", "in", "(", "'optional'", ",", "'required'", ")", ":", "optionality", "=", "info_piece", "elif", "info_piece", "==", "'ignored'", ":", "optionality", "=", "'optional'", "param_type", "=", "'string'", "elif", "info_piece", "==", "'nonempty'", ":", "optionality", "=", "'required'", "elif", "info_piece", "==", "'deprecated'", ":", "optionality", "=", "'optional'", "else", ":", "param_type", "=", "info_piece", "method", "[", "optionality", "]", "[", "name", "]", "=", "map_param_type", "(", "param_type", ")", "return", "dict", "(", "parsed_interfaces", ")"], "docstring": "Parse the conduit.query json dict response\n    This performs the logic of parsing the non-standard params dict\n        and then returning a dict Resource can understand", "docstring_tokens": ["Parse", "the", "conduit", ".", "query", "json", "dict", "response", "This", "performs", "the", "logic", "of", "parsing", "the", "non", "-", "standard", "params", "dict", "and", "then", "returning", "a", "dict", "Resource", "can", "understand"], "sha": "ad08e335081531fae053a78a1c708cd11e3e6c49", "url": "https://github.com/disqus/python-phabricator/blob/ad08e335081531fae053a78a1c708cd11e3e6c49/phabricator/__init__.py#L137-L180", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "examples/example_17_wrapping_an_existing_project/original.py", "func_name": "convert_rule", "original_string": "def convert_rule(rule_number):\n    \"\"\" Converts a rule given as an integer into a binary list representation.\n\n    It reads from left to right (contrary to the Wikipedia article given below),\n    i.e. the 2**0 is found on the left hand side and 2**7 on the right.\n\n    For example:\n\n        ``convert_rule(30)`` returns [0, 1, 1, 1, 1, 0, 0, 0]\n\n\n    The resulting binary list can be interpreted as\n    the following transition table:\n\n         neighborhood  new cell state\n                000     0\n                001     1\n                010     1\n                011     1\n                100     1\n                101     0\n                110     0\n                111     0\n\n    For more information about this rule\n    see: http://en.wikipedia.org/wiki/Rule_30\n\n    \"\"\"\n    binary_rule = [(rule_number // pow(2,i)) % 2 for i in range(8)]\n    return np.array(binary_rule)", "language": "python", "code": "def convert_rule(rule_number):\n    \"\"\" Converts a rule given as an integer into a binary list representation.\n\n    It reads from left to right (contrary to the Wikipedia article given below),\n    i.e. the 2**0 is found on the left hand side and 2**7 on the right.\n\n    For example:\n\n        ``convert_rule(30)`` returns [0, 1, 1, 1, 1, 0, 0, 0]\n\n\n    The resulting binary list can be interpreted as\n    the following transition table:\n\n         neighborhood  new cell state\n                000     0\n                001     1\n                010     1\n                011     1\n                100     1\n                101     0\n                110     0\n                111     0\n\n    For more information about this rule\n    see: http://en.wikipedia.org/wiki/Rule_30\n\n    \"\"\"\n    binary_rule = [(rule_number // pow(2,i)) % 2 for i in range(8)]\n    return np.array(binary_rule)", "code_tokens": ["def", "convert_rule", "(", "rule_number", ")", ":", "binary_rule", "=", "[", "(", "rule_number", "//", "pow", "(", "2", ",", "i", ")", ")", "%", "2", "for", "i", "in", "range", "(", "8", ")", "]", "return", "np", ".", "array", "(", "binary_rule", ")"], "docstring": "Converts a rule given as an integer into a binary list representation.\n\n    It reads from left to right (contrary to the Wikipedia article given below),\n    i.e. the 2**0 is found on the left hand side and 2**7 on the right.\n\n    For example:\n\n        ``convert_rule(30)`` returns [0, 1, 1, 1, 1, 0, 0, 0]\n\n\n    The resulting binary list can be interpreted as\n    the following transition table:\n\n         neighborhood  new cell state\n                000     0\n                001     1\n                010     1\n                011     1\n                100     1\n                101     0\n                110     0\n                111     0\n\n    For more information about this rule\n    see: http://en.wikipedia.org/wiki/Rule_30", "docstring_tokens": ["Converts", "a", "rule", "given", "as", "an", "integer", "into", "a", "binary", "list", "representation", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/examples/example_17_wrapping_an_existing_project/original.py#L17-L46", "partition": "test"}
{"repo": "intel-analytics/BigDL", "path": "pyspark/bigdl/transform/vision/image.py", "func_name": "DistributedImageFrame.get_predict", "original_string": "def get_predict(self, key=\"predict\"):\n        \"\"\"\n        get prediction rdd from ImageFrame\n        \"\"\"\n        predicts = callBigDlFunc(self.bigdl_type, \"distributedImageFrameToPredict\", self.value, key)\n        return predicts.map(lambda predict: (predict[0], predict[1].to_ndarray()) if predict[1] else (predict[0], None))", "language": "python", "code": "def get_predict(self, key=\"predict\"):\n        \"\"\"\n        get prediction rdd from ImageFrame\n        \"\"\"\n        predicts = callBigDlFunc(self.bigdl_type, \"distributedImageFrameToPredict\", self.value, key)\n        return predicts.map(lambda predict: (predict[0], predict[1].to_ndarray()) if predict[1] else (predict[0], None))", "code_tokens": ["def", "get_predict", "(", "self", ",", "key", "=", "\"predict\"", ")", ":", "predicts", "=", "callBigDlFunc", "(", "self", ".", "bigdl_type", ",", "\"distributedImageFrameToPredict\"", ",", "self", ".", "value", ",", "key", ")", "return", "predicts", ".", "map", "(", "lambda", "predict", ":", "(", "predict", "[", "0", "]", ",", "predict", "[", "1", "]", ".", "to_ndarray", "(", ")", ")", "if", "predict", "[", "1", "]", "else", "(", "predict", "[", "0", "]", ",", "None", ")", ")"], "docstring": "get prediction rdd from ImageFrame", "docstring_tokens": ["get", "prediction", "rdd", "from", "ImageFrame"], "sha": "e9c19788285986ab789a2e2998f9a85d7524779f", "url": "https://github.com/intel-analytics/BigDL/blob/e9c19788285986ab789a2e2998f9a85d7524779f/pyspark/bigdl/transform/vision/image.py#L290-L295", "partition": "test"}
{"repo": "reingart/gui2py", "path": "gui/controls/gridview.py", "func_name": "ComboCellEditor.IsAcceptedKey", "original_string": "def IsAcceptedKey(self, evt):\r\n        \"Return True to allow the given key to start editing\"\r\n        ## Oops, there's a bug here, we'll have to do it ourself..\r\n        ##return self.base_IsAcceptedKey(evt)\r\n        return (not (evt.ControlDown() or evt.AltDown()) and\r\n                evt.GetKeyCode() != wx.WXK_SHIFT)", "language": "python", "code": "def IsAcceptedKey(self, evt):\r\n        \"Return True to allow the given key to start editing\"\r\n        ## Oops, there's a bug here, we'll have to do it ourself..\r\n        ##return self.base_IsAcceptedKey(evt)\r\n        return (not (evt.ControlDown() or evt.AltDown()) and\r\n                evt.GetKeyCode() != wx.WXK_SHIFT)", "code_tokens": ["def", "IsAcceptedKey", "(", "self", ",", "evt", ")", ":", "## Oops, there's a bug here, we'll have to do it ourself..\r", "##return self.base_IsAcceptedKey(evt)\r", "return", "(", "not", "(", "evt", ".", "ControlDown", "(", ")", "or", "evt", ".", "AltDown", "(", ")", ")", "and", "evt", ".", "GetKeyCode", "(", ")", "!=", "wx", ".", "WXK_SHIFT", ")"], "docstring": "Return True to allow the given key to start editing", "docstring_tokens": ["Return", "True", "to", "allow", "the", "given", "key", "to", "start", "editing"], "sha": "aca0a05f6fcde55c94ad7cc058671a06608b01a4", "url": "https://github.com/reingart/gui2py/blob/aca0a05f6fcde55c94ad7cc058671a06608b01a4/gui/controls/gridview.py#L584-L589", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_transfer_hook.py", "func_name": "GCPTransferServiceHook.create_transfer_job", "original_string": "def create_transfer_job(self, body):\n        \"\"\"\n        Creates a transfer job that runs periodically.\n\n        :param body: (Required) A request body, as described in\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n        :type body: dict\n        :return: transfer job.\n            See:\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n        :rtype: dict\n        \"\"\"\n        body = self._inject_project_id(body, BODY, PROJECT_ID)\n        return self.get_conn().transferJobs().create(body=body).execute(num_retries=self.num_retries)", "language": "python", "code": "def create_transfer_job(self, body):\n        \"\"\"\n        Creates a transfer job that runs periodically.\n\n        :param body: (Required) A request body, as described in\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n        :type body: dict\n        :return: transfer job.\n            See:\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n        :rtype: dict\n        \"\"\"\n        body = self._inject_project_id(body, BODY, PROJECT_ID)\n        return self.get_conn().transferJobs().create(body=body).execute(num_retries=self.num_retries)", "code_tokens": ["def", "create_transfer_job", "(", "self", ",", "body", ")", ":", "body", "=", "self", ".", "_inject_project_id", "(", "body", ",", "BODY", ",", "PROJECT_ID", ")", "return", "self", ".", "get_conn", "(", ")", ".", "transferJobs", "(", ")", ".", "create", "(", "body", "=", "body", ")", ".", "execute", "(", "num_retries", "=", "self", ".", "num_retries", ")"], "docstring": "Creates a transfer job that runs periodically.\n\n        :param body: (Required) A request body, as described in\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n        :type body: dict\n        :return: transfer job.\n            See:\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n        :rtype: dict", "docstring_tokens": ["Creates", "a", "transfer", "job", "that", "runs", "periodically", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L119-L132", "partition": "test"}
{"repo": "BHSPitMonkey/vmflib", "path": "tools/buildbsp.py", "func_name": "Game.get_game_dir", "original_string": "def get_game_dir(self, username=False):\n        \"\"\"Returns joined game directory path relative to Steamapps\"\"\"\n        if not self.common and not username:\n            raise RuntimeError(\"Can't determine this game's directory without username\")\n        if self.common:\n            subdir = \"common\"\n        else:\n            subdir = \"username\"\n        subsubdir = self.dir\n        if WIN32 or CYGWIN:\n            subsubdir = subsubdir.lower()\n        return os.path.join(subdir, subsubdir)", "language": "python", "code": "def get_game_dir(self, username=False):\n        \"\"\"Returns joined game directory path relative to Steamapps\"\"\"\n        if not self.common and not username:\n            raise RuntimeError(\"Can't determine this game's directory without username\")\n        if self.common:\n            subdir = \"common\"\n        else:\n            subdir = \"username\"\n        subsubdir = self.dir\n        if WIN32 or CYGWIN:\n            subsubdir = subsubdir.lower()\n        return os.path.join(subdir, subsubdir)", "code_tokens": ["def", "get_game_dir", "(", "self", ",", "username", "=", "False", ")", ":", "if", "not", "self", ".", "common", "and", "not", "username", ":", "raise", "RuntimeError", "(", "\"Can't determine this game's directory without username\"", ")", "if", "self", ".", "common", ":", "subdir", "=", "\"common\"", "else", ":", "subdir", "=", "\"username\"", "subsubdir", "=", "self", ".", "dir", "if", "WIN32", "or", "CYGWIN", ":", "subsubdir", "=", "subsubdir", ".", "lower", "(", ")", "return", "os", ".", "path", ".", "join", "(", "subdir", ",", "subsubdir", ")"], "docstring": "Returns joined game directory path relative to Steamapps", "docstring_tokens": ["Returns", "joined", "game", "directory", "path", "relative", "to", "Steamapps"], "sha": "322757fcba98e05041ee8f416c8ffe847ca1fe64", "url": "https://github.com/BHSPitMonkey/vmflib/blob/322757fcba98e05041ee8f416c8ffe847ca1fe64/tools/buildbsp.py#L34-L45", "partition": "test"}
{"repo": "LuminosoInsight/luminoso-api-client-python", "path": "luminoso_api/v5_upload.py", "func_name": "upload_docs", "original_string": "def upload_docs(\n    client, input_filename, language, name, account=None, progress=False\n):\n    \"\"\"\n    Given a LuminosoClient pointing to the root of the API, and a filename to\n    read JSON lines from, create a project from the documents in that file.\n    \"\"\"\n    docs = iterate_json_lines(input_filename)\n    return create_project_with_docs(\n        client, docs, language, name, account, progress=progress\n    )", "language": "python", "code": "def upload_docs(\n    client, input_filename, language, name, account=None, progress=False\n):\n    \"\"\"\n    Given a LuminosoClient pointing to the root of the API, and a filename to\n    read JSON lines from, create a project from the documents in that file.\n    \"\"\"\n    docs = iterate_json_lines(input_filename)\n    return create_project_with_docs(\n        client, docs, language, name, account, progress=progress\n    )", "code_tokens": ["def", "upload_docs", "(", "client", ",", "input_filename", ",", "language", ",", "name", ",", "account", "=", "None", ",", "progress", "=", "False", ")", ":", "docs", "=", "iterate_json_lines", "(", "input_filename", ")", "return", "create_project_with_docs", "(", "client", ",", "docs", ",", "language", ",", "name", ",", "account", ",", "progress", "=", "progress", ")"], "docstring": "Given a LuminosoClient pointing to the root of the API, and a filename to\n    read JSON lines from, create a project from the documents in that file.", "docstring_tokens": ["Given", "a", "LuminosoClient", "pointing", "to", "the", "root", "of", "the", "API", "and", "a", "filename", "to", "read", "JSON", "lines", "from", "create", "a", "project", "from", "the", "documents", "in", "that", "file", "."], "sha": "3bedf2a454aee39214c11fbf556ead3eecc27881", "url": "https://github.com/LuminosoInsight/luminoso-api-client-python/blob/3bedf2a454aee39214c11fbf556ead3eecc27881/luminoso_api/v5_upload.py#L108-L118", "partition": "test"}
{"repo": "umich-brcf-bioinf/Jacquard", "path": "jacquard/variant_caller_transforms/variant_caller_factory.py", "func_name": "VariantCallerFactory.claim", "original_string": "def claim(self, unclaimed_file_readers):\n        \"\"\"Allows each caller to claim incoming files as they are recognized.\n\n        Args:\n            unclaimed_file_readers: Usually, all files in the input dir.\n\n        Returns:\n            A tuple of unclaimed file readers and claimed VcfReaders. The\n            presence of any unclaimed file readers could indicate stray files\n            in the input dir.\n        \"\"\"\n        claimed_vcf_readers = []\n        for caller in self._callers:\n            (unclaimed_file_readers,\n             translated_vcf_readers) = caller.claim(unclaimed_file_readers)\n            claimed_vcf_readers.extend(translated_vcf_readers)\n\n        return unclaimed_file_readers, claimed_vcf_readers", "language": "python", "code": "def claim(self, unclaimed_file_readers):\n        \"\"\"Allows each caller to claim incoming files as they are recognized.\n\n        Args:\n            unclaimed_file_readers: Usually, all files in the input dir.\n\n        Returns:\n            A tuple of unclaimed file readers and claimed VcfReaders. The\n            presence of any unclaimed file readers could indicate stray files\n            in the input dir.\n        \"\"\"\n        claimed_vcf_readers = []\n        for caller in self._callers:\n            (unclaimed_file_readers,\n             translated_vcf_readers) = caller.claim(unclaimed_file_readers)\n            claimed_vcf_readers.extend(translated_vcf_readers)\n\n        return unclaimed_file_readers, claimed_vcf_readers", "code_tokens": ["def", "claim", "(", "self", ",", "unclaimed_file_readers", ")", ":", "claimed_vcf_readers", "=", "[", "]", "for", "caller", "in", "self", ".", "_callers", ":", "(", "unclaimed_file_readers", ",", "translated_vcf_readers", ")", "=", "caller", ".", "claim", "(", "unclaimed_file_readers", ")", "claimed_vcf_readers", ".", "extend", "(", "translated_vcf_readers", ")", "return", "unclaimed_file_readers", ",", "claimed_vcf_readers"], "docstring": "Allows each caller to claim incoming files as they are recognized.\n\n        Args:\n            unclaimed_file_readers: Usually, all files in the input dir.\n\n        Returns:\n            A tuple of unclaimed file readers and claimed VcfReaders. The\n            presence of any unclaimed file readers could indicate stray files\n            in the input dir.", "docstring_tokens": ["Allows", "each", "caller", "to", "claim", "incoming", "files", "as", "they", "are", "recognized", "."], "sha": "83dd61dd2b5e4110468493beec7bc121e6cb3cd1", "url": "https://github.com/umich-brcf-bioinf/Jacquard/blob/83dd61dd2b5e4110468493beec7bc121e6cb3cd1/jacquard/variant_caller_transforms/variant_caller_factory.py#L25-L42", "partition": "test"}
{"repo": "20tab/twentytab-treeeditor", "path": "treeeditor/admin.py", "func_name": "ajax_editable_boolean_cell", "original_string": "def ajax_editable_boolean_cell(item, attr, text='', override=None):\n    \"\"\"\n    Generate a html snippet for showing a boolean value on the admin page.\n    Item is an object, attr is the attribute name we should display. Text\n    is an optional explanatory text to be included in the output.\n\n    This function will emit code to produce a checkbox input with its state\n    corresponding to the item.attr attribute if no override value is passed.\n    This input is wired to run a JS ajax updater to toggle the value.\n\n    If override is passed in, ignores the attr attribute and returns a\n    static image for the override boolean with no user interaction possible\n    (useful for \"disabled and you can't change it\" situations).\n    \"\"\"\n    if text:\n        text = '&nbsp;(%s)' % unicode(text)\n\n    if override is not None:\n        a = [django_boolean_icon(override, text), text]\n    else:\n        value = getattr(item, attr)\n        a = [\n            '<input type=\"checkbox\"',\n            value and ' checked=\"checked\"' or '',\n            ' onclick=\"return inplace_toggle_boolean(%d, \\'%s\\')\";' % (item.id, attr),\n            ' />',\n            text,\n        ]\n\n    a.insert(0, '<div id=\"wrap_%s_%d\">' % ( attr, item.id ))\n    a.append('</div>')\n    return unicode(''.join(a))", "language": "python", "code": "def ajax_editable_boolean_cell(item, attr, text='', override=None):\n    \"\"\"\n    Generate a html snippet for showing a boolean value on the admin page.\n    Item is an object, attr is the attribute name we should display. Text\n    is an optional explanatory text to be included in the output.\n\n    This function will emit code to produce a checkbox input with its state\n    corresponding to the item.attr attribute if no override value is passed.\n    This input is wired to run a JS ajax updater to toggle the value.\n\n    If override is passed in, ignores the attr attribute and returns a\n    static image for the override boolean with no user interaction possible\n    (useful for \"disabled and you can't change it\" situations).\n    \"\"\"\n    if text:\n        text = '&nbsp;(%s)' % unicode(text)\n\n    if override is not None:\n        a = [django_boolean_icon(override, text), text]\n    else:\n        value = getattr(item, attr)\n        a = [\n            '<input type=\"checkbox\"',\n            value and ' checked=\"checked\"' or '',\n            ' onclick=\"return inplace_toggle_boolean(%d, \\'%s\\')\";' % (item.id, attr),\n            ' />',\n            text,\n        ]\n\n    a.insert(0, '<div id=\"wrap_%s_%d\">' % ( attr, item.id ))\n    a.append('</div>')\n    return unicode(''.join(a))", "code_tokens": ["def", "ajax_editable_boolean_cell", "(", "item", ",", "attr", ",", "text", "=", "''", ",", "override", "=", "None", ")", ":", "if", "text", ":", "text", "=", "'&nbsp;(%s)'", "%", "unicode", "(", "text", ")", "if", "override", "is", "not", "None", ":", "a", "=", "[", "django_boolean_icon", "(", "override", ",", "text", ")", ",", "text", "]", "else", ":", "value", "=", "getattr", "(", "item", ",", "attr", ")", "a", "=", "[", "'<input type=\"checkbox\"'", ",", "value", "and", "' checked=\"checked\"'", "or", "''", ",", "' onclick=\"return inplace_toggle_boolean(%d, \\'%s\\')\";'", "%", "(", "item", ".", "id", ",", "attr", ")", ",", "' />'", ",", "text", ",", "]", "a", ".", "insert", "(", "0", ",", "'<div id=\"wrap_%s_%d\">'", "%", "(", "attr", ",", "item", ".", "id", ")", ")", "a", ".", "append", "(", "'</div>'", ")", "return", "unicode", "(", "''", ".", "join", "(", "a", ")", ")"], "docstring": "Generate a html snippet for showing a boolean value on the admin page.\n    Item is an object, attr is the attribute name we should display. Text\n    is an optional explanatory text to be included in the output.\n\n    This function will emit code to produce a checkbox input with its state\n    corresponding to the item.attr attribute if no override value is passed.\n    This input is wired to run a JS ajax updater to toggle the value.\n\n    If override is passed in, ignores the attr attribute and returns a\n    static image for the override boolean with no user interaction possible\n    (useful for \"disabled and you can't change it\" situations).", "docstring_tokens": ["Generate", "a", "html", "snippet", "for", "showing", "a", "boolean", "value", "on", "the", "admin", "page", ".", "Item", "is", "an", "object", "attr", "is", "the", "attribute", "name", "we", "should", "display", ".", "Text", "is", "an", "optional", "explanatory", "text", "to", "be", "included", "in", "the", "output", "."], "sha": "f89d459b1348961880cd488df95690e68529f96b", "url": "https://github.com/20tab/twentytab-treeeditor/blob/f89d459b1348961880cd488df95690e68529f96b/treeeditor/admin.py#L61-L92", "partition": "test"}
{"repo": "mental32/spotify.py", "path": "spotify/http.py", "func_name": "HTTPClient.recommendations", "original_string": "def recommendations(self, seed_artists, seed_genres, seed_tracks, *, limit=20, market=None, **filters):\n        \"\"\"Get Recommendations Based on Seeds.\n\n        Parameters\n        ----------\n        seed_artists : str\n            A comma separated list of Spotify IDs for seed artists. Up to 5 seed values may be provided.\n        seed_genres : str\n            A comma separated list of any genres in the set of available genre seeds. Up to 5 seed values may be provided.\n        seed_tracks : str\n            A comma separated list of Spotify IDs for a seed track. Up to 5 seed values may be provided.\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code.\n        max_* : Optional[Keyword arguments]\n            For each tunable track attribute, a hard ceiling on the selected track attribute\u2019s value can be provided.\n        min_* : Optional[Keyword arguments]\n            For each tunable track attribute, a hard floor on the selected track attribute\u2019s value can be provided.\n        target_* : Optional[Keyword arguments]\n            For each of the tunable track attributes (below) a target value may be provided.\n        \"\"\"\n        route = Route('GET', '/recommendations')\n        payload = {'seed_artists': seed_artists, 'seed_genres': seed_genres, 'seed_tracks': seed_tracks, 'limit': limit}\n\n        if market:\n            payload['market'] = market\n\n        if filters:\n            payload.update(filters)\n\n        return self.request(route, param=payload)", "language": "python", "code": "def recommendations(self, seed_artists, seed_genres, seed_tracks, *, limit=20, market=None, **filters):\n        \"\"\"Get Recommendations Based on Seeds.\n\n        Parameters\n        ----------\n        seed_artists : str\n            A comma separated list of Spotify IDs for seed artists. Up to 5 seed values may be provided.\n        seed_genres : str\n            A comma separated list of any genres in the set of available genre seeds. Up to 5 seed values may be provided.\n        seed_tracks : str\n            A comma separated list of Spotify IDs for a seed track. Up to 5 seed values may be provided.\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code.\n        max_* : Optional[Keyword arguments]\n            For each tunable track attribute, a hard ceiling on the selected track attribute\u2019s value can be provided.\n        min_* : Optional[Keyword arguments]\n            For each tunable track attribute, a hard floor on the selected track attribute\u2019s value can be provided.\n        target_* : Optional[Keyword arguments]\n            For each of the tunable track attributes (below) a target value may be provided.\n        \"\"\"\n        route = Route('GET', '/recommendations')\n        payload = {'seed_artists': seed_artists, 'seed_genres': seed_genres, 'seed_tracks': seed_tracks, 'limit': limit}\n\n        if market:\n            payload['market'] = market\n\n        if filters:\n            payload.update(filters)\n\n        return self.request(route, param=payload)", "code_tokens": ["def", "recommendations", "(", "self", ",", "seed_artists", ",", "seed_genres", ",", "seed_tracks", ",", "*", ",", "limit", "=", "20", ",", "market", "=", "None", ",", "*", "*", "filters", ")", ":", "route", "=", "Route", "(", "'GET'", ",", "'/recommendations'", ")", "payload", "=", "{", "'seed_artists'", ":", "seed_artists", ",", "'seed_genres'", ":", "seed_genres", ",", "'seed_tracks'", ":", "seed_tracks", ",", "'limit'", ":", "limit", "}", "if", "market", ":", "payload", "[", "'market'", "]", "=", "market", "if", "filters", ":", "payload", ".", "update", "(", "filters", ")", "return", "self", ".", "request", "(", "route", ",", "param", "=", "payload", ")"], "docstring": "Get Recommendations Based on Seeds.\n\n        Parameters\n        ----------\n        seed_artists : str\n            A comma separated list of Spotify IDs for seed artists. Up to 5 seed values may be provided.\n        seed_genres : str\n            A comma separated list of any genres in the set of available genre seeds. Up to 5 seed values may be provided.\n        seed_tracks : str\n            A comma separated list of Spotify IDs for a seed track. Up to 5 seed values may be provided.\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code.\n        max_* : Optional[Keyword arguments]\n            For each tunable track attribute, a hard ceiling on the selected track attribute\u2019s value can be provided.\n        min_* : Optional[Keyword arguments]\n            For each tunable track attribute, a hard floor on the selected track attribute\u2019s value can be provided.\n        target_* : Optional[Keyword arguments]\n            For each of the tunable track attributes (below) a target value may be provided.", "docstring_tokens": ["Get", "Recommendations", "Based", "on", "Seeds", "."], "sha": "bb296cac7c3dd289908906b7069bd80f43950515", "url": "https://github.com/mental32/spotify.py/blob/bb296cac7c3dd289908906b7069bd80f43950515/spotify/http.py#L429-L460", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/circuit/quantumcircuit.py", "func_name": "QuantumCircuit._check_dups", "original_string": "def _check_dups(self, qubits):\n        \"\"\"Raise exception if list of qubits contains duplicates.\"\"\"\n        squbits = set(qubits)\n        if len(squbits) != len(qubits):\n            raise QiskitError(\"duplicate qubit arguments\")", "language": "python", "code": "def _check_dups(self, qubits):\n        \"\"\"Raise exception if list of qubits contains duplicates.\"\"\"\n        squbits = set(qubits)\n        if len(squbits) != len(qubits):\n            raise QiskitError(\"duplicate qubit arguments\")", "code_tokens": ["def", "_check_dups", "(", "self", ",", "qubits", ")", ":", "squbits", "=", "set", "(", "qubits", ")", "if", "len", "(", "squbits", ")", "!=", "len", "(", "qubits", ")", ":", "raise", "QiskitError", "(", "\"duplicate qubit arguments\"", ")"], "docstring": "Raise exception if list of qubits contains duplicates.", "docstring_tokens": ["Raise", "exception", "if", "list", "of", "qubits", "contains", "duplicates", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/circuit/quantumcircuit.py#L325-L329", "partition": "test"}
{"repo": "reingart/gui2py", "path": "gui/resource.py", "func_name": "save", "original_string": "def save(filename, rsrc):\n    \"Save the resource to the source file\"\n    s = pprint.pformat(rsrc)\n    ## s = s.encode(\"utf8\")\n    open(filename, \"w\").write(s)", "language": "python", "code": "def save(filename, rsrc):\n    \"Save the resource to the source file\"\n    s = pprint.pformat(rsrc)\n    ## s = s.encode(\"utf8\")\n    open(filename, \"w\").write(s)", "code_tokens": ["def", "save", "(", "filename", ",", "rsrc", ")", ":", "s", "=", "pprint", ".", "pformat", "(", "rsrc", ")", "## s = s.encode(\"utf8\")", "open", "(", "filename", ",", "\"w\"", ")", ".", "write", "(", "s", ")"], "docstring": "Save the resource to the source file", "docstring_tokens": ["Save", "the", "resource", "to", "the", "source", "file"], "sha": "aca0a05f6fcde55c94ad7cc058671a06608b01a4", "url": "https://github.com/reingart/gui2py/blob/aca0a05f6fcde55c94ad7cc058671a06608b01a4/gui/resource.py#L34-L38", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "embed_check_categorical_event_shape", "original_string": "def embed_check_categorical_event_shape(\n    categorical_param, name=\"embed_check_categorical_event_shape\"):\n  \"\"\"Embeds checks that categorical distributions don't have too many classes.\n\n  A categorical-type distribution is one which, e.g., returns the class label\n  rather than a one-hot encoding.  E.g., `Categorical(probs)`.\n\n  Since distributions output samples in the same dtype as the parameters, we\n  must ensure that casting doesn't lose precision. That is, the\n  `parameter.dtype` implies a maximum number of classes. However, since shape is\n  `int32` and categorical variables are presumed to be indexes into a `Tensor`,\n  we must also ensure that the number of classes is no larger than the largest\n  possible `int32` index, i.e., `2**31-1`.\n\n  In other words the number of classes, `K`, must satisfy the following\n  condition:\n\n  ```python\n  K <= min(\n      int(2**31 - 1),  # Largest float as an index.\n      {\n          tf.float16: int(2**11),   # Largest int as a float16.\n          tf.float32: int(2**24),\n          tf.float64: int(2**53),\n      }.get(dtype_util.base_dtype(categorical_param.dtype), 0))\n  ```\n\n  Args:\n    categorical_param: Floating-point `Tensor` representing parameters of\n      distribution over categories. The rightmost shape is presumed to be the\n      number of categories.\n    name: A name for this operation (optional).\n\n  Returns:\n    categorical_param: Input `Tensor` with appropriate assertions embedded.\n\n  Raises:\n    TypeError: if `categorical_param` has an unknown `dtype`.\n    ValueError: if we can statically identify `categorical_param` as being too\n      large (for being closed under int32/float casting).\n  \"\"\"\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=categorical_param, name=\"categorical_param\")\n    # The size must not exceed both of:\n    # - The largest possible int32 (since categorical values are presumed to be\n    #   indexes into a Tensor).\n    # - The largest possible integer exactly representable under the given\n    #   floating-point dtype (since we need to cast to/from).\n    #\n    # The chosen floating-point thresholds are 2**(1 + mantissa_bits).\n    # For more details, see:\n    # https://en.wikipedia.org/wiki/Floating-point_arithmetic#Internal_representation\n    x_dtype = dtype_util.base_dtype(x.dtype)\n    max_event_size = (\n        _largest_integer_by_dtype(x_dtype)\n        if dtype_util.is_floating(x_dtype) else 0)\n    if max_event_size is 0:\n      raise TypeError(\"Unable to validate size of unrecognized dtype \"\n                      \"({}).\".format(dtype_util.name(x_dtype)))\n    try:\n      x_shape_static = tensorshape_util.with_rank_at_least(x.shape, 1)\n    except ValueError:\n      raise ValueError(\"A categorical-distribution parameter must have \"\n                       \"at least 1 dimension.\")\n    event_size = tf.compat.dimension_value(x_shape_static[-1])\n    if event_size is not None:\n      if event_size < 2:\n        raise ValueError(\"A categorical-distribution parameter must have at \"\n                         \"least 2 events.\")\n      if event_size > max_event_size:\n        raise ValueError(\"Number of classes exceeds `dtype` precision, i.e., \"\n                         \"{} implies shape ({}) cannot exceed {}.\".format(\n                             dtype_util.name(x_dtype), event_size,\n                             max_event_size))\n      return x\n    else:\n      event_size = tf.shape(input=x, out_type=tf.int64, name=\"x_shape\")[-1]\n      return with_dependencies([\n          assert_util.assert_rank_at_least(\n              x,\n              1,\n              message=(\"A categorical-distribution parameter must have \"\n                       \"at least 1 dimension.\")),\n          assert_util.assert_greater_equal(\n              tf.shape(input=x)[-1],\n              2,\n              message=(\"A categorical-distribution parameter must have at \"\n                       \"least 2 events.\")),\n          assert_util.assert_less_equal(\n              event_size,\n              tf.convert_to_tensor(max_event_size, dtype=tf.int64),\n              message=\"Number of classes exceeds `dtype` precision, \"\n              \"i.e., {} dtype cannot exceed {} shape.\".format(\n                  dtype_util.name(x_dtype), max_event_size)),\n      ], x)", "language": "python", "code": "def embed_check_categorical_event_shape(\n    categorical_param, name=\"embed_check_categorical_event_shape\"):\n  \"\"\"Embeds checks that categorical distributions don't have too many classes.\n\n  A categorical-type distribution is one which, e.g., returns the class label\n  rather than a one-hot encoding.  E.g., `Categorical(probs)`.\n\n  Since distributions output samples in the same dtype as the parameters, we\n  must ensure that casting doesn't lose precision. That is, the\n  `parameter.dtype` implies a maximum number of classes. However, since shape is\n  `int32` and categorical variables are presumed to be indexes into a `Tensor`,\n  we must also ensure that the number of classes is no larger than the largest\n  possible `int32` index, i.e., `2**31-1`.\n\n  In other words the number of classes, `K`, must satisfy the following\n  condition:\n\n  ```python\n  K <= min(\n      int(2**31 - 1),  # Largest float as an index.\n      {\n          tf.float16: int(2**11),   # Largest int as a float16.\n          tf.float32: int(2**24),\n          tf.float64: int(2**53),\n      }.get(dtype_util.base_dtype(categorical_param.dtype), 0))\n  ```\n\n  Args:\n    categorical_param: Floating-point `Tensor` representing parameters of\n      distribution over categories. The rightmost shape is presumed to be the\n      number of categories.\n    name: A name for this operation (optional).\n\n  Returns:\n    categorical_param: Input `Tensor` with appropriate assertions embedded.\n\n  Raises:\n    TypeError: if `categorical_param` has an unknown `dtype`.\n    ValueError: if we can statically identify `categorical_param` as being too\n      large (for being closed under int32/float casting).\n  \"\"\"\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=categorical_param, name=\"categorical_param\")\n    # The size must not exceed both of:\n    # - The largest possible int32 (since categorical values are presumed to be\n    #   indexes into a Tensor).\n    # - The largest possible integer exactly representable under the given\n    #   floating-point dtype (since we need to cast to/from).\n    #\n    # The chosen floating-point thresholds are 2**(1 + mantissa_bits).\n    # For more details, see:\n    # https://en.wikipedia.org/wiki/Floating-point_arithmetic#Internal_representation\n    x_dtype = dtype_util.base_dtype(x.dtype)\n    max_event_size = (\n        _largest_integer_by_dtype(x_dtype)\n        if dtype_util.is_floating(x_dtype) else 0)\n    if max_event_size is 0:\n      raise TypeError(\"Unable to validate size of unrecognized dtype \"\n                      \"({}).\".format(dtype_util.name(x_dtype)))\n    try:\n      x_shape_static = tensorshape_util.with_rank_at_least(x.shape, 1)\n    except ValueError:\n      raise ValueError(\"A categorical-distribution parameter must have \"\n                       \"at least 1 dimension.\")\n    event_size = tf.compat.dimension_value(x_shape_static[-1])\n    if event_size is not None:\n      if event_size < 2:\n        raise ValueError(\"A categorical-distribution parameter must have at \"\n                         \"least 2 events.\")\n      if event_size > max_event_size:\n        raise ValueError(\"Number of classes exceeds `dtype` precision, i.e., \"\n                         \"{} implies shape ({}) cannot exceed {}.\".format(\n                             dtype_util.name(x_dtype), event_size,\n                             max_event_size))\n      return x\n    else:\n      event_size = tf.shape(input=x, out_type=tf.int64, name=\"x_shape\")[-1]\n      return with_dependencies([\n          assert_util.assert_rank_at_least(\n              x,\n              1,\n              message=(\"A categorical-distribution parameter must have \"\n                       \"at least 1 dimension.\")),\n          assert_util.assert_greater_equal(\n              tf.shape(input=x)[-1],\n              2,\n              message=(\"A categorical-distribution parameter must have at \"\n                       \"least 2 events.\")),\n          assert_util.assert_less_equal(\n              event_size,\n              tf.convert_to_tensor(max_event_size, dtype=tf.int64),\n              message=\"Number of classes exceeds `dtype` precision, \"\n              \"i.e., {} dtype cannot exceed {} shape.\".format(\n                  dtype_util.name(x_dtype), max_event_size)),\n      ], x)", "code_tokens": ["def", "embed_check_categorical_event_shape", "(", "categorical_param", ",", "name", "=", "\"embed_check_categorical_event_shape\"", ")", ":", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "categorical_param", ",", "name", "=", "\"categorical_param\"", ")", "# The size must not exceed both of:", "# - The largest possible int32 (since categorical values are presumed to be", "#   indexes into a Tensor).", "# - The largest possible integer exactly representable under the given", "#   floating-point dtype (since we need to cast to/from).", "#", "# The chosen floating-point thresholds are 2**(1 + mantissa_bits).", "# For more details, see:", "# https://en.wikipedia.org/wiki/Floating-point_arithmetic#Internal_representation", "x_dtype", "=", "dtype_util", ".", "base_dtype", "(", "x", ".", "dtype", ")", "max_event_size", "=", "(", "_largest_integer_by_dtype", "(", "x_dtype", ")", "if", "dtype_util", ".", "is_floating", "(", "x_dtype", ")", "else", "0", ")", "if", "max_event_size", "is", "0", ":", "raise", "TypeError", "(", "\"Unable to validate size of unrecognized dtype \"", "\"({}).\"", ".", "format", "(", "dtype_util", ".", "name", "(", "x_dtype", ")", ")", ")", "try", ":", "x_shape_static", "=", "tensorshape_util", ".", "with_rank_at_least", "(", "x", ".", "shape", ",", "1", ")", "except", "ValueError", ":", "raise", "ValueError", "(", "\"A categorical-distribution parameter must have \"", "\"at least 1 dimension.\"", ")", "event_size", "=", "tf", ".", "compat", ".", "dimension_value", "(", "x_shape_static", "[", "-", "1", "]", ")", "if", "event_size", "is", "not", "None", ":", "if", "event_size", "<", "2", ":", "raise", "ValueError", "(", "\"A categorical-distribution parameter must have at \"", "\"least 2 events.\"", ")", "if", "event_size", ">", "max_event_size", ":", "raise", "ValueError", "(", "\"Number of classes exceeds `dtype` precision, i.e., \"", "\"{} implies shape ({}) cannot exceed {}.\"", ".", "format", "(", "dtype_util", ".", "name", "(", "x_dtype", ")", ",", "event_size", ",", "max_event_size", ")", ")", "return", "x", "else", ":", "event_size", "=", "tf", ".", "shape", "(", "input", "=", "x", ",", "out_type", "=", "tf", ".", "int64", ",", "name", "=", "\"x_shape\"", ")", "[", "-", "1", "]", "return", "with_dependencies", "(", "[", "assert_util", ".", "assert_rank_at_least", "(", "x", ",", "1", ",", "message", "=", "(", "\"A categorical-distribution parameter must have \"", "\"at least 1 dimension.\"", ")", ")", ",", "assert_util", ".", "assert_greater_equal", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", "[", "-", "1", "]", ",", "2", ",", "message", "=", "(", "\"A categorical-distribution parameter must have at \"", "\"least 2 events.\"", ")", ")", ",", "assert_util", ".", "assert_less_equal", "(", "event_size", ",", "tf", ".", "convert_to_tensor", "(", "max_event_size", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "message", "=", "\"Number of classes exceeds `dtype` precision, \"", "\"i.e., {} dtype cannot exceed {} shape.\"", ".", "format", "(", "dtype_util", ".", "name", "(", "x_dtype", ")", ",", "max_event_size", ")", ")", ",", "]", ",", "x", ")"], "docstring": "Embeds checks that categorical distributions don't have too many classes.\n\n  A categorical-type distribution is one which, e.g., returns the class label\n  rather than a one-hot encoding.  E.g., `Categorical(probs)`.\n\n  Since distributions output samples in the same dtype as the parameters, we\n  must ensure that casting doesn't lose precision. That is, the\n  `parameter.dtype` implies a maximum number of classes. However, since shape is\n  `int32` and categorical variables are presumed to be indexes into a `Tensor`,\n  we must also ensure that the number of classes is no larger than the largest\n  possible `int32` index, i.e., `2**31-1`.\n\n  In other words the number of classes, `K`, must satisfy the following\n  condition:\n\n  ```python\n  K <= min(\n      int(2**31 - 1),  # Largest float as an index.\n      {\n          tf.float16: int(2**11),   # Largest int as a float16.\n          tf.float32: int(2**24),\n          tf.float64: int(2**53),\n      }.get(dtype_util.base_dtype(categorical_param.dtype), 0))\n  ```\n\n  Args:\n    categorical_param: Floating-point `Tensor` representing parameters of\n      distribution over categories. The rightmost shape is presumed to be the\n      number of categories.\n    name: A name for this operation (optional).\n\n  Returns:\n    categorical_param: Input `Tensor` with appropriate assertions embedded.\n\n  Raises:\n    TypeError: if `categorical_param` has an unknown `dtype`.\n    ValueError: if we can statically identify `categorical_param` as being too\n      large (for being closed under int32/float casting).", "docstring_tokens": ["Embeds", "checks", "that", "categorical", "distributions", "don", "t", "have", "too", "many", "classes", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L905-L999", "partition": "test"}
{"repo": "nkgilley/python-ecobee-api", "path": "pyecobee/__init__.py", "func_name": "Ecobee.set_humidity", "original_string": "def set_humidity(self, index, humidity):\n        ''' Set humidity level'''\n        body = {\"selection\": {\"selectionType\": \"thermostats\",\n                              \"selectionMatch\": self.thermostats[index]['identifier']},\n                              \"thermostat\": {\n                                  \"settings\": {\n                                      \"humidity\": int(humidity)\n                                  }\n                              }}\n\n        log_msg_action = \"set humidity level\"\n        return self.make_request(body, log_msg_action)", "language": "python", "code": "def set_humidity(self, index, humidity):\n        ''' Set humidity level'''\n        body = {\"selection\": {\"selectionType\": \"thermostats\",\n                              \"selectionMatch\": self.thermostats[index]['identifier']},\n                              \"thermostat\": {\n                                  \"settings\": {\n                                      \"humidity\": int(humidity)\n                                  }\n                              }}\n\n        log_msg_action = \"set humidity level\"\n        return self.make_request(body, log_msg_action)", "code_tokens": ["def", "set_humidity", "(", "self", ",", "index", ",", "humidity", ")", ":", "body", "=", "{", "\"selection\"", ":", "{", "\"selectionType\"", ":", "\"thermostats\"", ",", "\"selectionMatch\"", ":", "self", ".", "thermostats", "[", "index", "]", "[", "'identifier'", "]", "}", ",", "\"thermostat\"", ":", "{", "\"settings\"", ":", "{", "\"humidity\"", ":", "int", "(", "humidity", ")", "}", "}", "}", "log_msg_action", "=", "\"set humidity level\"", "return", "self", ".", "make_request", "(", "body", ",", "log_msg_action", ")"], "docstring": "Set humidity level", "docstring_tokens": ["Set", "humidity", "level"], "sha": "cc8d90d20abcb9ef5b66ec9cb035bae2f06ba174", "url": "https://github.com/nkgilley/python-ecobee-api/blob/cc8d90d20abcb9ef5b66ec9cb035bae2f06ba174/pyecobee/__init__.py#L307-L318", "partition": "test"}
{"repo": "xtuml/pyxtuml", "path": "xtuml/meta.py", "func_name": "MetaModel.define_class", "original_string": "def define_class(self, kind, attributes, doc=''):\n        '''\n        Define a new class in the metamodel, and return its metaclass.\n        '''\n        ukind = kind.upper()\n        if ukind in self.metaclasses:\n            raise MetaModelException('A class with the name %s is already defined' % kind)\n\n        metaclass = MetaClass(kind, self)\n        for name, ty in attributes:\n            metaclass.append_attribute(name, ty)\n            \n        self.metaclasses[ukind] = metaclass\n        \n        return metaclass", "language": "python", "code": "def define_class(self, kind, attributes, doc=''):\n        '''\n        Define a new class in the metamodel, and return its metaclass.\n        '''\n        ukind = kind.upper()\n        if ukind in self.metaclasses:\n            raise MetaModelException('A class with the name %s is already defined' % kind)\n\n        metaclass = MetaClass(kind, self)\n        for name, ty in attributes:\n            metaclass.append_attribute(name, ty)\n            \n        self.metaclasses[ukind] = metaclass\n        \n        return metaclass", "code_tokens": ["def", "define_class", "(", "self", ",", "kind", ",", "attributes", ",", "doc", "=", "''", ")", ":", "ukind", "=", "kind", ".", "upper", "(", ")", "if", "ukind", "in", "self", ".", "metaclasses", ":", "raise", "MetaModelException", "(", "'A class with the name %s is already defined'", "%", "kind", ")", "metaclass", "=", "MetaClass", "(", "kind", ",", "self", ")", "for", "name", ",", "ty", "in", "attributes", ":", "metaclass", ".", "append_attribute", "(", "name", ",", "ty", ")", "self", ".", "metaclasses", "[", "ukind", "]", "=", "metaclass", "return", "metaclass"], "docstring": "Define a new class in the metamodel, and return its metaclass.", "docstring_tokens": ["Define", "a", "new", "class", "in", "the", "metamodel", "and", "return", "its", "metaclass", "."], "sha": "7dd9343b9a0191d1db1887ab9288d0a026608d9a", "url": "https://github.com/xtuml/pyxtuml/blob/7dd9343b9a0191d1db1887ab9288d0a026608d9a/xtuml/meta.py#L1156-L1170", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/tools/jupyter/backend_overview.py", "func_name": "update_backend_info", "original_string": "def update_backend_info(self, interval=60):\n    \"\"\"Updates the monitor info\n    Called from another thread.\n    \"\"\"\n    my_thread = threading.currentThread()\n    current_interval = 0\n    started = False\n    all_dead = False\n    stati = [None]*len(self._backends)\n    while getattr(my_thread, \"do_run\", True) and not all_dead:\n        if current_interval == interval or started is False:\n            for ind, back in enumerate(self._backends):\n                _value = self.children[ind].children[2].value\n                _head = _value.split('<b>')[0]\n                try:\n                    _status = back.status()\n                    stati[ind] = _status\n                except Exception:  # pylint: disable=W0703\n                    self.children[ind].children[2].value = _value.replace(\n                        _head, \"<h5 style='color:#ff5c49'>\")\n                    self.children[ind]._is_alive = False\n                else:\n                    self.children[ind]._is_alive = True\n                    self.children[ind].children[2].value = _value.replace(\n                        _head, \"<h5>\")\n\n            idx = list(range(len(self._backends)))\n            pending = [s.pending_jobs for s in stati]\n            _, least_idx = zip(*sorted(zip(pending, idx)))\n\n            # Make sure least pending is operational\n            for ind in least_idx:\n                if stati[ind].operational:\n                    least_pending_idx = ind\n                    break\n\n            for var in idx:\n                if var == least_pending_idx:\n                    self.children[var].children[4].value = \"<h5 style='color:#34bc6e'>True</h5>\"\n                else:\n                    self.children[var].children[4].value = \"<h5 style='color:#dc267f'>False</h5>\"\n\n                self.children[var].children[3].children[1].value = pending[var]\n                self.children[var].children[3].children[1].max = max(\n                    self.children[var].children[3].children[1].max, pending[var]+10)\n                if stati[var].operational:\n                    self.children[var].children[5].value = \"<h5 style='color:#34bc6e'>True</h5>\"\n                else:\n                    self.children[var].children[5].value = \"<h5 style='color:#dc267f'>False</h5>\"\n\n            started = True\n            current_interval = 0\n        time.sleep(1)\n        all_dead = not any([wid._is_alive for wid in self.children])\n        current_interval += 1", "language": "python", "code": "def update_backend_info(self, interval=60):\n    \"\"\"Updates the monitor info\n    Called from another thread.\n    \"\"\"\n    my_thread = threading.currentThread()\n    current_interval = 0\n    started = False\n    all_dead = False\n    stati = [None]*len(self._backends)\n    while getattr(my_thread, \"do_run\", True) and not all_dead:\n        if current_interval == interval or started is False:\n            for ind, back in enumerate(self._backends):\n                _value = self.children[ind].children[2].value\n                _head = _value.split('<b>')[0]\n                try:\n                    _status = back.status()\n                    stati[ind] = _status\n                except Exception:  # pylint: disable=W0703\n                    self.children[ind].children[2].value = _value.replace(\n                        _head, \"<h5 style='color:#ff5c49'>\")\n                    self.children[ind]._is_alive = False\n                else:\n                    self.children[ind]._is_alive = True\n                    self.children[ind].children[2].value = _value.replace(\n                        _head, \"<h5>\")\n\n            idx = list(range(len(self._backends)))\n            pending = [s.pending_jobs for s in stati]\n            _, least_idx = zip(*sorted(zip(pending, idx)))\n\n            # Make sure least pending is operational\n            for ind in least_idx:\n                if stati[ind].operational:\n                    least_pending_idx = ind\n                    break\n\n            for var in idx:\n                if var == least_pending_idx:\n                    self.children[var].children[4].value = \"<h5 style='color:#34bc6e'>True</h5>\"\n                else:\n                    self.children[var].children[4].value = \"<h5 style='color:#dc267f'>False</h5>\"\n\n                self.children[var].children[3].children[1].value = pending[var]\n                self.children[var].children[3].children[1].max = max(\n                    self.children[var].children[3].children[1].max, pending[var]+10)\n                if stati[var].operational:\n                    self.children[var].children[5].value = \"<h5 style='color:#34bc6e'>True</h5>\"\n                else:\n                    self.children[var].children[5].value = \"<h5 style='color:#dc267f'>False</h5>\"\n\n            started = True\n            current_interval = 0\n        time.sleep(1)\n        all_dead = not any([wid._is_alive for wid in self.children])\n        current_interval += 1", "code_tokens": ["def", "update_backend_info", "(", "self", ",", "interval", "=", "60", ")", ":", "my_thread", "=", "threading", ".", "currentThread", "(", ")", "current_interval", "=", "0", "started", "=", "False", "all_dead", "=", "False", "stati", "=", "[", "None", "]", "*", "len", "(", "self", ".", "_backends", ")", "while", "getattr", "(", "my_thread", ",", "\"do_run\"", ",", "True", ")", "and", "not", "all_dead", ":", "if", "current_interval", "==", "interval", "or", "started", "is", "False", ":", "for", "ind", ",", "back", "in", "enumerate", "(", "self", ".", "_backends", ")", ":", "_value", "=", "self", ".", "children", "[", "ind", "]", ".", "children", "[", "2", "]", ".", "value", "_head", "=", "_value", ".", "split", "(", "'<b>'", ")", "[", "0", "]", "try", ":", "_status", "=", "back", ".", "status", "(", ")", "stati", "[", "ind", "]", "=", "_status", "except", "Exception", ":", "# pylint: disable=W0703", "self", ".", "children", "[", "ind", "]", ".", "children", "[", "2", "]", ".", "value", "=", "_value", ".", "replace", "(", "_head", ",", "\"<h5 style='color:#ff5c49'>\"", ")", "self", ".", "children", "[", "ind", "]", ".", "_is_alive", "=", "False", "else", ":", "self", ".", "children", "[", "ind", "]", ".", "_is_alive", "=", "True", "self", ".", "children", "[", "ind", "]", ".", "children", "[", "2", "]", ".", "value", "=", "_value", ".", "replace", "(", "_head", ",", "\"<h5>\"", ")", "idx", "=", "list", "(", "range", "(", "len", "(", "self", ".", "_backends", ")", ")", ")", "pending", "=", "[", "s", ".", "pending_jobs", "for", "s", "in", "stati", "]", "_", ",", "least_idx", "=", "zip", "(", "*", "sorted", "(", "zip", "(", "pending", ",", "idx", ")", ")", ")", "# Make sure least pending is operational", "for", "ind", "in", "least_idx", ":", "if", "stati", "[", "ind", "]", ".", "operational", ":", "least_pending_idx", "=", "ind", "break", "for", "var", "in", "idx", ":", "if", "var", "==", "least_pending_idx", ":", "self", ".", "children", "[", "var", "]", ".", "children", "[", "4", "]", ".", "value", "=", "\"<h5 style='color:#34bc6e'>True</h5>\"", "else", ":", "self", ".", "children", "[", "var", "]", ".", "children", "[", "4", "]", ".", "value", "=", "\"<h5 style='color:#dc267f'>False</h5>\"", "self", ".", "children", "[", "var", "]", ".", "children", "[", "3", "]", ".", "children", "[", "1", "]", ".", "value", "=", "pending", "[", "var", "]", "self", ".", "children", "[", "var", "]", ".", "children", "[", "3", "]", ".", "children", "[", "1", "]", ".", "max", "=", "max", "(", "self", ".", "children", "[", "var", "]", ".", "children", "[", "3", "]", ".", "children", "[", "1", "]", ".", "max", ",", "pending", "[", "var", "]", "+", "10", ")", "if", "stati", "[", "var", "]", ".", "operational", ":", "self", ".", "children", "[", "var", "]", ".", "children", "[", "5", "]", ".", "value", "=", "\"<h5 style='color:#34bc6e'>True</h5>\"", "else", ":", "self", ".", "children", "[", "var", "]", ".", "children", "[", "5", "]", ".", "value", "=", "\"<h5 style='color:#dc267f'>False</h5>\"", "started", "=", "True", "current_interval", "=", "0", "time", ".", "sleep", "(", "1", ")", "all_dead", "=", "not", "any", "(", "[", "wid", ".", "_is_alive", "for", "wid", "in", "self", ".", "children", "]", ")", "current_interval", "+=", "1"], "docstring": "Updates the monitor info\n    Called from another thread.", "docstring_tokens": ["Updates", "the", "monitor", "info", "Called", "from", "another", "thread", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/tools/jupyter/backend_overview.py#L170-L224", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/core/time_frequency.py", "func_name": "A_weighting", "original_string": "def A_weighting(frequencies, min_db=-80.0):     # pylint: disable=invalid-name\n    '''Compute the A-weighting of a set of frequencies.\n\n    Parameters\n    ----------\n    frequencies : scalar or np.ndarray [shape=(n,)]\n        One or more frequencies (in Hz)\n\n    min_db : float [scalar] or None\n        Clip weights below this threshold.\n        If `None`, no clipping is performed.\n\n    Returns\n    -------\n    A_weighting : scalar or np.ndarray [shape=(n,)]\n        `A_weighting[i]` is the A-weighting of `frequencies[i]`\n\n    See Also\n    --------\n    perceptual_weighting\n\n\n    Examples\n    --------\n\n    Get the A-weighting for CQT frequencies\n\n    >>> import matplotlib.pyplot as plt\n    >>> freqs = librosa.cqt_frequencies(108, librosa.note_to_hz('C1'))\n    >>> aw = librosa.A_weighting(freqs)\n    >>> plt.plot(freqs, aw)\n    >>> plt.xlabel('Frequency (Hz)')\n    >>> plt.ylabel('Weighting (log10)')\n    >>> plt.title('A-Weighting of CQT frequencies')\n\n    '''\n\n    # Vectorize to make our lives easier\n    frequencies = np.asanyarray(frequencies)\n\n    # Pre-compute squared frequency\n    f_sq = frequencies**2.0\n\n    const = np.array([12200, 20.6, 107.7, 737.9])**2.0\n\n    weights = 2.0 + 20.0 * (np.log10(const[0]) + 4 * np.log10(frequencies)\n                            - np.log10(f_sq + const[0])\n                            - np.log10(f_sq + const[1])\n                            - 0.5 * np.log10(f_sq + const[2])\n                            - 0.5 * np.log10(f_sq + const[3]))\n\n    if min_db is not None:\n        weights = np.maximum(min_db, weights)\n\n    return weights", "language": "python", "code": "def A_weighting(frequencies, min_db=-80.0):     # pylint: disable=invalid-name\n    '''Compute the A-weighting of a set of frequencies.\n\n    Parameters\n    ----------\n    frequencies : scalar or np.ndarray [shape=(n,)]\n        One or more frequencies (in Hz)\n\n    min_db : float [scalar] or None\n        Clip weights below this threshold.\n        If `None`, no clipping is performed.\n\n    Returns\n    -------\n    A_weighting : scalar or np.ndarray [shape=(n,)]\n        `A_weighting[i]` is the A-weighting of `frequencies[i]`\n\n    See Also\n    --------\n    perceptual_weighting\n\n\n    Examples\n    --------\n\n    Get the A-weighting for CQT frequencies\n\n    >>> import matplotlib.pyplot as plt\n    >>> freqs = librosa.cqt_frequencies(108, librosa.note_to_hz('C1'))\n    >>> aw = librosa.A_weighting(freqs)\n    >>> plt.plot(freqs, aw)\n    >>> plt.xlabel('Frequency (Hz)')\n    >>> plt.ylabel('Weighting (log10)')\n    >>> plt.title('A-Weighting of CQT frequencies')\n\n    '''\n\n    # Vectorize to make our lives easier\n    frequencies = np.asanyarray(frequencies)\n\n    # Pre-compute squared frequency\n    f_sq = frequencies**2.0\n\n    const = np.array([12200, 20.6, 107.7, 737.9])**2.0\n\n    weights = 2.0 + 20.0 * (np.log10(const[0]) + 4 * np.log10(frequencies)\n                            - np.log10(f_sq + const[0])\n                            - np.log10(f_sq + const[1])\n                            - 0.5 * np.log10(f_sq + const[2])\n                            - 0.5 * np.log10(f_sq + const[3]))\n\n    if min_db is not None:\n        weights = np.maximum(min_db, weights)\n\n    return weights", "code_tokens": ["def", "A_weighting", "(", "frequencies", ",", "min_db", "=", "-", "80.0", ")", ":", "# pylint: disable=invalid-name", "# Vectorize to make our lives easier", "frequencies", "=", "np", ".", "asanyarray", "(", "frequencies", ")", "# Pre-compute squared frequency", "f_sq", "=", "frequencies", "**", "2.0", "const", "=", "np", ".", "array", "(", "[", "12200", ",", "20.6", ",", "107.7", ",", "737.9", "]", ")", "**", "2.0", "weights", "=", "2.0", "+", "20.0", "*", "(", "np", ".", "log10", "(", "const", "[", "0", "]", ")", "+", "4", "*", "np", ".", "log10", "(", "frequencies", ")", "-", "np", ".", "log10", "(", "f_sq", "+", "const", "[", "0", "]", ")", "-", "np", ".", "log10", "(", "f_sq", "+", "const", "[", "1", "]", ")", "-", "0.5", "*", "np", ".", "log10", "(", "f_sq", "+", "const", "[", "2", "]", ")", "-", "0.5", "*", "np", ".", "log10", "(", "f_sq", "+", "const", "[", "3", "]", ")", ")", "if", "min_db", "is", "not", "None", ":", "weights", "=", "np", ".", "maximum", "(", "min_db", ",", "weights", ")", "return", "weights"], "docstring": "Compute the A-weighting of a set of frequencies.\n\n    Parameters\n    ----------\n    frequencies : scalar or np.ndarray [shape=(n,)]\n        One or more frequencies (in Hz)\n\n    min_db : float [scalar] or None\n        Clip weights below this threshold.\n        If `None`, no clipping is performed.\n\n    Returns\n    -------\n    A_weighting : scalar or np.ndarray [shape=(n,)]\n        `A_weighting[i]` is the A-weighting of `frequencies[i]`\n\n    See Also\n    --------\n    perceptual_weighting\n\n\n    Examples\n    --------\n\n    Get the A-weighting for CQT frequencies\n\n    >>> import matplotlib.pyplot as plt\n    >>> freqs = librosa.cqt_frequencies(108, librosa.note_to_hz('C1'))\n    >>> aw = librosa.A_weighting(freqs)\n    >>> plt.plot(freqs, aw)\n    >>> plt.xlabel('Frequency (Hz)')\n    >>> plt.ylabel('Weighting (log10)')\n    >>> plt.title('A-Weighting of CQT frequencies')", "docstring_tokens": ["Compute", "the", "A", "-", "weighting", "of", "a", "set", "of", "frequencies", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/core/time_frequency.py#L957-L1011", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/imports.py", "func_name": "_get_import_name", "original_string": "def _get_import_name(importnode, modname):\n    \"\"\"Get a prepared module name from the given import node\n\n    In the case of relative imports, this will return the\n    absolute qualified module name, which might be useful\n    for debugging. Otherwise, the initial module name\n    is returned unchanged.\n    \"\"\"\n    if isinstance(importnode, astroid.ImportFrom):\n        if importnode.level:\n            root = importnode.root()\n            if isinstance(root, astroid.Module):\n                modname = root.relative_to_absolute_name(\n                    modname, level=importnode.level\n                )\n    return modname", "language": "python", "code": "def _get_import_name(importnode, modname):\n    \"\"\"Get a prepared module name from the given import node\n\n    In the case of relative imports, this will return the\n    absolute qualified module name, which might be useful\n    for debugging. Otherwise, the initial module name\n    is returned unchanged.\n    \"\"\"\n    if isinstance(importnode, astroid.ImportFrom):\n        if importnode.level:\n            root = importnode.root()\n            if isinstance(root, astroid.Module):\n                modname = root.relative_to_absolute_name(\n                    modname, level=importnode.level\n                )\n    return modname", "code_tokens": ["def", "_get_import_name", "(", "importnode", ",", "modname", ")", ":", "if", "isinstance", "(", "importnode", ",", "astroid", ".", "ImportFrom", ")", ":", "if", "importnode", ".", "level", ":", "root", "=", "importnode", ".", "root", "(", ")", "if", "isinstance", "(", "root", ",", "astroid", ".", "Module", ")", ":", "modname", "=", "root", ".", "relative_to_absolute_name", "(", "modname", ",", "level", "=", "importnode", ".", "level", ")", "return", "modname"], "docstring": "Get a prepared module name from the given import node\n\n    In the case of relative imports, this will return the\n    absolute qualified module name, which might be useful\n    for debugging. Otherwise, the initial module name\n    is returned unchanged.", "docstring_tokens": ["Get", "a", "prepared", "module", "name", "from", "the", "given", "import", "node"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/imports.py#L67-L82", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/github.py", "func_name": "GitHub.__get_pull_commits", "original_string": "def __get_pull_commits(self, pr_number):\n        \"\"\"Get pull request commit hashes\"\"\"\n\n        hashes = []\n        group_pull_commits = self.client.pull_commits(pr_number)\n\n        for raw_pull_commits in group_pull_commits:\n\n            for commit in json.loads(raw_pull_commits):\n                commit_hash = commit['sha']\n                hashes.append(commit_hash)\n\n        return hashes", "language": "python", "code": "def __get_pull_commits(self, pr_number):\n        \"\"\"Get pull request commit hashes\"\"\"\n\n        hashes = []\n        group_pull_commits = self.client.pull_commits(pr_number)\n\n        for raw_pull_commits in group_pull_commits:\n\n            for commit in json.loads(raw_pull_commits):\n                commit_hash = commit['sha']\n                hashes.append(commit_hash)\n\n        return hashes", "code_tokens": ["def", "__get_pull_commits", "(", "self", ",", "pr_number", ")", ":", "hashes", "=", "[", "]", "group_pull_commits", "=", "self", ".", "client", ".", "pull_commits", "(", "pr_number", ")", "for", "raw_pull_commits", "in", "group_pull_commits", ":", "for", "commit", "in", "json", ".", "loads", "(", "raw_pull_commits", ")", ":", "commit_hash", "=", "commit", "[", "'sha'", "]", "hashes", ".", "append", "(", "commit_hash", ")", "return", "hashes"], "docstring": "Get pull request commit hashes", "docstring_tokens": ["Get", "pull", "request", "commit", "hashes"], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/github.py#L391-L403", "partition": "test"}
{"repo": "karimbahgat/PyCRS", "path": "pycrs/parse.py", "func_name": "from_epsg_code", "original_string": "def from_epsg_code(code):\n    \"\"\"\n    Load crs object from epsg code, via spatialreference.org.\n    Parses based on the proj4 representation.\n\n    Arguments:\n\n    - *code*: The EPSG code as an integer.\n\n    Returns:\n\n    - A CS instance of the indicated type. \n    \"\"\"\n    # must go online (or look up local table) to get crs details\n    code = str(code)\n    proj4 = utils.crscode_to_string(\"epsg\", code, \"proj4\")\n    crs = from_proj4(proj4)\n    return crs", "language": "python", "code": "def from_epsg_code(code):\n    \"\"\"\n    Load crs object from epsg code, via spatialreference.org.\n    Parses based on the proj4 representation.\n\n    Arguments:\n\n    - *code*: The EPSG code as an integer.\n\n    Returns:\n\n    - A CS instance of the indicated type. \n    \"\"\"\n    # must go online (or look up local table) to get crs details\n    code = str(code)\n    proj4 = utils.crscode_to_string(\"epsg\", code, \"proj4\")\n    crs = from_proj4(proj4)\n    return crs", "code_tokens": ["def", "from_epsg_code", "(", "code", ")", ":", "# must go online (or look up local table) to get crs details", "code", "=", "str", "(", "code", ")", "proj4", "=", "utils", ".", "crscode_to_string", "(", "\"epsg\"", ",", "code", ",", "\"proj4\"", ")", "crs", "=", "from_proj4", "(", "proj4", ")", "return", "crs"], "docstring": "Load crs object from epsg code, via spatialreference.org.\n    Parses based on the proj4 representation.\n\n    Arguments:\n\n    - *code*: The EPSG code as an integer.\n\n    Returns:\n\n    - A CS instance of the indicated type.", "docstring_tokens": ["Load", "crs", "object", "from", "epsg", "code", "via", "spatialreference", ".", "org", ".", "Parses", "based", "on", "the", "proj4", "representation", "."], "sha": "d6a8bb9c28787a25b4a1d59a7e4603db3221eaef", "url": "https://github.com/karimbahgat/PyCRS/blob/d6a8bb9c28787a25b4a1d59a7e4603db3221eaef/pycrs/parse.py#L24-L41", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/regression.py", "func_name": "_zero_dimensional_mvndiag", "original_string": "def _zero_dimensional_mvndiag(dtype):\n  \"\"\"Build a zero-dimensional MVNDiag object.\"\"\"\n  dummy_mvndiag = tfd.MultivariateNormalDiag(\n      scale_diag=tf.ones([0], dtype=dtype))\n  dummy_mvndiag.covariance = lambda: dummy_mvndiag.variance()[..., tf.newaxis]\n  return dummy_mvndiag", "language": "python", "code": "def _zero_dimensional_mvndiag(dtype):\n  \"\"\"Build a zero-dimensional MVNDiag object.\"\"\"\n  dummy_mvndiag = tfd.MultivariateNormalDiag(\n      scale_diag=tf.ones([0], dtype=dtype))\n  dummy_mvndiag.covariance = lambda: dummy_mvndiag.variance()[..., tf.newaxis]\n  return dummy_mvndiag", "code_tokens": ["def", "_zero_dimensional_mvndiag", "(", "dtype", ")", ":", "dummy_mvndiag", "=", "tfd", ".", "MultivariateNormalDiag", "(", "scale_diag", "=", "tf", ".", "ones", "(", "[", "0", "]", ",", "dtype", "=", "dtype", ")", ")", "dummy_mvndiag", ".", "covariance", "=", "lambda", ":", "dummy_mvndiag", ".", "variance", "(", ")", "[", "...", ",", "tf", ".", "newaxis", "]", "return", "dummy_mvndiag"], "docstring": "Build a zero-dimensional MVNDiag object.", "docstring_tokens": ["Build", "a", "zero", "-", "dimensional", "MVNDiag", "object", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/regression.py#L32-L37", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/pypetlogging.py", "func_name": "LoggingManager._find_multiproc_dict", "original_string": "def _find_multiproc_dict(dictionary):\n        \"\"\" Searches for multiprocessing options in a given `dictionary`.\n\n        If found they are copied (without the `'multiproc_'` prefix)\n        into a new dictionary\n\n        \"\"\"\n        if not any(key.startswith('multiproc_') for key in dictionary.keys()):\n            return None\n        mp_dictionary = {}\n        for key in dictionary.keys():\n            if key.startswith('multiproc_'):\n                new_key = key.replace('multiproc_', '')\n                mp_dictionary[new_key] = dictionary[key]\n        mp_dictionary['version'] = dictionary['version']\n        if 'disable_existing_loggers' in dictionary:\n            mp_dictionary['disable_existing_loggers'] = dictionary['disable_existing_loggers']\n        return mp_dictionary", "language": "python", "code": "def _find_multiproc_dict(dictionary):\n        \"\"\" Searches for multiprocessing options in a given `dictionary`.\n\n        If found they are copied (without the `'multiproc_'` prefix)\n        into a new dictionary\n\n        \"\"\"\n        if not any(key.startswith('multiproc_') for key in dictionary.keys()):\n            return None\n        mp_dictionary = {}\n        for key in dictionary.keys():\n            if key.startswith('multiproc_'):\n                new_key = key.replace('multiproc_', '')\n                mp_dictionary[new_key] = dictionary[key]\n        mp_dictionary['version'] = dictionary['version']\n        if 'disable_existing_loggers' in dictionary:\n            mp_dictionary['disable_existing_loggers'] = dictionary['disable_existing_loggers']\n        return mp_dictionary", "code_tokens": ["def", "_find_multiproc_dict", "(", "dictionary", ")", ":", "if", "not", "any", "(", "key", ".", "startswith", "(", "'multiproc_'", ")", "for", "key", "in", "dictionary", ".", "keys", "(", ")", ")", ":", "return", "None", "mp_dictionary", "=", "{", "}", "for", "key", "in", "dictionary", ".", "keys", "(", ")", ":", "if", "key", ".", "startswith", "(", "'multiproc_'", ")", ":", "new_key", "=", "key", ".", "replace", "(", "'multiproc_'", ",", "''", ")", "mp_dictionary", "[", "new_key", "]", "=", "dictionary", "[", "key", "]", "mp_dictionary", "[", "'version'", "]", "=", "dictionary", "[", "'version'", "]", "if", "'disable_existing_loggers'", "in", "dictionary", ":", "mp_dictionary", "[", "'disable_existing_loggers'", "]", "=", "dictionary", "[", "'disable_existing_loggers'", "]", "return", "mp_dictionary"], "docstring": "Searches for multiprocessing options in a given `dictionary`.\n\n        If found they are copied (without the `'multiproc_'` prefix)\n        into a new dictionary", "docstring_tokens": ["Searches", "for", "multiprocessing", "options", "in", "a", "given", "dictionary", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/pypetlogging.py#L479-L496", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/utils/dates.py", "func_name": "date_range", "original_string": "def date_range(start_date, end_date=None, num=None, delta=None):\n    \"\"\"\n    Get a set of dates as a list based on a start, end and delta, delta\n    can be something that can be added to `datetime.datetime`\n    or a cron expression as a `str`\n\n    :Example::\n\n        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta=timedelta(1))\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n            datetime.datetime(2016, 1, 3, 0, 0)]\n        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta='0 0 * * *')\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n            datetime.datetime(2016, 1, 3, 0, 0)]\n        date_range(datetime(2016, 1, 1), datetime(2016, 3, 3), delta=\"0 0 0 * *\")\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 2, 1, 0, 0),\n            datetime.datetime(2016, 3, 1, 0, 0)]\n\n    :param start_date: anchor date to start the series from\n    :type start_date: datetime.datetime\n    :param end_date: right boundary for the date range\n    :type end_date: datetime.datetime\n    :param num: alternatively to end_date, you can specify the number of\n        number of entries you want in the range. This number can be negative,\n        output will always be sorted regardless\n    :type num: int\n    \"\"\"\n    if not delta:\n        return []\n    if end_date and start_date > end_date:\n        raise Exception(\"Wait. start_date needs to be before end_date\")\n    if end_date and num:\n        raise Exception(\"Wait. Either specify end_date OR num\")\n    if not end_date and not num:\n        end_date = timezone.utcnow()\n\n    delta_iscron = False\n    tz = start_date.tzinfo\n    if isinstance(delta, six.string_types):\n        delta_iscron = True\n        start_date = timezone.make_naive(start_date, tz)\n        cron = croniter(delta, start_date)\n    elif isinstance(delta, timedelta):\n        delta = abs(delta)\n    dates = []\n    if end_date:\n        if timezone.is_naive(start_date):\n            end_date = timezone.make_naive(end_date, tz)\n        while start_date <= end_date:\n            if timezone.is_naive(start_date):\n                dates.append(timezone.make_aware(start_date, tz))\n            else:\n                dates.append(start_date)\n\n            if delta_iscron:\n                start_date = cron.get_next(datetime)\n            else:\n                start_date += delta\n    else:\n        for _ in range(abs(num)):\n            if timezone.is_naive(start_date):\n                dates.append(timezone.make_aware(start_date, tz))\n            else:\n                dates.append(start_date)\n\n            if delta_iscron:\n                if num > 0:\n                    start_date = cron.get_next(datetime)\n                else:\n                    start_date = cron.get_prev(datetime)\n            else:\n                if num > 0:\n                    start_date += delta\n                else:\n                    start_date -= delta\n    return sorted(dates)", "language": "python", "code": "def date_range(start_date, end_date=None, num=None, delta=None):\n    \"\"\"\n    Get a set of dates as a list based on a start, end and delta, delta\n    can be something that can be added to `datetime.datetime`\n    or a cron expression as a `str`\n\n    :Example::\n\n        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta=timedelta(1))\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n            datetime.datetime(2016, 1, 3, 0, 0)]\n        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta='0 0 * * *')\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n            datetime.datetime(2016, 1, 3, 0, 0)]\n        date_range(datetime(2016, 1, 1), datetime(2016, 3, 3), delta=\"0 0 0 * *\")\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 2, 1, 0, 0),\n            datetime.datetime(2016, 3, 1, 0, 0)]\n\n    :param start_date: anchor date to start the series from\n    :type start_date: datetime.datetime\n    :param end_date: right boundary for the date range\n    :type end_date: datetime.datetime\n    :param num: alternatively to end_date, you can specify the number of\n        number of entries you want in the range. This number can be negative,\n        output will always be sorted regardless\n    :type num: int\n    \"\"\"\n    if not delta:\n        return []\n    if end_date and start_date > end_date:\n        raise Exception(\"Wait. start_date needs to be before end_date\")\n    if end_date and num:\n        raise Exception(\"Wait. Either specify end_date OR num\")\n    if not end_date and not num:\n        end_date = timezone.utcnow()\n\n    delta_iscron = False\n    tz = start_date.tzinfo\n    if isinstance(delta, six.string_types):\n        delta_iscron = True\n        start_date = timezone.make_naive(start_date, tz)\n        cron = croniter(delta, start_date)\n    elif isinstance(delta, timedelta):\n        delta = abs(delta)\n    dates = []\n    if end_date:\n        if timezone.is_naive(start_date):\n            end_date = timezone.make_naive(end_date, tz)\n        while start_date <= end_date:\n            if timezone.is_naive(start_date):\n                dates.append(timezone.make_aware(start_date, tz))\n            else:\n                dates.append(start_date)\n\n            if delta_iscron:\n                start_date = cron.get_next(datetime)\n            else:\n                start_date += delta\n    else:\n        for _ in range(abs(num)):\n            if timezone.is_naive(start_date):\n                dates.append(timezone.make_aware(start_date, tz))\n            else:\n                dates.append(start_date)\n\n            if delta_iscron:\n                if num > 0:\n                    start_date = cron.get_next(datetime)\n                else:\n                    start_date = cron.get_prev(datetime)\n            else:\n                if num > 0:\n                    start_date += delta\n                else:\n                    start_date -= delta\n    return sorted(dates)", "code_tokens": ["def", "date_range", "(", "start_date", ",", "end_date", "=", "None", ",", "num", "=", "None", ",", "delta", "=", "None", ")", ":", "if", "not", "delta", ":", "return", "[", "]", "if", "end_date", "and", "start_date", ">", "end_date", ":", "raise", "Exception", "(", "\"Wait. start_date needs to be before end_date\"", ")", "if", "end_date", "and", "num", ":", "raise", "Exception", "(", "\"Wait. Either specify end_date OR num\"", ")", "if", "not", "end_date", "and", "not", "num", ":", "end_date", "=", "timezone", ".", "utcnow", "(", ")", "delta_iscron", "=", "False", "tz", "=", "start_date", ".", "tzinfo", "if", "isinstance", "(", "delta", ",", "six", ".", "string_types", ")", ":", "delta_iscron", "=", "True", "start_date", "=", "timezone", ".", "make_naive", "(", "start_date", ",", "tz", ")", "cron", "=", "croniter", "(", "delta", ",", "start_date", ")", "elif", "isinstance", "(", "delta", ",", "timedelta", ")", ":", "delta", "=", "abs", "(", "delta", ")", "dates", "=", "[", "]", "if", "end_date", ":", "if", "timezone", ".", "is_naive", "(", "start_date", ")", ":", "end_date", "=", "timezone", ".", "make_naive", "(", "end_date", ",", "tz", ")", "while", "start_date", "<=", "end_date", ":", "if", "timezone", ".", "is_naive", "(", "start_date", ")", ":", "dates", ".", "append", "(", "timezone", ".", "make_aware", "(", "start_date", ",", "tz", ")", ")", "else", ":", "dates", ".", "append", "(", "start_date", ")", "if", "delta_iscron", ":", "start_date", "=", "cron", ".", "get_next", "(", "datetime", ")", "else", ":", "start_date", "+=", "delta", "else", ":", "for", "_", "in", "range", "(", "abs", "(", "num", ")", ")", ":", "if", "timezone", ".", "is_naive", "(", "start_date", ")", ":", "dates", ".", "append", "(", "timezone", ".", "make_aware", "(", "start_date", ",", "tz", ")", ")", "else", ":", "dates", ".", "append", "(", "start_date", ")", "if", "delta_iscron", ":", "if", "num", ">", "0", ":", "start_date", "=", "cron", ".", "get_next", "(", "datetime", ")", "else", ":", "start_date", "=", "cron", ".", "get_prev", "(", "datetime", ")", "else", ":", "if", "num", ">", "0", ":", "start_date", "+=", "delta", "else", ":", "start_date", "-=", "delta", "return", "sorted", "(", "dates", ")"], "docstring": "Get a set of dates as a list based on a start, end and delta, delta\n    can be something that can be added to `datetime.datetime`\n    or a cron expression as a `str`\n\n    :Example::\n\n        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta=timedelta(1))\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n            datetime.datetime(2016, 1, 3, 0, 0)]\n        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta='0 0 * * *')\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n            datetime.datetime(2016, 1, 3, 0, 0)]\n        date_range(datetime(2016, 1, 1), datetime(2016, 3, 3), delta=\"0 0 0 * *\")\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 2, 1, 0, 0),\n            datetime.datetime(2016, 3, 1, 0, 0)]\n\n    :param start_date: anchor date to start the series from\n    :type start_date: datetime.datetime\n    :param end_date: right boundary for the date range\n    :type end_date: datetime.datetime\n    :param num: alternatively to end_date, you can specify the number of\n        number of entries you want in the range. This number can be negative,\n        output will always be sorted regardless\n    :type num: int", "docstring_tokens": ["Get", "a", "set", "of", "dates", "as", "a", "list", "based", "on", "a", "start", "end", "and", "delta", "delta", "can", "be", "something", "that", "can", "be", "added", "to", "datetime", ".", "datetime", "or", "a", "cron", "expression", "as", "a", "str"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dates.py#L36-L111", "partition": "test"}
{"repo": "eyeseast/python-frontmatter", "path": "frontmatter/__init__.py", "func_name": "detect_format", "original_string": "def detect_format(text, handlers):\n    \"\"\"\n    Figure out which handler to use, based on metadata.\n    Returns a handler instance or None.\n\n    ``text`` should be unicode text about to be parsed.\n\n    ``handlers`` is a dictionary where keys are opening delimiters \n    and values are handler instances.\n    \"\"\"\n    for pattern, handler in handlers.items():\n        if pattern.match(text):\n            return handler\n\n    # nothing matched, give nothing back\n    return None", "language": "python", "code": "def detect_format(text, handlers):\n    \"\"\"\n    Figure out which handler to use, based on metadata.\n    Returns a handler instance or None.\n\n    ``text`` should be unicode text about to be parsed.\n\n    ``handlers`` is a dictionary where keys are opening delimiters \n    and values are handler instances.\n    \"\"\"\n    for pattern, handler in handlers.items():\n        if pattern.match(text):\n            return handler\n\n    # nothing matched, give nothing back\n    return None", "code_tokens": ["def", "detect_format", "(", "text", ",", "handlers", ")", ":", "for", "pattern", ",", "handler", "in", "handlers", ".", "items", "(", ")", ":", "if", "pattern", ".", "match", "(", "text", ")", ":", "return", "handler", "# nothing matched, give nothing back", "return", "None"], "docstring": "Figure out which handler to use, based on metadata.\n    Returns a handler instance or None.\n\n    ``text`` should be unicode text about to be parsed.\n\n    ``handlers`` is a dictionary where keys are opening delimiters \n    and values are handler instances.", "docstring_tokens": ["Figure", "out", "which", "handler", "to", "use", "based", "on", "metadata", ".", "Returns", "a", "handler", "instance", "or", "None", "."], "sha": "c318e583c48599eb597e0ad59c5d972258c3febc", "url": "https://github.com/eyeseast/python-frontmatter/blob/c318e583c48599eb597e0ad59c5d972258c3febc/frontmatter/__init__.py#L34-L49", "partition": "test"}
{"repo": "pyca/pyopenssl", "path": "src/OpenSSL/crypto.py", "func_name": "Revoked.set_rev_date", "original_string": "def set_rev_date(self, when):\n        \"\"\"\n        Set the revocation timestamp.\n\n        :param bytes when: The timestamp of the revocation,\n            as ASN.1 TIME.\n        :return: ``None``\n        \"\"\"\n        dt = _lib.X509_REVOKED_get0_revocationDate(self._revoked)\n        return _set_asn1_time(dt, when)", "language": "python", "code": "def set_rev_date(self, when):\n        \"\"\"\n        Set the revocation timestamp.\n\n        :param bytes when: The timestamp of the revocation,\n            as ASN.1 TIME.\n        :return: ``None``\n        \"\"\"\n        dt = _lib.X509_REVOKED_get0_revocationDate(self._revoked)\n        return _set_asn1_time(dt, when)", "code_tokens": ["def", "set_rev_date", "(", "self", ",", "when", ")", ":", "dt", "=", "_lib", ".", "X509_REVOKED_get0_revocationDate", "(", "self", ".", "_revoked", ")", "return", "_set_asn1_time", "(", "dt", ",", "when", ")"], "docstring": "Set the revocation timestamp.\n\n        :param bytes when: The timestamp of the revocation,\n            as ASN.1 TIME.\n        :return: ``None``", "docstring_tokens": ["Set", "the", "revocation", "timestamp", "."], "sha": "1fbe064c50fd030948141d7d630673761525b0d0", "url": "https://github.com/pyca/pyopenssl/blob/1fbe064c50fd030948141d7d630673761525b0d0/src/OpenSSL/crypto.py#L2058-L2067", "partition": "test"}
{"repo": "gunyarakun/python-shogi", "path": "shogi/__init__.py", "func_name": "Board.set_piece_at", "original_string": "def set_piece_at(self, square, piece, from_hand=False, into_hand=False):\n        '''Sets a piece at the given square. An existing piece is replaced.'''\n        if from_hand:\n            self.remove_piece_from_hand(piece.piece_type, self.turn)\n\n        self.remove_piece_at(square, into_hand)\n\n        self.pieces[square] = piece.piece_type\n\n        mask = BB_SQUARES[square]\n\n        piece_type = piece.piece_type\n\n        self.piece_bb[piece_type] |= mask\n\n        if piece_type == KING:\n            self.king_squares[piece.color] = square\n\n        self.occupied.ixor(mask, piece.color, square)\n\n        # Update incremental zorbist hash.\n        if piece.color == BLACK:\n            piece_index = (piece.piece_type - 1) * 2\n        else:\n            piece_index = (piece.piece_type - 1) * 2 + 1\n        self.incremental_zobrist_hash ^= DEFAULT_RANDOM_ARRAY[81 * piece_index + 9 * rank_index(square) + file_index(square)]", "language": "python", "code": "def set_piece_at(self, square, piece, from_hand=False, into_hand=False):\n        '''Sets a piece at the given square. An existing piece is replaced.'''\n        if from_hand:\n            self.remove_piece_from_hand(piece.piece_type, self.turn)\n\n        self.remove_piece_at(square, into_hand)\n\n        self.pieces[square] = piece.piece_type\n\n        mask = BB_SQUARES[square]\n\n        piece_type = piece.piece_type\n\n        self.piece_bb[piece_type] |= mask\n\n        if piece_type == KING:\n            self.king_squares[piece.color] = square\n\n        self.occupied.ixor(mask, piece.color, square)\n\n        # Update incremental zorbist hash.\n        if piece.color == BLACK:\n            piece_index = (piece.piece_type - 1) * 2\n        else:\n            piece_index = (piece.piece_type - 1) * 2 + 1\n        self.incremental_zobrist_hash ^= DEFAULT_RANDOM_ARRAY[81 * piece_index + 9 * rank_index(square) + file_index(square)]", "code_tokens": ["def", "set_piece_at", "(", "self", ",", "square", ",", "piece", ",", "from_hand", "=", "False", ",", "into_hand", "=", "False", ")", ":", "if", "from_hand", ":", "self", ".", "remove_piece_from_hand", "(", "piece", ".", "piece_type", ",", "self", ".", "turn", ")", "self", ".", "remove_piece_at", "(", "square", ",", "into_hand", ")", "self", ".", "pieces", "[", "square", "]", "=", "piece", ".", "piece_type", "mask", "=", "BB_SQUARES", "[", "square", "]", "piece_type", "=", "piece", ".", "piece_type", "self", ".", "piece_bb", "[", "piece_type", "]", "|=", "mask", "if", "piece_type", "==", "KING", ":", "self", ".", "king_squares", "[", "piece", ".", "color", "]", "=", "square", "self", ".", "occupied", ".", "ixor", "(", "mask", ",", "piece", ".", "color", ",", "square", ")", "# Update incremental zorbist hash.", "if", "piece", ".", "color", "==", "BLACK", ":", "piece_index", "=", "(", "piece", ".", "piece_type", "-", "1", ")", "*", "2", "else", ":", "piece_index", "=", "(", "piece", ".", "piece_type", "-", "1", ")", "*", "2", "+", "1", "self", ".", "incremental_zobrist_hash", "^=", "DEFAULT_RANDOM_ARRAY", "[", "81", "*", "piece_index", "+", "9", "*", "rank_index", "(", "square", ")", "+", "file_index", "(", "square", ")", "]"], "docstring": "Sets a piece at the given square. An existing piece is replaced.", "docstring_tokens": ["Sets", "a", "piece", "at", "the", "given", "square", ".", "An", "existing", "piece", "is", "replaced", "."], "sha": "137fe5f5e72251e8a97a1dba4a9b44b7c3c79914", "url": "https://github.com/gunyarakun/python-shogi/blob/137fe5f5e72251e8a97a1dba4a9b44b7c3c79914/shogi/__init__.py#L659-L684", "partition": "test"}
{"repo": "librosa/librosa", "path": "examples/time_stretch.py", "func_name": "stretch_demo", "original_string": "def stretch_demo(input_file, output_file, speed):\n    '''Phase-vocoder time stretch demo function.\n\n    :parameters:\n      - input_file : str\n          path to input audio\n      - output_file : str\n          path to save output (wav)\n      - speed : float > 0\n          speed up by this factor\n    '''\n\n    # 1. Load the wav file, resample\n    print('Loading ', input_file)\n\n    y, sr = librosa.load(input_file)\n\n    # 2. Time-stretch through effects module\n    print('Playing back at {:3.0f}% speed'.format(speed * 100))\n\n    y_stretch = librosa.effects.time_stretch(y, speed)\n\n    print('Saving stretched audio to: ', output_file)\n    librosa.output.write_wav(output_file, y_stretch, sr)", "language": "python", "code": "def stretch_demo(input_file, output_file, speed):\n    '''Phase-vocoder time stretch demo function.\n\n    :parameters:\n      - input_file : str\n          path to input audio\n      - output_file : str\n          path to save output (wav)\n      - speed : float > 0\n          speed up by this factor\n    '''\n\n    # 1. Load the wav file, resample\n    print('Loading ', input_file)\n\n    y, sr = librosa.load(input_file)\n\n    # 2. Time-stretch through effects module\n    print('Playing back at {:3.0f}% speed'.format(speed * 100))\n\n    y_stretch = librosa.effects.time_stretch(y, speed)\n\n    print('Saving stretched audio to: ', output_file)\n    librosa.output.write_wav(output_file, y_stretch, sr)", "code_tokens": ["def", "stretch_demo", "(", "input_file", ",", "output_file", ",", "speed", ")", ":", "# 1. Load the wav file, resample", "print", "(", "'Loading '", ",", "input_file", ")", "y", ",", "sr", "=", "librosa", ".", "load", "(", "input_file", ")", "# 2. Time-stretch through effects module", "print", "(", "'Playing back at {:3.0f}% speed'", ".", "format", "(", "speed", "*", "100", ")", ")", "y_stretch", "=", "librosa", ".", "effects", ".", "time_stretch", "(", "y", ",", "speed", ")", "print", "(", "'Saving stretched audio to: '", ",", "output_file", ")", "librosa", ".", "output", ".", "write_wav", "(", "output_file", ",", "y_stretch", ",", "sr", ")"], "docstring": "Phase-vocoder time stretch demo function.\n\n    :parameters:\n      - input_file : str\n          path to input audio\n      - output_file : str\n          path to save output (wav)\n      - speed : float > 0\n          speed up by this factor", "docstring_tokens": ["Phase", "-", "vocoder", "time", "stretch", "demo", "function", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/examples/time_stretch.py#L13-L36", "partition": "test"}
{"repo": "hydrosquall/tiingo-python", "path": "tiingo/api.py", "func_name": "TiingoClient.get_ticker_price", "original_string": "def get_ticker_price(self, ticker,\n                         startDate=None, endDate=None,\n                         fmt='json', frequency='daily'):\n        \"\"\"By default, return latest EOD Composite Price for a stock ticker.\n           On average, each feed contains 3 data sources.\n\n            Supported tickers + Available Day Ranges are here:\n            https://apimedia.tiingo.com/docs/tiingo/daily/supported_tickers.zip\n\n            Args:\n                ticker (string): Unique identifier for stock ticker\n                startDate (string): Start of ticker range in YYYY-MM-DD format\n                endDate (string): End of ticker range in YYYY-MM-DD format\n                fmt (string): 'csv' or 'json'\n                frequency (string): Resample frequency\n        \"\"\"\n        url = self._get_url(ticker, frequency)\n        params = {\n            'format': fmt if fmt != \"object\" else 'json',  # conversion local\n            'resampleFreq': frequency\n        }\n\n        if startDate:\n            params['startDate'] = startDate\n        if endDate:\n            params['endDate'] = endDate\n\n        # TODO: evaluate whether to stream CSV to cache on disk, or\n        # load as array in memory, or just pass plain text\n        response = self._request('GET', url, params=params)\n        if fmt == \"json\":\n            return response.json()\n        elif fmt == \"object\":\n            data = response.json()\n            return [dict_to_object(item, \"TickerPrice\") for item in data]\n        else:\n            return response.content.decode(\"utf-8\")", "language": "python", "code": "def get_ticker_price(self, ticker,\n                         startDate=None, endDate=None,\n                         fmt='json', frequency='daily'):\n        \"\"\"By default, return latest EOD Composite Price for a stock ticker.\n           On average, each feed contains 3 data sources.\n\n            Supported tickers + Available Day Ranges are here:\n            https://apimedia.tiingo.com/docs/tiingo/daily/supported_tickers.zip\n\n            Args:\n                ticker (string): Unique identifier for stock ticker\n                startDate (string): Start of ticker range in YYYY-MM-DD format\n                endDate (string): End of ticker range in YYYY-MM-DD format\n                fmt (string): 'csv' or 'json'\n                frequency (string): Resample frequency\n        \"\"\"\n        url = self._get_url(ticker, frequency)\n        params = {\n            'format': fmt if fmt != \"object\" else 'json',  # conversion local\n            'resampleFreq': frequency\n        }\n\n        if startDate:\n            params['startDate'] = startDate\n        if endDate:\n            params['endDate'] = endDate\n\n        # TODO: evaluate whether to stream CSV to cache on disk, or\n        # load as array in memory, or just pass plain text\n        response = self._request('GET', url, params=params)\n        if fmt == \"json\":\n            return response.json()\n        elif fmt == \"object\":\n            data = response.json()\n            return [dict_to_object(item, \"TickerPrice\") for item in data]\n        else:\n            return response.content.decode(\"utf-8\")", "code_tokens": ["def", "get_ticker_price", "(", "self", ",", "ticker", ",", "startDate", "=", "None", ",", "endDate", "=", "None", ",", "fmt", "=", "'json'", ",", "frequency", "=", "'daily'", ")", ":", "url", "=", "self", ".", "_get_url", "(", "ticker", ",", "frequency", ")", "params", "=", "{", "'format'", ":", "fmt", "if", "fmt", "!=", "\"object\"", "else", "'json'", ",", "# conversion local", "'resampleFreq'", ":", "frequency", "}", "if", "startDate", ":", "params", "[", "'startDate'", "]", "=", "startDate", "if", "endDate", ":", "params", "[", "'endDate'", "]", "=", "endDate", "# TODO: evaluate whether to stream CSV to cache on disk, or", "# load as array in memory, or just pass plain text", "response", "=", "self", ".", "_request", "(", "'GET'", ",", "url", ",", "params", "=", "params", ")", "if", "fmt", "==", "\"json\"", ":", "return", "response", ".", "json", "(", ")", "elif", "fmt", "==", "\"object\"", ":", "data", "=", "response", ".", "json", "(", ")", "return", "[", "dict_to_object", "(", "item", ",", "\"TickerPrice\"", ")", "for", "item", "in", "data", "]", "else", ":", "return", "response", ".", "content", ".", "decode", "(", "\"utf-8\"", ")"], "docstring": "By default, return latest EOD Composite Price for a stock ticker.\n           On average, each feed contains 3 data sources.\n\n            Supported tickers + Available Day Ranges are here:\n            https://apimedia.tiingo.com/docs/tiingo/daily/supported_tickers.zip\n\n            Args:\n                ticker (string): Unique identifier for stock ticker\n                startDate (string): Start of ticker range in YYYY-MM-DD format\n                endDate (string): End of ticker range in YYYY-MM-DD format\n                fmt (string): 'csv' or 'json'\n                frequency (string): Resample frequency", "docstring_tokens": ["By", "default", "return", "latest", "EOD", "Composite", "Price", "for", "a", "stock", "ticker", ".", "On", "average", "each", "feed", "contains", "3", "data", "sources", "."], "sha": "9bb98ca9d24f2e4db651cf0590e4b47184546482", "url": "https://github.com/hydrosquall/tiingo-python/blob/9bb98ca9d24f2e4db651cf0590e4b47184546482/tiingo/api.py#L169-L205", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/langevin.py", "func_name": "_get_drift", "original_string": "def _get_drift(step_size_parts, volatility_parts, grads_volatility,\n               grads_target_log_prob,\n               name=None):\n  \"\"\"Compute diffusion drift at the current location `current_state`.\n\n  The drift of the diffusion at is computed as\n\n  ```none\n  0.5 * `step_size` * volatility_parts * `target_log_prob_fn(current_state)`\n  + `step_size` * `grads_volatility`\n  ```\n\n  where `volatility_parts` = `volatility_fn(current_state)**2` and\n  `grads_volatility` is a gradient of `volatility_parts` at the `current_state`.\n\n  Args:\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      Euler-Maruyama method. Must broadcast with the shape of\n      `volatility_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    volatility_parts: Python `list` of `Tensor`s representing the value of\n      `volatility_fn(*state_parts)`.\n    grads_volatility: Python list of `Tensor`s representing the value of the\n      gradient of `volatility_parts**2` wrt the state of the chain.\n    grads_target_log_prob: Python list of `Tensor`s representing\n      gradient of `target_log_prob_fn(*state_parts`) wrt `state_parts`. Must\n      have same shape as `volatility_parts`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'mala_get_drift').\n\n  Returns:\n    drift_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, 'mala_get_drift', [\n      step_size_parts, volatility_parts, grads_volatility, grads_target_log_prob\n  ]):\n\n    drift_parts = []\n\n    for step_size, volatility, grad_volatility, grad_target_log_prob in (\n        zip(step_size_parts,\n            volatility_parts,\n            grads_volatility,\n            grads_target_log_prob)):\n      volatility_squared = tf.square(volatility)\n      drift = 0.5 * step_size * (volatility_squared * grad_target_log_prob\n                                 + grad_volatility)\n      drift_parts.append(drift)\n\n    return drift_parts", "language": "python", "code": "def _get_drift(step_size_parts, volatility_parts, grads_volatility,\n               grads_target_log_prob,\n               name=None):\n  \"\"\"Compute diffusion drift at the current location `current_state`.\n\n  The drift of the diffusion at is computed as\n\n  ```none\n  0.5 * `step_size` * volatility_parts * `target_log_prob_fn(current_state)`\n  + `step_size` * `grads_volatility`\n  ```\n\n  where `volatility_parts` = `volatility_fn(current_state)**2` and\n  `grads_volatility` is a gradient of `volatility_parts` at the `current_state`.\n\n  Args:\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      Euler-Maruyama method. Must broadcast with the shape of\n      `volatility_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    volatility_parts: Python `list` of `Tensor`s representing the value of\n      `volatility_fn(*state_parts)`.\n    grads_volatility: Python list of `Tensor`s representing the value of the\n      gradient of `volatility_parts**2` wrt the state of the chain.\n    grads_target_log_prob: Python list of `Tensor`s representing\n      gradient of `target_log_prob_fn(*state_parts`) wrt `state_parts`. Must\n      have same shape as `volatility_parts`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'mala_get_drift').\n\n  Returns:\n    drift_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, 'mala_get_drift', [\n      step_size_parts, volatility_parts, grads_volatility, grads_target_log_prob\n  ]):\n\n    drift_parts = []\n\n    for step_size, volatility, grad_volatility, grad_target_log_prob in (\n        zip(step_size_parts,\n            volatility_parts,\n            grads_volatility,\n            grads_target_log_prob)):\n      volatility_squared = tf.square(volatility)\n      drift = 0.5 * step_size * (volatility_squared * grad_target_log_prob\n                                 + grad_volatility)\n      drift_parts.append(drift)\n\n    return drift_parts", "code_tokens": ["def", "_get_drift", "(", "step_size_parts", ",", "volatility_parts", ",", "grads_volatility", ",", "grads_target_log_prob", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'mala_get_drift'", ",", "[", "step_size_parts", ",", "volatility_parts", ",", "grads_volatility", ",", "grads_target_log_prob", "]", ")", ":", "drift_parts", "=", "[", "]", "for", "step_size", ",", "volatility", ",", "grad_volatility", ",", "grad_target_log_prob", "in", "(", "zip", "(", "step_size_parts", ",", "volatility_parts", ",", "grads_volatility", ",", "grads_target_log_prob", ")", ")", ":", "volatility_squared", "=", "tf", ".", "square", "(", "volatility", ")", "drift", "=", "0.5", "*", "step_size", "*", "(", "volatility_squared", "*", "grad_target_log_prob", "+", "grad_volatility", ")", "drift_parts", ".", "append", "(", "drift", ")", "return", "drift_parts"], "docstring": "Compute diffusion drift at the current location `current_state`.\n\n  The drift of the diffusion at is computed as\n\n  ```none\n  0.5 * `step_size` * volatility_parts * `target_log_prob_fn(current_state)`\n  + `step_size` * `grads_volatility`\n  ```\n\n  where `volatility_parts` = `volatility_fn(current_state)**2` and\n  `grads_volatility` is a gradient of `volatility_parts` at the `current_state`.\n\n  Args:\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      Euler-Maruyama method. Must broadcast with the shape of\n      `volatility_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    volatility_parts: Python `list` of `Tensor`s representing the value of\n      `volatility_fn(*state_parts)`.\n    grads_volatility: Python list of `Tensor`s representing the value of the\n      gradient of `volatility_parts**2` wrt the state of the chain.\n    grads_target_log_prob: Python list of `Tensor`s representing\n      gradient of `target_log_prob_fn(*state_parts`) wrt `state_parts`. Must\n      have same shape as `volatility_parts`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'mala_get_drift').\n\n  Returns:\n    drift_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.", "docstring_tokens": ["Compute", "diffusion", "drift", "at", "the", "current", "location", "current_state", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/langevin.py#L691-L745", "partition": "test"}
{"repo": "dbcli/cli_helpers", "path": "tasks.py", "func_name": "BaseCommand.call_in_sequence", "original_string": "def call_in_sequence(self, cmds, shell=True):\n        \"\"\"Run multiple commmands in a row, exiting if one fails.\"\"\"\n        for cmd in cmds:\n            if subprocess.call(cmd, shell=shell) == 1:\n                sys.exit(1)", "language": "python", "code": "def call_in_sequence(self, cmds, shell=True):\n        \"\"\"Run multiple commmands in a row, exiting if one fails.\"\"\"\n        for cmd in cmds:\n            if subprocess.call(cmd, shell=shell) == 1:\n                sys.exit(1)", "code_tokens": ["def", "call_in_sequence", "(", "self", ",", "cmds", ",", "shell", "=", "True", ")", ":", "for", "cmd", "in", "cmds", ":", "if", "subprocess", ".", "call", "(", "cmd", ",", "shell", "=", "shell", ")", "==", "1", ":", "sys", ".", "exit", "(", "1", ")"], "docstring": "Run multiple commmands in a row, exiting if one fails.", "docstring_tokens": ["Run", "multiple", "commmands", "in", "a", "row", "exiting", "if", "one", "fails", "."], "sha": "3ebd891ac0c02bad061182dbcb54a47fb21980ae", "url": "https://github.com/dbcli/cli_helpers/blob/3ebd891ac0c02bad061182dbcb54a47fb21980ae/tasks.py#L35-L39", "partition": "test"}
{"repo": "mental32/spotify.py", "path": "spotify/client.py", "func_name": "Client.oauth2_url", "original_string": "def oauth2_url(self, redirect_uri: str, scope: Optional[str] = None, state: Optional[str] = None) -> str:\n        \"\"\"Generate an outh2 url for user authentication.\n\n        Parameters\n        ----------\n        redirect_uri : str\n            Where spotify should redirect the user to after authentication.\n        scope : Optional[str]\n            Space seperated spotify scopes for different levels of access.\n        state : Optional[str]\n            Using a state value can increase your assurance that an incoming connection is the result of an authentication request.\n\n        Returns\n        -------\n        url : str\n            The OAuth2 url.\n        \"\"\"\n        return OAuth2.url_(self.http.client_id, redirect_uri, scope=scope, state=state)", "language": "python", "code": "def oauth2_url(self, redirect_uri: str, scope: Optional[str] = None, state: Optional[str] = None) -> str:\n        \"\"\"Generate an outh2 url for user authentication.\n\n        Parameters\n        ----------\n        redirect_uri : str\n            Where spotify should redirect the user to after authentication.\n        scope : Optional[str]\n            Space seperated spotify scopes for different levels of access.\n        state : Optional[str]\n            Using a state value can increase your assurance that an incoming connection is the result of an authentication request.\n\n        Returns\n        -------\n        url : str\n            The OAuth2 url.\n        \"\"\"\n        return OAuth2.url_(self.http.client_id, redirect_uri, scope=scope, state=state)", "code_tokens": ["def", "oauth2_url", "(", "self", ",", "redirect_uri", ":", "str", ",", "scope", ":", "Optional", "[", "str", "]", "=", "None", ",", "state", ":", "Optional", "[", "str", "]", "=", "None", ")", "->", "str", ":", "return", "OAuth2", ".", "url_", "(", "self", ".", "http", ".", "client_id", ",", "redirect_uri", ",", "scope", "=", "scope", ",", "state", "=", "state", ")"], "docstring": "Generate an outh2 url for user authentication.\n\n        Parameters\n        ----------\n        redirect_uri : str\n            Where spotify should redirect the user to after authentication.\n        scope : Optional[str]\n            Space seperated spotify scopes for different levels of access.\n        state : Optional[str]\n            Using a state value can increase your assurance that an incoming connection is the result of an authentication request.\n\n        Returns\n        -------\n        url : str\n            The OAuth2 url.", "docstring_tokens": ["Generate", "an", "outh2", "url", "for", "user", "authentication", "."], "sha": "bb296cac7c3dd289908906b7069bd80f43950515", "url": "https://github.com/mental32/spotify.py/blob/bb296cac7c3dd289908906b7069bd80f43950515/spotify/client.py#L68-L85", "partition": "test"}
{"repo": "LLNL/scraper", "path": "scraper/github/util.py", "func_name": "_license_obj", "original_string": "def _license_obj(license):\n    \"\"\"\n    A helper function to look up license object information\n\n    Use names from: https://api.github.com/licenses\n    \"\"\"\n    obj = None\n\n    if license in ('MIT', 'MIT License'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/mit',\n            'name': 'MIT'\n        }\n    elif license in ('BSD 2-clause \"Simplified\" License'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/bsd-2-clause',\n            'name': 'BSD-2-Clause'\n        }\n    elif license in ('BSD 3-clause \"New\" or \"Revised\" License'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/bsd-3-clause',\n            'name': 'BSD-3-Clause'\n        }\n    elif license in ('Apache License 2.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/apache-2.0',\n            'name': 'Apache-2.0'\n        }\n    elif license in ('GNU General Public License v2.1'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/gpl-2.1',\n            'name': 'GPL-2.1'\n        }\n    elif license in ('GNU General Public License v2.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/gpl-2.0',\n            'name': 'GPL-2.0'\n        }\n    elif license in ('GNU Lesser General Public License v2.1'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/lgpl-2.1',\n            'name': 'LGPL-2.1'\n        }\n    elif license in ('GNU General Public License v3.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/gpl-3.0',\n            'name': 'GPL-3.0'\n        }\n    elif license in ('GNU Lesser General Public License v3.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/lgpl-3.0',\n            'name': 'LGPL-3.0'\n        }\n    elif license in ('Eclipse Public License 1.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/epl-1.0',\n            'name': 'EPL-1.0',\n        }\n    elif license in ('Mozilla Public License 2.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/mpl-2.0',\n            'name': 'MPL-2.0',\n        }\n    elif license in ('The Unlicense'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/unlicense',\n            'name': 'Unlicense',\n        }\n    elif license in ('GNU Affero General Public License v3.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/agpl-3.0',\n            'name': 'AGPL-3.0',\n        }\n    elif license in ('Eclipse Public License 2.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/epl-2.0',\n            'name': 'EPL-2.0',\n        }\n\n    if obj is None:\n        logger.warn('I dont understand the license: %s', license)\n        raise ValueError('Aborting!')\n\n    return obj", "language": "python", "code": "def _license_obj(license):\n    \"\"\"\n    A helper function to look up license object information\n\n    Use names from: https://api.github.com/licenses\n    \"\"\"\n    obj = None\n\n    if license in ('MIT', 'MIT License'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/mit',\n            'name': 'MIT'\n        }\n    elif license in ('BSD 2-clause \"Simplified\" License'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/bsd-2-clause',\n            'name': 'BSD-2-Clause'\n        }\n    elif license in ('BSD 3-clause \"New\" or \"Revised\" License'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/bsd-3-clause',\n            'name': 'BSD-3-Clause'\n        }\n    elif license in ('Apache License 2.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/apache-2.0',\n            'name': 'Apache-2.0'\n        }\n    elif license in ('GNU General Public License v2.1'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/gpl-2.1',\n            'name': 'GPL-2.1'\n        }\n    elif license in ('GNU General Public License v2.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/gpl-2.0',\n            'name': 'GPL-2.0'\n        }\n    elif license in ('GNU Lesser General Public License v2.1'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/lgpl-2.1',\n            'name': 'LGPL-2.1'\n        }\n    elif license in ('GNU General Public License v3.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/gpl-3.0',\n            'name': 'GPL-3.0'\n        }\n    elif license in ('GNU Lesser General Public License v3.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/lgpl-3.0',\n            'name': 'LGPL-3.0'\n        }\n    elif license in ('Eclipse Public License 1.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/epl-1.0',\n            'name': 'EPL-1.0',\n        }\n    elif license in ('Mozilla Public License 2.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/mpl-2.0',\n            'name': 'MPL-2.0',\n        }\n    elif license in ('The Unlicense'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/unlicense',\n            'name': 'Unlicense',\n        }\n    elif license in ('GNU Affero General Public License v3.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/agpl-3.0',\n            'name': 'AGPL-3.0',\n        }\n    elif license in ('Eclipse Public License 2.0'):\n        obj = {\n            'URL': 'https://api.github.com/licenses/epl-2.0',\n            'name': 'EPL-2.0',\n        }\n\n    if obj is None:\n        logger.warn('I dont understand the license: %s', license)\n        raise ValueError('Aborting!')\n\n    return obj", "code_tokens": ["def", "_license_obj", "(", "license", ")", ":", "obj", "=", "None", "if", "license", "in", "(", "'MIT'", ",", "'MIT License'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/mit'", ",", "'name'", ":", "'MIT'", "}", "elif", "license", "in", "(", "'BSD 2-clause \"Simplified\" License'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/bsd-2-clause'", ",", "'name'", ":", "'BSD-2-Clause'", "}", "elif", "license", "in", "(", "'BSD 3-clause \"New\" or \"Revised\" License'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/bsd-3-clause'", ",", "'name'", ":", "'BSD-3-Clause'", "}", "elif", "license", "in", "(", "'Apache License 2.0'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/apache-2.0'", ",", "'name'", ":", "'Apache-2.0'", "}", "elif", "license", "in", "(", "'GNU General Public License v2.1'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/gpl-2.1'", ",", "'name'", ":", "'GPL-2.1'", "}", "elif", "license", "in", "(", "'GNU General Public License v2.0'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/gpl-2.0'", ",", "'name'", ":", "'GPL-2.0'", "}", "elif", "license", "in", "(", "'GNU Lesser General Public License v2.1'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/lgpl-2.1'", ",", "'name'", ":", "'LGPL-2.1'", "}", "elif", "license", "in", "(", "'GNU General Public License v3.0'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/gpl-3.0'", ",", "'name'", ":", "'GPL-3.0'", "}", "elif", "license", "in", "(", "'GNU Lesser General Public License v3.0'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/lgpl-3.0'", ",", "'name'", ":", "'LGPL-3.0'", "}", "elif", "license", "in", "(", "'Eclipse Public License 1.0'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/epl-1.0'", ",", "'name'", ":", "'EPL-1.0'", ",", "}", "elif", "license", "in", "(", "'Mozilla Public License 2.0'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/mpl-2.0'", ",", "'name'", ":", "'MPL-2.0'", ",", "}", "elif", "license", "in", "(", "'The Unlicense'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/unlicense'", ",", "'name'", ":", "'Unlicense'", ",", "}", "elif", "license", "in", "(", "'GNU Affero General Public License v3.0'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/agpl-3.0'", ",", "'name'", ":", "'AGPL-3.0'", ",", "}", "elif", "license", "in", "(", "'Eclipse Public License 2.0'", ")", ":", "obj", "=", "{", "'URL'", ":", "'https://api.github.com/licenses/epl-2.0'", ",", "'name'", ":", "'EPL-2.0'", ",", "}", "if", "obj", "is", "None", ":", "logger", ".", "warn", "(", "'I dont understand the license: %s'", ",", "license", ")", "raise", "ValueError", "(", "'Aborting!'", ")", "return", "obj"], "docstring": "A helper function to look up license object information\n\n    Use names from: https://api.github.com/licenses", "docstring_tokens": ["A", "helper", "function", "to", "look", "up", "license", "object", "information"], "sha": "881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea", "url": "https://github.com/LLNL/scraper/blob/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea/scraper/github/util.py#L6-L89", "partition": "test"}
{"repo": "pyca/pyopenssl", "path": "src/OpenSSL/crypto.py", "func_name": "X509.get_serial_number", "original_string": "def get_serial_number(self):\n        \"\"\"\n        Return the serial number of this certificate.\n\n        :return: The serial number.\n        :rtype: int\n        \"\"\"\n        asn1_serial = _lib.X509_get_serialNumber(self._x509)\n        bignum_serial = _lib.ASN1_INTEGER_to_BN(asn1_serial, _ffi.NULL)\n        try:\n            hex_serial = _lib.BN_bn2hex(bignum_serial)\n            try:\n                hexstring_serial = _ffi.string(hex_serial)\n                serial = int(hexstring_serial, 16)\n                return serial\n            finally:\n                _lib.OPENSSL_free(hex_serial)\n        finally:\n            _lib.BN_free(bignum_serial)", "language": "python", "code": "def get_serial_number(self):\n        \"\"\"\n        Return the serial number of this certificate.\n\n        :return: The serial number.\n        :rtype: int\n        \"\"\"\n        asn1_serial = _lib.X509_get_serialNumber(self._x509)\n        bignum_serial = _lib.ASN1_INTEGER_to_BN(asn1_serial, _ffi.NULL)\n        try:\n            hex_serial = _lib.BN_bn2hex(bignum_serial)\n            try:\n                hexstring_serial = _ffi.string(hex_serial)\n                serial = int(hexstring_serial, 16)\n                return serial\n            finally:\n                _lib.OPENSSL_free(hex_serial)\n        finally:\n            _lib.BN_free(bignum_serial)", "code_tokens": ["def", "get_serial_number", "(", "self", ")", ":", "asn1_serial", "=", "_lib", ".", "X509_get_serialNumber", "(", "self", ".", "_x509", ")", "bignum_serial", "=", "_lib", ".", "ASN1_INTEGER_to_BN", "(", "asn1_serial", ",", "_ffi", ".", "NULL", ")", "try", ":", "hex_serial", "=", "_lib", ".", "BN_bn2hex", "(", "bignum_serial", ")", "try", ":", "hexstring_serial", "=", "_ffi", ".", "string", "(", "hex_serial", ")", "serial", "=", "int", "(", "hexstring_serial", ",", "16", ")", "return", "serial", "finally", ":", "_lib", ".", "OPENSSL_free", "(", "hex_serial", ")", "finally", ":", "_lib", ".", "BN_free", "(", "bignum_serial", ")"], "docstring": "Return the serial number of this certificate.\n\n        :return: The serial number.\n        :rtype: int", "docstring_tokens": ["Return", "the", "serial", "number", "of", "this", "certificate", "."], "sha": "1fbe064c50fd030948141d7d630673761525b0d0", "url": "https://github.com/pyca/pyopenssl/blob/1fbe064c50fd030948141d7d630673761525b0d0/src/OpenSSL/crypto.py#L1276-L1294", "partition": "test"}
{"repo": "ankitmathur3193/song-cli", "path": "song/commands/MusicWebsiteParser/MrJattParser.py", "func_name": "MrJattParser.check_if_song_name", "original_string": "def check_if_song_name(self,html):\n\t\t'''\n\t\tReturns true if user entered artist or movie name\n\t\t'''\n\t\tsoup=BeautifulSoup(html)\n\t\ta_list=soup.findAll('a','touch')\n\t\t#print a_list\n\t\ttext=[str(x) for x in a_list]\n\t\ttext=''.join(text)\n\t\ttext=text.lower()\n\t\tstring1='download in 48 kbps'\n\t\tstring2='download in 128 kbps'\n\t\tstring3='download in 320 kbps'\n\n\t\thref=''\n\t\tif string3 in text:\n\t\t\t#print 'Downloading in 320 kbps'\n\t\t\thref=a_list[2].get('href')\n\t\t\t\n\t\telif string2 in text:\n\t\t\t#print 'Downloading in 128 kbps'\n\t\t\thref=a_list[1].get('href')\n\t\t\t\n\t\telif string1 in text:\n\t\t\t#print 'Downloading in 48 kbps'\t\n\t\t\thref=a_list[0].get('href')\n\t\telse:\n\t\t\treturn (True,'nothing')\t\t\n\n\t\treturn (False,href)", "language": "python", "code": "def check_if_song_name(self,html):\n\t\t'''\n\t\tReturns true if user entered artist or movie name\n\t\t'''\n\t\tsoup=BeautifulSoup(html)\n\t\ta_list=soup.findAll('a','touch')\n\t\t#print a_list\n\t\ttext=[str(x) for x in a_list]\n\t\ttext=''.join(text)\n\t\ttext=text.lower()\n\t\tstring1='download in 48 kbps'\n\t\tstring2='download in 128 kbps'\n\t\tstring3='download in 320 kbps'\n\n\t\thref=''\n\t\tif string3 in text:\n\t\t\t#print 'Downloading in 320 kbps'\n\t\t\thref=a_list[2].get('href')\n\t\t\t\n\t\telif string2 in text:\n\t\t\t#print 'Downloading in 128 kbps'\n\t\t\thref=a_list[1].get('href')\n\t\t\t\n\t\telif string1 in text:\n\t\t\t#print 'Downloading in 48 kbps'\t\n\t\t\thref=a_list[0].get('href')\n\t\telse:\n\t\t\treturn (True,'nothing')\t\t\n\n\t\treturn (False,href)", "code_tokens": ["def", "check_if_song_name", "(", "self", ",", "html", ")", ":", "soup", "=", "BeautifulSoup", "(", "html", ")", "a_list", "=", "soup", ".", "findAll", "(", "'a'", ",", "'touch'", ")", "#print a_list", "text", "=", "[", "str", "(", "x", ")", "for", "x", "in", "a_list", "]", "text", "=", "''", ".", "join", "(", "text", ")", "text", "=", "text", ".", "lower", "(", ")", "string1", "=", "'download in 48 kbps'", "string2", "=", "'download in 128 kbps'", "string3", "=", "'download in 320 kbps'", "href", "=", "''", "if", "string3", "in", "text", ":", "#print 'Downloading in 320 kbps'", "href", "=", "a_list", "[", "2", "]", ".", "get", "(", "'href'", ")", "elif", "string2", "in", "text", ":", "#print 'Downloading in 128 kbps'", "href", "=", "a_list", "[", "1", "]", ".", "get", "(", "'href'", ")", "elif", "string1", "in", "text", ":", "#print 'Downloading in 48 kbps'\t", "href", "=", "a_list", "[", "0", "]", ".", "get", "(", "'href'", ")", "else", ":", "return", "(", "True", ",", "'nothing'", ")", "return", "(", "False", ",", "href", ")"], "docstring": "Returns true if user entered artist or movie name", "docstring_tokens": ["Returns", "true", "if", "user", "entered", "artist", "or", "movie", "name"], "sha": "ca8ccfe547e9d702313ff6d14e81ae4355989a67", "url": "https://github.com/ankitmathur3193/song-cli/blob/ca8ccfe547e9d702313ff6d14e81ae4355989a67/song/commands/MusicWebsiteParser/MrJattParser.py#L47-L76", "partition": "test"}
{"repo": "nats-io/asyncio-nats", "path": "nats/aio/client.py", "func_name": "Client.unsubscribe", "original_string": "def unsubscribe(self, ssid, max_msgs=0):\n        \"\"\"\n        Takes a subscription sequence id and removes the subscription\n        from the client, optionally after receiving more than max_msgs.\n        \"\"\"\n        if self.is_closed:\n            raise ErrConnectionClosed\n        if self.is_draining:\n            raise ErrConnectionDraining\n\n        self._remove_sub(ssid, max_msgs)\n\n        # We will send these for all subs when we reconnect anyway,\n        # so that we can suppress here.\n        if not self.is_reconnecting:\n            yield from self.auto_unsubscribe(ssid, max_msgs)", "language": "python", "code": "def unsubscribe(self, ssid, max_msgs=0):\n        \"\"\"\n        Takes a subscription sequence id and removes the subscription\n        from the client, optionally after receiving more than max_msgs.\n        \"\"\"\n        if self.is_closed:\n            raise ErrConnectionClosed\n        if self.is_draining:\n            raise ErrConnectionDraining\n\n        self._remove_sub(ssid, max_msgs)\n\n        # We will send these for all subs when we reconnect anyway,\n        # so that we can suppress here.\n        if not self.is_reconnecting:\n            yield from self.auto_unsubscribe(ssid, max_msgs)", "code_tokens": ["def", "unsubscribe", "(", "self", ",", "ssid", ",", "max_msgs", "=", "0", ")", ":", "if", "self", ".", "is_closed", ":", "raise", "ErrConnectionClosed", "if", "self", ".", "is_draining", ":", "raise", "ErrConnectionDraining", "self", ".", "_remove_sub", "(", "ssid", ",", "max_msgs", ")", "# We will send these for all subs when we reconnect anyway,", "# so that we can suppress here.", "if", "not", "self", ".", "is_reconnecting", ":", "yield", "from", "self", ".", "auto_unsubscribe", "(", "ssid", ",", "max_msgs", ")"], "docstring": "Takes a subscription sequence id and removes the subscription\n        from the client, optionally after receiving more than max_msgs.", "docstring_tokens": ["Takes", "a", "subscription", "sequence", "id", "and", "removes", "the", "subscription", "from", "the", "client", "optionally", "after", "receiving", "more", "than", "max_msgs", "."], "sha": "39e840be0b12ce326edac0bba69aeb1be930dcb8", "url": "https://github.com/nats-io/asyncio-nats/blob/39e840be0b12ce326edac0bba69aeb1be930dcb8/nats/aio/client.py#L600-L615", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/frame.py", "func_name": "H2OFrame.grep", "original_string": "def grep(self,pattern, ignore_case = False, invert = False, output_logical = False):\n        \"\"\"\n        Searches for matches to argument `pattern` within each element\n        of a string column.\n\n        Default behavior is to return indices of the elements matching the pattern. Parameter\n        `output_logical` can be used to return a logical vector indicating if the element matches\n        the pattern (1) or not (0).\n\n        :param str pattern: A character string containing a regular expression.\n        :param bool ignore_case: If True, then case is ignored during matching.\n        :param bool invert:  If True, then identify elements that do not match the pattern.\n        :param bool output_logical: If True, then return logical vector of indicators instead of list of matching positions\n        :return: H2OFrame holding the matching positions or a logical list if `output_logical` is enabled.\n        \"\"\"\n        return H2OFrame._expr(expr=ExprNode(\"grep\", self, pattern, ignore_case, invert, output_logical))", "language": "python", "code": "def grep(self,pattern, ignore_case = False, invert = False, output_logical = False):\n        \"\"\"\n        Searches for matches to argument `pattern` within each element\n        of a string column.\n\n        Default behavior is to return indices of the elements matching the pattern. Parameter\n        `output_logical` can be used to return a logical vector indicating if the element matches\n        the pattern (1) or not (0).\n\n        :param str pattern: A character string containing a regular expression.\n        :param bool ignore_case: If True, then case is ignored during matching.\n        :param bool invert:  If True, then identify elements that do not match the pattern.\n        :param bool output_logical: If True, then return logical vector of indicators instead of list of matching positions\n        :return: H2OFrame holding the matching positions or a logical list if `output_logical` is enabled.\n        \"\"\"\n        return H2OFrame._expr(expr=ExprNode(\"grep\", self, pattern, ignore_case, invert, output_logical))", "code_tokens": ["def", "grep", "(", "self", ",", "pattern", ",", "ignore_case", "=", "False", ",", "invert", "=", "False", ",", "output_logical", "=", "False", ")", ":", "return", "H2OFrame", ".", "_expr", "(", "expr", "=", "ExprNode", "(", "\"grep\"", ",", "self", ",", "pattern", ",", "ignore_case", ",", "invert", ",", "output_logical", ")", ")"], "docstring": "Searches for matches to argument `pattern` within each element\n        of a string column.\n\n        Default behavior is to return indices of the elements matching the pattern. Parameter\n        `output_logical` can be used to return a logical vector indicating if the element matches\n        the pattern (1) or not (0).\n\n        :param str pattern: A character string containing a regular expression.\n        :param bool ignore_case: If True, then case is ignored during matching.\n        :param bool invert:  If True, then identify elements that do not match the pattern.\n        :param bool output_logical: If True, then return logical vector of indicators instead of list of matching positions\n        :return: H2OFrame holding the matching positions or a logical list if `output_logical` is enabled.", "docstring_tokens": ["Searches", "for", "matches", "to", "argument", "pattern", "within", "each", "element", "of", "a", "string", "column", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/frame.py#L2932-L2947", "partition": "test"}
{"repo": "edx/web-fragments", "path": "web_fragments/fragment.py", "func_name": "Fragment.to_dict", "original_string": "def to_dict(self):\n        \"\"\"\n        Returns the fragment in a dictionary representation.\n        \"\"\"\n        return {\n            'content': self.content,\n            'resources': [r._asdict() for r in self.resources],  # pylint: disable=W0212\n            'js_init_fn': self.js_init_fn,\n            'js_init_version': self.js_init_version,\n            'json_init_args': self.json_init_args\n        }", "language": "python", "code": "def to_dict(self):\n        \"\"\"\n        Returns the fragment in a dictionary representation.\n        \"\"\"\n        return {\n            'content': self.content,\n            'resources': [r._asdict() for r in self.resources],  # pylint: disable=W0212\n            'js_init_fn': self.js_init_fn,\n            'js_init_version': self.js_init_version,\n            'json_init_args': self.json_init_args\n        }", "code_tokens": ["def", "to_dict", "(", "self", ")", ":", "return", "{", "'content'", ":", "self", ".", "content", ",", "'resources'", ":", "[", "r", ".", "_asdict", "(", ")", "for", "r", "in", "self", ".", "resources", "]", ",", "# pylint: disable=W0212", "'js_init_fn'", ":", "self", ".", "js_init_fn", ",", "'js_init_version'", ":", "self", ".", "js_init_version", ",", "'json_init_args'", ":", "self", ".", "json_init_args", "}"], "docstring": "Returns the fragment in a dictionary representation.", "docstring_tokens": ["Returns", "the", "fragment", "in", "a", "dictionary", "representation", "."], "sha": "42d760d700d70465e4e573b7b41442d8802ccd3c", "url": "https://github.com/edx/web-fragments/blob/42d760d700d70465e4e573b7b41442d8802ccd3c/web_fragments/fragment.py#L54-L64", "partition": "test"}
{"repo": "idlesign/django-etc", "path": "etc/toolbox.py", "func_name": "set_form_widgets_attrs", "original_string": "def set_form_widgets_attrs(form, attrs):\n    \"\"\"Applies a given HTML attributes to each field widget of a given form.\n\n    Example:\n\n        set_form_widgets_attrs(my_form, {'class': 'clickable'})\n\n    \"\"\"\n    for _, field in form.fields.items():\n        attrs_ = dict(attrs)\n        for name, val in attrs.items():\n            if hasattr(val, '__call__'):\n                attrs_[name] = val(field)\n        field.widget.attrs = field.widget.build_attrs(attrs_)", "language": "python", "code": "def set_form_widgets_attrs(form, attrs):\n    \"\"\"Applies a given HTML attributes to each field widget of a given form.\n\n    Example:\n\n        set_form_widgets_attrs(my_form, {'class': 'clickable'})\n\n    \"\"\"\n    for _, field in form.fields.items():\n        attrs_ = dict(attrs)\n        for name, val in attrs.items():\n            if hasattr(val, '__call__'):\n                attrs_[name] = val(field)\n        field.widget.attrs = field.widget.build_attrs(attrs_)", "code_tokens": ["def", "set_form_widgets_attrs", "(", "form", ",", "attrs", ")", ":", "for", "_", ",", "field", "in", "form", ".", "fields", ".", "items", "(", ")", ":", "attrs_", "=", "dict", "(", "attrs", ")", "for", "name", ",", "val", "in", "attrs", ".", "items", "(", ")", ":", "if", "hasattr", "(", "val", ",", "'__call__'", ")", ":", "attrs_", "[", "name", "]", "=", "val", "(", "field", ")", "field", ".", "widget", ".", "attrs", "=", "field", ".", "widget", ".", "build_attrs", "(", "attrs_", ")"], "docstring": "Applies a given HTML attributes to each field widget of a given form.\n\n    Example:\n\n        set_form_widgets_attrs(my_form, {'class': 'clickable'})", "docstring_tokens": ["Applies", "a", "given", "HTML", "attributes", "to", "each", "field", "widget", "of", "a", "given", "form", "."], "sha": "dbfc7e9dfc4fdfe69547f71ba4921989f9e97dbe", "url": "https://github.com/idlesign/django-etc/blob/dbfc7e9dfc4fdfe69547f71ba4921989f9e97dbe/etc/toolbox.py#L74-L87", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/backend/connection.py", "func_name": "H2OConnection.session_id", "original_string": "def session_id(self):\n        \"\"\"\n        Return the session id of the current connection.\n\n        The session id is issued (through an API request) the first time it is requested, but no sooner. This is\n        because generating a session id puts it into the DKV on the server, which effectively locks the cluster. Once\n        issued, the session id will stay the same until the connection is closed.\n        \"\"\"\n        if self._session_id is None:\n            req = self.request(\"POST /4/sessions\")\n            self._session_id = req.get(\"session_key\") or req.get(\"session_id\")\n        return CallableString(self._session_id)", "language": "python", "code": "def session_id(self):\n        \"\"\"\n        Return the session id of the current connection.\n\n        The session id is issued (through an API request) the first time it is requested, but no sooner. This is\n        because generating a session id puts it into the DKV on the server, which effectively locks the cluster. Once\n        issued, the session id will stay the same until the connection is closed.\n        \"\"\"\n        if self._session_id is None:\n            req = self.request(\"POST /4/sessions\")\n            self._session_id = req.get(\"session_key\") or req.get(\"session_id\")\n        return CallableString(self._session_id)", "code_tokens": ["def", "session_id", "(", "self", ")", ":", "if", "self", ".", "_session_id", "is", "None", ":", "req", "=", "self", ".", "request", "(", "\"POST /4/sessions\"", ")", "self", ".", "_session_id", "=", "req", ".", "get", "(", "\"session_key\"", ")", "or", "req", ".", "get", "(", "\"session_id\"", ")", "return", "CallableString", "(", "self", ".", "_session_id", ")"], "docstring": "Return the session id of the current connection.\n\n        The session id is issued (through an API request) the first time it is requested, but no sooner. This is\n        because generating a session id puts it into the DKV on the server, which effectively locks the cluster. Once\n        issued, the session id will stay the same until the connection is closed.", "docstring_tokens": ["Return", "the", "session", "id", "of", "the", "current", "connection", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/backend/connection.py#L448-L459", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/adapter/mongo/clinvar.py", "func_name": "ClinVarHandler.update_clinvar_id", "original_string": "def update_clinvar_id(self, clinvar_id, submission_id ):\n        \"\"\"saves an official clinvar submission ID in a clinvar submission object\n\n            Args:\n                clinvar_id(str): a string with a format: SUB[0-9]. It is obtained from clinvar portal when starting a new submission\n                submission_id(str): submission_id(str) : id of the submission to be updated\n\n            Returns:\n                updated_submission(obj): a clinvar submission object, updated\n        \"\"\"\n        updated_submission = self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, { '$set' : {'clinvar_subm_id' : clinvar_id, 'updated_at': datetime.now()} }, upsert=True, return_document=pymongo.ReturnDocument.AFTER )\n        return updated_submission", "language": "python", "code": "def update_clinvar_id(self, clinvar_id, submission_id ):\n        \"\"\"saves an official clinvar submission ID in a clinvar submission object\n\n            Args:\n                clinvar_id(str): a string with a format: SUB[0-9]. It is obtained from clinvar portal when starting a new submission\n                submission_id(str): submission_id(str) : id of the submission to be updated\n\n            Returns:\n                updated_submission(obj): a clinvar submission object, updated\n        \"\"\"\n        updated_submission = self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, { '$set' : {'clinvar_subm_id' : clinvar_id, 'updated_at': datetime.now()} }, upsert=True, return_document=pymongo.ReturnDocument.AFTER )\n        return updated_submission", "code_tokens": ["def", "update_clinvar_id", "(", "self", ",", "clinvar_id", ",", "submission_id", ")", ":", "updated_submission", "=", "self", ".", "clinvar_submission_collection", ".", "find_one_and_update", "(", "{", "'_id'", ":", "ObjectId", "(", "submission_id", ")", "}", ",", "{", "'$set'", ":", "{", "'clinvar_subm_id'", ":", "clinvar_id", ",", "'updated_at'", ":", "datetime", ".", "now", "(", ")", "}", "}", ",", "upsert", "=", "True", ",", "return_document", "=", "pymongo", ".", "ReturnDocument", ".", "AFTER", ")", "return", "updated_submission"], "docstring": "saves an official clinvar submission ID in a clinvar submission object\n\n            Args:\n                clinvar_id(str): a string with a format: SUB[0-9]. It is obtained from clinvar portal when starting a new submission\n                submission_id(str): submission_id(str) : id of the submission to be updated\n\n            Returns:\n                updated_submission(obj): a clinvar submission object, updated", "docstring_tokens": ["saves", "an", "official", "clinvar", "submission", "ID", "in", "a", "clinvar", "submission", "object"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/adapter/mongo/clinvar.py#L95-L106", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/utils/hdf5compression.py", "func_name": "compact_hdf5_file", "original_string": "def compact_hdf5_file(filename, name=None, index=None, keep_backup=True):\n    \"\"\"Can compress an HDF5 to reduce file size.\n\n    The properties on how to compress the new file are taken from a given\n    trajectory in the file.\n    Simply calls ``ptrepack`` from the command line.\n    (Se also https://pytables.github.io/usersguide/utilities.html#ptrepackdescr)\n\n    Currently only supported under Linux, no guarantee for Windows usage.\n\n    :param filename:\n\n        Name of the file to compact\n\n    :param name:\n\n        The name of the trajectory from which the compression properties are taken\n\n    :param index:\n\n        Instead of a name you could also specify an index, i.e -1 for the last trajectory\n        in the file.\n\n    :param keep_backup:\n\n        If a back up version of the original file should be kept.\n        The backup file is named as the original but `_backup` is appended to the end.\n\n    :return:\n\n        The return/error code of ptrepack\n\n    \"\"\"\n    if name is None and index is None:\n        index = -1\n\n    tmp_traj = load_trajectory(name, index, as_new=False, load_all=pypetconstants.LOAD_NOTHING,\n                               force=True, filename=filename)\n    service = tmp_traj.v_storage_service\n    complevel = service.complevel\n    complib = service.complib\n    shuffle = service.shuffle\n    fletcher32 = service.fletcher32\n\n    name_wo_ext, ext = os.path.splitext(filename)\n    tmp_filename = name_wo_ext + '_tmp' + ext\n\n    abs_filename = os.path.abspath(filename)\n    abs_tmp_filename = os.path.abspath(tmp_filename)\n\n    command = ['ptrepack', '-v',\n               '--complib', complib,\n               '--complevel', str(complevel),\n               '--shuffle', str(int(shuffle)),\n               '--fletcher32', str(int(fletcher32)),\n               abs_filename, abs_tmp_filename]\n    str_command = ' '.join(command)\n    print('Executing command `%s`' % str_command)\n\n    retcode = subprocess.call(command)\n    if retcode != 0:\n        print('#### ERROR: Compacting `%s` failed with errorcode %s! ####' %\n              (filename, str(retcode)))\n    else:\n        print('#### Compacting successful ####')\n        print('Renaming files')\n        if keep_backup:\n            backup_file_name = name_wo_ext + '_backup' + ext\n            os.rename(filename, backup_file_name)\n        else:\n            os.remove(filename)\n        os.rename(tmp_filename, filename)\n        print('### Compacting and Renaming finished ####')\n\n    return retcode", "language": "python", "code": "def compact_hdf5_file(filename, name=None, index=None, keep_backup=True):\n    \"\"\"Can compress an HDF5 to reduce file size.\n\n    The properties on how to compress the new file are taken from a given\n    trajectory in the file.\n    Simply calls ``ptrepack`` from the command line.\n    (Se also https://pytables.github.io/usersguide/utilities.html#ptrepackdescr)\n\n    Currently only supported under Linux, no guarantee for Windows usage.\n\n    :param filename:\n\n        Name of the file to compact\n\n    :param name:\n\n        The name of the trajectory from which the compression properties are taken\n\n    :param index:\n\n        Instead of a name you could also specify an index, i.e -1 for the last trajectory\n        in the file.\n\n    :param keep_backup:\n\n        If a back up version of the original file should be kept.\n        The backup file is named as the original but `_backup` is appended to the end.\n\n    :return:\n\n        The return/error code of ptrepack\n\n    \"\"\"\n    if name is None and index is None:\n        index = -1\n\n    tmp_traj = load_trajectory(name, index, as_new=False, load_all=pypetconstants.LOAD_NOTHING,\n                               force=True, filename=filename)\n    service = tmp_traj.v_storage_service\n    complevel = service.complevel\n    complib = service.complib\n    shuffle = service.shuffle\n    fletcher32 = service.fletcher32\n\n    name_wo_ext, ext = os.path.splitext(filename)\n    tmp_filename = name_wo_ext + '_tmp' + ext\n\n    abs_filename = os.path.abspath(filename)\n    abs_tmp_filename = os.path.abspath(tmp_filename)\n\n    command = ['ptrepack', '-v',\n               '--complib', complib,\n               '--complevel', str(complevel),\n               '--shuffle', str(int(shuffle)),\n               '--fletcher32', str(int(fletcher32)),\n               abs_filename, abs_tmp_filename]\n    str_command = ' '.join(command)\n    print('Executing command `%s`' % str_command)\n\n    retcode = subprocess.call(command)\n    if retcode != 0:\n        print('#### ERROR: Compacting `%s` failed with errorcode %s! ####' %\n              (filename, str(retcode)))\n    else:\n        print('#### Compacting successful ####')\n        print('Renaming files')\n        if keep_backup:\n            backup_file_name = name_wo_ext + '_backup' + ext\n            os.rename(filename, backup_file_name)\n        else:\n            os.remove(filename)\n        os.rename(tmp_filename, filename)\n        print('### Compacting and Renaming finished ####')\n\n    return retcode", "code_tokens": ["def", "compact_hdf5_file", "(", "filename", ",", "name", "=", "None", ",", "index", "=", "None", ",", "keep_backup", "=", "True", ")", ":", "if", "name", "is", "None", "and", "index", "is", "None", ":", "index", "=", "-", "1", "tmp_traj", "=", "load_trajectory", "(", "name", ",", "index", ",", "as_new", "=", "False", ",", "load_all", "=", "pypetconstants", ".", "LOAD_NOTHING", ",", "force", "=", "True", ",", "filename", "=", "filename", ")", "service", "=", "tmp_traj", ".", "v_storage_service", "complevel", "=", "service", ".", "complevel", "complib", "=", "service", ".", "complib", "shuffle", "=", "service", ".", "shuffle", "fletcher32", "=", "service", ".", "fletcher32", "name_wo_ext", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "filename", ")", "tmp_filename", "=", "name_wo_ext", "+", "'_tmp'", "+", "ext", "abs_filename", "=", "os", ".", "path", ".", "abspath", "(", "filename", ")", "abs_tmp_filename", "=", "os", ".", "path", ".", "abspath", "(", "tmp_filename", ")", "command", "=", "[", "'ptrepack'", ",", "'-v'", ",", "'--complib'", ",", "complib", ",", "'--complevel'", ",", "str", "(", "complevel", ")", ",", "'--shuffle'", ",", "str", "(", "int", "(", "shuffle", ")", ")", ",", "'--fletcher32'", ",", "str", "(", "int", "(", "fletcher32", ")", ")", ",", "abs_filename", ",", "abs_tmp_filename", "]", "str_command", "=", "' '", ".", "join", "(", "command", ")", "print", "(", "'Executing command `%s`'", "%", "str_command", ")", "retcode", "=", "subprocess", ".", "call", "(", "command", ")", "if", "retcode", "!=", "0", ":", "print", "(", "'#### ERROR: Compacting `%s` failed with errorcode %s! ####'", "%", "(", "filename", ",", "str", "(", "retcode", ")", ")", ")", "else", ":", "print", "(", "'#### Compacting successful ####'", ")", "print", "(", "'Renaming files'", ")", "if", "keep_backup", ":", "backup_file_name", "=", "name_wo_ext", "+", "'_backup'", "+", "ext", "os", ".", "rename", "(", "filename", ",", "backup_file_name", ")", "else", ":", "os", ".", "remove", "(", "filename", ")", "os", ".", "rename", "(", "tmp_filename", ",", "filename", ")", "print", "(", "'### Compacting and Renaming finished ####'", ")", "return", "retcode"], "docstring": "Can compress an HDF5 to reduce file size.\n\n    The properties on how to compress the new file are taken from a given\n    trajectory in the file.\n    Simply calls ``ptrepack`` from the command line.\n    (Se also https://pytables.github.io/usersguide/utilities.html#ptrepackdescr)\n\n    Currently only supported under Linux, no guarantee for Windows usage.\n\n    :param filename:\n\n        Name of the file to compact\n\n    :param name:\n\n        The name of the trajectory from which the compression properties are taken\n\n    :param index:\n\n        Instead of a name you could also specify an index, i.e -1 for the last trajectory\n        in the file.\n\n    :param keep_backup:\n\n        If a back up version of the original file should be kept.\n        The backup file is named as the original but `_backup` is appended to the end.\n\n    :return:\n\n        The return/error code of ptrepack", "docstring_tokens": ["Can", "compress", "an", "HDF5", "to", "reduce", "file", "size", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/utils/hdf5compression.py#L12-L86", "partition": "test"}
{"repo": "avelino/bottle-auth", "path": "bottle_auth/core/auth.py", "func_name": "GoogleMixin.authorize_redirect", "original_string": "def authorize_redirect(self, oauth_scope, callback_uri=None,\n                           ax_attrs=[\"name\",\"email\",\"language\",\"username\"]):\n        \"\"\"Authenticates and authorizes for the given Google resource.\n\n        Some of the available resources are:\n\n        * Gmail Contacts - http://www.google.com/m8/feeds/\n        * Calendar - http://www.google.com/calendar/feeds/\n        * Finance - http://finance.google.com/finance/feeds/\n\n        You can authorize multiple resources by separating the resource\n        URLs with a space.\n        \"\"\"\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs,\n                                 oauth_scope=oauth_scope)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))", "language": "python", "code": "def authorize_redirect(self, oauth_scope, callback_uri=None,\n                           ax_attrs=[\"name\",\"email\",\"language\",\"username\"]):\n        \"\"\"Authenticates and authorizes for the given Google resource.\n\n        Some of the available resources are:\n\n        * Gmail Contacts - http://www.google.com/m8/feeds/\n        * Calendar - http://www.google.com/calendar/feeds/\n        * Finance - http://finance.google.com/finance/feeds/\n\n        You can authorize multiple resources by separating the resource\n        URLs with a space.\n        \"\"\"\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs,\n                                 oauth_scope=oauth_scope)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))", "code_tokens": ["def", "authorize_redirect", "(", "self", ",", "oauth_scope", ",", "callback_uri", "=", "None", ",", "ax_attrs", "=", "[", "\"name\"", ",", "\"email\"", ",", "\"language\"", ",", "\"username\"", "]", ")", ":", "callback_uri", "=", "callback_uri", "or", "self", ".", "request", ".", "uri", "args", "=", "self", ".", "_openid_args", "(", "callback_uri", ",", "ax_attrs", "=", "ax_attrs", ",", "oauth_scope", "=", "oauth_scope", ")", "self", ".", "redirect", "(", "self", ".", "_OPENID_ENDPOINT", "+", "\"?\"", "+", "urllib", ".", "urlencode", "(", "args", ")", ")"], "docstring": "Authenticates and authorizes for the given Google resource.\n\n        Some of the available resources are:\n\n        * Gmail Contacts - http://www.google.com/m8/feeds/\n        * Calendar - http://www.google.com/calendar/feeds/\n        * Finance - http://finance.google.com/finance/feeds/\n\n        You can authorize multiple resources by separating the resource\n        URLs with a space.", "docstring_tokens": ["Authenticates", "and", "authorizes", "for", "the", "given", "Google", "resource", "."], "sha": "db07e526864aeac05ee68444b47e5db29540ce18", "url": "https://github.com/avelino/bottle-auth/blob/db07e526864aeac05ee68444b47e5db29540ce18/bottle_auth/core/auth.py#L885-L901", "partition": "test"}
{"repo": "inveniosoftware/invenio-records-ui", "path": "examples/permsapp.py", "func_name": "my_permission_factory", "original_string": "def my_permission_factory(record, *args, **kwargs):\n    \"\"\"My permission factory.\"\"\"\n    def can(self):\n        rec = Record.get_record(record.id)\n        return rec.get('access', '') == 'open'\n    return type('MyPermissionChecker', (), {'can': can})()", "language": "python", "code": "def my_permission_factory(record, *args, **kwargs):\n    \"\"\"My permission factory.\"\"\"\n    def can(self):\n        rec = Record.get_record(record.id)\n        return rec.get('access', '') == 'open'\n    return type('MyPermissionChecker', (), {'can': can})()", "code_tokens": ["def", "my_permission_factory", "(", "record", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "def", "can", "(", "self", ")", ":", "rec", "=", "Record", ".", "get_record", "(", "record", ".", "id", ")", "return", "rec", ".", "get", "(", "'access'", ",", "''", ")", "==", "'open'", "return", "type", "(", "'MyPermissionChecker'", ",", "(", ")", ",", "{", "'can'", ":", "can", "}", ")", "(", ")"], "docstring": "My permission factory.", "docstring_tokens": ["My", "permission", "factory", "."], "sha": "ae92367978f2e1e96634685bd296f0fd92b4da54", "url": "https://github.com/inveniosoftware/invenio-records-ui/blob/ae92367978f2e1e96634685bd296f0fd92b4da54/examples/permsapp.py#L66-L71", "partition": "test"}
{"repo": "vaexio/vaex", "path": "packages/vaex-viz/vaex/viz/mpl.py", "func_name": "patch", "original_string": "def patch(f):\n    '''Adds method f to the DataFrame class'''\n    name = f.__name__\n    setattr(DataFrame, name, f)\n    return f", "language": "python", "code": "def patch(f):\n    '''Adds method f to the DataFrame class'''\n    name = f.__name__\n    setattr(DataFrame, name, f)\n    return f", "code_tokens": ["def", "patch", "(", "f", ")", ":", "name", "=", "f", ".", "__name__", "setattr", "(", "DataFrame", ",", "name", ",", "f", ")", "return", "f"], "docstring": "Adds method f to the DataFrame class", "docstring_tokens": ["Adds", "method", "f", "to", "the", "DataFrame", "class"], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-viz/vaex/viz/mpl.py#L21-L25", "partition": "test"}
{"repo": "singularityhub/sregistry-cli", "path": "sregistry/auth/secrets.py", "func_name": "read_client_secrets", "original_string": "def read_client_secrets():\n    '''for private or protected registries, a client secrets file is required\n       to be located at .sregistry. If no secrets are found, we use default\n       of Singularity Hub, and return a dummy secrets.\n    '''\n    client_secrets = _default_client_secrets()\n\n    # If token file not provided, check environment\n    secrets = get_secrets_file()\n\n    # If exists, load\n    if secrets is not None:\n        client_secrets = read_json(secrets)\n\n    # Otherwise, initialize\n    else:\n        from sregistry.defaults import SREGISTRY_CLIENT_SECRETS\n        write_json(client_secrets, SREGISTRY_CLIENT_SECRETS)\n\n    return client_secrets", "language": "python", "code": "def read_client_secrets():\n    '''for private or protected registries, a client secrets file is required\n       to be located at .sregistry. If no secrets are found, we use default\n       of Singularity Hub, and return a dummy secrets.\n    '''\n    client_secrets = _default_client_secrets()\n\n    # If token file not provided, check environment\n    secrets = get_secrets_file()\n\n    # If exists, load\n    if secrets is not None:\n        client_secrets = read_json(secrets)\n\n    # Otherwise, initialize\n    else:\n        from sregistry.defaults import SREGISTRY_CLIENT_SECRETS\n        write_json(client_secrets, SREGISTRY_CLIENT_SECRETS)\n\n    return client_secrets", "code_tokens": ["def", "read_client_secrets", "(", ")", ":", "client_secrets", "=", "_default_client_secrets", "(", ")", "# If token file not provided, check environment", "secrets", "=", "get_secrets_file", "(", ")", "# If exists, load", "if", "secrets", "is", "not", "None", ":", "client_secrets", "=", "read_json", "(", "secrets", ")", "# Otherwise, initialize", "else", ":", "from", "sregistry", ".", "defaults", "import", "SREGISTRY_CLIENT_SECRETS", "write_json", "(", "client_secrets", ",", "SREGISTRY_CLIENT_SECRETS", ")", "return", "client_secrets"], "docstring": "for private or protected registries, a client secrets file is required\n       to be located at .sregistry. If no secrets are found, we use default\n       of Singularity Hub, and return a dummy secrets.", "docstring_tokens": ["for", "private", "or", "protected", "registries", "a", "client", "secrets", "file", "is", "required", "to", "be", "located", "at", ".", "sregistry", ".", "If", "no", "secrets", "are", "found", "we", "use", "default", "of", "Singularity", "Hub", "and", "return", "a", "dummy", "secrets", "."], "sha": "abc96140a1d15b5e96d83432e1e0e1f4f8f36331", "url": "https://github.com/singularityhub/sregistry-cli/blob/abc96140a1d15b5e96d83432e1e0e1f4f8f36331/sregistry/auth/secrets.py#L77-L96", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/ultratb.py", "func_name": "VerboseTB.debugger", "original_string": "def debugger(self,force=False):\n        \"\"\"Call up the pdb debugger if desired, always clean up the tb\n        reference.\n\n        Keywords:\n\n          - force(False): by default, this routine checks the instance call_pdb\n          flag and does not actually invoke the debugger if the flag is false.\n          The 'force' option forces the debugger to activate even if the flag\n          is false.\n\n        If the call_pdb flag is set, the pdb interactive debugger is\n        invoked. In all cases, the self.tb reference to the current traceback\n        is deleted to prevent lingering references which hamper memory\n        management.\n\n        Note that each call to pdb() does an 'import readline', so if your app\n        requires a special setup for the readline completers, you'll have to\n        fix that by hand after invoking the exception handler.\"\"\"\n\n        if force or self.call_pdb:\n            if self.pdb is None:\n                self.pdb = debugger.Pdb(\n                    self.color_scheme_table.active_scheme_name)\n            # the system displayhook may have changed, restore the original\n            # for pdb\n            display_trap = DisplayTrap(hook=sys.__displayhook__)\n            with display_trap:\n                self.pdb.reset()\n                # Find the right frame so we don't pop up inside ipython itself\n                if hasattr(self,'tb') and self.tb is not None:\n                    etb = self.tb\n                else:\n                    etb = self.tb = sys.last_traceback\n                while self.tb is not None and self.tb.tb_next is not None:\n                    self.tb = self.tb.tb_next\n                if etb and etb.tb_next:\n                    etb = etb.tb_next\n                self.pdb.botframe = etb.tb_frame\n                self.pdb.interaction(self.tb.tb_frame, self.tb)\n\n        if hasattr(self,'tb'):\n            del self.tb", "language": "python", "code": "def debugger(self,force=False):\n        \"\"\"Call up the pdb debugger if desired, always clean up the tb\n        reference.\n\n        Keywords:\n\n          - force(False): by default, this routine checks the instance call_pdb\n          flag and does not actually invoke the debugger if the flag is false.\n          The 'force' option forces the debugger to activate even if the flag\n          is false.\n\n        If the call_pdb flag is set, the pdb interactive debugger is\n        invoked. In all cases, the self.tb reference to the current traceback\n        is deleted to prevent lingering references which hamper memory\n        management.\n\n        Note that each call to pdb() does an 'import readline', so if your app\n        requires a special setup for the readline completers, you'll have to\n        fix that by hand after invoking the exception handler.\"\"\"\n\n        if force or self.call_pdb:\n            if self.pdb is None:\n                self.pdb = debugger.Pdb(\n                    self.color_scheme_table.active_scheme_name)\n            # the system displayhook may have changed, restore the original\n            # for pdb\n            display_trap = DisplayTrap(hook=sys.__displayhook__)\n            with display_trap:\n                self.pdb.reset()\n                # Find the right frame so we don't pop up inside ipython itself\n                if hasattr(self,'tb') and self.tb is not None:\n                    etb = self.tb\n                else:\n                    etb = self.tb = sys.last_traceback\n                while self.tb is not None and self.tb.tb_next is not None:\n                    self.tb = self.tb.tb_next\n                if etb and etb.tb_next:\n                    etb = etb.tb_next\n                self.pdb.botframe = etb.tb_frame\n                self.pdb.interaction(self.tb.tb_frame, self.tb)\n\n        if hasattr(self,'tb'):\n            del self.tb", "code_tokens": ["def", "debugger", "(", "self", ",", "force", "=", "False", ")", ":", "if", "force", "or", "self", ".", "call_pdb", ":", "if", "self", ".", "pdb", "is", "None", ":", "self", ".", "pdb", "=", "debugger", ".", "Pdb", "(", "self", ".", "color_scheme_table", ".", "active_scheme_name", ")", "# the system displayhook may have changed, restore the original", "# for pdb", "display_trap", "=", "DisplayTrap", "(", "hook", "=", "sys", ".", "__displayhook__", ")", "with", "display_trap", ":", "self", ".", "pdb", ".", "reset", "(", ")", "# Find the right frame so we don't pop up inside ipython itself", "if", "hasattr", "(", "self", ",", "'tb'", ")", "and", "self", ".", "tb", "is", "not", "None", ":", "etb", "=", "self", ".", "tb", "else", ":", "etb", "=", "self", ".", "tb", "=", "sys", ".", "last_traceback", "while", "self", ".", "tb", "is", "not", "None", "and", "self", ".", "tb", ".", "tb_next", "is", "not", "None", ":", "self", ".", "tb", "=", "self", ".", "tb", ".", "tb_next", "if", "etb", "and", "etb", ".", "tb_next", ":", "etb", "=", "etb", ".", "tb_next", "self", ".", "pdb", ".", "botframe", "=", "etb", ".", "tb_frame", "self", ".", "pdb", ".", "interaction", "(", "self", ".", "tb", ".", "tb_frame", ",", "self", ".", "tb", ")", "if", "hasattr", "(", "self", ",", "'tb'", ")", ":", "del", "self", ".", "tb"], "docstring": "Call up the pdb debugger if desired, always clean up the tb\n        reference.\n\n        Keywords:\n\n          - force(False): by default, this routine checks the instance call_pdb\n          flag and does not actually invoke the debugger if the flag is false.\n          The 'force' option forces the debugger to activate even if the flag\n          is false.\n\n        If the call_pdb flag is set, the pdb interactive debugger is\n        invoked. In all cases, the self.tb reference to the current traceback\n        is deleted to prevent lingering references which hamper memory\n        management.\n\n        Note that each call to pdb() does an 'import readline', so if your app\n        requires a special setup for the readline completers, you'll have to\n        fix that by hand after invoking the exception handler.", "docstring_tokens": ["Call", "up", "the", "pdb", "debugger", "if", "desired", "always", "clean", "up", "the", "tb", "reference", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/ultratb.py#L968-L1010", "partition": "test"}
{"repo": "LuminosoInsight/luminoso-api-client-python", "path": "luminoso_api/v4_client.py", "func_name": "LuminosoClient.save_token", "original_string": "def save_token(self, token_file=None):\n        \"\"\"\n        Obtain the user's long-lived API token and save it in a local file.\n        If the user has no long-lived API token, one will be created.\n        Returns the token that was saved.\n        \"\"\"\n        tokens = self._json_request('get', self.root_url + '/user/tokens/')\n        long_lived = [token['type'] == 'long_lived' for token in tokens]\n        if any(long_lived):\n            dic = tokens[long_lived.index(True)]\n        else:\n            # User doesn't have a long-lived token, so create one\n            dic = self._json_request('post', self.root_url + '/user/tokens/')\n        token = dic['token']\n        token_file = token_file or get_token_filename()\n        if os.path.exists(token_file):\n            saved_tokens = json.load(open(token_file))\n        else:\n            saved_tokens = {}\n        saved_tokens[urlparse(self.root_url).netloc] = token\n        directory, filename = os.path.split(token_file)\n        if directory and not os.path.exists(directory):\n            os.makedirs(directory)\n        with open(token_file, 'w') as f:\n            json.dump(saved_tokens, f)\n        return token", "language": "python", "code": "def save_token(self, token_file=None):\n        \"\"\"\n        Obtain the user's long-lived API token and save it in a local file.\n        If the user has no long-lived API token, one will be created.\n        Returns the token that was saved.\n        \"\"\"\n        tokens = self._json_request('get', self.root_url + '/user/tokens/')\n        long_lived = [token['type'] == 'long_lived' for token in tokens]\n        if any(long_lived):\n            dic = tokens[long_lived.index(True)]\n        else:\n            # User doesn't have a long-lived token, so create one\n            dic = self._json_request('post', self.root_url + '/user/tokens/')\n        token = dic['token']\n        token_file = token_file or get_token_filename()\n        if os.path.exists(token_file):\n            saved_tokens = json.load(open(token_file))\n        else:\n            saved_tokens = {}\n        saved_tokens[urlparse(self.root_url).netloc] = token\n        directory, filename = os.path.split(token_file)\n        if directory and not os.path.exists(directory):\n            os.makedirs(directory)\n        with open(token_file, 'w') as f:\n            json.dump(saved_tokens, f)\n        return token", "code_tokens": ["def", "save_token", "(", "self", ",", "token_file", "=", "None", ")", ":", "tokens", "=", "self", ".", "_json_request", "(", "'get'", ",", "self", ".", "root_url", "+", "'/user/tokens/'", ")", "long_lived", "=", "[", "token", "[", "'type'", "]", "==", "'long_lived'", "for", "token", "in", "tokens", "]", "if", "any", "(", "long_lived", ")", ":", "dic", "=", "tokens", "[", "long_lived", ".", "index", "(", "True", ")", "]", "else", ":", "# User doesn't have a long-lived token, so create one", "dic", "=", "self", ".", "_json_request", "(", "'post'", ",", "self", ".", "root_url", "+", "'/user/tokens/'", ")", "token", "=", "dic", "[", "'token'", "]", "token_file", "=", "token_file", "or", "get_token_filename", "(", ")", "if", "os", ".", "path", ".", "exists", "(", "token_file", ")", ":", "saved_tokens", "=", "json", ".", "load", "(", "open", "(", "token_file", ")", ")", "else", ":", "saved_tokens", "=", "{", "}", "saved_tokens", "[", "urlparse", "(", "self", ".", "root_url", ")", ".", "netloc", "]", "=", "token", "directory", ",", "filename", "=", "os", ".", "path", ".", "split", "(", "token_file", ")", "if", "directory", "and", "not", "os", ".", "path", ".", "exists", "(", "directory", ")", ":", "os", ".", "makedirs", "(", "directory", ")", "with", "open", "(", "token_file", ",", "'w'", ")", "as", "f", ":", "json", ".", "dump", "(", "saved_tokens", ",", "f", ")", "return", "token"], "docstring": "Obtain the user's long-lived API token and save it in a local file.\n        If the user has no long-lived API token, one will be created.\n        Returns the token that was saved.", "docstring_tokens": ["Obtain", "the", "user", "s", "long", "-", "lived", "API", "token", "and", "save", "it", "in", "a", "local", "file", ".", "If", "the", "user", "has", "no", "long", "-", "lived", "API", "token", "one", "will", "be", "created", ".", "Returns", "the", "token", "that", "was", "saved", "."], "sha": "3bedf2a454aee39214c11fbf556ead3eecc27881", "url": "https://github.com/LuminosoInsight/luminoso-api-client-python/blob/3bedf2a454aee39214c11fbf556ead3eecc27881/luminoso_api/v4_client.py#L142-L167", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py", "func_name": "ServiceManagementService.get_hosted_service_properties", "original_string": "def get_hosted_service_properties(self, service_name, embed_detail=False):\n        '''\n        Retrieves system properties for the specified hosted service. These\n        properties include the service name and service type; the name of the\n        affinity group to which the service belongs, or its location if it is\n        not part of an affinity group; and optionally, information on the\n        service's deployments.\n\n        service_name:\n            Name of the hosted service.\n        embed_detail:\n            When True, the management service returns properties for all\n            deployments of the service, as well as for the service itself.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('embed_detail', embed_detail)\n        return self._perform_get(\n            self._get_hosted_service_path(service_name) +\n            '?embed-detail=' +\n            _str(embed_detail).lower(),\n            HostedService)", "language": "python", "code": "def get_hosted_service_properties(self, service_name, embed_detail=False):\n        '''\n        Retrieves system properties for the specified hosted service. These\n        properties include the service name and service type; the name of the\n        affinity group to which the service belongs, or its location if it is\n        not part of an affinity group; and optionally, information on the\n        service's deployments.\n\n        service_name:\n            Name of the hosted service.\n        embed_detail:\n            When True, the management service returns properties for all\n            deployments of the service, as well as for the service itself.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('embed_detail', embed_detail)\n        return self._perform_get(\n            self._get_hosted_service_path(service_name) +\n            '?embed-detail=' +\n            _str(embed_detail).lower(),\n            HostedService)", "code_tokens": ["def", "get_hosted_service_properties", "(", "self", ",", "service_name", ",", "embed_detail", "=", "False", ")", ":", "_validate_not_none", "(", "'service_name'", ",", "service_name", ")", "_validate_not_none", "(", "'embed_detail'", ",", "embed_detail", ")", "return", "self", ".", "_perform_get", "(", "self", ".", "_get_hosted_service_path", "(", "service_name", ")", "+", "'?embed-detail='", "+", "_str", "(", "embed_detail", ")", ".", "lower", "(", ")", ",", "HostedService", ")"], "docstring": "Retrieves system properties for the specified hosted service. These\n        properties include the service name and service type; the name of the\n        affinity group to which the service belongs, or its location if it is\n        not part of an affinity group; and optionally, information on the\n        service's deployments.\n\n        service_name:\n            Name of the hosted service.\n        embed_detail:\n            When True, the management service returns properties for all\n            deployments of the service, as well as for the service itself.", "docstring_tokens": ["Retrieves", "system", "properties", "for", "the", "specified", "hosted", "service", ".", "These", "properties", "include", "the", "service", "name", "and", "service", "type", ";", "the", "name", "of", "the", "affinity", "group", "to", "which", "the", "service", "belongs", "or", "its", "location", "if", "it", "is", "not", "part", "of", "an", "affinity", "group", ";", "and", "optionally", "information", "on", "the", "service", "s", "deployments", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py#L321-L341", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/lib/demo.py", "func_name": "Demo.fload", "original_string": "def fload(self):\n        \"\"\"Load file object.\"\"\"\n        # read data and parse into blocks\n        if hasattr(self, 'fobj') and self.fobj is not None:\n           self.fobj.close()\n        if hasattr(self.src, \"read\"):\n             # It seems to be a file or a file-like object\n            self.fobj = self.src\n        else:\n             # Assume it's a string or something that can be converted to one\n            self.fobj = open(self.fname)", "language": "python", "code": "def fload(self):\n        \"\"\"Load file object.\"\"\"\n        # read data and parse into blocks\n        if hasattr(self, 'fobj') and self.fobj is not None:\n           self.fobj.close()\n        if hasattr(self.src, \"read\"):\n             # It seems to be a file or a file-like object\n            self.fobj = self.src\n        else:\n             # Assume it's a string or something that can be converted to one\n            self.fobj = open(self.fname)", "code_tokens": ["def", "fload", "(", "self", ")", ":", "# read data and parse into blocks", "if", "hasattr", "(", "self", ",", "'fobj'", ")", "and", "self", ".", "fobj", "is", "not", "None", ":", "self", ".", "fobj", ".", "close", "(", ")", "if", "hasattr", "(", "self", ".", "src", ",", "\"read\"", ")", ":", "# It seems to be a file or a file-like object", "self", ".", "fobj", "=", "self", ".", "src", "else", ":", "# Assume it's a string or something that can be converted to one", "self", ".", "fobj", "=", "open", "(", "self", ".", "fname", ")"], "docstring": "Load file object.", "docstring_tokens": ["Load", "file", "object", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/lib/demo.py#L256-L266", "partition": "test"}
{"repo": "blockstack/zone-file-py", "path": "blockstack_zones/parse_zone_file.py", "func_name": "remove_comments", "original_string": "def remove_comments(text):\n    \"\"\"\n    Remove comments from a zonefile\n    \"\"\"\n    ret = []\n    lines = text.split(\"\\n\")\n    for line in lines:\n        if len(line) == 0:\n            continue \n\n        line = serialize(tokenize_line(line))\n        ret.append(line)\n\n    return \"\\n\".join(ret)", "language": "python", "code": "def remove_comments(text):\n    \"\"\"\n    Remove comments from a zonefile\n    \"\"\"\n    ret = []\n    lines = text.split(\"\\n\")\n    for line in lines:\n        if len(line) == 0:\n            continue \n\n        line = serialize(tokenize_line(line))\n        ret.append(line)\n\n    return \"\\n\".join(ret)", "code_tokens": ["def", "remove_comments", "(", "text", ")", ":", "ret", "=", "[", "]", "lines", "=", "text", ".", "split", "(", "\"\\n\"", ")", "for", "line", "in", "lines", ":", "if", "len", "(", "line", ")", "==", "0", ":", "continue", "line", "=", "serialize", "(", "tokenize_line", "(", "line", ")", ")", "ret", ".", "append", "(", "line", ")", "return", "\"\\n\"", ".", "join", "(", "ret", ")"], "docstring": "Remove comments from a zonefile", "docstring_tokens": ["Remove", "comments", "from", "a", "zonefile"], "sha": "c1078c8c3c28f0881bc9a3af53d4972c4a6862d0", "url": "https://github.com/blockstack/zone-file-py/blob/c1078c8c3c28f0881bc9a3af53d4972c4a6862d0/blockstack_zones/parse_zone_file.py#L182-L195", "partition": "test"}
{"repo": "marians/py-daterangestr", "path": "daterangestr.py", "func_name": "to_dates", "original_string": "def to_dates(param):\n    \"\"\"\n    This function takes a date string in various formats\n    and converts it to a normalized and validated date range. A list\n    with two elements is returned, lower and upper date boundary.\n\n    Valid inputs are, for example:\n    2012              => Jan 1 20012 - Dec 31 2012 (whole year)\n    201201            => Jan 1 2012  - Jan 31 2012 (whole month)\n    2012101           => Jan 1 2012 - Jan 1 2012   (whole day)\n    2011-2011         => same as \"2011\", which means whole year 2012\n    2011-2012         => Jan 1 2011 - Dec 31 2012  (two years)\n    201104-2012       => Apr 1 2011 - Dec 31 2012\n    201104-201203     => Apr 1 2011 - March 31 2012\n    20110408-2011     => Apr 8 2011 - Dec 31 2011\n    20110408-201105   => Apr 8 2011 - May 31 2011\n    20110408-20110507 => Apr 8 2011 - May 07 2011\n    2011-             => Jan 1 2012 - Dec 31 9999 (unlimited)\n    201104-           => Apr 1 2011 - Dec 31 9999 (unlimited)\n    20110408-         => Apr 8 2011 - Dec 31 9999 (unlimited)\n    -2011             Jan 1 0000 - Dez 31 2011\n    -201104           Jan 1 0000 - Apr 30, 2011\n    -20110408         Jan 1 0000 - Apr 8, 2011\n    \"\"\"\n    pos = param.find('-')\n    lower, upper = (None, None)\n    if pos == -1:\n        # no seperator given\n        lower, upper = (param, param)\n    else:\n        lower, upper = param.split('-')\n    ret = (expand_date_param(lower, 'lower'), expand_date_param(upper, 'upper'))\n    return ret", "language": "python", "code": "def to_dates(param):\n    \"\"\"\n    This function takes a date string in various formats\n    and converts it to a normalized and validated date range. A list\n    with two elements is returned, lower and upper date boundary.\n\n    Valid inputs are, for example:\n    2012              => Jan 1 20012 - Dec 31 2012 (whole year)\n    201201            => Jan 1 2012  - Jan 31 2012 (whole month)\n    2012101           => Jan 1 2012 - Jan 1 2012   (whole day)\n    2011-2011         => same as \"2011\", which means whole year 2012\n    2011-2012         => Jan 1 2011 - Dec 31 2012  (two years)\n    201104-2012       => Apr 1 2011 - Dec 31 2012\n    201104-201203     => Apr 1 2011 - March 31 2012\n    20110408-2011     => Apr 8 2011 - Dec 31 2011\n    20110408-201105   => Apr 8 2011 - May 31 2011\n    20110408-20110507 => Apr 8 2011 - May 07 2011\n    2011-             => Jan 1 2012 - Dec 31 9999 (unlimited)\n    201104-           => Apr 1 2011 - Dec 31 9999 (unlimited)\n    20110408-         => Apr 8 2011 - Dec 31 9999 (unlimited)\n    -2011             Jan 1 0000 - Dez 31 2011\n    -201104           Jan 1 0000 - Apr 30, 2011\n    -20110408         Jan 1 0000 - Apr 8, 2011\n    \"\"\"\n    pos = param.find('-')\n    lower, upper = (None, None)\n    if pos == -1:\n        # no seperator given\n        lower, upper = (param, param)\n    else:\n        lower, upper = param.split('-')\n    ret = (expand_date_param(lower, 'lower'), expand_date_param(upper, 'upper'))\n    return ret", "code_tokens": ["def", "to_dates", "(", "param", ")", ":", "pos", "=", "param", ".", "find", "(", "'-'", ")", "lower", ",", "upper", "=", "(", "None", ",", "None", ")", "if", "pos", "==", "-", "1", ":", "# no seperator given", "lower", ",", "upper", "=", "(", "param", ",", "param", ")", "else", ":", "lower", ",", "upper", "=", "param", ".", "split", "(", "'-'", ")", "ret", "=", "(", "expand_date_param", "(", "lower", ",", "'lower'", ")", ",", "expand_date_param", "(", "upper", ",", "'upper'", ")", ")", "return", "ret"], "docstring": "This function takes a date string in various formats\n    and converts it to a normalized and validated date range. A list\n    with two elements is returned, lower and upper date boundary.\n\n    Valid inputs are, for example:\n    2012              => Jan 1 20012 - Dec 31 2012 (whole year)\n    201201            => Jan 1 2012  - Jan 31 2012 (whole month)\n    2012101           => Jan 1 2012 - Jan 1 2012   (whole day)\n    2011-2011         => same as \"2011\", which means whole year 2012\n    2011-2012         => Jan 1 2011 - Dec 31 2012  (two years)\n    201104-2012       => Apr 1 2011 - Dec 31 2012\n    201104-201203     => Apr 1 2011 - March 31 2012\n    20110408-2011     => Apr 8 2011 - Dec 31 2011\n    20110408-201105   => Apr 8 2011 - May 31 2011\n    20110408-20110507 => Apr 8 2011 - May 07 2011\n    2011-             => Jan 1 2012 - Dec 31 9999 (unlimited)\n    201104-           => Apr 1 2011 - Dec 31 9999 (unlimited)\n    20110408-         => Apr 8 2011 - Dec 31 9999 (unlimited)\n    -2011             Jan 1 0000 - Dez 31 2011\n    -201104           Jan 1 0000 - Apr 30, 2011\n    -20110408         Jan 1 0000 - Apr 8, 2011", "docstring_tokens": ["This", "function", "takes", "a", "date", "string", "in", "various", "formats", "and", "converts", "it", "to", "a", "normalized", "and", "validated", "date", "range", ".", "A", "list", "with", "two", "elements", "is", "returned", "lower", "and", "upper", "date", "boundary", "."], "sha": "fa5dd78c8fea5f91a85bf732af8a0bc307641134", "url": "https://github.com/marians/py-daterangestr/blob/fa5dd78c8fea5f91a85bf732af8a0bc307641134/daterangestr.py#L33-L65", "partition": "test"}
{"repo": "mental32/spotify.py", "path": "spotify/utils.py", "func_name": "OAuth2.attrs", "original_string": "def attrs(self):\n        \"\"\"Attributes used when constructing url parameters.\"\"\"\n        data = {\n            'client_id': self.client_id,\n            'redirect_uri': quote(self.redirect_uri),\n        }\n\n        if self.scope is not None:\n            data['scope'] = quote(self.scope)\n\n        if self.state is not None:\n            data['state'] = self.state\n\n        return data", "language": "python", "code": "def attrs(self):\n        \"\"\"Attributes used when constructing url parameters.\"\"\"\n        data = {\n            'client_id': self.client_id,\n            'redirect_uri': quote(self.redirect_uri),\n        }\n\n        if self.scope is not None:\n            data['scope'] = quote(self.scope)\n\n        if self.state is not None:\n            data['state'] = self.state\n\n        return data", "code_tokens": ["def", "attrs", "(", "self", ")", ":", "data", "=", "{", "'client_id'", ":", "self", ".", "client_id", ",", "'redirect_uri'", ":", "quote", "(", "self", ".", "redirect_uri", ")", ",", "}", "if", "self", ".", "scope", "is", "not", "None", ":", "data", "[", "'scope'", "]", "=", "quote", "(", "self", ".", "scope", ")", "if", "self", ".", "state", "is", "not", "None", ":", "data", "[", "'state'", "]", "=", "self", ".", "state", "return", "data"], "docstring": "Attributes used when constructing url parameters.", "docstring_tokens": ["Attributes", "used", "when", "constructing", "url", "parameters", "."], "sha": "bb296cac7c3dd289908906b7069bd80f43950515", "url": "https://github.com/mental32/spotify.py/blob/bb296cac7c3dd289908906b7069bd80f43950515/spotify/utils.py#L134-L147", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/hooks/S3_hook.py", "func_name": "S3Hook.list_prefixes", "original_string": "def list_prefixes(self, bucket_name, prefix='', delimiter='',\n                      page_size=None, max_items=None):\n        \"\"\"\n        Lists prefixes in a bucket under prefix\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int\n        \"\"\"\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('list_objects_v2')\n        response = paginator.paginate(Bucket=bucket_name,\n                                      Prefix=prefix,\n                                      Delimiter=delimiter,\n                                      PaginationConfig=config)\n\n        has_results = False\n        prefixes = []\n        for page in response:\n            if 'CommonPrefixes' in page:\n                has_results = True\n                for p in page['CommonPrefixes']:\n                    prefixes.append(p['Prefix'])\n\n        if has_results:\n            return prefixes", "language": "python", "code": "def list_prefixes(self, bucket_name, prefix='', delimiter='',\n                      page_size=None, max_items=None):\n        \"\"\"\n        Lists prefixes in a bucket under prefix\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int\n        \"\"\"\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('list_objects_v2')\n        response = paginator.paginate(Bucket=bucket_name,\n                                      Prefix=prefix,\n                                      Delimiter=delimiter,\n                                      PaginationConfig=config)\n\n        has_results = False\n        prefixes = []\n        for page in response:\n            if 'CommonPrefixes' in page:\n                has_results = True\n                for p in page['CommonPrefixes']:\n                    prefixes.append(p['Prefix'])\n\n        if has_results:\n            return prefixes", "code_tokens": ["def", "list_prefixes", "(", "self", ",", "bucket_name", ",", "prefix", "=", "''", ",", "delimiter", "=", "''", ",", "page_size", "=", "None", ",", "max_items", "=", "None", ")", ":", "config", "=", "{", "'PageSize'", ":", "page_size", ",", "'MaxItems'", ":", "max_items", ",", "}", "paginator", "=", "self", ".", "get_conn", "(", ")", ".", "get_paginator", "(", "'list_objects_v2'", ")", "response", "=", "paginator", ".", "paginate", "(", "Bucket", "=", "bucket_name", ",", "Prefix", "=", "prefix", ",", "Delimiter", "=", "delimiter", ",", "PaginationConfig", "=", "config", ")", "has_results", "=", "False", "prefixes", "=", "[", "]", "for", "page", "in", "response", ":", "if", "'CommonPrefixes'", "in", "page", ":", "has_results", "=", "True", "for", "p", "in", "page", "[", "'CommonPrefixes'", "]", ":", "prefixes", ".", "append", "(", "p", "[", "'Prefix'", "]", ")", "if", "has_results", ":", "return", "prefixes"], "docstring": "Lists prefixes in a bucket under prefix\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int", "docstring_tokens": ["Lists", "prefixes", "in", "a", "bucket", "under", "prefix"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L109-L145", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/adapter/mongo/hgnc.py", "func_name": "GeneHandler.gene_by_alias", "original_string": "def gene_by_alias(self, symbol, build='37'):\n        \"\"\"Return a iterable with hgnc_genes.\n\n        If the gene symbol is listed as primary the iterable will only have\n        one result. If not the iterable will include all hgnc genes that have\n        the symbol as an alias.\n\n        Args:\n            symbol(str)\n            build(str)\n\n        Returns:\n            res(pymongo.Cursor(dict))\n        \"\"\"\n        res = self.hgnc_collection.find({'hgnc_symbol': symbol, 'build':build})\n        if res.count() == 0:\n            res = self.hgnc_collection.find({'aliases': symbol, 'build':build})\n\n        return res", "language": "python", "code": "def gene_by_alias(self, symbol, build='37'):\n        \"\"\"Return a iterable with hgnc_genes.\n\n        If the gene symbol is listed as primary the iterable will only have\n        one result. If not the iterable will include all hgnc genes that have\n        the symbol as an alias.\n\n        Args:\n            symbol(str)\n            build(str)\n\n        Returns:\n            res(pymongo.Cursor(dict))\n        \"\"\"\n        res = self.hgnc_collection.find({'hgnc_symbol': symbol, 'build':build})\n        if res.count() == 0:\n            res = self.hgnc_collection.find({'aliases': symbol, 'build':build})\n\n        return res", "code_tokens": ["def", "gene_by_alias", "(", "self", ",", "symbol", ",", "build", "=", "'37'", ")", ":", "res", "=", "self", ".", "hgnc_collection", ".", "find", "(", "{", "'hgnc_symbol'", ":", "symbol", ",", "'build'", ":", "build", "}", ")", "if", "res", ".", "count", "(", ")", "==", "0", ":", "res", "=", "self", ".", "hgnc_collection", ".", "find", "(", "{", "'aliases'", ":", "symbol", ",", "'build'", ":", "build", "}", ")", "return", "res"], "docstring": "Return a iterable with hgnc_genes.\n\n        If the gene symbol is listed as primary the iterable will only have\n        one result. If not the iterable will include all hgnc genes that have\n        the symbol as an alias.\n\n        Args:\n            symbol(str)\n            build(str)\n\n        Returns:\n            res(pymongo.Cursor(dict))", "docstring_tokens": ["Return", "a", "iterable", "with", "hgnc_genes", "."], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/adapter/mongo/hgnc.py#L302-L320", "partition": "test"}
{"repo": "SketchingDev/Doodle-Dashboard", "path": "doodledashboard/dashboard.py", "func_name": "DashboardRunner.cycle", "original_string": "def cycle(self):\n        \"\"\"\n        Cycles through notifications with latest results from data feeds.\n        \"\"\"\n        messages = self.poll_datafeeds()\n        notifications = self.process_notifications(messages)\n\n        self.draw_notifications(notifications)", "language": "python", "code": "def cycle(self):\n        \"\"\"\n        Cycles through notifications with latest results from data feeds.\n        \"\"\"\n        messages = self.poll_datafeeds()\n        notifications = self.process_notifications(messages)\n\n        self.draw_notifications(notifications)", "code_tokens": ["def", "cycle", "(", "self", ")", ":", "messages", "=", "self", ".", "poll_datafeeds", "(", ")", "notifications", "=", "self", ".", "process_notifications", "(", "messages", ")", "self", ".", "draw_notifications", "(", "notifications", ")"], "docstring": "Cycles through notifications with latest results from data feeds.", "docstring_tokens": ["Cycles", "through", "notifications", "with", "latest", "results", "from", "data", "feeds", "."], "sha": "4d7f4c248875f82a962c275009aac4aa76bd0320", "url": "https://github.com/SketchingDev/Doodle-Dashboard/blob/4d7f4c248875f82a962c275009aac4aa76bd0320/doodledashboard/dashboard.py#L39-L46", "partition": "test"}
{"repo": "Yelp/py_zipkin", "path": "py_zipkin/zipkin.py", "func_name": "zipkin_span.override_span_name", "original_string": "def override_span_name(self, name):\n        \"\"\"Overrides the current span name.\n\n        This is useful if you don't know the span name yet when you create the\n        zipkin_span object. i.e. pyramid_zipkin doesn't know which route the\n        request matched until the function wrapped by the context manager\n        completes.\n\n        :param name: New span name\n        :type name: str\n        \"\"\"\n        self.span_name = name\n        if self.logging_context:\n            self.logging_context.span_name = name", "language": "python", "code": "def override_span_name(self, name):\n        \"\"\"Overrides the current span name.\n\n        This is useful if you don't know the span name yet when you create the\n        zipkin_span object. i.e. pyramid_zipkin doesn't know which route the\n        request matched until the function wrapped by the context manager\n        completes.\n\n        :param name: New span name\n        :type name: str\n        \"\"\"\n        self.span_name = name\n        if self.logging_context:\n            self.logging_context.span_name = name", "code_tokens": ["def", "override_span_name", "(", "self", ",", "name", ")", ":", "self", ".", "span_name", "=", "name", "if", "self", ".", "logging_context", ":", "self", ".", "logging_context", ".", "span_name", "=", "name"], "docstring": "Overrides the current span name.\n\n        This is useful if you don't know the span name yet when you create the\n        zipkin_span object. i.e. pyramid_zipkin doesn't know which route the\n        request matched until the function wrapped by the context manager\n        completes.\n\n        :param name: New span name\n        :type name: str", "docstring_tokens": ["Overrides", "the", "current", "span", "name", "."], "sha": "0944d9a3fb1f1798dbb276694aeed99f2b4283ba", "url": "https://github.com/Yelp/py_zipkin/blob/0944d9a3fb1f1798dbb276694aeed99f2b4283ba/py_zipkin/zipkin.py#L584-L597", "partition": "test"}
{"repo": "rocky/python3-trepan", "path": "trepan/processor/cmdfns.py", "func_name": "run_show_val", "original_string": "def run_show_val(obj, name):\n    \"\"\"Generic subcommand value display\"\"\"\n    val = obj.debugger.settings[obj.name]\n    obj.msg(\"%s is %s.\" % (obj.name, obj.cmd.proc._saferepr(val),))\n    return False", "language": "python", "code": "def run_show_val(obj, name):\n    \"\"\"Generic subcommand value display\"\"\"\n    val = obj.debugger.settings[obj.name]\n    obj.msg(\"%s is %s.\" % (obj.name, obj.cmd.proc._saferepr(val),))\n    return False", "code_tokens": ["def", "run_show_val", "(", "obj", ",", "name", ")", ":", "val", "=", "obj", ".", "debugger", ".", "settings", "[", "obj", ".", "name", "]", "obj", ".", "msg", "(", "\"%s is %s.\"", "%", "(", "obj", ".", "name", ",", "obj", ".", "cmd", ".", "proc", ".", "_saferepr", "(", "val", ")", ",", ")", ")", "return", "False"], "docstring": "Generic subcommand value display", "docstring_tokens": ["Generic", "subcommand", "value", "display"], "sha": "14e91bc0acce090d67be145b1ac040cab92ac5f3", "url": "https://github.com/rocky/python3-trepan/blob/14e91bc0acce090d67be145b1ac040cab92ac5f3/trepan/processor/cmdfns.py#L201-L205", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/storageservice.py", "func_name": "HDF5StorageService._trj_backup_trajectory", "original_string": "def _trj_backup_trajectory(self, traj, backup_filename=None):\n        \"\"\"Backs up a trajectory.\n\n        :param traj: Trajectory that should be backed up\n\n        :param backup_filename:\n\n            Path and filename of backup file. If None is specified the storage service\n            defaults to `path_to_trajectory_hdf5_file/backup_trajectory_name.hdf`.\n\n        \"\"\"\n        self._logger.info('Storing backup of %s.' % traj.v_name)\n\n        mypath, _ = os.path.split(self._filename)\n\n        if backup_filename is None:\n            backup_filename = os.path.join('%s' % mypath, 'backup_%s.hdf5' % traj.v_name)\n\n        backup_hdf5file = pt.open_file(filename=backup_filename,\n                                             mode='a', title=backup_filename)\n\n        if '/' + self._trajectory_name in backup_hdf5file:\n            raise ValueError('I cannot backup  `%s` into file `%s`, there is already a '\n                             'trajectory with that name.' % (traj.v_name, backup_filename))\n\n        backup_root = backup_hdf5file.root\n\n        self._trajectory_group._f_copy(newparent=backup_root, recursive=True)\n\n        backup_hdf5file.flush()\n        backup_hdf5file.close()\n\n        self._logger.info('Finished backup of %s.' % traj.v_name)", "language": "python", "code": "def _trj_backup_trajectory(self, traj, backup_filename=None):\n        \"\"\"Backs up a trajectory.\n\n        :param traj: Trajectory that should be backed up\n\n        :param backup_filename:\n\n            Path and filename of backup file. If None is specified the storage service\n            defaults to `path_to_trajectory_hdf5_file/backup_trajectory_name.hdf`.\n\n        \"\"\"\n        self._logger.info('Storing backup of %s.' % traj.v_name)\n\n        mypath, _ = os.path.split(self._filename)\n\n        if backup_filename is None:\n            backup_filename = os.path.join('%s' % mypath, 'backup_%s.hdf5' % traj.v_name)\n\n        backup_hdf5file = pt.open_file(filename=backup_filename,\n                                             mode='a', title=backup_filename)\n\n        if '/' + self._trajectory_name in backup_hdf5file:\n            raise ValueError('I cannot backup  `%s` into file `%s`, there is already a '\n                             'trajectory with that name.' % (traj.v_name, backup_filename))\n\n        backup_root = backup_hdf5file.root\n\n        self._trajectory_group._f_copy(newparent=backup_root, recursive=True)\n\n        backup_hdf5file.flush()\n        backup_hdf5file.close()\n\n        self._logger.info('Finished backup of %s.' % traj.v_name)", "code_tokens": ["def", "_trj_backup_trajectory", "(", "self", ",", "traj", ",", "backup_filename", "=", "None", ")", ":", "self", ".", "_logger", ".", "info", "(", "'Storing backup of %s.'", "%", "traj", ".", "v_name", ")", "mypath", ",", "_", "=", "os", ".", "path", ".", "split", "(", "self", ".", "_filename", ")", "if", "backup_filename", "is", "None", ":", "backup_filename", "=", "os", ".", "path", ".", "join", "(", "'%s'", "%", "mypath", ",", "'backup_%s.hdf5'", "%", "traj", ".", "v_name", ")", "backup_hdf5file", "=", "pt", ".", "open_file", "(", "filename", "=", "backup_filename", ",", "mode", "=", "'a'", ",", "title", "=", "backup_filename", ")", "if", "'/'", "+", "self", ".", "_trajectory_name", "in", "backup_hdf5file", ":", "raise", "ValueError", "(", "'I cannot backup  `%s` into file `%s`, there is already a '", "'trajectory with that name.'", "%", "(", "traj", ".", "v_name", ",", "backup_filename", ")", ")", "backup_root", "=", "backup_hdf5file", ".", "root", "self", ".", "_trajectory_group", ".", "_f_copy", "(", "newparent", "=", "backup_root", ",", "recursive", "=", "True", ")", "backup_hdf5file", ".", "flush", "(", ")", "backup_hdf5file", ".", "close", "(", ")", "self", ".", "_logger", ".", "info", "(", "'Finished backup of %s.'", "%", "traj", ".", "v_name", ")"], "docstring": "Backs up a trajectory.\n\n        :param traj: Trajectory that should be backed up\n\n        :param backup_filename:\n\n            Path and filename of backup file. If None is specified the storage service\n            defaults to `path_to_trajectory_hdf5_file/backup_trajectory_name.hdf`.", "docstring_tokens": ["Backs", "up", "a", "trajectory", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/storageservice.py#L1607-L1639", "partition": "test"}
{"repo": "reingart/gui2py", "path": "gui/controls/gridview.py", "func_name": "ComboCellEditor.StartingKey", "original_string": "def StartingKey(self, evt):\r\n        \"This will be called to let the editor do something with the first key\"\r\n        key = evt.GetKeyCode()\r\n        ch = None\r\n        if key in [wx.WXK_NUMPAD0, wx.WXK_NUMPAD1, wx.WXK_NUMPAD2, wx.WXK_NUMPAD3, wx.WXK_NUMPAD4,\r\n                   wx.WXK_NUMPAD5, wx.WXK_NUMPAD6, wx.WXK_NUMPAD7, wx.WXK_NUMPAD8, wx.WXK_NUMPAD9]:\r\n            ch = ch = chr(ord('0') + key - wx.WXK_NUMPAD0)\r\n        elif key < 256 and key >= 0 and chr(key) in string.printable:\r\n            ch = chr(key)\r\n            if not evt.ShiftDown():\r\n                ch = ch.lower()\r\n        if ch is not None:\r\n            self._tc.SetStringSelection(ch)\r\n        else:\r\n            evt.Skip()", "language": "python", "code": "def StartingKey(self, evt):\r\n        \"This will be called to let the editor do something with the first key\"\r\n        key = evt.GetKeyCode()\r\n        ch = None\r\n        if key in [wx.WXK_NUMPAD0, wx.WXK_NUMPAD1, wx.WXK_NUMPAD2, wx.WXK_NUMPAD3, wx.WXK_NUMPAD4,\r\n                   wx.WXK_NUMPAD5, wx.WXK_NUMPAD6, wx.WXK_NUMPAD7, wx.WXK_NUMPAD8, wx.WXK_NUMPAD9]:\r\n            ch = ch = chr(ord('0') + key - wx.WXK_NUMPAD0)\r\n        elif key < 256 and key >= 0 and chr(key) in string.printable:\r\n            ch = chr(key)\r\n            if not evt.ShiftDown():\r\n                ch = ch.lower()\r\n        if ch is not None:\r\n            self._tc.SetStringSelection(ch)\r\n        else:\r\n            evt.Skip()", "code_tokens": ["def", "StartingKey", "(", "self", ",", "evt", ")", ":", "key", "=", "evt", ".", "GetKeyCode", "(", ")", "ch", "=", "None", "if", "key", "in", "[", "wx", ".", "WXK_NUMPAD0", ",", "wx", ".", "WXK_NUMPAD1", ",", "wx", ".", "WXK_NUMPAD2", ",", "wx", ".", "WXK_NUMPAD3", ",", "wx", ".", "WXK_NUMPAD4", ",", "wx", ".", "WXK_NUMPAD5", ",", "wx", ".", "WXK_NUMPAD6", ",", "wx", ".", "WXK_NUMPAD7", ",", "wx", ".", "WXK_NUMPAD8", ",", "wx", ".", "WXK_NUMPAD9", "]", ":", "ch", "=", "ch", "=", "chr", "(", "ord", "(", "'0'", ")", "+", "key", "-", "wx", ".", "WXK_NUMPAD0", ")", "elif", "key", "<", "256", "and", "key", ">=", "0", "and", "chr", "(", "key", ")", "in", "string", ".", "printable", ":", "ch", "=", "chr", "(", "key", ")", "if", "not", "evt", ".", "ShiftDown", "(", ")", ":", "ch", "=", "ch", ".", "lower", "(", ")", "if", "ch", "is", "not", "None", ":", "self", ".", "_tc", ".", "SetStringSelection", "(", "ch", ")", "else", ":", "evt", ".", "Skip", "(", ")"], "docstring": "This will be called to let the editor do something with the first key", "docstring_tokens": ["This", "will", "be", "called", "to", "let", "the", "editor", "do", "something", "with", "the", "first", "key"], "sha": "aca0a05f6fcde55c94ad7cc058671a06608b01a4", "url": "https://github.com/reingart/gui2py/blob/aca0a05f6fcde55c94ad7cc058671a06608b01a4/gui/controls/gridview.py#L591-L605", "partition": "test"}
{"repo": "zomux/deepy", "path": "deepy/networks/auto_encoder.py", "func_name": "AutoEncoder.stack_decoders", "original_string": "def stack_decoders(self, *layers):\n        \"\"\"\n        Stack decoding layers.\n        \"\"\"\n        self.stack(*layers)\n        self.decoding_layers.extend(layers)", "language": "python", "code": "def stack_decoders(self, *layers):\n        \"\"\"\n        Stack decoding layers.\n        \"\"\"\n        self.stack(*layers)\n        self.decoding_layers.extend(layers)", "code_tokens": ["def", "stack_decoders", "(", "self", ",", "*", "layers", ")", ":", "self", ".", "stack", "(", "*", "layers", ")", "self", ".", "decoding_layers", ".", "extend", "(", "layers", ")"], "docstring": "Stack decoding layers.", "docstring_tokens": ["Stack", "decoding", "layers", "."], "sha": "090fbad22a08a809b12951cd0d4984f5bd432698", "url": "https://github.com/zomux/deepy/blob/090fbad22a08a809b12951cd0d4984f5bd432698/deepy/networks/auto_encoder.py#L48-L53", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/sample_halton_sequence.py", "func_name": "_get_indices", "original_string": "def _get_indices(num_results, sequence_indices, dtype, name=None):\n  \"\"\"Generates starting points for the Halton sequence procedure.\n\n  The k'th element of the sequence is generated starting from a positive integer\n  which must be distinct for each `k`. It is conventional to choose the starting\n  point as `k` itself (or `k+1` if k is zero based). This function generates\n  the starting integers for the required elements and reshapes the result for\n  later use.\n\n  Args:\n    num_results: Positive scalar `Tensor` of dtype int32. The number of samples\n      to generate. If this parameter is supplied, then `sequence_indices`\n      should be None.\n    sequence_indices: `Tensor` of dtype int32 and rank 1. The entries\n      index into the Halton sequence starting with 0 and hence, must be whole\n      numbers. For example, sequence_indices=[0, 5, 6] will produce the first,\n      sixth and seventh elements of the sequence. If this parameter is not None\n      then `n` must be None.\n    dtype: The dtype of the sample. One of `float32` or `float64`.\n      Default is `float32`.\n    name: Python `str` name which describes ops created by this function.\n\n  Returns:\n    indices: `Tensor` of dtype `dtype` and shape = `[n, 1, 1]`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, '_get_indices',\n                               [num_results, sequence_indices]):\n    if sequence_indices is None:\n      num_results = tf.cast(num_results, dtype=dtype)\n      sequence_indices = tf.range(num_results, dtype=dtype)\n    else:\n      sequence_indices = tf.cast(sequence_indices, dtype)\n\n    # Shift the indices so they are 1 based.\n    indices = sequence_indices + 1\n\n    # Reshape to make space for the event dimension and the place value\n    # coefficients.\n    return tf.reshape(indices, [-1, 1, 1])", "language": "python", "code": "def _get_indices(num_results, sequence_indices, dtype, name=None):\n  \"\"\"Generates starting points for the Halton sequence procedure.\n\n  The k'th element of the sequence is generated starting from a positive integer\n  which must be distinct for each `k`. It is conventional to choose the starting\n  point as `k` itself (or `k+1` if k is zero based). This function generates\n  the starting integers for the required elements and reshapes the result for\n  later use.\n\n  Args:\n    num_results: Positive scalar `Tensor` of dtype int32. The number of samples\n      to generate. If this parameter is supplied, then `sequence_indices`\n      should be None.\n    sequence_indices: `Tensor` of dtype int32 and rank 1. The entries\n      index into the Halton sequence starting with 0 and hence, must be whole\n      numbers. For example, sequence_indices=[0, 5, 6] will produce the first,\n      sixth and seventh elements of the sequence. If this parameter is not None\n      then `n` must be None.\n    dtype: The dtype of the sample. One of `float32` or `float64`.\n      Default is `float32`.\n    name: Python `str` name which describes ops created by this function.\n\n  Returns:\n    indices: `Tensor` of dtype `dtype` and shape = `[n, 1, 1]`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, '_get_indices',\n                               [num_results, sequence_indices]):\n    if sequence_indices is None:\n      num_results = tf.cast(num_results, dtype=dtype)\n      sequence_indices = tf.range(num_results, dtype=dtype)\n    else:\n      sequence_indices = tf.cast(sequence_indices, dtype)\n\n    # Shift the indices so they are 1 based.\n    indices = sequence_indices + 1\n\n    # Reshape to make space for the event dimension and the place value\n    # coefficients.\n    return tf.reshape(indices, [-1, 1, 1])", "code_tokens": ["def", "_get_indices", "(", "num_results", ",", "sequence_indices", ",", "dtype", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'_get_indices'", ",", "[", "num_results", ",", "sequence_indices", "]", ")", ":", "if", "sequence_indices", "is", "None", ":", "num_results", "=", "tf", ".", "cast", "(", "num_results", ",", "dtype", "=", "dtype", ")", "sequence_indices", "=", "tf", ".", "range", "(", "num_results", ",", "dtype", "=", "dtype", ")", "else", ":", "sequence_indices", "=", "tf", ".", "cast", "(", "sequence_indices", ",", "dtype", ")", "# Shift the indices so they are 1 based.", "indices", "=", "sequence_indices", "+", "1", "# Reshape to make space for the event dimension and the place value", "# coefficients.", "return", "tf", ".", "reshape", "(", "indices", ",", "[", "-", "1", ",", "1", ",", "1", "]", ")"], "docstring": "Generates starting points for the Halton sequence procedure.\n\n  The k'th element of the sequence is generated starting from a positive integer\n  which must be distinct for each `k`. It is conventional to choose the starting\n  point as `k` itself (or `k+1` if k is zero based). This function generates\n  the starting integers for the required elements and reshapes the result for\n  later use.\n\n  Args:\n    num_results: Positive scalar `Tensor` of dtype int32. The number of samples\n      to generate. If this parameter is supplied, then `sequence_indices`\n      should be None.\n    sequence_indices: `Tensor` of dtype int32 and rank 1. The entries\n      index into the Halton sequence starting with 0 and hence, must be whole\n      numbers. For example, sequence_indices=[0, 5, 6] will produce the first,\n      sixth and seventh elements of the sequence. If this parameter is not None\n      then `n` must be None.\n    dtype: The dtype of the sample. One of `float32` or `float64`.\n      Default is `float32`.\n    name: Python `str` name which describes ops created by this function.\n\n  Returns:\n    indices: `Tensor` of dtype `dtype` and shape = `[n, 1, 1]`.", "docstring_tokens": ["Generates", "starting", "points", "for", "the", "Halton", "sequence", "procedure", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/sample_halton_sequence.py#L304-L342", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/transpiler/passes/mapping/noise_adaptive_layout.py", "func_name": "NoiseAdaptiveLayout._select_best_remaining_cx", "original_string": "def _select_best_remaining_cx(self):\n        \"\"\"\n        Select best remaining CNOT in the hardware for the next program edge.\n        \"\"\"\n        candidates = []\n        for gate in self.gate_list:\n            chk1 = gate[0] in self.available_hw_qubits\n            chk2 = gate[1] in self.available_hw_qubits\n            if chk1 and chk2:\n                candidates.append(gate)\n        best_reliab = 0\n        best_item = None\n        for item in candidates:\n            if self.gate_cost[item] > best_reliab:\n                best_reliab = self.gate_cost[item]\n                best_item = item\n        return best_item", "language": "python", "code": "def _select_best_remaining_cx(self):\n        \"\"\"\n        Select best remaining CNOT in the hardware for the next program edge.\n        \"\"\"\n        candidates = []\n        for gate in self.gate_list:\n            chk1 = gate[0] in self.available_hw_qubits\n            chk2 = gate[1] in self.available_hw_qubits\n            if chk1 and chk2:\n                candidates.append(gate)\n        best_reliab = 0\n        best_item = None\n        for item in candidates:\n            if self.gate_cost[item] > best_reliab:\n                best_reliab = self.gate_cost[item]\n                best_item = item\n        return best_item", "code_tokens": ["def", "_select_best_remaining_cx", "(", "self", ")", ":", "candidates", "=", "[", "]", "for", "gate", "in", "self", ".", "gate_list", ":", "chk1", "=", "gate", "[", "0", "]", "in", "self", ".", "available_hw_qubits", "chk2", "=", "gate", "[", "1", "]", "in", "self", ".", "available_hw_qubits", "if", "chk1", "and", "chk2", ":", "candidates", ".", "append", "(", "gate", ")", "best_reliab", "=", "0", "best_item", "=", "None", "for", "item", "in", "candidates", ":", "if", "self", ".", "gate_cost", "[", "item", "]", ">", "best_reliab", ":", "best_reliab", "=", "self", ".", "gate_cost", "[", "item", "]", "best_item", "=", "item", "return", "best_item"], "docstring": "Select best remaining CNOT in the hardware for the next program edge.", "docstring_tokens": ["Select", "best", "remaining", "CNOT", "in", "the", "hardware", "for", "the", "next", "program", "edge", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/transpiler/passes/mapping/noise_adaptive_layout.py#L168-L184", "partition": "test"}
{"repo": "kgiusti/pyngus", "path": "pyngus/link.py", "func_name": "_SessionProxy.new_receiver", "original_string": "def new_receiver(self, name):\n        \"\"\"Create a new receiver link.\"\"\"\n        pn_link = self._pn_session.receiver(name)\n        return self.request_receiver(pn_link)", "language": "python", "code": "def new_receiver(self, name):\n        \"\"\"Create a new receiver link.\"\"\"\n        pn_link = self._pn_session.receiver(name)\n        return self.request_receiver(pn_link)", "code_tokens": ["def", "new_receiver", "(", "self", ",", "name", ")", ":", "pn_link", "=", "self", ".", "_pn_session", ".", "receiver", "(", "name", ")", "return", "self", ".", "request_receiver", "(", "pn_link", ")"], "docstring": "Create a new receiver link.", "docstring_tokens": ["Create", "a", "new", "receiver", "link", "."], "sha": "5392392046989f1bb84ba938c30e4d48311075f1", "url": "https://github.com/kgiusti/pyngus/blob/5392392046989f1bb84ba938c30e4d48311075f1/pyngus/link.py#L791-L794", "partition": "test"}
{"repo": "open-mmlab/mmcv", "path": "mmcv/image/transforms/geometry.py", "func_name": "bbox_scaling", "original_string": "def bbox_scaling(bboxes, scale, clip_shape=None):\n    \"\"\"Scaling bboxes w.r.t the box center.\n\n    Args:\n        bboxes (ndarray): Shape(..., 4).\n        scale (float): Scaling factor.\n        clip_shape (tuple, optional): If specified, bboxes that exceed the\n            boundary will be clipped according to the given shape (h, w).\n\n    Returns:\n        ndarray: Scaled bboxes.\n    \"\"\"\n    if float(scale) == 1.0:\n        scaled_bboxes = bboxes.copy()\n    else:\n        w = bboxes[..., 2] - bboxes[..., 0] + 1\n        h = bboxes[..., 3] - bboxes[..., 1] + 1\n        dw = (w * (scale - 1)) * 0.5\n        dh = (h * (scale - 1)) * 0.5\n        scaled_bboxes = bboxes + np.stack((-dw, -dh, dw, dh), axis=-1)\n    if clip_shape is not None:\n        return bbox_clip(scaled_bboxes, clip_shape)\n    else:\n        return scaled_bboxes", "language": "python", "code": "def bbox_scaling(bboxes, scale, clip_shape=None):\n    \"\"\"Scaling bboxes w.r.t the box center.\n\n    Args:\n        bboxes (ndarray): Shape(..., 4).\n        scale (float): Scaling factor.\n        clip_shape (tuple, optional): If specified, bboxes that exceed the\n            boundary will be clipped according to the given shape (h, w).\n\n    Returns:\n        ndarray: Scaled bboxes.\n    \"\"\"\n    if float(scale) == 1.0:\n        scaled_bboxes = bboxes.copy()\n    else:\n        w = bboxes[..., 2] - bboxes[..., 0] + 1\n        h = bboxes[..., 3] - bboxes[..., 1] + 1\n        dw = (w * (scale - 1)) * 0.5\n        dh = (h * (scale - 1)) * 0.5\n        scaled_bboxes = bboxes + np.stack((-dw, -dh, dw, dh), axis=-1)\n    if clip_shape is not None:\n        return bbox_clip(scaled_bboxes, clip_shape)\n    else:\n        return scaled_bboxes", "code_tokens": ["def", "bbox_scaling", "(", "bboxes", ",", "scale", ",", "clip_shape", "=", "None", ")", ":", "if", "float", "(", "scale", ")", "==", "1.0", ":", "scaled_bboxes", "=", "bboxes", ".", "copy", "(", ")", "else", ":", "w", "=", "bboxes", "[", "...", ",", "2", "]", "-", "bboxes", "[", "...", ",", "0", "]", "+", "1", "h", "=", "bboxes", "[", "...", ",", "3", "]", "-", "bboxes", "[", "...", ",", "1", "]", "+", "1", "dw", "=", "(", "w", "*", "(", "scale", "-", "1", ")", ")", "*", "0.5", "dh", "=", "(", "h", "*", "(", "scale", "-", "1", ")", ")", "*", "0.5", "scaled_bboxes", "=", "bboxes", "+", "np", ".", "stack", "(", "(", "-", "dw", ",", "-", "dh", ",", "dw", ",", "dh", ")", ",", "axis", "=", "-", "1", ")", "if", "clip_shape", "is", "not", "None", ":", "return", "bbox_clip", "(", "scaled_bboxes", ",", "clip_shape", ")", "else", ":", "return", "scaled_bboxes"], "docstring": "Scaling bboxes w.r.t the box center.\n\n    Args:\n        bboxes (ndarray): Shape(..., 4).\n        scale (float): Scaling factor.\n        clip_shape (tuple, optional): If specified, bboxes that exceed the\n            boundary will be clipped according to the given shape (h, w).\n\n    Returns:\n        ndarray: Scaled bboxes.", "docstring_tokens": ["Scaling", "bboxes", "w", ".", "r", ".", "t", "the", "box", "center", "."], "sha": "0d77f61450aab4dde8b8585a577cc496acb95d7f", "url": "https://github.com/open-mmlab/mmcv/blob/0d77f61450aab4dde8b8585a577cc496acb95d7f/mmcv/image/transforms/geometry.py#L86-L109", "partition": "test"}
{"repo": "openvenues/pypostal", "path": "postal/expand.py", "func_name": "expand_address", "original_string": "def expand_address(address, languages=None, **kw):\n    \"\"\"\n    Expand the given address into one or more normalized strings.\n\n    Required\n    --------\n    @param address: the address as either Unicode or a UTF-8 encoded string\n\n    Options\n    -------\n    @param languages: a tuple or list of ISO language code strings (e.g. \"en\", \"fr\", \"de\", etc.)\n                      to use in expansion. If None is passed, use language classifier\n                      to detect language automatically.\n    @param address_components: an integer (bit-set) of address component expansions\n                               to use e.g. ADDRESS_NAME | ADDRESS_STREET would use\n                               only expansions which apply to venue names or streets.\n    @param latin_ascii: use the Latin to ASCII transliterator, which normalizes e.g. \u00e6 => ae\n    @param transliterate: use any available transliterators for non-Latin scripts, e.g.\n                          for the Greek phrase \u03b4\u03b9\u03b1\u03c6\u03bf\u03c1\u03b5\u03c4\u03b9\u03ba\u03bf\u03cd\u03c2 becomes diaphoretiko\u00fas\u0331\n    @param strip_accents: strip accented characters e.g. \u00e9 => e, \u00e7 => c. This loses some\n                          information in various languags, but in general we want\n    @param decompose: perform Unicode normalization (NFD form)\n    @param lowercase: UTF-8 lowercase the string\n    @param trim_string: trim spaces on either side of the string\n    @param replace_word_hyphens: add version of the string replacing hyphens with space\n    @param delete_word_hyphens: add version of the string with hyphens deleted\n    @param replace_numeric_hyphens: add version of the string with numeric hyphens replaced \n                                    e.g. 12345-6789 => 12345 6789\n    @param delete_numeric_hyphens: add version of the string with numeric hyphens removed\n                                   e.g. 12345-6789 => 123456789\n    @param split_alpha_from_numeric: split tokens like CR17 into CR 17, helps with expansion\n                                     of certain types of highway abbreviations\n    @param delete_final_periods: remove final periods on abbreviations e.g. St. => St\n    @param delete_acronym_periods: remove periods in acronyms e.g. U.S.A. => USA\n    @param drop_english_possessives: normalize possessives e.g. Mark's => Marks\n    @param delete_apostrophes: delete other types of hyphens e.g. O'Malley => OMalley\n    @param expand_numex: converts numeric expressions e.g. Twenty sixth => 26th,\n                         using either the supplied languages or the result of\n                         automated language classification.\n    @param roman_numerals: normalize Roman numerals e.g. IX => 9. Since these can be\n                           ambiguous (especially I and V), turning this on simply\n                           adds another version of the string if any potential\n                           Roman numerals are found.\n    \"\"\"\n    address = safe_decode(address, 'utf-8')\n    return _expand.expand_address(address, languages=languages, **kw)", "language": "python", "code": "def expand_address(address, languages=None, **kw):\n    \"\"\"\n    Expand the given address into one or more normalized strings.\n\n    Required\n    --------\n    @param address: the address as either Unicode or a UTF-8 encoded string\n\n    Options\n    -------\n    @param languages: a tuple or list of ISO language code strings (e.g. \"en\", \"fr\", \"de\", etc.)\n                      to use in expansion. If None is passed, use language classifier\n                      to detect language automatically.\n    @param address_components: an integer (bit-set) of address component expansions\n                               to use e.g. ADDRESS_NAME | ADDRESS_STREET would use\n                               only expansions which apply to venue names or streets.\n    @param latin_ascii: use the Latin to ASCII transliterator, which normalizes e.g. \u00e6 => ae\n    @param transliterate: use any available transliterators for non-Latin scripts, e.g.\n                          for the Greek phrase \u03b4\u03b9\u03b1\u03c6\u03bf\u03c1\u03b5\u03c4\u03b9\u03ba\u03bf\u03cd\u03c2 becomes diaphoretiko\u00fas\u0331\n    @param strip_accents: strip accented characters e.g. \u00e9 => e, \u00e7 => c. This loses some\n                          information in various languags, but in general we want\n    @param decompose: perform Unicode normalization (NFD form)\n    @param lowercase: UTF-8 lowercase the string\n    @param trim_string: trim spaces on either side of the string\n    @param replace_word_hyphens: add version of the string replacing hyphens with space\n    @param delete_word_hyphens: add version of the string with hyphens deleted\n    @param replace_numeric_hyphens: add version of the string with numeric hyphens replaced \n                                    e.g. 12345-6789 => 12345 6789\n    @param delete_numeric_hyphens: add version of the string with numeric hyphens removed\n                                   e.g. 12345-6789 => 123456789\n    @param split_alpha_from_numeric: split tokens like CR17 into CR 17, helps with expansion\n                                     of certain types of highway abbreviations\n    @param delete_final_periods: remove final periods on abbreviations e.g. St. => St\n    @param delete_acronym_periods: remove periods in acronyms e.g. U.S.A. => USA\n    @param drop_english_possessives: normalize possessives e.g. Mark's => Marks\n    @param delete_apostrophes: delete other types of hyphens e.g. O'Malley => OMalley\n    @param expand_numex: converts numeric expressions e.g. Twenty sixth => 26th,\n                         using either the supplied languages or the result of\n                         automated language classification.\n    @param roman_numerals: normalize Roman numerals e.g. IX => 9. Since these can be\n                           ambiguous (especially I and V), turning this on simply\n                           adds another version of the string if any potential\n                           Roman numerals are found.\n    \"\"\"\n    address = safe_decode(address, 'utf-8')\n    return _expand.expand_address(address, languages=languages, **kw)", "code_tokens": ["def", "expand_address", "(", "address", ",", "languages", "=", "None", ",", "*", "*", "kw", ")", ":", "address", "=", "safe_decode", "(", "address", ",", "'utf-8'", ")", "return", "_expand", ".", "expand_address", "(", "address", ",", "languages", "=", "languages", ",", "*", "*", "kw", ")"], "docstring": "Expand the given address into one or more normalized strings.\n\n    Required\n    --------\n    @param address: the address as either Unicode or a UTF-8 encoded string\n\n    Options\n    -------\n    @param languages: a tuple or list of ISO language code strings (e.g. \"en\", \"fr\", \"de\", etc.)\n                      to use in expansion. If None is passed, use language classifier\n                      to detect language automatically.\n    @param address_components: an integer (bit-set) of address component expansions\n                               to use e.g. ADDRESS_NAME | ADDRESS_STREET would use\n                               only expansions which apply to venue names or streets.\n    @param latin_ascii: use the Latin to ASCII transliterator, which normalizes e.g. \u00e6 => ae\n    @param transliterate: use any available transliterators for non-Latin scripts, e.g.\n                          for the Greek phrase \u03b4\u03b9\u03b1\u03c6\u03bf\u03c1\u03b5\u03c4\u03b9\u03ba\u03bf\u03cd\u03c2 becomes diaphoretiko\u00fas\u0331\n    @param strip_accents: strip accented characters e.g. \u00e9 => e, \u00e7 => c. This loses some\n                          information in various languags, but in general we want\n    @param decompose: perform Unicode normalization (NFD form)\n    @param lowercase: UTF-8 lowercase the string\n    @param trim_string: trim spaces on either side of the string\n    @param replace_word_hyphens: add version of the string replacing hyphens with space\n    @param delete_word_hyphens: add version of the string with hyphens deleted\n    @param replace_numeric_hyphens: add version of the string with numeric hyphens replaced \n                                    e.g. 12345-6789 => 12345 6789\n    @param delete_numeric_hyphens: add version of the string with numeric hyphens removed\n                                   e.g. 12345-6789 => 123456789\n    @param split_alpha_from_numeric: split tokens like CR17 into CR 17, helps with expansion\n                                     of certain types of highway abbreviations\n    @param delete_final_periods: remove final periods on abbreviations e.g. St. => St\n    @param delete_acronym_periods: remove periods in acronyms e.g. U.S.A. => USA\n    @param drop_english_possessives: normalize possessives e.g. Mark's => Marks\n    @param delete_apostrophes: delete other types of hyphens e.g. O'Malley => OMalley\n    @param expand_numex: converts numeric expressions e.g. Twenty sixth => 26th,\n                         using either the supplied languages or the result of\n                         automated language classification.\n    @param roman_numerals: normalize Roman numerals e.g. IX => 9. Since these can be\n                           ambiguous (especially I and V), turning this on simply\n                           adds another version of the string if any potential\n                           Roman numerals are found.", "docstring_tokens": ["Expand", "the", "given", "address", "into", "one", "or", "more", "normalized", "strings", "."], "sha": "1c0fd96b5e2463b7015cd3625ac276db520c69fe", "url": "https://github.com/openvenues/pypostal/blob/1c0fd96b5e2463b7015cd3625ac276db520c69fe/postal/expand.py#L9-L54", "partition": "test"}
{"repo": "vaexio/vaex", "path": "packages/vaex-core/vaex/dataframe.py", "func_name": "DataFrame.to_pandas_df", "original_string": "def to_pandas_df(self, column_names=None, selection=None, strings=True, virtual=False, index_name=None):\n        \"\"\"Return a pandas DataFrame containing the ndarray corresponding to the evaluated data\n\n         If index is given, that column is used for the index of the dataframe.\n\n         Example\n\n         >>> df_pandas = df.to_pandas_df([\"x\", \"y\", \"z\"])\n         >>> df_copy = vaex.from_pandas(df_pandas)\n\n        :param column_names: list of column names, to export, when None DataFrame.get_column_names(strings=strings, virtual=virtual) is used\n        :param selection: {selection}\n        :param strings: argument passed to DataFrame.get_column_names when column_names is None\n        :param virtual: argument passed to DataFrame.get_column_names when column_names is None\n        :param index_column: if this column is given it is used for the index of the DataFrame\n        :return: pandas.DataFrame object\n        \"\"\"\n        import pandas as pd\n        data = self.to_dict(column_names=column_names, selection=selection, strings=strings, virtual=virtual)\n        if index_name is not None:\n            if index_name in data:\n                index = data.pop(index_name)\n            else:\n                index = self.evaluate(index_name, selection=selection)\n        else:\n            index = None\n        df = pd.DataFrame(data=data, index=index)\n        if index is not None:\n            df.index.name = index_name\n        return df", "language": "python", "code": "def to_pandas_df(self, column_names=None, selection=None, strings=True, virtual=False, index_name=None):\n        \"\"\"Return a pandas DataFrame containing the ndarray corresponding to the evaluated data\n\n         If index is given, that column is used for the index of the dataframe.\n\n         Example\n\n         >>> df_pandas = df.to_pandas_df([\"x\", \"y\", \"z\"])\n         >>> df_copy = vaex.from_pandas(df_pandas)\n\n        :param column_names: list of column names, to export, when None DataFrame.get_column_names(strings=strings, virtual=virtual) is used\n        :param selection: {selection}\n        :param strings: argument passed to DataFrame.get_column_names when column_names is None\n        :param virtual: argument passed to DataFrame.get_column_names when column_names is None\n        :param index_column: if this column is given it is used for the index of the DataFrame\n        :return: pandas.DataFrame object\n        \"\"\"\n        import pandas as pd\n        data = self.to_dict(column_names=column_names, selection=selection, strings=strings, virtual=virtual)\n        if index_name is not None:\n            if index_name in data:\n                index = data.pop(index_name)\n            else:\n                index = self.evaluate(index_name, selection=selection)\n        else:\n            index = None\n        df = pd.DataFrame(data=data, index=index)\n        if index is not None:\n            df.index.name = index_name\n        return df", "code_tokens": ["def", "to_pandas_df", "(", "self", ",", "column_names", "=", "None", ",", "selection", "=", "None", ",", "strings", "=", "True", ",", "virtual", "=", "False", ",", "index_name", "=", "None", ")", ":", "import", "pandas", "as", "pd", "data", "=", "self", ".", "to_dict", "(", "column_names", "=", "column_names", ",", "selection", "=", "selection", ",", "strings", "=", "strings", ",", "virtual", "=", "virtual", ")", "if", "index_name", "is", "not", "None", ":", "if", "index_name", "in", "data", ":", "index", "=", "data", ".", "pop", "(", "index_name", ")", "else", ":", "index", "=", "self", ".", "evaluate", "(", "index_name", ",", "selection", "=", "selection", ")", "else", ":", "index", "=", "None", "df", "=", "pd", ".", "DataFrame", "(", "data", "=", "data", ",", "index", "=", "index", ")", "if", "index", "is", "not", "None", ":", "df", ".", "index", ".", "name", "=", "index_name", "return", "df"], "docstring": "Return a pandas DataFrame containing the ndarray corresponding to the evaluated data\n\n         If index is given, that column is used for the index of the dataframe.\n\n         Example\n\n         >>> df_pandas = df.to_pandas_df([\"x\", \"y\", \"z\"])\n         >>> df_copy = vaex.from_pandas(df_pandas)\n\n        :param column_names: list of column names, to export, when None DataFrame.get_column_names(strings=strings, virtual=virtual) is used\n        :param selection: {selection}\n        :param strings: argument passed to DataFrame.get_column_names when column_names is None\n        :param virtual: argument passed to DataFrame.get_column_names when column_names is None\n        :param index_column: if this column is given it is used for the index of the DataFrame\n        :return: pandas.DataFrame object", "docstring_tokens": ["Return", "a", "pandas", "DataFrame", "containing", "the", "ndarray", "corresponding", "to", "the", "evaluated", "data"], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/dataframe.py#L2639-L2668", "partition": "test"}
{"repo": "urda/nistbeacon", "path": "scripts/version_manager.py", "func_name": "FileVersionInfo.get_version", "original_string": "def get_version(self) -> str:\n        \"\"\"\n        Open the file referenced in this object, and scrape the version.\n\n        :return:\n            The version as a string, an empty string if there is no match\n            to the magic_line, or any file exception messages encountered.\n        \"\"\"\n\n        try:\n            f = open(self.file_path, 'r')\n            lines = f.readlines()\n            f.close()\n        except Exception as e:\n            return str(e)\n\n        result = ''\n\n        for line in lines:\n            if self.magic_line in line:\n                start = len(self.magic_line)\n                end = len(line) - self.strip_end_chars\n                result = line[start:end]\n                break\n\n        return result", "language": "python", "code": "def get_version(self) -> str:\n        \"\"\"\n        Open the file referenced in this object, and scrape the version.\n\n        :return:\n            The version as a string, an empty string if there is no match\n            to the magic_line, or any file exception messages encountered.\n        \"\"\"\n\n        try:\n            f = open(self.file_path, 'r')\n            lines = f.readlines()\n            f.close()\n        except Exception as e:\n            return str(e)\n\n        result = ''\n\n        for line in lines:\n            if self.magic_line in line:\n                start = len(self.magic_line)\n                end = len(line) - self.strip_end_chars\n                result = line[start:end]\n                break\n\n        return result", "code_tokens": ["def", "get_version", "(", "self", ")", "->", "str", ":", "try", ":", "f", "=", "open", "(", "self", ".", "file_path", ",", "'r'", ")", "lines", "=", "f", ".", "readlines", "(", ")", "f", ".", "close", "(", ")", "except", "Exception", "as", "e", ":", "return", "str", "(", "e", ")", "result", "=", "''", "for", "line", "in", "lines", ":", "if", "self", ".", "magic_line", "in", "line", ":", "start", "=", "len", "(", "self", ".", "magic_line", ")", "end", "=", "len", "(", "line", ")", "-", "self", ".", "strip_end_chars", "result", "=", "line", "[", "start", ":", "end", "]", "break", "return", "result"], "docstring": "Open the file referenced in this object, and scrape the version.\n\n        :return:\n            The version as a string, an empty string if there is no match\n            to the magic_line, or any file exception messages encountered.", "docstring_tokens": ["Open", "the", "file", "referenced", "in", "this", "object", "and", "scrape", "the", "version", "."], "sha": "43e0c3d1e186e71387f072daf98911abb14469dd", "url": "https://github.com/urda/nistbeacon/blob/43e0c3d1e186e71387f072daf98911abb14469dd/scripts/version_manager.py#L58-L83", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/storageservice.py", "func_name": "HDF5StorageService._tree_store_nodes_dfs", "original_string": "def _tree_store_nodes_dfs(self, parent_traj_node, name, store_data, with_links, recursive,\n                          max_depth, current_depth,\n                          parent_hdf5_group):\n        \"\"\"Stores a node to hdf5 and if desired stores recursively everything below it.\n\n        :param parent_traj_node: The parental node\n        :param name: Name of node to be stored\n        :param store_data: How to store data\n        :param with_links: If links should be stored\n        :param recursive: Whether to store recursively the subtree\n        :param max_depth: Maximum recursion depth in tree\n        :param current_depth: Current depth\n        :param parent_hdf5_group: Parent hdf5 group\n\n        \"\"\"\n        if max_depth is None:\n            max_depth = float('inf')\n\n        store_list = [(parent_traj_node, name, current_depth, parent_hdf5_group)]\n\n        while store_list:\n            parent_traj_node, name, current_depth, parent_hdf5_group = store_list.pop()\n\n            # Check if we create a link\n            if name in parent_traj_node._links:\n                if with_links:\n                    self._tree_store_link(parent_traj_node, name, parent_hdf5_group)\n                continue\n\n            traj_node = parent_traj_node._children[name]\n\n            # If the node does not exist in the hdf5 file create it\n            if not hasattr(parent_hdf5_group, name):\n                newly_created = True\n                new_hdf5_group = self._hdf5file.create_group(where=parent_hdf5_group,\n                                                       name=name, filters=self._all_get_filters())\n            else:\n                newly_created = False\n                new_hdf5_group = getattr(parent_hdf5_group, name)\n\n            if traj_node.v_is_leaf:\n                self._prm_store_parameter_or_result(traj_node, store_data=store_data,\n                                                     _hdf5_group=new_hdf5_group,\n                                                    _newly_created=newly_created)\n\n            else:\n                self._grp_store_group(traj_node, store_data=store_data, with_links=with_links,\n                                      recursive=False, max_depth=max_depth,\n                                      _hdf5_group=new_hdf5_group,\n                                      _newly_created=newly_created)\n\n                if recursive and current_depth < max_depth:\n                    for child in traj_node._children.keys():\n                        store_list.append((traj_node, child, current_depth + 1, new_hdf5_group))", "language": "python", "code": "def _tree_store_nodes_dfs(self, parent_traj_node, name, store_data, with_links, recursive,\n                          max_depth, current_depth,\n                          parent_hdf5_group):\n        \"\"\"Stores a node to hdf5 and if desired stores recursively everything below it.\n\n        :param parent_traj_node: The parental node\n        :param name: Name of node to be stored\n        :param store_data: How to store data\n        :param with_links: If links should be stored\n        :param recursive: Whether to store recursively the subtree\n        :param max_depth: Maximum recursion depth in tree\n        :param current_depth: Current depth\n        :param parent_hdf5_group: Parent hdf5 group\n\n        \"\"\"\n        if max_depth is None:\n            max_depth = float('inf')\n\n        store_list = [(parent_traj_node, name, current_depth, parent_hdf5_group)]\n\n        while store_list:\n            parent_traj_node, name, current_depth, parent_hdf5_group = store_list.pop()\n\n            # Check if we create a link\n            if name in parent_traj_node._links:\n                if with_links:\n                    self._tree_store_link(parent_traj_node, name, parent_hdf5_group)\n                continue\n\n            traj_node = parent_traj_node._children[name]\n\n            # If the node does not exist in the hdf5 file create it\n            if not hasattr(parent_hdf5_group, name):\n                newly_created = True\n                new_hdf5_group = self._hdf5file.create_group(where=parent_hdf5_group,\n                                                       name=name, filters=self._all_get_filters())\n            else:\n                newly_created = False\n                new_hdf5_group = getattr(parent_hdf5_group, name)\n\n            if traj_node.v_is_leaf:\n                self._prm_store_parameter_or_result(traj_node, store_data=store_data,\n                                                     _hdf5_group=new_hdf5_group,\n                                                    _newly_created=newly_created)\n\n            else:\n                self._grp_store_group(traj_node, store_data=store_data, with_links=with_links,\n                                      recursive=False, max_depth=max_depth,\n                                      _hdf5_group=new_hdf5_group,\n                                      _newly_created=newly_created)\n\n                if recursive and current_depth < max_depth:\n                    for child in traj_node._children.keys():\n                        store_list.append((traj_node, child, current_depth + 1, new_hdf5_group))", "code_tokens": ["def", "_tree_store_nodes_dfs", "(", "self", ",", "parent_traj_node", ",", "name", ",", "store_data", ",", "with_links", ",", "recursive", ",", "max_depth", ",", "current_depth", ",", "parent_hdf5_group", ")", ":", "if", "max_depth", "is", "None", ":", "max_depth", "=", "float", "(", "'inf'", ")", "store_list", "=", "[", "(", "parent_traj_node", ",", "name", ",", "current_depth", ",", "parent_hdf5_group", ")", "]", "while", "store_list", ":", "parent_traj_node", ",", "name", ",", "current_depth", ",", "parent_hdf5_group", "=", "store_list", ".", "pop", "(", ")", "# Check if we create a link", "if", "name", "in", "parent_traj_node", ".", "_links", ":", "if", "with_links", ":", "self", ".", "_tree_store_link", "(", "parent_traj_node", ",", "name", ",", "parent_hdf5_group", ")", "continue", "traj_node", "=", "parent_traj_node", ".", "_children", "[", "name", "]", "# If the node does not exist in the hdf5 file create it", "if", "not", "hasattr", "(", "parent_hdf5_group", ",", "name", ")", ":", "newly_created", "=", "True", "new_hdf5_group", "=", "self", ".", "_hdf5file", ".", "create_group", "(", "where", "=", "parent_hdf5_group", ",", "name", "=", "name", ",", "filters", "=", "self", ".", "_all_get_filters", "(", ")", ")", "else", ":", "newly_created", "=", "False", "new_hdf5_group", "=", "getattr", "(", "parent_hdf5_group", ",", "name", ")", "if", "traj_node", ".", "v_is_leaf", ":", "self", ".", "_prm_store_parameter_or_result", "(", "traj_node", ",", "store_data", "=", "store_data", ",", "_hdf5_group", "=", "new_hdf5_group", ",", "_newly_created", "=", "newly_created", ")", "else", ":", "self", ".", "_grp_store_group", "(", "traj_node", ",", "store_data", "=", "store_data", ",", "with_links", "=", "with_links", ",", "recursive", "=", "False", ",", "max_depth", "=", "max_depth", ",", "_hdf5_group", "=", "new_hdf5_group", ",", "_newly_created", "=", "newly_created", ")", "if", "recursive", "and", "current_depth", "<", "max_depth", ":", "for", "child", "in", "traj_node", ".", "_children", ".", "keys", "(", ")", ":", "store_list", ".", "append", "(", "(", "traj_node", ",", "child", ",", "current_depth", "+", "1", ",", "new_hdf5_group", ")", ")"], "docstring": "Stores a node to hdf5 and if desired stores recursively everything below it.\n\n        :param parent_traj_node: The parental node\n        :param name: Name of node to be stored\n        :param store_data: How to store data\n        :param with_links: If links should be stored\n        :param recursive: Whether to store recursively the subtree\n        :param max_depth: Maximum recursion depth in tree\n        :param current_depth: Current depth\n        :param parent_hdf5_group: Parent hdf5 group", "docstring_tokens": ["Stores", "a", "node", "to", "hdf5", "and", "if", "desired", "stores", "recursively", "everything", "below", "it", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/storageservice.py#L2777-L2830", "partition": "test"}
{"repo": "lvh/txampext", "path": "txampext/jsondialect.py", "func_name": "JSONAMPDialectReceiver.connectionLost", "original_string": "def connectionLost(self, reason):\n        \"\"\"\n        Tells the box receiver to stop receiving boxes.\n        \"\"\"\n        self._remote.boxReceiver.stopReceivingBoxes(reason)\n        return basic.NetstringReceiver.connectionLost(self, reason)", "language": "python", "code": "def connectionLost(self, reason):\n        \"\"\"\n        Tells the box receiver to stop receiving boxes.\n        \"\"\"\n        self._remote.boxReceiver.stopReceivingBoxes(reason)\n        return basic.NetstringReceiver.connectionLost(self, reason)", "code_tokens": ["def", "connectionLost", "(", "self", ",", "reason", ")", ":", "self", ".", "_remote", ".", "boxReceiver", ".", "stopReceivingBoxes", "(", "reason", ")", "return", "basic", ".", "NetstringReceiver", ".", "connectionLost", "(", "self", ",", "reason", ")"], "docstring": "Tells the box receiver to stop receiving boxes.", "docstring_tokens": ["Tells", "the", "box", "receiver", "to", "stop", "receiving", "boxes", "."], "sha": "a7d6cb9f1e9200dba597378cd40eb6a2096d4fd9", "url": "https://github.com/lvh/txampext/blob/a7d6cb9f1e9200dba597378cd40eb6a2096d4fd9/txampext/jsondialect.py#L119-L124", "partition": "test"}
{"repo": "reingart/gui2py", "path": "gui/tools/designer.py", "func_name": "BasicDesigner.mouse_down", "original_string": "def mouse_down(self, evt): \n        \"Get the selected object and store start position\"\n        if DEBUG: print \"down!\"\n        if (not evt.ControlDown() and not evt.ShiftDown()) or evt.AltDown():\n            for obj in self.selection:\n                # clear marker\n                if obj.sel_marker:\n                    obj.sel_marker.show(False)\n                    obj.sel_marker.destroy()\n                    obj.sel_marker = None\n            self.selection = []  # clear previous selection\n\n        wx_obj = evt.GetEventObject()\n\n        if wx_obj.Parent is None or evt.AltDown():\n            if not evt.AltDown():\n                evt.Skip()\n            # start the rubberband effect (multiple selection using the mouse) \n            self.current = wx_obj\n            self.overlay = wx.Overlay()\n            self.pos = evt.GetPosition() \n            self.parent.wx_obj.CaptureMouse()\n            #if self.inspector and hasattr(wx_obj, \"obj\"):\n            #    self.inspector.inspect(wx_obj.obj)  # inspect top level window\n            #self.dclick = False\n        else:\n            # create the selection marker and assign it to the control\n            obj = wx_obj.obj\n            self.overlay = None\n            if DEBUG: print wx_obj\n            sx, sy = wx_obj.ScreenToClient(wx_obj.GetPositionTuple())\n            dx, dy = wx_obj.ScreenToClient(wx.GetMousePosition())\n            self.pos = wx_obj.ScreenToClient(wx.GetMousePosition())\n            self.start = (sx - dx, sy - dy)\n            self.current = wx_obj\n            if DEBUG: print \"capture...\"\n            # do not capture on TextCtrl, it will fail (blocking) at least in gtk\n            # do not capture on wx.Notebook to allow selecting the tabs\n            if not isinstance(wx_obj, wx.Notebook):\n                self.parent.wx_obj.CaptureMouse()\n            self.select(obj, keep_selection=True)", "language": "python", "code": "def mouse_down(self, evt): \n        \"Get the selected object and store start position\"\n        if DEBUG: print \"down!\"\n        if (not evt.ControlDown() and not evt.ShiftDown()) or evt.AltDown():\n            for obj in self.selection:\n                # clear marker\n                if obj.sel_marker:\n                    obj.sel_marker.show(False)\n                    obj.sel_marker.destroy()\n                    obj.sel_marker = None\n            self.selection = []  # clear previous selection\n\n        wx_obj = evt.GetEventObject()\n\n        if wx_obj.Parent is None or evt.AltDown():\n            if not evt.AltDown():\n                evt.Skip()\n            # start the rubberband effect (multiple selection using the mouse) \n            self.current = wx_obj\n            self.overlay = wx.Overlay()\n            self.pos = evt.GetPosition() \n            self.parent.wx_obj.CaptureMouse()\n            #if self.inspector and hasattr(wx_obj, \"obj\"):\n            #    self.inspector.inspect(wx_obj.obj)  # inspect top level window\n            #self.dclick = False\n        else:\n            # create the selection marker and assign it to the control\n            obj = wx_obj.obj\n            self.overlay = None\n            if DEBUG: print wx_obj\n            sx, sy = wx_obj.ScreenToClient(wx_obj.GetPositionTuple())\n            dx, dy = wx_obj.ScreenToClient(wx.GetMousePosition())\n            self.pos = wx_obj.ScreenToClient(wx.GetMousePosition())\n            self.start = (sx - dx, sy - dy)\n            self.current = wx_obj\n            if DEBUG: print \"capture...\"\n            # do not capture on TextCtrl, it will fail (blocking) at least in gtk\n            # do not capture on wx.Notebook to allow selecting the tabs\n            if not isinstance(wx_obj, wx.Notebook):\n                self.parent.wx_obj.CaptureMouse()\n            self.select(obj, keep_selection=True)", "code_tokens": ["def", "mouse_down", "(", "self", ",", "evt", ")", ":", "if", "DEBUG", ":", "print", "\"down!\"", "if", "(", "not", "evt", ".", "ControlDown", "(", ")", "and", "not", "evt", ".", "ShiftDown", "(", ")", ")", "or", "evt", ".", "AltDown", "(", ")", ":", "for", "obj", "in", "self", ".", "selection", ":", "# clear marker", "if", "obj", ".", "sel_marker", ":", "obj", ".", "sel_marker", ".", "show", "(", "False", ")", "obj", ".", "sel_marker", ".", "destroy", "(", ")", "obj", ".", "sel_marker", "=", "None", "self", ".", "selection", "=", "[", "]", "# clear previous selection", "wx_obj", "=", "evt", ".", "GetEventObject", "(", ")", "if", "wx_obj", ".", "Parent", "is", "None", "or", "evt", ".", "AltDown", "(", ")", ":", "if", "not", "evt", ".", "AltDown", "(", ")", ":", "evt", ".", "Skip", "(", ")", "# start the rubberband effect (multiple selection using the mouse) ", "self", ".", "current", "=", "wx_obj", "self", ".", "overlay", "=", "wx", ".", "Overlay", "(", ")", "self", ".", "pos", "=", "evt", ".", "GetPosition", "(", ")", "self", ".", "parent", ".", "wx_obj", ".", "CaptureMouse", "(", ")", "#if self.inspector and hasattr(wx_obj, \"obj\"):", "#    self.inspector.inspect(wx_obj.obj)  # inspect top level window", "#self.dclick = False", "else", ":", "# create the selection marker and assign it to the control", "obj", "=", "wx_obj", ".", "obj", "self", ".", "overlay", "=", "None", "if", "DEBUG", ":", "print", "wx_obj", "sx", ",", "sy", "=", "wx_obj", ".", "ScreenToClient", "(", "wx_obj", ".", "GetPositionTuple", "(", ")", ")", "dx", ",", "dy", "=", "wx_obj", ".", "ScreenToClient", "(", "wx", ".", "GetMousePosition", "(", ")", ")", "self", ".", "pos", "=", "wx_obj", ".", "ScreenToClient", "(", "wx", ".", "GetMousePosition", "(", ")", ")", "self", ".", "start", "=", "(", "sx", "-", "dx", ",", "sy", "-", "dy", ")", "self", ".", "current", "=", "wx_obj", "if", "DEBUG", ":", "print", "\"capture...\"", "# do not capture on TextCtrl, it will fail (blocking) at least in gtk", "# do not capture on wx.Notebook to allow selecting the tabs", "if", "not", "isinstance", "(", "wx_obj", ",", "wx", ".", "Notebook", ")", ":", "self", ".", "parent", ".", "wx_obj", ".", "CaptureMouse", "(", ")", "self", ".", "select", "(", "obj", ",", "keep_selection", "=", "True", ")"], "docstring": "Get the selected object and store start position", "docstring_tokens": ["Get", "the", "selected", "object", "and", "store", "start", "position"], "sha": "aca0a05f6fcde55c94ad7cc058671a06608b01a4", "url": "https://github.com/reingart/gui2py/blob/aca0a05f6fcde55c94ad7cc058671a06608b01a4/gui/tools/designer.py#L123-L163", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/bigquery_hook.py", "func_name": "BigQueryBaseCursor.run_table_upsert", "original_string": "def run_table_upsert(self, dataset_id, table_resource, project_id=None):\n        \"\"\"\n        creates a new, empty table in the dataset;\n        If the table already exists, update the existing table.\n        Since BigQuery does not natively allow table upserts, this is not an\n        atomic operation.\n\n        :param dataset_id: the dataset to upsert the table into.\n        :type dataset_id: str\n        :param table_resource: a table resource. see\n            https://cloud.google.com/bigquery/docs/reference/v2/tables#resource\n        :type table_resource: dict\n        :param project_id: the project to upsert the table into.  If None,\n            project will be self.project_id.\n        :return:\n        \"\"\"\n        # check to see if the table exists\n        table_id = table_resource['tableReference']['tableId']\n        project_id = project_id if project_id is not None else self.project_id\n        tables_list_resp = self.service.tables().list(\n            projectId=project_id, datasetId=dataset_id).execute(num_retries=self.num_retries)\n        while True:\n            for table in tables_list_resp.get('tables', []):\n                if table['tableReference']['tableId'] == table_id:\n                    # found the table, do update\n                    self.log.info('Table %s:%s.%s exists, updating.',\n                                  project_id, dataset_id, table_id)\n                    return self.service.tables().update(\n                        projectId=project_id,\n                        datasetId=dataset_id,\n                        tableId=table_id,\n                        body=table_resource).execute(num_retries=self.num_retries)\n            # If there is a next page, we need to check the next page.\n            if 'nextPageToken' in tables_list_resp:\n                tables_list_resp = self.service.tables()\\\n                    .list(projectId=project_id,\n                          datasetId=dataset_id,\n                          pageToken=tables_list_resp['nextPageToken'])\\\n                    .execute(num_retries=self.num_retries)\n            # If there is no next page, then the table doesn't exist.\n            else:\n                # do insert\n                self.log.info('Table %s:%s.%s does not exist. creating.',\n                              project_id, dataset_id, table_id)\n                return self.service.tables().insert(\n                    projectId=project_id,\n                    datasetId=dataset_id,\n                    body=table_resource).execute(num_retries=self.num_retries)", "language": "python", "code": "def run_table_upsert(self, dataset_id, table_resource, project_id=None):\n        \"\"\"\n        creates a new, empty table in the dataset;\n        If the table already exists, update the existing table.\n        Since BigQuery does not natively allow table upserts, this is not an\n        atomic operation.\n\n        :param dataset_id: the dataset to upsert the table into.\n        :type dataset_id: str\n        :param table_resource: a table resource. see\n            https://cloud.google.com/bigquery/docs/reference/v2/tables#resource\n        :type table_resource: dict\n        :param project_id: the project to upsert the table into.  If None,\n            project will be self.project_id.\n        :return:\n        \"\"\"\n        # check to see if the table exists\n        table_id = table_resource['tableReference']['tableId']\n        project_id = project_id if project_id is not None else self.project_id\n        tables_list_resp = self.service.tables().list(\n            projectId=project_id, datasetId=dataset_id).execute(num_retries=self.num_retries)\n        while True:\n            for table in tables_list_resp.get('tables', []):\n                if table['tableReference']['tableId'] == table_id:\n                    # found the table, do update\n                    self.log.info('Table %s:%s.%s exists, updating.',\n                                  project_id, dataset_id, table_id)\n                    return self.service.tables().update(\n                        projectId=project_id,\n                        datasetId=dataset_id,\n                        tableId=table_id,\n                        body=table_resource).execute(num_retries=self.num_retries)\n            # If there is a next page, we need to check the next page.\n            if 'nextPageToken' in tables_list_resp:\n                tables_list_resp = self.service.tables()\\\n                    .list(projectId=project_id,\n                          datasetId=dataset_id,\n                          pageToken=tables_list_resp['nextPageToken'])\\\n                    .execute(num_retries=self.num_retries)\n            # If there is no next page, then the table doesn't exist.\n            else:\n                # do insert\n                self.log.info('Table %s:%s.%s does not exist. creating.',\n                              project_id, dataset_id, table_id)\n                return self.service.tables().insert(\n                    projectId=project_id,\n                    datasetId=dataset_id,\n                    body=table_resource).execute(num_retries=self.num_retries)", "code_tokens": ["def", "run_table_upsert", "(", "self", ",", "dataset_id", ",", "table_resource", ",", "project_id", "=", "None", ")", ":", "# check to see if the table exists", "table_id", "=", "table_resource", "[", "'tableReference'", "]", "[", "'tableId'", "]", "project_id", "=", "project_id", "if", "project_id", "is", "not", "None", "else", "self", ".", "project_id", "tables_list_resp", "=", "self", ".", "service", ".", "tables", "(", ")", ".", "list", "(", "projectId", "=", "project_id", ",", "datasetId", "=", "dataset_id", ")", ".", "execute", "(", "num_retries", "=", "self", ".", "num_retries", ")", "while", "True", ":", "for", "table", "in", "tables_list_resp", ".", "get", "(", "'tables'", ",", "[", "]", ")", ":", "if", "table", "[", "'tableReference'", "]", "[", "'tableId'", "]", "==", "table_id", ":", "# found the table, do update", "self", ".", "log", ".", "info", "(", "'Table %s:%s.%s exists, updating.'", ",", "project_id", ",", "dataset_id", ",", "table_id", ")", "return", "self", ".", "service", ".", "tables", "(", ")", ".", "update", "(", "projectId", "=", "project_id", ",", "datasetId", "=", "dataset_id", ",", "tableId", "=", "table_id", ",", "body", "=", "table_resource", ")", ".", "execute", "(", "num_retries", "=", "self", ".", "num_retries", ")", "# If there is a next page, we need to check the next page.", "if", "'nextPageToken'", "in", "tables_list_resp", ":", "tables_list_resp", "=", "self", ".", "service", ".", "tables", "(", ")", ".", "list", "(", "projectId", "=", "project_id", ",", "datasetId", "=", "dataset_id", ",", "pageToken", "=", "tables_list_resp", "[", "'nextPageToken'", "]", ")", ".", "execute", "(", "num_retries", "=", "self", ".", "num_retries", ")", "# If there is no next page, then the table doesn't exist.", "else", ":", "# do insert", "self", ".", "log", ".", "info", "(", "'Table %s:%s.%s does not exist. creating.'", ",", "project_id", ",", "dataset_id", ",", "table_id", ")", "return", "self", ".", "service", ".", "tables", "(", ")", ".", "insert", "(", "projectId", "=", "project_id", ",", "datasetId", "=", "dataset_id", ",", "body", "=", "table_resource", ")", ".", "execute", "(", "num_retries", "=", "self", ".", "num_retries", ")"], "docstring": "creates a new, empty table in the dataset;\n        If the table already exists, update the existing table.\n        Since BigQuery does not natively allow table upserts, this is not an\n        atomic operation.\n\n        :param dataset_id: the dataset to upsert the table into.\n        :type dataset_id: str\n        :param table_resource: a table resource. see\n            https://cloud.google.com/bigquery/docs/reference/v2/tables#resource\n        :type table_resource: dict\n        :param project_id: the project to upsert the table into.  If None,\n            project will be self.project_id.\n        :return:", "docstring_tokens": ["creates", "a", "new", "empty", "table", "in", "the", "dataset", ";", "If", "the", "table", "already", "exists", "update", "the", "existing", "table", ".", "Since", "BigQuery", "does", "not", "natively", "allow", "table", "upserts", "this", "is", "not", "an", "atomic", "operation", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/bigquery_hook.py#L1421-L1468", "partition": "test"}
{"repo": "tomasbasham/dominos", "path": "dominos/api.py", "func_name": "Client.set_delivery_system", "original_string": "def set_delivery_system(self, store, postcode, fulfilment_method=FULFILMENT_METHOD.DELIVERY):\n        '''\n        Set local cookies by initialising the delivery system on the remote.\n        Requires a store ID and a delivery postcode.\n\n        :param Store store: Store id.\n        :param string postcode: A postcode.\n        :return: A response having initialised the delivery system.\n        :rtype: requests.Response\n        '''\n        method = 'delivery' if fulfilment_method == FULFILMENT_METHOD.DELIVERY else 'collection'\n\n        params = {\n            'fulfilmentMethod': method,\n            'postcode': postcode,\n            'storeid': store.store_id\n        }\n\n        return self.__post('/Journey/Initialize', json=params)", "language": "python", "code": "def set_delivery_system(self, store, postcode, fulfilment_method=FULFILMENT_METHOD.DELIVERY):\n        '''\n        Set local cookies by initialising the delivery system on the remote.\n        Requires a store ID and a delivery postcode.\n\n        :param Store store: Store id.\n        :param string postcode: A postcode.\n        :return: A response having initialised the delivery system.\n        :rtype: requests.Response\n        '''\n        method = 'delivery' if fulfilment_method == FULFILMENT_METHOD.DELIVERY else 'collection'\n\n        params = {\n            'fulfilmentMethod': method,\n            'postcode': postcode,\n            'storeid': store.store_id\n        }\n\n        return self.__post('/Journey/Initialize', json=params)", "code_tokens": ["def", "set_delivery_system", "(", "self", ",", "store", ",", "postcode", ",", "fulfilment_method", "=", "FULFILMENT_METHOD", ".", "DELIVERY", ")", ":", "method", "=", "'delivery'", "if", "fulfilment_method", "==", "FULFILMENT_METHOD", ".", "DELIVERY", "else", "'collection'", "params", "=", "{", "'fulfilmentMethod'", ":", "method", ",", "'postcode'", ":", "postcode", ",", "'storeid'", ":", "store", ".", "store_id", "}", "return", "self", ".", "__post", "(", "'/Journey/Initialize'", ",", "json", "=", "params", ")"], "docstring": "Set local cookies by initialising the delivery system on the remote.\n        Requires a store ID and a delivery postcode.\n\n        :param Store store: Store id.\n        :param string postcode: A postcode.\n        :return: A response having initialised the delivery system.\n        :rtype: requests.Response", "docstring_tokens": ["Set", "local", "cookies", "by", "initialising", "the", "delivery", "system", "on", "the", "remote", ".", "Requires", "a", "store", "ID", "and", "a", "delivery", "postcode", "."], "sha": "59729a8bdca0ae30a84115a0e93e9b1f259faf0e", "url": "https://github.com/tomasbasham/dominos/blob/59729a8bdca0ae30a84115a0e93e9b1f259faf0e/dominos/api.py#L82-L100", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/parallel/client/map.py", "func_name": "Map.getPartition", "original_string": "def getPartition(self, seq, p, q):\n        \"\"\"Returns the pth partition of q partitions of seq.\"\"\"\n        \n        # Test for error conditions here\n        if p<0 or p>=q:\n          print \"No partition exists.\"\n          return\n          \n        remainder = len(seq)%q\n        basesize = len(seq)//q\n        hi = []\n        lo = []\n        for n in range(q):\n            if n < remainder:\n                lo.append(n * (basesize + 1))\n                hi.append(lo[-1] + basesize + 1)\n            else:\n                lo.append(n*basesize + remainder)\n                hi.append(lo[-1] + basesize)\n        \n        try:\n            result = seq[lo[p]:hi[p]]\n        except TypeError:\n            # some objects (iterators) can't be sliced,\n            # use islice:\n            result = list(islice(seq, lo[p], hi[p]))\n            \n        return result", "language": "python", "code": "def getPartition(self, seq, p, q):\n        \"\"\"Returns the pth partition of q partitions of seq.\"\"\"\n        \n        # Test for error conditions here\n        if p<0 or p>=q:\n          print \"No partition exists.\"\n          return\n          \n        remainder = len(seq)%q\n        basesize = len(seq)//q\n        hi = []\n        lo = []\n        for n in range(q):\n            if n < remainder:\n                lo.append(n * (basesize + 1))\n                hi.append(lo[-1] + basesize + 1)\n            else:\n                lo.append(n*basesize + remainder)\n                hi.append(lo[-1] + basesize)\n        \n        try:\n            result = seq[lo[p]:hi[p]]\n        except TypeError:\n            # some objects (iterators) can't be sliced,\n            # use islice:\n            result = list(islice(seq, lo[p], hi[p]))\n            \n        return result", "code_tokens": ["def", "getPartition", "(", "self", ",", "seq", ",", "p", ",", "q", ")", ":", "# Test for error conditions here", "if", "p", "<", "0", "or", "p", ">=", "q", ":", "print", "\"No partition exists.\"", "return", "remainder", "=", "len", "(", "seq", ")", "%", "q", "basesize", "=", "len", "(", "seq", ")", "//", "q", "hi", "=", "[", "]", "lo", "=", "[", "]", "for", "n", "in", "range", "(", "q", ")", ":", "if", "n", "<", "remainder", ":", "lo", ".", "append", "(", "n", "*", "(", "basesize", "+", "1", ")", ")", "hi", ".", "append", "(", "lo", "[", "-", "1", "]", "+", "basesize", "+", "1", ")", "else", ":", "lo", ".", "append", "(", "n", "*", "basesize", "+", "remainder", ")", "hi", ".", "append", "(", "lo", "[", "-", "1", "]", "+", "basesize", ")", "try", ":", "result", "=", "seq", "[", "lo", "[", "p", "]", ":", "hi", "[", "p", "]", "]", "except", "TypeError", ":", "# some objects (iterators) can't be sliced,", "# use islice:", "result", "=", "list", "(", "islice", "(", "seq", ",", "lo", "[", "p", "]", ",", "hi", "[", "p", "]", ")", ")", "return", "result"], "docstring": "Returns the pth partition of q partitions of seq.", "docstring_tokens": ["Returns", "the", "pth", "partition", "of", "q", "partitions", "of", "seq", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/parallel/client/map.py#L62-L89", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/converters/dag_to_circuit.py", "func_name": "dag_to_circuit", "original_string": "def dag_to_circuit(dag):\n    \"\"\"Build a ``QuantumCircuit`` object from a ``DAGCircuit``.\n\n    Args:\n        dag (DAGCircuit): the input dag.\n\n    Return:\n        QuantumCircuit: the circuit representing the input dag.\n    \"\"\"\n    qregs = collections.OrderedDict()\n    for qreg in dag.qregs.values():\n        qreg_tmp = QuantumRegister(qreg.size, name=qreg.name)\n        qregs[qreg.name] = qreg_tmp\n    cregs = collections.OrderedDict()\n    for creg in dag.cregs.values():\n        creg_tmp = ClassicalRegister(creg.size, name=creg.name)\n        cregs[creg.name] = creg_tmp\n\n    name = dag.name or None\n    circuit = QuantumCircuit(*qregs.values(), *cregs.values(), name=name)\n\n    for node in dag.topological_op_nodes():\n        qubits = []\n        for qubit in node.qargs:\n            qubits.append(qregs[qubit[0].name][qubit[1]])\n\n        clbits = []\n        for clbit in node.cargs:\n            clbits.append(cregs[clbit[0].name][clbit[1]])\n\n        # Get arguments for classical control (if any)\n        if node.condition is None:\n            control = None\n        else:\n            control = (node.condition[0], node.condition[1])\n\n        inst = node.op.copy()\n        inst.control = control\n        circuit.append(inst, qubits, clbits)\n    return circuit", "language": "python", "code": "def dag_to_circuit(dag):\n    \"\"\"Build a ``QuantumCircuit`` object from a ``DAGCircuit``.\n\n    Args:\n        dag (DAGCircuit): the input dag.\n\n    Return:\n        QuantumCircuit: the circuit representing the input dag.\n    \"\"\"\n    qregs = collections.OrderedDict()\n    for qreg in dag.qregs.values():\n        qreg_tmp = QuantumRegister(qreg.size, name=qreg.name)\n        qregs[qreg.name] = qreg_tmp\n    cregs = collections.OrderedDict()\n    for creg in dag.cregs.values():\n        creg_tmp = ClassicalRegister(creg.size, name=creg.name)\n        cregs[creg.name] = creg_tmp\n\n    name = dag.name or None\n    circuit = QuantumCircuit(*qregs.values(), *cregs.values(), name=name)\n\n    for node in dag.topological_op_nodes():\n        qubits = []\n        for qubit in node.qargs:\n            qubits.append(qregs[qubit[0].name][qubit[1]])\n\n        clbits = []\n        for clbit in node.cargs:\n            clbits.append(cregs[clbit[0].name][clbit[1]])\n\n        # Get arguments for classical control (if any)\n        if node.condition is None:\n            control = None\n        else:\n            control = (node.condition[0], node.condition[1])\n\n        inst = node.op.copy()\n        inst.control = control\n        circuit.append(inst, qubits, clbits)\n    return circuit", "code_tokens": ["def", "dag_to_circuit", "(", "dag", ")", ":", "qregs", "=", "collections", ".", "OrderedDict", "(", ")", "for", "qreg", "in", "dag", ".", "qregs", ".", "values", "(", ")", ":", "qreg_tmp", "=", "QuantumRegister", "(", "qreg", ".", "size", ",", "name", "=", "qreg", ".", "name", ")", "qregs", "[", "qreg", ".", "name", "]", "=", "qreg_tmp", "cregs", "=", "collections", ".", "OrderedDict", "(", ")", "for", "creg", "in", "dag", ".", "cregs", ".", "values", "(", ")", ":", "creg_tmp", "=", "ClassicalRegister", "(", "creg", ".", "size", ",", "name", "=", "creg", ".", "name", ")", "cregs", "[", "creg", ".", "name", "]", "=", "creg_tmp", "name", "=", "dag", ".", "name", "or", "None", "circuit", "=", "QuantumCircuit", "(", "*", "qregs", ".", "values", "(", ")", ",", "*", "cregs", ".", "values", "(", ")", ",", "name", "=", "name", ")", "for", "node", "in", "dag", ".", "topological_op_nodes", "(", ")", ":", "qubits", "=", "[", "]", "for", "qubit", "in", "node", ".", "qargs", ":", "qubits", ".", "append", "(", "qregs", "[", "qubit", "[", "0", "]", ".", "name", "]", "[", "qubit", "[", "1", "]", "]", ")", "clbits", "=", "[", "]", "for", "clbit", "in", "node", ".", "cargs", ":", "clbits", ".", "append", "(", "cregs", "[", "clbit", "[", "0", "]", ".", "name", "]", "[", "clbit", "[", "1", "]", "]", ")", "# Get arguments for classical control (if any)", "if", "node", ".", "condition", "is", "None", ":", "control", "=", "None", "else", ":", "control", "=", "(", "node", ".", "condition", "[", "0", "]", ",", "node", ".", "condition", "[", "1", "]", ")", "inst", "=", "node", ".", "op", ".", "copy", "(", ")", "inst", ".", "control", "=", "control", "circuit", ".", "append", "(", "inst", ",", "qubits", ",", "clbits", ")", "return", "circuit"], "docstring": "Build a ``QuantumCircuit`` object from a ``DAGCircuit``.\n\n    Args:\n        dag (DAGCircuit): the input dag.\n\n    Return:\n        QuantumCircuit: the circuit representing the input dag.", "docstring_tokens": ["Build", "a", "QuantumCircuit", "object", "from", "a", "DAGCircuit", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/converters/dag_to_circuit.py#L16-L55", "partition": "test"}
{"repo": "5j9/wikitextparser", "path": "wikitextparser/_wikitext.py", "func_name": "SubWikiText.parent", "original_string": "def parent(self, type_: Optional[str] = None) -> Optional['WikiText']:\n        \"\"\"Return the parent node of the current object.\n\n        :param type_: the type of the desired parent object.\n            Currently the following types are supported: {Template,\n            ParserFunction, WikiLink, Comment, Parameter, ExtensionTag}.\n            The default is None and means the first parent, of any type above.\n        :return: parent WikiText object or None if no parent with the desired\n            `type_` is found.\n        \"\"\"\n        ancestors = self.ancestors(type_)\n        if ancestors:\n            return ancestors[0]\n        return None", "language": "python", "code": "def parent(self, type_: Optional[str] = None) -> Optional['WikiText']:\n        \"\"\"Return the parent node of the current object.\n\n        :param type_: the type of the desired parent object.\n            Currently the following types are supported: {Template,\n            ParserFunction, WikiLink, Comment, Parameter, ExtensionTag}.\n            The default is None and means the first parent, of any type above.\n        :return: parent WikiText object or None if no parent with the desired\n            `type_` is found.\n        \"\"\"\n        ancestors = self.ancestors(type_)\n        if ancestors:\n            return ancestors[0]\n        return None", "code_tokens": ["def", "parent", "(", "self", ",", "type_", ":", "Optional", "[", "str", "]", "=", "None", ")", "->", "Optional", "[", "'WikiText'", "]", ":", "ancestors", "=", "self", ".", "ancestors", "(", "type_", ")", "if", "ancestors", ":", "return", "ancestors", "[", "0", "]", "return", "None"], "docstring": "Return the parent node of the current object.\n\n        :param type_: the type of the desired parent object.\n            Currently the following types are supported: {Template,\n            ParserFunction, WikiLink, Comment, Parameter, ExtensionTag}.\n            The default is None and means the first parent, of any type above.\n        :return: parent WikiText object or None if no parent with the desired\n            `type_` is found.", "docstring_tokens": ["Return", "the", "parent", "node", "of", "the", "current", "object", "."], "sha": "1347425814361d7955342c53212edbb27f0ff4b5", "url": "https://github.com/5j9/wikitextparser/blob/1347425814361d7955342c53212edbb27f0ff4b5/wikitextparser/_wikitext.py#L1090-L1103", "partition": "test"}
{"repo": "scheibler/khard", "path": "khard/actions.py", "func_name": "Actions.get_action", "original_string": "def get_action(cls, alias):\n        \"\"\"Find the name of the action for the supplied alias.  If no action is\n        asociated with the given alias, None is returned.\n\n        :param alias: the alias to look up\n        :type alias: str\n        :rturns: the name of the corresponding action or None\n        :rtype: str or NoneType\n\n        \"\"\"\n        for action, alias_list in cls.action_map.items():\n            if alias in alias_list:\n                return action\n        return None", "language": "python", "code": "def get_action(cls, alias):\n        \"\"\"Find the name of the action for the supplied alias.  If no action is\n        asociated with the given alias, None is returned.\n\n        :param alias: the alias to look up\n        :type alias: str\n        :rturns: the name of the corresponding action or None\n        :rtype: str or NoneType\n\n        \"\"\"\n        for action, alias_list in cls.action_map.items():\n            if alias in alias_list:\n                return action\n        return None", "code_tokens": ["def", "get_action", "(", "cls", ",", "alias", ")", ":", "for", "action", ",", "alias_list", "in", "cls", ".", "action_map", ".", "items", "(", ")", ":", "if", "alias", "in", "alias_list", ":", "return", "action", "return", "None"], "docstring": "Find the name of the action for the supplied alias.  If no action is\n        asociated with the given alias, None is returned.\n\n        :param alias: the alias to look up\n        :type alias: str\n        :rturns: the name of the corresponding action or None\n        :rtype: str or NoneType", "docstring_tokens": ["Find", "the", "name", "of", "the", "action", "for", "the", "supplied", "alias", ".", "If", "no", "action", "is", "asociated", "with", "the", "given", "alias", "None", "is", "returned", "."], "sha": "0f69430c2680f1ff5f073a977a3c5b753b96cc17", "url": "https://github.com/scheibler/khard/blob/0f69430c2680f1ff5f073a977a3c5b753b96cc17/khard/actions.py#L30-L43", "partition": "test"}
{"repo": "neherlab/treetime", "path": "treetime/treeanc.py", "func_name": "TreeAnc.seq_len", "original_string": "def seq_len(self,L):\n        \"\"\"set the length of the uncompressed sequence. its inverse 'one_mutation'\n        is frequently used as a general length scale. This can't be changed once\n        it is set.\n\n        Parameters\n        ----------\n        L : int\n            length of the sequence alignment\n        \"\"\"\n        if (not hasattr(self, '_seq_len')) or self._seq_len is None:\n            if L:\n                self._seq_len = int(L)\n        else:\n            self.logger(\"TreeAnc: one_mutation and sequence length can't be reset\",1)", "language": "python", "code": "def seq_len(self,L):\n        \"\"\"set the length of the uncompressed sequence. its inverse 'one_mutation'\n        is frequently used as a general length scale. This can't be changed once\n        it is set.\n\n        Parameters\n        ----------\n        L : int\n            length of the sequence alignment\n        \"\"\"\n        if (not hasattr(self, '_seq_len')) or self._seq_len is None:\n            if L:\n                self._seq_len = int(L)\n        else:\n            self.logger(\"TreeAnc: one_mutation and sequence length can't be reset\",1)", "code_tokens": ["def", "seq_len", "(", "self", ",", "L", ")", ":", "if", "(", "not", "hasattr", "(", "self", ",", "'_seq_len'", ")", ")", "or", "self", ".", "_seq_len", "is", "None", ":", "if", "L", ":", "self", ".", "_seq_len", "=", "int", "(", "L", ")", "else", ":", "self", ".", "logger", "(", "\"TreeAnc: one_mutation and sequence length can't be reset\"", ",", "1", ")"], "docstring": "set the length of the uncompressed sequence. its inverse 'one_mutation'\n        is frequently used as a general length scale. This can't be changed once\n        it is set.\n\n        Parameters\n        ----------\n        L : int\n            length of the sequence alignment", "docstring_tokens": ["set", "the", "length", "of", "the", "uncompressed", "sequence", ".", "its", "inverse", "one_mutation", "is", "frequently", "used", "as", "a", "general", "length", "scale", ".", "This", "can", "t", "be", "changed", "once", "it", "is", "set", "."], "sha": "f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0", "url": "https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/treeanc.py#L371-L385", "partition": "test"}
{"repo": "respondcreate/django-versatileimagefield", "path": "versatileimagefield/datastructures/sizedimage.py", "func_name": "SizedImage.ppoi_as_str", "original_string": "def ppoi_as_str(self):\n        \"\"\"Return PPOI value as a string.\"\"\"\n        return \"%s__%s\" % (\n            str(self.ppoi[0]).replace('.', '-'),\n            str(self.ppoi[1]).replace('.', '-')\n        )", "language": "python", "code": "def ppoi_as_str(self):\n        \"\"\"Return PPOI value as a string.\"\"\"\n        return \"%s__%s\" % (\n            str(self.ppoi[0]).replace('.', '-'),\n            str(self.ppoi[1]).replace('.', '-')\n        )", "code_tokens": ["def", "ppoi_as_str", "(", "self", ")", ":", "return", "\"%s__%s\"", "%", "(", "str", "(", "self", ".", "ppoi", "[", "0", "]", ")", ".", "replace", "(", "'.'", ",", "'-'", ")", ",", "str", "(", "self", ".", "ppoi", "[", "1", "]", ")", ".", "replace", "(", "'.'", ",", "'-'", ")", ")"], "docstring": "Return PPOI value as a string.", "docstring_tokens": ["Return", "PPOI", "value", "as", "a", "string", "."], "sha": "d41e279c39cccffafbe876c67596184704ae8877", "url": "https://github.com/respondcreate/django-versatileimagefield/blob/d41e279c39cccffafbe876c67596184704ae8877/versatileimagefield/datastructures/sizedimage.py#L64-L69", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicemanagement-legacy/azure/servicemanagement/_serialization.py", "func_name": "_MinidomXmlToObject.get_entry_properties_from_node", "original_string": "def get_entry_properties_from_node(entry, include_id, id_prefix_to_skip=None, use_title_as_id=False):\n        ''' get properties from entry xml '''\n        properties = {}\n\n        etag = entry.getAttributeNS(METADATA_NS, 'etag')\n        if etag:\n            properties['etag'] = etag\n        for updated in _MinidomXmlToObject.get_child_nodes(entry, 'updated'):\n            properties['updated'] = updated.firstChild.nodeValue\n        for name in _MinidomXmlToObject.get_children_from_path(entry, 'author', 'name'):\n            if name.firstChild is not None:\n                properties['author'] = name.firstChild.nodeValue\n\n        if include_id:\n            if use_title_as_id:\n                for title in _MinidomXmlToObject.get_child_nodes(entry, 'title'):\n                    properties['name'] = title.firstChild.nodeValue\n            else:\n                # TODO: check if this is used\n                for id in _MinidomXmlToObject.get_child_nodes(entry, 'id'):\n                    properties['name'] = _get_readable_id(\n                        id.firstChild.nodeValue, id_prefix_to_skip)\n\n        return properties", "language": "python", "code": "def get_entry_properties_from_node(entry, include_id, id_prefix_to_skip=None, use_title_as_id=False):\n        ''' get properties from entry xml '''\n        properties = {}\n\n        etag = entry.getAttributeNS(METADATA_NS, 'etag')\n        if etag:\n            properties['etag'] = etag\n        for updated in _MinidomXmlToObject.get_child_nodes(entry, 'updated'):\n            properties['updated'] = updated.firstChild.nodeValue\n        for name in _MinidomXmlToObject.get_children_from_path(entry, 'author', 'name'):\n            if name.firstChild is not None:\n                properties['author'] = name.firstChild.nodeValue\n\n        if include_id:\n            if use_title_as_id:\n                for title in _MinidomXmlToObject.get_child_nodes(entry, 'title'):\n                    properties['name'] = title.firstChild.nodeValue\n            else:\n                # TODO: check if this is used\n                for id in _MinidomXmlToObject.get_child_nodes(entry, 'id'):\n                    properties['name'] = _get_readable_id(\n                        id.firstChild.nodeValue, id_prefix_to_skip)\n\n        return properties", "code_tokens": ["def", "get_entry_properties_from_node", "(", "entry", ",", "include_id", ",", "id_prefix_to_skip", "=", "None", ",", "use_title_as_id", "=", "False", ")", ":", "properties", "=", "{", "}", "etag", "=", "entry", ".", "getAttributeNS", "(", "METADATA_NS", ",", "'etag'", ")", "if", "etag", ":", "properties", "[", "'etag'", "]", "=", "etag", "for", "updated", "in", "_MinidomXmlToObject", ".", "get_child_nodes", "(", "entry", ",", "'updated'", ")", ":", "properties", "[", "'updated'", "]", "=", "updated", ".", "firstChild", ".", "nodeValue", "for", "name", "in", "_MinidomXmlToObject", ".", "get_children_from_path", "(", "entry", ",", "'author'", ",", "'name'", ")", ":", "if", "name", ".", "firstChild", "is", "not", "None", ":", "properties", "[", "'author'", "]", "=", "name", ".", "firstChild", ".", "nodeValue", "if", "include_id", ":", "if", "use_title_as_id", ":", "for", "title", "in", "_MinidomXmlToObject", ".", "get_child_nodes", "(", "entry", ",", "'title'", ")", ":", "properties", "[", "'name'", "]", "=", "title", ".", "firstChild", ".", "nodeValue", "else", ":", "# TODO: check if this is used", "for", "id", "in", "_MinidomXmlToObject", ".", "get_child_nodes", "(", "entry", ",", "'id'", ")", ":", "properties", "[", "'name'", "]", "=", "_get_readable_id", "(", "id", ".", "firstChild", ".", "nodeValue", ",", "id_prefix_to_skip", ")", "return", "properties"], "docstring": "get properties from entry xml", "docstring_tokens": ["get", "properties", "from", "entry", "xml"], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/_serialization.py#L141-L164", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/util/utils.py", "func_name": "roll_sparse", "original_string": "def roll_sparse(x, shift, axis=0):\n    '''Sparse matrix roll\n\n    This operation is equivalent to ``numpy.roll``, but operates on sparse matrices.\n\n    Parameters\n    ----------\n    x : scipy.sparse.spmatrix or np.ndarray\n        The sparse matrix input\n\n    shift : int\n        The number of positions to roll the specified axis\n\n    axis : (0, 1, -1)\n        The axis along which to roll.\n\n    Returns\n    -------\n    x_rolled : same type as `x`\n        The rolled matrix, with the same format as `x`\n\n    See Also\n    --------\n    numpy.roll\n\n    Examples\n    --------\n    >>> # Generate a random sparse binary matrix\n    >>> X = scipy.sparse.lil_matrix(np.random.randint(0, 2, size=(5,5)))\n    >>> X_roll = roll_sparse(X, 2, axis=0)  # Roll by 2 on the first axis\n    >>> X_dense_r = roll_sparse(X.toarray(), 2, axis=0)  # Equivalent dense roll\n    >>> np.allclose(X_roll, X_dense_r.toarray())\n    True\n    '''\n    if not scipy.sparse.isspmatrix(x):\n        return np.roll(x, shift, axis=axis)\n\n    # shift-mod-length lets us have shift > x.shape[axis]\n    if axis not in [0, 1, -1]:\n        raise ParameterError('axis must be one of (0, 1, -1)')\n\n    shift = np.mod(shift, x.shape[axis])\n\n    if shift == 0:\n        return x.copy()\n\n    fmt = x.format\n    if axis == 0:\n        x = x.tocsc()\n    elif axis in (-1, 1):\n        x = x.tocsr()\n\n    # lil matrix to start\n    x_r = scipy.sparse.lil_matrix(x.shape, dtype=x.dtype)\n\n    idx_in = [slice(None)] * x.ndim\n    idx_out = [slice(None)] * x_r.ndim\n\n    idx_in[axis] = slice(0, -shift)\n    idx_out[axis] = slice(shift, None)\n    x_r[tuple(idx_out)] = x[tuple(idx_in)]\n\n    idx_out[axis] = slice(0, shift)\n    idx_in[axis] = slice(-shift, None)\n    x_r[tuple(idx_out)] = x[tuple(idx_in)]\n\n    return x_r.asformat(fmt)", "language": "python", "code": "def roll_sparse(x, shift, axis=0):\n    '''Sparse matrix roll\n\n    This operation is equivalent to ``numpy.roll``, but operates on sparse matrices.\n\n    Parameters\n    ----------\n    x : scipy.sparse.spmatrix or np.ndarray\n        The sparse matrix input\n\n    shift : int\n        The number of positions to roll the specified axis\n\n    axis : (0, 1, -1)\n        The axis along which to roll.\n\n    Returns\n    -------\n    x_rolled : same type as `x`\n        The rolled matrix, with the same format as `x`\n\n    See Also\n    --------\n    numpy.roll\n\n    Examples\n    --------\n    >>> # Generate a random sparse binary matrix\n    >>> X = scipy.sparse.lil_matrix(np.random.randint(0, 2, size=(5,5)))\n    >>> X_roll = roll_sparse(X, 2, axis=0)  # Roll by 2 on the first axis\n    >>> X_dense_r = roll_sparse(X.toarray(), 2, axis=0)  # Equivalent dense roll\n    >>> np.allclose(X_roll, X_dense_r.toarray())\n    True\n    '''\n    if not scipy.sparse.isspmatrix(x):\n        return np.roll(x, shift, axis=axis)\n\n    # shift-mod-length lets us have shift > x.shape[axis]\n    if axis not in [0, 1, -1]:\n        raise ParameterError('axis must be one of (0, 1, -1)')\n\n    shift = np.mod(shift, x.shape[axis])\n\n    if shift == 0:\n        return x.copy()\n\n    fmt = x.format\n    if axis == 0:\n        x = x.tocsc()\n    elif axis in (-1, 1):\n        x = x.tocsr()\n\n    # lil matrix to start\n    x_r = scipy.sparse.lil_matrix(x.shape, dtype=x.dtype)\n\n    idx_in = [slice(None)] * x.ndim\n    idx_out = [slice(None)] * x_r.ndim\n\n    idx_in[axis] = slice(0, -shift)\n    idx_out[axis] = slice(shift, None)\n    x_r[tuple(idx_out)] = x[tuple(idx_in)]\n\n    idx_out[axis] = slice(0, shift)\n    idx_in[axis] = slice(-shift, None)\n    x_r[tuple(idx_out)] = x[tuple(idx_in)]\n\n    return x_r.asformat(fmt)", "code_tokens": ["def", "roll_sparse", "(", "x", ",", "shift", ",", "axis", "=", "0", ")", ":", "if", "not", "scipy", ".", "sparse", ".", "isspmatrix", "(", "x", ")", ":", "return", "np", ".", "roll", "(", "x", ",", "shift", ",", "axis", "=", "axis", ")", "# shift-mod-length lets us have shift > x.shape[axis]", "if", "axis", "not", "in", "[", "0", ",", "1", ",", "-", "1", "]", ":", "raise", "ParameterError", "(", "'axis must be one of (0, 1, -1)'", ")", "shift", "=", "np", ".", "mod", "(", "shift", ",", "x", ".", "shape", "[", "axis", "]", ")", "if", "shift", "==", "0", ":", "return", "x", ".", "copy", "(", ")", "fmt", "=", "x", ".", "format", "if", "axis", "==", "0", ":", "x", "=", "x", ".", "tocsc", "(", ")", "elif", "axis", "in", "(", "-", "1", ",", "1", ")", ":", "x", "=", "x", ".", "tocsr", "(", ")", "# lil matrix to start", "x_r", "=", "scipy", ".", "sparse", ".", "lil_matrix", "(", "x", ".", "shape", ",", "dtype", "=", "x", ".", "dtype", ")", "idx_in", "=", "[", "slice", "(", "None", ")", "]", "*", "x", ".", "ndim", "idx_out", "=", "[", "slice", "(", "None", ")", "]", "*", "x_r", ".", "ndim", "idx_in", "[", "axis", "]", "=", "slice", "(", "0", ",", "-", "shift", ")", "idx_out", "[", "axis", "]", "=", "slice", "(", "shift", ",", "None", ")", "x_r", "[", "tuple", "(", "idx_out", ")", "]", "=", "x", "[", "tuple", "(", "idx_in", ")", "]", "idx_out", "[", "axis", "]", "=", "slice", "(", "0", ",", "shift", ")", "idx_in", "[", "axis", "]", "=", "slice", "(", "-", "shift", ",", "None", ")", "x_r", "[", "tuple", "(", "idx_out", ")", "]", "=", "x", "[", "tuple", "(", "idx_in", ")", "]", "return", "x_r", ".", "asformat", "(", "fmt", ")"], "docstring": "Sparse matrix roll\n\n    This operation is equivalent to ``numpy.roll``, but operates on sparse matrices.\n\n    Parameters\n    ----------\n    x : scipy.sparse.spmatrix or np.ndarray\n        The sparse matrix input\n\n    shift : int\n        The number of positions to roll the specified axis\n\n    axis : (0, 1, -1)\n        The axis along which to roll.\n\n    Returns\n    -------\n    x_rolled : same type as `x`\n        The rolled matrix, with the same format as `x`\n\n    See Also\n    --------\n    numpy.roll\n\n    Examples\n    --------\n    >>> # Generate a random sparse binary matrix\n    >>> X = scipy.sparse.lil_matrix(np.random.randint(0, 2, size=(5,5)))\n    >>> X_roll = roll_sparse(X, 2, axis=0)  # Roll by 2 on the first axis\n    >>> X_dense_r = roll_sparse(X.toarray(), 2, axis=0)  # Equivalent dense roll\n    >>> np.allclose(X_roll, X_dense_r.toarray())\n    True", "docstring_tokens": ["Sparse", "matrix", "roll"], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/util/utils.py#L1101-L1167", "partition": "test"}
{"repo": "c-soft/satel_integra", "path": "satel_integra/satel_integra.py", "func_name": "demo", "original_string": "def demo(host, port):\n    \"\"\"Basic demo of the monitoring capabilities.\"\"\"\n    # logging.basicConfig(level=logging.DEBUG)\n\n    loop = asyncio.get_event_loop()\n    stl = AsyncSatel(host,\n                     port,\n                     loop,\n                     [1, 2, 3, 4, 5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 18, 19,\n                      20, 21, 22, 23, 25, 26, 27, 28, 29, 30],\n                     [8, 9, 10]\n                     )\n\n    loop.run_until_complete(stl.connect())\n    loop.create_task(stl.arm(\"3333\", 1))\n    loop.create_task(stl.disarm(\"3333\"))\n    loop.create_task(stl.keep_alive())\n    loop.create_task(stl.monitor_status())\n\n    loop.run_forever()\n    loop.close()", "language": "python", "code": "def demo(host, port):\n    \"\"\"Basic demo of the monitoring capabilities.\"\"\"\n    # logging.basicConfig(level=logging.DEBUG)\n\n    loop = asyncio.get_event_loop()\n    stl = AsyncSatel(host,\n                     port,\n                     loop,\n                     [1, 2, 3, 4, 5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 18, 19,\n                      20, 21, 22, 23, 25, 26, 27, 28, 29, 30],\n                     [8, 9, 10]\n                     )\n\n    loop.run_until_complete(stl.connect())\n    loop.create_task(stl.arm(\"3333\", 1))\n    loop.create_task(stl.disarm(\"3333\"))\n    loop.create_task(stl.keep_alive())\n    loop.create_task(stl.monitor_status())\n\n    loop.run_forever()\n    loop.close()", "code_tokens": ["def", "demo", "(", "host", ",", "port", ")", ":", "# logging.basicConfig(level=logging.DEBUG)", "loop", "=", "asyncio", ".", "get_event_loop", "(", ")", "stl", "=", "AsyncSatel", "(", "host", ",", "port", ",", "loop", ",", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", ",", "7", ",", "8", ",", "12", ",", "13", ",", "14", ",", "15", ",", "16", ",", "17", ",", "18", ",", "19", ",", "20", ",", "21", ",", "22", ",", "23", ",", "25", ",", "26", ",", "27", ",", "28", ",", "29", ",", "30", "]", ",", "[", "8", ",", "9", ",", "10", "]", ")", "loop", ".", "run_until_complete", "(", "stl", ".", "connect", "(", ")", ")", "loop", ".", "create_task", "(", "stl", ".", "arm", "(", "\"3333\"", ",", "1", ")", ")", "loop", ".", "create_task", "(", "stl", ".", "disarm", "(", "\"3333\"", ")", ")", "loop", ".", "create_task", "(", "stl", ".", "keep_alive", "(", ")", ")", "loop", ".", "create_task", "(", "stl", ".", "monitor_status", "(", ")", ")", "loop", ".", "run_forever", "(", ")", "loop", ".", "close", "(", ")"], "docstring": "Basic demo of the monitoring capabilities.", "docstring_tokens": ["Basic", "demo", "of", "the", "monitoring", "capabilities", "."], "sha": "3b6d2020d1e10dc5aa40f30ee4ecc0f3a053eb3c", "url": "https://github.com/c-soft/satel_integra/blob/3b6d2020d1e10dc5aa40f30ee4ecc0f3a053eb3c/satel_integra/satel_integra.py#L453-L473", "partition": "test"}
{"repo": "oscarbranson/latools", "path": "latools/helpers/config.py", "func_name": "read_latoolscfg", "original_string": "def read_latoolscfg():\n    \"\"\"\n    Reads configuration, returns a ConfigParser object.\n\n    Distinct from read_configuration, which returns a dict.\n    \"\"\"\n    config_file = pkgrs.resource_filename('latools', 'latools.cfg')\n    cf = configparser.ConfigParser()\n    cf.read(config_file)\n    return config_file, cf", "language": "python", "code": "def read_latoolscfg():\n    \"\"\"\n    Reads configuration, returns a ConfigParser object.\n\n    Distinct from read_configuration, which returns a dict.\n    \"\"\"\n    config_file = pkgrs.resource_filename('latools', 'latools.cfg')\n    cf = configparser.ConfigParser()\n    cf.read(config_file)\n    return config_file, cf", "code_tokens": ["def", "read_latoolscfg", "(", ")", ":", "config_file", "=", "pkgrs", ".", "resource_filename", "(", "'latools'", ",", "'latools.cfg'", ")", "cf", "=", "configparser", ".", "ConfigParser", "(", ")", "cf", ".", "read", "(", "config_file", ")", "return", "config_file", ",", "cf"], "docstring": "Reads configuration, returns a ConfigParser object.\n\n    Distinct from read_configuration, which returns a dict.", "docstring_tokens": ["Reads", "configuration", "returns", "a", "ConfigParser", "object", "."], "sha": "cd25a650cfee318152f234d992708511f7047fbe", "url": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/helpers/config.py#L30-L39", "partition": "test"}
{"repo": "dogoncouch/lightcli", "path": "lightcli.py", "func_name": "outfile_input", "original_string": "def outfile_input(extension=None):\n    \"\"\"Get an output file name as input\"\"\"\n    \n    fileok = False\n    \n    while not fileok:\n        filename = string_input('File name? ')\n        if extension:\n            if not filename.endswith(extension):\n                if extension.startswith('.'):\n                    filename = filename + extension\n                else:\n                    filename = filename + '.' + extension\n        if os.path.isfile(filename):\n            choice = choice_input(prompt=filename + \\\n                    ' already exists. Overwrite?',\n                    options=['y', 'n'])\n            if choice == 'y':\n                try:\n                    nowtime = time.time()\n                    with open(filename, 'a') as f:\n                        os.utime(filename, (nowtime, nowtime))\n                    fileok = True\n                except IOError:\n                    print('Write permission denied on ' + filename + \\\n                            '. Try again.')\n                except PermissionError:\n                    print('Write permission denied on ' + filename + \\\n                            '. Try again.')\n                except FileNotFoundError:\n                    print(filename + ': directory not found. Try again.')\n\n        else:\n            choice = choice_input(\n                    prompt=filename + ' does not exist. Create it?',\n                    options=['y', 'n'])\n            if choice == 'y':\n                try:\n                    nowtime = time.time()\n                    with open(filename, 'w') as f:\n                        os.utime(filename, (nowtime, nowtime))\n                    fileok = True\n                except IOError:\n                    print('Write permission denied on ' + filename + \\\n                            '. Try again.')\n                except PermissionError:\n                    print('Write permission denied on ' + filename + \\\n                            '. Try again.')\n                except FileNotFoundError:\n                    print(filename + ': directory not found. Try again.')\n\n    return filename", "language": "python", "code": "def outfile_input(extension=None):\n    \"\"\"Get an output file name as input\"\"\"\n    \n    fileok = False\n    \n    while not fileok:\n        filename = string_input('File name? ')\n        if extension:\n            if not filename.endswith(extension):\n                if extension.startswith('.'):\n                    filename = filename + extension\n                else:\n                    filename = filename + '.' + extension\n        if os.path.isfile(filename):\n            choice = choice_input(prompt=filename + \\\n                    ' already exists. Overwrite?',\n                    options=['y', 'n'])\n            if choice == 'y':\n                try:\n                    nowtime = time.time()\n                    with open(filename, 'a') as f:\n                        os.utime(filename, (nowtime, nowtime))\n                    fileok = True\n                except IOError:\n                    print('Write permission denied on ' + filename + \\\n                            '. Try again.')\n                except PermissionError:\n                    print('Write permission denied on ' + filename + \\\n                            '. Try again.')\n                except FileNotFoundError:\n                    print(filename + ': directory not found. Try again.')\n\n        else:\n            choice = choice_input(\n                    prompt=filename + ' does not exist. Create it?',\n                    options=['y', 'n'])\n            if choice == 'y':\n                try:\n                    nowtime = time.time()\n                    with open(filename, 'w') as f:\n                        os.utime(filename, (nowtime, nowtime))\n                    fileok = True\n                except IOError:\n                    print('Write permission denied on ' + filename + \\\n                            '. Try again.')\n                except PermissionError:\n                    print('Write permission denied on ' + filename + \\\n                            '. Try again.')\n                except FileNotFoundError:\n                    print(filename + ': directory not found. Try again.')\n\n    return filename", "code_tokens": ["def", "outfile_input", "(", "extension", "=", "None", ")", ":", "fileok", "=", "False", "while", "not", "fileok", ":", "filename", "=", "string_input", "(", "'File name? '", ")", "if", "extension", ":", "if", "not", "filename", ".", "endswith", "(", "extension", ")", ":", "if", "extension", ".", "startswith", "(", "'.'", ")", ":", "filename", "=", "filename", "+", "extension", "else", ":", "filename", "=", "filename", "+", "'.'", "+", "extension", "if", "os", ".", "path", ".", "isfile", "(", "filename", ")", ":", "choice", "=", "choice_input", "(", "prompt", "=", "filename", "+", "' already exists. Overwrite?'", ",", "options", "=", "[", "'y'", ",", "'n'", "]", ")", "if", "choice", "==", "'y'", ":", "try", ":", "nowtime", "=", "time", ".", "time", "(", ")", "with", "open", "(", "filename", ",", "'a'", ")", "as", "f", ":", "os", ".", "utime", "(", "filename", ",", "(", "nowtime", ",", "nowtime", ")", ")", "fileok", "=", "True", "except", "IOError", ":", "print", "(", "'Write permission denied on '", "+", "filename", "+", "'. Try again.'", ")", "except", "PermissionError", ":", "print", "(", "'Write permission denied on '", "+", "filename", "+", "'. Try again.'", ")", "except", "FileNotFoundError", ":", "print", "(", "filename", "+", "': directory not found. Try again.'", ")", "else", ":", "choice", "=", "choice_input", "(", "prompt", "=", "filename", "+", "' does not exist. Create it?'", ",", "options", "=", "[", "'y'", ",", "'n'", "]", ")", "if", "choice", "==", "'y'", ":", "try", ":", "nowtime", "=", "time", ".", "time", "(", ")", "with", "open", "(", "filename", ",", "'w'", ")", "as", "f", ":", "os", ".", "utime", "(", "filename", ",", "(", "nowtime", ",", "nowtime", ")", ")", "fileok", "=", "True", "except", "IOError", ":", "print", "(", "'Write permission denied on '", "+", "filename", "+", "'. Try again.'", ")", "except", "PermissionError", ":", "print", "(", "'Write permission denied on '", "+", "filename", "+", "'. Try again.'", ")", "except", "FileNotFoundError", ":", "print", "(", "filename", "+", "': directory not found. Try again.'", ")", "return", "filename"], "docstring": "Get an output file name as input", "docstring_tokens": ["Get", "an", "output", "file", "name", "as", "input"], "sha": "e63093dfc4f983ec9c9571ff186bf114c1f782c3", "url": "https://github.com/dogoncouch/lightcli/blob/e63093dfc4f983ec9c9571ff186bf114c1f782c3/lightcli.py#L158-L209", "partition": "test"}
{"repo": "ncolony/ncolony", "path": "ncolony/service.py", "func_name": "makeService", "original_string": "def makeService(opt):\n    \"\"\"Return a service based on parsed command-line options\n\n    :param opt: dict-like object. Relevant keys are config, messages,\n                pid, frequency, threshold, killtime, minrestartdelay\n                and maxrestartdelay\n    :returns: service, {twisted.application.interfaces.IService}\n    \"\"\"\n    ret = get(config=opt['config'], messages=opt['messages'],\n              pidDir=opt['pid'], freq=opt['frequency'])\n    pm = ret.getServiceNamed(\"procmon\")\n    pm.threshold = opt[\"threshold\"]\n    pm.killTime = opt[\"killtime\"]\n    pm.minRestartDelay = opt[\"minrestartdelay\"]\n    pm.maxRestartDelay = opt[\"maxrestartdelay\"]\n    return ret", "language": "python", "code": "def makeService(opt):\n    \"\"\"Return a service based on parsed command-line options\n\n    :param opt: dict-like object. Relevant keys are config, messages,\n                pid, frequency, threshold, killtime, minrestartdelay\n                and maxrestartdelay\n    :returns: service, {twisted.application.interfaces.IService}\n    \"\"\"\n    ret = get(config=opt['config'], messages=opt['messages'],\n              pidDir=opt['pid'], freq=opt['frequency'])\n    pm = ret.getServiceNamed(\"procmon\")\n    pm.threshold = opt[\"threshold\"]\n    pm.killTime = opt[\"killtime\"]\n    pm.minRestartDelay = opt[\"minrestartdelay\"]\n    pm.maxRestartDelay = opt[\"maxrestartdelay\"]\n    return ret", "code_tokens": ["def", "makeService", "(", "opt", ")", ":", "ret", "=", "get", "(", "config", "=", "opt", "[", "'config'", "]", ",", "messages", "=", "opt", "[", "'messages'", "]", ",", "pidDir", "=", "opt", "[", "'pid'", "]", ",", "freq", "=", "opt", "[", "'frequency'", "]", ")", "pm", "=", "ret", ".", "getServiceNamed", "(", "\"procmon\"", ")", "pm", ".", "threshold", "=", "opt", "[", "\"threshold\"", "]", "pm", ".", "killTime", "=", "opt", "[", "\"killtime\"", "]", "pm", ".", "minRestartDelay", "=", "opt", "[", "\"minrestartdelay\"", "]", "pm", ".", "maxRestartDelay", "=", "opt", "[", "\"maxrestartdelay\"", "]", "return", "ret"], "docstring": "Return a service based on parsed command-line options\n\n    :param opt: dict-like object. Relevant keys are config, messages,\n                pid, frequency, threshold, killtime, minrestartdelay\n                and maxrestartdelay\n    :returns: service, {twisted.application.interfaces.IService}", "docstring_tokens": ["Return", "a", "service", "based", "on", "parsed", "command", "-", "line", "options"], "sha": "6ac71bda1de6706fb34244ae4972e36db5f062d3", "url": "https://github.com/ncolony/ncolony/blob/6ac71bda1de6706fb34244ae4972e36db5f062d3/ncolony/service.py#L117-L132", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/core/harmonic.py", "func_name": "harmonics_1d", "original_string": "def harmonics_1d(harmonic_out, x, freqs, h_range, kind='linear',\n                 fill_value=0, axis=0):\n    '''Populate a harmonic tensor from a time-frequency representation.\n\n    Parameters\n    ----------\n    harmonic_out : np.ndarray, shape=(len(h_range), X.shape)\n        The output array to store harmonics\n\n    X : np.ndarray\n        The input energy\n\n    freqs : np.ndarray, shape=(x.shape[axis])\n        The frequency values corresponding to x's elements along the\n        chosen axis.\n\n    h_range : list-like, non-negative\n        Harmonics to compute.  The first harmonic (1) corresponds to `x`\n        itself.\n        Values less than one (e.g., 1/2) correspond to sub-harmonics.\n\n    kind : str\n        Interpolation type.  See `scipy.interpolate.interp1d`.\n\n    fill_value : float\n        The value to fill when extrapolating beyond the observed\n        frequency range.\n\n    axis : int\n        The axis along which to compute harmonics\n\n    See Also\n    --------\n    harmonics\n    scipy.interpolate.interp1d\n\n\n    Examples\n    --------\n    Estimate the harmonics of a time-averaged tempogram\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      duration=15, offset=30)\n    >>> # Compute the time-varying tempogram and average over time\n    >>> tempi = np.mean(librosa.feature.tempogram(y=y, sr=sr), axis=1)\n    >>> # We'll measure the first five harmonics\n    >>> h_range = [1, 2, 3, 4, 5]\n    >>> f_tempo = librosa.tempo_frequencies(len(tempi), sr=sr)\n    >>> # Build the harmonic tensor\n    >>> t_harmonics = librosa.interp_harmonics(tempi, f_tempo, h_range)\n    >>> print(t_harmonics.shape)\n    (5, 384)\n\n    >>> # And plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure()\n    >>> librosa.display.specshow(t_harmonics, x_axis='tempo', sr=sr)\n    >>> plt.yticks(0.5 + np.arange(len(h_range)),\n    ...            ['{:.3g}'.format(_) for _ in h_range])\n    >>> plt.ylabel('Harmonic')\n    >>> plt.xlabel('Tempo (BPM)')\n    >>> plt.tight_layout()\n\n    We can also compute frequency harmonics for spectrograms.\n    To calculate subharmonic energy, use values < 1.\n\n    >>> h_range = [1./3, 1./2, 1, 2, 3, 4]\n    >>> S = np.abs(librosa.stft(y))\n    >>> fft_freqs = librosa.fft_frequencies(sr=sr)\n    >>> S_harm = librosa.interp_harmonics(S, fft_freqs, h_range, axis=0)\n    >>> print(S_harm.shape)\n    (6, 1025, 646)\n\n    >>> plt.figure()\n    >>> for i, _sh in enumerate(S_harm, 1):\n    ...     plt.subplot(3,2,i)\n    ...     librosa.display.specshow(librosa.amplitude_to_db(_sh,\n    ...                                                      ref=S.max()),\n    ...                              sr=sr, y_axis='log')\n    ...     plt.title('h={:.3g}'.format(h_range[i-1]))\n    ...     plt.yticks([])\n    >>> plt.tight_layout()\n    '''\n\n    # Note: this only works for fixed-grid, 1d interpolation\n    f_interp = scipy.interpolate.interp1d(freqs, x,\n                                          kind=kind,\n                                          axis=axis,\n                                          copy=False,\n                                          bounds_error=False,\n                                          fill_value=fill_value)\n\n    idx_out = [slice(None)] * harmonic_out.ndim\n\n    # Compute the output index of the interpolated values\n    interp_axis = 1 + (axis % x.ndim)\n\n    # Iterate over the harmonics range\n    for h_index, harmonic in enumerate(h_range):\n        idx_out[0] = h_index\n\n        # Iterate over frequencies\n        for f_index, frequency in enumerate(freqs):\n            # Offset the output axis by 1 to account for the harmonic index\n            idx_out[interp_axis] = f_index\n\n            # Estimate the harmonic energy at this frequency across time\n            harmonic_out[tuple(idx_out)] = f_interp(harmonic * frequency)", "language": "python", "code": "def harmonics_1d(harmonic_out, x, freqs, h_range, kind='linear',\n                 fill_value=0, axis=0):\n    '''Populate a harmonic tensor from a time-frequency representation.\n\n    Parameters\n    ----------\n    harmonic_out : np.ndarray, shape=(len(h_range), X.shape)\n        The output array to store harmonics\n\n    X : np.ndarray\n        The input energy\n\n    freqs : np.ndarray, shape=(x.shape[axis])\n        The frequency values corresponding to x's elements along the\n        chosen axis.\n\n    h_range : list-like, non-negative\n        Harmonics to compute.  The first harmonic (1) corresponds to `x`\n        itself.\n        Values less than one (e.g., 1/2) correspond to sub-harmonics.\n\n    kind : str\n        Interpolation type.  See `scipy.interpolate.interp1d`.\n\n    fill_value : float\n        The value to fill when extrapolating beyond the observed\n        frequency range.\n\n    axis : int\n        The axis along which to compute harmonics\n\n    See Also\n    --------\n    harmonics\n    scipy.interpolate.interp1d\n\n\n    Examples\n    --------\n    Estimate the harmonics of a time-averaged tempogram\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      duration=15, offset=30)\n    >>> # Compute the time-varying tempogram and average over time\n    >>> tempi = np.mean(librosa.feature.tempogram(y=y, sr=sr), axis=1)\n    >>> # We'll measure the first five harmonics\n    >>> h_range = [1, 2, 3, 4, 5]\n    >>> f_tempo = librosa.tempo_frequencies(len(tempi), sr=sr)\n    >>> # Build the harmonic tensor\n    >>> t_harmonics = librosa.interp_harmonics(tempi, f_tempo, h_range)\n    >>> print(t_harmonics.shape)\n    (5, 384)\n\n    >>> # And plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure()\n    >>> librosa.display.specshow(t_harmonics, x_axis='tempo', sr=sr)\n    >>> plt.yticks(0.5 + np.arange(len(h_range)),\n    ...            ['{:.3g}'.format(_) for _ in h_range])\n    >>> plt.ylabel('Harmonic')\n    >>> plt.xlabel('Tempo (BPM)')\n    >>> plt.tight_layout()\n\n    We can also compute frequency harmonics for spectrograms.\n    To calculate subharmonic energy, use values < 1.\n\n    >>> h_range = [1./3, 1./2, 1, 2, 3, 4]\n    >>> S = np.abs(librosa.stft(y))\n    >>> fft_freqs = librosa.fft_frequencies(sr=sr)\n    >>> S_harm = librosa.interp_harmonics(S, fft_freqs, h_range, axis=0)\n    >>> print(S_harm.shape)\n    (6, 1025, 646)\n\n    >>> plt.figure()\n    >>> for i, _sh in enumerate(S_harm, 1):\n    ...     plt.subplot(3,2,i)\n    ...     librosa.display.specshow(librosa.amplitude_to_db(_sh,\n    ...                                                      ref=S.max()),\n    ...                              sr=sr, y_axis='log')\n    ...     plt.title('h={:.3g}'.format(h_range[i-1]))\n    ...     plt.yticks([])\n    >>> plt.tight_layout()\n    '''\n\n    # Note: this only works for fixed-grid, 1d interpolation\n    f_interp = scipy.interpolate.interp1d(freqs, x,\n                                          kind=kind,\n                                          axis=axis,\n                                          copy=False,\n                                          bounds_error=False,\n                                          fill_value=fill_value)\n\n    idx_out = [slice(None)] * harmonic_out.ndim\n\n    # Compute the output index of the interpolated values\n    interp_axis = 1 + (axis % x.ndim)\n\n    # Iterate over the harmonics range\n    for h_index, harmonic in enumerate(h_range):\n        idx_out[0] = h_index\n\n        # Iterate over frequencies\n        for f_index, frequency in enumerate(freqs):\n            # Offset the output axis by 1 to account for the harmonic index\n            idx_out[interp_axis] = f_index\n\n            # Estimate the harmonic energy at this frequency across time\n            harmonic_out[tuple(idx_out)] = f_interp(harmonic * frequency)", "code_tokens": ["def", "harmonics_1d", "(", "harmonic_out", ",", "x", ",", "freqs", ",", "h_range", ",", "kind", "=", "'linear'", ",", "fill_value", "=", "0", ",", "axis", "=", "0", ")", ":", "# Note: this only works for fixed-grid, 1d interpolation", "f_interp", "=", "scipy", ".", "interpolate", ".", "interp1d", "(", "freqs", ",", "x", ",", "kind", "=", "kind", ",", "axis", "=", "axis", ",", "copy", "=", "False", ",", "bounds_error", "=", "False", ",", "fill_value", "=", "fill_value", ")", "idx_out", "=", "[", "slice", "(", "None", ")", "]", "*", "harmonic_out", ".", "ndim", "# Compute the output index of the interpolated values", "interp_axis", "=", "1", "+", "(", "axis", "%", "x", ".", "ndim", ")", "# Iterate over the harmonics range", "for", "h_index", ",", "harmonic", "in", "enumerate", "(", "h_range", ")", ":", "idx_out", "[", "0", "]", "=", "h_index", "# Iterate over frequencies", "for", "f_index", ",", "frequency", "in", "enumerate", "(", "freqs", ")", ":", "# Offset the output axis by 1 to account for the harmonic index", "idx_out", "[", "interp_axis", "]", "=", "f_index", "# Estimate the harmonic energy at this frequency across time", "harmonic_out", "[", "tuple", "(", "idx_out", ")", "]", "=", "f_interp", "(", "harmonic", "*", "frequency", ")"], "docstring": "Populate a harmonic tensor from a time-frequency representation.\n\n    Parameters\n    ----------\n    harmonic_out : np.ndarray, shape=(len(h_range), X.shape)\n        The output array to store harmonics\n\n    X : np.ndarray\n        The input energy\n\n    freqs : np.ndarray, shape=(x.shape[axis])\n        The frequency values corresponding to x's elements along the\n        chosen axis.\n\n    h_range : list-like, non-negative\n        Harmonics to compute.  The first harmonic (1) corresponds to `x`\n        itself.\n        Values less than one (e.g., 1/2) correspond to sub-harmonics.\n\n    kind : str\n        Interpolation type.  See `scipy.interpolate.interp1d`.\n\n    fill_value : float\n        The value to fill when extrapolating beyond the observed\n        frequency range.\n\n    axis : int\n        The axis along which to compute harmonics\n\n    See Also\n    --------\n    harmonics\n    scipy.interpolate.interp1d\n\n\n    Examples\n    --------\n    Estimate the harmonics of a time-averaged tempogram\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      duration=15, offset=30)\n    >>> # Compute the time-varying tempogram and average over time\n    >>> tempi = np.mean(librosa.feature.tempogram(y=y, sr=sr), axis=1)\n    >>> # We'll measure the first five harmonics\n    >>> h_range = [1, 2, 3, 4, 5]\n    >>> f_tempo = librosa.tempo_frequencies(len(tempi), sr=sr)\n    >>> # Build the harmonic tensor\n    >>> t_harmonics = librosa.interp_harmonics(tempi, f_tempo, h_range)\n    >>> print(t_harmonics.shape)\n    (5, 384)\n\n    >>> # And plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure()\n    >>> librosa.display.specshow(t_harmonics, x_axis='tempo', sr=sr)\n    >>> plt.yticks(0.5 + np.arange(len(h_range)),\n    ...            ['{:.3g}'.format(_) for _ in h_range])\n    >>> plt.ylabel('Harmonic')\n    >>> plt.xlabel('Tempo (BPM)')\n    >>> plt.tight_layout()\n\n    We can also compute frequency harmonics for spectrograms.\n    To calculate subharmonic energy, use values < 1.\n\n    >>> h_range = [1./3, 1./2, 1, 2, 3, 4]\n    >>> S = np.abs(librosa.stft(y))\n    >>> fft_freqs = librosa.fft_frequencies(sr=sr)\n    >>> S_harm = librosa.interp_harmonics(S, fft_freqs, h_range, axis=0)\n    >>> print(S_harm.shape)\n    (6, 1025, 646)\n\n    >>> plt.figure()\n    >>> for i, _sh in enumerate(S_harm, 1):\n    ...     plt.subplot(3,2,i)\n    ...     librosa.display.specshow(librosa.amplitude_to_db(_sh,\n    ...                                                      ref=S.max()),\n    ...                              sr=sr, y_axis='log')\n    ...     plt.title('h={:.3g}'.format(h_range[i-1]))\n    ...     plt.yticks([])\n    >>> plt.tight_layout()", "docstring_tokens": ["Populate", "a", "harmonic", "tensor", "from", "a", "time", "-", "frequency", "representation", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/core/harmonic.py#L221-L328", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/core/attributes.py", "func_name": "Digest.read", "original_string": "def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the Digest object and decode it into its\n        constituent parts.\n\n        Args:\n            istream (Stream): A data stream containing encoded object data,\n                supporting a read method; usually a BytearrayStream object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"\n        super(Digest, self).read(istream, kmip_version=kmip_version)\n        tstream = BytearrayStream(istream.read(self.length))\n\n        self.hashing_algorithm.read(tstream, kmip_version=kmip_version)\n        self.digest_value.read(tstream, kmip_version=kmip_version)\n        self.key_format_type.read(tstream, kmip_version=kmip_version)\n\n        self.is_oversized(tstream)\n        self.validate()", "language": "python", "code": "def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the Digest object and decode it into its\n        constituent parts.\n\n        Args:\n            istream (Stream): A data stream containing encoded object data,\n                supporting a read method; usually a BytearrayStream object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"\n        super(Digest, self).read(istream, kmip_version=kmip_version)\n        tstream = BytearrayStream(istream.read(self.length))\n\n        self.hashing_algorithm.read(tstream, kmip_version=kmip_version)\n        self.digest_value.read(tstream, kmip_version=kmip_version)\n        self.key_format_type.read(tstream, kmip_version=kmip_version)\n\n        self.is_oversized(tstream)\n        self.validate()", "code_tokens": ["def", "read", "(", "self", ",", "istream", ",", "kmip_version", "=", "enums", ".", "KMIPVersion", ".", "KMIP_1_0", ")", ":", "super", "(", "Digest", ",", "self", ")", ".", "read", "(", "istream", ",", "kmip_version", "=", "kmip_version", ")", "tstream", "=", "BytearrayStream", "(", "istream", ".", "read", "(", "self", ".", "length", ")", ")", "self", ".", "hashing_algorithm", ".", "read", "(", "tstream", ",", "kmip_version", "=", "kmip_version", ")", "self", ".", "digest_value", ".", "read", "(", "tstream", ",", "kmip_version", "=", "kmip_version", ")", "self", ".", "key_format_type", ".", "read", "(", "tstream", ",", "kmip_version", "=", "kmip_version", ")", "self", ".", "is_oversized", "(", "tstream", ")", "self", ".", "validate", "(", ")"], "docstring": "Read the data encoding the Digest object and decode it into its\n        constituent parts.\n\n        Args:\n            istream (Stream): A data stream containing encoded object data,\n                supporting a read method; usually a BytearrayStream object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.", "docstring_tokens": ["Read", "the", "data", "encoding", "the", "Digest", "object", "and", "decode", "it", "into", "its", "constituent", "parts", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/core/attributes.py#L899-L919", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/brian2/network.py", "func_name": "NetworkManager.add_parameters", "original_string": "def add_parameters(self, traj):\n        \"\"\"Adds parameters for a network simulation.\n\n        Calls :func:`~pypet.brian2.network.NetworkComponent.add_parameters` for all components,\n        analyser, and the network runner (in this order).\n\n        :param traj:  Trajectory container\n\n        \"\"\"\n        self._logger.info('Adding Parameters of Components')\n\n        for component in self.components:\n            component.add_parameters(traj)\n\n        if self.analysers:\n            self._logger.info('Adding Parameters of Analysers')\n\n            for analyser in self.analysers:\n                analyser.add_parameters(traj)\n\n        self._logger.info('Adding Parameters of Runner')\n\n        self.network_runner.add_parameters(traj)", "language": "python", "code": "def add_parameters(self, traj):\n        \"\"\"Adds parameters for a network simulation.\n\n        Calls :func:`~pypet.brian2.network.NetworkComponent.add_parameters` for all components,\n        analyser, and the network runner (in this order).\n\n        :param traj:  Trajectory container\n\n        \"\"\"\n        self._logger.info('Adding Parameters of Components')\n\n        for component in self.components:\n            component.add_parameters(traj)\n\n        if self.analysers:\n            self._logger.info('Adding Parameters of Analysers')\n\n            for analyser in self.analysers:\n                analyser.add_parameters(traj)\n\n        self._logger.info('Adding Parameters of Runner')\n\n        self.network_runner.add_parameters(traj)", "code_tokens": ["def", "add_parameters", "(", "self", ",", "traj", ")", ":", "self", ".", "_logger", ".", "info", "(", "'Adding Parameters of Components'", ")", "for", "component", "in", "self", ".", "components", ":", "component", ".", "add_parameters", "(", "traj", ")", "if", "self", ".", "analysers", ":", "self", ".", "_logger", ".", "info", "(", "'Adding Parameters of Analysers'", ")", "for", "analyser", "in", "self", ".", "analysers", ":", "analyser", ".", "add_parameters", "(", "traj", ")", "self", ".", "_logger", ".", "info", "(", "'Adding Parameters of Runner'", ")", "self", ".", "network_runner", ".", "add_parameters", "(", "traj", ")"], "docstring": "Adds parameters for a network simulation.\n\n        Calls :func:`~pypet.brian2.network.NetworkComponent.add_parameters` for all components,\n        analyser, and the network runner (in this order).\n\n        :param traj:  Trajectory container", "docstring_tokens": ["Adds", "parameters", "for", "a", "network", "simulation", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/brian2/network.py#L541-L563", "partition": "test"}
{"repo": "rwl/godot", "path": "godot/base_graph.py", "func_name": "BaseGraph.create", "original_string": "def create(self, prog=None, format=None):\n        \"\"\" Creates and returns a representation of the graph using the\n            Graphviz layout program given by 'prog', according to the given\n            format.\n\n            Writes the graph to a temporary dot file and processes it with\n            the program given by 'prog' (which defaults to 'dot'), reading\n            the output and returning it as a string if the operation is\n            successful. On failure None is returned.\n        \"\"\"\n        prog = self.program if prog is None else prog\n        format = self.format if format is None else format\n\n        # Make a temporary file ...\n        tmp_fd, tmp_name = tempfile.mkstemp()\n        os.close( tmp_fd )\n        # ... and save the graph to it.\n        dot_fd = file( tmp_name, \"w+b\" )\n        self.save_dot( dot_fd )\n        dot_fd.close()\n\n        # Get the temporary file directory name.\n        tmp_dir = os.path.dirname( tmp_name )\n\n        # TODO: Shape image files (See PyDot). Important.\n\n        # Process the file using the layout program, specifying the format.\n        p = subprocess.Popen(\n            ( self.programs[ prog ], '-T'+format, tmp_name ),\n            cwd=tmp_dir,\n            stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\n        stderr = p.stderr\n        stdout = p.stdout\n\n        # Make sense of the standard output form the process.\n        stdout_output = list()\n        while True:\n            data = stdout.read()\n            if not data:\n                break\n            stdout_output.append(data)\n        stdout.close()\n\n        if stdout_output:\n            stdout_output = ''.join(stdout_output)\n\n        # Similarly so for any standard error.\n        if not stderr.closed:\n            stderr_output = list()\n            while True:\n                data = stderr.read()\n                if not data:\n                    break\n                stderr_output.append(data)\n            stderr.close()\n\n            if stderr_output:\n                stderr_output = ''.join(stderr_output)\n\n        #pid, status = os.waitpid(p.pid, 0)\n        status = p.wait()\n\n        if status != 0 :\n            logger.error(\"Program terminated with status: %d. stderr \" \\\n                \"follows: %s\" % ( status, stderr_output ) )\n        elif stderr_output:\n            logger.error( \"%s\", stderr_output )\n\n        # TODO: Remove shape image files from the temporary directory.\n\n        # Remove the temporary file.\n        os.unlink(tmp_name)\n\n        return stdout_output", "language": "python", "code": "def create(self, prog=None, format=None):\n        \"\"\" Creates and returns a representation of the graph using the\n            Graphviz layout program given by 'prog', according to the given\n            format.\n\n            Writes the graph to a temporary dot file and processes it with\n            the program given by 'prog' (which defaults to 'dot'), reading\n            the output and returning it as a string if the operation is\n            successful. On failure None is returned.\n        \"\"\"\n        prog = self.program if prog is None else prog\n        format = self.format if format is None else format\n\n        # Make a temporary file ...\n        tmp_fd, tmp_name = tempfile.mkstemp()\n        os.close( tmp_fd )\n        # ... and save the graph to it.\n        dot_fd = file( tmp_name, \"w+b\" )\n        self.save_dot( dot_fd )\n        dot_fd.close()\n\n        # Get the temporary file directory name.\n        tmp_dir = os.path.dirname( tmp_name )\n\n        # TODO: Shape image files (See PyDot). Important.\n\n        # Process the file using the layout program, specifying the format.\n        p = subprocess.Popen(\n            ( self.programs[ prog ], '-T'+format, tmp_name ),\n            cwd=tmp_dir,\n            stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\n        stderr = p.stderr\n        stdout = p.stdout\n\n        # Make sense of the standard output form the process.\n        stdout_output = list()\n        while True:\n            data = stdout.read()\n            if not data:\n                break\n            stdout_output.append(data)\n        stdout.close()\n\n        if stdout_output:\n            stdout_output = ''.join(stdout_output)\n\n        # Similarly so for any standard error.\n        if not stderr.closed:\n            stderr_output = list()\n            while True:\n                data = stderr.read()\n                if not data:\n                    break\n                stderr_output.append(data)\n            stderr.close()\n\n            if stderr_output:\n                stderr_output = ''.join(stderr_output)\n\n        #pid, status = os.waitpid(p.pid, 0)\n        status = p.wait()\n\n        if status != 0 :\n            logger.error(\"Program terminated with status: %d. stderr \" \\\n                \"follows: %s\" % ( status, stderr_output ) )\n        elif stderr_output:\n            logger.error( \"%s\", stderr_output )\n\n        # TODO: Remove shape image files from the temporary directory.\n\n        # Remove the temporary file.\n        os.unlink(tmp_name)\n\n        return stdout_output", "code_tokens": ["def", "create", "(", "self", ",", "prog", "=", "None", ",", "format", "=", "None", ")", ":", "prog", "=", "self", ".", "program", "if", "prog", "is", "None", "else", "prog", "format", "=", "self", ".", "format", "if", "format", "is", "None", "else", "format", "# Make a temporary file ...", "tmp_fd", ",", "tmp_name", "=", "tempfile", ".", "mkstemp", "(", ")", "os", ".", "close", "(", "tmp_fd", ")", "# ... and save the graph to it.", "dot_fd", "=", "file", "(", "tmp_name", ",", "\"w+b\"", ")", "self", ".", "save_dot", "(", "dot_fd", ")", "dot_fd", ".", "close", "(", ")", "# Get the temporary file directory name.", "tmp_dir", "=", "os", ".", "path", ".", "dirname", "(", "tmp_name", ")", "# TODO: Shape image files (See PyDot). Important.", "# Process the file using the layout program, specifying the format.", "p", "=", "subprocess", ".", "Popen", "(", "(", "self", ".", "programs", "[", "prog", "]", ",", "'-T'", "+", "format", ",", "tmp_name", ")", ",", "cwd", "=", "tmp_dir", ",", "stderr", "=", "subprocess", ".", "PIPE", ",", "stdout", "=", "subprocess", ".", "PIPE", ")", "stderr", "=", "p", ".", "stderr", "stdout", "=", "p", ".", "stdout", "# Make sense of the standard output form the process.", "stdout_output", "=", "list", "(", ")", "while", "True", ":", "data", "=", "stdout", ".", "read", "(", ")", "if", "not", "data", ":", "break", "stdout_output", ".", "append", "(", "data", ")", "stdout", ".", "close", "(", ")", "if", "stdout_output", ":", "stdout_output", "=", "''", ".", "join", "(", "stdout_output", ")", "# Similarly so for any standard error.", "if", "not", "stderr", ".", "closed", ":", "stderr_output", "=", "list", "(", ")", "while", "True", ":", "data", "=", "stderr", ".", "read", "(", ")", "if", "not", "data", ":", "break", "stderr_output", ".", "append", "(", "data", ")", "stderr", ".", "close", "(", ")", "if", "stderr_output", ":", "stderr_output", "=", "''", ".", "join", "(", "stderr_output", ")", "#pid, status = os.waitpid(p.pid, 0)", "status", "=", "p", ".", "wait", "(", ")", "if", "status", "!=", "0", ":", "logger", ".", "error", "(", "\"Program terminated with status: %d. stderr \"", "\"follows: %s\"", "%", "(", "status", ",", "stderr_output", ")", ")", "elif", "stderr_output", ":", "logger", ".", "error", "(", "\"%s\"", ",", "stderr_output", ")", "# TODO: Remove shape image files from the temporary directory.", "# Remove the temporary file.", "os", ".", "unlink", "(", "tmp_name", ")", "return", "stdout_output"], "docstring": "Creates and returns a representation of the graph using the\n            Graphviz layout program given by 'prog', according to the given\n            format.\n\n            Writes the graph to a temporary dot file and processes it with\n            the program given by 'prog' (which defaults to 'dot'), reading\n            the output and returning it as a string if the operation is\n            successful. On failure None is returned.", "docstring_tokens": ["Creates", "and", "returns", "a", "representation", "of", "the", "graph", "using", "the", "Graphviz", "layout", "program", "given", "by", "prog", "according", "to", "the", "given", "format", "."], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/base_graph.py#L340-L414", "partition": "test"}
{"repo": "UCBerkeleySETI/blimpy", "path": "blimpy/file_wrapper.py", "func_name": "Reader._setup_selection_range", "original_string": "def _setup_selection_range(self, f_start=None, f_stop=None, t_start=None, t_stop=None, init=False):\n        \"\"\"Making sure the selection if time and frequency are within the file limits.\n\n        Args:\n            init (bool): If call during __init__\n        \"\"\"\n\n        # This avoids resetting values\n        if init is True:\n            if t_start is None:\n                t_start = self.t_begin\n            if t_stop is None:\n                t_stop = self.t_end\n            if f_start is None:\n                f_start = self.f_begin\n            if f_stop is None:\n                f_stop = self.f_end\n        else:\n            if f_start is None:\n                f_start = self.f_start\n            if f_stop is None:\n                f_stop = self.f_stop\n            if t_start is None:\n                t_start = self.t_start\n            if t_stop is None:\n                t_stop = self.t_stop\n\n        # By now, all values start/stop are populated.\n\n        if t_stop >= 0 and t_start >= 0 and t_stop < t_start:\n            t_stop, t_start = t_start,t_stop\n            logger.warning('Given t_stop < t_start, assuming reversed values.')\n        if f_stop and f_start and f_stop < f_start:\n            f_stop, f_start = f_start,f_stop\n            logger.warning('Given f_stop < f_start, assuming reversed values.')\n\n        if t_start >= self.t_begin and t_start < self.t_end:\n            self.t_start = int(t_start)\n        else:\n            if init is False or t_start != None:\n                logger.warning('Setting t_start = %f, since t_start not given or not valid.'%self.t_begin)\n            self.t_start = self.t_begin\n\n        if t_stop <= self.t_end  and t_stop > self.t_begin:\n            self.t_stop = int(t_stop)\n        else:\n            if init is False or t_stop:\n                logger.warning('Setting t_stop = %f, since t_stop not given or not valid.'%self.t_end)\n            self.t_stop = self.t_end\n\n        if f_start >= self.f_begin and f_start < self.f_end:\n            self.f_start = f_start\n        else:\n            if init is False or f_start:\n                logger.warning('Setting f_start = %f, since f_start not given or not valid.'%self.f_begin)\n            self.f_start = self.f_begin\n\n        if f_stop <= self.f_end and f_stop > self.f_begin:\n            self.f_stop = f_stop\n        else:\n            if init is False or f_stop:\n                logger.warning('Setting f_stop = %f, since f_stop not given or not valid.'%self.f_end)\n            self.f_stop = self.f_end\n\n        # Now we have setup bounds, we can calculate shape of selection\n        self.selection_shape = self._calc_selection_shape()", "language": "python", "code": "def _setup_selection_range(self, f_start=None, f_stop=None, t_start=None, t_stop=None, init=False):\n        \"\"\"Making sure the selection if time and frequency are within the file limits.\n\n        Args:\n            init (bool): If call during __init__\n        \"\"\"\n\n        # This avoids resetting values\n        if init is True:\n            if t_start is None:\n                t_start = self.t_begin\n            if t_stop is None:\n                t_stop = self.t_end\n            if f_start is None:\n                f_start = self.f_begin\n            if f_stop is None:\n                f_stop = self.f_end\n        else:\n            if f_start is None:\n                f_start = self.f_start\n            if f_stop is None:\n                f_stop = self.f_stop\n            if t_start is None:\n                t_start = self.t_start\n            if t_stop is None:\n                t_stop = self.t_stop\n\n        # By now, all values start/stop are populated.\n\n        if t_stop >= 0 and t_start >= 0 and t_stop < t_start:\n            t_stop, t_start = t_start,t_stop\n            logger.warning('Given t_stop < t_start, assuming reversed values.')\n        if f_stop and f_start and f_stop < f_start:\n            f_stop, f_start = f_start,f_stop\n            logger.warning('Given f_stop < f_start, assuming reversed values.')\n\n        if t_start >= self.t_begin and t_start < self.t_end:\n            self.t_start = int(t_start)\n        else:\n            if init is False or t_start != None:\n                logger.warning('Setting t_start = %f, since t_start not given or not valid.'%self.t_begin)\n            self.t_start = self.t_begin\n\n        if t_stop <= self.t_end  and t_stop > self.t_begin:\n            self.t_stop = int(t_stop)\n        else:\n            if init is False or t_stop:\n                logger.warning('Setting t_stop = %f, since t_stop not given or not valid.'%self.t_end)\n            self.t_stop = self.t_end\n\n        if f_start >= self.f_begin and f_start < self.f_end:\n            self.f_start = f_start\n        else:\n            if init is False or f_start:\n                logger.warning('Setting f_start = %f, since f_start not given or not valid.'%self.f_begin)\n            self.f_start = self.f_begin\n\n        if f_stop <= self.f_end and f_stop > self.f_begin:\n            self.f_stop = f_stop\n        else:\n            if init is False or f_stop:\n                logger.warning('Setting f_stop = %f, since f_stop not given or not valid.'%self.f_end)\n            self.f_stop = self.f_end\n\n        # Now we have setup bounds, we can calculate shape of selection\n        self.selection_shape = self._calc_selection_shape()", "code_tokens": ["def", "_setup_selection_range", "(", "self", ",", "f_start", "=", "None", ",", "f_stop", "=", "None", ",", "t_start", "=", "None", ",", "t_stop", "=", "None", ",", "init", "=", "False", ")", ":", "# This avoids resetting values", "if", "init", "is", "True", ":", "if", "t_start", "is", "None", ":", "t_start", "=", "self", ".", "t_begin", "if", "t_stop", "is", "None", ":", "t_stop", "=", "self", ".", "t_end", "if", "f_start", "is", "None", ":", "f_start", "=", "self", ".", "f_begin", "if", "f_stop", "is", "None", ":", "f_stop", "=", "self", ".", "f_end", "else", ":", "if", "f_start", "is", "None", ":", "f_start", "=", "self", ".", "f_start", "if", "f_stop", "is", "None", ":", "f_stop", "=", "self", ".", "f_stop", "if", "t_start", "is", "None", ":", "t_start", "=", "self", ".", "t_start", "if", "t_stop", "is", "None", ":", "t_stop", "=", "self", ".", "t_stop", "# By now, all values start/stop are populated.", "if", "t_stop", ">=", "0", "and", "t_start", ">=", "0", "and", "t_stop", "<", "t_start", ":", "t_stop", ",", "t_start", "=", "t_start", ",", "t_stop", "logger", ".", "warning", "(", "'Given t_stop < t_start, assuming reversed values.'", ")", "if", "f_stop", "and", "f_start", "and", "f_stop", "<", "f_start", ":", "f_stop", ",", "f_start", "=", "f_start", ",", "f_stop", "logger", ".", "warning", "(", "'Given f_stop < f_start, assuming reversed values.'", ")", "if", "t_start", ">=", "self", ".", "t_begin", "and", "t_start", "<", "self", ".", "t_end", ":", "self", ".", "t_start", "=", "int", "(", "t_start", ")", "else", ":", "if", "init", "is", "False", "or", "t_start", "!=", "None", ":", "logger", ".", "warning", "(", "'Setting t_start = %f, since t_start not given or not valid.'", "%", "self", ".", "t_begin", ")", "self", ".", "t_start", "=", "self", ".", "t_begin", "if", "t_stop", "<=", "self", ".", "t_end", "and", "t_stop", ">", "self", ".", "t_begin", ":", "self", ".", "t_stop", "=", "int", "(", "t_stop", ")", "else", ":", "if", "init", "is", "False", "or", "t_stop", ":", "logger", ".", "warning", "(", "'Setting t_stop = %f, since t_stop not given or not valid.'", "%", "self", ".", "t_end", ")", "self", ".", "t_stop", "=", "self", ".", "t_end", "if", "f_start", ">=", "self", ".", "f_begin", "and", "f_start", "<", "self", ".", "f_end", ":", "self", ".", "f_start", "=", "f_start", "else", ":", "if", "init", "is", "False", "or", "f_start", ":", "logger", ".", "warning", "(", "'Setting f_start = %f, since f_start not given or not valid.'", "%", "self", ".", "f_begin", ")", "self", ".", "f_start", "=", "self", ".", "f_begin", "if", "f_stop", "<=", "self", ".", "f_end", "and", "f_stop", ">", "self", ".", "f_begin", ":", "self", ".", "f_stop", "=", "f_stop", "else", ":", "if", "init", "is", "False", "or", "f_stop", ":", "logger", ".", "warning", "(", "'Setting f_stop = %f, since f_stop not given or not valid.'", "%", "self", ".", "f_end", ")", "self", ".", "f_stop", "=", "self", ".", "f_end", "# Now we have setup bounds, we can calculate shape of selection", "self", ".", "selection_shape", "=", "self", ".", "_calc_selection_shape", "(", ")"], "docstring": "Making sure the selection if time and frequency are within the file limits.\n\n        Args:\n            init (bool): If call during __init__", "docstring_tokens": ["Making", "sure", "the", "selection", "if", "time", "and", "frequency", "are", "within", "the", "file", "limits", "."], "sha": "b8822d3e3e911944370d84371a91fa0c29e9772e", "url": "https://github.com/UCBerkeleySETI/blimpy/blob/b8822d3e3e911944370d84371a91fa0c29e9772e/blimpy/file_wrapper.py#L45-L110", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/runtime.py", "func_name": "first", "original_string": "def first(o):\n    \"\"\"If o is a ISeq, return the first element from o. If o is None, return\n    None. Otherwise, coerces o to a Seq and returns the first.\"\"\"\n    if o is None:\n        return None\n    if isinstance(o, ISeq):\n        return o.first\n    s = to_seq(o)\n    if s is None:\n        return None\n    return s.first", "language": "python", "code": "def first(o):\n    \"\"\"If o is a ISeq, return the first element from o. If o is None, return\n    None. Otherwise, coerces o to a Seq and returns the first.\"\"\"\n    if o is None:\n        return None\n    if isinstance(o, ISeq):\n        return o.first\n    s = to_seq(o)\n    if s is None:\n        return None\n    return s.first", "code_tokens": ["def", "first", "(", "o", ")", ":", "if", "o", "is", "None", ":", "return", "None", "if", "isinstance", "(", "o", ",", "ISeq", ")", ":", "return", "o", ".", "first", "s", "=", "to_seq", "(", "o", ")", "if", "s", "is", "None", ":", "return", "None", "return", "s", ".", "first"], "docstring": "If o is a ISeq, return the first element from o. If o is None, return\n    None. Otherwise, coerces o to a Seq and returns the first.", "docstring_tokens": ["If", "o", "is", "a", "ISeq", "return", "the", "first", "element", "from", "o", ".", "If", "o", "is", "None", "return", "None", ".", "Otherwise", "coerces", "o", "to", "a", "Seq", "and", "returns", "the", "first", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/runtime.py#L699-L709", "partition": "test"}
{"repo": "opencast/pyCA", "path": "pyca/config.py", "func_name": "check", "original_string": "def check():\n    '''Check configuration for sanity.\n    '''\n    if config('server')['insecure']:\n        logger.warning('HTTPS CHECKS ARE TURNED OFF. A SECURE CONNECTION IS '\n                       'NOT GUARANTEED')\n    if config('server')['certificate']:\n        # Ensure certificate exists and is readable\n        open(config('server')['certificate'], 'rb').close()\n    if config('agent')['backup_mode']:\n        logger.info('Agent runs in backup mode. No data will be sent to '\n                    'Opencast')", "language": "python", "code": "def check():\n    '''Check configuration for sanity.\n    '''\n    if config('server')['insecure']:\n        logger.warning('HTTPS CHECKS ARE TURNED OFF. A SECURE CONNECTION IS '\n                       'NOT GUARANTEED')\n    if config('server')['certificate']:\n        # Ensure certificate exists and is readable\n        open(config('server')['certificate'], 'rb').close()\n    if config('agent')['backup_mode']:\n        logger.info('Agent runs in backup mode. No data will be sent to '\n                    'Opencast')", "code_tokens": ["def", "check", "(", ")", ":", "if", "config", "(", "'server'", ")", "[", "'insecure'", "]", ":", "logger", ".", "warning", "(", "'HTTPS CHECKS ARE TURNED OFF. A SECURE CONNECTION IS '", "'NOT GUARANTEED'", ")", "if", "config", "(", "'server'", ")", "[", "'certificate'", "]", ":", "# Ensure certificate exists and is readable", "open", "(", "config", "(", "'server'", ")", "[", "'certificate'", "]", ",", "'rb'", ")", ".", "close", "(", ")", "if", "config", "(", "'agent'", ")", "[", "'backup_mode'", "]", ":", "logger", ".", "info", "(", "'Agent runs in backup mode. No data will be sent to '", "'Opencast'", ")"], "docstring": "Check configuration for sanity.", "docstring_tokens": ["Check", "configuration", "for", "sanity", "."], "sha": "c89b168d4780d157e1b3f7676628c1b131956a88", "url": "https://github.com/opencast/pyCA/blob/c89b168d4780d157e1b3f7676628c1b131956a88/pyca/config.py#L100-L111", "partition": "test"}
{"repo": "nvdv/vprof", "path": "vprof/stats_server.py", "func_name": "StatsHandler.do_POST", "original_string": "def do_POST(self):\n        \"\"\"Handles HTTP POST requests.\"\"\"\n        post_data = self.rfile.read(int(self.headers['Content-Length']))\n        json_data = gzip.decompress(post_data)\n        self._profile_json.update(json.loads(json_data.decode('utf-8')))\n        self._send_response(\n            200, headers=(('Content-type', '%s; charset=utf-8' % 'text/json'),\n                          ('Content-Encoding', 'gzip'),\n                          ('Content-Length', len(post_data))))", "language": "python", "code": "def do_POST(self):\n        \"\"\"Handles HTTP POST requests.\"\"\"\n        post_data = self.rfile.read(int(self.headers['Content-Length']))\n        json_data = gzip.decompress(post_data)\n        self._profile_json.update(json.loads(json_data.decode('utf-8')))\n        self._send_response(\n            200, headers=(('Content-type', '%s; charset=utf-8' % 'text/json'),\n                          ('Content-Encoding', 'gzip'),\n                          ('Content-Length', len(post_data))))", "code_tokens": ["def", "do_POST", "(", "self", ")", ":", "post_data", "=", "self", ".", "rfile", ".", "read", "(", "int", "(", "self", ".", "headers", "[", "'Content-Length'", "]", ")", ")", "json_data", "=", "gzip", ".", "decompress", "(", "post_data", ")", "self", ".", "_profile_json", ".", "update", "(", "json", ".", "loads", "(", "json_data", ".", "decode", "(", "'utf-8'", ")", ")", ")", "self", ".", "_send_response", "(", "200", ",", "headers", "=", "(", "(", "'Content-type'", ",", "'%s; charset=utf-8'", "%", "'text/json'", ")", ",", "(", "'Content-Encoding'", ",", "'gzip'", ")", ",", "(", "'Content-Length'", ",", "len", "(", "post_data", ")", ")", ")", ")"], "docstring": "Handles HTTP POST requests.", "docstring_tokens": ["Handles", "HTTP", "POST", "requests", "."], "sha": "4c3ff78f8920ab10cb9c00b14143452aa09ff6bb", "url": "https://github.com/nvdv/vprof/blob/4c3ff78f8920ab10cb9c00b14143452aa09ff6bb/vprof/stats_server.py#L68-L76", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/utils.py", "func_name": "safe_infer", "original_string": "def safe_infer(\n    node: astroid.node_classes.NodeNG, context=None\n) -> Optional[astroid.node_classes.NodeNG]:\n    \"\"\"Return the inferred value for the given node.\n\n    Return None if inference failed or if there is some ambiguity (more than\n    one node has been inferred).\n    \"\"\"\n    try:\n        inferit = node.infer(context=context)\n        value = next(inferit)\n    except astroid.InferenceError:\n        return None\n    try:\n        next(inferit)\n        return None  # None if there is ambiguity on the inferred node\n    except astroid.InferenceError:\n        return None  # there is some kind of ambiguity\n    except StopIteration:\n        return value", "language": "python", "code": "def safe_infer(\n    node: astroid.node_classes.NodeNG, context=None\n) -> Optional[astroid.node_classes.NodeNG]:\n    \"\"\"Return the inferred value for the given node.\n\n    Return None if inference failed or if there is some ambiguity (more than\n    one node has been inferred).\n    \"\"\"\n    try:\n        inferit = node.infer(context=context)\n        value = next(inferit)\n    except astroid.InferenceError:\n        return None\n    try:\n        next(inferit)\n        return None  # None if there is ambiguity on the inferred node\n    except astroid.InferenceError:\n        return None  # there is some kind of ambiguity\n    except StopIteration:\n        return value", "code_tokens": ["def", "safe_infer", "(", "node", ":", "astroid", ".", "node_classes", ".", "NodeNG", ",", "context", "=", "None", ")", "->", "Optional", "[", "astroid", ".", "node_classes", ".", "NodeNG", "]", ":", "try", ":", "inferit", "=", "node", ".", "infer", "(", "context", "=", "context", ")", "value", "=", "next", "(", "inferit", ")", "except", "astroid", ".", "InferenceError", ":", "return", "None", "try", ":", "next", "(", "inferit", ")", "return", "None", "# None if there is ambiguity on the inferred node", "except", "astroid", ".", "InferenceError", ":", "return", "None", "# there is some kind of ambiguity", "except", "StopIteration", ":", "return", "value"], "docstring": "Return the inferred value for the given node.\n\n    Return None if inference failed or if there is some ambiguity (more than\n    one node has been inferred).", "docstring_tokens": ["Return", "the", "inferred", "value", "for", "the", "given", "node", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/utils.py#L1057-L1076", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/refactoring.py", "func_name": "RefactoringChecker._check_exception_inherit_from_stopiteration", "original_string": "def _check_exception_inherit_from_stopiteration(exc):\n        \"\"\"Return True if the exception node in argument inherit from StopIteration\"\"\"\n        stopiteration_qname = \"{}.StopIteration\".format(utils.EXCEPTIONS_MODULE)\n        return any(_class.qname() == stopiteration_qname for _class in exc.mro())", "language": "python", "code": "def _check_exception_inherit_from_stopiteration(exc):\n        \"\"\"Return True if the exception node in argument inherit from StopIteration\"\"\"\n        stopiteration_qname = \"{}.StopIteration\".format(utils.EXCEPTIONS_MODULE)\n        return any(_class.qname() == stopiteration_qname for _class in exc.mro())", "code_tokens": ["def", "_check_exception_inherit_from_stopiteration", "(", "exc", ")", ":", "stopiteration_qname", "=", "\"{}.StopIteration\"", ".", "format", "(", "utils", ".", "EXCEPTIONS_MODULE", ")", "return", "any", "(", "_class", ".", "qname", "(", ")", "==", "stopiteration_qname", "for", "_class", "in", "exc", ".", "mro", "(", ")", ")"], "docstring": "Return True if the exception node in argument inherit from StopIteration", "docstring_tokens": ["Return", "True", "if", "the", "exception", "node", "in", "argument", "inherit", "from", "StopIteration"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/refactoring.py#L611-L614", "partition": "test"}
{"repo": "rwl/godot", "path": "godot/edge.py", "func_name": "Edge._on_drawing", "original_string": "def _on_drawing(self, object, name, old, new):\n        \"\"\" Handles the containers of drawing components being set.\n        \"\"\"\n        attrs = [ \"drawing\", \"arrowhead_drawing\" ]\n\n        others = [getattr(self, a) for a in attrs \\\n            if (a != name) and (getattr(self, a) is not None)]\n\n        x, y = self.component.position\n        print \"POS:\", x, y, self.component.position\n\n        abs_x = [d.x + x for d in others]\n        abs_y = [d.y + y for d in others]\n\n        print \"ABS:\", abs_x, abs_y\n\n        # Assume that he new drawing is positioned relative to graph origin.\n        x1 = min( abs_x + [new.x] )\n        y1 = min( abs_y + [new.y] )\n\n        print \"DRAW:\", new.position\n        new.position = [ new.x - x1, new.y - y1 ]\n        print \"DRAW:\", new.position\n\n#        for i, b in enumerate( others ):\n#            self.drawing.position = [100, 100]\n#            self.drawing.request_redraw()\n#            print \"OTHER:\", b.position, abs_x[i] - x1\n#            b.position = [ abs_x[i] - x1, abs_y[i] - y1 ]\n#            b.x = 50\n#            b.y = 50\n#            print \"OTHER:\", b.position, abs_x[i], x1\n\n#        for attr in attrs:\n#            if attr != name:\n#                if getattr(self, attr) is not None:\n#                    drawing = getattr(self, attr)\n#                    drawing.position = [50, 50]\n\n        if old is not None:\n            self.component.remove( old )\n        if new is not None:\n            self.component.add( new )\n\n        print \"POS NEW:\", self.component.position\n        self.component.position = [ x1, y1 ]\n        print \"POS NEW:\", self.component.position\n        self.component.request_redraw()\n        print \"POS NEW:\", self.component.position", "language": "python", "code": "def _on_drawing(self, object, name, old, new):\n        \"\"\" Handles the containers of drawing components being set.\n        \"\"\"\n        attrs = [ \"drawing\", \"arrowhead_drawing\" ]\n\n        others = [getattr(self, a) for a in attrs \\\n            if (a != name) and (getattr(self, a) is not None)]\n\n        x, y = self.component.position\n        print \"POS:\", x, y, self.component.position\n\n        abs_x = [d.x + x for d in others]\n        abs_y = [d.y + y for d in others]\n\n        print \"ABS:\", abs_x, abs_y\n\n        # Assume that he new drawing is positioned relative to graph origin.\n        x1 = min( abs_x + [new.x] )\n        y1 = min( abs_y + [new.y] )\n\n        print \"DRAW:\", new.position\n        new.position = [ new.x - x1, new.y - y1 ]\n        print \"DRAW:\", new.position\n\n#        for i, b in enumerate( others ):\n#            self.drawing.position = [100, 100]\n#            self.drawing.request_redraw()\n#            print \"OTHER:\", b.position, abs_x[i] - x1\n#            b.position = [ abs_x[i] - x1, abs_y[i] - y1 ]\n#            b.x = 50\n#            b.y = 50\n#            print \"OTHER:\", b.position, abs_x[i], x1\n\n#        for attr in attrs:\n#            if attr != name:\n#                if getattr(self, attr) is not None:\n#                    drawing = getattr(self, attr)\n#                    drawing.position = [50, 50]\n\n        if old is not None:\n            self.component.remove( old )\n        if new is not None:\n            self.component.add( new )\n\n        print \"POS NEW:\", self.component.position\n        self.component.position = [ x1, y1 ]\n        print \"POS NEW:\", self.component.position\n        self.component.request_redraw()\n        print \"POS NEW:\", self.component.position", "code_tokens": ["def", "_on_drawing", "(", "self", ",", "object", ",", "name", ",", "old", ",", "new", ")", ":", "attrs", "=", "[", "\"drawing\"", ",", "\"arrowhead_drawing\"", "]", "others", "=", "[", "getattr", "(", "self", ",", "a", ")", "for", "a", "in", "attrs", "if", "(", "a", "!=", "name", ")", "and", "(", "getattr", "(", "self", ",", "a", ")", "is", "not", "None", ")", "]", "x", ",", "y", "=", "self", ".", "component", ".", "position", "print", "\"POS:\"", ",", "x", ",", "y", ",", "self", ".", "component", ".", "position", "abs_x", "=", "[", "d", ".", "x", "+", "x", "for", "d", "in", "others", "]", "abs_y", "=", "[", "d", ".", "y", "+", "y", "for", "d", "in", "others", "]", "print", "\"ABS:\"", ",", "abs_x", ",", "abs_y", "# Assume that he new drawing is positioned relative to graph origin.", "x1", "=", "min", "(", "abs_x", "+", "[", "new", ".", "x", "]", ")", "y1", "=", "min", "(", "abs_y", "+", "[", "new", ".", "y", "]", ")", "print", "\"DRAW:\"", ",", "new", ".", "position", "new", ".", "position", "=", "[", "new", ".", "x", "-", "x1", ",", "new", ".", "y", "-", "y1", "]", "print", "\"DRAW:\"", ",", "new", ".", "position", "#        for i, b in enumerate( others ):", "#            self.drawing.position = [100, 100]", "#            self.drawing.request_redraw()", "#            print \"OTHER:\", b.position, abs_x[i] - x1", "#            b.position = [ abs_x[i] - x1, abs_y[i] - y1 ]", "#            b.x = 50", "#            b.y = 50", "#            print \"OTHER:\", b.position, abs_x[i], x1", "#        for attr in attrs:", "#            if attr != name:", "#                if getattr(self, attr) is not None:", "#                    drawing = getattr(self, attr)", "#                    drawing.position = [50, 50]", "if", "old", "is", "not", "None", ":", "self", ".", "component", ".", "remove", "(", "old", ")", "if", "new", "is", "not", "None", ":", "self", ".", "component", ".", "add", "(", "new", ")", "print", "\"POS NEW:\"", ",", "self", ".", "component", ".", "position", "self", ".", "component", ".", "position", "=", "[", "x1", ",", "y1", "]", "print", "\"POS NEW:\"", ",", "self", ".", "component", ".", "position", "self", ".", "component", ".", "request_redraw", "(", ")", "print", "\"POS NEW:\"", ",", "self", ".", "component", ".", "position"], "docstring": "Handles the containers of drawing components being set.", "docstring_tokens": ["Handles", "the", "containers", "of", "drawing", "components", "being", "set", "."], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/edge.py#L780-L828", "partition": "test"}
{"repo": "bloomreach/s4cmd", "path": "s4cmd.py", "func_name": "BotoClient.add_options", "original_string": "def add_options(parser):\n    '''Add the whole list of API parameters into optparse.'''\n    for param, param_type, param_doc in BotoClient.EXTRA_CLIENT_PARAMS:\n      parser.add_option('--API-' + param, help=param_doc, type=param_type, dest=param)", "language": "python", "code": "def add_options(parser):\n    '''Add the whole list of API parameters into optparse.'''\n    for param, param_type, param_doc in BotoClient.EXTRA_CLIENT_PARAMS:\n      parser.add_option('--API-' + param, help=param_doc, type=param_type, dest=param)", "code_tokens": ["def", "add_options", "(", "parser", ")", ":", "for", "param", ",", "param_type", ",", "param_doc", "in", "BotoClient", ".", "EXTRA_CLIENT_PARAMS", ":", "parser", ".", "add_option", "(", "'--API-'", "+", "param", ",", "help", "=", "param_doc", ",", "type", "=", "param_type", ",", "dest", "=", "param", ")"], "docstring": "Add the whole list of API parameters into optparse.", "docstring_tokens": ["Add", "the", "whole", "list", "of", "API", "parameters", "into", "optparse", "."], "sha": "bb51075bf43703e7cd95aa39288cf7732ec13a6d", "url": "https://github.com/bloomreach/s4cmd/blob/bb51075bf43703e7cd95aa39288cf7732ec13a6d/s4cmd.py#L441-L444", "partition": "test"}
{"repo": "giffels/CloudStackAIO", "path": "CloudStackAIO/CloudStack.py", "func_name": "CloudStack._sign", "original_string": "def _sign(self, url_parameters: dict) -> dict:\n        \"\"\"\n        According to the CloudStack documentation, each request needs to be signed in order to authenticate the user\n        account executing the API command. The signature is generated using a combination of the api secret and a SHA-1\n        hash of the url parameters including the command string. In order to generate a unique identifier, the url\n        parameters have to be transformed to lower case and ordered alphabetically.\n\n        :param url_parameters: The url parameters of the API call including the command string\n        :type url_parameters: dict\n        :return: The url parameters including a new key, which contains the signature\n        :rtype: dict\n        \"\"\"\n        if url_parameters:\n            url_parameters.pop('signature', None)  # remove potential existing signature from url parameters\n            request_string = urlencode(sorted(url_parameters.items()), safe='.-*_', quote_via=quote).lower()\n            digest = hmac.new(self.api_secret.encode('utf-8'), request_string.encode('utf-8'), hashlib.sha1).digest()\n            url_parameters['signature'] = base64.b64encode(digest).decode('utf-8').strip()\n        return url_parameters", "language": "python", "code": "def _sign(self, url_parameters: dict) -> dict:\n        \"\"\"\n        According to the CloudStack documentation, each request needs to be signed in order to authenticate the user\n        account executing the API command. The signature is generated using a combination of the api secret and a SHA-1\n        hash of the url parameters including the command string. In order to generate a unique identifier, the url\n        parameters have to be transformed to lower case and ordered alphabetically.\n\n        :param url_parameters: The url parameters of the API call including the command string\n        :type url_parameters: dict\n        :return: The url parameters including a new key, which contains the signature\n        :rtype: dict\n        \"\"\"\n        if url_parameters:\n            url_parameters.pop('signature', None)  # remove potential existing signature from url parameters\n            request_string = urlencode(sorted(url_parameters.items()), safe='.-*_', quote_via=quote).lower()\n            digest = hmac.new(self.api_secret.encode('utf-8'), request_string.encode('utf-8'), hashlib.sha1).digest()\n            url_parameters['signature'] = base64.b64encode(digest).decode('utf-8').strip()\n        return url_parameters", "code_tokens": ["def", "_sign", "(", "self", ",", "url_parameters", ":", "dict", ")", "->", "dict", ":", "if", "url_parameters", ":", "url_parameters", ".", "pop", "(", "'signature'", ",", "None", ")", "# remove potential existing signature from url parameters", "request_string", "=", "urlencode", "(", "sorted", "(", "url_parameters", ".", "items", "(", ")", ")", ",", "safe", "=", "'.-*_'", ",", "quote_via", "=", "quote", ")", ".", "lower", "(", ")", "digest", "=", "hmac", ".", "new", "(", "self", ".", "api_secret", ".", "encode", "(", "'utf-8'", ")", ",", "request_string", ".", "encode", "(", "'utf-8'", ")", ",", "hashlib", ".", "sha1", ")", ".", "digest", "(", ")", "url_parameters", "[", "'signature'", "]", "=", "base64", ".", "b64encode", "(", "digest", ")", ".", "decode", "(", "'utf-8'", ")", ".", "strip", "(", ")", "return", "url_parameters"], "docstring": "According to the CloudStack documentation, each request needs to be signed in order to authenticate the user\n        account executing the API command. The signature is generated using a combination of the api secret and a SHA-1\n        hash of the url parameters including the command string. In order to generate a unique identifier, the url\n        parameters have to be transformed to lower case and ordered alphabetically.\n\n        :param url_parameters: The url parameters of the API call including the command string\n        :type url_parameters: dict\n        :return: The url parameters including a new key, which contains the signature\n        :rtype: dict", "docstring_tokens": ["According", "to", "the", "CloudStack", "documentation", "each", "request", "needs", "to", "be", "signed", "in", "order", "to", "authenticate", "the", "user", "account", "executing", "the", "API", "command", ".", "The", "signature", "is", "generated", "using", "a", "combination", "of", "the", "api", "secret", "and", "a", "SHA", "-", "1", "hash", "of", "the", "url", "parameters", "including", "the", "command", "string", ".", "In", "order", "to", "generate", "a", "unique", "identifier", "the", "url", "parameters", "have", "to", "be", "transformed", "to", "lower", "case", "and", "ordered", "alphabetically", "."], "sha": "f9df856622eb8966a6816f8008cc16d75b0067e5", "url": "https://github.com/giffels/CloudStackAIO/blob/f9df856622eb8966a6816f8008cc16d75b0067e5/CloudStackAIO/CloudStack.py#L203-L220", "partition": "test"}
{"repo": "oscarbranson/latools", "path": "latools/helpers/config.py", "func_name": "copy_SRM_file", "original_string": "def copy_SRM_file(destination=None, config='DEFAULT'):\n    \"\"\"\n    Creates a copy of the default SRM table at the specified location.\n\n    Parameters\n    ----------\n    destination : str\n        The save location for the SRM file. If no location specified, \n        saves it as 'LAtools_[config]_SRMTable.csv' in the current working \n        directory.\n    config : str\n        It's possible to set up different configurations with different\n        SRM files. This specifies the name of the configuration that you \n        want to copy the SRM file from. If not specified, the 'DEFAULT'\n        configuration is used.\n    \"\"\"\n    # find SRM file from configuration    \n    conf = read_configuration()\n\n    src = pkgrs.resource_filename('latools', conf['srmfile'])\n\n    # work out destination path (if not given)\n    if destination is None:\n        destination = './LAtools_' + conf['config'] + '_SRMTable.csv'\n    \n    if os.path.isdir(destination):\n        destination += 'LAtools_' + conf['config'] + '_SRMTable.csv'\n\n    copyfile(src, destination)\n\n    print(src + ' \\n    copied to:\\n      ' + destination)\n    return", "language": "python", "code": "def copy_SRM_file(destination=None, config='DEFAULT'):\n    \"\"\"\n    Creates a copy of the default SRM table at the specified location.\n\n    Parameters\n    ----------\n    destination : str\n        The save location for the SRM file. If no location specified, \n        saves it as 'LAtools_[config]_SRMTable.csv' in the current working \n        directory.\n    config : str\n        It's possible to set up different configurations with different\n        SRM files. This specifies the name of the configuration that you \n        want to copy the SRM file from. If not specified, the 'DEFAULT'\n        configuration is used.\n    \"\"\"\n    # find SRM file from configuration    \n    conf = read_configuration()\n\n    src = pkgrs.resource_filename('latools', conf['srmfile'])\n\n    # work out destination path (if not given)\n    if destination is None:\n        destination = './LAtools_' + conf['config'] + '_SRMTable.csv'\n    \n    if os.path.isdir(destination):\n        destination += 'LAtools_' + conf['config'] + '_SRMTable.csv'\n\n    copyfile(src, destination)\n\n    print(src + ' \\n    copied to:\\n      ' + destination)\n    return", "code_tokens": ["def", "copy_SRM_file", "(", "destination", "=", "None", ",", "config", "=", "'DEFAULT'", ")", ":", "# find SRM file from configuration    ", "conf", "=", "read_configuration", "(", ")", "src", "=", "pkgrs", ".", "resource_filename", "(", "'latools'", ",", "conf", "[", "'srmfile'", "]", ")", "# work out destination path (if not given)", "if", "destination", "is", "None", ":", "destination", "=", "'./LAtools_'", "+", "conf", "[", "'config'", "]", "+", "'_SRMTable.csv'", "if", "os", ".", "path", ".", "isdir", "(", "destination", ")", ":", "destination", "+=", "'LAtools_'", "+", "conf", "[", "'config'", "]", "+", "'_SRMTable.csv'", "copyfile", "(", "src", ",", "destination", ")", "print", "(", "src", "+", "' \\n    copied to:\\n      '", "+", "destination", ")", "return"], "docstring": "Creates a copy of the default SRM table at the specified location.\n\n    Parameters\n    ----------\n    destination : str\n        The save location for the SRM file. If no location specified, \n        saves it as 'LAtools_[config]_SRMTable.csv' in the current working \n        directory.\n    config : str\n        It's possible to set up different configurations with different\n        SRM files. This specifies the name of the configuration that you \n        want to copy the SRM file from. If not specified, the 'DEFAULT'\n        configuration is used.", "docstring_tokens": ["Creates", "a", "copy", "of", "the", "default", "SRM", "table", "at", "the", "specified", "location", "."], "sha": "cd25a650cfee318152f234d992708511f7047fbe", "url": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/helpers/config.py#L78-L109", "partition": "test"}
{"repo": "dopefishh/pympi", "path": "pympi/Elan.py", "func_name": "Eaf.shift_annotations", "original_string": "def shift_annotations(self, time):\n        \"\"\"Shift all annotations in time. Annotations that are in the beginning\n        and a left shift is applied can be squashed or discarded.\n\n        :param int time: Time shift width, negative numbers make a left shift.\n        :returns: Tuple of a list of squashed annotations and a list of removed\n                  annotations in the format: ``(tiername, start, end, value)``.\n        \"\"\"\n        total_re = []\n        total_sq = []\n        for name, tier in self.tiers.items():\n            squashed = []\n            for aid, (begin, end, value, _) in tier[0].items():\n                if self.timeslots[end]+time <= 0:\n                    squashed.append((name, aid))\n                elif self.timeslots[begin]+time < 0:\n                    total_sq.append((name, self.timeslots[begin],\n                                     self.timeslots[end], value))\n                    self.timeslots[begin] = 0\n                else:\n                    self.timeslots[begin] += time\n                    self.timeslots[end] += time\n            for name, aid in squashed:\n                start, end, value, _ = self.tiers[name][0][aid]\n                del(self.tiers[name][0][aid])\n                del(self.annotations[aid])\n                total_re.append(\n                    (name, self.timeslots[start], self.timeslots[end], value))\n        return total_sq, total_re", "language": "python", "code": "def shift_annotations(self, time):\n        \"\"\"Shift all annotations in time. Annotations that are in the beginning\n        and a left shift is applied can be squashed or discarded.\n\n        :param int time: Time shift width, negative numbers make a left shift.\n        :returns: Tuple of a list of squashed annotations and a list of removed\n                  annotations in the format: ``(tiername, start, end, value)``.\n        \"\"\"\n        total_re = []\n        total_sq = []\n        for name, tier in self.tiers.items():\n            squashed = []\n            for aid, (begin, end, value, _) in tier[0].items():\n                if self.timeslots[end]+time <= 0:\n                    squashed.append((name, aid))\n                elif self.timeslots[begin]+time < 0:\n                    total_sq.append((name, self.timeslots[begin],\n                                     self.timeslots[end], value))\n                    self.timeslots[begin] = 0\n                else:\n                    self.timeslots[begin] += time\n                    self.timeslots[end] += time\n            for name, aid in squashed:\n                start, end, value, _ = self.tiers[name][0][aid]\n                del(self.tiers[name][0][aid])\n                del(self.annotations[aid])\n                total_re.append(\n                    (name, self.timeslots[start], self.timeslots[end], value))\n        return total_sq, total_re", "code_tokens": ["def", "shift_annotations", "(", "self", ",", "time", ")", ":", "total_re", "=", "[", "]", "total_sq", "=", "[", "]", "for", "name", ",", "tier", "in", "self", ".", "tiers", ".", "items", "(", ")", ":", "squashed", "=", "[", "]", "for", "aid", ",", "(", "begin", ",", "end", ",", "value", ",", "_", ")", "in", "tier", "[", "0", "]", ".", "items", "(", ")", ":", "if", "self", ".", "timeslots", "[", "end", "]", "+", "time", "<=", "0", ":", "squashed", ".", "append", "(", "(", "name", ",", "aid", ")", ")", "elif", "self", ".", "timeslots", "[", "begin", "]", "+", "time", "<", "0", ":", "total_sq", ".", "append", "(", "(", "name", ",", "self", ".", "timeslots", "[", "begin", "]", ",", "self", ".", "timeslots", "[", "end", "]", ",", "value", ")", ")", "self", ".", "timeslots", "[", "begin", "]", "=", "0", "else", ":", "self", ".", "timeslots", "[", "begin", "]", "+=", "time", "self", ".", "timeslots", "[", "end", "]", "+=", "time", "for", "name", ",", "aid", "in", "squashed", ":", "start", ",", "end", ",", "value", ",", "_", "=", "self", ".", "tiers", "[", "name", "]", "[", "0", "]", "[", "aid", "]", "del", "(", "self", ".", "tiers", "[", "name", "]", "[", "0", "]", "[", "aid", "]", ")", "del", "(", "self", ".", "annotations", "[", "aid", "]", ")", "total_re", ".", "append", "(", "(", "name", ",", "self", ".", "timeslots", "[", "start", "]", ",", "self", ".", "timeslots", "[", "end", "]", ",", "value", ")", ")", "return", "total_sq", ",", "total_re"], "docstring": "Shift all annotations in time. Annotations that are in the beginning\n        and a left shift is applied can be squashed or discarded.\n\n        :param int time: Time shift width, negative numbers make a left shift.\n        :returns: Tuple of a list of squashed annotations and a list of removed\n                  annotations in the format: ``(tiername, start, end, value)``.", "docstring_tokens": ["Shift", "all", "annotations", "in", "time", ".", "Annotations", "that", "are", "in", "the", "beginning", "and", "a", "left", "shift", "is", "applied", "can", "be", "squashed", "or", "discarded", "."], "sha": "79c747cde45b5ba203ed93154d8c123ac9c3ef56", "url": "https://github.com/dopefishh/pympi/blob/79c747cde45b5ba203ed93154d8c123ac9c3ef56/pympi/Elan.py#L1290-L1318", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/magic.py", "func_name": "record_magic", "original_string": "def record_magic(dct, magic_kind, magic_name, func):\n    \"\"\"Utility function to store a function as a magic of a specific kind.\n\n    Parameters\n    ----------\n    dct : dict\n      A dictionary with 'line' and 'cell' subdicts.\n\n    magic_kind : str\n      Kind of magic to be stored.\n\n    magic_name : str\n      Key to store the magic as.\n\n    func : function\n      Callable object to store.\n    \"\"\"\n    if magic_kind == 'line_cell':\n        dct['line'][magic_name] = dct['cell'][magic_name] = func\n    else:\n        dct[magic_kind][magic_name] = func", "language": "python", "code": "def record_magic(dct, magic_kind, magic_name, func):\n    \"\"\"Utility function to store a function as a magic of a specific kind.\n\n    Parameters\n    ----------\n    dct : dict\n      A dictionary with 'line' and 'cell' subdicts.\n\n    magic_kind : str\n      Kind of magic to be stored.\n\n    magic_name : str\n      Key to store the magic as.\n\n    func : function\n      Callable object to store.\n    \"\"\"\n    if magic_kind == 'line_cell':\n        dct['line'][magic_name] = dct['cell'][magic_name] = func\n    else:\n        dct[magic_kind][magic_name] = func", "code_tokens": ["def", "record_magic", "(", "dct", ",", "magic_kind", ",", "magic_name", ",", "func", ")", ":", "if", "magic_kind", "==", "'line_cell'", ":", "dct", "[", "'line'", "]", "[", "magic_name", "]", "=", "dct", "[", "'cell'", "]", "[", "magic_name", "]", "=", "func", "else", ":", "dct", "[", "magic_kind", "]", "[", "magic_name", "]", "=", "func"], "docstring": "Utility function to store a function as a magic of a specific kind.\n\n    Parameters\n    ----------\n    dct : dict\n      A dictionary with 'line' and 'cell' subdicts.\n\n    magic_kind : str\n      Kind of magic to be stored.\n\n    magic_name : str\n      Key to store the magic as.\n\n    func : function\n      Callable object to store.", "docstring_tokens": ["Utility", "function", "to", "store", "a", "function", "as", "a", "magic", "of", "a", "specific", "kind", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/magic.py#L118-L138", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", "func_name": "_prepare_args", "original_string": "def _prepare_args(value_and_gradients_function,\n                  initial_step_size,\n                  val_initial,\n                  val_0,\n                  approximate_wolfe_threshold):\n  \"\"\"Prepares the arguments for the line search initialization.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    initial_step_size: Scalar positive `Tensor` of real dtype, or a tensor of\n      shape [n] in batching mode. The initial value (or values) to try to\n      bracket the minimum. Default is `1.` as a float32.\n      Note that this point need not necessarily bracket the minimum for the line\n      search to work correctly but the supplied value must be greater than 0.\n      A good initial value will make the search converge faster.\n    val_initial: The full return value of evaluating\n      value_and_gradients_function at initial_step_size, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If not None the value of\n      `initial_step_size` will be ignored, otherwise the tuple will be computed\n      by evaluating value_and_gradients_function.\n    val_0: The full return value of value_and_gradients_function at `0.`, i.e.\n      a namedtuple with 'x', 'f', 'df', if already known by the caller. If None\n      the tuple will be computed by evaluating value_and_gradients_function.\n    approximate_wolfe_threshold: Scalar positive `Tensor` of\n      real dtype. Corresponds to the parameter 'epsilon' in\n      [Hager and Zhang (2006)][2]. Used to estimate the\n      threshold at which the line search switches to approximate Wolfe\n      conditions.\n\n  Returns:\n    left: A namedtuple, as returned by value_and_gradients_function,\n      containing the value and derivative of the function at `0.`.\n    val_initial: A namedtuple, as returned by value_and_gradients_function,\n      containing the value and derivative of the function at\n      `initial_step_size`.\n    f_lim: Real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    eval_count: Scalar int32 `Tensor`. The number of target function\n      evaluations made by this function.\n  \"\"\"\n  eval_count = 0\n  if val_initial is None:\n    if initial_step_size is not None:\n      initial_step_size = tf.convert_to_tensor(value=initial_step_size)\n    else:\n      initial_step_size = tf.convert_to_tensor(value=1.0, dtype=tf.float32)\n    val_initial = value_and_gradients_function(initial_step_size)\n    eval_count += 1\n\n  if val_0 is None:\n    x_0 = tf.zeros_like(val_initial.x)\n    val_0 = value_and_gradients_function(x_0)\n    eval_count += 1\n\n  f_lim = val_0.f + (approximate_wolfe_threshold * tf.abs(val_0.f))\n  return val_0, val_initial, f_lim, tf.convert_to_tensor(value=eval_count)", "language": "python", "code": "def _prepare_args(value_and_gradients_function,\n                  initial_step_size,\n                  val_initial,\n                  val_0,\n                  approximate_wolfe_threshold):\n  \"\"\"Prepares the arguments for the line search initialization.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    initial_step_size: Scalar positive `Tensor` of real dtype, or a tensor of\n      shape [n] in batching mode. The initial value (or values) to try to\n      bracket the minimum. Default is `1.` as a float32.\n      Note that this point need not necessarily bracket the minimum for the line\n      search to work correctly but the supplied value must be greater than 0.\n      A good initial value will make the search converge faster.\n    val_initial: The full return value of evaluating\n      value_and_gradients_function at initial_step_size, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If not None the value of\n      `initial_step_size` will be ignored, otherwise the tuple will be computed\n      by evaluating value_and_gradients_function.\n    val_0: The full return value of value_and_gradients_function at `0.`, i.e.\n      a namedtuple with 'x', 'f', 'df', if already known by the caller. If None\n      the tuple will be computed by evaluating value_and_gradients_function.\n    approximate_wolfe_threshold: Scalar positive `Tensor` of\n      real dtype. Corresponds to the parameter 'epsilon' in\n      [Hager and Zhang (2006)][2]. Used to estimate the\n      threshold at which the line search switches to approximate Wolfe\n      conditions.\n\n  Returns:\n    left: A namedtuple, as returned by value_and_gradients_function,\n      containing the value and derivative of the function at `0.`.\n    val_initial: A namedtuple, as returned by value_and_gradients_function,\n      containing the value and derivative of the function at\n      `initial_step_size`.\n    f_lim: Real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    eval_count: Scalar int32 `Tensor`. The number of target function\n      evaluations made by this function.\n  \"\"\"\n  eval_count = 0\n  if val_initial is None:\n    if initial_step_size is not None:\n      initial_step_size = tf.convert_to_tensor(value=initial_step_size)\n    else:\n      initial_step_size = tf.convert_to_tensor(value=1.0, dtype=tf.float32)\n    val_initial = value_and_gradients_function(initial_step_size)\n    eval_count += 1\n\n  if val_0 is None:\n    x_0 = tf.zeros_like(val_initial.x)\n    val_0 = value_and_gradients_function(x_0)\n    eval_count += 1\n\n  f_lim = val_0.f + (approximate_wolfe_threshold * tf.abs(val_0.f))\n  return val_0, val_initial, f_lim, tf.convert_to_tensor(value=eval_count)", "code_tokens": ["def", "_prepare_args", "(", "value_and_gradients_function", ",", "initial_step_size", ",", "val_initial", ",", "val_0", ",", "approximate_wolfe_threshold", ")", ":", "eval_count", "=", "0", "if", "val_initial", "is", "None", ":", "if", "initial_step_size", "is", "not", "None", ":", "initial_step_size", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "initial_step_size", ")", "else", ":", "initial_step_size", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "1.0", ",", "dtype", "=", "tf", ".", "float32", ")", "val_initial", "=", "value_and_gradients_function", "(", "initial_step_size", ")", "eval_count", "+=", "1", "if", "val_0", "is", "None", ":", "x_0", "=", "tf", ".", "zeros_like", "(", "val_initial", ".", "x", ")", "val_0", "=", "value_and_gradients_function", "(", "x_0", ")", "eval_count", "+=", "1", "f_lim", "=", "val_0", ".", "f", "+", "(", "approximate_wolfe_threshold", "*", "tf", ".", "abs", "(", "val_0", ".", "f", ")", ")", "return", "val_0", ",", "val_initial", ",", "f_lim", ",", "tf", ".", "convert_to_tensor", "(", "value", "=", "eval_count", ")"], "docstring": "Prepares the arguments for the line search initialization.\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    initial_step_size: Scalar positive `Tensor` of real dtype, or a tensor of\n      shape [n] in batching mode. The initial value (or values) to try to\n      bracket the minimum. Default is `1.` as a float32.\n      Note that this point need not necessarily bracket the minimum for the line\n      search to work correctly but the supplied value must be greater than 0.\n      A good initial value will make the search converge faster.\n    val_initial: The full return value of evaluating\n      value_and_gradients_function at initial_step_size, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If not None the value of\n      `initial_step_size` will be ignored, otherwise the tuple will be computed\n      by evaluating value_and_gradients_function.\n    val_0: The full return value of value_and_gradients_function at `0.`, i.e.\n      a namedtuple with 'x', 'f', 'df', if already known by the caller. If None\n      the tuple will be computed by evaluating value_and_gradients_function.\n    approximate_wolfe_threshold: Scalar positive `Tensor` of\n      real dtype. Corresponds to the parameter 'epsilon' in\n      [Hager and Zhang (2006)][2]. Used to estimate the\n      threshold at which the line search switches to approximate Wolfe\n      conditions.\n\n  Returns:\n    left: A namedtuple, as returned by value_and_gradients_function,\n      containing the value and derivative of the function at `0.`.\n    val_initial: A namedtuple, as returned by value_and_gradients_function,\n      containing the value and derivative of the function at\n      `initial_step_size`.\n    f_lim: Real `Tensor` of shape [n]. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    eval_count: Scalar int32 `Tensor`. The number of target function\n      evaluations made by this function.", "docstring_tokens": ["Prepares", "the", "arguments", "for", "the", "line", "search", "initialization", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py#L579-L650", "partition": "test"}
{"repo": "boundary/pulse-api-cli", "path": "boundary/measurement_get.py", "func_name": "MeasurementGet.output_csv", "original_string": "def output_csv(self, text):\n        \"\"\"\n        Output results in CSV format\n        \"\"\"\n        payload = json.loads(text)\n        # Print CSV header\n        print(\"{0},{1},{2},{3},{4}\".format('timestamp', 'metric', 'aggregate', 'source', 'value'))\n        metric_name = self._metric_name\n        # Loop through the aggregates one row per timestamp, and 1 or more source/value pairs\n        for r in payload['result']['aggregates']['key']:\n            timestamp = self._format_timestamp(r[0][0])\n            # timestamp = string.strip(timestamp, ' ')\n            # timestamp = string.strip(timestamp, \"'\")\n            for s in r[1]:\n                print('{0},\"{1}\",\"{2}\",\"{3}\",{4}'.format(timestamp, metric_name, self.aggregate, s[0], s[1]))", "language": "python", "code": "def output_csv(self, text):\n        \"\"\"\n        Output results in CSV format\n        \"\"\"\n        payload = json.loads(text)\n        # Print CSV header\n        print(\"{0},{1},{2},{3},{4}\".format('timestamp', 'metric', 'aggregate', 'source', 'value'))\n        metric_name = self._metric_name\n        # Loop through the aggregates one row per timestamp, and 1 or more source/value pairs\n        for r in payload['result']['aggregates']['key']:\n            timestamp = self._format_timestamp(r[0][0])\n            # timestamp = string.strip(timestamp, ' ')\n            # timestamp = string.strip(timestamp, \"'\")\n            for s in r[1]:\n                print('{0},\"{1}\",\"{2}\",\"{3}\",{4}'.format(timestamp, metric_name, self.aggregate, s[0], s[1]))", "code_tokens": ["def", "output_csv", "(", "self", ",", "text", ")", ":", "payload", "=", "json", ".", "loads", "(", "text", ")", "# Print CSV header", "print", "(", "\"{0},{1},{2},{3},{4}\"", ".", "format", "(", "'timestamp'", ",", "'metric'", ",", "'aggregate'", ",", "'source'", ",", "'value'", ")", ")", "metric_name", "=", "self", ".", "_metric_name", "# Loop through the aggregates one row per timestamp, and 1 or more source/value pairs", "for", "r", "in", "payload", "[", "'result'", "]", "[", "'aggregates'", "]", "[", "'key'", "]", ":", "timestamp", "=", "self", ".", "_format_timestamp", "(", "r", "[", "0", "]", "[", "0", "]", ")", "# timestamp = string.strip(timestamp, ' ')", "# timestamp = string.strip(timestamp, \"'\")", "for", "s", "in", "r", "[", "1", "]", ":", "print", "(", "'{0},\"{1}\",\"{2}\",\"{3}\",{4}'", ".", "format", "(", "timestamp", ",", "metric_name", ",", "self", ".", "aggregate", ",", "s", "[", "0", "]", ",", "s", "[", "1", "]", ")", ")"], "docstring": "Output results in CSV format", "docstring_tokens": ["Output", "results", "in", "CSV", "format"], "sha": "b01ca65b442eed19faac309c9d62bbc3cb2c098f", "url": "https://github.com/boundary/pulse-api-cli/blob/b01ca65b442eed19faac309c9d62bbc3cb2c098f/boundary/measurement_get.py#L157-L171", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/macros/__init__.py", "func_name": "ds_format", "original_string": "def ds_format(ds, input_format, output_format):\n    \"\"\"\n    Takes an input string and outputs another string\n    as specified in the output format\n\n    :param ds: input string which contains a date\n    :type ds: str\n    :param input_format: input string format. E.g. %Y-%m-%d\n    :type input_format: str\n    :param output_format: output string format  E.g. %Y-%m-%d\n    :type output_format: str\n\n    >>> ds_format('2015-01-01', \"%Y-%m-%d\", \"%m-%d-%y\")\n    '01-01-15'\n    >>> ds_format('1/5/2015', \"%m/%d/%Y\",  \"%Y-%m-%d\")\n    '2015-01-05'\n    \"\"\"\n    return datetime.strptime(ds, input_format).strftime(output_format)", "language": "python", "code": "def ds_format(ds, input_format, output_format):\n    \"\"\"\n    Takes an input string and outputs another string\n    as specified in the output format\n\n    :param ds: input string which contains a date\n    :type ds: str\n    :param input_format: input string format. E.g. %Y-%m-%d\n    :type input_format: str\n    :param output_format: output string format  E.g. %Y-%m-%d\n    :type output_format: str\n\n    >>> ds_format('2015-01-01', \"%Y-%m-%d\", \"%m-%d-%y\")\n    '01-01-15'\n    >>> ds_format('1/5/2015', \"%m/%d/%Y\",  \"%Y-%m-%d\")\n    '2015-01-05'\n    \"\"\"\n    return datetime.strptime(ds, input_format).strftime(output_format)", "code_tokens": ["def", "ds_format", "(", "ds", ",", "input_format", ",", "output_format", ")", ":", "return", "datetime", ".", "strptime", "(", "ds", ",", "input_format", ")", ".", "strftime", "(", "output_format", ")"], "docstring": "Takes an input string and outputs another string\n    as specified in the output format\n\n    :param ds: input string which contains a date\n    :type ds: str\n    :param input_format: input string format. E.g. %Y-%m-%d\n    :type input_format: str\n    :param output_format: output string format  E.g. %Y-%m-%d\n    :type output_format: str\n\n    >>> ds_format('2015-01-01', \"%Y-%m-%d\", \"%m-%d-%y\")\n    '01-01-15'\n    >>> ds_format('1/5/2015', \"%m/%d/%Y\",  \"%Y-%m-%d\")\n    '2015-01-05'", "docstring_tokens": ["Takes", "an", "input", "string", "and", "outputs", "another", "string", "as", "specified", "in", "the", "output", "format"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/__init__.py#L49-L66", "partition": "test"}
{"repo": "datacamp/pythonwhat", "path": "pythonwhat/checks/check_object.py", "func_name": "is_instance", "original_string": "def is_instance(state, inst, not_instance_msg=None):\n    \"\"\"Check whether an object is an instance of a certain class.\n\n    ``is_instance()`` can currently only be used when chained from ``check_object()``, the function that is\n    used to 'zoom in' on the object of interest.\n\n    Args:\n        inst (class): The class that the object should have.\n        not_instance_msg (str): When specified, this overrides the automatically generated message in case\n            the object does not have the expected class.\n        state (State): The state that is passed in through the SCT chain (don't specify this).\n\n    :Example:\n\n        Student code and solution code::\n\n            import numpy as np\n            arr = np.array([1, 2, 3, 4, 5])\n\n        SCT::\n\n            # Verify the class of arr\n            import numpy\n            Ex().check_object('arr').is_instance(numpy.ndarray)\n    \"\"\"\n\n    state.assert_is([\"object_assignments\"], \"is_instance\", [\"check_object\"])\n\n    sol_name = state.solution_parts.get(\"name\")\n    stu_name = state.student_parts.get(\"name\")\n\n    if not_instance_msg is None:\n        not_instance_msg = \"Is it a {{inst.__name__}}?\"\n\n    if not isInstanceInProcess(sol_name, inst, state.solution_process):\n        raise InstructorError(\n            \"`is_instance()` noticed that `%s` is not a `%s` in the solution process.\"\n            % (sol_name, inst.__name__)\n        )\n\n    _msg = state.build_message(not_instance_msg, {\"inst\": inst})\n    feedback = Feedback(_msg, state)\n    state.do_test(InstanceProcessTest(stu_name, inst, state.student_process, feedback))\n\n    return state", "language": "python", "code": "def is_instance(state, inst, not_instance_msg=None):\n    \"\"\"Check whether an object is an instance of a certain class.\n\n    ``is_instance()`` can currently only be used when chained from ``check_object()``, the function that is\n    used to 'zoom in' on the object of interest.\n\n    Args:\n        inst (class): The class that the object should have.\n        not_instance_msg (str): When specified, this overrides the automatically generated message in case\n            the object does not have the expected class.\n        state (State): The state that is passed in through the SCT chain (don't specify this).\n\n    :Example:\n\n        Student code and solution code::\n\n            import numpy as np\n            arr = np.array([1, 2, 3, 4, 5])\n\n        SCT::\n\n            # Verify the class of arr\n            import numpy\n            Ex().check_object('arr').is_instance(numpy.ndarray)\n    \"\"\"\n\n    state.assert_is([\"object_assignments\"], \"is_instance\", [\"check_object\"])\n\n    sol_name = state.solution_parts.get(\"name\")\n    stu_name = state.student_parts.get(\"name\")\n\n    if not_instance_msg is None:\n        not_instance_msg = \"Is it a {{inst.__name__}}?\"\n\n    if not isInstanceInProcess(sol_name, inst, state.solution_process):\n        raise InstructorError(\n            \"`is_instance()` noticed that `%s` is not a `%s` in the solution process.\"\n            % (sol_name, inst.__name__)\n        )\n\n    _msg = state.build_message(not_instance_msg, {\"inst\": inst})\n    feedback = Feedback(_msg, state)\n    state.do_test(InstanceProcessTest(stu_name, inst, state.student_process, feedback))\n\n    return state", "code_tokens": ["def", "is_instance", "(", "state", ",", "inst", ",", "not_instance_msg", "=", "None", ")", ":", "state", ".", "assert_is", "(", "[", "\"object_assignments\"", "]", ",", "\"is_instance\"", ",", "[", "\"check_object\"", "]", ")", "sol_name", "=", "state", ".", "solution_parts", ".", "get", "(", "\"name\"", ")", "stu_name", "=", "state", ".", "student_parts", ".", "get", "(", "\"name\"", ")", "if", "not_instance_msg", "is", "None", ":", "not_instance_msg", "=", "\"Is it a {{inst.__name__}}?\"", "if", "not", "isInstanceInProcess", "(", "sol_name", ",", "inst", ",", "state", ".", "solution_process", ")", ":", "raise", "InstructorError", "(", "\"`is_instance()` noticed that `%s` is not a `%s` in the solution process.\"", "%", "(", "sol_name", ",", "inst", ".", "__name__", ")", ")", "_msg", "=", "state", ".", "build_message", "(", "not_instance_msg", ",", "{", "\"inst\"", ":", "inst", "}", ")", "feedback", "=", "Feedback", "(", "_msg", ",", "state", ")", "state", ".", "do_test", "(", "InstanceProcessTest", "(", "stu_name", ",", "inst", ",", "state", ".", "student_process", ",", "feedback", ")", ")", "return", "state"], "docstring": "Check whether an object is an instance of a certain class.\n\n    ``is_instance()`` can currently only be used when chained from ``check_object()``, the function that is\n    used to 'zoom in' on the object of interest.\n\n    Args:\n        inst (class): The class that the object should have.\n        not_instance_msg (str): When specified, this overrides the automatically generated message in case\n            the object does not have the expected class.\n        state (State): The state that is passed in through the SCT chain (don't specify this).\n\n    :Example:\n\n        Student code and solution code::\n\n            import numpy as np\n            arr = np.array([1, 2, 3, 4, 5])\n\n        SCT::\n\n            # Verify the class of arr\n            import numpy\n            Ex().check_object('arr').is_instance(numpy.ndarray)", "docstring_tokens": ["Check", "whether", "an", "object", "is", "an", "instance", "of", "a", "certain", "class", "."], "sha": "ffbf7f8436a51f77c22f3bed75ba3bc37a5c666f", "url": "https://github.com/datacamp/pythonwhat/blob/ffbf7f8436a51f77c22f3bed75ba3bc37a5c666f/pythonwhat/checks/check_object.py#L199-L243", "partition": "test"}
{"repo": "tmontaigu/pylas", "path": "pylas/vlrs/geotiff.py", "func_name": "parse_geo_tiff_keys_from_vlrs", "original_string": "def parse_geo_tiff_keys_from_vlrs(vlr_list: vlrlist.VLRList) -> List[GeoTiffKey]:\n    \"\"\" Gets the 3 GeoTiff vlrs from the vlr_list and parse them into\n    a nicer structure\n\n    Parameters\n    ----------\n    vlr_list: pylas.vrls.vlrslist.VLRList list of vlrs from a las file\n\n    Raises\n    ------\n        IndexError if any of the needed GeoTiffVLR is not found in the list\n\n    Returns\n    -------\n    List of GeoTiff keys parsed from the VLRs\n\n    \"\"\"\n    geo_key_dir = vlr_list.get_by_id(\n        GeoKeyDirectoryVlr.official_user_id(), GeoKeyDirectoryVlr.official_record_ids()\n    )[0]\n    geo_doubles = vlr_list.get_by_id(\n        GeoDoubleParamsVlr.official_user_id(), GeoDoubleParamsVlr.official_record_ids()\n    )[0]\n    geo_ascii = vlr_list.get_by_id(\n        GeoAsciiParamsVlr.official_user_id(), GeoAsciiParamsVlr.official_record_ids()\n    )[0]\n    return parse_geo_tiff(geo_key_dir, geo_doubles, geo_ascii)", "language": "python", "code": "def parse_geo_tiff_keys_from_vlrs(vlr_list: vlrlist.VLRList) -> List[GeoTiffKey]:\n    \"\"\" Gets the 3 GeoTiff vlrs from the vlr_list and parse them into\n    a nicer structure\n\n    Parameters\n    ----------\n    vlr_list: pylas.vrls.vlrslist.VLRList list of vlrs from a las file\n\n    Raises\n    ------\n        IndexError if any of the needed GeoTiffVLR is not found in the list\n\n    Returns\n    -------\n    List of GeoTiff keys parsed from the VLRs\n\n    \"\"\"\n    geo_key_dir = vlr_list.get_by_id(\n        GeoKeyDirectoryVlr.official_user_id(), GeoKeyDirectoryVlr.official_record_ids()\n    )[0]\n    geo_doubles = vlr_list.get_by_id(\n        GeoDoubleParamsVlr.official_user_id(), GeoDoubleParamsVlr.official_record_ids()\n    )[0]\n    geo_ascii = vlr_list.get_by_id(\n        GeoAsciiParamsVlr.official_user_id(), GeoAsciiParamsVlr.official_record_ids()\n    )[0]\n    return parse_geo_tiff(geo_key_dir, geo_doubles, geo_ascii)", "code_tokens": ["def", "parse_geo_tiff_keys_from_vlrs", "(", "vlr_list", ":", "vlrlist", ".", "VLRList", ")", "->", "List", "[", "GeoTiffKey", "]", ":", "geo_key_dir", "=", "vlr_list", ".", "get_by_id", "(", "GeoKeyDirectoryVlr", ".", "official_user_id", "(", ")", ",", "GeoKeyDirectoryVlr", ".", "official_record_ids", "(", ")", ")", "[", "0", "]", "geo_doubles", "=", "vlr_list", ".", "get_by_id", "(", "GeoDoubleParamsVlr", ".", "official_user_id", "(", ")", ",", "GeoDoubleParamsVlr", ".", "official_record_ids", "(", ")", ")", "[", "0", "]", "geo_ascii", "=", "vlr_list", ".", "get_by_id", "(", "GeoAsciiParamsVlr", ".", "official_user_id", "(", ")", ",", "GeoAsciiParamsVlr", ".", "official_record_ids", "(", ")", ")", "[", "0", "]", "return", "parse_geo_tiff", "(", "geo_key_dir", ",", "geo_doubles", ",", "geo_ascii", ")"], "docstring": "Gets the 3 GeoTiff vlrs from the vlr_list and parse them into\n    a nicer structure\n\n    Parameters\n    ----------\n    vlr_list: pylas.vrls.vlrslist.VLRList list of vlrs from a las file\n\n    Raises\n    ------\n        IndexError if any of the needed GeoTiffVLR is not found in the list\n\n    Returns\n    -------\n    List of GeoTiff keys parsed from the VLRs", "docstring_tokens": ["Gets", "the", "3", "GeoTiff", "vlrs", "from", "the", "vlr_list", "and", "parse", "them", "into", "a", "nicer", "structure"], "sha": "8335a1a7d7677f0e4bc391bb6fa3c75b42ed5b06", "url": "https://github.com/tmontaigu/pylas/blob/8335a1a7d7677f0e4bc391bb6fa3c75b42ed5b06/pylas/vlrs/geotiff.py#L23-L49", "partition": "test"}
{"repo": "codeghar/brokerlso", "path": "brokerlso/qmfv2.py", "func_name": "RequestCmd.delete_queue", "original_string": "def delete_queue(self, name):\n        \"\"\"Create message content and properties to delete queue with QMFv2\n\n        :param name: Name of queue to delete\n        :type name: str\n\n        :returns: Tuple containing content and method properties\n        \"\"\"\n        content = {\"_object_id\": {\"_object_name\": self.object_name},\n                   \"_method_name\": \"delete\",\n                   \"_arguments\": {\"type\": \"queue\",\n                                  \"name\": name,\n                                  \"options\": dict()}}  # \"A nested map with the key options. This is presently unused.\"\n        logger.debug(\"Message content -> {0}\".format(content))\n\n        return content, self.method_properties", "language": "python", "code": "def delete_queue(self, name):\n        \"\"\"Create message content and properties to delete queue with QMFv2\n\n        :param name: Name of queue to delete\n        :type name: str\n\n        :returns: Tuple containing content and method properties\n        \"\"\"\n        content = {\"_object_id\": {\"_object_name\": self.object_name},\n                   \"_method_name\": \"delete\",\n                   \"_arguments\": {\"type\": \"queue\",\n                                  \"name\": name,\n                                  \"options\": dict()}}  # \"A nested map with the key options. This is presently unused.\"\n        logger.debug(\"Message content -> {0}\".format(content))\n\n        return content, self.method_properties", "code_tokens": ["def", "delete_queue", "(", "self", ",", "name", ")", ":", "content", "=", "{", "\"_object_id\"", ":", "{", "\"_object_name\"", ":", "self", ".", "object_name", "}", ",", "\"_method_name\"", ":", "\"delete\"", ",", "\"_arguments\"", ":", "{", "\"type\"", ":", "\"queue\"", ",", "\"name\"", ":", "name", ",", "\"options\"", ":", "dict", "(", ")", "}", "}", "# \"A nested map with the key options. This is presently unused.\"", "logger", ".", "debug", "(", "\"Message content -> {0}\"", ".", "format", "(", "content", ")", ")", "return", "content", ",", "self", ".", "method_properties"], "docstring": "Create message content and properties to delete queue with QMFv2\n\n        :param name: Name of queue to delete\n        :type name: str\n\n        :returns: Tuple containing content and method properties", "docstring_tokens": ["Create", "message", "content", "and", "properties", "to", "delete", "queue", "with", "QMFv2"], "sha": "e110e12502b090e12b06c7615dd0a96a14a92585", "url": "https://github.com/codeghar/brokerlso/blob/e110e12502b090e12b06c7615dd0a96a14a92585/brokerlso/qmfv2.py#L109-L124", "partition": "test"}
{"repo": "kxepal/viivakoodi", "path": "barcode/base.py", "func_name": "Barcode.save", "original_string": "def save(self, filename, options=None):\n        \"\"\"Renders the barcode and saves it in `filename`.\n\n        :parameters:\n            filename : String\n                Filename to save the barcode in (without filename\n                extension).\n            options : Dict\n                The same as in `self.render`.\n\n        :returns: The full filename with extension.\n        :rtype: String\n        \"\"\"\n        output = self.render(options)\n        _filename = self.writer.save(filename, output)\n        return _filename", "language": "python", "code": "def save(self, filename, options=None):\n        \"\"\"Renders the barcode and saves it in `filename`.\n\n        :parameters:\n            filename : String\n                Filename to save the barcode in (without filename\n                extension).\n            options : Dict\n                The same as in `self.render`.\n\n        :returns: The full filename with extension.\n        :rtype: String\n        \"\"\"\n        output = self.render(options)\n        _filename = self.writer.save(filename, output)\n        return _filename", "code_tokens": ["def", "save", "(", "self", ",", "filename", ",", "options", "=", "None", ")", ":", "output", "=", "self", ".", "render", "(", "options", ")", "_filename", "=", "self", ".", "writer", ".", "save", "(", "filename", ",", "output", ")", "return", "_filename"], "docstring": "Renders the barcode and saves it in `filename`.\n\n        :parameters:\n            filename : String\n                Filename to save the barcode in (without filename\n                extension).\n            options : Dict\n                The same as in `self.render`.\n\n        :returns: The full filename with extension.\n        :rtype: String", "docstring_tokens": ["Renders", "the", "barcode", "and", "saves", "it", "in", "filename", "."], "sha": "79f5e866465f481982f9870c31f49a815e921c28", "url": "https://github.com/kxepal/viivakoodi/blob/79f5e866465f481982f9870c31f49a815e921c28/barcode/base.py#L54-L69", "partition": "test"}
{"repo": "Chilipp/sphinx-nbexamples", "path": "sphinx_nbexamples/__init__.py", "func_name": "NotebookProcessor.save_thumbnail", "original_string": "def save_thumbnail(self, image_path):\n        \"\"\"Save the thumbnail image\"\"\"\n        thumb_dir = os.path.join(os.path.dirname(image_path), 'thumb')\n        create_dirs(thumb_dir)\n\n        thumb_file = os.path.join(thumb_dir,\n                                  '%s_thumb.png' % self.reference)\n        if os.path.exists(image_path):\n            logger.info('Scaling %s to thumbnail %s', image_path, thumb_file)\n            self.scale_image(image_path, thumb_file, 400, 280)\n        self.thumb_file = thumb_file", "language": "python", "code": "def save_thumbnail(self, image_path):\n        \"\"\"Save the thumbnail image\"\"\"\n        thumb_dir = os.path.join(os.path.dirname(image_path), 'thumb')\n        create_dirs(thumb_dir)\n\n        thumb_file = os.path.join(thumb_dir,\n                                  '%s_thumb.png' % self.reference)\n        if os.path.exists(image_path):\n            logger.info('Scaling %s to thumbnail %s', image_path, thumb_file)\n            self.scale_image(image_path, thumb_file, 400, 280)\n        self.thumb_file = thumb_file", "code_tokens": ["def", "save_thumbnail", "(", "self", ",", "image_path", ")", ":", "thumb_dir", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "image_path", ")", ",", "'thumb'", ")", "create_dirs", "(", "thumb_dir", ")", "thumb_file", "=", "os", ".", "path", ".", "join", "(", "thumb_dir", ",", "'%s_thumb.png'", "%", "self", ".", "reference", ")", "if", "os", ".", "path", ".", "exists", "(", "image_path", ")", ":", "logger", ".", "info", "(", "'Scaling %s to thumbnail %s'", ",", "image_path", ",", "thumb_file", ")", "self", ".", "scale_image", "(", "image_path", ",", "thumb_file", ",", "400", ",", "280", ")", "self", ".", "thumb_file", "=", "thumb_file"], "docstring": "Save the thumbnail image", "docstring_tokens": ["Save", "the", "thumbnail", "image"], "sha": "08e0319ff3c70f8a931dfa8890caf48add4d0470", "url": "https://github.com/Chilipp/sphinx-nbexamples/blob/08e0319ff3c70f8a931dfa8890caf48add4d0470/sphinx_nbexamples/__init__.py#L587-L597", "partition": "test"}
{"repo": "singularityhub/sregistry-cli", "path": "sregistry/main/dropbox/share.py", "func_name": "share", "original_string": "def share(self, query, share_to=None):\n    '''share will use the client to get a shareable link for an image of choice.\n       the functions returns a url of choice to send to a recipient.\n    '''\n\n    names = parse_image_name(remove_uri(query))\n\n    # Dropbox path is the path in storage with a slash\n    dropbox_path = '/%s' % names['storage']        \n\n    # First ensure that exists\n    if self.exists(dropbox_path) is True:\n\n        # Create new shared link\n        try:\n            share = self.dbx.sharing_create_shared_link_with_settings(dropbox_path)\n\n        # Already exists!\n        except ApiError as err:\n            share = self.dbx.sharing_create_shared_link(dropbox_path)\n\n        bot.info(share.url)\n    return share.url", "language": "python", "code": "def share(self, query, share_to=None):\n    '''share will use the client to get a shareable link for an image of choice.\n       the functions returns a url of choice to send to a recipient.\n    '''\n\n    names = parse_image_name(remove_uri(query))\n\n    # Dropbox path is the path in storage with a slash\n    dropbox_path = '/%s' % names['storage']        \n\n    # First ensure that exists\n    if self.exists(dropbox_path) is True:\n\n        # Create new shared link\n        try:\n            share = self.dbx.sharing_create_shared_link_with_settings(dropbox_path)\n\n        # Already exists!\n        except ApiError as err:\n            share = self.dbx.sharing_create_shared_link(dropbox_path)\n\n        bot.info(share.url)\n    return share.url", "code_tokens": ["def", "share", "(", "self", ",", "query", ",", "share_to", "=", "None", ")", ":", "names", "=", "parse_image_name", "(", "remove_uri", "(", "query", ")", ")", "# Dropbox path is the path in storage with a slash", "dropbox_path", "=", "'/%s'", "%", "names", "[", "'storage'", "]", "# First ensure that exists", "if", "self", ".", "exists", "(", "dropbox_path", ")", "is", "True", ":", "# Create new shared link", "try", ":", "share", "=", "self", ".", "dbx", ".", "sharing_create_shared_link_with_settings", "(", "dropbox_path", ")", "# Already exists!", "except", "ApiError", "as", "err", ":", "share", "=", "self", ".", "dbx", ".", "sharing_create_shared_link", "(", "dropbox_path", ")", "bot", ".", "info", "(", "share", ".", "url", ")", "return", "share", ".", "url"], "docstring": "share will use the client to get a shareable link for an image of choice.\n       the functions returns a url of choice to send to a recipient.", "docstring_tokens": ["share", "will", "use", "the", "client", "to", "get", "a", "shareable", "link", "for", "an", "image", "of", "choice", ".", "the", "functions", "returns", "a", "url", "of", "choice", "to", "send", "to", "a", "recipient", "."], "sha": "abc96140a1d15b5e96d83432e1e0e1f4f8f36331", "url": "https://github.com/singularityhub/sregistry-cli/blob/abc96140a1d15b5e96d83432e1e0e1f4f8f36331/sregistry/main/dropbox/share.py#L16-L38", "partition": "test"}
{"repo": "LordSputnik/mutagen", "path": "mutagen/flac.py", "func_name": "FLAC.delete", "original_string": "def delete(self, filename=None):\n        \"\"\"Remove Vorbis comments from a file.\n\n        If no filename is given, the one most recently loaded is used.\n        \"\"\"\n        if filename is None:\n            filename = self.filename\n        for s in list(self.metadata_blocks):\n            if isinstance(s, VCFLACDict):\n                self.metadata_blocks.remove(s)\n                self.tags = None\n                self.save()\n                break", "language": "python", "code": "def delete(self, filename=None):\n        \"\"\"Remove Vorbis comments from a file.\n\n        If no filename is given, the one most recently loaded is used.\n        \"\"\"\n        if filename is None:\n            filename = self.filename\n        for s in list(self.metadata_blocks):\n            if isinstance(s, VCFLACDict):\n                self.metadata_blocks.remove(s)\n                self.tags = None\n                self.save()\n                break", "code_tokens": ["def", "delete", "(", "self", ",", "filename", "=", "None", ")", ":", "if", "filename", "is", "None", ":", "filename", "=", "self", ".", "filename", "for", "s", "in", "list", "(", "self", ".", "metadata_blocks", ")", ":", "if", "isinstance", "(", "s", ",", "VCFLACDict", ")", ":", "self", ".", "metadata_blocks", ".", "remove", "(", "s", ")", "self", ".", "tags", "=", "None", "self", ".", "save", "(", ")", "break"], "docstring": "Remove Vorbis comments from a file.\n\n        If no filename is given, the one most recently loaded is used.", "docstring_tokens": ["Remove", "Vorbis", "comments", "from", "a", "file", "."], "sha": "38e62c8dc35c72b16554f5dbe7c0fde91acc3411", "url": "https://github.com/LordSputnik/mutagen/blob/38e62c8dc35c72b16554f5dbe7c0fde91acc3411/mutagen/flac.py#L686-L698", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/display.py", "func_name": "waveplot", "original_string": "def waveplot(y, sr=22050, max_points=5e4, x_axis='time', offset=0.0,\n             max_sr=1000, ax=None, **kwargs):\n    '''Plot the amplitude envelope of a waveform.\n\n    If `y` is monophonic, a filled curve is drawn between `[-abs(y), abs(y)]`.\n\n    If `y` is stereo, the curve is drawn between `[-abs(y[1]), abs(y[0])]`,\n    so that the left and right channels are drawn above and below the axis,\n    respectively.\n\n    Long signals (`duration >= max_points`) are down-sampled to at\n    most `max_sr` before plotting.\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,) or (2,n)]\n        audio time series (mono or stereo)\n\n    sr : number > 0 [scalar]\n        sampling rate of `y`\n\n    max_points : postive number or None\n        Maximum number of time-points to plot: if `max_points` exceeds\n        the duration of `y`, then `y` is downsampled.\n\n        If `None`, no downsampling is performed.\n\n    x_axis : str {'time', 'off', 'none'} or None\n        If 'time', the x-axis is given time tick-marks.\n\n    ax : matplotlib.axes.Axes or None\n        Axes to plot on instead of the default `plt.gca()`.\n\n    offset : float\n        Horizontal offset (in seconds) to start the waveform plot\n\n    max_sr : number > 0 [scalar]\n        Maximum sampling rate for the visualization\n\n    kwargs\n        Additional keyword arguments to `matplotlib.pyplot.fill_between`\n\n    Returns\n    -------\n    pc : matplotlib.collections.PolyCollection\n        The PolyCollection created by `fill_between`.\n\n    See also\n    --------\n    librosa.core.resample\n    matplotlib.pyplot.fill_between\n\n\n    Examples\n    --------\n    Plot a monophonic waveform\n\n    >>> import matplotlib.pyplot as plt\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(), duration=10)\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> librosa.display.waveplot(y, sr=sr)\n    >>> plt.title('Monophonic')\n\n    Or a stereo waveform\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      mono=False, duration=10)\n    >>> plt.subplot(3, 1, 2)\n    >>> librosa.display.waveplot(y, sr=sr)\n    >>> plt.title('Stereo')\n\n    Or harmonic and percussive components with transparency\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(), duration=10)\n    >>> y_harm, y_perc = librosa.effects.hpss(y)\n    >>> plt.subplot(3, 1, 3)\n    >>> librosa.display.waveplot(y_harm, sr=sr, alpha=0.25)\n    >>> librosa.display.waveplot(y_perc, sr=sr, color='r', alpha=0.5)\n    >>> plt.title('Harmonic + Percussive')\n    >>> plt.tight_layout()\n    '''\n\n    util.valid_audio(y, mono=False)\n\n    if not (isinstance(max_sr, int) and max_sr > 0):\n        raise ParameterError('max_sr must be a non-negative integer')\n\n    target_sr = sr\n    hop_length = 1\n\n    if max_points is not None:\n        if max_points <= 0:\n            raise ParameterError('max_points must be strictly positive')\n\n        if max_points < y.shape[-1]:\n            target_sr = min(max_sr, (sr * y.shape[-1]) // max_points)\n\n        hop_length = sr // target_sr\n\n        if y.ndim == 1:\n            y = __envelope(y, hop_length)\n        else:\n            y = np.vstack([__envelope(_, hop_length) for _ in y])\n\n    if y.ndim > 1:\n        y_top = y[0]\n        y_bottom = -y[1]\n    else:\n        y_top = y\n        y_bottom = -y\n\n    axes = __check_axes(ax)\n\n    kwargs.setdefault('color', next(axes._get_lines.prop_cycler)['color'])\n\n    locs = offset + core.frames_to_time(np.arange(len(y_top)),\n                                        sr=sr,\n                                        hop_length=hop_length)\n\n    out = axes.fill_between(locs, y_bottom, y_top, **kwargs)\n\n    axes.set_xlim([locs.min(), locs.max()])\n    if x_axis == 'time':\n        axes.xaxis.set_major_formatter(TimeFormatter(lag=False))\n        axes.xaxis.set_label_text('Time')\n    elif x_axis is None or x_axis in ['off', 'none']:\n        axes.set_xticks([])\n    else:\n        raise ParameterError('Unknown x_axis value: {}'.format(x_axis))\n\n    return out", "language": "python", "code": "def waveplot(y, sr=22050, max_points=5e4, x_axis='time', offset=0.0,\n             max_sr=1000, ax=None, **kwargs):\n    '''Plot the amplitude envelope of a waveform.\n\n    If `y` is monophonic, a filled curve is drawn between `[-abs(y), abs(y)]`.\n\n    If `y` is stereo, the curve is drawn between `[-abs(y[1]), abs(y[0])]`,\n    so that the left and right channels are drawn above and below the axis,\n    respectively.\n\n    Long signals (`duration >= max_points`) are down-sampled to at\n    most `max_sr` before plotting.\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,) or (2,n)]\n        audio time series (mono or stereo)\n\n    sr : number > 0 [scalar]\n        sampling rate of `y`\n\n    max_points : postive number or None\n        Maximum number of time-points to plot: if `max_points` exceeds\n        the duration of `y`, then `y` is downsampled.\n\n        If `None`, no downsampling is performed.\n\n    x_axis : str {'time', 'off', 'none'} or None\n        If 'time', the x-axis is given time tick-marks.\n\n    ax : matplotlib.axes.Axes or None\n        Axes to plot on instead of the default `plt.gca()`.\n\n    offset : float\n        Horizontal offset (in seconds) to start the waveform plot\n\n    max_sr : number > 0 [scalar]\n        Maximum sampling rate for the visualization\n\n    kwargs\n        Additional keyword arguments to `matplotlib.pyplot.fill_between`\n\n    Returns\n    -------\n    pc : matplotlib.collections.PolyCollection\n        The PolyCollection created by `fill_between`.\n\n    See also\n    --------\n    librosa.core.resample\n    matplotlib.pyplot.fill_between\n\n\n    Examples\n    --------\n    Plot a monophonic waveform\n\n    >>> import matplotlib.pyplot as plt\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(), duration=10)\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> librosa.display.waveplot(y, sr=sr)\n    >>> plt.title('Monophonic')\n\n    Or a stereo waveform\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      mono=False, duration=10)\n    >>> plt.subplot(3, 1, 2)\n    >>> librosa.display.waveplot(y, sr=sr)\n    >>> plt.title('Stereo')\n\n    Or harmonic and percussive components with transparency\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(), duration=10)\n    >>> y_harm, y_perc = librosa.effects.hpss(y)\n    >>> plt.subplot(3, 1, 3)\n    >>> librosa.display.waveplot(y_harm, sr=sr, alpha=0.25)\n    >>> librosa.display.waveplot(y_perc, sr=sr, color='r', alpha=0.5)\n    >>> plt.title('Harmonic + Percussive')\n    >>> plt.tight_layout()\n    '''\n\n    util.valid_audio(y, mono=False)\n\n    if not (isinstance(max_sr, int) and max_sr > 0):\n        raise ParameterError('max_sr must be a non-negative integer')\n\n    target_sr = sr\n    hop_length = 1\n\n    if max_points is not None:\n        if max_points <= 0:\n            raise ParameterError('max_points must be strictly positive')\n\n        if max_points < y.shape[-1]:\n            target_sr = min(max_sr, (sr * y.shape[-1]) // max_points)\n\n        hop_length = sr // target_sr\n\n        if y.ndim == 1:\n            y = __envelope(y, hop_length)\n        else:\n            y = np.vstack([__envelope(_, hop_length) for _ in y])\n\n    if y.ndim > 1:\n        y_top = y[0]\n        y_bottom = -y[1]\n    else:\n        y_top = y\n        y_bottom = -y\n\n    axes = __check_axes(ax)\n\n    kwargs.setdefault('color', next(axes._get_lines.prop_cycler)['color'])\n\n    locs = offset + core.frames_to_time(np.arange(len(y_top)),\n                                        sr=sr,\n                                        hop_length=hop_length)\n\n    out = axes.fill_between(locs, y_bottom, y_top, **kwargs)\n\n    axes.set_xlim([locs.min(), locs.max()])\n    if x_axis == 'time':\n        axes.xaxis.set_major_formatter(TimeFormatter(lag=False))\n        axes.xaxis.set_label_text('Time')\n    elif x_axis is None or x_axis in ['off', 'none']:\n        axes.set_xticks([])\n    else:\n        raise ParameterError('Unknown x_axis value: {}'.format(x_axis))\n\n    return out", "code_tokens": ["def", "waveplot", "(", "y", ",", "sr", "=", "22050", ",", "max_points", "=", "5e4", ",", "x_axis", "=", "'time'", ",", "offset", "=", "0.0", ",", "max_sr", "=", "1000", ",", "ax", "=", "None", ",", "*", "*", "kwargs", ")", ":", "util", ".", "valid_audio", "(", "y", ",", "mono", "=", "False", ")", "if", "not", "(", "isinstance", "(", "max_sr", ",", "int", ")", "and", "max_sr", ">", "0", ")", ":", "raise", "ParameterError", "(", "'max_sr must be a non-negative integer'", ")", "target_sr", "=", "sr", "hop_length", "=", "1", "if", "max_points", "is", "not", "None", ":", "if", "max_points", "<=", "0", ":", "raise", "ParameterError", "(", "'max_points must be strictly positive'", ")", "if", "max_points", "<", "y", ".", "shape", "[", "-", "1", "]", ":", "target_sr", "=", "min", "(", "max_sr", ",", "(", "sr", "*", "y", ".", "shape", "[", "-", "1", "]", ")", "//", "max_points", ")", "hop_length", "=", "sr", "//", "target_sr", "if", "y", ".", "ndim", "==", "1", ":", "y", "=", "__envelope", "(", "y", ",", "hop_length", ")", "else", ":", "y", "=", "np", ".", "vstack", "(", "[", "__envelope", "(", "_", ",", "hop_length", ")", "for", "_", "in", "y", "]", ")", "if", "y", ".", "ndim", ">", "1", ":", "y_top", "=", "y", "[", "0", "]", "y_bottom", "=", "-", "y", "[", "1", "]", "else", ":", "y_top", "=", "y", "y_bottom", "=", "-", "y", "axes", "=", "__check_axes", "(", "ax", ")", "kwargs", ".", "setdefault", "(", "'color'", ",", "next", "(", "axes", ".", "_get_lines", ".", "prop_cycler", ")", "[", "'color'", "]", ")", "locs", "=", "offset", "+", "core", ".", "frames_to_time", "(", "np", ".", "arange", "(", "len", "(", "y_top", ")", ")", ",", "sr", "=", "sr", ",", "hop_length", "=", "hop_length", ")", "out", "=", "axes", ".", "fill_between", "(", "locs", ",", "y_bottom", ",", "y_top", ",", "*", "*", "kwargs", ")", "axes", ".", "set_xlim", "(", "[", "locs", ".", "min", "(", ")", ",", "locs", ".", "max", "(", ")", "]", ")", "if", "x_axis", "==", "'time'", ":", "axes", ".", "xaxis", ".", "set_major_formatter", "(", "TimeFormatter", "(", "lag", "=", "False", ")", ")", "axes", ".", "xaxis", ".", "set_label_text", "(", "'Time'", ")", "elif", "x_axis", "is", "None", "or", "x_axis", "in", "[", "'off'", ",", "'none'", "]", ":", "axes", ".", "set_xticks", "(", "[", "]", ")", "else", ":", "raise", "ParameterError", "(", "'Unknown x_axis value: {}'", ".", "format", "(", "x_axis", ")", ")", "return", "out"], "docstring": "Plot the amplitude envelope of a waveform.\n\n    If `y` is monophonic, a filled curve is drawn between `[-abs(y), abs(y)]`.\n\n    If `y` is stereo, the curve is drawn between `[-abs(y[1]), abs(y[0])]`,\n    so that the left and right channels are drawn above and below the axis,\n    respectively.\n\n    Long signals (`duration >= max_points`) are down-sampled to at\n    most `max_sr` before plotting.\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,) or (2,n)]\n        audio time series (mono or stereo)\n\n    sr : number > 0 [scalar]\n        sampling rate of `y`\n\n    max_points : postive number or None\n        Maximum number of time-points to plot: if `max_points` exceeds\n        the duration of `y`, then `y` is downsampled.\n\n        If `None`, no downsampling is performed.\n\n    x_axis : str {'time', 'off', 'none'} or None\n        If 'time', the x-axis is given time tick-marks.\n\n    ax : matplotlib.axes.Axes or None\n        Axes to plot on instead of the default `plt.gca()`.\n\n    offset : float\n        Horizontal offset (in seconds) to start the waveform plot\n\n    max_sr : number > 0 [scalar]\n        Maximum sampling rate for the visualization\n\n    kwargs\n        Additional keyword arguments to `matplotlib.pyplot.fill_between`\n\n    Returns\n    -------\n    pc : matplotlib.collections.PolyCollection\n        The PolyCollection created by `fill_between`.\n\n    See also\n    --------\n    librosa.core.resample\n    matplotlib.pyplot.fill_between\n\n\n    Examples\n    --------\n    Plot a monophonic waveform\n\n    >>> import matplotlib.pyplot as plt\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(), duration=10)\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> librosa.display.waveplot(y, sr=sr)\n    >>> plt.title('Monophonic')\n\n    Or a stereo waveform\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      mono=False, duration=10)\n    >>> plt.subplot(3, 1, 2)\n    >>> librosa.display.waveplot(y, sr=sr)\n    >>> plt.title('Stereo')\n\n    Or harmonic and percussive components with transparency\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(), duration=10)\n    >>> y_harm, y_perc = librosa.effects.hpss(y)\n    >>> plt.subplot(3, 1, 3)\n    >>> librosa.display.waveplot(y_harm, sr=sr, alpha=0.25)\n    >>> librosa.display.waveplot(y_perc, sr=sr, color='r', alpha=0.5)\n    >>> plt.title('Harmonic + Percussive')\n    >>> plt.tight_layout()", "docstring_tokens": ["Plot", "the", "amplitude", "envelope", "of", "a", "waveform", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/display.py#L357-L488", "partition": "test"}
{"repo": "umich-brcf-bioinf/Jacquard", "path": "jacquard/jacquard.py", "func_name": "_JacquardArgumentParser.error", "original_string": "def error(self, message):\n        '''Suppress default exit behavior'''\n        message = self._remessage_invalid_subparser(message)\n        raise utils.UsageError(message)", "language": "python", "code": "def error(self, message):\n        '''Suppress default exit behavior'''\n        message = self._remessage_invalid_subparser(message)\n        raise utils.UsageError(message)", "code_tokens": ["def", "error", "(", "self", ",", "message", ")", ":", "message", "=", "self", ".", "_remessage_invalid_subparser", "(", "message", ")", "raise", "utils", ".", "UsageError", "(", "message", ")"], "docstring": "Suppress default exit behavior", "docstring_tokens": ["Suppress", "default", "exit", "behavior"], "sha": "83dd61dd2b5e4110468493beec7bc121e6cb3cd1", "url": "https://github.com/umich-brcf-bioinf/Jacquard/blob/83dd61dd2b5e4110468493beec7bc121e6cb3cd1/jacquard/jacquard.py#L81-L84", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/adapter/mongo/institute.py", "func_name": "InstituteHandler.update_institute", "original_string": "def update_institute(self, internal_id, sanger_recipient=None, coverage_cutoff=None, \n                         frequency_cutoff=None, display_name=None, remove_sanger=None,\n                         phenotype_groups=None, group_abbreviations=None, add_groups=None):\n        \"\"\"Update the information for an institute\n\n        Args:\n            internal_id(str): The internal institute id\n            sanger_recipient(str): Email adress to add for sanger order\n            coverage_cutoff(int): Update coverage cutoff\n            frequency_cutoff(float): New frequency cutoff\n            display_name(str): New display name\n            remove_sanger(str): Email adress for sanger user to be removed\n            phenotype_groups(iterable(str)): New phenotype groups\n            group_abbreviations(iterable(str))\n            add_groups: If groups should be added. If False replace groups\n        \n        Returns:\n            updated_institute(dict)\n                     \n        \"\"\"\n        add_groups = add_groups or False\n        institute_obj = self.institute(internal_id)\n        if not institute_obj:\n            raise IntegrityError(\"Institute {} does not exist in database\".format(internal_id))\n        \n        updates = {}\n        updated_institute = institute_obj\n\n        if sanger_recipient:\n            user_obj = self.user(sanger_recipient)\n            if not user_obj:\n                raise IntegrityError(\"user {} does not exist in database\".format(sanger_recipient))\n                \n            LOG.info(\"Updating sanger recipients for institute: {0} with {1}\".format(\n                     internal_id, sanger_recipient))\n            updates['$push'] = {'sanger_recipients':remove_sanger}\n\n        if remove_sanger:\n            LOG.info(\"Removing sanger recipient {0} from institute: {1}\".format(\n                     remove_sanger, internal_id))\n            updates['$pull'] = {'sanger_recipients':remove_sanger}\n\n        if coverage_cutoff:\n            LOG.info(\"Updating coverage cutoff for institute: {0} to {1}\".format(\n                            internal_id, coverage_cutoff))\n            updates['$set'] = {'coverage_cutoff': coverage_cutoff}\n        \n        if frequency_cutoff:\n            LOG.info(\"Updating frequency cutoff for institute: {0} to {1}\".format(\n                            internal_id, frequency_cutoff))\n            if not '$set' in updates:\n                updates['$set'] = {}\n            updates['$set'] = {'frequency_cutoff': frequency_cutoff}\n\n        if display_name:\n            LOG.info(\"Updating display name for institute: {0} to {1}\".format(\n                            internal_id, display_name))\n            if not '$set' in updates:\n                updates['$set'] = {}\n            updates['$set'] = {'display_name': display_name}\n\n        if phenotype_groups:\n            if group_abbreviations:\n                group_abbreviations = list(group_abbreviations)\n            existing_groups = {}\n            if add_groups:\n                existing_groups = institute_obj.get('phenotype_groups', PHENOTYPE_GROUPS)\n\n            for i,hpo_term in enumerate(phenotype_groups):\n                hpo_obj = self.hpo_term(hpo_term)\n                if not hpo_obj:\n                    raise IntegrityError(\"Term {} does not exist\".format(hpo_term))\n                hpo_id = hpo_obj['hpo_id']\n                description = hpo_obj['description']\n                abbreviation = None\n                if group_abbreviations:\n                    abbreviation = group_abbreviations[i]\n                existing_groups[hpo_term] = {'name': description, 'abbr':abbreviation}\n            updates['$set'] = {'phenotype_groups': existing_groups}\n        \n        if updates:\n            if not '$set' in updates:\n                updates['$set'] = {}\n            \n            updates['$set']['updated_at'] = datetime.now()\n            \n            updated_institute = self.institute_collection.find_one_and_update(\n                {'_id':internal_id}, updates, return_document = pymongo.ReturnDocument.AFTER)\n            \n            LOG.info(\"Institute updated\")\n        \n        return updated_institute", "language": "python", "code": "def update_institute(self, internal_id, sanger_recipient=None, coverage_cutoff=None, \n                         frequency_cutoff=None, display_name=None, remove_sanger=None,\n                         phenotype_groups=None, group_abbreviations=None, add_groups=None):\n        \"\"\"Update the information for an institute\n\n        Args:\n            internal_id(str): The internal institute id\n            sanger_recipient(str): Email adress to add for sanger order\n            coverage_cutoff(int): Update coverage cutoff\n            frequency_cutoff(float): New frequency cutoff\n            display_name(str): New display name\n            remove_sanger(str): Email adress for sanger user to be removed\n            phenotype_groups(iterable(str)): New phenotype groups\n            group_abbreviations(iterable(str))\n            add_groups: If groups should be added. If False replace groups\n        \n        Returns:\n            updated_institute(dict)\n                     \n        \"\"\"\n        add_groups = add_groups or False\n        institute_obj = self.institute(internal_id)\n        if not institute_obj:\n            raise IntegrityError(\"Institute {} does not exist in database\".format(internal_id))\n        \n        updates = {}\n        updated_institute = institute_obj\n\n        if sanger_recipient:\n            user_obj = self.user(sanger_recipient)\n            if not user_obj:\n                raise IntegrityError(\"user {} does not exist in database\".format(sanger_recipient))\n                \n            LOG.info(\"Updating sanger recipients for institute: {0} with {1}\".format(\n                     internal_id, sanger_recipient))\n            updates['$push'] = {'sanger_recipients':remove_sanger}\n\n        if remove_sanger:\n            LOG.info(\"Removing sanger recipient {0} from institute: {1}\".format(\n                     remove_sanger, internal_id))\n            updates['$pull'] = {'sanger_recipients':remove_sanger}\n\n        if coverage_cutoff:\n            LOG.info(\"Updating coverage cutoff for institute: {0} to {1}\".format(\n                            internal_id, coverage_cutoff))\n            updates['$set'] = {'coverage_cutoff': coverage_cutoff}\n        \n        if frequency_cutoff:\n            LOG.info(\"Updating frequency cutoff for institute: {0} to {1}\".format(\n                            internal_id, frequency_cutoff))\n            if not '$set' in updates:\n                updates['$set'] = {}\n            updates['$set'] = {'frequency_cutoff': frequency_cutoff}\n\n        if display_name:\n            LOG.info(\"Updating display name for institute: {0} to {1}\".format(\n                            internal_id, display_name))\n            if not '$set' in updates:\n                updates['$set'] = {}\n            updates['$set'] = {'display_name': display_name}\n\n        if phenotype_groups:\n            if group_abbreviations:\n                group_abbreviations = list(group_abbreviations)\n            existing_groups = {}\n            if add_groups:\n                existing_groups = institute_obj.get('phenotype_groups', PHENOTYPE_GROUPS)\n\n            for i,hpo_term in enumerate(phenotype_groups):\n                hpo_obj = self.hpo_term(hpo_term)\n                if not hpo_obj:\n                    raise IntegrityError(\"Term {} does not exist\".format(hpo_term))\n                hpo_id = hpo_obj['hpo_id']\n                description = hpo_obj['description']\n                abbreviation = None\n                if group_abbreviations:\n                    abbreviation = group_abbreviations[i]\n                existing_groups[hpo_term] = {'name': description, 'abbr':abbreviation}\n            updates['$set'] = {'phenotype_groups': existing_groups}\n        \n        if updates:\n            if not '$set' in updates:\n                updates['$set'] = {}\n            \n            updates['$set']['updated_at'] = datetime.now()\n            \n            updated_institute = self.institute_collection.find_one_and_update(\n                {'_id':internal_id}, updates, return_document = pymongo.ReturnDocument.AFTER)\n            \n            LOG.info(\"Institute updated\")\n        \n        return updated_institute", "code_tokens": ["def", "update_institute", "(", "self", ",", "internal_id", ",", "sanger_recipient", "=", "None", ",", "coverage_cutoff", "=", "None", ",", "frequency_cutoff", "=", "None", ",", "display_name", "=", "None", ",", "remove_sanger", "=", "None", ",", "phenotype_groups", "=", "None", ",", "group_abbreviations", "=", "None", ",", "add_groups", "=", "None", ")", ":", "add_groups", "=", "add_groups", "or", "False", "institute_obj", "=", "self", ".", "institute", "(", "internal_id", ")", "if", "not", "institute_obj", ":", "raise", "IntegrityError", "(", "\"Institute {} does not exist in database\"", ".", "format", "(", "internal_id", ")", ")", "updates", "=", "{", "}", "updated_institute", "=", "institute_obj", "if", "sanger_recipient", ":", "user_obj", "=", "self", ".", "user", "(", "sanger_recipient", ")", "if", "not", "user_obj", ":", "raise", "IntegrityError", "(", "\"user {} does not exist in database\"", ".", "format", "(", "sanger_recipient", ")", ")", "LOG", ".", "info", "(", "\"Updating sanger recipients for institute: {0} with {1}\"", ".", "format", "(", "internal_id", ",", "sanger_recipient", ")", ")", "updates", "[", "'$push'", "]", "=", "{", "'sanger_recipients'", ":", "remove_sanger", "}", "if", "remove_sanger", ":", "LOG", ".", "info", "(", "\"Removing sanger recipient {0} from institute: {1}\"", ".", "format", "(", "remove_sanger", ",", "internal_id", ")", ")", "updates", "[", "'$pull'", "]", "=", "{", "'sanger_recipients'", ":", "remove_sanger", "}", "if", "coverage_cutoff", ":", "LOG", ".", "info", "(", "\"Updating coverage cutoff for institute: {0} to {1}\"", ".", "format", "(", "internal_id", ",", "coverage_cutoff", ")", ")", "updates", "[", "'$set'", "]", "=", "{", "'coverage_cutoff'", ":", "coverage_cutoff", "}", "if", "frequency_cutoff", ":", "LOG", ".", "info", "(", "\"Updating frequency cutoff for institute: {0} to {1}\"", ".", "format", "(", "internal_id", ",", "frequency_cutoff", ")", ")", "if", "not", "'$set'", "in", "updates", ":", "updates", "[", "'$set'", "]", "=", "{", "}", "updates", "[", "'$set'", "]", "=", "{", "'frequency_cutoff'", ":", "frequency_cutoff", "}", "if", "display_name", ":", "LOG", ".", "info", "(", "\"Updating display name for institute: {0} to {1}\"", ".", "format", "(", "internal_id", ",", "display_name", ")", ")", "if", "not", "'$set'", "in", "updates", ":", "updates", "[", "'$set'", "]", "=", "{", "}", "updates", "[", "'$set'", "]", "=", "{", "'display_name'", ":", "display_name", "}", "if", "phenotype_groups", ":", "if", "group_abbreviations", ":", "group_abbreviations", "=", "list", "(", "group_abbreviations", ")", "existing_groups", "=", "{", "}", "if", "add_groups", ":", "existing_groups", "=", "institute_obj", ".", "get", "(", "'phenotype_groups'", ",", "PHENOTYPE_GROUPS", ")", "for", "i", ",", "hpo_term", "in", "enumerate", "(", "phenotype_groups", ")", ":", "hpo_obj", "=", "self", ".", "hpo_term", "(", "hpo_term", ")", "if", "not", "hpo_obj", ":", "raise", "IntegrityError", "(", "\"Term {} does not exist\"", ".", "format", "(", "hpo_term", ")", ")", "hpo_id", "=", "hpo_obj", "[", "'hpo_id'", "]", "description", "=", "hpo_obj", "[", "'description'", "]", "abbreviation", "=", "None", "if", "group_abbreviations", ":", "abbreviation", "=", "group_abbreviations", "[", "i", "]", "existing_groups", "[", "hpo_term", "]", "=", "{", "'name'", ":", "description", ",", "'abbr'", ":", "abbreviation", "}", "updates", "[", "'$set'", "]", "=", "{", "'phenotype_groups'", ":", "existing_groups", "}", "if", "updates", ":", "if", "not", "'$set'", "in", "updates", ":", "updates", "[", "'$set'", "]", "=", "{", "}", "updates", "[", "'$set'", "]", "[", "'updated_at'", "]", "=", "datetime", ".", "now", "(", ")", "updated_institute", "=", "self", ".", "institute_collection", ".", "find_one_and_update", "(", "{", "'_id'", ":", "internal_id", "}", ",", "updates", ",", "return_document", "=", "pymongo", ".", "ReturnDocument", ".", "AFTER", ")", "LOG", ".", "info", "(", "\"Institute updated\"", ")", "return", "updated_institute"], "docstring": "Update the information for an institute\n\n        Args:\n            internal_id(str): The internal institute id\n            sanger_recipient(str): Email adress to add for sanger order\n            coverage_cutoff(int): Update coverage cutoff\n            frequency_cutoff(float): New frequency cutoff\n            display_name(str): New display name\n            remove_sanger(str): Email adress for sanger user to be removed\n            phenotype_groups(iterable(str)): New phenotype groups\n            group_abbreviations(iterable(str))\n            add_groups: If groups should be added. If False replace groups\n        \n        Returns:\n            updated_institute(dict)", "docstring_tokens": ["Update", "the", "information", "for", "an", "institute"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/adapter/mongo/institute.py#L38-L129", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/lbfgs.py", "func_name": "minimize", "original_string": "def minimize(value_and_gradients_function,\n             initial_position,\n             num_correction_pairs=10,\n             tolerance=1e-8,\n             x_tolerance=0,\n             f_relative_tolerance=0,\n             initial_inverse_hessian_estimate=None,\n             max_iterations=50,\n             parallel_iterations=1,\n             stopping_condition=None,\n             name=None):\n  \"\"\"Applies the L-BFGS algorithm to minimize a differentiable function.\n\n  Performs unconstrained minimization of a differentiable function using the\n  L-BFGS scheme. See [Nocedal and Wright(2006)][1] for details of the algorithm.\n\n  ### Usage:\n\n  The following example demonstrates the L-BFGS optimizer attempting to find the\n  minimum for a simple high-dimensional quadratic objective function.\n\n  ```python\n    # A high-dimensional quadratic bowl.\n    ndims = 60\n    minimum = np.ones([ndims], dtype='float64')\n    scales = np.arange(ndims, dtype='float64') + 1.0\n\n    # The objective function and the gradient.\n    def quadratic(x):\n      value = tf.reduce_sum(scales * (x - minimum) ** 2)\n      return value, tf.gradients(value, x)[0]\n\n    start = np.arange(ndims, 0, -1, dtype='float64')\n    optim_results = tfp.optimizer.lbfgs_minimize(\n        quadratic, initial_position=start, num_correction_pairs=10,\n        tolerance=1e-8)\n\n    with tf.Session() as session:\n      results = session.run(optim_results)\n      # Check that the search converged\n      assert(results.converged)\n      # Check that the argmin is close to the actual value.\n      np.testing.assert_allclose(results.position, minimum)\n  ```\n\n  ### References:\n\n  [1] Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series\n      in Operations Research. pp 176-180. 2006\n\n  http://pages.mtu.edu/~struther/Courses/OLD/Sp2013/5630/Jorge_Nocedal_Numerical_optimization_267490.pdf\n\n  Args:\n    value_and_gradients_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a tuple of `Tensor`s of real dtype containing\n      the value of the function and its gradient at that point. The function\n      to be minimized. The input is of shape `[..., n]`, where `n` is the size\n      of the domain of input points, and all others are batching dimensions.\n      The first component of the return value is a real `Tensor` of matching\n      shape `[...]`. The second component (the gradient) is also of shape\n      `[..., n]` like the input value to the function.\n    initial_position: Real `Tensor` of shape `[..., n]`. The starting point, or\n      points when using batching dimensions, of the search procedure. At these\n      points the function value and the gradient norm should be finite.\n    num_correction_pairs: Positive integer. Specifies the maximum number of\n      (position_delta, gradient_delta) correction pairs to keep as implicit\n      approximation of the Hessian matrix.\n    tolerance: Scalar `Tensor` of real dtype. Specifies the gradient tolerance\n      for the procedure. If the supremum norm of the gradient vector is below\n      this number, the algorithm is stopped.\n    x_tolerance: Scalar `Tensor` of real dtype. If the absolute change in the\n      position between one iteration and the next is smaller than this number,\n      the algorithm is stopped.\n    f_relative_tolerance: Scalar `Tensor` of real dtype. If the relative change\n      in the objective value between one iteration and the next is smaller\n      than this value, the algorithm is stopped.\n    initial_inverse_hessian_estimate: None. Option currently not supported.\n    max_iterations: Scalar positive int32 `Tensor`. The maximum number of\n      iterations for L-BFGS updates.\n    parallel_iterations: Positive integer. The number of iterations allowed to\n      run in parallel.\n    stopping_condition: (Optional) A Python function that takes as input two\n      Boolean tensors of shape `[...]`, and returns a Boolean scalar tensor.\n      The input tensors are `converged` and `failed`, indicating the current\n      status of each respective batch member; the return value states whether\n      the algorithm should stop. The default is tfp.optimizer.converged_all\n      which only stops when all batch members have either converged or failed.\n      An alternative is tfp.optimizer.converged_any which stops as soon as one\n      batch member has converged, or when all have failed.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'minimize' is used.\n\n  Returns:\n    optimizer_results: A namedtuple containing the following items:\n      converged: Scalar boolean tensor indicating whether the minimum was\n        found within tolerance.\n      failed:  Scalar boolean tensor indicating whether a line search\n        step failed to find a suitable step size satisfying Wolfe\n        conditions. In the absence of any constraints on the\n        number of objective evaluations permitted, this value will\n        be the complement of `converged`. However, if there is\n        a constraint and the search stopped due to available\n        evaluations being exhausted, both `failed` and `converged`\n        will be simultaneously False.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A tensor containing the last argument value found\n        during the search. If the search converged, then\n        this value is the argmin of the objective function.\n      objective_value: A tensor containing the value of the objective\n        function at the `position`. If the search converged, then this is\n        the (local) minimum of the objective function.\n      objective_gradient: A tensor containing the gradient of the objective\n        function at the `position`. If the search converged the\n        max-norm of this tensor should be below the tolerance.\n      position_deltas: A tensor encoding information about the latest\n        changes in `position` during the algorithm execution.\n      gradient_deltas: A tensor encoding information about the latest\n        changes in `objective_gradient` during the algorithm execution.\n  \"\"\"\n  if initial_inverse_hessian_estimate is not None:\n    raise NotImplementedError(\n        'Support of initial_inverse_hessian_estimate arg not yet implemented')\n\n  if stopping_condition is None:\n    stopping_condition = bfgs_utils.converged_all\n\n  with tf.compat.v1.name_scope(name, 'minimize', [initial_position, tolerance]):\n    initial_position = tf.convert_to_tensor(\n        value=initial_position, name='initial_position')\n    dtype = initial_position.dtype.base_dtype\n    tolerance = tf.convert_to_tensor(\n        value=tolerance, dtype=dtype, name='grad_tolerance')\n    f_relative_tolerance = tf.convert_to_tensor(\n        value=f_relative_tolerance, dtype=dtype, name='f_relative_tolerance')\n    x_tolerance = tf.convert_to_tensor(\n        value=x_tolerance, dtype=dtype, name='x_tolerance')\n    max_iterations = tf.convert_to_tensor(\n        value=max_iterations, name='max_iterations')\n\n    # The `state` here is a `LBfgsOptimizerResults` tuple with values for the\n    # current state of the algorithm computation.\n    def _cond(state):\n      \"\"\"Continue if iterations remain and stopping condition is not met.\"\"\"\n      return ((state.num_iterations < max_iterations) &\n              tf.logical_not(stopping_condition(state.converged, state.failed)))\n\n    def _body(current_state):\n      \"\"\"Main optimization loop.\"\"\"\n      search_direction = _get_search_direction(current_state)\n\n      # TODO(b/120134934): Check if the derivative at the start point is not\n      # negative, if so then reset position/gradient deltas and recompute\n      # search direction.\n\n      next_state = bfgs_utils.line_search_step(\n          current_state,\n          value_and_gradients_function, search_direction,\n          tolerance, f_relative_tolerance, x_tolerance, stopping_condition)\n\n      # If not failed or converged, update the Hessian estimate.\n      should_update = ~(next_state.converged | next_state.failed)\n      state_after_inv_hessian_update = bfgs_utils.update_fields(\n          next_state,\n          position_deltas=_queue_push(\n              current_state.position_deltas, should_update,\n              next_state.position - current_state.position),\n          gradient_deltas=_queue_push(\n              current_state.gradient_deltas, should_update,\n              next_state.objective_gradient - current_state.objective_gradient))\n      return [state_after_inv_hessian_update]\n\n    initial_state = _get_initial_state(value_and_gradients_function,\n                                       initial_position,\n                                       num_correction_pairs,\n                                       tolerance)\n    return tf.while_loop(\n        cond=_cond,\n        body=_body,\n        loop_vars=[initial_state],\n        parallel_iterations=parallel_iterations)[0]", "language": "python", "code": "def minimize(value_and_gradients_function,\n             initial_position,\n             num_correction_pairs=10,\n             tolerance=1e-8,\n             x_tolerance=0,\n             f_relative_tolerance=0,\n             initial_inverse_hessian_estimate=None,\n             max_iterations=50,\n             parallel_iterations=1,\n             stopping_condition=None,\n             name=None):\n  \"\"\"Applies the L-BFGS algorithm to minimize a differentiable function.\n\n  Performs unconstrained minimization of a differentiable function using the\n  L-BFGS scheme. See [Nocedal and Wright(2006)][1] for details of the algorithm.\n\n  ### Usage:\n\n  The following example demonstrates the L-BFGS optimizer attempting to find the\n  minimum for a simple high-dimensional quadratic objective function.\n\n  ```python\n    # A high-dimensional quadratic bowl.\n    ndims = 60\n    minimum = np.ones([ndims], dtype='float64')\n    scales = np.arange(ndims, dtype='float64') + 1.0\n\n    # The objective function and the gradient.\n    def quadratic(x):\n      value = tf.reduce_sum(scales * (x - minimum) ** 2)\n      return value, tf.gradients(value, x)[0]\n\n    start = np.arange(ndims, 0, -1, dtype='float64')\n    optim_results = tfp.optimizer.lbfgs_minimize(\n        quadratic, initial_position=start, num_correction_pairs=10,\n        tolerance=1e-8)\n\n    with tf.Session() as session:\n      results = session.run(optim_results)\n      # Check that the search converged\n      assert(results.converged)\n      # Check that the argmin is close to the actual value.\n      np.testing.assert_allclose(results.position, minimum)\n  ```\n\n  ### References:\n\n  [1] Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series\n      in Operations Research. pp 176-180. 2006\n\n  http://pages.mtu.edu/~struther/Courses/OLD/Sp2013/5630/Jorge_Nocedal_Numerical_optimization_267490.pdf\n\n  Args:\n    value_and_gradients_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a tuple of `Tensor`s of real dtype containing\n      the value of the function and its gradient at that point. The function\n      to be minimized. The input is of shape `[..., n]`, where `n` is the size\n      of the domain of input points, and all others are batching dimensions.\n      The first component of the return value is a real `Tensor` of matching\n      shape `[...]`. The second component (the gradient) is also of shape\n      `[..., n]` like the input value to the function.\n    initial_position: Real `Tensor` of shape `[..., n]`. The starting point, or\n      points when using batching dimensions, of the search procedure. At these\n      points the function value and the gradient norm should be finite.\n    num_correction_pairs: Positive integer. Specifies the maximum number of\n      (position_delta, gradient_delta) correction pairs to keep as implicit\n      approximation of the Hessian matrix.\n    tolerance: Scalar `Tensor` of real dtype. Specifies the gradient tolerance\n      for the procedure. If the supremum norm of the gradient vector is below\n      this number, the algorithm is stopped.\n    x_tolerance: Scalar `Tensor` of real dtype. If the absolute change in the\n      position between one iteration and the next is smaller than this number,\n      the algorithm is stopped.\n    f_relative_tolerance: Scalar `Tensor` of real dtype. If the relative change\n      in the objective value between one iteration and the next is smaller\n      than this value, the algorithm is stopped.\n    initial_inverse_hessian_estimate: None. Option currently not supported.\n    max_iterations: Scalar positive int32 `Tensor`. The maximum number of\n      iterations for L-BFGS updates.\n    parallel_iterations: Positive integer. The number of iterations allowed to\n      run in parallel.\n    stopping_condition: (Optional) A Python function that takes as input two\n      Boolean tensors of shape `[...]`, and returns a Boolean scalar tensor.\n      The input tensors are `converged` and `failed`, indicating the current\n      status of each respective batch member; the return value states whether\n      the algorithm should stop. The default is tfp.optimizer.converged_all\n      which only stops when all batch members have either converged or failed.\n      An alternative is tfp.optimizer.converged_any which stops as soon as one\n      batch member has converged, or when all have failed.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'minimize' is used.\n\n  Returns:\n    optimizer_results: A namedtuple containing the following items:\n      converged: Scalar boolean tensor indicating whether the minimum was\n        found within tolerance.\n      failed:  Scalar boolean tensor indicating whether a line search\n        step failed to find a suitable step size satisfying Wolfe\n        conditions. In the absence of any constraints on the\n        number of objective evaluations permitted, this value will\n        be the complement of `converged`. However, if there is\n        a constraint and the search stopped due to available\n        evaluations being exhausted, both `failed` and `converged`\n        will be simultaneously False.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A tensor containing the last argument value found\n        during the search. If the search converged, then\n        this value is the argmin of the objective function.\n      objective_value: A tensor containing the value of the objective\n        function at the `position`. If the search converged, then this is\n        the (local) minimum of the objective function.\n      objective_gradient: A tensor containing the gradient of the objective\n        function at the `position`. If the search converged the\n        max-norm of this tensor should be below the tolerance.\n      position_deltas: A tensor encoding information about the latest\n        changes in `position` during the algorithm execution.\n      gradient_deltas: A tensor encoding information about the latest\n        changes in `objective_gradient` during the algorithm execution.\n  \"\"\"\n  if initial_inverse_hessian_estimate is not None:\n    raise NotImplementedError(\n        'Support of initial_inverse_hessian_estimate arg not yet implemented')\n\n  if stopping_condition is None:\n    stopping_condition = bfgs_utils.converged_all\n\n  with tf.compat.v1.name_scope(name, 'minimize', [initial_position, tolerance]):\n    initial_position = tf.convert_to_tensor(\n        value=initial_position, name='initial_position')\n    dtype = initial_position.dtype.base_dtype\n    tolerance = tf.convert_to_tensor(\n        value=tolerance, dtype=dtype, name='grad_tolerance')\n    f_relative_tolerance = tf.convert_to_tensor(\n        value=f_relative_tolerance, dtype=dtype, name='f_relative_tolerance')\n    x_tolerance = tf.convert_to_tensor(\n        value=x_tolerance, dtype=dtype, name='x_tolerance')\n    max_iterations = tf.convert_to_tensor(\n        value=max_iterations, name='max_iterations')\n\n    # The `state` here is a `LBfgsOptimizerResults` tuple with values for the\n    # current state of the algorithm computation.\n    def _cond(state):\n      \"\"\"Continue if iterations remain and stopping condition is not met.\"\"\"\n      return ((state.num_iterations < max_iterations) &\n              tf.logical_not(stopping_condition(state.converged, state.failed)))\n\n    def _body(current_state):\n      \"\"\"Main optimization loop.\"\"\"\n      search_direction = _get_search_direction(current_state)\n\n      # TODO(b/120134934): Check if the derivative at the start point is not\n      # negative, if so then reset position/gradient deltas and recompute\n      # search direction.\n\n      next_state = bfgs_utils.line_search_step(\n          current_state,\n          value_and_gradients_function, search_direction,\n          tolerance, f_relative_tolerance, x_tolerance, stopping_condition)\n\n      # If not failed or converged, update the Hessian estimate.\n      should_update = ~(next_state.converged | next_state.failed)\n      state_after_inv_hessian_update = bfgs_utils.update_fields(\n          next_state,\n          position_deltas=_queue_push(\n              current_state.position_deltas, should_update,\n              next_state.position - current_state.position),\n          gradient_deltas=_queue_push(\n              current_state.gradient_deltas, should_update,\n              next_state.objective_gradient - current_state.objective_gradient))\n      return [state_after_inv_hessian_update]\n\n    initial_state = _get_initial_state(value_and_gradients_function,\n                                       initial_position,\n                                       num_correction_pairs,\n                                       tolerance)\n    return tf.while_loop(\n        cond=_cond,\n        body=_body,\n        loop_vars=[initial_state],\n        parallel_iterations=parallel_iterations)[0]", "code_tokens": ["def", "minimize", "(", "value_and_gradients_function", ",", "initial_position", ",", "num_correction_pairs", "=", "10", ",", "tolerance", "=", "1e-8", ",", "x_tolerance", "=", "0", ",", "f_relative_tolerance", "=", "0", ",", "initial_inverse_hessian_estimate", "=", "None", ",", "max_iterations", "=", "50", ",", "parallel_iterations", "=", "1", ",", "stopping_condition", "=", "None", ",", "name", "=", "None", ")", ":", "if", "initial_inverse_hessian_estimate", "is", "not", "None", ":", "raise", "NotImplementedError", "(", "'Support of initial_inverse_hessian_estimate arg not yet implemented'", ")", "if", "stopping_condition", "is", "None", ":", "stopping_condition", "=", "bfgs_utils", ".", "converged_all", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'minimize'", ",", "[", "initial_position", ",", "tolerance", "]", ")", ":", "initial_position", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "initial_position", ",", "name", "=", "'initial_position'", ")", "dtype", "=", "initial_position", ".", "dtype", ".", "base_dtype", "tolerance", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "tolerance", ",", "dtype", "=", "dtype", ",", "name", "=", "'grad_tolerance'", ")", "f_relative_tolerance", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "f_relative_tolerance", ",", "dtype", "=", "dtype", ",", "name", "=", "'f_relative_tolerance'", ")", "x_tolerance", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x_tolerance", ",", "dtype", "=", "dtype", ",", "name", "=", "'x_tolerance'", ")", "max_iterations", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "max_iterations", ",", "name", "=", "'max_iterations'", ")", "# The `state` here is a `LBfgsOptimizerResults` tuple with values for the", "# current state of the algorithm computation.", "def", "_cond", "(", "state", ")", ":", "\"\"\"Continue if iterations remain and stopping condition is not met.\"\"\"", "return", "(", "(", "state", ".", "num_iterations", "<", "max_iterations", ")", "&", "tf", ".", "logical_not", "(", "stopping_condition", "(", "state", ".", "converged", ",", "state", ".", "failed", ")", ")", ")", "def", "_body", "(", "current_state", ")", ":", "\"\"\"Main optimization loop.\"\"\"", "search_direction", "=", "_get_search_direction", "(", "current_state", ")", "# TODO(b/120134934): Check if the derivative at the start point is not", "# negative, if so then reset position/gradient deltas and recompute", "# search direction.", "next_state", "=", "bfgs_utils", ".", "line_search_step", "(", "current_state", ",", "value_and_gradients_function", ",", "search_direction", ",", "tolerance", ",", "f_relative_tolerance", ",", "x_tolerance", ",", "stopping_condition", ")", "# If not failed or converged, update the Hessian estimate.", "should_update", "=", "~", "(", "next_state", ".", "converged", "|", "next_state", ".", "failed", ")", "state_after_inv_hessian_update", "=", "bfgs_utils", ".", "update_fields", "(", "next_state", ",", "position_deltas", "=", "_queue_push", "(", "current_state", ".", "position_deltas", ",", "should_update", ",", "next_state", ".", "position", "-", "current_state", ".", "position", ")", ",", "gradient_deltas", "=", "_queue_push", "(", "current_state", ".", "gradient_deltas", ",", "should_update", ",", "next_state", ".", "objective_gradient", "-", "current_state", ".", "objective_gradient", ")", ")", "return", "[", "state_after_inv_hessian_update", "]", "initial_state", "=", "_get_initial_state", "(", "value_and_gradients_function", ",", "initial_position", ",", "num_correction_pairs", ",", "tolerance", ")", "return", "tf", ".", "while_loop", "(", "cond", "=", "_cond", ",", "body", "=", "_body", ",", "loop_vars", "=", "[", "initial_state", "]", ",", "parallel_iterations", "=", "parallel_iterations", ")", "[", "0", "]"], "docstring": "Applies the L-BFGS algorithm to minimize a differentiable function.\n\n  Performs unconstrained minimization of a differentiable function using the\n  L-BFGS scheme. See [Nocedal and Wright(2006)][1] for details of the algorithm.\n\n  ### Usage:\n\n  The following example demonstrates the L-BFGS optimizer attempting to find the\n  minimum for a simple high-dimensional quadratic objective function.\n\n  ```python\n    # A high-dimensional quadratic bowl.\n    ndims = 60\n    minimum = np.ones([ndims], dtype='float64')\n    scales = np.arange(ndims, dtype='float64') + 1.0\n\n    # The objective function and the gradient.\n    def quadratic(x):\n      value = tf.reduce_sum(scales * (x - minimum) ** 2)\n      return value, tf.gradients(value, x)[0]\n\n    start = np.arange(ndims, 0, -1, dtype='float64')\n    optim_results = tfp.optimizer.lbfgs_minimize(\n        quadratic, initial_position=start, num_correction_pairs=10,\n        tolerance=1e-8)\n\n    with tf.Session() as session:\n      results = session.run(optim_results)\n      # Check that the search converged\n      assert(results.converged)\n      # Check that the argmin is close to the actual value.\n      np.testing.assert_allclose(results.position, minimum)\n  ```\n\n  ### References:\n\n  [1] Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series\n      in Operations Research. pp 176-180. 2006\n\n  http://pages.mtu.edu/~struther/Courses/OLD/Sp2013/5630/Jorge_Nocedal_Numerical_optimization_267490.pdf\n\n  Args:\n    value_and_gradients_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a tuple of `Tensor`s of real dtype containing\n      the value of the function and its gradient at that point. The function\n      to be minimized. The input is of shape `[..., n]`, where `n` is the size\n      of the domain of input points, and all others are batching dimensions.\n      The first component of the return value is a real `Tensor` of matching\n      shape `[...]`. The second component (the gradient) is also of shape\n      `[..., n]` like the input value to the function.\n    initial_position: Real `Tensor` of shape `[..., n]`. The starting point, or\n      points when using batching dimensions, of the search procedure. At these\n      points the function value and the gradient norm should be finite.\n    num_correction_pairs: Positive integer. Specifies the maximum number of\n      (position_delta, gradient_delta) correction pairs to keep as implicit\n      approximation of the Hessian matrix.\n    tolerance: Scalar `Tensor` of real dtype. Specifies the gradient tolerance\n      for the procedure. If the supremum norm of the gradient vector is below\n      this number, the algorithm is stopped.\n    x_tolerance: Scalar `Tensor` of real dtype. If the absolute change in the\n      position between one iteration and the next is smaller than this number,\n      the algorithm is stopped.\n    f_relative_tolerance: Scalar `Tensor` of real dtype. If the relative change\n      in the objective value between one iteration and the next is smaller\n      than this value, the algorithm is stopped.\n    initial_inverse_hessian_estimate: None. Option currently not supported.\n    max_iterations: Scalar positive int32 `Tensor`. The maximum number of\n      iterations for L-BFGS updates.\n    parallel_iterations: Positive integer. The number of iterations allowed to\n      run in parallel.\n    stopping_condition: (Optional) A Python function that takes as input two\n      Boolean tensors of shape `[...]`, and returns a Boolean scalar tensor.\n      The input tensors are `converged` and `failed`, indicating the current\n      status of each respective batch member; the return value states whether\n      the algorithm should stop. The default is tfp.optimizer.converged_all\n      which only stops when all batch members have either converged or failed.\n      An alternative is tfp.optimizer.converged_any which stops as soon as one\n      batch member has converged, or when all have failed.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'minimize' is used.\n\n  Returns:\n    optimizer_results: A namedtuple containing the following items:\n      converged: Scalar boolean tensor indicating whether the minimum was\n        found within tolerance.\n      failed:  Scalar boolean tensor indicating whether a line search\n        step failed to find a suitable step size satisfying Wolfe\n        conditions. In the absence of any constraints on the\n        number of objective evaluations permitted, this value will\n        be the complement of `converged`. However, if there is\n        a constraint and the search stopped due to available\n        evaluations being exhausted, both `failed` and `converged`\n        will be simultaneously False.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A tensor containing the last argument value found\n        during the search. If the search converged, then\n        this value is the argmin of the objective function.\n      objective_value: A tensor containing the value of the objective\n        function at the `position`. If the search converged, then this is\n        the (local) minimum of the objective function.\n      objective_gradient: A tensor containing the gradient of the objective\n        function at the `position`. If the search converged the\n        max-norm of this tensor should be below the tolerance.\n      position_deltas: A tensor encoding information about the latest\n        changes in `position` during the algorithm execution.\n      gradient_deltas: A tensor encoding information about the latest\n        changes in `objective_gradient` during the algorithm execution.", "docstring_tokens": ["Applies", "the", "L", "-", "BFGS", "algorithm", "to", "minimize", "a", "differentiable", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/lbfgs.py#L80-L260", "partition": "test"}
{"repo": "PythonSanSebastian/docstamp", "path": "docstamp/model.py", "func_name": "JSONMixin.to_json_str", "original_string": "def to_json_str(self):\n        \"\"\"Convert data to json string representation.\n\n        Returns:\n          json representation as string.\n        \"\"\"\n        adict = dict(vars(self), sort_keys=True)\n        adict['type'] = self.__class__.__name__\n        return json.dumps(adict)", "language": "python", "code": "def to_json_str(self):\n        \"\"\"Convert data to json string representation.\n\n        Returns:\n          json representation as string.\n        \"\"\"\n        adict = dict(vars(self), sort_keys=True)\n        adict['type'] = self.__class__.__name__\n        return json.dumps(adict)", "code_tokens": ["def", "to_json_str", "(", "self", ")", ":", "adict", "=", "dict", "(", "vars", "(", "self", ")", ",", "sort_keys", "=", "True", ")", "adict", "[", "'type'", "]", "=", "self", ".", "__class__", ".", "__name__", "return", "json", ".", "dumps", "(", "adict", ")"], "docstring": "Convert data to json string representation.\n\n        Returns:\n          json representation as string.", "docstring_tokens": ["Convert", "data", "to", "json", "string", "representation", "."], "sha": "b43808f2e15351b0b2f0b7eade9c7ef319c9e646", "url": "https://github.com/PythonSanSebastian/docstamp/blob/b43808f2e15351b0b2f0b7eade9c7ef319c9e646/docstamp/model.py#L62-L70", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/adapter/client.py", "func_name": "get_connection", "original_string": "def get_connection(host='localhost', port=27017, username=None, password=None,\n                   uri=None, mongodb=None, authdb=None, timeout=20, *args, **kwargs):\n    \"\"\"Get a client to the mongo database\n\n        host(str): Host of database\n        port(int): Port of database\n        username(str)\n        password(str)\n        uri(str)\n        authdb (str): database to use for authentication\n        timeout(int): How long should the client try to connect\n\n    \"\"\"\n    authdb = authdb or mongodb\n    if uri is None:\n        if username and password:\n            uri = (\"mongodb://{}:{}@{}:{}/{}\"\n                   .format(quote_plus(username), quote_plus(password), host, port, authdb))\n            log_uri = (\"mongodb://{}:****@{}:{}/{}\"\n                   .format(quote_plus(username), host, port, authdb))\n        else:\n            log_uri = uri = \"mongodb://%s:%s\" % (host, port)\n            \n\n    LOG.info(\"Try to connect to %s\" % log_uri)\n    try:\n        client = MongoClient(uri, serverSelectionTimeoutMS=timeout)\n    except ServerSelectionTimeoutError as err:\n        LOG.warning(\"Connection Refused\")\n        raise ConnectionFailure\n\n    LOG.info(\"Connection established\")\n    return client", "language": "python", "code": "def get_connection(host='localhost', port=27017, username=None, password=None,\n                   uri=None, mongodb=None, authdb=None, timeout=20, *args, **kwargs):\n    \"\"\"Get a client to the mongo database\n\n        host(str): Host of database\n        port(int): Port of database\n        username(str)\n        password(str)\n        uri(str)\n        authdb (str): database to use for authentication\n        timeout(int): How long should the client try to connect\n\n    \"\"\"\n    authdb = authdb or mongodb\n    if uri is None:\n        if username and password:\n            uri = (\"mongodb://{}:{}@{}:{}/{}\"\n                   .format(quote_plus(username), quote_plus(password), host, port, authdb))\n            log_uri = (\"mongodb://{}:****@{}:{}/{}\"\n                   .format(quote_plus(username), host, port, authdb))\n        else:\n            log_uri = uri = \"mongodb://%s:%s\" % (host, port)\n            \n\n    LOG.info(\"Try to connect to %s\" % log_uri)\n    try:\n        client = MongoClient(uri, serverSelectionTimeoutMS=timeout)\n    except ServerSelectionTimeoutError as err:\n        LOG.warning(\"Connection Refused\")\n        raise ConnectionFailure\n\n    LOG.info(\"Connection established\")\n    return client", "code_tokens": ["def", "get_connection", "(", "host", "=", "'localhost'", ",", "port", "=", "27017", ",", "username", "=", "None", ",", "password", "=", "None", ",", "uri", "=", "None", ",", "mongodb", "=", "None", ",", "authdb", "=", "None", ",", "timeout", "=", "20", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "authdb", "=", "authdb", "or", "mongodb", "if", "uri", "is", "None", ":", "if", "username", "and", "password", ":", "uri", "=", "(", "\"mongodb://{}:{}@{}:{}/{}\"", ".", "format", "(", "quote_plus", "(", "username", ")", ",", "quote_plus", "(", "password", ")", ",", "host", ",", "port", ",", "authdb", ")", ")", "log_uri", "=", "(", "\"mongodb://{}:****@{}:{}/{}\"", ".", "format", "(", "quote_plus", "(", "username", ")", ",", "host", ",", "port", ",", "authdb", ")", ")", "else", ":", "log_uri", "=", "uri", "=", "\"mongodb://%s:%s\"", "%", "(", "host", ",", "port", ")", "LOG", ".", "info", "(", "\"Try to connect to %s\"", "%", "log_uri", ")", "try", ":", "client", "=", "MongoClient", "(", "uri", ",", "serverSelectionTimeoutMS", "=", "timeout", ")", "except", "ServerSelectionTimeoutError", "as", "err", ":", "LOG", ".", "warning", "(", "\"Connection Refused\"", ")", "raise", "ConnectionFailure", "LOG", ".", "info", "(", "\"Connection established\"", ")", "return", "client"], "docstring": "Get a client to the mongo database\n\n        host(str): Host of database\n        port(int): Port of database\n        username(str)\n        password(str)\n        uri(str)\n        authdb (str): database to use for authentication\n        timeout(int): How long should the client try to connect", "docstring_tokens": ["Get", "a", "client", "to", "the", "mongo", "database"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/adapter/client.py#L23-L55", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/quantum_info/operators/pauli.py", "func_name": "pauli_group", "original_string": "def pauli_group(number_of_qubits, case='weight'):\n    \"\"\"Return the Pauli group with 4^n elements.\n\n    The phases have been removed.\n    case 'weight' is ordered by Pauli weights and\n    case 'tensor' is ordered by I,X,Y,Z counting lowest qubit fastest.\n\n    Args:\n        number_of_qubits (int): number of qubits\n        case (str): determines ordering of group elements ('weight' or 'tensor')\n\n    Returns:\n        list: list of Pauli objects\n\n    Raises:\n        QiskitError: case is not 'weight' or 'tensor'\n        QiskitError: number_of_qubits is larger than 4\n    \"\"\"\n    if number_of_qubits < 5:\n        temp_set = []\n\n        if case == 'weight':\n            tmp = pauli_group(number_of_qubits, case='tensor')\n            # sort on the weight of the Pauli operator\n            return sorted(tmp, key=lambda x: -np.count_nonzero(\n                np.array(x.to_label(), 'c') == b'I'))\n        elif case == 'tensor':\n            # the Pauli set is in tensor order II IX IY IZ XI ...\n            for k in range(4 ** number_of_qubits):\n                z = np.zeros(number_of_qubits, dtype=np.bool)\n                x = np.zeros(number_of_qubits, dtype=np.bool)\n                # looping over all the qubits\n                for j in range(number_of_qubits):\n                    # making the Pauli for each j fill it in from the\n                    # end first\n                    element = (k // (4 ** j)) % 4\n                    if element == 1:\n                        x[j] = True\n                    elif element == 2:\n                        z[j] = True\n                        x[j] = True\n                    elif element == 3:\n                        z[j] = True\n                temp_set.append(Pauli(z, x))\n            return temp_set\n        else:\n            raise QiskitError(\"Only support 'weight' or 'tensor' cases \"\n                              \"but you have {}.\".format(case))\n\n    raise QiskitError(\"Only support number of qubits is less than 5\")", "language": "python", "code": "def pauli_group(number_of_qubits, case='weight'):\n    \"\"\"Return the Pauli group with 4^n elements.\n\n    The phases have been removed.\n    case 'weight' is ordered by Pauli weights and\n    case 'tensor' is ordered by I,X,Y,Z counting lowest qubit fastest.\n\n    Args:\n        number_of_qubits (int): number of qubits\n        case (str): determines ordering of group elements ('weight' or 'tensor')\n\n    Returns:\n        list: list of Pauli objects\n\n    Raises:\n        QiskitError: case is not 'weight' or 'tensor'\n        QiskitError: number_of_qubits is larger than 4\n    \"\"\"\n    if number_of_qubits < 5:\n        temp_set = []\n\n        if case == 'weight':\n            tmp = pauli_group(number_of_qubits, case='tensor')\n            # sort on the weight of the Pauli operator\n            return sorted(tmp, key=lambda x: -np.count_nonzero(\n                np.array(x.to_label(), 'c') == b'I'))\n        elif case == 'tensor':\n            # the Pauli set is in tensor order II IX IY IZ XI ...\n            for k in range(4 ** number_of_qubits):\n                z = np.zeros(number_of_qubits, dtype=np.bool)\n                x = np.zeros(number_of_qubits, dtype=np.bool)\n                # looping over all the qubits\n                for j in range(number_of_qubits):\n                    # making the Pauli for each j fill it in from the\n                    # end first\n                    element = (k // (4 ** j)) % 4\n                    if element == 1:\n                        x[j] = True\n                    elif element == 2:\n                        z[j] = True\n                        x[j] = True\n                    elif element == 3:\n                        z[j] = True\n                temp_set.append(Pauli(z, x))\n            return temp_set\n        else:\n            raise QiskitError(\"Only support 'weight' or 'tensor' cases \"\n                              \"but you have {}.\".format(case))\n\n    raise QiskitError(\"Only support number of qubits is less than 5\")", "code_tokens": ["def", "pauli_group", "(", "number_of_qubits", ",", "case", "=", "'weight'", ")", ":", "if", "number_of_qubits", "<", "5", ":", "temp_set", "=", "[", "]", "if", "case", "==", "'weight'", ":", "tmp", "=", "pauli_group", "(", "number_of_qubits", ",", "case", "=", "'tensor'", ")", "# sort on the weight of the Pauli operator", "return", "sorted", "(", "tmp", ",", "key", "=", "lambda", "x", ":", "-", "np", ".", "count_nonzero", "(", "np", ".", "array", "(", "x", ".", "to_label", "(", ")", ",", "'c'", ")", "==", "b'I'", ")", ")", "elif", "case", "==", "'tensor'", ":", "# the Pauli set is in tensor order II IX IY IZ XI ...", "for", "k", "in", "range", "(", "4", "**", "number_of_qubits", ")", ":", "z", "=", "np", ".", "zeros", "(", "number_of_qubits", ",", "dtype", "=", "np", ".", "bool", ")", "x", "=", "np", ".", "zeros", "(", "number_of_qubits", ",", "dtype", "=", "np", ".", "bool", ")", "# looping over all the qubits", "for", "j", "in", "range", "(", "number_of_qubits", ")", ":", "# making the Pauli for each j fill it in from the", "# end first", "element", "=", "(", "k", "//", "(", "4", "**", "j", ")", ")", "%", "4", "if", "element", "==", "1", ":", "x", "[", "j", "]", "=", "True", "elif", "element", "==", "2", ":", "z", "[", "j", "]", "=", "True", "x", "[", "j", "]", "=", "True", "elif", "element", "==", "3", ":", "z", "[", "j", "]", "=", "True", "temp_set", ".", "append", "(", "Pauli", "(", "z", ",", "x", ")", ")", "return", "temp_set", "else", ":", "raise", "QiskitError", "(", "\"Only support 'weight' or 'tensor' cases \"", "\"but you have {}.\"", ".", "format", "(", "case", ")", ")", "raise", "QiskitError", "(", "\"Only support number of qubits is less than 5\"", ")"], "docstring": "Return the Pauli group with 4^n elements.\n\n    The phases have been removed.\n    case 'weight' is ordered by Pauli weights and\n    case 'tensor' is ordered by I,X,Y,Z counting lowest qubit fastest.\n\n    Args:\n        number_of_qubits (int): number of qubits\n        case (str): determines ordering of group elements ('weight' or 'tensor')\n\n    Returns:\n        list: list of Pauli objects\n\n    Raises:\n        QiskitError: case is not 'weight' or 'tensor'\n        QiskitError: number_of_qubits is larger than 4", "docstring_tokens": ["Return", "the", "Pauli", "group", "with", "4^n", "elements", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/quantum_info/operators/pauli.py#L517-L566", "partition": "test"}
{"repo": "h2non/pook", "path": "pook/matcher.py", "func_name": "MatcherEngine.match", "original_string": "def match(self, request):\n        \"\"\"\n        Match the given HTTP request instance against the registered\n        matcher functions in the current engine.\n\n        Arguments:\n            request (pook.Request): outgoing request to match.\n\n        Returns:\n            tuple(bool, list[Exception]): ``True`` if all matcher tests\n                passes, otherwise ``False``. Also returns an optional list\n                of error exceptions.\n        \"\"\"\n        errors = []\n\n        def match(matcher):\n            try:\n                return matcher.match(request)\n            except Exception as err:\n                err = '{}: {}'.format(type(matcher).__name__, err)\n                errors.append(err)\n                return False\n\n        return all([match(matcher) for matcher in self]), errors", "language": "python", "code": "def match(self, request):\n        \"\"\"\n        Match the given HTTP request instance against the registered\n        matcher functions in the current engine.\n\n        Arguments:\n            request (pook.Request): outgoing request to match.\n\n        Returns:\n            tuple(bool, list[Exception]): ``True`` if all matcher tests\n                passes, otherwise ``False``. Also returns an optional list\n                of error exceptions.\n        \"\"\"\n        errors = []\n\n        def match(matcher):\n            try:\n                return matcher.match(request)\n            except Exception as err:\n                err = '{}: {}'.format(type(matcher).__name__, err)\n                errors.append(err)\n                return False\n\n        return all([match(matcher) for matcher in self]), errors", "code_tokens": ["def", "match", "(", "self", ",", "request", ")", ":", "errors", "=", "[", "]", "def", "match", "(", "matcher", ")", ":", "try", ":", "return", "matcher", ".", "match", "(", "request", ")", "except", "Exception", "as", "err", ":", "err", "=", "'{}: {}'", ".", "format", "(", "type", "(", "matcher", ")", ".", "__name__", ",", "err", ")", "errors", ".", "append", "(", "err", ")", "return", "False", "return", "all", "(", "[", "match", "(", "matcher", ")", "for", "matcher", "in", "self", "]", ")", ",", "errors"], "docstring": "Match the given HTTP request instance against the registered\n        matcher functions in the current engine.\n\n        Arguments:\n            request (pook.Request): outgoing request to match.\n\n        Returns:\n            tuple(bool, list[Exception]): ``True`` if all matcher tests\n                passes, otherwise ``False``. Also returns an optional list\n                of error exceptions.", "docstring_tokens": ["Match", "the", "given", "HTTP", "request", "instance", "against", "the", "registered", "matcher", "functions", "in", "the", "current", "engine", "."], "sha": "e64094e41e4d89d98d2d29af7608ef27dc50cf19", "url": "https://github.com/h2non/pook/blob/e64094e41e4d89d98d2d29af7608ef27dc50cf19/pook/matcher.py#L23-L46", "partition": "test"}
{"repo": "kenneth-reitz/dynamo", "path": "dynamo.py", "func_name": "tables", "original_string": "def tables(auth=None, eager=True):\n    \"\"\"Returns a list of tables for the given user.\"\"\"\n    auth = auth or []\n    dynamodb = boto.connect_dynamodb(*auth)\n\n    return [table(t, auth, eager=eager) for t in dynamodb.list_tables()]", "language": "python", "code": "def tables(auth=None, eager=True):\n    \"\"\"Returns a list of tables for the given user.\"\"\"\n    auth = auth or []\n    dynamodb = boto.connect_dynamodb(*auth)\n\n    return [table(t, auth, eager=eager) for t in dynamodb.list_tables()]", "code_tokens": ["def", "tables", "(", "auth", "=", "None", ",", "eager", "=", "True", ")", ":", "auth", "=", "auth", "or", "[", "]", "dynamodb", "=", "boto", ".", "connect_dynamodb", "(", "*", "auth", ")", "return", "[", "table", "(", "t", ",", "auth", ",", "eager", "=", "eager", ")", "for", "t", "in", "dynamodb", ".", "list_tables", "(", ")", "]"], "docstring": "Returns a list of tables for the given user.", "docstring_tokens": ["Returns", "a", "list", "of", "tables", "for", "the", "given", "user", "."], "sha": "e24276a7e68d868857fd1d0deabccd001920e0c2", "url": "https://github.com/kenneth-reitz/dynamo/blob/e24276a7e68d868857fd1d0deabccd001920e0c2/dynamo.py#L122-L127", "partition": "test"}
{"repo": "kgiusti/pyngus", "path": "examples/utils.py", "func_name": "get_host_port", "original_string": "def get_host_port(server_address):\n    \"\"\"Parse the hostname and port out of the server_address.\"\"\"\n    regex = re.compile(r\"^amqp://([a-zA-Z0-9.]+)(:([\\d]+))?$\")\n    x = regex.match(server_address)\n    if not x:\n        raise Exception(\"Bad address syntax: %s\" % server_address)\n    matches = x.groups()\n    host = matches[0]\n    port = int(matches[2]) if matches[2] else None\n    return host, port", "language": "python", "code": "def get_host_port(server_address):\n    \"\"\"Parse the hostname and port out of the server_address.\"\"\"\n    regex = re.compile(r\"^amqp://([a-zA-Z0-9.]+)(:([\\d]+))?$\")\n    x = regex.match(server_address)\n    if not x:\n        raise Exception(\"Bad address syntax: %s\" % server_address)\n    matches = x.groups()\n    host = matches[0]\n    port = int(matches[2]) if matches[2] else None\n    return host, port", "code_tokens": ["def", "get_host_port", "(", "server_address", ")", ":", "regex", "=", "re", ".", "compile", "(", "r\"^amqp://([a-zA-Z0-9.]+)(:([\\d]+))?$\"", ")", "x", "=", "regex", ".", "match", "(", "server_address", ")", "if", "not", "x", ":", "raise", "Exception", "(", "\"Bad address syntax: %s\"", "%", "server_address", ")", "matches", "=", "x", ".", "groups", "(", ")", "host", "=", "matches", "[", "0", "]", "port", "=", "int", "(", "matches", "[", "2", "]", ")", "if", "matches", "[", "2", "]", "else", "None", "return", "host", ",", "port"], "docstring": "Parse the hostname and port out of the server_address.", "docstring_tokens": ["Parse", "the", "hostname", "and", "port", "out", "of", "the", "server_address", "."], "sha": "5392392046989f1bb84ba938c30e4d48311075f1", "url": "https://github.com/kgiusti/pyngus/blob/5392392046989f1bb84ba938c30e4d48311075f1/examples/utils.py#L33-L42", "partition": "test"}
{"repo": "spotify/pyschema", "path": "pyschema/core.py", "func_name": "loads", "original_string": "def loads(\n        s,\n        record_store=None,\n        schema=None,\n        loader=from_json_compatible,\n        record_class=None  # deprecated in favor of schema\n):\n    \"\"\" Create a Record instance from a json serialized dictionary\n\n    :param s:\n        String with a json-serialized dictionary\n\n    :param record_store:\n        Record store to use for schema lookups (when $schema field is present)\n\n    :param loader:\n        Function called to fetch attributes from json. Typically shouldn't be used by end users\n\n    :param schema:\n        PySchema Record class for the record to load.\n        This will override any $schema fields specified in `s`\n\n    :param record_class:\n        DEPRECATED option, old name for the `schema` parameter\n\n    \"\"\"\n    if record_class is not None:\n        warnings.warn(\n            \"The record_class parameter is deprecated in favour of schema\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        schema = record_class\n    if not isinstance(s, unicode):\n        s = s.decode('utf8')\n    if s.startswith(u\"{\"):\n        json_dct = json.loads(s)\n        return load_json_dct(json_dct, record_store, schema, loader)\n    else:\n        raise ParseError(\"Not a json record\")", "language": "python", "code": "def loads(\n        s,\n        record_store=None,\n        schema=None,\n        loader=from_json_compatible,\n        record_class=None  # deprecated in favor of schema\n):\n    \"\"\" Create a Record instance from a json serialized dictionary\n\n    :param s:\n        String with a json-serialized dictionary\n\n    :param record_store:\n        Record store to use for schema lookups (when $schema field is present)\n\n    :param loader:\n        Function called to fetch attributes from json. Typically shouldn't be used by end users\n\n    :param schema:\n        PySchema Record class for the record to load.\n        This will override any $schema fields specified in `s`\n\n    :param record_class:\n        DEPRECATED option, old name for the `schema` parameter\n\n    \"\"\"\n    if record_class is not None:\n        warnings.warn(\n            \"The record_class parameter is deprecated in favour of schema\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        schema = record_class\n    if not isinstance(s, unicode):\n        s = s.decode('utf8')\n    if s.startswith(u\"{\"):\n        json_dct = json.loads(s)\n        return load_json_dct(json_dct, record_store, schema, loader)\n    else:\n        raise ParseError(\"Not a json record\")", "code_tokens": ["def", "loads", "(", "s", ",", "record_store", "=", "None", ",", "schema", "=", "None", ",", "loader", "=", "from_json_compatible", ",", "record_class", "=", "None", "# deprecated in favor of schema", ")", ":", "if", "record_class", "is", "not", "None", ":", "warnings", ".", "warn", "(", "\"The record_class parameter is deprecated in favour of schema\"", ",", "DeprecationWarning", ",", "stacklevel", "=", "2", ")", "schema", "=", "record_class", "if", "not", "isinstance", "(", "s", ",", "unicode", ")", ":", "s", "=", "s", ".", "decode", "(", "'utf8'", ")", "if", "s", ".", "startswith", "(", "u\"{\"", ")", ":", "json_dct", "=", "json", ".", "loads", "(", "s", ")", "return", "load_json_dct", "(", "json_dct", ",", "record_store", ",", "schema", ",", "loader", ")", "else", ":", "raise", "ParseError", "(", "\"Not a json record\"", ")"], "docstring": "Create a Record instance from a json serialized dictionary\n\n    :param s:\n        String with a json-serialized dictionary\n\n    :param record_store:\n        Record store to use for schema lookups (when $schema field is present)\n\n    :param loader:\n        Function called to fetch attributes from json. Typically shouldn't be used by end users\n\n    :param schema:\n        PySchema Record class for the record to load.\n        This will override any $schema fields specified in `s`\n\n    :param record_class:\n        DEPRECATED option, old name for the `schema` parameter", "docstring_tokens": ["Create", "a", "Record", "instance", "from", "a", "json", "serialized", "dictionary"], "sha": "7e6c3934150bcb040c628d74ace6caf5fcf867df", "url": "https://github.com/spotify/pyschema/blob/7e6c3934150bcb040c628d74ace6caf5fcf867df/pyschema/core.py#L589-L628", "partition": "test"}
{"repo": "stevelittlefish/littlefish", "path": "littlefish/lfsmailer.py", "func_name": "LfsSmtpHandler.add_details", "original_string": "def add_details(self, message):\n        \"\"\"\n        Add extra details to the message.  Separate so that it can be overridden\n        \"\"\"\n        msg = message\n        # Try to append Flask request details\n        try:\n            from flask import request\n            url = request.url\n            method = request.method\n            endpoint = request.endpoint\n\n            # Obscure password field and prettify a little bit\n            form_dict = dict(request.form)\n            for key in form_dict:\n                if key.lower() in _error_reporting_obscured_fields:\n                    form_dict[key] = '******'\n                elif len(form_dict[key]) == 1:\n                    form_dict[key] = form_dict[key][0]\n\n            form = pprint.pformat(form_dict).replace('\\n', '\\n          ')\n\n            msg = '%s\\nRequest:\\n\\nurl:      %s\\nmethod:   %s\\nendpoint: %s\\nform:     %s\\n' % \\\n                (msg, url, method, endpoint, form)\n        except Exception:\n            traceback.print_exc()\n\n        # Try to append the session\n        try:\n            from flask import session\n            from flask.json import JSONEncoder\n            session_str = json.dumps(\n                dict(**session),\n                indent=2,\n                cls=JSONEncoder\n            )\n            msg = '%s\\nSession:\\n\\n%s\\n' % (msg, session_str)\n        except Exception:\n            traceback.print_exc()\n        \n        return msg", "language": "python", "code": "def add_details(self, message):\n        \"\"\"\n        Add extra details to the message.  Separate so that it can be overridden\n        \"\"\"\n        msg = message\n        # Try to append Flask request details\n        try:\n            from flask import request\n            url = request.url\n            method = request.method\n            endpoint = request.endpoint\n\n            # Obscure password field and prettify a little bit\n            form_dict = dict(request.form)\n            for key in form_dict:\n                if key.lower() in _error_reporting_obscured_fields:\n                    form_dict[key] = '******'\n                elif len(form_dict[key]) == 1:\n                    form_dict[key] = form_dict[key][0]\n\n            form = pprint.pformat(form_dict).replace('\\n', '\\n          ')\n\n            msg = '%s\\nRequest:\\n\\nurl:      %s\\nmethod:   %s\\nendpoint: %s\\nform:     %s\\n' % \\\n                (msg, url, method, endpoint, form)\n        except Exception:\n            traceback.print_exc()\n\n        # Try to append the session\n        try:\n            from flask import session\n            from flask.json import JSONEncoder\n            session_str = json.dumps(\n                dict(**session),\n                indent=2,\n                cls=JSONEncoder\n            )\n            msg = '%s\\nSession:\\n\\n%s\\n' % (msg, session_str)\n        except Exception:\n            traceback.print_exc()\n        \n        return msg", "code_tokens": ["def", "add_details", "(", "self", ",", "message", ")", ":", "msg", "=", "message", "# Try to append Flask request details", "try", ":", "from", "flask", "import", "request", "url", "=", "request", ".", "url", "method", "=", "request", ".", "method", "endpoint", "=", "request", ".", "endpoint", "# Obscure password field and prettify a little bit", "form_dict", "=", "dict", "(", "request", ".", "form", ")", "for", "key", "in", "form_dict", ":", "if", "key", ".", "lower", "(", ")", "in", "_error_reporting_obscured_fields", ":", "form_dict", "[", "key", "]", "=", "'******'", "elif", "len", "(", "form_dict", "[", "key", "]", ")", "==", "1", ":", "form_dict", "[", "key", "]", "=", "form_dict", "[", "key", "]", "[", "0", "]", "form", "=", "pprint", ".", "pformat", "(", "form_dict", ")", ".", "replace", "(", "'\\n'", ",", "'\\n          '", ")", "msg", "=", "'%s\\nRequest:\\n\\nurl:      %s\\nmethod:   %s\\nendpoint: %s\\nform:     %s\\n'", "%", "(", "msg", ",", "url", ",", "method", ",", "endpoint", ",", "form", ")", "except", "Exception", ":", "traceback", ".", "print_exc", "(", ")", "# Try to append the session", "try", ":", "from", "flask", "import", "session", "from", "flask", ".", "json", "import", "JSONEncoder", "session_str", "=", "json", ".", "dumps", "(", "dict", "(", "*", "*", "session", ")", ",", "indent", "=", "2", ",", "cls", "=", "JSONEncoder", ")", "msg", "=", "'%s\\nSession:\\n\\n%s\\n'", "%", "(", "msg", ",", "session_str", ")", "except", "Exception", ":", "traceback", ".", "print_exc", "(", ")", "return", "msg"], "docstring": "Add extra details to the message.  Separate so that it can be overridden", "docstring_tokens": ["Add", "extra", "details", "to", "the", "message", ".", "Separate", "so", "that", "it", "can", "be", "overridden"], "sha": "6deee7f81fab30716c743efe2e94e786c6e17016", "url": "https://github.com/stevelittlefish/littlefish/blob/6deee7f81fab30716c743efe2e94e786c6e17016/littlefish/lfsmailer.py#L196-L236", "partition": "test"}
{"repo": "chrisjrn/registrasion", "path": "registrasion/views.py", "func_name": "manual_payment", "original_string": "def manual_payment(request, invoice_id):\n    ''' Allows staff to make manual payments or refunds on an invoice.\n\n    This form requires a login, and the logged in user needs to be staff.\n\n    Arguments:\n        invoice_id (castable to int): The invoice ID to be paid\n\n    Returns:\n        render:\n            Renders ``registrasion/manual_payment.html`` with the following\n            data::\n\n                {\n                    \"invoice\": models.commerce.Invoice(),\n                    \"form\": form,   # A form that saves a ``ManualPayment``\n                                    # object.\n                }\n\n    '''\n\n    FORM_PREFIX = \"manual_payment\"\n\n    current_invoice = InvoiceController.for_id_or_404(invoice_id)\n\n    form = forms.ManualPaymentForm(\n        request.POST or None,\n        prefix=FORM_PREFIX,\n    )\n\n    if request.POST and form.is_valid():\n        form.instance.invoice = current_invoice.invoice\n        form.instance.entered_by = request.user\n        form.save()\n        current_invoice.update_status()\n        form = forms.ManualPaymentForm(prefix=FORM_PREFIX)\n\n    data = {\n        \"invoice\": current_invoice.invoice,\n        \"form\": form,\n    }\n\n    return render(request, \"registrasion/manual_payment.html\", data)", "language": "python", "code": "def manual_payment(request, invoice_id):\n    ''' Allows staff to make manual payments or refunds on an invoice.\n\n    This form requires a login, and the logged in user needs to be staff.\n\n    Arguments:\n        invoice_id (castable to int): The invoice ID to be paid\n\n    Returns:\n        render:\n            Renders ``registrasion/manual_payment.html`` with the following\n            data::\n\n                {\n                    \"invoice\": models.commerce.Invoice(),\n                    \"form\": form,   # A form that saves a ``ManualPayment``\n                                    # object.\n                }\n\n    '''\n\n    FORM_PREFIX = \"manual_payment\"\n\n    current_invoice = InvoiceController.for_id_or_404(invoice_id)\n\n    form = forms.ManualPaymentForm(\n        request.POST or None,\n        prefix=FORM_PREFIX,\n    )\n\n    if request.POST and form.is_valid():\n        form.instance.invoice = current_invoice.invoice\n        form.instance.entered_by = request.user\n        form.save()\n        current_invoice.update_status()\n        form = forms.ManualPaymentForm(prefix=FORM_PREFIX)\n\n    data = {\n        \"invoice\": current_invoice.invoice,\n        \"form\": form,\n    }\n\n    return render(request, \"registrasion/manual_payment.html\", data)", "code_tokens": ["def", "manual_payment", "(", "request", ",", "invoice_id", ")", ":", "FORM_PREFIX", "=", "\"manual_payment\"", "current_invoice", "=", "InvoiceController", ".", "for_id_or_404", "(", "invoice_id", ")", "form", "=", "forms", ".", "ManualPaymentForm", "(", "request", ".", "POST", "or", "None", ",", "prefix", "=", "FORM_PREFIX", ",", ")", "if", "request", ".", "POST", "and", "form", ".", "is_valid", "(", ")", ":", "form", ".", "instance", ".", "invoice", "=", "current_invoice", ".", "invoice", "form", ".", "instance", ".", "entered_by", "=", "request", ".", "user", "form", ".", "save", "(", ")", "current_invoice", ".", "update_status", "(", ")", "form", "=", "forms", ".", "ManualPaymentForm", "(", "prefix", "=", "FORM_PREFIX", ")", "data", "=", "{", "\"invoice\"", ":", "current_invoice", ".", "invoice", ",", "\"form\"", ":", "form", ",", "}", "return", "render", "(", "request", ",", "\"registrasion/manual_payment.html\"", ",", "data", ")"], "docstring": "Allows staff to make manual payments or refunds on an invoice.\n\n    This form requires a login, and the logged in user needs to be staff.\n\n    Arguments:\n        invoice_id (castable to int): The invoice ID to be paid\n\n    Returns:\n        render:\n            Renders ``registrasion/manual_payment.html`` with the following\n            data::\n\n                {\n                    \"invoice\": models.commerce.Invoice(),\n                    \"form\": form,   # A form that saves a ``ManualPayment``\n                                    # object.\n                }", "docstring_tokens": ["Allows", "staff", "to", "make", "manual", "payments", "or", "refunds", "on", "an", "invoice", "."], "sha": "461d5846c6f9f3b7099322a94f5d9911564448e4", "url": "https://github.com/chrisjrn/registrasion/blob/461d5846c6f9f3b7099322a94f5d9911564448e4/registrasion/views.py#L784-L826", "partition": "test"}
{"repo": "c-soft/satel_integra", "path": "satel_integra/cli.py", "func_name": "main", "original_string": "def main(port, ip, command, loglevel):\n    \"\"\"Console script for satel_integra.\"\"\"\n    numeric_level = getattr(logging, loglevel.upper(), None)\n    if not isinstance(numeric_level, int):\n        raise ValueError('Invalid log level: %s' % loglevel)\n        \n    logging.basicConfig(level=numeric_level)\n\n    click.echo(\"Demo of satel_integra library\")\n    if command == \"demo\":\n        demo(ip, port)", "language": "python", "code": "def main(port, ip, command, loglevel):\n    \"\"\"Console script for satel_integra.\"\"\"\n    numeric_level = getattr(logging, loglevel.upper(), None)\n    if not isinstance(numeric_level, int):\n        raise ValueError('Invalid log level: %s' % loglevel)\n        \n    logging.basicConfig(level=numeric_level)\n\n    click.echo(\"Demo of satel_integra library\")\n    if command == \"demo\":\n        demo(ip, port)", "code_tokens": ["def", "main", "(", "port", ",", "ip", ",", "command", ",", "loglevel", ")", ":", "numeric_level", "=", "getattr", "(", "logging", ",", "loglevel", ".", "upper", "(", ")", ",", "None", ")", "if", "not", "isinstance", "(", "numeric_level", ",", "int", ")", ":", "raise", "ValueError", "(", "'Invalid log level: %s'", "%", "loglevel", ")", "logging", ".", "basicConfig", "(", "level", "=", "numeric_level", ")", "click", ".", "echo", "(", "\"Demo of satel_integra library\"", ")", "if", "command", "==", "\"demo\"", ":", "demo", "(", "ip", ",", "port", ")"], "docstring": "Console script for satel_integra.", "docstring_tokens": ["Console", "script", "for", "satel_integra", "."], "sha": "3b6d2020d1e10dc5aa40f30ee4ecc0f3a053eb3c", "url": "https://github.com/c-soft/satel_integra/blob/3b6d2020d1e10dc5aa40f30ee4ecc0f3a053eb3c/satel_integra/cli.py#L16-L26", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/control.py", "func_name": "coverage._atexit", "original_string": "def _atexit(self):\n        \"\"\"Clean up on process shutdown.\"\"\"\n        if self._started:\n            self.stop()\n        if self.auto_data:\n            self.save()", "language": "python", "code": "def _atexit(self):\n        \"\"\"Clean up on process shutdown.\"\"\"\n        if self._started:\n            self.stop()\n        if self.auto_data:\n            self.save()", "code_tokens": ["def", "_atexit", "(", "self", ")", ":", "if", "self", ".", "_started", ":", "self", ".", "stop", "(", ")", "if", "self", ".", "auto_data", ":", "self", ".", "save", "(", ")"], "docstring": "Clean up on process shutdown.", "docstring_tokens": ["Clean", "up", "on", "process", "shutdown", "."], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/control.py#L412-L417", "partition": "test"}
{"repo": "mcs07/MolVS", "path": "molvs/resonance.py", "func_name": "ResonanceEnumerator.enumerate", "original_string": "def enumerate(self, mol):\n        \"\"\"Enumerate all possible resonance forms and return them as a list.\n\n        :param mol: The input molecule.\n        :type mol: rdkit.Chem.rdchem.Mol\n        :return: A list of all possible resonance forms of the molecule.\n        :rtype: list of rdkit.Chem.rdchem.Mol\n        \"\"\"\n        flags = 0\n        if self.kekule_all:\n            flags = flags | Chem.KEKULE_ALL\n        if self.allow_incomplete_octets:\n            flags = flags | Chem.ALLOW_INCOMPLETE_OCTETS\n        if self.allow_charge_separation:\n            flags = flags | Chem.ALLOW_CHARGE_SEPARATION\n        if self.unconstrained_anions:\n            flags = flags | Chem.UNCONSTRAINED_ANIONS\n        if self.unconstrained_cations:\n            flags = flags | Chem.UNCONSTRAINED_CATIONS\n        results = []\n        for result in Chem.ResonanceMolSupplier(mol, flags=flags, maxStructs=self.max_structures):\n            # This seems necessary? ResonanceMolSupplier only does a partial sanitization\n            Chem.SanitizeMol(result)\n            results.append(result)\n        return results", "language": "python", "code": "def enumerate(self, mol):\n        \"\"\"Enumerate all possible resonance forms and return them as a list.\n\n        :param mol: The input molecule.\n        :type mol: rdkit.Chem.rdchem.Mol\n        :return: A list of all possible resonance forms of the molecule.\n        :rtype: list of rdkit.Chem.rdchem.Mol\n        \"\"\"\n        flags = 0\n        if self.kekule_all:\n            flags = flags | Chem.KEKULE_ALL\n        if self.allow_incomplete_octets:\n            flags = flags | Chem.ALLOW_INCOMPLETE_OCTETS\n        if self.allow_charge_separation:\n            flags = flags | Chem.ALLOW_CHARGE_SEPARATION\n        if self.unconstrained_anions:\n            flags = flags | Chem.UNCONSTRAINED_ANIONS\n        if self.unconstrained_cations:\n            flags = flags | Chem.UNCONSTRAINED_CATIONS\n        results = []\n        for result in Chem.ResonanceMolSupplier(mol, flags=flags, maxStructs=self.max_structures):\n            # This seems necessary? ResonanceMolSupplier only does a partial sanitization\n            Chem.SanitizeMol(result)\n            results.append(result)\n        return results", "code_tokens": ["def", "enumerate", "(", "self", ",", "mol", ")", ":", "flags", "=", "0", "if", "self", ".", "kekule_all", ":", "flags", "=", "flags", "|", "Chem", ".", "KEKULE_ALL", "if", "self", ".", "allow_incomplete_octets", ":", "flags", "=", "flags", "|", "Chem", ".", "ALLOW_INCOMPLETE_OCTETS", "if", "self", ".", "allow_charge_separation", ":", "flags", "=", "flags", "|", "Chem", ".", "ALLOW_CHARGE_SEPARATION", "if", "self", ".", "unconstrained_anions", ":", "flags", "=", "flags", "|", "Chem", ".", "UNCONSTRAINED_ANIONS", "if", "self", ".", "unconstrained_cations", ":", "flags", "=", "flags", "|", "Chem", ".", "UNCONSTRAINED_CATIONS", "results", "=", "[", "]", "for", "result", "in", "Chem", ".", "ResonanceMolSupplier", "(", "mol", ",", "flags", "=", "flags", ",", "maxStructs", "=", "self", ".", "max_structures", ")", ":", "# This seems necessary? ResonanceMolSupplier only does a partial sanitization", "Chem", ".", "SanitizeMol", "(", "result", ")", "results", ".", "append", "(", "result", ")", "return", "results"], "docstring": "Enumerate all possible resonance forms and return them as a list.\n\n        :param mol: The input molecule.\n        :type mol: rdkit.Chem.rdchem.Mol\n        :return: A list of all possible resonance forms of the molecule.\n        :rtype: list of rdkit.Chem.rdchem.Mol", "docstring_tokens": ["Enumerate", "all", "possible", "resonance", "forms", "and", "return", "them", "as", "a", "list", "."], "sha": "d815fe52d160abcecbcbf117e6437bf727dbd8ad", "url": "https://github.com/mcs07/MolVS/blob/d815fe52d160abcecbcbf117e6437bf727dbd8ad/molvs/resonance.py#L52-L76", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/flask/helpers.py", "func_name": "get_root_path", "original_string": "def get_root_path(import_name):\n    \"\"\"Returns the path to a package or cwd if that cannot be found.  This\n    returns the path of a package or the folder that contains a module.\n\n    Not to be confused with the package path returned by :func:`find_package`.\n    \"\"\"\n    # Module already imported and has a file attribute.  Use that first.\n    mod = sys.modules.get(import_name)\n    if mod is not None and hasattr(mod, '__file__'):\n        return os.path.dirname(os.path.abspath(mod.__file__))\n\n    # Next attempt: check the loader.\n    loader = pkgutil.get_loader(import_name)\n\n    # Loader does not exist or we're referring to an unloaded main module\n    # or a main module without path (interactive sessions), go with the\n    # current working directory.\n    if loader is None or import_name == '__main__':\n        return os.getcwd()\n\n    # For .egg, zipimporter does not have get_filename until Python 2.7.\n    # Some other loaders might exhibit the same behavior.\n    if hasattr(loader, 'get_filename'):\n        filepath = loader.get_filename(import_name)\n    else:\n        # Fall back to imports.\n        __import__(import_name)\n        filepath = sys.modules[import_name].__file__\n\n    # filepath is import_name.py for a module, or __init__.py for a package.\n    return os.path.dirname(os.path.abspath(filepath))", "language": "python", "code": "def get_root_path(import_name):\n    \"\"\"Returns the path to a package or cwd if that cannot be found.  This\n    returns the path of a package or the folder that contains a module.\n\n    Not to be confused with the package path returned by :func:`find_package`.\n    \"\"\"\n    # Module already imported and has a file attribute.  Use that first.\n    mod = sys.modules.get(import_name)\n    if mod is not None and hasattr(mod, '__file__'):\n        return os.path.dirname(os.path.abspath(mod.__file__))\n\n    # Next attempt: check the loader.\n    loader = pkgutil.get_loader(import_name)\n\n    # Loader does not exist or we're referring to an unloaded main module\n    # or a main module without path (interactive sessions), go with the\n    # current working directory.\n    if loader is None or import_name == '__main__':\n        return os.getcwd()\n\n    # For .egg, zipimporter does not have get_filename until Python 2.7.\n    # Some other loaders might exhibit the same behavior.\n    if hasattr(loader, 'get_filename'):\n        filepath = loader.get_filename(import_name)\n    else:\n        # Fall back to imports.\n        __import__(import_name)\n        filepath = sys.modules[import_name].__file__\n\n    # filepath is import_name.py for a module, or __init__.py for a package.\n    return os.path.dirname(os.path.abspath(filepath))", "code_tokens": ["def", "get_root_path", "(", "import_name", ")", ":", "# Module already imported and has a file attribute.  Use that first.", "mod", "=", "sys", ".", "modules", ".", "get", "(", "import_name", ")", "if", "mod", "is", "not", "None", "and", "hasattr", "(", "mod", ",", "'__file__'", ")", ":", "return", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "mod", ".", "__file__", ")", ")", "# Next attempt: check the loader.", "loader", "=", "pkgutil", ".", "get_loader", "(", "import_name", ")", "# Loader does not exist or we're referring to an unloaded main module", "# or a main module without path (interactive sessions), go with the", "# current working directory.", "if", "loader", "is", "None", "or", "import_name", "==", "'__main__'", ":", "return", "os", ".", "getcwd", "(", ")", "# For .egg, zipimporter does not have get_filename until Python 2.7.", "# Some other loaders might exhibit the same behavior.", "if", "hasattr", "(", "loader", ",", "'get_filename'", ")", ":", "filepath", "=", "loader", ".", "get_filename", "(", "import_name", ")", "else", ":", "# Fall back to imports.", "__import__", "(", "import_name", ")", "filepath", "=", "sys", ".", "modules", "[", "import_name", "]", ".", "__file__", "# filepath is import_name.py for a module, or __init__.py for a package.", "return", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "filepath", ")", ")"], "docstring": "Returns the path to a package or cwd if that cannot be found.  This\n    returns the path of a package or the folder that contains a module.\n\n    Not to be confused with the package path returned by :func:`find_package`.", "docstring_tokens": ["Returns", "the", "path", "to", "a", "package", "or", "cwd", "if", "that", "cannot", "be", "found", ".", "This", "returns", "the", "path", "of", "a", "package", "or", "the", "folder", "that", "contains", "a", "module", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/flask/helpers.py#L619-L649", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/console_widget.py", "func_name": "ConsoleWidget._insert_continuation_prompt", "original_string": "def _insert_continuation_prompt(self, cursor):\n        \"\"\" Inserts new continuation prompt using the specified cursor.\n        \"\"\"\n        if self._continuation_prompt_html is None:\n            self._insert_plain_text(cursor, self._continuation_prompt)\n        else:\n            self._continuation_prompt = self._insert_html_fetching_plain_text(\n                cursor, self._continuation_prompt_html)", "language": "python", "code": "def _insert_continuation_prompt(self, cursor):\n        \"\"\" Inserts new continuation prompt using the specified cursor.\n        \"\"\"\n        if self._continuation_prompt_html is None:\n            self._insert_plain_text(cursor, self._continuation_prompt)\n        else:\n            self._continuation_prompt = self._insert_html_fetching_plain_text(\n                cursor, self._continuation_prompt_html)", "code_tokens": ["def", "_insert_continuation_prompt", "(", "self", ",", "cursor", ")", ":", "if", "self", ".", "_continuation_prompt_html", "is", "None", ":", "self", ".", "_insert_plain_text", "(", "cursor", ",", "self", ".", "_continuation_prompt", ")", "else", ":", "self", ".", "_continuation_prompt", "=", "self", ".", "_insert_html_fetching_plain_text", "(", "cursor", ",", "self", ".", "_continuation_prompt_html", ")"], "docstring": "Inserts new continuation prompt using the specified cursor.", "docstring_tokens": ["Inserts", "new", "continuation", "prompt", "using", "the", "specified", "cursor", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/console_widget.py#L1539-L1546", "partition": "test"}
{"repo": "urinieto/msaf", "path": "msaf/algorithms/cnmf/segmenter.py", "func_name": "compute_labels", "original_string": "def compute_labels(X, rank, R, bound_idxs, niter=300):\n    \"\"\"Computes the labels using the bounds.\"\"\"\n\n    try:\n        F, G = cnmf(X, rank, niter=niter, hull=False)\n    except:\n        return [1]\n\n    label_frames = filter_activation_matrix(G.T, R)\n    label_frames = np.asarray(label_frames, dtype=int)\n\n    #labels = [label_frames[0]]\n    labels = []\n    bound_inters = zip(bound_idxs[:-1], bound_idxs[1:])\n    for bound_inter in bound_inters:\n        if bound_inter[1] - bound_inter[0] <= 0:\n            labels.append(np.max(label_frames) + 1)\n        else:\n            labels.append(most_frequent(\n                label_frames[bound_inter[0]: bound_inter[1]]))\n        #print bound_inter, labels[-1]\n    #labels.append(label_frames[-1])\n\n    return labels", "language": "python", "code": "def compute_labels(X, rank, R, bound_idxs, niter=300):\n    \"\"\"Computes the labels using the bounds.\"\"\"\n\n    try:\n        F, G = cnmf(X, rank, niter=niter, hull=False)\n    except:\n        return [1]\n\n    label_frames = filter_activation_matrix(G.T, R)\n    label_frames = np.asarray(label_frames, dtype=int)\n\n    #labels = [label_frames[0]]\n    labels = []\n    bound_inters = zip(bound_idxs[:-1], bound_idxs[1:])\n    for bound_inter in bound_inters:\n        if bound_inter[1] - bound_inter[0] <= 0:\n            labels.append(np.max(label_frames) + 1)\n        else:\n            labels.append(most_frequent(\n                label_frames[bound_inter[0]: bound_inter[1]]))\n        #print bound_inter, labels[-1]\n    #labels.append(label_frames[-1])\n\n    return labels", "code_tokens": ["def", "compute_labels", "(", "X", ",", "rank", ",", "R", ",", "bound_idxs", ",", "niter", "=", "300", ")", ":", "try", ":", "F", ",", "G", "=", "cnmf", "(", "X", ",", "rank", ",", "niter", "=", "niter", ",", "hull", "=", "False", ")", "except", ":", "return", "[", "1", "]", "label_frames", "=", "filter_activation_matrix", "(", "G", ".", "T", ",", "R", ")", "label_frames", "=", "np", ".", "asarray", "(", "label_frames", ",", "dtype", "=", "int", ")", "#labels = [label_frames[0]]", "labels", "=", "[", "]", "bound_inters", "=", "zip", "(", "bound_idxs", "[", ":", "-", "1", "]", ",", "bound_idxs", "[", "1", ":", "]", ")", "for", "bound_inter", "in", "bound_inters", ":", "if", "bound_inter", "[", "1", "]", "-", "bound_inter", "[", "0", "]", "<=", "0", ":", "labels", ".", "append", "(", "np", ".", "max", "(", "label_frames", ")", "+", "1", ")", "else", ":", "labels", ".", "append", "(", "most_frequent", "(", "label_frames", "[", "bound_inter", "[", "0", "]", ":", "bound_inter", "[", "1", "]", "]", ")", ")", "#print bound_inter, labels[-1]", "#labels.append(label_frames[-1])", "return", "labels"], "docstring": "Computes the labels using the bounds.", "docstring_tokens": ["Computes", "the", "labels", "using", "the", "bounds", "."], "sha": "9dbb57d77a1310465a65cc40f1641d083ca74385", "url": "https://github.com/urinieto/msaf/blob/9dbb57d77a1310465a65cc40f1641d083ca74385/msaf/algorithms/cnmf/segmenter.py#L52-L75", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/askbot.py", "func_name": "AskbotParser.parse_number_of_html_pages", "original_string": "def parse_number_of_html_pages(html_question):\n        \"\"\"Parse number of answer pages to paginate over them.\n\n        :param html_question: raw HTML question element\n\n        :returns: an integer with the number of pages\n        \"\"\"\n        bs_question = bs4.BeautifulSoup(html_question, \"html.parser\")\n        try:\n            bs_question.select('div.paginator')[0]\n        except IndexError:\n            return 1\n        else:\n            return int(bs_question.select('div.paginator')[0].attrs['data-num-pages'])", "language": "python", "code": "def parse_number_of_html_pages(html_question):\n        \"\"\"Parse number of answer pages to paginate over them.\n\n        :param html_question: raw HTML question element\n\n        :returns: an integer with the number of pages\n        \"\"\"\n        bs_question = bs4.BeautifulSoup(html_question, \"html.parser\")\n        try:\n            bs_question.select('div.paginator')[0]\n        except IndexError:\n            return 1\n        else:\n            return int(bs_question.select('div.paginator')[0].attrs['data-num-pages'])", "code_tokens": ["def", "parse_number_of_html_pages", "(", "html_question", ")", ":", "bs_question", "=", "bs4", ".", "BeautifulSoup", "(", "html_question", ",", "\"html.parser\"", ")", "try", ":", "bs_question", ".", "select", "(", "'div.paginator'", ")", "[", "0", "]", "except", "IndexError", ":", "return", "1", "else", ":", "return", "int", "(", "bs_question", ".", "select", "(", "'div.paginator'", ")", "[", "0", "]", ".", "attrs", "[", "'data-num-pages'", "]", ")"], "docstring": "Parse number of answer pages to paginate over them.\n\n        :param html_question: raw HTML question element\n\n        :returns: an integer with the number of pages", "docstring_tokens": ["Parse", "number", "of", "answer", "pages", "to", "paginate", "over", "them", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/askbot.py#L465-L478", "partition": "test"}
{"repo": "Crypto-toolbox/btfxwss", "path": "btfxwss/connection.py", "func_name": "WebSocketConnection.send_ping", "original_string": "def send_ping(self):\n        \"\"\"Sends a ping message to the API and starts pong timers.\n\n        :return:\n        \"\"\"\n        self.log.debug(\"send_ping(): Sending ping to API..\")\n        self.socket.send(json.dumps({'event': 'ping'}))\n        self.pong_timer = Timer(self.pong_timeout, self._check_pong)\n        self.pong_timer.start()", "language": "python", "code": "def send_ping(self):\n        \"\"\"Sends a ping message to the API and starts pong timers.\n\n        :return:\n        \"\"\"\n        self.log.debug(\"send_ping(): Sending ping to API..\")\n        self.socket.send(json.dumps({'event': 'ping'}))\n        self.pong_timer = Timer(self.pong_timeout, self._check_pong)\n        self.pong_timer.start()", "code_tokens": ["def", "send_ping", "(", "self", ")", ":", "self", ".", "log", ".", "debug", "(", "\"send_ping(): Sending ping to API..\"", ")", "self", ".", "socket", ".", "send", "(", "json", ".", "dumps", "(", "{", "'event'", ":", "'ping'", "}", ")", ")", "self", ".", "pong_timer", "=", "Timer", "(", "self", ".", "pong_timeout", ",", "self", ".", "_check_pong", ")", "self", ".", "pong_timer", ".", "start", "(", ")"], "docstring": "Sends a ping message to the API and starts pong timers.\n\n        :return:", "docstring_tokens": ["Sends", "a", "ping", "message", "to", "the", "API", "and", "starts", "pong", "timers", "."], "sha": "16827fa6aacb2c0e289aa852bf61a18df6905835", "url": "https://github.com/Crypto-toolbox/btfxwss/blob/16827fa6aacb2c0e289aa852bf61a18df6905835/btfxwss/connection.py#L266-L274", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/pie/client.py", "func_name": "ProxyKmipClient.derive_key", "original_string": "def derive_key(self,\n                   object_type,\n                   unique_identifiers,\n                   derivation_method,\n                   derivation_parameters,\n                   **kwargs):\n        \"\"\"\n        Derive a new key or secret data from existing managed objects.\n\n        Args:\n            object_type (ObjectType): An ObjectType enumeration specifying\n                what type of object to derive. Only SymmetricKeys and\n                SecretData can be specified. Required.\n            unique_identifiers (list): A list of strings specifying the\n                unique IDs of the existing managed objects to use for\n                derivation. Multiple objects can be specified to fit the\n                requirements of the given derivation method. Required.\n            derivation_method (DerivationMethod): A DerivationMethod\n                enumeration specifying how key derivation should be done.\n                Required.\n            derivation_parameters (dict): A dictionary containing various\n                settings for the key derivation process. See Note below.\n                Required.\n            **kwargs (various): A placeholder for object attributes that\n                should be set on the newly derived object. Currently\n                supported attributes include:\n                    cryptographic_algorithm (enums.CryptographicAlgorithm)\n                    cryptographic_length (int)\n\n        Returns:\n            string: The unique ID of the newly derived object.\n\n        Raises:\n            ClientConnectionNotOpen: if the client connection is unusable\n            KmipOperationFailure: if the operation result is a failure\n            TypeError: if the input arguments are invalid\n\n        Notes:\n            The derivation_parameters argument is a dictionary that can\n            contain the following key/value pairs:\n\n            Key                        | Value\n            ---------------------------|---------------------------------------\n            'cryptographic_parameters' | A dictionary containing additional\n                                       | cryptographic settings. See the\n                                       | decrypt method for more information.\n            'initialization_vector'    | Bytes to be used to initialize the key\n                                       | derivation function, if needed.\n            'derivation_data'          | Bytes to be used as the basis for the\n                                       | key derivation process (e.g., the\n                                       | bytes to be encrypted, hashed, etc).\n            'salt'                     | Bytes to used as a salt value for the\n                                       | key derivation function, if needed.\n                                       | Usually used with PBKDF2.\n            'iteration_count'          | An integer defining how many\n                                       | iterations should be used with the key\n                                       | derivation function, if needed.\n                                       | Usually used with PBKDF2.\n        \"\"\"\n        # Check input\n        if not isinstance(object_type, enums.ObjectType):\n            raise TypeError(\"Object type must be an ObjectType enumeration.\")\n        if not isinstance(unique_identifiers, list):\n            raise TypeError(\"Unique identifiers must be a list of strings.\")\n        else:\n            for unique_identifier in unique_identifiers:\n                if not isinstance(unique_identifier, six.string_types):\n                    raise TypeError(\n                        \"Unique identifiers must be a list of strings.\"\n                    )\n        if not isinstance(derivation_method, enums.DerivationMethod):\n            raise TypeError(\n                \"Derivation method must be a DerivationMethod enumeration.\"\n            )\n        if not isinstance(derivation_parameters, dict):\n            raise TypeError(\"Derivation parameters must be a dictionary.\")\n\n        derivation_parameters = DerivationParameters(\n            cryptographic_parameters=self._build_cryptographic_parameters(\n                derivation_parameters.get('cryptographic_parameters')\n            ),\n            initialization_vector=derivation_parameters.get(\n                'initialization_vector'\n            ),\n            derivation_data=derivation_parameters.get('derivation_data'),\n            salt=derivation_parameters.get('salt'),\n            iteration_count=derivation_parameters.get('iteration_count')\n        )\n\n        # Handle object attributes\n        attributes = []\n        if kwargs.get('cryptographic_length'):\n            attributes.append(\n                self.attribute_factory.create_attribute(\n                    enums.AttributeType.CRYPTOGRAPHIC_LENGTH,\n                    kwargs.get('cryptographic_length')\n                )\n            )\n        if kwargs.get('cryptographic_algorithm'):\n            attributes.append(\n                self.attribute_factory.create_attribute(\n                    enums.AttributeType.CRYPTOGRAPHIC_ALGORITHM,\n                    kwargs.get('cryptographic_algorithm')\n                )\n            )\n        if kwargs.get('cryptographic_usage_mask'):\n            attributes.append(\n                self.attribute_factory.create_attribute(\n                    enums.AttributeType.CRYPTOGRAPHIC_USAGE_MASK,\n                    kwargs.get('cryptographic_usage_mask')\n                )\n            )\n        template_attribute = cobjects.TemplateAttribute(\n            attributes=attributes\n        )\n\n        # Derive the new key/data and handle the results\n        result = self.proxy.derive_key(\n            object_type,\n            unique_identifiers,\n            derivation_method,\n            derivation_parameters,\n            template_attribute\n        )\n\n        status = result.get('result_status')\n        if status == enums.ResultStatus.SUCCESS:\n            return result.get('unique_identifier')\n        else:\n            raise exceptions.KmipOperationFailure(\n                status,\n                result.get('result_reason'),\n                result.get('result_message')\n            )", "language": "python", "code": "def derive_key(self,\n                   object_type,\n                   unique_identifiers,\n                   derivation_method,\n                   derivation_parameters,\n                   **kwargs):\n        \"\"\"\n        Derive a new key or secret data from existing managed objects.\n\n        Args:\n            object_type (ObjectType): An ObjectType enumeration specifying\n                what type of object to derive. Only SymmetricKeys and\n                SecretData can be specified. Required.\n            unique_identifiers (list): A list of strings specifying the\n                unique IDs of the existing managed objects to use for\n                derivation. Multiple objects can be specified to fit the\n                requirements of the given derivation method. Required.\n            derivation_method (DerivationMethod): A DerivationMethod\n                enumeration specifying how key derivation should be done.\n                Required.\n            derivation_parameters (dict): A dictionary containing various\n                settings for the key derivation process. See Note below.\n                Required.\n            **kwargs (various): A placeholder for object attributes that\n                should be set on the newly derived object. Currently\n                supported attributes include:\n                    cryptographic_algorithm (enums.CryptographicAlgorithm)\n                    cryptographic_length (int)\n\n        Returns:\n            string: The unique ID of the newly derived object.\n\n        Raises:\n            ClientConnectionNotOpen: if the client connection is unusable\n            KmipOperationFailure: if the operation result is a failure\n            TypeError: if the input arguments are invalid\n\n        Notes:\n            The derivation_parameters argument is a dictionary that can\n            contain the following key/value pairs:\n\n            Key                        | Value\n            ---------------------------|---------------------------------------\n            'cryptographic_parameters' | A dictionary containing additional\n                                       | cryptographic settings. See the\n                                       | decrypt method for more information.\n            'initialization_vector'    | Bytes to be used to initialize the key\n                                       | derivation function, if needed.\n            'derivation_data'          | Bytes to be used as the basis for the\n                                       | key derivation process (e.g., the\n                                       | bytes to be encrypted, hashed, etc).\n            'salt'                     | Bytes to used as a salt value for the\n                                       | key derivation function, if needed.\n                                       | Usually used with PBKDF2.\n            'iteration_count'          | An integer defining how many\n                                       | iterations should be used with the key\n                                       | derivation function, if needed.\n                                       | Usually used with PBKDF2.\n        \"\"\"\n        # Check input\n        if not isinstance(object_type, enums.ObjectType):\n            raise TypeError(\"Object type must be an ObjectType enumeration.\")\n        if not isinstance(unique_identifiers, list):\n            raise TypeError(\"Unique identifiers must be a list of strings.\")\n        else:\n            for unique_identifier in unique_identifiers:\n                if not isinstance(unique_identifier, six.string_types):\n                    raise TypeError(\n                        \"Unique identifiers must be a list of strings.\"\n                    )\n        if not isinstance(derivation_method, enums.DerivationMethod):\n            raise TypeError(\n                \"Derivation method must be a DerivationMethod enumeration.\"\n            )\n        if not isinstance(derivation_parameters, dict):\n            raise TypeError(\"Derivation parameters must be a dictionary.\")\n\n        derivation_parameters = DerivationParameters(\n            cryptographic_parameters=self._build_cryptographic_parameters(\n                derivation_parameters.get('cryptographic_parameters')\n            ),\n            initialization_vector=derivation_parameters.get(\n                'initialization_vector'\n            ),\n            derivation_data=derivation_parameters.get('derivation_data'),\n            salt=derivation_parameters.get('salt'),\n            iteration_count=derivation_parameters.get('iteration_count')\n        )\n\n        # Handle object attributes\n        attributes = []\n        if kwargs.get('cryptographic_length'):\n            attributes.append(\n                self.attribute_factory.create_attribute(\n                    enums.AttributeType.CRYPTOGRAPHIC_LENGTH,\n                    kwargs.get('cryptographic_length')\n                )\n            )\n        if kwargs.get('cryptographic_algorithm'):\n            attributes.append(\n                self.attribute_factory.create_attribute(\n                    enums.AttributeType.CRYPTOGRAPHIC_ALGORITHM,\n                    kwargs.get('cryptographic_algorithm')\n                )\n            )\n        if kwargs.get('cryptographic_usage_mask'):\n            attributes.append(\n                self.attribute_factory.create_attribute(\n                    enums.AttributeType.CRYPTOGRAPHIC_USAGE_MASK,\n                    kwargs.get('cryptographic_usage_mask')\n                )\n            )\n        template_attribute = cobjects.TemplateAttribute(\n            attributes=attributes\n        )\n\n        # Derive the new key/data and handle the results\n        result = self.proxy.derive_key(\n            object_type,\n            unique_identifiers,\n            derivation_method,\n            derivation_parameters,\n            template_attribute\n        )\n\n        status = result.get('result_status')\n        if status == enums.ResultStatus.SUCCESS:\n            return result.get('unique_identifier')\n        else:\n            raise exceptions.KmipOperationFailure(\n                status,\n                result.get('result_reason'),\n                result.get('result_message')\n            )", "code_tokens": ["def", "derive_key", "(", "self", ",", "object_type", ",", "unique_identifiers", ",", "derivation_method", ",", "derivation_parameters", ",", "*", "*", "kwargs", ")", ":", "# Check input", "if", "not", "isinstance", "(", "object_type", ",", "enums", ".", "ObjectType", ")", ":", "raise", "TypeError", "(", "\"Object type must be an ObjectType enumeration.\"", ")", "if", "not", "isinstance", "(", "unique_identifiers", ",", "list", ")", ":", "raise", "TypeError", "(", "\"Unique identifiers must be a list of strings.\"", ")", "else", ":", "for", "unique_identifier", "in", "unique_identifiers", ":", "if", "not", "isinstance", "(", "unique_identifier", ",", "six", ".", "string_types", ")", ":", "raise", "TypeError", "(", "\"Unique identifiers must be a list of strings.\"", ")", "if", "not", "isinstance", "(", "derivation_method", ",", "enums", ".", "DerivationMethod", ")", ":", "raise", "TypeError", "(", "\"Derivation method must be a DerivationMethod enumeration.\"", ")", "if", "not", "isinstance", "(", "derivation_parameters", ",", "dict", ")", ":", "raise", "TypeError", "(", "\"Derivation parameters must be a dictionary.\"", ")", "derivation_parameters", "=", "DerivationParameters", "(", "cryptographic_parameters", "=", "self", ".", "_build_cryptographic_parameters", "(", "derivation_parameters", ".", "get", "(", "'cryptographic_parameters'", ")", ")", ",", "initialization_vector", "=", "derivation_parameters", ".", "get", "(", "'initialization_vector'", ")", ",", "derivation_data", "=", "derivation_parameters", ".", "get", "(", "'derivation_data'", ")", ",", "salt", "=", "derivation_parameters", ".", "get", "(", "'salt'", ")", ",", "iteration_count", "=", "derivation_parameters", ".", "get", "(", "'iteration_count'", ")", ")", "# Handle object attributes", "attributes", "=", "[", "]", "if", "kwargs", ".", "get", "(", "'cryptographic_length'", ")", ":", "attributes", ".", "append", "(", "self", ".", "attribute_factory", ".", "create_attribute", "(", "enums", ".", "AttributeType", ".", "CRYPTOGRAPHIC_LENGTH", ",", "kwargs", ".", "get", "(", "'cryptographic_length'", ")", ")", ")", "if", "kwargs", ".", "get", "(", "'cryptographic_algorithm'", ")", ":", "attributes", ".", "append", "(", "self", ".", "attribute_factory", ".", "create_attribute", "(", "enums", ".", "AttributeType", ".", "CRYPTOGRAPHIC_ALGORITHM", ",", "kwargs", ".", "get", "(", "'cryptographic_algorithm'", ")", ")", ")", "if", "kwargs", ".", "get", "(", "'cryptographic_usage_mask'", ")", ":", "attributes", ".", "append", "(", "self", ".", "attribute_factory", ".", "create_attribute", "(", "enums", ".", "AttributeType", ".", "CRYPTOGRAPHIC_USAGE_MASK", ",", "kwargs", ".", "get", "(", "'cryptographic_usage_mask'", ")", ")", ")", "template_attribute", "=", "cobjects", ".", "TemplateAttribute", "(", "attributes", "=", "attributes", ")", "# Derive the new key/data and handle the results", "result", "=", "self", ".", "proxy", ".", "derive_key", "(", "object_type", ",", "unique_identifiers", ",", "derivation_method", ",", "derivation_parameters", ",", "template_attribute", ")", "status", "=", "result", ".", "get", "(", "'result_status'", ")", "if", "status", "==", "enums", ".", "ResultStatus", ".", "SUCCESS", ":", "return", "result", ".", "get", "(", "'unique_identifier'", ")", "else", ":", "raise", "exceptions", ".", "KmipOperationFailure", "(", "status", ",", "result", ".", "get", "(", "'result_reason'", ")", ",", "result", ".", "get", "(", "'result_message'", ")", ")"], "docstring": "Derive a new key or secret data from existing managed objects.\n\n        Args:\n            object_type (ObjectType): An ObjectType enumeration specifying\n                what type of object to derive. Only SymmetricKeys and\n                SecretData can be specified. Required.\n            unique_identifiers (list): A list of strings specifying the\n                unique IDs of the existing managed objects to use for\n                derivation. Multiple objects can be specified to fit the\n                requirements of the given derivation method. Required.\n            derivation_method (DerivationMethod): A DerivationMethod\n                enumeration specifying how key derivation should be done.\n                Required.\n            derivation_parameters (dict): A dictionary containing various\n                settings for the key derivation process. See Note below.\n                Required.\n            **kwargs (various): A placeholder for object attributes that\n                should be set on the newly derived object. Currently\n                supported attributes include:\n                    cryptographic_algorithm (enums.CryptographicAlgorithm)\n                    cryptographic_length (int)\n\n        Returns:\n            string: The unique ID of the newly derived object.\n\n        Raises:\n            ClientConnectionNotOpen: if the client connection is unusable\n            KmipOperationFailure: if the operation result is a failure\n            TypeError: if the input arguments are invalid\n\n        Notes:\n            The derivation_parameters argument is a dictionary that can\n            contain the following key/value pairs:\n\n            Key                        | Value\n            ---------------------------|---------------------------------------\n            'cryptographic_parameters' | A dictionary containing additional\n                                       | cryptographic settings. See the\n                                       | decrypt method for more information.\n            'initialization_vector'    | Bytes to be used to initialize the key\n                                       | derivation function, if needed.\n            'derivation_data'          | Bytes to be used as the basis for the\n                                       | key derivation process (e.g., the\n                                       | bytes to be encrypted, hashed, etc).\n            'salt'                     | Bytes to used as a salt value for the\n                                       | key derivation function, if needed.\n                                       | Usually used with PBKDF2.\n            'iteration_count'          | An integer defining how many\n                                       | iterations should be used with the key\n                                       | derivation function, if needed.\n                                       | Usually used with PBKDF2.", "docstring_tokens": ["Derive", "a", "new", "key", "or", "secret", "data", "from", "existing", "managed", "objects", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/pie/client.py#L527-L660", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/share/doc/ipython/examples/parallel/interengine/interengine.py", "func_name": "send", "original_string": "def send(client, sender, targets, msg_name, dest_name=None, block=None):\n    \"\"\"send a message from one to one-or-more engines.\"\"\"\n    dest_name = msg_name if dest_name is None else dest_name\n    def _send(targets, m_name):\n        msg = globals()[m_name]\n        return com.send(targets, msg)\n        \n    client[sender].apply_async(_send, targets, msg_name)\n    \n    return client[targets].execute('%s=com.recv()'%dest_name, block=None)", "language": "python", "code": "def send(client, sender, targets, msg_name, dest_name=None, block=None):\n    \"\"\"send a message from one to one-or-more engines.\"\"\"\n    dest_name = msg_name if dest_name is None else dest_name\n    def _send(targets, m_name):\n        msg = globals()[m_name]\n        return com.send(targets, msg)\n        \n    client[sender].apply_async(_send, targets, msg_name)\n    \n    return client[targets].execute('%s=com.recv()'%dest_name, block=None)", "code_tokens": ["def", "send", "(", "client", ",", "sender", ",", "targets", ",", "msg_name", ",", "dest_name", "=", "None", ",", "block", "=", "None", ")", ":", "dest_name", "=", "msg_name", "if", "dest_name", "is", "None", "else", "dest_name", "def", "_send", "(", "targets", ",", "m_name", ")", ":", "msg", "=", "globals", "(", ")", "[", "m_name", "]", "return", "com", ".", "send", "(", "targets", ",", "msg", ")", "client", "[", "sender", "]", ".", "apply_async", "(", "_send", ",", "targets", ",", "msg_name", ")", "return", "client", "[", "targets", "]", ".", "execute", "(", "'%s=com.recv()'", "%", "dest_name", ",", "block", "=", "None", ")"], "docstring": "send a message from one to one-or-more engines.", "docstring_tokens": ["send", "a", "message", "from", "one", "to", "one", "-", "or", "-", "more", "engines", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/share/doc/ipython/examples/parallel/interengine/interengine.py#L30-L39", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/aws_hook.py", "func_name": "AwsHook.get_credentials", "original_string": "def get_credentials(self, region_name=None):\n        \"\"\"Get the underlying `botocore.Credentials` object.\n\n        This contains the following authentication attributes: access_key, secret_key and token.\n        \"\"\"\n        session, _ = self._get_credentials(region_name)\n        # Credentials are refreshable, so accessing your access key and\n        # secret key separately can lead to a race condition.\n        # See https://stackoverflow.com/a/36291428/8283373\n        return session.get_credentials().get_frozen_credentials()", "language": "python", "code": "def get_credentials(self, region_name=None):\n        \"\"\"Get the underlying `botocore.Credentials` object.\n\n        This contains the following authentication attributes: access_key, secret_key and token.\n        \"\"\"\n        session, _ = self._get_credentials(region_name)\n        # Credentials are refreshable, so accessing your access key and\n        # secret key separately can lead to a race condition.\n        # See https://stackoverflow.com/a/36291428/8283373\n        return session.get_credentials().get_frozen_credentials()", "code_tokens": ["def", "get_credentials", "(", "self", ",", "region_name", "=", "None", ")", ":", "session", ",", "_", "=", "self", ".", "_get_credentials", "(", "region_name", ")", "# Credentials are refreshable, so accessing your access key and", "# secret key separately can lead to a race condition.", "# See https://stackoverflow.com/a/36291428/8283373", "return", "session", ".", "get_credentials", "(", ")", ".", "get_frozen_credentials", "(", ")"], "docstring": "Get the underlying `botocore.Credentials` object.\n\n        This contains the following authentication attributes: access_key, secret_key and token.", "docstring_tokens": ["Get", "the", "underlying", "botocore", ".", "Credentials", "object", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_hook.py#L183-L192", "partition": "test"}
{"repo": "mseclab/PyJFuzz", "path": "gramfuzz/gramfuzz/__init__.py", "func_name": "GramFuzzer.add_to_cat_group", "original_string": "def add_to_cat_group(self, cat, cat_group, def_name):\n        \"\"\"Associate the provided rule definition name ``def_name`` with the\n        category group ``cat_group`` in the category ``cat``.\n\n        :param str cat: The category the rule definition was declared in\n        :param str cat_group: The group within the category the rule belongs to\n        :param str def_name: The name of the rule definition\n        \"\"\"\n        self.cat_groups.setdefault(cat, {}).setdefault(cat_group, deque()).append(def_name)", "language": "python", "code": "def add_to_cat_group(self, cat, cat_group, def_name):\n        \"\"\"Associate the provided rule definition name ``def_name`` with the\n        category group ``cat_group`` in the category ``cat``.\n\n        :param str cat: The category the rule definition was declared in\n        :param str cat_group: The group within the category the rule belongs to\n        :param str def_name: The name of the rule definition\n        \"\"\"\n        self.cat_groups.setdefault(cat, {}).setdefault(cat_group, deque()).append(def_name)", "code_tokens": ["def", "add_to_cat_group", "(", "self", ",", "cat", ",", "cat_group", ",", "def_name", ")", ":", "self", ".", "cat_groups", ".", "setdefault", "(", "cat", ",", "{", "}", ")", ".", "setdefault", "(", "cat_group", ",", "deque", "(", ")", ")", ".", "append", "(", "def_name", ")"], "docstring": "Associate the provided rule definition name ``def_name`` with the\n        category group ``cat_group`` in the category ``cat``.\n\n        :param str cat: The category the rule definition was declared in\n        :param str cat_group: The group within the category the rule belongs to\n        :param str def_name: The name of the rule definition", "docstring_tokens": ["Associate", "the", "provided", "rule", "definition", "name", "def_name", "with", "the", "category", "group", "cat_group", "in", "the", "category", "cat", "."], "sha": "f777067076f62c9ab74ffea6e90fd54402b7a1b4", "url": "https://github.com/mseclab/PyJFuzz/blob/f777067076f62c9ab74ffea6e90fd54402b7a1b4/gramfuzz/gramfuzz/__init__.py#L350-L358", "partition": "test"}
{"repo": "not-na/peng3d", "path": "peng3d/resource.py", "func_name": "ResourceManager.getModelData", "original_string": "def getModelData(self,name):\n        \"\"\"\n        Gets the model data associated with the given name.\n        \n        If it was loaded, a cached copy will be returned.\n        It it was not loaded, it will be loaded and cached.\n        \"\"\"\n        if name in self.modelcache:\n            return self.modelcache[name]\n        return self.loadModelData(name)", "language": "python", "code": "def getModelData(self,name):\n        \"\"\"\n        Gets the model data associated with the given name.\n        \n        If it was loaded, a cached copy will be returned.\n        It it was not loaded, it will be loaded and cached.\n        \"\"\"\n        if name in self.modelcache:\n            return self.modelcache[name]\n        return self.loadModelData(name)", "code_tokens": ["def", "getModelData", "(", "self", ",", "name", ")", ":", "if", "name", "in", "self", ".", "modelcache", ":", "return", "self", ".", "modelcache", "[", "name", "]", "return", "self", ".", "loadModelData", "(", "name", ")"], "docstring": "Gets the model data associated with the given name.\n        \n        If it was loaded, a cached copy will be returned.\n        It it was not loaded, it will be loaded and cached.", "docstring_tokens": ["Gets", "the", "model", "data", "associated", "with", "the", "given", "name", ".", "If", "it", "was", "loaded", "a", "cached", "copy", "will", "be", "returned", ".", "It", "it", "was", "not", "loaded", "it", "will", "be", "loaded", "and", "cached", "."], "sha": "1151be665b26cc8a479f6307086ba919e4d32d85", "url": "https://github.com/not-na/peng3d/blob/1151be665b26cc8a479f6307086ba919e4d32d85/peng3d/resource.py#L240-L249", "partition": "test"}
{"repo": "bjoernricks/python-quilt", "path": "quilt/db.py", "func_name": "PatchSeries.insert_patches", "original_string": "def insert_patches(self, patches):\n        \"\"\" Insert list of patches at the front of the curent patches list \"\"\"\n        patchlines = []\n        for patch_name in patches:\n            patchline = PatchLine(patch_name)\n            patch = patchline.get_patch()\n            if patch:\n                self.patch2line[patch] = patchline\n            patchlines.append(patchline)\n        patchlines.extend(self.patchlines)\n        self.patchlines = patchlines", "language": "python", "code": "def insert_patches(self, patches):\n        \"\"\" Insert list of patches at the front of the curent patches list \"\"\"\n        patchlines = []\n        for patch_name in patches:\n            patchline = PatchLine(patch_name)\n            patch = patchline.get_patch()\n            if patch:\n                self.patch2line[patch] = patchline\n            patchlines.append(patchline)\n        patchlines.extend(self.patchlines)\n        self.patchlines = patchlines", "code_tokens": ["def", "insert_patches", "(", "self", ",", "patches", ")", ":", "patchlines", "=", "[", "]", "for", "patch_name", "in", "patches", ":", "patchline", "=", "PatchLine", "(", "patch_name", ")", "patch", "=", "patchline", ".", "get_patch", "(", ")", "if", "patch", ":", "self", ".", "patch2line", "[", "patch", "]", "=", "patchline", "patchlines", ".", "append", "(", "patchline", ")", "patchlines", ".", "extend", "(", "self", ".", "patchlines", ")", "self", ".", "patchlines", "=", "patchlines"], "docstring": "Insert list of patches at the front of the curent patches list", "docstring_tokens": ["Insert", "list", "of", "patches", "at", "the", "front", "of", "the", "curent", "patches", "list"], "sha": "fae88237f601848cc34d073584d9dcb409f01777", "url": "https://github.com/bjoernricks/python-quilt/blob/fae88237f601848cc34d073584d9dcb409f01777/quilt/db.py#L144-L154", "partition": "test"}
{"repo": "rocky/python3-trepan", "path": "trepan/processor/command/info_subcmd/line.py", "func_name": "InfoLine.run", "original_string": "def run(self, args):\n        \"\"\"Current line number in source file\"\"\"\n        # info line identifier\n        if not self.proc.curframe:\n            self.errmsg(\"No line number information available.\")\n            return\n        if len(args) == 3:\n            # lineinfo returns (item, file, lineno) or (None,)\n            answer = self.lineinfo(args[2])\n            if answer[0]:\n                item, filename, lineno = answer\n                if not os.path.isfile(filename):\n                    filename = Mclifns.search_file(filename,\n                                                   self.core.search_path,\n                                                   self.main_dirname)\n                self.msg('Line %s of \"%s\" <%s>' %\n                         (lineno, filename, item))\n            return\n        filename=self.core.canonic_filename(self.proc.curframe)\n        if not os.path.isfile(filename):\n            filename = Mclifns.search_file(filename, self.core.search_path,\n                                           self.main_dirname)\n            pass\n\n        filename = self.core.canonic_filename(self.proc.curframe)\n        msg1 = 'Line %d of \\\"%s\\\"'  % (inspect.getlineno(self.proc.curframe),\n                                       self.core.filename(filename))\n        msg2 = ('at instruction %d' % self.proc.curframe.f_lasti)\n        if self.proc.event:\n            msg2 += ', %s event' % self.proc.event\n            pass\n        self.msg(Mmisc.wrapped_lines(msg1, msg2, self.settings['width']))\n        return False", "language": "python", "code": "def run(self, args):\n        \"\"\"Current line number in source file\"\"\"\n        # info line identifier\n        if not self.proc.curframe:\n            self.errmsg(\"No line number information available.\")\n            return\n        if len(args) == 3:\n            # lineinfo returns (item, file, lineno) or (None,)\n            answer = self.lineinfo(args[2])\n            if answer[0]:\n                item, filename, lineno = answer\n                if not os.path.isfile(filename):\n                    filename = Mclifns.search_file(filename,\n                                                   self.core.search_path,\n                                                   self.main_dirname)\n                self.msg('Line %s of \"%s\" <%s>' %\n                         (lineno, filename, item))\n            return\n        filename=self.core.canonic_filename(self.proc.curframe)\n        if not os.path.isfile(filename):\n            filename = Mclifns.search_file(filename, self.core.search_path,\n                                           self.main_dirname)\n            pass\n\n        filename = self.core.canonic_filename(self.proc.curframe)\n        msg1 = 'Line %d of \\\"%s\\\"'  % (inspect.getlineno(self.proc.curframe),\n                                       self.core.filename(filename))\n        msg2 = ('at instruction %d' % self.proc.curframe.f_lasti)\n        if self.proc.event:\n            msg2 += ', %s event' % self.proc.event\n            pass\n        self.msg(Mmisc.wrapped_lines(msg1, msg2, self.settings['width']))\n        return False", "code_tokens": ["def", "run", "(", "self", ",", "args", ")", ":", "# info line identifier", "if", "not", "self", ".", "proc", ".", "curframe", ":", "self", ".", "errmsg", "(", "\"No line number information available.\"", ")", "return", "if", "len", "(", "args", ")", "==", "3", ":", "# lineinfo returns (item, file, lineno) or (None,)", "answer", "=", "self", ".", "lineinfo", "(", "args", "[", "2", "]", ")", "if", "answer", "[", "0", "]", ":", "item", ",", "filename", ",", "lineno", "=", "answer", "if", "not", "os", ".", "path", ".", "isfile", "(", "filename", ")", ":", "filename", "=", "Mclifns", ".", "search_file", "(", "filename", ",", "self", ".", "core", ".", "search_path", ",", "self", ".", "main_dirname", ")", "self", ".", "msg", "(", "'Line %s of \"%s\" <%s>'", "%", "(", "lineno", ",", "filename", ",", "item", ")", ")", "return", "filename", "=", "self", ".", "core", ".", "canonic_filename", "(", "self", ".", "proc", ".", "curframe", ")", "if", "not", "os", ".", "path", ".", "isfile", "(", "filename", ")", ":", "filename", "=", "Mclifns", ".", "search_file", "(", "filename", ",", "self", ".", "core", ".", "search_path", ",", "self", ".", "main_dirname", ")", "pass", "filename", "=", "self", ".", "core", ".", "canonic_filename", "(", "self", ".", "proc", ".", "curframe", ")", "msg1", "=", "'Line %d of \\\"%s\\\"'", "%", "(", "inspect", ".", "getlineno", "(", "self", ".", "proc", ".", "curframe", ")", ",", "self", ".", "core", ".", "filename", "(", "filename", ")", ")", "msg2", "=", "(", "'at instruction %d'", "%", "self", ".", "proc", ".", "curframe", ".", "f_lasti", ")", "if", "self", ".", "proc", ".", "event", ":", "msg2", "+=", "', %s event'", "%", "self", ".", "proc", ".", "event", "pass", "self", ".", "msg", "(", "Mmisc", ".", "wrapped_lines", "(", "msg1", ",", "msg2", ",", "self", ".", "settings", "[", "'width'", "]", ")", ")", "return", "False"], "docstring": "Current line number in source file", "docstring_tokens": ["Current", "line", "number", "in", "source", "file"], "sha": "14e91bc0acce090d67be145b1ac040cab92ac5f3", "url": "https://github.com/rocky/python3-trepan/blob/14e91bc0acce090d67be145b1ac040cab92ac5f3/trepan/processor/command/info_subcmd/line.py#L94-L126", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/utils/helpers.py", "func_name": "chain", "original_string": "def chain(*tasks):\n    \"\"\"\n    Given a number of tasks, builds a dependency chain.\n\n    chain(task_1, task_2, task_3, task_4)\n\n    is equivalent to\n\n    task_1.set_downstream(task_2)\n    task_2.set_downstream(task_3)\n    task_3.set_downstream(task_4)\n    \"\"\"\n    for up_task, down_task in zip(tasks[:-1], tasks[1:]):\n        up_task.set_downstream(down_task)", "language": "python", "code": "def chain(*tasks):\n    \"\"\"\n    Given a number of tasks, builds a dependency chain.\n\n    chain(task_1, task_2, task_3, task_4)\n\n    is equivalent to\n\n    task_1.set_downstream(task_2)\n    task_2.set_downstream(task_3)\n    task_3.set_downstream(task_4)\n    \"\"\"\n    for up_task, down_task in zip(tasks[:-1], tasks[1:]):\n        up_task.set_downstream(down_task)", "code_tokens": ["def", "chain", "(", "*", "tasks", ")", ":", "for", "up_task", ",", "down_task", "in", "zip", "(", "tasks", "[", ":", "-", "1", "]", ",", "tasks", "[", "1", ":", "]", ")", ":", "up_task", ".", "set_downstream", "(", "down_task", ")"], "docstring": "Given a number of tasks, builds a dependency chain.\n\n    chain(task_1, task_2, task_3, task_4)\n\n    is equivalent to\n\n    task_1.set_downstream(task_2)\n    task_2.set_downstream(task_3)\n    task_3.set_downstream(task_4)", "docstring_tokens": ["Given", "a", "number", "of", "tasks", "builds", "a", "dependency", "chain", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L153-L166", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/models/taskinstance.py", "func_name": "TaskInstance.init_run_context", "original_string": "def init_run_context(self, raw=False):\n        \"\"\"\n        Sets the log context.\n        \"\"\"\n        self.raw = raw\n        self._set_context(self)", "language": "python", "code": "def init_run_context(self, raw=False):\n        \"\"\"\n        Sets the log context.\n        \"\"\"\n        self.raw = raw\n        self._set_context(self)", "code_tokens": ["def", "init_run_context", "(", "self", ",", "raw", "=", "False", ")", ":", "self", ".", "raw", "=", "raw", "self", ".", "_set_context", "(", "self", ")"], "docstring": "Sets the log context.", "docstring_tokens": ["Sets", "the", "log", "context", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L1363-L1368", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/adapter/mongo/hgnc.py", "func_name": "GeneHandler.load_transcript_bulk", "original_string": "def load_transcript_bulk(self, transcript_objs):\n        \"\"\"Load a bulk of transcript objects to the database\n\n        Arguments:\n            transcript_objs(iterable(scout.models.hgnc_transcript))\n\n        \"\"\"\n        LOG.info(\"Loading transcript bulk\")\n        try:\n            result = self.transcript_collection.insert_many(transcript_objs)\n        except (DuplicateKeyError, BulkWriteError) as err:\n            raise IntegrityError(err)\n        \n        return result", "language": "python", "code": "def load_transcript_bulk(self, transcript_objs):\n        \"\"\"Load a bulk of transcript objects to the database\n\n        Arguments:\n            transcript_objs(iterable(scout.models.hgnc_transcript))\n\n        \"\"\"\n        LOG.info(\"Loading transcript bulk\")\n        try:\n            result = self.transcript_collection.insert_many(transcript_objs)\n        except (DuplicateKeyError, BulkWriteError) as err:\n            raise IntegrityError(err)\n        \n        return result", "code_tokens": ["def", "load_transcript_bulk", "(", "self", ",", "transcript_objs", ")", ":", "LOG", ".", "info", "(", "\"Loading transcript bulk\"", ")", "try", ":", "result", "=", "self", ".", "transcript_collection", ".", "insert_many", "(", "transcript_objs", ")", "except", "(", "DuplicateKeyError", ",", "BulkWriteError", ")", "as", "err", ":", "raise", "IntegrityError", "(", "err", ")", "return", "result"], "docstring": "Load a bulk of transcript objects to the database\n\n        Arguments:\n            transcript_objs(iterable(scout.models.hgnc_transcript))", "docstring_tokens": ["Load", "a", "bulk", "of", "transcript", "objects", "to", "the", "database"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/adapter/mongo/hgnc.py#L57-L70", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval-mozilla", "path": "perceval/backends/mozilla/crates.py", "func_name": "CratesClient.crates", "original_string": "def crates(self, from_page=1):\n        \"\"\"Get crates in alphabetical order\"\"\"\n\n        path = urijoin(CRATES_API_URL, CATEGORY_CRATES)\n        raw_crates = self.__fetch_items(path, from_page)\n\n        return raw_crates", "language": "python", "code": "def crates(self, from_page=1):\n        \"\"\"Get crates in alphabetical order\"\"\"\n\n        path = urijoin(CRATES_API_URL, CATEGORY_CRATES)\n        raw_crates = self.__fetch_items(path, from_page)\n\n        return raw_crates", "code_tokens": ["def", "crates", "(", "self", ",", "from_page", "=", "1", ")", ":", "path", "=", "urijoin", "(", "CRATES_API_URL", ",", "CATEGORY_CRATES", ")", "raw_crates", "=", "self", ".", "__fetch_items", "(", "path", ",", "from_page", ")", "return", "raw_crates"], "docstring": "Get crates in alphabetical order", "docstring_tokens": ["Get", "crates", "in", "alphabetical", "order"], "sha": "4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4", "url": "https://github.com/chaoss/grimoirelab-perceval-mozilla/blob/4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4/perceval/backends/mozilla/crates.py#L275-L281", "partition": "test"}
{"repo": "nicolas-van/mailflash", "path": "mailflash.py", "func_name": "Message.has_bad_headers", "original_string": "def has_bad_headers(self, default_from=None):\n        \"\"\"Checks for bad headers i.e. newlines in subject, sender or recipients.\n        \"\"\"\n\n        sender = self.sender or default_from\n        reply_to = self.reply_to or ''\n        for val in [self.subject, sender, reply_to] + self.recipients:\n            for c in '\\r\\n':\n                if c in val:\n                    return True\n        return False", "language": "python", "code": "def has_bad_headers(self, default_from=None):\n        \"\"\"Checks for bad headers i.e. newlines in subject, sender or recipients.\n        \"\"\"\n\n        sender = self.sender or default_from\n        reply_to = self.reply_to or ''\n        for val in [self.subject, sender, reply_to] + self.recipients:\n            for c in '\\r\\n':\n                if c in val:\n                    return True\n        return False", "code_tokens": ["def", "has_bad_headers", "(", "self", ",", "default_from", "=", "None", ")", ":", "sender", "=", "self", ".", "sender", "or", "default_from", "reply_to", "=", "self", ".", "reply_to", "or", "''", "for", "val", "in", "[", "self", ".", "subject", ",", "sender", ",", "reply_to", "]", "+", "self", ".", "recipients", ":", "for", "c", "in", "'\\r\\n'", ":", "if", "c", "in", "val", ":", "return", "True", "return", "False"], "docstring": "Checks for bad headers i.e. newlines in subject, sender or recipients.", "docstring_tokens": ["Checks", "for", "bad", "headers", "i", ".", "e", ".", "newlines", "in", "subject", "sender", "or", "recipients", "."], "sha": "794598d9df0e343bb1f64b03d09a68a540229774", "url": "https://github.com/nicolas-van/mailflash/blob/794598d9df0e343bb1f64b03d09a68a540229774/mailflash.py#L379-L389", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/salesforce_hook.py", "func_name": "SalesforceHook.get_available_fields", "original_string": "def get_available_fields(self, obj):\n        \"\"\"\n        Get a list of all available fields for an object.\n\n        :param obj: The name of the Salesforce object that we are getting a description of.\n        :type obj: str\n        :return: the names of the fields.\n        :rtype: list of str\n        \"\"\"\n        self.get_conn()\n\n        obj_description = self.describe_object(obj)\n\n        return [field['name'] for field in obj_description['fields']]", "language": "python", "code": "def get_available_fields(self, obj):\n        \"\"\"\n        Get a list of all available fields for an object.\n\n        :param obj: The name of the Salesforce object that we are getting a description of.\n        :type obj: str\n        :return: the names of the fields.\n        :rtype: list of str\n        \"\"\"\n        self.get_conn()\n\n        obj_description = self.describe_object(obj)\n\n        return [field['name'] for field in obj_description['fields']]", "code_tokens": ["def", "get_available_fields", "(", "self", ",", "obj", ")", ":", "self", ".", "get_conn", "(", ")", "obj_description", "=", "self", ".", "describe_object", "(", "obj", ")", "return", "[", "field", "[", "'name'", "]", "for", "field", "in", "obj_description", "[", "'fields'", "]", "]"], "docstring": "Get a list of all available fields for an object.\n\n        :param obj: The name of the Salesforce object that we are getting a description of.\n        :type obj: str\n        :return: the names of the fields.\n        :rtype: list of str", "docstring_tokens": ["Get", "a", "list", "of", "all", "available", "fields", "for", "an", "object", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L110-L123", "partition": "test"}
{"repo": "scheibler/khard", "path": "khard/khard.py", "func_name": "merge_subcommand", "original_string": "def merge_subcommand(vcard_list, selected_address_books, search_terms,\n                     target_uid):\n    \"\"\"Merge two contacts into one.\n\n    :param vcard_list: the vcards from which to choose contacts for mergeing\n    :type vcard_list: list of carddav_object.CarddavObject\n    :param selected_address_books: the addressbooks to use to find the target\n        contact\n    :type selected_address_books: list(addressbook.AddressBook)\n    :param search_terms: the search terms to find the target contact\n    :type search_terms: str\n    :param target_uid: the uid of the target contact or empty\n    :type target_uid: str\n    :returns: None\n    :rtype: None\n\n    \"\"\"\n    # Check arguments.\n    if target_uid != \"\" and search_terms != \"\":\n        print(\"You can not specify a target uid and target search terms for a \"\n              \"merge.\")\n        sys.exit(1)\n    # Find possible target contacts.\n    if target_uid != \"\":\n        target_vcards = get_contacts(selected_address_books, target_uid,\n                                     method=\"uid\")\n        # We require that the uid given can uniquely identify a contact.\n        if len(target_vcards) != 1:\n            if not target_vcards:\n                print(\"Found no contact for target uid %s\" % target_uid)\n            else:\n                print(\"Found multiple contacts for target uid %s\" % target_uid)\n                for vcard in target_vcards:\n                    print(\"    %s: %s\" % (vcard, vcard.get_uid()))\n            sys.exit(1)\n    else:\n        target_vcards = get_contact_list_by_user_selection(\n            selected_address_books, search_terms, False)\n    # get the source vcard, from which to merge\n    source_vcard = choose_vcard_from_list(\"Select contact from which to merge\",\n                                          vcard_list)\n    if source_vcard is None:\n        print(\"Found no source contact for merging\")\n        sys.exit(1)\n    else:\n        print(\"Merge from %s from address book %s\\n\\n\"\n              % (source_vcard, source_vcard.address_book))\n    # get the target vcard, into which to merge\n    target_vcard = choose_vcard_from_list(\"Select contact into which to merge\",\n                                          target_vcards)\n    if target_vcard is None:\n        print(\"Found no target contact for merging\")\n        sys.exit(1)\n    else:\n        print(\"Merge into %s from address book %s\\n\\n\"\n              % (target_vcard, target_vcard.address_book))\n    # merging\n    if source_vcard == target_vcard:\n        print(\"The selected contacts are already identical\")\n    else:\n        merge_existing_contacts(source_vcard, target_vcard, True)", "language": "python", "code": "def merge_subcommand(vcard_list, selected_address_books, search_terms,\n                     target_uid):\n    \"\"\"Merge two contacts into one.\n\n    :param vcard_list: the vcards from which to choose contacts for mergeing\n    :type vcard_list: list of carddav_object.CarddavObject\n    :param selected_address_books: the addressbooks to use to find the target\n        contact\n    :type selected_address_books: list(addressbook.AddressBook)\n    :param search_terms: the search terms to find the target contact\n    :type search_terms: str\n    :param target_uid: the uid of the target contact or empty\n    :type target_uid: str\n    :returns: None\n    :rtype: None\n\n    \"\"\"\n    # Check arguments.\n    if target_uid != \"\" and search_terms != \"\":\n        print(\"You can not specify a target uid and target search terms for a \"\n              \"merge.\")\n        sys.exit(1)\n    # Find possible target contacts.\n    if target_uid != \"\":\n        target_vcards = get_contacts(selected_address_books, target_uid,\n                                     method=\"uid\")\n        # We require that the uid given can uniquely identify a contact.\n        if len(target_vcards) != 1:\n            if not target_vcards:\n                print(\"Found no contact for target uid %s\" % target_uid)\n            else:\n                print(\"Found multiple contacts for target uid %s\" % target_uid)\n                for vcard in target_vcards:\n                    print(\"    %s: %s\" % (vcard, vcard.get_uid()))\n            sys.exit(1)\n    else:\n        target_vcards = get_contact_list_by_user_selection(\n            selected_address_books, search_terms, False)\n    # get the source vcard, from which to merge\n    source_vcard = choose_vcard_from_list(\"Select contact from which to merge\",\n                                          vcard_list)\n    if source_vcard is None:\n        print(\"Found no source contact for merging\")\n        sys.exit(1)\n    else:\n        print(\"Merge from %s from address book %s\\n\\n\"\n              % (source_vcard, source_vcard.address_book))\n    # get the target vcard, into which to merge\n    target_vcard = choose_vcard_from_list(\"Select contact into which to merge\",\n                                          target_vcards)\n    if target_vcard is None:\n        print(\"Found no target contact for merging\")\n        sys.exit(1)\n    else:\n        print(\"Merge into %s from address book %s\\n\\n\"\n              % (target_vcard, target_vcard.address_book))\n    # merging\n    if source_vcard == target_vcard:\n        print(\"The selected contacts are already identical\")\n    else:\n        merge_existing_contacts(source_vcard, target_vcard, True)", "code_tokens": ["def", "merge_subcommand", "(", "vcard_list", ",", "selected_address_books", ",", "search_terms", ",", "target_uid", ")", ":", "# Check arguments.", "if", "target_uid", "!=", "\"\"", "and", "search_terms", "!=", "\"\"", ":", "print", "(", "\"You can not specify a target uid and target search terms for a \"", "\"merge.\"", ")", "sys", ".", "exit", "(", "1", ")", "# Find possible target contacts.", "if", "target_uid", "!=", "\"\"", ":", "target_vcards", "=", "get_contacts", "(", "selected_address_books", ",", "target_uid", ",", "method", "=", "\"uid\"", ")", "# We require that the uid given can uniquely identify a contact.", "if", "len", "(", "target_vcards", ")", "!=", "1", ":", "if", "not", "target_vcards", ":", "print", "(", "\"Found no contact for target uid %s\"", "%", "target_uid", ")", "else", ":", "print", "(", "\"Found multiple contacts for target uid %s\"", "%", "target_uid", ")", "for", "vcard", "in", "target_vcards", ":", "print", "(", "\"    %s: %s\"", "%", "(", "vcard", ",", "vcard", ".", "get_uid", "(", ")", ")", ")", "sys", ".", "exit", "(", "1", ")", "else", ":", "target_vcards", "=", "get_contact_list_by_user_selection", "(", "selected_address_books", ",", "search_terms", ",", "False", ")", "# get the source vcard, from which to merge", "source_vcard", "=", "choose_vcard_from_list", "(", "\"Select contact from which to merge\"", ",", "vcard_list", ")", "if", "source_vcard", "is", "None", ":", "print", "(", "\"Found no source contact for merging\"", ")", "sys", ".", "exit", "(", "1", ")", "else", ":", "print", "(", "\"Merge from %s from address book %s\\n\\n\"", "%", "(", "source_vcard", ",", "source_vcard", ".", "address_book", ")", ")", "# get the target vcard, into which to merge", "target_vcard", "=", "choose_vcard_from_list", "(", "\"Select contact into which to merge\"", ",", "target_vcards", ")", "if", "target_vcard", "is", "None", ":", "print", "(", "\"Found no target contact for merging\"", ")", "sys", ".", "exit", "(", "1", ")", "else", ":", "print", "(", "\"Merge into %s from address book %s\\n\\n\"", "%", "(", "target_vcard", ",", "target_vcard", ".", "address_book", ")", ")", "# merging", "if", "source_vcard", "==", "target_vcard", ":", "print", "(", "\"The selected contacts are already identical\"", ")", "else", ":", "merge_existing_contacts", "(", "source_vcard", ",", "target_vcard", ",", "True", ")"], "docstring": "Merge two contacts into one.\n\n    :param vcard_list: the vcards from which to choose contacts for mergeing\n    :type vcard_list: list of carddav_object.CarddavObject\n    :param selected_address_books: the addressbooks to use to find the target\n        contact\n    :type selected_address_books: list(addressbook.AddressBook)\n    :param search_terms: the search terms to find the target contact\n    :type search_terms: str\n    :param target_uid: the uid of the target contact or empty\n    :type target_uid: str\n    :returns: None\n    :rtype: None", "docstring_tokens": ["Merge", "two", "contacts", "into", "one", "."], "sha": "0f69430c2680f1ff5f073a977a3c5b753b96cc17", "url": "https://github.com/scheibler/khard/blob/0f69430c2680f1ff5f073a977a3c5b753b96cc17/khard/khard.py#L1250-L1310", "partition": "test"}
{"repo": "LuminosoInsight/luminoso-api-client-python", "path": "luminoso_api/v4_json_stream.py", "func_name": "open_csv_somehow_py2", "original_string": "def open_csv_somehow_py2(filename):\n    \"\"\"\n    Open a CSV file using Python 2's CSV module, working around the deficiency\n    where it can't handle the null bytes of UTF-16.\n    \"\"\"\n    encoding = detect_file_encoding(filename)\n    if encoding.startswith('UTF-16'):\n        csvfile = transcode_to_utf8(filename, encoding)\n        encoding = 'UTF-8'\n    else:\n        csvfile = open(filename, 'rU')\n    line = csvfile.readline()\n    csvfile.seek(0)\n\n    if '\\t' in line:\n        # tab-separated\n        reader = csv.reader(csvfile, delimiter='\\t')\n    else:\n        reader = csv.reader(csvfile, dialect='excel')\n\n    header = reader.next()\n    header = [cell.decode(encoding).lower().strip() for cell in header]\n    encode_fn = lambda x: x.decode(encoding, 'replace')\n    return _read_csv(reader, header, encode_fn)", "language": "python", "code": "def open_csv_somehow_py2(filename):\n    \"\"\"\n    Open a CSV file using Python 2's CSV module, working around the deficiency\n    where it can't handle the null bytes of UTF-16.\n    \"\"\"\n    encoding = detect_file_encoding(filename)\n    if encoding.startswith('UTF-16'):\n        csvfile = transcode_to_utf8(filename, encoding)\n        encoding = 'UTF-8'\n    else:\n        csvfile = open(filename, 'rU')\n    line = csvfile.readline()\n    csvfile.seek(0)\n\n    if '\\t' in line:\n        # tab-separated\n        reader = csv.reader(csvfile, delimiter='\\t')\n    else:\n        reader = csv.reader(csvfile, dialect='excel')\n\n    header = reader.next()\n    header = [cell.decode(encoding).lower().strip() for cell in header]\n    encode_fn = lambda x: x.decode(encoding, 'replace')\n    return _read_csv(reader, header, encode_fn)", "code_tokens": ["def", "open_csv_somehow_py2", "(", "filename", ")", ":", "encoding", "=", "detect_file_encoding", "(", "filename", ")", "if", "encoding", ".", "startswith", "(", "'UTF-16'", ")", ":", "csvfile", "=", "transcode_to_utf8", "(", "filename", ",", "encoding", ")", "encoding", "=", "'UTF-8'", "else", ":", "csvfile", "=", "open", "(", "filename", ",", "'rU'", ")", "line", "=", "csvfile", ".", "readline", "(", ")", "csvfile", ".", "seek", "(", "0", ")", "if", "'\\t'", "in", "line", ":", "# tab-separated", "reader", "=", "csv", ".", "reader", "(", "csvfile", ",", "delimiter", "=", "'\\t'", ")", "else", ":", "reader", "=", "csv", ".", "reader", "(", "csvfile", ",", "dialect", "=", "'excel'", ")", "header", "=", "reader", ".", "next", "(", ")", "header", "=", "[", "cell", ".", "decode", "(", "encoding", ")", ".", "lower", "(", ")", ".", "strip", "(", ")", "for", "cell", "in", "header", "]", "encode_fn", "=", "lambda", "x", ":", "x", ".", "decode", "(", "encoding", ",", "'replace'", ")", "return", "_read_csv", "(", "reader", ",", "header", ",", "encode_fn", ")"], "docstring": "Open a CSV file using Python 2's CSV module, working around the deficiency\n    where it can't handle the null bytes of UTF-16.", "docstring_tokens": ["Open", "a", "CSV", "file", "using", "Python", "2", "s", "CSV", "module", "working", "around", "the", "deficiency", "where", "it", "can", "t", "handle", "the", "null", "bytes", "of", "UTF", "-", "16", "."], "sha": "3bedf2a454aee39214c11fbf556ead3eecc27881", "url": "https://github.com/LuminosoInsight/luminoso-api-client-python/blob/3bedf2a454aee39214c11fbf556ead3eecc27881/luminoso_api/v4_json_stream.py#L213-L236", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/distribute-0.6.31-py2.7.egg/setuptools/command/bdist_egg.py", "func_name": "scan_module", "original_string": "def scan_module(egg_dir, base, name, stubs):\n    \"\"\"Check whether module possibly uses unsafe-for-zipfile stuff\"\"\"\n\n    filename = os.path.join(base,name)\n    if filename[:-1] in stubs:\n        return True     # Extension module\n    pkg = base[len(egg_dir)+1:].replace(os.sep,'.')\n    module = pkg+(pkg and '.' or '')+os.path.splitext(name)[0]\n    if sys.version_info < (3, 3):\n        skip = 8   # skip magic & date\n    else:\n        skip = 12  # skip magic & date & file size\n    f = open(filename,'rb'); f.read(skip)\n    code = marshal.load(f); f.close()\n    safe = True\n    symbols = dict.fromkeys(iter_symbols(code))\n    for bad in ['__file__', '__path__']:\n        if bad in symbols:\n            log.warn(\"%s: module references %s\", module, bad)\n            safe = False\n    if 'inspect' in symbols:\n        for bad in [\n            'getsource', 'getabsfile', 'getsourcefile', 'getfile'\n            'getsourcelines', 'findsource', 'getcomments', 'getframeinfo',\n            'getinnerframes', 'getouterframes', 'stack', 'trace'\n        ]:\n            if bad in symbols:\n                log.warn(\"%s: module MAY be using inspect.%s\", module, bad)\n                safe = False\n    if '__name__' in symbols and '__main__' in symbols and '.' not in module:\n        if sys.version[:3]==\"2.4\":  # -m works w/zipfiles in 2.5\n            log.warn(\"%s: top-level module may be 'python -m' script\", module)\n            safe = False\n    return safe", "language": "python", "code": "def scan_module(egg_dir, base, name, stubs):\n    \"\"\"Check whether module possibly uses unsafe-for-zipfile stuff\"\"\"\n\n    filename = os.path.join(base,name)\n    if filename[:-1] in stubs:\n        return True     # Extension module\n    pkg = base[len(egg_dir)+1:].replace(os.sep,'.')\n    module = pkg+(pkg and '.' or '')+os.path.splitext(name)[0]\n    if sys.version_info < (3, 3):\n        skip = 8   # skip magic & date\n    else:\n        skip = 12  # skip magic & date & file size\n    f = open(filename,'rb'); f.read(skip)\n    code = marshal.load(f); f.close()\n    safe = True\n    symbols = dict.fromkeys(iter_symbols(code))\n    for bad in ['__file__', '__path__']:\n        if bad in symbols:\n            log.warn(\"%s: module references %s\", module, bad)\n            safe = False\n    if 'inspect' in symbols:\n        for bad in [\n            'getsource', 'getabsfile', 'getsourcefile', 'getfile'\n            'getsourcelines', 'findsource', 'getcomments', 'getframeinfo',\n            'getinnerframes', 'getouterframes', 'stack', 'trace'\n        ]:\n            if bad in symbols:\n                log.warn(\"%s: module MAY be using inspect.%s\", module, bad)\n                safe = False\n    if '__name__' in symbols and '__main__' in symbols and '.' not in module:\n        if sys.version[:3]==\"2.4\":  # -m works w/zipfiles in 2.5\n            log.warn(\"%s: top-level module may be 'python -m' script\", module)\n            safe = False\n    return safe", "code_tokens": ["def", "scan_module", "(", "egg_dir", ",", "base", ",", "name", ",", "stubs", ")", ":", "filename", "=", "os", ".", "path", ".", "join", "(", "base", ",", "name", ")", "if", "filename", "[", ":", "-", "1", "]", "in", "stubs", ":", "return", "True", "# Extension module", "pkg", "=", "base", "[", "len", "(", "egg_dir", ")", "+", "1", ":", "]", ".", "replace", "(", "os", ".", "sep", ",", "'.'", ")", "module", "=", "pkg", "+", "(", "pkg", "and", "'.'", "or", "''", ")", "+", "os", ".", "path", ".", "splitext", "(", "name", ")", "[", "0", "]", "if", "sys", ".", "version_info", "<", "(", "3", ",", "3", ")", ":", "skip", "=", "8", "# skip magic & date", "else", ":", "skip", "=", "12", "# skip magic & date & file size", "f", "=", "open", "(", "filename", ",", "'rb'", ")", "f", ".", "read", "(", "skip", ")", "code", "=", "marshal", ".", "load", "(", "f", ")", "f", ".", "close", "(", ")", "safe", "=", "True", "symbols", "=", "dict", ".", "fromkeys", "(", "iter_symbols", "(", "code", ")", ")", "for", "bad", "in", "[", "'__file__'", ",", "'__path__'", "]", ":", "if", "bad", "in", "symbols", ":", "log", ".", "warn", "(", "\"%s: module references %s\"", ",", "module", ",", "bad", ")", "safe", "=", "False", "if", "'inspect'", "in", "symbols", ":", "for", "bad", "in", "[", "'getsource'", ",", "'getabsfile'", ",", "'getsourcefile'", ",", "'getfile'", "'getsourcelines'", ",", "'findsource'", ",", "'getcomments'", ",", "'getframeinfo'", ",", "'getinnerframes'", ",", "'getouterframes'", ",", "'stack'", ",", "'trace'", "]", ":", "if", "bad", "in", "symbols", ":", "log", ".", "warn", "(", "\"%s: module MAY be using inspect.%s\"", ",", "module", ",", "bad", ")", "safe", "=", "False", "if", "'__name__'", "in", "symbols", "and", "'__main__'", "in", "symbols", "and", "'.'", "not", "in", "module", ":", "if", "sys", ".", "version", "[", ":", "3", "]", "==", "\"2.4\"", ":", "# -m works w/zipfiles in 2.5", "log", ".", "warn", "(", "\"%s: top-level module may be 'python -m' script\"", ",", "module", ")", "safe", "=", "False", "return", "safe"], "docstring": "Check whether module possibly uses unsafe-for-zipfile stuff", "docstring_tokens": ["Check", "whether", "module", "possibly", "uses", "unsafe", "-", "for", "-", "zipfile", "stuff"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/distribute-0.6.31-py2.7.egg/setuptools/command/bdist_egg.py#L420-L453", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/googlehits.py", "func_name": "GoogleHits.fetch", "original_string": "def fetch(self, category=CATEGORY_HITS):\n        \"\"\"Fetch data from Google API.\n\n        The method retrieves a list of hits for some\n        given keywords using the Google API.\n\n        :param category: the category of items to fetch\n\n        :returns: a generator of data\n        \"\"\"\n        kwargs = {}\n        items = super().fetch(category, **kwargs)\n\n        return items", "language": "python", "code": "def fetch(self, category=CATEGORY_HITS):\n        \"\"\"Fetch data from Google API.\n\n        The method retrieves a list of hits for some\n        given keywords using the Google API.\n\n        :param category: the category of items to fetch\n\n        :returns: a generator of data\n        \"\"\"\n        kwargs = {}\n        items = super().fetch(category, **kwargs)\n\n        return items", "code_tokens": ["def", "fetch", "(", "self", ",", "category", "=", "CATEGORY_HITS", ")", ":", "kwargs", "=", "{", "}", "items", "=", "super", "(", ")", ".", "fetch", "(", "category", ",", "*", "*", "kwargs", ")", "return", "items"], "docstring": "Fetch data from Google API.\n\n        The method retrieves a list of hits for some\n        given keywords using the Google API.\n\n        :param category: the category of items to fetch\n\n        :returns: a generator of data", "docstring_tokens": ["Fetch", "data", "from", "Google", "API", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/googlehits.py#L81-L94", "partition": "test"}
{"repo": "vaexio/vaex", "path": "packages/vaex-core/vaex/dataframe.py", "func_name": "DataFrame.sort", "original_string": "def sort(self, by, ascending=True, kind='quicksort'):\n        '''Return a sorted DataFrame, sorted by the expression 'by'\n\n        {note_copy}\n\n        {note_filter}\n\n        Example:\n\n        >>> import vaex, numpy as np\n        >>> df = vaex.from_arrays(s=np.array(['a', 'b', 'c', 'd']), x=np.arange(1,5))\n        >>> df['y'] = (df.x-1.8)**2\n        >>> df\n          #  s      x     y\n          0  a      1  0.64\n          1  b      2  0.04\n          2  c      3  1.44\n          3  d      4  4.84\n        >>> df.sort('y', ascending=False)  # Note: passing '(x-1.8)**2' gives the same result\n          #  s      x     y\n          0  d      4  4.84\n          1  c      3  1.44\n          2  a      1  0.64\n          3  b      2  0.04\n\n        :param str or expression by: expression to sort by\n        :param bool ascending: ascending (default, True) or descending (False)\n        :param str kind: kind of algorithm to use (passed to numpy.argsort)\n        '''\n        self = self.trim()\n        values = self.evaluate(by, filtered=False)\n        indices = np.argsort(values, kind=kind)\n        if not ascending:\n            indices = indices[::-1].copy()  # this may be used a lot, so copy for performance\n        return self.take(indices)", "language": "python", "code": "def sort(self, by, ascending=True, kind='quicksort'):\n        '''Return a sorted DataFrame, sorted by the expression 'by'\n\n        {note_copy}\n\n        {note_filter}\n\n        Example:\n\n        >>> import vaex, numpy as np\n        >>> df = vaex.from_arrays(s=np.array(['a', 'b', 'c', 'd']), x=np.arange(1,5))\n        >>> df['y'] = (df.x-1.8)**2\n        >>> df\n          #  s      x     y\n          0  a      1  0.64\n          1  b      2  0.04\n          2  c      3  1.44\n          3  d      4  4.84\n        >>> df.sort('y', ascending=False)  # Note: passing '(x-1.8)**2' gives the same result\n          #  s      x     y\n          0  d      4  4.84\n          1  c      3  1.44\n          2  a      1  0.64\n          3  b      2  0.04\n\n        :param str or expression by: expression to sort by\n        :param bool ascending: ascending (default, True) or descending (False)\n        :param str kind: kind of algorithm to use (passed to numpy.argsort)\n        '''\n        self = self.trim()\n        values = self.evaluate(by, filtered=False)\n        indices = np.argsort(values, kind=kind)\n        if not ascending:\n            indices = indices[::-1].copy()  # this may be used a lot, so copy for performance\n        return self.take(indices)", "code_tokens": ["def", "sort", "(", "self", ",", "by", ",", "ascending", "=", "True", ",", "kind", "=", "'quicksort'", ")", ":", "self", "=", "self", ".", "trim", "(", ")", "values", "=", "self", ".", "evaluate", "(", "by", ",", "filtered", "=", "False", ")", "indices", "=", "np", ".", "argsort", "(", "values", ",", "kind", "=", "kind", ")", "if", "not", "ascending", ":", "indices", "=", "indices", "[", ":", ":", "-", "1", "]", ".", "copy", "(", ")", "# this may be used a lot, so copy for performance", "return", "self", ".", "take", "(", "indices", ")"], "docstring": "Return a sorted DataFrame, sorted by the expression 'by'\n\n        {note_copy}\n\n        {note_filter}\n\n        Example:\n\n        >>> import vaex, numpy as np\n        >>> df = vaex.from_arrays(s=np.array(['a', 'b', 'c', 'd']), x=np.arange(1,5))\n        >>> df['y'] = (df.x-1.8)**2\n        >>> df\n          #  s      x     y\n          0  a      1  0.64\n          1  b      2  0.04\n          2  c      3  1.44\n          3  d      4  4.84\n        >>> df.sort('y', ascending=False)  # Note: passing '(x-1.8)**2' gives the same result\n          #  s      x     y\n          0  d      4  4.84\n          1  c      3  1.44\n          2  a      1  0.64\n          3  b      2  0.04\n\n        :param str or expression by: expression to sort by\n        :param bool ascending: ascending (default, True) or descending (False)\n        :param str kind: kind of algorithm to use (passed to numpy.argsort)", "docstring_tokens": ["Return", "a", "sorted", "DataFrame", "sorted", "by", "the", "expression", "by"], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/dataframe.py#L3969-L4003", "partition": "test"}
{"repo": "neurosynth/neurosynth", "path": "neurosynth/analysis/stats.py", "func_name": "fdr", "original_string": "def fdr(p, q=.05):\n    \"\"\" Determine FDR threshold given a p value array and desired false\n    discovery rate q. \"\"\"\n    s = np.sort(p)\n    nvox = p.shape[0]\n    null = np.array(range(1, nvox + 1), dtype='float') * q / nvox\n    below = np.where(s <= null)[0]\n    return s[max(below)] if len(below) else -1", "language": "python", "code": "def fdr(p, q=.05):\n    \"\"\" Determine FDR threshold given a p value array and desired false\n    discovery rate q. \"\"\"\n    s = np.sort(p)\n    nvox = p.shape[0]\n    null = np.array(range(1, nvox + 1), dtype='float') * q / nvox\n    below = np.where(s <= null)[0]\n    return s[max(below)] if len(below) else -1", "code_tokens": ["def", "fdr", "(", "p", ",", "q", "=", ".05", ")", ":", "s", "=", "np", ".", "sort", "(", "p", ")", "nvox", "=", "p", ".", "shape", "[", "0", "]", "null", "=", "np", ".", "array", "(", "range", "(", "1", ",", "nvox", "+", "1", ")", ",", "dtype", "=", "'float'", ")", "*", "q", "/", "nvox", "below", "=", "np", ".", "where", "(", "s", "<=", "null", ")", "[", "0", "]", "return", "s", "[", "max", "(", "below", ")", "]", "if", "len", "(", "below", ")", "else", "-", "1"], "docstring": "Determine FDR threshold given a p value array and desired false\n    discovery rate q.", "docstring_tokens": ["Determine", "FDR", "threshold", "given", "a", "p", "value", "array", "and", "desired", "false", "discovery", "rate", "q", "."], "sha": "948ce7edce15d7df693446e76834e0c23bfe8f11", "url": "https://github.com/neurosynth/neurosynth/blob/948ce7edce15d7df693446e76834e0c23bfe8f11/neurosynth/analysis/stats.py#L62-L69", "partition": "test"}
{"repo": "bennylope/django-taggit-labels", "path": "taggit_labels/widgets.py", "func_name": "LabelWidget.tag_list", "original_string": "def tag_list(self, tags):\n        \"\"\"\n        Generates a list of tags identifying those previously selected.\n\n        Returns a list of tuples of the form (<tag name>, <CSS class name>).\n\n        Uses the string names rather than the tags themselves in order to work\n        with tag lists built from forms not fully submitted.\n        \"\"\"\n        return [\n            (tag.name, \"selected taggit-tag\" if tag.name in tags else \"taggit-tag\")\n            for tag in self.model.objects.all()\n        ]", "language": "python", "code": "def tag_list(self, tags):\n        \"\"\"\n        Generates a list of tags identifying those previously selected.\n\n        Returns a list of tuples of the form (<tag name>, <CSS class name>).\n\n        Uses the string names rather than the tags themselves in order to work\n        with tag lists built from forms not fully submitted.\n        \"\"\"\n        return [\n            (tag.name, \"selected taggit-tag\" if tag.name in tags else \"taggit-tag\")\n            for tag in self.model.objects.all()\n        ]", "code_tokens": ["def", "tag_list", "(", "self", ",", "tags", ")", ":", "return", "[", "(", "tag", ".", "name", ",", "\"selected taggit-tag\"", "if", "tag", ".", "name", "in", "tags", "else", "\"taggit-tag\"", ")", "for", "tag", "in", "self", ".", "model", ".", "objects", ".", "all", "(", ")", "]"], "docstring": "Generates a list of tags identifying those previously selected.\n\n        Returns a list of tuples of the form (<tag name>, <CSS class name>).\n\n        Uses the string names rather than the tags themselves in order to work\n        with tag lists built from forms not fully submitted.", "docstring_tokens": ["Generates", "a", "list", "of", "tags", "identifying", "those", "previously", "selected", "."], "sha": "7afef34125653e958dc5dba0280904a0714aa808", "url": "https://github.com/bennylope/django-taggit-labels/blob/7afef34125653e958dc5dba0280904a0714aa808/taggit_labels/widgets.py#L33-L45", "partition": "test"}
{"repo": "ellmetha/neojsonrpc", "path": "neojsonrpc/client.py", "func_name": "Client.get_asset_state", "original_string": "def get_asset_state(self, asset_id, **kwargs):\n        \"\"\" Returns the asset information associated with a specific asset ID.\n\n        :param asset_id:\n            an asset identifier (the transaction ID of the RegistTransaction when the asset is\n            registered)\n        :type asset_id: str\n        :return: dictionary containing the asset state information\n        :rtype: dict\n\n        \"\"\"\n        return self._call(JSONRPCMethods.GET_ASSET_STATE.value, params=[asset_id, ], **kwargs)", "language": "python", "code": "def get_asset_state(self, asset_id, **kwargs):\n        \"\"\" Returns the asset information associated with a specific asset ID.\n\n        :param asset_id:\n            an asset identifier (the transaction ID of the RegistTransaction when the asset is\n            registered)\n        :type asset_id: str\n        :return: dictionary containing the asset state information\n        :rtype: dict\n\n        \"\"\"\n        return self._call(JSONRPCMethods.GET_ASSET_STATE.value, params=[asset_id, ], **kwargs)", "code_tokens": ["def", "get_asset_state", "(", "self", ",", "asset_id", ",", "*", "*", "kwargs", ")", ":", "return", "self", ".", "_call", "(", "JSONRPCMethods", ".", "GET_ASSET_STATE", ".", "value", ",", "params", "=", "[", "asset_id", ",", "]", ",", "*", "*", "kwargs", ")"], "docstring": "Returns the asset information associated with a specific asset ID.\n\n        :param asset_id:\n            an asset identifier (the transaction ID of the RegistTransaction when the asset is\n            registered)\n        :type asset_id: str\n        :return: dictionary containing the asset state information\n        :rtype: dict", "docstring_tokens": ["Returns", "the", "asset", "information", "associated", "with", "a", "specific", "asset", "ID", "."], "sha": "e369b633a727482d5f9e310f0c3337ae5f7265db", "url": "https://github.com/ellmetha/neojsonrpc/blob/e369b633a727482d5f9e310f0c3337ae5f7265db/neojsonrpc/client.py#L85-L96", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/synthesizer/rtlLevel/netlist.py", "func_name": "cut_off_drivers_of", "original_string": "def cut_off_drivers_of(dstSignal, statements):\n    \"\"\"\n    Cut off drivers from statements\n    \"\"\"\n    separated = []\n    stm_filter = []\n    for stm in statements:\n        stm._clean_signal_meta()\n        d = stm._cut_off_drivers_of(dstSignal)\n        if d is not None:\n            separated.append(d)\n\n        f = d is not stm\n        stm_filter.append(f)\n\n    return list(compress(statements, stm_filter)), separated", "language": "python", "code": "def cut_off_drivers_of(dstSignal, statements):\n    \"\"\"\n    Cut off drivers from statements\n    \"\"\"\n    separated = []\n    stm_filter = []\n    for stm in statements:\n        stm._clean_signal_meta()\n        d = stm._cut_off_drivers_of(dstSignal)\n        if d is not None:\n            separated.append(d)\n\n        f = d is not stm\n        stm_filter.append(f)\n\n    return list(compress(statements, stm_filter)), separated", "code_tokens": ["def", "cut_off_drivers_of", "(", "dstSignal", ",", "statements", ")", ":", "separated", "=", "[", "]", "stm_filter", "=", "[", "]", "for", "stm", "in", "statements", ":", "stm", ".", "_clean_signal_meta", "(", ")", "d", "=", "stm", ".", "_cut_off_drivers_of", "(", "dstSignal", ")", "if", "d", "is", "not", "None", ":", "separated", ".", "append", "(", "d", ")", "f", "=", "d", "is", "not", "stm", "stm_filter", ".", "append", "(", "f", ")", "return", "list", "(", "compress", "(", "statements", ",", "stm_filter", ")", ")", ",", "separated"], "docstring": "Cut off drivers from statements", "docstring_tokens": ["Cut", "off", "drivers", "from", "statements"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/synthesizer/rtlLevel/netlist.py#L49-L64", "partition": "test"}
{"repo": "amcat/progressmonitor", "path": "progressmonitor/listener.py", "func_name": "log_listener", "original_string": "def log_listener(log:logging.Logger=None, level=logging.INFO):\n    \"\"\"Progress Monitor listener that logs all updates to the given logger\"\"\"\n    if log is None:\n        log = logging.getLogger(\"ProgressMonitor\")\n    def listen(monitor):\n        name = \"{}: \".format(monitor.name) if monitor.name is not None else \"\"\n        perc = int(monitor.progress * 100)\n        msg = \"[{name}{perc:3d}%] {monitor.message}\".format(**locals())\n        log.log(level, msg)\n    return listen", "language": "python", "code": "def log_listener(log:logging.Logger=None, level=logging.INFO):\n    \"\"\"Progress Monitor listener that logs all updates to the given logger\"\"\"\n    if log is None:\n        log = logging.getLogger(\"ProgressMonitor\")\n    def listen(monitor):\n        name = \"{}: \".format(monitor.name) if monitor.name is not None else \"\"\n        perc = int(monitor.progress * 100)\n        msg = \"[{name}{perc:3d}%] {monitor.message}\".format(**locals())\n        log.log(level, msg)\n    return listen", "code_tokens": ["def", "log_listener", "(", "log", ":", "logging", ".", "Logger", "=", "None", ",", "level", "=", "logging", ".", "INFO", ")", ":", "if", "log", "is", "None", ":", "log", "=", "logging", ".", "getLogger", "(", "\"ProgressMonitor\"", ")", "def", "listen", "(", "monitor", ")", ":", "name", "=", "\"{}: \"", ".", "format", "(", "monitor", ".", "name", ")", "if", "monitor", ".", "name", "is", "not", "None", "else", "\"\"", "perc", "=", "int", "(", "monitor", ".", "progress", "*", "100", ")", "msg", "=", "\"[{name}{perc:3d}%] {monitor.message}\"", ".", "format", "(", "*", "*", "locals", "(", ")", ")", "log", ".", "log", "(", "level", ",", "msg", ")", "return", "listen"], "docstring": "Progress Monitor listener that logs all updates to the given logger", "docstring_tokens": ["Progress", "Monitor", "listener", "that", "logs", "all", "updates", "to", "the", "given", "logger"], "sha": "d4cabebc95bfd1447120f601c094b20bee954285", "url": "https://github.com/amcat/progressmonitor/blob/d4cabebc95bfd1447120f601c094b20bee954285/progressmonitor/listener.py#L13-L22", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/jobs.py", "func_name": "BackfillJob._manage_executor_state", "original_string": "def _manage_executor_state(self, running):\n        \"\"\"\n        Checks if the executor agrees with the state of task instances\n        that are running\n\n        :param running: dict of key, task to verify\n        \"\"\"\n        executor = self.executor\n\n        for key, state in list(executor.get_event_buffer().items()):\n            if key not in running:\n                self.log.warning(\n                    \"%s state %s not in running=%s\",\n                    key, state, running.values()\n                )\n                continue\n\n            ti = running[key]\n            ti.refresh_from_db()\n\n            self.log.debug(\"Executor state: %s task %s\", state, ti)\n\n            if state == State.FAILED or state == State.SUCCESS:\n                if ti.state == State.RUNNING or ti.state == State.QUEUED:\n                    msg = (\"Executor reports task instance {} finished ({}) \"\n                           \"although the task says its {}. Was the task \"\n                           \"killed externally?\".format(ti, state, ti.state))\n                    self.log.error(msg)\n                    ti.handle_failure(msg)", "language": "python", "code": "def _manage_executor_state(self, running):\n        \"\"\"\n        Checks if the executor agrees with the state of task instances\n        that are running\n\n        :param running: dict of key, task to verify\n        \"\"\"\n        executor = self.executor\n\n        for key, state in list(executor.get_event_buffer().items()):\n            if key not in running:\n                self.log.warning(\n                    \"%s state %s not in running=%s\",\n                    key, state, running.values()\n                )\n                continue\n\n            ti = running[key]\n            ti.refresh_from_db()\n\n            self.log.debug(\"Executor state: %s task %s\", state, ti)\n\n            if state == State.FAILED or state == State.SUCCESS:\n                if ti.state == State.RUNNING or ti.state == State.QUEUED:\n                    msg = (\"Executor reports task instance {} finished ({}) \"\n                           \"although the task says its {}. Was the task \"\n                           \"killed externally?\".format(ti, state, ti.state))\n                    self.log.error(msg)\n                    ti.handle_failure(msg)", "code_tokens": ["def", "_manage_executor_state", "(", "self", ",", "running", ")", ":", "executor", "=", "self", ".", "executor", "for", "key", ",", "state", "in", "list", "(", "executor", ".", "get_event_buffer", "(", ")", ".", "items", "(", ")", ")", ":", "if", "key", "not", "in", "running", ":", "self", ".", "log", ".", "warning", "(", "\"%s state %s not in running=%s\"", ",", "key", ",", "state", ",", "running", ".", "values", "(", ")", ")", "continue", "ti", "=", "running", "[", "key", "]", "ti", ".", "refresh_from_db", "(", ")", "self", ".", "log", ".", "debug", "(", "\"Executor state: %s task %s\"", ",", "state", ",", "ti", ")", "if", "state", "==", "State", ".", "FAILED", "or", "state", "==", "State", ".", "SUCCESS", ":", "if", "ti", ".", "state", "==", "State", ".", "RUNNING", "or", "ti", ".", "state", "==", "State", ".", "QUEUED", ":", "msg", "=", "(", "\"Executor reports task instance {} finished ({}) \"", "\"although the task says its {}. Was the task \"", "\"killed externally?\"", ".", "format", "(", "ti", ",", "state", ",", "ti", ".", "state", ")", ")", "self", ".", "log", ".", "error", "(", "msg", ")", "ti", ".", "handle_failure", "(", "msg", ")"], "docstring": "Checks if the executor agrees with the state of task instances\n        that are running\n\n        :param running: dict of key, task to verify", "docstring_tokens": ["Checks", "if", "the", "executor", "agrees", "with", "the", "state", "of", "task", "instances", "that", "are", "running"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L1979-L2007", "partition": "test"}
{"repo": "nesdis/djongo", "path": "djongo/base.py", "func_name": "DatabaseWrapper.get_new_connection", "original_string": "def get_new_connection(self, connection_params):\n        \"\"\"\n        Receives a dictionary connection_params to setup\n        a connection to the database.\n\n        Dictionary correct setup is made through the\n        get_connection_params method.\n\n        TODO: This needs to be made more generic to accept\n        other MongoClient parameters.\n        \"\"\"\n\n        name = connection_params.pop('name')\n        es = connection_params.pop('enforce_schema')\n\n        connection_params['document_class'] = OrderedDict\n        # connection_params['tz_aware'] = True\n        # To prevent leaving unclosed connections behind,\n        # client_conn must be closed before a new connection\n        # is created.\n        if self.client_connection is not None:\n            self.client_connection.close()\n\n        self.client_connection = Database.connect(**connection_params)\n        database = self.client_connection[name]\n        self.djongo_connection = DjongoClient(database, es)\n        return self.client_connection[name]", "language": "python", "code": "def get_new_connection(self, connection_params):\n        \"\"\"\n        Receives a dictionary connection_params to setup\n        a connection to the database.\n\n        Dictionary correct setup is made through the\n        get_connection_params method.\n\n        TODO: This needs to be made more generic to accept\n        other MongoClient parameters.\n        \"\"\"\n\n        name = connection_params.pop('name')\n        es = connection_params.pop('enforce_schema')\n\n        connection_params['document_class'] = OrderedDict\n        # connection_params['tz_aware'] = True\n        # To prevent leaving unclosed connections behind,\n        # client_conn must be closed before a new connection\n        # is created.\n        if self.client_connection is not None:\n            self.client_connection.close()\n\n        self.client_connection = Database.connect(**connection_params)\n        database = self.client_connection[name]\n        self.djongo_connection = DjongoClient(database, es)\n        return self.client_connection[name]", "code_tokens": ["def", "get_new_connection", "(", "self", ",", "connection_params", ")", ":", "name", "=", "connection_params", ".", "pop", "(", "'name'", ")", "es", "=", "connection_params", ".", "pop", "(", "'enforce_schema'", ")", "connection_params", "[", "'document_class'", "]", "=", "OrderedDict", "# connection_params['tz_aware'] = True", "# To prevent leaving unclosed connections behind,", "# client_conn must be closed before a new connection", "# is created.", "if", "self", ".", "client_connection", "is", "not", "None", ":", "self", ".", "client_connection", ".", "close", "(", ")", "self", ".", "client_connection", "=", "Database", ".", "connect", "(", "*", "*", "connection_params", ")", "database", "=", "self", ".", "client_connection", "[", "name", "]", "self", ".", "djongo_connection", "=", "DjongoClient", "(", "database", ",", "es", ")", "return", "self", ".", "client_connection", "[", "name", "]"], "docstring": "Receives a dictionary connection_params to setup\n        a connection to the database.\n\n        Dictionary correct setup is made through the\n        get_connection_params method.\n\n        TODO: This needs to be made more generic to accept\n        other MongoClient parameters.", "docstring_tokens": ["Receives", "a", "dictionary", "connection_params", "to", "setup", "a", "connection", "to", "the", "database", "."], "sha": "7f9d79455cf030cb5eee0b822502c50a0d9d3abb", "url": "https://github.com/nesdis/djongo/blob/7f9d79455cf030cb5eee0b822502c50a0d9d3abb/djongo/base.py#L159-L185", "partition": "test"}
{"repo": "UCBerkeleySETI/blimpy", "path": "blimpy/waterfall.py", "func_name": "Waterfall.__update_header", "original_string": "def __update_header(self):\n        \"\"\" Updates the header information from the original file to the selection.\n        \"\"\"\n\n        #Updating frequency of first channel from selection\n        if self.header[b'foff'] < 0:\n            self.header[b'fch1'] = self.container.f_stop\n        else:\n            self.header[b'fch1'] = self.container.f_start\n\n        #Updating number of coarse channels.\n        self.header[b'nchans'] = self.container.selection_shape[self.freq_axis]\n\n        #Updating time stamp for first time bin from selection\n        self.header[b'tstart'] = self.container.populate_timestamps(update_header=True)", "language": "python", "code": "def __update_header(self):\n        \"\"\" Updates the header information from the original file to the selection.\n        \"\"\"\n\n        #Updating frequency of first channel from selection\n        if self.header[b'foff'] < 0:\n            self.header[b'fch1'] = self.container.f_stop\n        else:\n            self.header[b'fch1'] = self.container.f_start\n\n        #Updating number of coarse channels.\n        self.header[b'nchans'] = self.container.selection_shape[self.freq_axis]\n\n        #Updating time stamp for first time bin from selection\n        self.header[b'tstart'] = self.container.populate_timestamps(update_header=True)", "code_tokens": ["def", "__update_header", "(", "self", ")", ":", "#Updating frequency of first channel from selection", "if", "self", ".", "header", "[", "b'foff'", "]", "<", "0", ":", "self", ".", "header", "[", "b'fch1'", "]", "=", "self", ".", "container", ".", "f_stop", "else", ":", "self", ".", "header", "[", "b'fch1'", "]", "=", "self", ".", "container", ".", "f_start", "#Updating number of coarse channels.", "self", ".", "header", "[", "b'nchans'", "]", "=", "self", ".", "container", ".", "selection_shape", "[", "self", ".", "freq_axis", "]", "#Updating time stamp for first time bin from selection", "self", ".", "header", "[", "b'tstart'", "]", "=", "self", ".", "container", ".", "populate_timestamps", "(", "update_header", "=", "True", ")"], "docstring": "Updates the header information from the original file to the selection.", "docstring_tokens": ["Updates", "the", "header", "information", "from", "the", "original", "file", "to", "the", "selection", "."], "sha": "b8822d3e3e911944370d84371a91fa0c29e9772e", "url": "https://github.com/UCBerkeleySETI/blimpy/blob/b8822d3e3e911944370d84371a91fa0c29e9772e/blimpy/waterfall.py#L167-L181", "partition": "test"}
{"repo": "codelv/enaml-web", "path": "web/impl/lxml_toolkit_object.py", "func_name": "WebComponent.create_widget", "original_string": "def create_widget(self):\n        \"\"\" Create the toolkit widget for the proxy object.\n\n        This method is called during the top-down pass, just before the\n        'init_widget()' method is called. This method should create the\n        toolkit widget and assign it to the 'widget' attribute.\n\n        \"\"\"\n        self.widget = SubElement(self.parent_widget(), self.declaration.tag)", "language": "python", "code": "def create_widget(self):\n        \"\"\" Create the toolkit widget for the proxy object.\n\n        This method is called during the top-down pass, just before the\n        'init_widget()' method is called. This method should create the\n        toolkit widget and assign it to the 'widget' attribute.\n\n        \"\"\"\n        self.widget = SubElement(self.parent_widget(), self.declaration.tag)", "code_tokens": ["def", "create_widget", "(", "self", ")", ":", "self", ".", "widget", "=", "SubElement", "(", "self", ".", "parent_widget", "(", ")", ",", "self", ".", "declaration", ".", "tag", ")"], "docstring": "Create the toolkit widget for the proxy object.\n\n        This method is called during the top-down pass, just before the\n        'init_widget()' method is called. This method should create the\n        toolkit widget and assign it to the 'widget' attribute.", "docstring_tokens": ["Create", "the", "toolkit", "widget", "for", "the", "proxy", "object", "."], "sha": "88f1131a7b3ba9e83467b4f44bc3bab6f0de7559", "url": "https://github.com/codelv/enaml-web/blob/88f1131a7b3ba9e83467b4f44bc3bab6f0de7559/web/impl/lxml_toolkit_object.py#L34-L42", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/imap_hook.py", "func_name": "ImapHook.download_mail_attachments", "original_string": "def download_mail_attachments(self,\n                                  name,\n                                  local_output_directory,\n                                  mail_folder='INBOX',\n                                  check_regex=False,\n                                  latest_only=False,\n                                  not_found_mode='raise'):\n        \"\"\"\n        Downloads mail's attachments in the mail folder by its name to the local directory.\n\n        :param name: The name of the attachment that will be downloaded.\n        :type name: str\n        :param local_output_directory: The output directory on the local machine\n                                       where the files will be downloaded to.\n        :type local_output_directory: str\n        :param mail_folder: The mail folder where to look at.\n        :type mail_folder: str\n        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :param latest_only: If set to True it will only download\n                            the first matched attachment.\n        :type latest_only: bool\n        :param not_found_mode: Specify what should happen if no attachment has been found.\n                               Supported values are 'raise', 'warn' and 'ignore'.\n                               If it is set to 'raise' it will raise an exception,\n                               if set to 'warn' it will only print a warning and\n                               if set to 'ignore' it won't notify you at all.\n        :type not_found_mode: str\n        \"\"\"\n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only)\n\n        if not mail_attachments:\n            self._handle_not_found_mode(not_found_mode)\n\n        self._create_files(mail_attachments, local_output_directory)", "language": "python", "code": "def download_mail_attachments(self,\n                                  name,\n                                  local_output_directory,\n                                  mail_folder='INBOX',\n                                  check_regex=False,\n                                  latest_only=False,\n                                  not_found_mode='raise'):\n        \"\"\"\n        Downloads mail's attachments in the mail folder by its name to the local directory.\n\n        :param name: The name of the attachment that will be downloaded.\n        :type name: str\n        :param local_output_directory: The output directory on the local machine\n                                       where the files will be downloaded to.\n        :type local_output_directory: str\n        :param mail_folder: The mail folder where to look at.\n        :type mail_folder: str\n        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :param latest_only: If set to True it will only download\n                            the first matched attachment.\n        :type latest_only: bool\n        :param not_found_mode: Specify what should happen if no attachment has been found.\n                               Supported values are 'raise', 'warn' and 'ignore'.\n                               If it is set to 'raise' it will raise an exception,\n                               if set to 'warn' it will only print a warning and\n                               if set to 'ignore' it won't notify you at all.\n        :type not_found_mode: str\n        \"\"\"\n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only)\n\n        if not mail_attachments:\n            self._handle_not_found_mode(not_found_mode)\n\n        self._create_files(mail_attachments, local_output_directory)", "code_tokens": ["def", "download_mail_attachments", "(", "self", ",", "name", ",", "local_output_directory", ",", "mail_folder", "=", "'INBOX'", ",", "check_regex", "=", "False", ",", "latest_only", "=", "False", ",", "not_found_mode", "=", "'raise'", ")", ":", "mail_attachments", "=", "self", ".", "_retrieve_mails_attachments_by_name", "(", "name", ",", "mail_folder", ",", "check_regex", ",", "latest_only", ")", "if", "not", "mail_attachments", ":", "self", ".", "_handle_not_found_mode", "(", "not_found_mode", ")", "self", ".", "_create_files", "(", "mail_attachments", ",", "local_output_directory", ")"], "docstring": "Downloads mail's attachments in the mail folder by its name to the local directory.\n\n        :param name: The name of the attachment that will be downloaded.\n        :type name: str\n        :param local_output_directory: The output directory on the local machine\n                                       where the files will be downloaded to.\n        :type local_output_directory: str\n        :param mail_folder: The mail folder where to look at.\n        :type mail_folder: str\n        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :param latest_only: If set to True it will only download\n                            the first matched attachment.\n        :type latest_only: bool\n        :param not_found_mode: Specify what should happen if no attachment has been found.\n                               Supported values are 'raise', 'warn' and 'ignore'.\n                               If it is set to 'raise' it will raise an exception,\n                               if set to 'warn' it will only print a warning and\n                               if set to 'ignore' it won't notify you at all.\n        :type not_found_mode: str", "docstring_tokens": ["Downloads", "mail", "s", "attachments", "in", "the", "mail", "folder", "by", "its", "name", "to", "the", "local", "directory", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/imap_hook.py#L104-L141", "partition": "test"}
{"repo": "pyca/pyopenssl", "path": "src/OpenSSL/crypto.py", "func_name": "X509.get_issuer", "original_string": "def get_issuer(self):\n        \"\"\"\n        Return the issuer of this certificate.\n\n        This creates a new :class:`X509Name` that wraps the underlying issuer\n        name field on the certificate. Modifying it will modify the underlying\n        certificate, and will have the effect of modifying any other\n        :class:`X509Name` that refers to this issuer.\n\n        :return: The issuer of this certificate.\n        :rtype: :class:`X509Name`\n        \"\"\"\n        name = self._get_name(_lib.X509_get_issuer_name)\n        self._issuer_invalidator.add(name)\n        return name", "language": "python", "code": "def get_issuer(self):\n        \"\"\"\n        Return the issuer of this certificate.\n\n        This creates a new :class:`X509Name` that wraps the underlying issuer\n        name field on the certificate. Modifying it will modify the underlying\n        certificate, and will have the effect of modifying any other\n        :class:`X509Name` that refers to this issuer.\n\n        :return: The issuer of this certificate.\n        :rtype: :class:`X509Name`\n        \"\"\"\n        name = self._get_name(_lib.X509_get_issuer_name)\n        self._issuer_invalidator.add(name)\n        return name", "code_tokens": ["def", "get_issuer", "(", "self", ")", ":", "name", "=", "self", ".", "_get_name", "(", "_lib", ".", "X509_get_issuer_name", ")", "self", ".", "_issuer_invalidator", ".", "add", "(", "name", ")", "return", "name"], "docstring": "Return the issuer of this certificate.\n\n        This creates a new :class:`X509Name` that wraps the underlying issuer\n        name field on the certificate. Modifying it will modify the underlying\n        certificate, and will have the effect of modifying any other\n        :class:`X509Name` that refers to this issuer.\n\n        :return: The issuer of this certificate.\n        :rtype: :class:`X509Name`", "docstring_tokens": ["Return", "the", "issuer", "of", "this", "certificate", "."], "sha": "1fbe064c50fd030948141d7d630673761525b0d0", "url": "https://github.com/pyca/pyopenssl/blob/1fbe064c50fd030948141d7d630673761525b0d0/src/OpenSSL/crypto.py#L1410-L1424", "partition": "test"}
{"repo": "quantopian/pgcontents", "path": "pgcontents/pgmanager.py", "func_name": "PostgresContentsManager.rename_file", "original_string": "def rename_file(self, old_path, path):\n        \"\"\"\n        Rename object from old_path to path.\n\n        NOTE: This method is unfortunately named on the base class.  It\n        actually moves a file or a directory.\n        \"\"\"\n        with self.engine.begin() as db:\n            try:\n                if self.file_exists(old_path):\n                    rename_file(db, self.user_id, old_path, path)\n                elif self.dir_exists(old_path):\n                    rename_directory(db, self.user_id, old_path, path)\n                else:\n                    self.no_such_entity(path)\n            except (FileExists, DirectoryExists):\n                self.already_exists(path)\n            except RenameRoot as e:\n                self.do_409(str(e))", "language": "python", "code": "def rename_file(self, old_path, path):\n        \"\"\"\n        Rename object from old_path to path.\n\n        NOTE: This method is unfortunately named on the base class.  It\n        actually moves a file or a directory.\n        \"\"\"\n        with self.engine.begin() as db:\n            try:\n                if self.file_exists(old_path):\n                    rename_file(db, self.user_id, old_path, path)\n                elif self.dir_exists(old_path):\n                    rename_directory(db, self.user_id, old_path, path)\n                else:\n                    self.no_such_entity(path)\n            except (FileExists, DirectoryExists):\n                self.already_exists(path)\n            except RenameRoot as e:\n                self.do_409(str(e))", "code_tokens": ["def", "rename_file", "(", "self", ",", "old_path", ",", "path", ")", ":", "with", "self", ".", "engine", ".", "begin", "(", ")", "as", "db", ":", "try", ":", "if", "self", ".", "file_exists", "(", "old_path", ")", ":", "rename_file", "(", "db", ",", "self", ".", "user_id", ",", "old_path", ",", "path", ")", "elif", "self", ".", "dir_exists", "(", "old_path", ")", ":", "rename_directory", "(", "db", ",", "self", ".", "user_id", ",", "old_path", ",", "path", ")", "else", ":", "self", ".", "no_such_entity", "(", "path", ")", "except", "(", "FileExists", ",", "DirectoryExists", ")", ":", "self", ".", "already_exists", "(", "path", ")", "except", "RenameRoot", "as", "e", ":", "self", ".", "do_409", "(", "str", "(", "e", ")", ")"], "docstring": "Rename object from old_path to path.\n\n        NOTE: This method is unfortunately named on the base class.  It\n        actually moves a file or a directory.", "docstring_tokens": ["Rename", "object", "from", "old_path", "to", "path", "."], "sha": "ed36268b7917332d16868208e1e565742a8753e1", "url": "https://github.com/quantopian/pgcontents/blob/ed36268b7917332d16868208e1e565742a8753e1/pgcontents/pgmanager.py#L380-L398", "partition": "test"}
{"repo": "saxix/sample-data-utils", "path": "sample_data_utils/people.py", "func_name": "last_name", "original_string": "def last_name(languages=None):\n    \"\"\"\n        return a random last name\n\n    >>> from mock import patch\n    >>> with patch('%s._get_lastnames' % __name__, lambda *args: ['aaa']):\n    ...     last_name()\n    'Aaa'\n    >>> with patch('%s.get_lastnames' % __name__, lambda lang: ['%s_lastname'% lang]):\n    ...     last_name(['it'])\n    'It_Lastname'\n    \"\"\"\n    choices = []\n    languages = languages or ['en']\n    for lang in languages:\n        samples = _get_lastnames(lang)\n        choices.extend(samples)\n    return random.choice(choices).title()", "language": "python", "code": "def last_name(languages=None):\n    \"\"\"\n        return a random last name\n\n    >>> from mock import patch\n    >>> with patch('%s._get_lastnames' % __name__, lambda *args: ['aaa']):\n    ...     last_name()\n    'Aaa'\n    >>> with patch('%s.get_lastnames' % __name__, lambda lang: ['%s_lastname'% lang]):\n    ...     last_name(['it'])\n    'It_Lastname'\n    \"\"\"\n    choices = []\n    languages = languages or ['en']\n    for lang in languages:\n        samples = _get_lastnames(lang)\n        choices.extend(samples)\n    return random.choice(choices).title()", "code_tokens": ["def", "last_name", "(", "languages", "=", "None", ")", ":", "choices", "=", "[", "]", "languages", "=", "languages", "or", "[", "'en'", "]", "for", "lang", "in", "languages", ":", "samples", "=", "_get_lastnames", "(", "lang", ")", "choices", ".", "extend", "(", "samples", ")", "return", "random", ".", "choice", "(", "choices", ")", ".", "title", "(", ")"], "docstring": "return a random last name\n\n    >>> from mock import patch\n    >>> with patch('%s._get_lastnames' % __name__, lambda *args: ['aaa']):\n    ...     last_name()\n    'Aaa'\n    >>> with patch('%s.get_lastnames' % __name__, lambda lang: ['%s_lastname'% lang]):\n    ...     last_name(['it'])\n    'It_Lastname'", "docstring_tokens": ["return", "a", "random", "last", "name"], "sha": "769f1b46e60def2675a14bd5872047af6d1ea398", "url": "https://github.com/saxix/sample-data-utils/blob/769f1b46e60def2675a14bd5872047af6d1ea398/sample_data_utils/people.py#L118-L135", "partition": "test"}
{"repo": "pyca/pyopenssl", "path": "src/OpenSSL/crypto.py", "func_name": "X509Store.set_time", "original_string": "def set_time(self, vfy_time):\n        \"\"\"\n        Set the time against which the certificates are verified.\n\n        Normally the current time is used.\n\n        .. note::\n\n          For example, you can determine if a certificate was valid at a given\n          time.\n\n        .. versionadded:: 17.0.0\n\n        :param datetime vfy_time: The verification time to set on this store.\n        :return: ``None`` if the verification time was successfully set.\n        \"\"\"\n        param = _lib.X509_VERIFY_PARAM_new()\n        param = _ffi.gc(param, _lib.X509_VERIFY_PARAM_free)\n\n        _lib.X509_VERIFY_PARAM_set_time(param, int(vfy_time.strftime('%s')))\n        _openssl_assert(_lib.X509_STORE_set1_param(self._store, param) != 0)", "language": "python", "code": "def set_time(self, vfy_time):\n        \"\"\"\n        Set the time against which the certificates are verified.\n\n        Normally the current time is used.\n\n        .. note::\n\n          For example, you can determine if a certificate was valid at a given\n          time.\n\n        .. versionadded:: 17.0.0\n\n        :param datetime vfy_time: The verification time to set on this store.\n        :return: ``None`` if the verification time was successfully set.\n        \"\"\"\n        param = _lib.X509_VERIFY_PARAM_new()\n        param = _ffi.gc(param, _lib.X509_VERIFY_PARAM_free)\n\n        _lib.X509_VERIFY_PARAM_set_time(param, int(vfy_time.strftime('%s')))\n        _openssl_assert(_lib.X509_STORE_set1_param(self._store, param) != 0)", "code_tokens": ["def", "set_time", "(", "self", ",", "vfy_time", ")", ":", "param", "=", "_lib", ".", "X509_VERIFY_PARAM_new", "(", ")", "param", "=", "_ffi", ".", "gc", "(", "param", ",", "_lib", ".", "X509_VERIFY_PARAM_free", ")", "_lib", ".", "X509_VERIFY_PARAM_set_time", "(", "param", ",", "int", "(", "vfy_time", ".", "strftime", "(", "'%s'", ")", ")", ")", "_openssl_assert", "(", "_lib", ".", "X509_STORE_set1_param", "(", "self", ".", "_store", ",", "param", ")", "!=", "0", ")"], "docstring": "Set the time against which the certificates are verified.\n\n        Normally the current time is used.\n\n        .. note::\n\n          For example, you can determine if a certificate was valid at a given\n          time.\n\n        .. versionadded:: 17.0.0\n\n        :param datetime vfy_time: The verification time to set on this store.\n        :return: ``None`` if the verification time was successfully set.", "docstring_tokens": ["Set", "the", "time", "against", "which", "the", "certificates", "are", "verified", "."], "sha": "1fbe064c50fd030948141d7d630673761525b0d0", "url": "https://github.com/pyca/pyopenssl/blob/1fbe064c50fd030948141d7d630673761525b0d0/src/OpenSSL/crypto.py#L1628-L1648", "partition": "test"}
{"repo": "lincolnloop/goodconf", "path": "goodconf/__init__.py", "func_name": "GoodConf.generate_yaml", "original_string": "def generate_yaml(cls, **override):\n        \"\"\"\n        Dumps initial config in YAML\n        \"\"\"\n        import ruamel.yaml\n        yaml = ruamel.yaml.YAML()\n        yaml_str = StringIO()\n        yaml.dump(cls.get_initial(**override), stream=yaml_str)\n        yaml_str.seek(0)\n        dict_from_yaml = yaml.load(yaml_str)\n        if cls.__doc__:\n            dict_from_yaml.yaml_set_start_comment(\n                '\\n' + cls.__doc__ + '\\n\\n')\n        for k in dict_from_yaml.keys():\n            if cls._values[k].help:\n                dict_from_yaml.yaml_set_comment_before_after_key(\n                    k, before='\\n' + cls._values[k].help)\n        yaml_str = StringIO()\n        yaml.dump(dict_from_yaml, yaml_str)\n        yaml_str.seek(0)\n        return yaml_str.read()", "language": "python", "code": "def generate_yaml(cls, **override):\n        \"\"\"\n        Dumps initial config in YAML\n        \"\"\"\n        import ruamel.yaml\n        yaml = ruamel.yaml.YAML()\n        yaml_str = StringIO()\n        yaml.dump(cls.get_initial(**override), stream=yaml_str)\n        yaml_str.seek(0)\n        dict_from_yaml = yaml.load(yaml_str)\n        if cls.__doc__:\n            dict_from_yaml.yaml_set_start_comment(\n                '\\n' + cls.__doc__ + '\\n\\n')\n        for k in dict_from_yaml.keys():\n            if cls._values[k].help:\n                dict_from_yaml.yaml_set_comment_before_after_key(\n                    k, before='\\n' + cls._values[k].help)\n        yaml_str = StringIO()\n        yaml.dump(dict_from_yaml, yaml_str)\n        yaml_str.seek(0)\n        return yaml_str.read()", "code_tokens": ["def", "generate_yaml", "(", "cls", ",", "*", "*", "override", ")", ":", "import", "ruamel", ".", "yaml", "yaml", "=", "ruamel", ".", "yaml", ".", "YAML", "(", ")", "yaml_str", "=", "StringIO", "(", ")", "yaml", ".", "dump", "(", "cls", ".", "get_initial", "(", "*", "*", "override", ")", ",", "stream", "=", "yaml_str", ")", "yaml_str", ".", "seek", "(", "0", ")", "dict_from_yaml", "=", "yaml", ".", "load", "(", "yaml_str", ")", "if", "cls", ".", "__doc__", ":", "dict_from_yaml", ".", "yaml_set_start_comment", "(", "'\\n'", "+", "cls", ".", "__doc__", "+", "'\\n\\n'", ")", "for", "k", "in", "dict_from_yaml", ".", "keys", "(", ")", ":", "if", "cls", ".", "_values", "[", "k", "]", ".", "help", ":", "dict_from_yaml", ".", "yaml_set_comment_before_after_key", "(", "k", ",", "before", "=", "'\\n'", "+", "cls", ".", "_values", "[", "k", "]", ".", "help", ")", "yaml_str", "=", "StringIO", "(", ")", "yaml", ".", "dump", "(", "dict_from_yaml", ",", "yaml_str", ")", "yaml_str", ".", "seek", "(", "0", ")", "return", "yaml_str", ".", "read", "(", ")"], "docstring": "Dumps initial config in YAML", "docstring_tokens": ["Dumps", "initial", "config", "in", "YAML"], "sha": "19515da5783f86b9516dbf81531107c2d9eae567", "url": "https://github.com/lincolnloop/goodconf/blob/19515da5783f86b9516dbf81531107c2d9eae567/goodconf/__init__.py#L99-L119", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/supybot.py", "func_name": "SupybotParser.parse", "original_string": "def parse(self):\n        \"\"\"Parse a Supybot IRC stream.\n\n        Returns an iterator of dicts. Each dicts contains information\n        about the date, type, nick and body of a single log entry.\n\n        :returns: iterator of parsed lines\n\n        :raises ParseError: when an invalid line is found parsing the given\n            stream\n        \"\"\"\n        for line in self.stream:\n            line = line.rstrip('\\n')\n            self.nline += 1\n\n            if self.SUPYBOT_EMPTY_REGEX.match(line):\n                continue\n\n            ts, msg = self._parse_supybot_timestamp(line)\n\n            if self.SUPYBOT_EMPTY_COMMENT_REGEX.match(msg):\n                continue\n            elif self.SUPYBOT_EMPTY_COMMENT_ACTION_REGEX.match(msg):\n                continue\n            elif self.SUPYBOT_EMPTY_BOT_REGEX.match(msg):\n                continue\n\n            itype, nick, body = self._parse_supybot_msg(msg)\n            item = self._build_item(ts, itype, nick, body)\n\n            yield item", "language": "python", "code": "def parse(self):\n        \"\"\"Parse a Supybot IRC stream.\n\n        Returns an iterator of dicts. Each dicts contains information\n        about the date, type, nick and body of a single log entry.\n\n        :returns: iterator of parsed lines\n\n        :raises ParseError: when an invalid line is found parsing the given\n            stream\n        \"\"\"\n        for line in self.stream:\n            line = line.rstrip('\\n')\n            self.nline += 1\n\n            if self.SUPYBOT_EMPTY_REGEX.match(line):\n                continue\n\n            ts, msg = self._parse_supybot_timestamp(line)\n\n            if self.SUPYBOT_EMPTY_COMMENT_REGEX.match(msg):\n                continue\n            elif self.SUPYBOT_EMPTY_COMMENT_ACTION_REGEX.match(msg):\n                continue\n            elif self.SUPYBOT_EMPTY_BOT_REGEX.match(msg):\n                continue\n\n            itype, nick, body = self._parse_supybot_msg(msg)\n            item = self._build_item(ts, itype, nick, body)\n\n            yield item", "code_tokens": ["def", "parse", "(", "self", ")", ":", "for", "line", "in", "self", ".", "stream", ":", "line", "=", "line", ".", "rstrip", "(", "'\\n'", ")", "self", ".", "nline", "+=", "1", "if", "self", ".", "SUPYBOT_EMPTY_REGEX", ".", "match", "(", "line", ")", ":", "continue", "ts", ",", "msg", "=", "self", ".", "_parse_supybot_timestamp", "(", "line", ")", "if", "self", ".", "SUPYBOT_EMPTY_COMMENT_REGEX", ".", "match", "(", "msg", ")", ":", "continue", "elif", "self", ".", "SUPYBOT_EMPTY_COMMENT_ACTION_REGEX", ".", "match", "(", "msg", ")", ":", "continue", "elif", "self", ".", "SUPYBOT_EMPTY_BOT_REGEX", ".", "match", "(", "msg", ")", ":", "continue", "itype", ",", "nick", ",", "body", "=", "self", ".", "_parse_supybot_msg", "(", "msg", ")", "item", "=", "self", ".", "_build_item", "(", "ts", ",", "itype", ",", "nick", ",", "body", ")", "yield", "item"], "docstring": "Parse a Supybot IRC stream.\n\n        Returns an iterator of dicts. Each dicts contains information\n        about the date, type, nick and body of a single log entry.\n\n        :returns: iterator of parsed lines\n\n        :raises ParseError: when an invalid line is found parsing the given\n            stream", "docstring_tokens": ["Parse", "a", "Supybot", "IRC", "stream", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/supybot.py#L345-L375", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/jobs.py", "func_name": "SchedulerJob._change_state_for_executable_task_instances", "original_string": "def _change_state_for_executable_task_instances(self, task_instances,\n                                                    acceptable_states, session=None):\n        \"\"\"\n        Changes the state of task instances in the list with one of the given states\n        to QUEUED atomically, and returns the TIs changed in SimpleTaskInstance format.\n\n        :param task_instances: TaskInstances to change the state of\n        :type task_instances: list[airflow.models.TaskInstance]\n        :param acceptable_states: Filters the TaskInstances updated to be in these states\n        :type acceptable_states: Iterable[State]\n        :rtype: list[airflow.utils.dag_processing.SimpleTaskInstance]\n        \"\"\"\n        if len(task_instances) == 0:\n            session.commit()\n            return []\n\n        TI = models.TaskInstance\n        filter_for_ti_state_change = (\n            [and_(\n                TI.dag_id == ti.dag_id,\n                TI.task_id == ti.task_id,\n                TI.execution_date == ti.execution_date)\n                for ti in task_instances])\n        ti_query = (\n            session\n            .query(TI)\n            .filter(or_(*filter_for_ti_state_change)))\n\n        if None in acceptable_states:\n            ti_query = ti_query.filter(\n                or_(TI.state == None, TI.state.in_(acceptable_states))  # noqa: E711\n            )\n        else:\n            ti_query = ti_query.filter(TI.state.in_(acceptable_states))\n\n        tis_to_set_to_queued = (\n            ti_query\n            .with_for_update()\n            .all())\n        if len(tis_to_set_to_queued) == 0:\n            self.log.info(\"No tasks were able to have their state changed to queued.\")\n            session.commit()\n            return []\n\n        # set TIs to queued state\n        for task_instance in tis_to_set_to_queued:\n            task_instance.state = State.QUEUED\n            task_instance.queued_dttm = (timezone.utcnow()\n                                         if not task_instance.queued_dttm\n                                         else task_instance.queued_dttm)\n            session.merge(task_instance)\n\n        # Generate a list of SimpleTaskInstance for the use of queuing\n        # them in the executor.\n        simple_task_instances = [SimpleTaskInstance(ti) for ti in\n                                 tis_to_set_to_queued]\n\n        task_instance_str = \"\\n\\t\".join(\n            [repr(x) for x in tis_to_set_to_queued])\n\n        session.commit()\n        self.log.info(\"Setting the following %s tasks to queued state:\\n\\t%s\",\n                      len(tis_to_set_to_queued), task_instance_str)\n        return simple_task_instances", "language": "python", "code": "def _change_state_for_executable_task_instances(self, task_instances,\n                                                    acceptable_states, session=None):\n        \"\"\"\n        Changes the state of task instances in the list with one of the given states\n        to QUEUED atomically, and returns the TIs changed in SimpleTaskInstance format.\n\n        :param task_instances: TaskInstances to change the state of\n        :type task_instances: list[airflow.models.TaskInstance]\n        :param acceptable_states: Filters the TaskInstances updated to be in these states\n        :type acceptable_states: Iterable[State]\n        :rtype: list[airflow.utils.dag_processing.SimpleTaskInstance]\n        \"\"\"\n        if len(task_instances) == 0:\n            session.commit()\n            return []\n\n        TI = models.TaskInstance\n        filter_for_ti_state_change = (\n            [and_(\n                TI.dag_id == ti.dag_id,\n                TI.task_id == ti.task_id,\n                TI.execution_date == ti.execution_date)\n                for ti in task_instances])\n        ti_query = (\n            session\n            .query(TI)\n            .filter(or_(*filter_for_ti_state_change)))\n\n        if None in acceptable_states:\n            ti_query = ti_query.filter(\n                or_(TI.state == None, TI.state.in_(acceptable_states))  # noqa: E711\n            )\n        else:\n            ti_query = ti_query.filter(TI.state.in_(acceptable_states))\n\n        tis_to_set_to_queued = (\n            ti_query\n            .with_for_update()\n            .all())\n        if len(tis_to_set_to_queued) == 0:\n            self.log.info(\"No tasks were able to have their state changed to queued.\")\n            session.commit()\n            return []\n\n        # set TIs to queued state\n        for task_instance in tis_to_set_to_queued:\n            task_instance.state = State.QUEUED\n            task_instance.queued_dttm = (timezone.utcnow()\n                                         if not task_instance.queued_dttm\n                                         else task_instance.queued_dttm)\n            session.merge(task_instance)\n\n        # Generate a list of SimpleTaskInstance for the use of queuing\n        # them in the executor.\n        simple_task_instances = [SimpleTaskInstance(ti) for ti in\n                                 tis_to_set_to_queued]\n\n        task_instance_str = \"\\n\\t\".join(\n            [repr(x) for x in tis_to_set_to_queued])\n\n        session.commit()\n        self.log.info(\"Setting the following %s tasks to queued state:\\n\\t%s\",\n                      len(tis_to_set_to_queued), task_instance_str)\n        return simple_task_instances", "code_tokens": ["def", "_change_state_for_executable_task_instances", "(", "self", ",", "task_instances", ",", "acceptable_states", ",", "session", "=", "None", ")", ":", "if", "len", "(", "task_instances", ")", "==", "0", ":", "session", ".", "commit", "(", ")", "return", "[", "]", "TI", "=", "models", ".", "TaskInstance", "filter_for_ti_state_change", "=", "(", "[", "and_", "(", "TI", ".", "dag_id", "==", "ti", ".", "dag_id", ",", "TI", ".", "task_id", "==", "ti", ".", "task_id", ",", "TI", ".", "execution_date", "==", "ti", ".", "execution_date", ")", "for", "ti", "in", "task_instances", "]", ")", "ti_query", "=", "(", "session", ".", "query", "(", "TI", ")", ".", "filter", "(", "or_", "(", "*", "filter_for_ti_state_change", ")", ")", ")", "if", "None", "in", "acceptable_states", ":", "ti_query", "=", "ti_query", ".", "filter", "(", "or_", "(", "TI", ".", "state", "==", "None", ",", "TI", ".", "state", ".", "in_", "(", "acceptable_states", ")", ")", "# noqa: E711", ")", "else", ":", "ti_query", "=", "ti_query", ".", "filter", "(", "TI", ".", "state", ".", "in_", "(", "acceptable_states", ")", ")", "tis_to_set_to_queued", "=", "(", "ti_query", ".", "with_for_update", "(", ")", ".", "all", "(", ")", ")", "if", "len", "(", "tis_to_set_to_queued", ")", "==", "0", ":", "self", ".", "log", ".", "info", "(", "\"No tasks were able to have their state changed to queued.\"", ")", "session", ".", "commit", "(", ")", "return", "[", "]", "# set TIs to queued state", "for", "task_instance", "in", "tis_to_set_to_queued", ":", "task_instance", ".", "state", "=", "State", ".", "QUEUED", "task_instance", ".", "queued_dttm", "=", "(", "timezone", ".", "utcnow", "(", ")", "if", "not", "task_instance", ".", "queued_dttm", "else", "task_instance", ".", "queued_dttm", ")", "session", ".", "merge", "(", "task_instance", ")", "# Generate a list of SimpleTaskInstance for the use of queuing", "# them in the executor.", "simple_task_instances", "=", "[", "SimpleTaskInstance", "(", "ti", ")", "for", "ti", "in", "tis_to_set_to_queued", "]", "task_instance_str", "=", "\"\\n\\t\"", ".", "join", "(", "[", "repr", "(", "x", ")", "for", "x", "in", "tis_to_set_to_queued", "]", ")", "session", ".", "commit", "(", ")", "self", ".", "log", ".", "info", "(", "\"Setting the following %s tasks to queued state:\\n\\t%s\"", ",", "len", "(", "tis_to_set_to_queued", ")", ",", "task_instance_str", ")", "return", "simple_task_instances"], "docstring": "Changes the state of task instances in the list with one of the given states\n        to QUEUED atomically, and returns the TIs changed in SimpleTaskInstance format.\n\n        :param task_instances: TaskInstances to change the state of\n        :type task_instances: list[airflow.models.TaskInstance]\n        :param acceptable_states: Filters the TaskInstances updated to be in these states\n        :type acceptable_states: Iterable[State]\n        :rtype: list[airflow.utils.dag_processing.SimpleTaskInstance]", "docstring_tokens": ["Changes", "the", "state", "of", "task", "instances", "in", "the", "list", "with", "one", "of", "the", "given", "states", "to", "QUEUED", "atomically", "and", "returns", "the", "TIs", "changed", "in", "SimpleTaskInstance", "format", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L1217-L1280", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/utils/progressbar.py", "func_name": "ProgressBar._draw", "original_string": "def _draw(self, txt, final=False):\n        \"\"\"Print the rendered string to the stdout.\"\"\"\n        if not self._file_mode:\n            # If the user presses Ctrl+C this ensures we still start writing from the beginning of the line\n            sys.stdout.write(\"\\r\")\n        sys.stdout.write(txt)\n        if final and not isinstance(self._widget, _HiddenWidget):\n            sys.stdout.write(\"\\n\")\n        else:\n            if not self._file_mode:\n                sys.stdout.write(\"\\r\")\n            sys.stdout.flush()", "language": "python", "code": "def _draw(self, txt, final=False):\n        \"\"\"Print the rendered string to the stdout.\"\"\"\n        if not self._file_mode:\n            # If the user presses Ctrl+C this ensures we still start writing from the beginning of the line\n            sys.stdout.write(\"\\r\")\n        sys.stdout.write(txt)\n        if final and not isinstance(self._widget, _HiddenWidget):\n            sys.stdout.write(\"\\n\")\n        else:\n            if not self._file_mode:\n                sys.stdout.write(\"\\r\")\n            sys.stdout.flush()", "code_tokens": ["def", "_draw", "(", "self", ",", "txt", ",", "final", "=", "False", ")", ":", "if", "not", "self", ".", "_file_mode", ":", "# If the user presses Ctrl+C this ensures we still start writing from the beginning of the line", "sys", ".", "stdout", ".", "write", "(", "\"\\r\"", ")", "sys", ".", "stdout", ".", "write", "(", "txt", ")", "if", "final", "and", "not", "isinstance", "(", "self", ".", "_widget", ",", "_HiddenWidget", ")", ":", "sys", ".", "stdout", ".", "write", "(", "\"\\n\"", ")", "else", ":", "if", "not", "self", ".", "_file_mode", ":", "sys", ".", "stdout", ".", "write", "(", "\"\\r\"", ")", "sys", ".", "stdout", ".", "flush", "(", ")"], "docstring": "Print the rendered string to the stdout.", "docstring_tokens": ["Print", "the", "rendered", "string", "to", "the", "stdout", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/utils/progressbar.py#L352-L363", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/parse/variant/ids.py", "func_name": "parse_ids", "original_string": "def parse_ids(chrom, pos, ref, alt, case_id, variant_type):\n    \"\"\"Construct the necessary ids for a variant\n\n    Args:\n        chrom(str): Variant chromosome\n        pos(int): Variant position\n        ref(str): Variant reference\n        alt(str): Variant alternative\n        case_id(str): Unique case id\n        variant_type(str): 'clinical' or 'research'\n\n    Returns:\n        ids(dict): Dictionary with the relevant ids\n    \"\"\"\n    ids = {}\n    pos = str(pos)\n\n    ids['simple_id'] = parse_simple_id(chrom, pos, ref, alt)\n    ids['variant_id'] = parse_variant_id(chrom, pos, ref, alt, variant_type)\n    ids['display_name'] = parse_display_name(chrom, pos, ref, alt, variant_type)\n    ids['document_id'] = parse_document_id(chrom, pos, ref, alt, variant_type, case_id)\n\n    return ids", "language": "python", "code": "def parse_ids(chrom, pos, ref, alt, case_id, variant_type):\n    \"\"\"Construct the necessary ids for a variant\n\n    Args:\n        chrom(str): Variant chromosome\n        pos(int): Variant position\n        ref(str): Variant reference\n        alt(str): Variant alternative\n        case_id(str): Unique case id\n        variant_type(str): 'clinical' or 'research'\n\n    Returns:\n        ids(dict): Dictionary with the relevant ids\n    \"\"\"\n    ids = {}\n    pos = str(pos)\n\n    ids['simple_id'] = parse_simple_id(chrom, pos, ref, alt)\n    ids['variant_id'] = parse_variant_id(chrom, pos, ref, alt, variant_type)\n    ids['display_name'] = parse_display_name(chrom, pos, ref, alt, variant_type)\n    ids['document_id'] = parse_document_id(chrom, pos, ref, alt, variant_type, case_id)\n\n    return ids", "code_tokens": ["def", "parse_ids", "(", "chrom", ",", "pos", ",", "ref", ",", "alt", ",", "case_id", ",", "variant_type", ")", ":", "ids", "=", "{", "}", "pos", "=", "str", "(", "pos", ")", "ids", "[", "'simple_id'", "]", "=", "parse_simple_id", "(", "chrom", ",", "pos", ",", "ref", ",", "alt", ")", "ids", "[", "'variant_id'", "]", "=", "parse_variant_id", "(", "chrom", ",", "pos", ",", "ref", ",", "alt", ",", "variant_type", ")", "ids", "[", "'display_name'", "]", "=", "parse_display_name", "(", "chrom", ",", "pos", ",", "ref", ",", "alt", ",", "variant_type", ")", "ids", "[", "'document_id'", "]", "=", "parse_document_id", "(", "chrom", ",", "pos", ",", "ref", ",", "alt", ",", "variant_type", ",", "case_id", ")", "return", "ids"], "docstring": "Construct the necessary ids for a variant\n\n    Args:\n        chrom(str): Variant chromosome\n        pos(int): Variant position\n        ref(str): Variant reference\n        alt(str): Variant alternative\n        case_id(str): Unique case id\n        variant_type(str): 'clinical' or 'research'\n\n    Returns:\n        ids(dict): Dictionary with the relevant ids", "docstring_tokens": ["Construct", "the", "necessary", "ids", "for", "a", "variant"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/parse/variant/ids.py#L3-L25", "partition": "test"}
{"repo": "jaraco/svg.charts", "path": "svg/charts/graph.py", "func_name": "Graph.add_popup", "original_string": "def add_popup(self, x, y, label):\n\t\t\"\"\"\n\t\tAdd pop-up information to a point on the graph.\n\t\t\"\"\"\n\t\ttxt_width = len(label) * self.font_size * 0.6 + 10\n\t\ttx = x + [5, -5][int(x + txt_width > self.width)]\n\t\tanchor = ['start', 'end'][x + txt_width > self.width]\n\t\tstyle = 'fill: #000; text-anchor: %s;' % anchor\n\t\tid = 'label-%s' % self._w3c_name(label)\n\t\tattrs = {\n\t\t\t'x': str(tx),\n\t\t\t'y': str(y - self.font_size),\n\t\t\t'visibility': 'hidden',\n\t\t\t'style': style,\n\t\t\t'text': label,\n\t\t\t'id': id,\n\t\t}\n\t\tetree.SubElement(self.foreground, 'text', attrs)\n\n\t\t# add the circle element to the foreground\n\t\tvis_tmpl = (\n\t\t\t\"document.getElementById('{id}').setAttribute('visibility', {val})\"\n\t\t)\n\t\tattrs = {\n\t\t\t'cx': str(x),\n\t\t\t'cy': str(y),\n\t\t\t'r': str(10),\n\t\t\t'style': 'opacity: 0;',\n\t\t\t'onmouseover': vis_tmpl.format(val='visible', id=id),\n\t\t\t'onmouseout': vis_tmpl.format(val='hidden', id=id),\n\t\t}\n\t\tetree.SubElement(self.foreground, 'circle', attrs)", "language": "python", "code": "def add_popup(self, x, y, label):\n\t\t\"\"\"\n\t\tAdd pop-up information to a point on the graph.\n\t\t\"\"\"\n\t\ttxt_width = len(label) * self.font_size * 0.6 + 10\n\t\ttx = x + [5, -5][int(x + txt_width > self.width)]\n\t\tanchor = ['start', 'end'][x + txt_width > self.width]\n\t\tstyle = 'fill: #000; text-anchor: %s;' % anchor\n\t\tid = 'label-%s' % self._w3c_name(label)\n\t\tattrs = {\n\t\t\t'x': str(tx),\n\t\t\t'y': str(y - self.font_size),\n\t\t\t'visibility': 'hidden',\n\t\t\t'style': style,\n\t\t\t'text': label,\n\t\t\t'id': id,\n\t\t}\n\t\tetree.SubElement(self.foreground, 'text', attrs)\n\n\t\t# add the circle element to the foreground\n\t\tvis_tmpl = (\n\t\t\t\"document.getElementById('{id}').setAttribute('visibility', {val})\"\n\t\t)\n\t\tattrs = {\n\t\t\t'cx': str(x),\n\t\t\t'cy': str(y),\n\t\t\t'r': str(10),\n\t\t\t'style': 'opacity: 0;',\n\t\t\t'onmouseover': vis_tmpl.format(val='visible', id=id),\n\t\t\t'onmouseout': vis_tmpl.format(val='hidden', id=id),\n\t\t}\n\t\tetree.SubElement(self.foreground, 'circle', attrs)", "code_tokens": ["def", "add_popup", "(", "self", ",", "x", ",", "y", ",", "label", ")", ":", "txt_width", "=", "len", "(", "label", ")", "*", "self", ".", "font_size", "*", "0.6", "+", "10", "tx", "=", "x", "+", "[", "5", ",", "-", "5", "]", "[", "int", "(", "x", "+", "txt_width", ">", "self", ".", "width", ")", "]", "anchor", "=", "[", "'start'", ",", "'end'", "]", "[", "x", "+", "txt_width", ">", "self", ".", "width", "]", "style", "=", "'fill: #000; text-anchor: %s;'", "%", "anchor", "id", "=", "'label-%s'", "%", "self", ".", "_w3c_name", "(", "label", ")", "attrs", "=", "{", "'x'", ":", "str", "(", "tx", ")", ",", "'y'", ":", "str", "(", "y", "-", "self", ".", "font_size", ")", ",", "'visibility'", ":", "'hidden'", ",", "'style'", ":", "style", ",", "'text'", ":", "label", ",", "'id'", ":", "id", ",", "}", "etree", ".", "SubElement", "(", "self", ".", "foreground", ",", "'text'", ",", "attrs", ")", "# add the circle element to the foreground", "vis_tmpl", "=", "(", "\"document.getElementById('{id}').setAttribute('visibility', {val})\"", ")", "attrs", "=", "{", "'cx'", ":", "str", "(", "x", ")", ",", "'cy'", ":", "str", "(", "y", ")", ",", "'r'", ":", "str", "(", "10", ")", ",", "'style'", ":", "'opacity: 0;'", ",", "'onmouseover'", ":", "vis_tmpl", ".", "format", "(", "val", "=", "'visible'", ",", "id", "=", "id", ")", ",", "'onmouseout'", ":", "vis_tmpl", ".", "format", "(", "val", "=", "'hidden'", ",", "id", "=", "id", ")", ",", "}", "etree", ".", "SubElement", "(", "self", ".", "foreground", ",", "'circle'", ",", "attrs", ")"], "docstring": "Add pop-up information to a point on the graph.", "docstring_tokens": ["Add", "pop", "-", "up", "information", "to", "a", "point", "on", "the", "graph", "."], "sha": "23053497b3f1af4e760f355050107ae3bc05909d", "url": "https://github.com/jaraco/svg.charts/blob/23053497b3f1af4e760f355050107ae3bc05909d/svg/charts/graph.py#L230-L261", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/pylabtools.py", "func_name": "getfigs", "original_string": "def getfigs(*fig_nums):\n    \"\"\"Get a list of matplotlib figures by figure numbers.\n\n    If no arguments are given, all available figures are returned.  If the\n    argument list contains references to invalid figures, a warning is printed\n    but the function continues pasting further figures.\n\n    Parameters\n    ----------\n    figs : tuple\n        A tuple of ints giving the figure numbers of the figures to return.\n    \"\"\"\n    from matplotlib._pylab_helpers import Gcf\n    if not fig_nums:\n        fig_managers = Gcf.get_all_fig_managers()\n        return [fm.canvas.figure for fm in fig_managers]\n    else:\n        figs = []\n        for num in fig_nums:\n            f = Gcf.figs.get(num)\n            if f is None:\n                print('Warning: figure %s not available.' % num)\n            else:\n                figs.append(f.canvas.figure)\n        return figs", "language": "python", "code": "def getfigs(*fig_nums):\n    \"\"\"Get a list of matplotlib figures by figure numbers.\n\n    If no arguments are given, all available figures are returned.  If the\n    argument list contains references to invalid figures, a warning is printed\n    but the function continues pasting further figures.\n\n    Parameters\n    ----------\n    figs : tuple\n        A tuple of ints giving the figure numbers of the figures to return.\n    \"\"\"\n    from matplotlib._pylab_helpers import Gcf\n    if not fig_nums:\n        fig_managers = Gcf.get_all_fig_managers()\n        return [fm.canvas.figure for fm in fig_managers]\n    else:\n        figs = []\n        for num in fig_nums:\n            f = Gcf.figs.get(num)\n            if f is None:\n                print('Warning: figure %s not available.' % num)\n            else:\n                figs.append(f.canvas.figure)\n        return figs", "code_tokens": ["def", "getfigs", "(", "*", "fig_nums", ")", ":", "from", "matplotlib", ".", "_pylab_helpers", "import", "Gcf", "if", "not", "fig_nums", ":", "fig_managers", "=", "Gcf", ".", "get_all_fig_managers", "(", ")", "return", "[", "fm", ".", "canvas", ".", "figure", "for", "fm", "in", "fig_managers", "]", "else", ":", "figs", "=", "[", "]", "for", "num", "in", "fig_nums", ":", "f", "=", "Gcf", ".", "figs", ".", "get", "(", "num", ")", "if", "f", "is", "None", ":", "print", "(", "'Warning: figure %s not available.'", "%", "num", ")", "else", ":", "figs", ".", "append", "(", "f", ".", "canvas", ".", "figure", ")", "return", "figs"], "docstring": "Get a list of matplotlib figures by figure numbers.\n\n    If no arguments are given, all available figures are returned.  If the\n    argument list contains references to invalid figures, a warning is printed\n    but the function continues pasting further figures.\n\n    Parameters\n    ----------\n    figs : tuple\n        A tuple of ints giving the figure numbers of the figures to return.", "docstring_tokens": ["Get", "a", "list", "of", "matplotlib", "figures", "by", "figure", "numbers", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/pylabtools.py#L53-L77", "partition": "test"}
